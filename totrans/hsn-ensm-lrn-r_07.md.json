["```py\n> source(\"Utilities.R\")\n> windows(height=100,width=100)\n> # Ensembling in Classification\n> # Illustrating the ensemble accuracy with same accuracy for each classifier\n> # Different p's and T's with p > 0.5\n> classifiers <- seq(9,45,2) # Number of classifiers \n> accuracy <- seq(0.55,0.85,.05)\n> plot(0,type='n',xlim=range(classifiers),ylim=c(0.6,1),\n+      xlab=\"Number of Classifiers\",ylab=\"Probability of Majority Voting\")\n> for(i in 1:length(accuracy)){\n+   Prob_MV <- NULL\n+   for(j in 1:length(classifiers)){\n+     Prob_MV[j] <- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+        prob=accuracy[i],size=classifiers[j]))\n+   }\n+   points(classifiers,Prob_MV,col=i,\"l\")\n+ }\n> title(\"Classifiers with Accuracy Better Than Random Guess\")\n```", "```py\n> Prob_MV <- NULL\n> for(j in 1:length(classifiers)){\n+   Prob_MV[j] <- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+                            prob=0.65,size=classifiers[j]))\n+ }\n> Prob_MV\n [1] 0.8282807 0.8513163 0.8705318 0.8867689 0.9006211 0.9125264 0.9228185\n [8] 0.9317586 0.9395551 0.9463770 0.9523633 0.9576292 0.9622714 0.9663716\n[15] 0.9699991 0.9732133 0.9760651 0.9785984 0.9808513\n```", "```py\n> # When p < 0.5, ensemble accuracy goes to zero\n> classifiers <- seq(6,50,2)\n> accuracy <- seq(0.45,0.05,-0.05)\n> plot(0,type='n',xlim=range(classifiers),ylim=c(0,0.3),\n+      xlab=\"Number of Classifiers\",ylab=\"Probability of Majority Voting\")\n> for(i in 1:length(accuracy)){\n+   Prob_MV <- NULL\n+   for(j in 1:length(classifiers)){\n+     Prob_MV[j] <- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+                              prob=accuracy[i],size=classifiers[j]))\n+   }\n+   points(classifiers,Prob_MV,col=i,\"l\")\n+   }\n> title(\"Classifiers with Accuracy Worse Than Random Guess\")\n```", "```py\n> classifiers <- seq(10,200,10)\n> Prob_MV <- NULL\n> for(j in 1:length(classifiers)){\n+   Prob_MV[j] <- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+                            prob=0.4999,size=classifiers[j]))\n+ }\n> Prob_MV\n [1] 0.3767071 0.4115491 0.4273344 0.4368132 0.4433011 0.4480955 0.4518222\n [8] 0.4548247 0.4573097 0.4594096 0.4612139 0.4627854 0.4641698 0.4654011\n[15] 0.4665053 0.4675025 0.4684088 0.4692370 0.4699975 0.4706989\n```", "```py\n> accuracy <- 0.5\n> classifiers <- seq(5,45,2)\n> Prob_MV <- NULL\n> for(j in 1:length(classifiers)){\n+   Prob_MV[j] <- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+                            prob=accuracy,size=classifiers[j]))\n+   }\n> Prob_MV\n [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n[19] 0.5 0.5 0.5\n> classifiers <- seq(10,50,2)\n> Prob_MV <- NULL\n> for(j in 1:length(classifiers)){\n+   Prob_MV[j] <- (sum(dbinom(floor(classifiers[j]/2):classifiers[j],\n+                             prob=accuracy,size=classifiers[j]))+\n+                    sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],\n+                               prob=accuracy,size=classifiers[j])))/2\n+   }\n> Prob_MV\n [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n[19] 0.5 0.5 0.5\n```", "```py\n> Get_Prob <- function(Logical,Probability){\n+   return(t(ifelse(Logical,Probability,1-Probability)))\n+ }\n```", "```py\n> # Different accuracies T's illustration\n> # For simplicity, we set the number of classifiers at odd number\n> # Each p_i's greater than 0.5\n> accuracy <- c(0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9)\n> NT <- length(accuracy) # Number of classifiers \n> APC <- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations\n> head(APC)\n   Var1  Var2  Var3 Var4 Var5 Var6 Var7 Var8 Var9\n1  TRUE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n2 FALSE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n3  TRUE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n4 FALSE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n5  TRUE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n6 FALSE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n> Elements_Prob <- t(apply(APC,1,Get_Prob,Probability=accuracy))\n> head(Elements_Prob)\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n[1,]  0.5 0.55  0.6 0.65  0.7 0.75  0.8 0.85  0.9\n[2,]  0.5 0.55  0.6 0.65  0.7 0.75  0.8 0.85  0.9\n[3,]  0.5 0.45  0.6 0.65  0.7 0.75  0.8 0.85  0.9\n[4,]  0.5 0.45  0.6 0.65  0.7 0.75  0.8 0.85  0.9\n[5,]  0.5 0.55  0.4 0.65  0.7 0.75  0.8 0.85  0.9\n[6,]  0.5 0.55  0.4 0.65  0.7 0.75  0.8 0.85  0.9\n> Events_Prob <- apply(Elements_Prob,1,prod)\n> Majority_Events <- (rowSums(APC)>NT/2)\n> sum(Events_Prob*Majority_Events)\n[1] 0.9112646\n```", "```py\n> accuracy <- c(0.7,0.7,0.7,0.9,0.9)\n> NT <- length(accuracy) # Number of classifiers\n> APC <- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations\n> Elements_Prob <- t(apply(APC,1,Get_Prob,Probability=accuracy))\n> Events_Prob <- apply(Elements_Prob,1,prod)\n> Majority_Events <- (rowSums(APC)>NT/2)\n> sum(Events_Prob*Majority_Events)\n[1] 0.93268\n```", "```py\n> # Each p_i's lesser than 0.5\n> accuracy <- 1-c(0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9)\n> NT <- length(accuracy) # Number of classifiers\n> APC <- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations\n> head(APC)\n   Var1  Var2  Var3 Var4 Var5 Var6 Var7 Var8 Var9\n1  TRUE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n2 FALSE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n3  TRUE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n4 FALSE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n5  TRUE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n6 FALSE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE\n> Elements_Prob <- t(apply(APC,1,Get_Prob,Probability=accuracy))\n> head(Elements_Prob)\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n[1,]  0.5 0.45  0.4 0.35  0.3 0.25  0.2 0.15  0.1\n[2,]  0.5 0.45  0.4 0.35  0.3 0.25  0.2 0.15  0.1\n[3,]  0.5 0.55  0.4 0.35  0.3 0.25  0.2 0.15  0.1\n[4,]  0.5 0.55  0.4 0.35  0.3 0.25  0.2 0.15  0.1\n[5,]  0.5 0.45  0.6 0.35  0.3 0.25  0.2 0.15  0.1\n[6,]  0.5 0.45  0.6 0.35  0.3 0.25  0.2 0.15  0.1\n> Events_Prob <- apply(Elements_Prob,1,prod)\n> Majority_Events <- (rowSums(APC)>NT/2)\n> sum(Events_Prob*Majority_Events)\n[1] 0.08873544\n```", "```py\n> # Mixture of p_i's, some > 0.5, and some < 0.5\n> Random_Accuracy <- function() {\n+   accuracy <- runif(9)\n+   NT <- length(accuracy) \n+   APC <- expand.grid(rep(list(c(TRUE,FALSE)),NT)) \n+   Elements_Prob <- t(apply(APC,1,Get_Prob,Probability=accuracy))\n+   Events_Prob <- apply(Elements_Prob,1,prod)\n+   Majority_Events <- (rowSums(APC)>NT/2)\n+   return(sum(Events_Prob*Majority_Events))\n+ }\n> Random_Accuracy()\n[1] 0.3423631\n> Random_Accuracy()\n[1] 0.3927145\n> Random_Accuracy()\n[1] 0.5341844\n> Random_Accuracy()\n[1] 0.1624876\n> Random_Accuracy()\n[1] 0.4065803\n> Random_Accuracy()\n[1] 0.4687087\n> Random_Accuracy()\n[1] 0.7819835\n> Random_Accuracy()\n[1] 0.3124515\n> Random_Accuracy()\n[1] 0.6842173\n> Random_Accuracy()\n[1] 0.2531727\n```", "```py\n> load(\"../Data/GC2.RData\")\n> set.seed(12345)\n> Train_Test <- sample(c(\"Train\",\"Test\"),nrow(GC2),\n+ replace = TRUE,prob = c(0.7,0.3))\n> GC2_Train <- GC2[Train_Test==\"Train\",]\n> GC2_TestX <- within(GC2[Train_Test==\"Test\",],rm(good_bad))\n> GC2_TestY <- GC2[Train_Test==\"Test\",\"good_bad\"]\n> GC2_Formula <- as.formula(\"good_bad~.\")\n> # RANDOM FOREST ANALYSIS\n> GC2_RF <- randomForest(GC2_Formula,data=GC2_Train,keep.inbag=TRUE,\n+                        ntree=500)\n```", "```py\n> # New data voting\n> GC2_RF_Test_Margin <- predict(GC2_RF,newdata = GC2_TestX,\n+                          type=\"class\")\n> GC2_RF_Test_Predict <- predict(GC2_RF,newdata=GC2_TestX,\n+                           type=\"class\",predict.all=TRUE\n+                           )\n```", "```py\n> Row_Count_Max <- function(x) names(which.max(table(x))) \n> # Majority Voting\n> Voting_Predict <- apply(GC2_RF_Test_Predict$individual,1,\n+ Row_Count_Max)\n> head(Voting_Predict);tail(Voting_Predict)\n     1      2      3      4      9     10 \n\"good\"  \"bad\" \"good\"  \"bad\" \"good\"  \"bad\" \n   974    980    983    984    988    996 \n \"bad\"  \"bad\" \"good\" \"good\" \"good\" \"good\" \n> all(Voting_Predict==GC2_RF_Test_Predict$aggregate)\n[1] TRUE\n> all(Voting_Predict==GC2_RF_Test_Margin)\n[1] TRUE\n> sum(Voting_Predict==GC2_TestY)/313\n[1] 0.7795527\n```", "```py\n> # Analyzing Accuracy of Trees of the Fitted Forest\n> GC2_RF_Train_Predict <- predict(GC2_RF,newdata=GC2_Train[,-20],\n+                                 type=\"class\",predict.all=TRUE)\n> head(GC2_RF_Train_Predict$individual[,c(1:5,496:500)])  \n   [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]   [,10] \n5  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"good\" \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\" \n6  \"good\" \"good\" \"good\" \"good\" \"good\" \"good\" \"bad\"  \"bad\"  \"bad\"  \"good\"\n7  \"good\" \"good\" \"good\" \"good\" \"good\" \"good\" \"good\" \"good\" \"good\" \"good\"\n8  \"good\" \"good\" \"good\" \"good\" \"good\" \"bad\"  \"good\" \"bad\"  \"good\" \"good\"\n11 \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"bad\" \n12 \"good\" \"bad\"  \"bad\"  \"bad\"  \"bad\"  \"good\" \"bad\"  \"bad\"  \"bad\"  \"bad\" \n> RF_Tree_Train_Accuracy <- NULL\n> for(i in 1:GC2_RF$ntree){\n+   RF_Tree_Train_Accuracy[i] <- sum(GC2_RF_Train_Predict$individual[,i]==\n+                                   GC2_Train$good_bad)/nrow(GC2_Train)\n+ }\n> headtail(sort(RF_Tree_Train_Accuracy),10)\n [1] 0.8340611 0.8369723 0.8384279 0.8398836 0.8398836 0.8413392 0.8413392\n [8] 0.8413392 0.8413392 0.8427948 0.8908297 0.8908297 0.8908297 0.8908297\n[15] 0.8922853 0.8922853 0.8937409 0.8937409 0.8966521 0.8981077\n```", "```py\n> # Bagging ANALYSIS\n> GC2_Bagg <- randomForest(GC2_Formula,data=GC2_Train,keep.inbag=TRUE,\n+                          mtry=ncol(GC2_TestX),ntree=500)\n> GC2_Bagg_Test_Predict <- predict(GC2_Bagg,newdata=GC2_TestX,\n+                                 type=\"class\",predict.all=TRUE)\n> GC2_Bagg_Train_Predict <- predict(GC2_Bagg,newdata=GC2_Train[,-20],\n+                                 type=\"class\",predict.all=TRUE)\n> Bagg_Tree_Train_Accuracy <- NULL\n> for(i in 1:GC2_Bagg$ntree){\n+   Bagg_Tree_Train_Accuracy[i] <- sum(GC2_Bagg_Train_Predict$individual[,i]==\n+                                   GC2_Train$good_bad)/nrow(GC2_Train)\n+ }\n> headtail(sort(Bagg_Tree_Train_Accuracy),10)\n [1] 0.8369723 0.8384279 0.8413392 0.8457060 0.8457060 0.8471616 0.8471616\n [8] 0.8471616 0.8471616 0.8486172 0.8966521 0.8966521 0.8966521 0.8966521\n[15] 0.8966521 0.8981077 0.8995633 0.8995633 0.9024745 0.9097525\n```", "```py\n> # Weighted Voting with Random Forest\n> RF_Weights <- RF_Tree_Train_Accuracy/sum(RF_Tree_Train_Accuracy)\n> Bagg_Weights <- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)\n> RF_Weighted_Vote <- data.frame(matrix(0,nrow(GC2_TestX),ncol=3))\n> names(RF_Weighted_Vote) <- c(\"Good_Weight\",\"Bad_Weight\",\"Prediction\")\n> for(i in 1:nrow(RF_Weighted_Vote)){\n+   RF_Weighted_Vote$Good_Weight[i] <- \n+     sum((GC2_RF_Test_Predict$individual[i,]==\"good\")*RF_Weights)\n+   RF_Weighted_Vote$Bad_Weight[i] <- \n+     sum((GC2_RF_Test_Predict$individual[i,]==\"bad\")*RF_Weights)\n+   RF_Weighted_Vote$Prediction[i] <- c(\"good\",\"bad\")[which.max(RF_Weighted_Vote[i,1:2])]\n+ }\n> head(RF_Weighted_Vote,10)\n   Good_Weight Bad_Weight Prediction\n1    0.8301541 0.16984588       good\n2    0.3260033 0.67399668        bad\n3    0.8397035 0.16029651       good\n4    0.4422527 0.55774733        bad\n5    0.9420565 0.05794355       good\n6    0.2378956 0.76210442        bad\n7    0.4759756 0.52402435        bad\n8    0.7443038 0.25569624       good\n9    0.8120180 0.18798195       good\n10   0.7799587 0.22004126       good\n```", "```py\n> # Weighted Voting with Bagging\n> Bagg_Weights <- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)\n> Bagg_Weights <- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)\n> Bagg_Weighted_Vote <- data.frame(matrix(0,nrow(GC2_TestX),ncol=3))\n> names(Bagg_Weighted_Vote) <- c(\"Good_Weight\",\"Bad_Weight\",\"Prediction\")\n> for(i in 1:nrow(Bagg_Weighted_Vote)){\n+   Bagg_Weighted_Vote$Good_Weight[i] <- \n+     sum((GC2_Bagg_Test_Predict$individual[i,]==\"good\")*Bagg_Weights)\n+   Bagg_Weighted_Vote$Bad_Weight[i] <- \n+     sum((GC2_Bagg_Test_Predict$individual[i,]==\"bad\")*Bagg_Weights)\n+   Bagg_Weighted_Vote$Prediction[i] <- c(\"good\",\"bad\")[which.max(Bagg_Weighted_Vote[i,1:2])]\n+ }\n> head(Bagg_Weighted_Vote,10)\n   Good_Weight Bad_Weight Prediction\n1    0.9279982 0.07200181       good\n2    0.1634505 0.83654949        bad\n3    0.8219618 0.17803818       good\n4    0.4724477 0.52755226        bad\n5    0.9619528 0.03804725       good\n6    0.1698628 0.83013718        bad\n7    0.4540574 0.54594265        bad\n8    0.7883772 0.21162281       good\n9    0.8301772 0.16982283       good\n10   0.7585720 0.24142804       good\n```", "```py\n> # Averaging for Regression Problems\n> load(\"../Data/ht_imp_author.Rdata\") # returns ht_imp object\n> load(\"../Data/htest_imp_author.Rdata\") # returns htest_imp\n> names(ht_imp)[69] <- \"SalePrice\"\n> dim(ht_imp)\n[1] 1460   69\n> dim(htest_imp)\n[1] 1459   68\n```", "```py\n> hf <- as.formula(\"SalePrice~.\")\n> SP_lm <- lm(hf,data=ht_imp)\n> SP_rpart2 <- rpart(hf,data=ht_imp,maxdepth=2)\n> SP_rpart4 <- rpart(hf,data=ht_imp,maxdepth=4)\n> SP_rpart6 <- rpart(hf,data=ht_imp,maxdepth=6)\n> SP_rpart8 <- rpart(hf,data=ht_imp,maxdepth=8)\n> SP_nn2 <- nnet(hf,data=ht_imp,size=2,linout=TRUE)\n# weights:  267\ninitial  value 56996872361441.906250 \nfinal  value 9207911334609.976562 \nconverged\n> SP_nn3 <- nnet(hf,data=ht_imp,size=3,linout=TRUE)\n# weights:  400\ninitial  value 56997125121706.257812 \nfinal  value 9207911334609.960938 \nconverged\n> SP_nn4 <- nnet(hf,data=ht_imp,size=4,linout=TRUE)\n# weights:  533\ninitial  value 56996951452602.304687 \niter  10 value 19328028546738.226562\niter  20 value 19324281941793.617187\nfinal  value 9080312934601.205078 \nconverged\n> SP_nn5 <- nnet(hf,data=ht_imp,size=5,linout=TRUE)\n# weights:  666\ninitial  value 56997435951836.507812 \nfinal  value 9196060713131.609375 \nconverged\n> SP_svm <- svm(hf,data=ht_imp)\n```", "```py\n> # Simple Averaging\n> SP_lm_pred <- predict(SP_lm,newdata=htest_imp)\nWarning message:\nIn predict.lm(SP_lm, newdata = htest_imp) :\n  prediction from a rank-deficient fit may be misleading\n> SP_rpart2_pred <- predict(SP_rpart2,newdata=htest_imp)\n> SP_rpart4_pred <- predict(SP_rpart4,newdata=htest_imp)\n> SP_rpart6_pred <- predict(SP_rpart6,newdata=htest_imp)\n> SP_rpart8_pred <- predict(SP_rpart8,newdata=htest_imp)\n> SP_nn2_pred <- predict(SP_nn2,newdata=htest_imp)\n> SP_nn3_pred <- predict(SP_nn3,newdata=htest_imp)\n> SP_nn4_pred <- predict(SP_nn4,newdata=htest_imp)\n> SP_nn5_pred <- predict(SP_nn5,newdata=htest_imp)\n> SP_svm_pred <- predict(SP_svm,newdata=htest_imp)\n```", "```py\n> windows(height=300,width=400)\n> par(mfrow=c(2,5))\n> plot.ts(SP_lm_pred,col=1)\n> plot.ts(SP_rpart2_pred,col=2)\n> plot.ts(SP_rpart4_pred,col=3)\n> plot.ts(SP_rpart6_pred,col=4)\n> plot.ts(SP_rpart8_pred,col=5)\n> plot.ts(SP_nn2_pred,col=6)\n> plot.ts(SP_nn3_pred,col=7)\n> plot.ts(SP_nn4_pred,col=8)\n> plot.ts(SP_nn5_pred,col=9)\n> plot.ts(SP_svm_pred,col=10)\n```", "```py\n> Avg_Ensemble_Prediction <- rowMeans(cbind(SP_lm_pred,SP_rpart2_pred,\n+     SP_rpart4_pred,SP_rpart6_pred,\n+                SP_rpart8_pred,SP_nn4_pred,SP_nn5_pred,SP_svm_pred))\n> plot.ts(Avg_Ensemble_Prediction)\n```", "```py\n> # Weighted Averaging\n> SP_lm_sigma <- mean(residuals(SP_lm)^2)\n> SP_rp2_sigma <- mean(residuals(SP_rpart2)^2)\n> SP_rp4_sigma <- mean(residuals(SP_rpart4)^2)\n> SP_rp6_sigma <- mean(residuals(SP_rpart6)^2)\n> SP_rp8_sigma <- mean(residuals(SP_rpart8)^2)\n> SP_nn4_sigma <- mean(residuals(SP_nn4)^2)\n> SP_nn5_sigma <- mean(residuals(SP_nn5)^2)\n> SP_svm_sigma <- mean(residuals(SP_svm)^2)\n```", "```py\n> sigma_sum <- SP_lm_sigma + SP_rp2_sigma + SP_rp4_sigma +\n+   SP_rp6_sigma + SP_rp8_sigma + SP_nn4_sigma +\n+   SP_nn5_sigma + SP_svm_sigma\n> sigma_sum\n[1] 20727111061\n> SP_lm_wts <- SP_lm_sigma/sigma_sum\n> SP_rp2_wts <- SP_rp2_sigma/sigma_sum\n> SP_rp4_wts <- SP_rp4_sigma/sigma_sum\n> SP_rp6_wts <- SP_rp6_sigma/sigma_sum\n> SP_rp8_wts <- SP_rp8_sigma/sigma_sum\n> SP_nn4_wts <- SP_nn4_sigma/sigma_sum\n> SP_nn5_wts <- SP_nn5_sigma/sigma_sum\n> SP_svm_wts <- SP_svm_sigma/sigma_sum\n```", "```py\n> Weighted_Ensemble_Prediction <- rowMeans(cbind(SP_lm_wts*SP_lm_pred,\n+                                           SP_rp2_wts*SP_rpart2_pred,\n+                                           SP_rp4_wts*SP_rpart4_pred,\n+                                           SP_rp6_wts*SP_rpart6_pred,\n+                                           SP_rp8_wts*SP_rpart8_pred,\n+                                           SP_nn4_wts*SP_nn4_pred,\n+                                           SP_nn5_wts*SP_nn5_pred,\n+                                           SP_svm_wts*SP_svm_pred))\n> plot.ts(Weighted_Ensemble_Prediction)\n```", "```py\n> SP_lm_train <- predict(SP_lm,newdata=ht_imp)\nWarning message:\nIn predict.lm(SP_lm, newdata = ht_imp) :\n  prediction from a rank-deficient fit may be misleading\n> SP_rpart2_train <- predict(SP_rpart2,newdata=ht_imp)\n> SP_rpart4_train <- predict(SP_rpart4,newdata=ht_imp)\n> SP_rpart6_train <- predict(SP_rpart6,newdata=ht_imp)\n> SP_rpart8_train <- predict(SP_rpart8,newdata=ht_imp)\n> SP_nn4_train <- predict(SP_nn4,newdata=ht_imp)\n> SP_nn5_train <- predict(SP_nn5,newdata=ht_imp)\n> SP_svm_train <- predict(SP_svm,newdata=ht_imp)\n> \n> ht_imp2 <- cbind(ht_imp[,-69],SP_lm_train,SP_rpart2_train,SP_rpart4_train,\n+                           SP_rpart6_train,SP_rpart8_train,SP_nn4_train,SP_nn5_train,\n+                           SP_svm_train,ht_imp[,69])\n> names(ht_imp2)[77] <- \"SalePrice\"\n> SP_gbm <- gbm(hf,data=ht_imp2,distribution = \"gaussian\",n.trees=200)\n> headtail(predict(SP_gbm,n.trees=100),20)\n [1] 180260.6 177793.3 181836.9 177793.3 191927.7 177793.3 191927.7 182237.3\n [9] 177793.3 177793.3 177793.3 191927.7 177793.3 187520.7 177793.3 177793.3\n[17] 177793.3 177793.3 177793.3 177793.3 177908.2 177793.3 191927.7 177793.3\n[25] 177793.3 177793.3 177793.3 191927.7 177793.3 177793.3 177793.3 191927.7\n[33] 177793.3 177793.3 177793.3 177793.3 179501.7 191927.7 177793.3 177793.3\n```"]