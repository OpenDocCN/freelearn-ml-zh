- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving the Performance of Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about different techniques for properly
    validating and assessing the performance of your machine learning models. The
    next step is to extend your knowledge of those techniques for improving the performance
    of your models.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about techniques to improve the performance
    and generalizability of your models by working on the data or algorithm you select
    for machine learning modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Options for improving model performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthetic data generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving pre-training data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization to improve model generalizability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be familiar with different techniques to
    improve the performance and generalizability of your models and you will know
    how you can benefit from Python in implementing them for your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements are needed for this chapter as they help you better
    understand the concepts and enable you to use them in your projects and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ray` >= 2.3.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tune_sklearn` >= 0.4.5'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bayesian_optimization` >= 1.4.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` >= 3.7.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge of machine learning validation techniques such as *k*-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: Options for improving model performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The changes we can make to improve the performance of our models could be related
    to the algorithms we use or the data we feed them to train our models (see *Table
    5.1*). Adding more data points could reduce the variance of the model, for example,
    by adding data close to the decision boundaries of classification models to increase
    confidence in the identified boundaries and reduce overfitting. Removing outliers
    could reduce both bias and variance by eliminating the effect of distant data
    points. Adding more features could help the model to become better at the training
    stage (that is, lower model bias), but it might result in higher variance. There
    could also be features that cause overfitting and their removal could help to
    increase model generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Change** | **Potential effect** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Adding more training data points | Reducing variance | We could add new data
    points randomly or try to add data points with specific feature values, output
    values, or labels. |'
  prefs: []
  type: TYPE_TB
- en: '| Removing outliers with lower stringency | Reducing bias and variance | Removing
    outliers could reduce errors in the training set but it could also help in training
    a more generalizable model (that is, a model with lower variance). |'
  prefs: []
  type: TYPE_TB
- en: '| Adding more features | Reducing bias | We could add features that provide
    unknown information to the model. For example, adding the crime rate of a neighborhood
    for house price prediction could improve model performance if that info is not
    already captured by existing features. |'
  prefs: []
  type: TYPE_TB
- en: '| Removing features | Reducing variance | Each feature could have a positive
    effect on training performance but it might add information that is not generalizable
    to new data points and result in higher variance. |'
  prefs: []
  type: TYPE_TB
- en: '| Running optimization process for more iterations | Reducing bias | Optimizing
    for more iteration reduces training error but might result in overfitting. |'
  prefs: []
  type: TYPE_TB
- en: '| Using more complex models | Reducing bias | Increasing the depth of decision
    trees is an example of an increase in model complexity that could result in lower
    model bias but potentially a higher chance of overfitting. |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Some of the options to reduce the bias and/or variance of machine
    learning models
  prefs: []
  type: TYPE_NORMAL
- en: Increasing model complexity could help to reduce bias, as we discussed in the
    previous chapter, but a model can have many hyperparameters that affect its complexity
    or result in improving or lowering model bias and generalizability. Some of the
    hyperparameters that you could start with in the optimization process for the
    widely used supervised and unsupervised learning methods are provided in *Table
    5.2*. These hyperparameters should help you to improve the performance of your
    models, but you don’t need to write new functions or classes for hyperparameter
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Hyperparameter** |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic regression`sklearn.linear_model.LogisticRegression()` |'
  prefs: []
  type: TYPE_TB
- en: '`penalty`: Choosing regularization between `l1`, `l2`, `elasticnet`, and `None`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_weight`: Associating weights to classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_ratio`: The Elastic-Net mixing parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| K-Nearest Neighbors`sklearn.neighbors.KNeighborsClassifier()` |'
  prefs: []
  type: TYPE_TB
- en: '`n_neighbors`: Number of neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights`: Choosing between `uniform` or `distance` to use neighbors equally
    or assign weights to them based on their distance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Support Vector Machine** (**SVM**) classifier or regressor`sklearn.svm.SVC()``sklearn.svm.SVR()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`C`: Inverse strength of regularization with the `l2` penalty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: An SVM kernel with prebuilt kernels including `linear`, `poly`, `rbf`,
    `sigmoid`, and `precomputed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`degree` (degree of polynomial): Degree of the polynomial kernel function (`poly`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_weight` (only for classification): Associating weights with classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Random forest classifier or regressor`sklearn.ensemble.RandomForestClassifier()``sklearn.ensemble.RandomForestRegressor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`n_estimators`: Number of trees in the forest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum depth of the trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_weight`: Associating weights with classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: The minimum number of samples required to be at a leaf
    node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| XGBoost classifier or regressor`xgboost.XGBClassifier()``xgboost.XGBRegressor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`booster` (`gbtree`, `gblinear`, or `dart`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a tree booster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eta`: Step size shrinkage to prevent overfitting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum depth of the trees'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_child_weight`: The minimum sum of data point weights needed to continue
    partitioning'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lambda`: L2 regularization factor'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: L1 regularization factor'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| LightGBM classifier or regressor`Lightgbm.LGBMClassifier()``Lightgbm.LGBMRegressor()`
    |'
  prefs: []
  type: TYPE_TB
- en: '`boosting_type` (`gbdt`, `dart`, or `rf`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_leaves`: Maximum tree leaves'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum tree depth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators`: Number of boosted trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_alpha`: L1 regularization term on weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_lambda`: L2 regularization term on weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| K-Means clustering`sklearn.cluster.KMeans()` |'
  prefs: []
  type: TYPE_TB
- en: '`n_clusters`: Number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Agglomerative clustering`sklearn.cluster.AgglomerativeClustering()` |'
  prefs: []
  type: TYPE_TB
- en: '`n_clusters`: Number of clusters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric`: Distance measures with prebuilt measures including `euclidean`, `l1`,
    `l2`, `manhattan`, `cosine`, or `precomputed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linkage`: Linkage criterion with prebuilt methods including `ward`, `complete`,
    `average`, and `single`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DBSCAN clustering`sklearn.cluster.DBSCAN()` |'
  prefs: []
  type: TYPE_TB
- en: '`eps`: Maximum allowed distance between data points for them to be considered
    neighbors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples`: The minimum number of neighbors a data point needs to be considered
    a core point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| UMAP`umap.UMAP()` |'
  prefs: []
  type: TYPE_TB
- en: '`n_neighbors`: Constraining the size of the local neighborhood for the learning
    data structure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_dist`: Controlling the compactness of groups in low-dimensional space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 – Some of the most important hyperparameters of widely used supervised
    and unsupervised machine learning methods to start hyperparameter optimization
    with
  prefs: []
  type: TYPE_NORMAL
- en: The Python libraries listed in *Table 5.3* have modules dedicated to different
    hyperparameter optimization techniques such as *grid search*, *random search*,
    *Bayesian search*, and *successive halving*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Library** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| `scikit-optimize` | [https://pypi.org/project/scikit-optimize/](https://pypi.org/project/scikit-optimize/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Optuna` | [https://pypi.org/project/optuna/](https://pypi.org/project/optuna/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `GpyOpt` | [https://pypi.org/project/GPyOpt/](https://pypi.org/project/GPyOpt/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Hyperopt` | [https://hyperopt.github.io/hyperopt/](https://hyperopt.github.io/hyperopt/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ray.tune` | [https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 5.3 – Commonly used Python libraries for hyperparameter optimization
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about each method in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method is about determining a series of hyperparameter nomination sets
    to be tested one by one to find the optimum combination. The cost of grid-searching
    to find an optimal combination is high. Also, considering there would be a specific
    set of hyperparameters that mattered for each problem, grid search with a predetermined
    set of hyperparameter combinations for all problems is not an effective approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of grid search hyperparameter optimization using `sklearn.model_selection.GridSearchCV()`
    for a random forest classifier model. 80% of the data is used for hyperparameter
    optimization and the performance of the model is assessed using stratified 5-fold
    CV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code, 10 estimators are used and different `min_samples_split` and `max_depth`
    values are considered for the hyperparameter optimization process. You can specify
    different performance metrics according to what you learned in the previous chapter
    using the scoring parameter, as one of the parameters of `sklearn.model_selection.GridSearchCV()`.
    A combination of `max_depth` of `10` and `min_samples_split` of `7` was identified
    as the best hyperparameter set in this case, which resulted in 0.948 accuracy
    using the stratified 5-fold CV. We can extract the best hyperparameter and corresponding
    score using `sklearn_gridsearch.best_params_` and `sklearn_gridsearch.best_score_`.
  prefs: []
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method is an alternative to grid search. It randomly tries different combinations
    of hyperparameter values. For the same high enough computational budget, it is
    shown that random search can achieve a higher performance model compared to grid
    search, as it can search a larger space (Bergstra and Bengio, 2012).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of random search hyperparameter optimization using `sklearn.model_selection.RandomizedSearchCV()`
    for the same model and data used in the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: With only five iterations, this random search resulted in 0.942 CV accuracy
    with less than one-third of running time, which could depend on your local or
    cloud system configuration. In this case, a combination of `max_depth` of `15`
    and `min_samples_split` of `7` was identified as the best hyperparameter set.
    Comparing the results of grid search and random search, we can conclude that models
    with different `max_depth` values could result in similar CV accuracies for this
    specific case of random forest modeling with 10 estimators using the digit dataset
    from `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Bayesian optimization, instead of randomly selecting hyperparameter combinations
    without checking the value of previous combination sets, each combination of hyperparameter
    sets gets selected in an iteration based on the history of previously tested hyperparameter
    sets. This process helps to reduce the computational cost compared to grid search
    but it doesn’t always beat random search. We want to use Ray Tune (`ray.tune`)
    here for this approach. You can read more about different functionalities available
    in Ray Tune such as *logging tune runs*, *how to stop and resume*, *analyzing
    tune experiment results*, and *deploying tune in the cloud* on the tutorial page:
    [https://docs.ray.io/en/latest/tune/tutorials/overview.html](https://docs.ray.io/en/latest/tune/tutorials/overview.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following implementation of Bayesian hyperparameter optimization using
    `ray.tune.sklearn.TuneSearchCV()` for the same random forest model, as explained
    previously, achieves 0.942 CV accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Successive halving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind successive having is to not invest in all hyperparameters equally.
    Candidate hyperparameter sets get evaluated using limited resources, for example,
    using only a fraction of training data or a limited number of trees in a random
    forest model in an iteration, and some of them pass to the next iteration. In
    later iterations, more resources get used until the last iteration in which all
    resources, for example, all training data, get used to evaluate the remaining
    hyperparameter sets. You can use `HalvingGridSearchCV()`and `HalvingRandomSearchCV()`as
    part of `sklearn.model_selection` to try out successive halving. You can read
    more about these two Python modules at [https://scikit-learn.org/stable/modules/grid_search.html#id3](https://scikit-learn.org/stable/modules/grid_search.html#id3).
  prefs: []
  type: TYPE_NORMAL
- en: There are other hyperparameter optimization techniques, such as **Hyperband**
    (Li et al., 2017) and **BOHB** (Falkner et al., 2018) that you can try out, but
    the general idea behind most advancements in hyperparameter optimization is to
    minimize the computational resources necessary to achieve an optimum hyperparameter
    set. There are also techniques and libraries for hyperparameter optimization in
    deep learning, which we will cover in [*Chapter 12*](B16369_12.xhtml#_idTextAnchor320),
    *Going Beyond ML Debugging with Deep Learning*, and [*Chapter 13*](B16369_13.xhtml#_idTextAnchor342),
    *Advanced Deep Learning Techniques*. Although hyperparameter optimization helps
    us to get better models, using the provided data for model training and the selected
    machine learning method, we can improve model performance with other approaches,
    such as generating synthetic data for model training, which is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data we have access to for training and evaluating our machine learning
    models may be limited. For example, in the case of classification models, we might
    have classes with a limited number of data points, resulting in lower performance
    of our models for unseen data points of the same classes. We will go through a
    few methods here to help you improve the performance of your models in these situations.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling for imbalanced data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imbalanced data classification is challenging due to the dominating effect of
    majority classes during training as well as in model performance reporting. For
    model performance reporting, we discussed different performance metrics in the
    previous chapter and how you can select a reliable metric even in the case of
    imbalanced data classification. Here, we want to talk about the concept of oversampling
    to help you improve the performance of your models by synthetically improving
    your training data. The concept of oversampling is to increase the number of data
    points in your minority classes using the real data points you have in your dataset.
    The simplest way of thinking about it is to duplicate some of the data points
    in minority classes, which is not a good approach as they will not provide complementary
    information to real data in the training process. There are techniques designed
    for oversampling processes, such as the **Synthetic Minority Oversampling Technique**
    (**SMOTE**) and its variations for tabular data, which we will present here.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling
  prefs: []
  type: TYPE_NORMAL
- en: In classifying imbalanced data, an alternative to oversampling is to decrease
    the imbalance by sampling the majority class. This process reduces the ratio of
    the majority-class to minority-class data points. As not all the data points get
    included in one set of sampling, multiple models can be built by sampling different
    subsets of majority-class data points and the output of those models can be combined,
    for example, through majority voting between the models. This process is called
    **undersampling**. Oversampling usually results in higher performance improvement
    compared to undersampling.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SMOTE is an old yet widely used approach to oversampling the minority class,
    for continuous feature sets, using the distribution of neighboring data points
    (Chawla et al., 2022; see *Figure 5**.1*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE,
    Borderline-SMOTE, and ADASYN](img/B16369_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE,
    Borderline-SMOTE, and ADASYN
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps in generating any synthetic data point using SMOTE can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a random data point from a minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the K-Nearest Neighbors for that data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose one of the neighbors randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a synthetic data point at a randomly selected point between the two
    data points in the feature space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE and two of its variations, **Borderline-SMOTE** and **Adaptive synthetic**
    (**ADASYN**), are shown in *Figure 5**.1*. *Steps 2* to *4* of SMOTE, Borderline-SMOTE,
    and ADASYN are similar. However, Borderline-SMOTE focuses on the real data points
    that divide the classes and ADASYN focuses on the data points of the minority
    class in regions of the feature space dominated by the majority classes. In this
    way, Borderline-SMOTE increases the confidence in decision boundary identification
    to avoid overfitting and ADASYN improves generalizability for minority-class prediction
    in the parts of the space dominated by majority classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `imblearn` Python library for synthetic data generation using
    SMOTE, Borderline-SMOTE, and ADASYN. However, before getting into using these
    functionalities, we need to write a plotting function for later use to show the
    data before and after the oversampling process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we generate a synthetic dataset with two classes and only two features
    (that is, two-dimensional data) and consider it our real dataset. We consider
    100 data points in one of the classes as the majority class, and 10 data points
    in another class as the minority class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The resulting data points are shown in the following scatter plot with red and
    black data points representing the minority and majority classes, respectively.
    We are using this synthetic data instead of a real dataset to visually show you
    how different synthetic data generation methods work.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Example dataset with two features (that is, dimensions), generated
    synthetically, to use for practicing with SMOTE and its alternatives](img/B16369_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Example dataset with two features (that is, dimensions), generated
    synthetically, to use for practicing with SMOTE and its alternatives
  prefs: []
  type: TYPE_NORMAL
- en: 'We now use SMOTE via `imblearn.over_sampling.SMOTE()`, as shown in the following
    code snippet, to generate synthetic data points for the minority class only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the following figure, the new oversampled data points will
    be within the gaps between the original data points of the minority class (that
    is, red data points). However, many of these new data points don’t help to identify
    a reliable decision boundary as they are grouped in the very top-right corner,
    far from the black data points and potential decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing
    SMOTE](img/B16369_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing
    SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Borderline-SMOTE instead via `imblearn.over_sampling.BorderlineSMOTE()`
    as follows for synthetic data generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the new synthetically generated data points are closer to the
    black data points of the majority class, which helps with identifying a generalizable
    decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing
    Borderline-SMOTE](img/B16369_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing
    Borderline-SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use ADASYN via `imblearn.over_sampling.ADASYN()`, which also generates
    more of the new synthetic data close to the black data points as it focuses on
    the regions with more majority-class samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The data including original and synthetically generated data points using ADASYN
    are shown in *Figure 5**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing
    ADASYN](img/B16369_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing
    ADASYN
  prefs: []
  type: TYPE_NORMAL
- en: There have been more recent methods built upon SMOTE for synthetic data generation
    such as **density-based synthetic minority over-sampling technique** (**DSMOTE**)
    (Xiaolong et al., 2019) and **k-means SMOTE** (Felix et al., 2017). Both of these
    methods try to capture groupings of data points either within the target minority
    class or the whole dataset. In DSMOTE, **Density-based spatial clustering of applications
    with noise** (**DBSCAN**) is used to divide data points of the minority class
    into three groups of core samples, borderline samples, and noise (i.e., outlying)
    samples, and then the core and borderline samples only get used for oversampling.
    This approach is shown to work better than SMOTE and Borderline-SMOTE (Xiaolong
    et al., 2019). K-means SMOTE is another recent alternative to SMOTE (Last et al.,
    2017) that relies on clustering of the whole dataset using a k-means clustering
    algorithm before oversampling (see *Figure 5**.6*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE
    (Last et al. 2017))](img/B16369_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE
    (Last et al. 2017))
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps in the k-means SMOTE method for data generation, which you
    can use via the `kmeans-smote` Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the decision boundary based on the original data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster data points into k clusters using k-means clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oversample using SMOTE for clusters with an **Imbalance Ratio** (**IR**) greater
    than the **Imbalance Ratio** **Threshold** (**IRT**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the decision boundary identification process. (Note: IRT can be chosen
    by the user or optimized like a hyperparameter.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can practice with different variations of SMOTE and find out which one works
    best for your datasets, but Borderline-SMOTE and K-means SMOTE could be good starting
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn about techniques that help you in improving the quality
    of your data before getting into model training.
  prefs: []
  type: TYPE_NORMAL
- en: Improving pre-training data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data processing in the early stages of a machine learning life cycle, before
    model training and evaluation, determines the quality of the data we feed into
    the training, validation, and testing process, and consequently our success in
    achieving a high-performance and reliable model.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection and outlier removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anomalies and outliers in your data could decrease the performance and reliability
    of your models in production. The existence of outliers in training data, the
    data you use for model evaluation, and unseen data in production could have different
    impacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers in model training**: The existence of outliers in the training data
    for supervised learning models could result in lower model generalizability. It
    could cause unnecessarily complex decision boundaries in classification or unnecessary
    nonlinearity in regression models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers in model evaluation**: Outliers in validation and test data could
    lower the model performance. As the models are not necessarily designed for outlying
    data points, they cause the model performance assessment to be unreliable by impacting
    the performance of the models, which cannot predict their labels or continuous
    values properly. This issue could make the process of model selection unreliable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers in production**: Unseen data points in production could be far from
    the distribution of training or even test data. Our model may have been designed
    to identify those anomalies, as in the case of fraud detection, but if that is
    not the objective, then we should tag those data points as samples, which our
    model is not confident doing or designed for. For example, if we designed a model
    to suggest drugs to cancer patients based on the genetic information of their
    tumors, our model should report low confidence for patients that need to be considered
    as outlier samples, as wrong medication could have life-threatening consequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 5.4* provides a summary of some of the anomaly detection methods you
    can use to identify anomalies in your data and remove outliers if necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Article** **and URL** |'
  prefs: []
  type: TYPE_TB
- en: '| **Isolation** **Forest** (**iForest**) | Liu et al. 2008[https://ieeexplore.ieee.org/abstract/document/4781136](https://ieeexplore.ieee.org/abstract/document/4781136)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Lightweight on-line detector of** **anomalies** (**Loda**) | Penvy, 2016[https://link.springer.com/article/10.1007/s10994-015-5521-0](https://link.springer.com/article/10.1007/s10994-015-5521-0)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Local outlier** **factor** (**LOF**) | Breunig et al., 2000[https://dl.acm.org/doi/abs/10.1145/342009.335388](https://dl.acm.org/doi/abs/10.1145/342009.335388)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Angle-Based Outlier** **Detection** (**ABOD**) | Kriegel et al., 2008[https://dl.acm.org/doi/abs/10.1145/1401890.1401946](https://dl.acm.org/doi/abs/10.1145/1401890.1401946)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Robust kernel density** **estimation** (**RKDE**) | Kim and Scott, 2008[https://ieeexplore.ieee.org/document/4518376](https://ieeexplore.ieee.org/document/4518376)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Support Vector Method for Novelty Detection | Schölkopf et al., 1999[https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html](https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 5.4 – Widely used anomaly detection techniques (Emmott et al., 2013 and
    2015)
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the effective methods for anomaly detection is `scikit-learn`. To try
    it out, we first generate a synthetic training dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use `IsolationForest()` from `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We used 10 decision trees in the previous code using `n_estimator = 10` when
    initializing `IsolationForest()`. This is one of the hyperparameters of iForest
    and we can play with it to get better results. You can see the resulting boundaries
    for `n_estimator = 10` and `n_estimator =` `100` next.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Decision boundaries of iForest using different numbers of estimators](img/B16369_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Decision boundaries of iForest using different numbers of estimators
  prefs: []
  type: TYPE_NORMAL
- en: If you accept the result of an anomaly detection method such as iForest without
    further investigation, you might decide to use only the data within the shown
    boundaries. However, there could be issues with these techniques, as with any
    other machine method. Although iForest is not a supervised learning method, the
    boundaries for identifying anomalies could be prone to overfitting and not generalizable
    for further evaluation or use on unseen data in production. Also, the choice of
    hyperparameters could result in considering a large fraction of the data points
    as outliers mistakenly.
  prefs: []
  type: TYPE_NORMAL
- en: Benefitting from data of lower quality or relevance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When doing supervised machine learning, we would like to ideally have access
    to a large quantity of high-quality data. However, features or output values do
    not have the same level of certainty across the data points we have access to.
    For example, in the case of classification, labels might not all have the same
    level of validity. In other words, our confidence in the labels of different data
    points could be different. Some of the commonly used labeling processes for data
    points are conducted by averaging experimental measurements (for example, as in
    biological or chemical contexts), or by using the annotations of multiple experts
    (or non-experts).
  prefs: []
  type: TYPE_NORMAL
- en: You could also have a problem such as predicting the response of breast cancer
    patients to specific drugs where you have access to data on patients’ response
    to the same or similar drugs in other cancer types. Then, part of your data has
    a lower level of relevance to the objective of breast cancer patients’ responses
    to the drug.
  prefs: []
  type: TYPE_NORMAL
- en: 'We preferably want to rely on high-quality data, or highly confident annotations
    and labels in these cases. However, we might have access to large quantities of
    data points that are either of lower quality, or lower relevance to the objective
    we have in mind. There are a few methods we could use to benefit from these data
    points of lower quality or relevance, although they are not successful all the
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`, after initializing a random forest model such as `rf_model
    = RandomForestClassifier(random_state = 42)`, you can specify the weight of each
    datapoint in the fitting step as `rf_model.fit(X_train,y_train, sample_weight
    = weights_array)`, where `weights_array` is an array of weights for each data
    point in the training set. These weights could be the confidence scores you have
    for each data point according to their relevance to the objective in mind or their
    quality. For example, if you use 10 different expert annotators for assigning
    labels to a series of data points, you can use a fraction of them to agree on
    a class label as the weight of each data point. If there is a data point with
    a class of 1 but only 7 out of 10 annotators agreed on this class, it will receive
    a lower weight compared to another class-1 data point for which all 10 annotators
    agreed on its label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: If you consider a distribution of the quality of or
    confidence score for each data point, then you can build different models using
    data points of each part of this distribution and then combine the predictions
    of the models, for example, using their weighted average (see *Figure 5**.8*).
    The weights assigned to each model could be a number, representative of the quality
    of the data points used for its training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning**: In transfer learning, we can train a model on a reference
    task, typically with many more data points, and then fine-tune it on a smaller
    task to come up with the task-specific predictions (Weiss et al., 2016, Madani
    Tonekaboni et al., 2020). This method can be used on data with different levels
    of confidence (Madani Tonekaboni et al., 2020). You can train a model on a large
    dataset with different levels of label confidence (see *Figure 5**.8*), excluding
    very low-confidence data and then fine-tune it on the very high-confidence part
    of your dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Techniques for using data points of different quality or relevance
    to the target problem in training machine learning models](img/B16369_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Techniques for using data points of different quality or relevance
    to the target problem in training machine learning models
  prefs: []
  type: TYPE_NORMAL
- en: These methods could help you reduce the need to generate more high-quality data.
    However, having more high-quality and highly relevant data is preferred, if possible.
  prefs: []
  type: TYPE_NORMAL
- en: As the final approach we want to go through in this chapter, we will talk about
    regularization as a technique to control overfitting and help in generating models
    with a higher chance of generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization to improve model generalizability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned in the previous chapter that high model complexity could cause overfitting.
    One of the approaches to controlling the model complexity and reducing the effect
    of features that affect model generalizability is **regularization**. In the regularization
    process, we consider a regularization or penalty term in the loss function to
    be optimized during the training process. Regularization, in the simple case of
    linear modeling, can be added as follows to the loss function during the optimization
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the first term is the loss and Ω(W) is the regularization term as a function
    of model weights, or parameters, W. However, regularization could be used with
    different machine learning methods such as SVMs or **LightGBM** (refer to *Table
    5.2*). Three of the common regularization terms are shown in the following table
    including **L1 regularization**, **L2 regularization**, and their combination.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Regularization term** | **Parameters** |'
  prefs: []
  type: TYPE_TB
- en: '| L2 regularization | Ω(W) = λ∑ j=1 p w j 2 | λ: The regularization parameter
    that determines the strength of regularization |'
  prefs: []
  type: TYPE_TB
- en: '| L1 regularization | Ω(W) = λ∑ j=1 p &#124;w p&#124; | λ: As in L2 regularization
    |'
  prefs: []
  type: TYPE_TB
- en: '| L2 and L1 | Ω(W) = λ( 1 − α _ 2  ∑ j=1 p w j 2 + α∑ j=1 p &#124;w j&#124;)
    | λ: As in L1 and L2 regularizationα: A missing parameter to determine the effect
    of L1 versus L2 in the regularization process |'
  prefs: []
  type: TYPE_TB
- en: Table 5.5 – Commonly used regularization methods for machine learning modeling
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider the process of optimization with regularization as the process
    of getting as close as possible to the optimal parameter set  ˆ β  while keeping
    the parameters bound to a constrained region, as shown in *Figure 5**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for
    controlling overfitting in a two-dimensional feature space](img/B16369_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for
    controlling overfitting in a two-dimensional feature space
  prefs: []
  type: TYPE_NORMAL
- en: Corners in parameter-constrained regions of L1 regularization result in the
    elimination of some of the parameters, or making their associated weights zero.
    However, the convexity of the constrained parameter region for L2 regularization
    only results in lowering the effect of parameters by decreasing their weights.
    This difference usually results in a higher robustness of L1 regularization to
    outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification models with L1 regularization and L2 regularization are
    called **Lasso** and **Ridge** regression, respectively (Tibshirani, 1996). Elastic-Net
    was proposed later using a combination of both L1 regularization and L2 regularization
    terms (Zou et al., 2005). Here, we want to practice using these three methods,
    but you can use regularization hyperparameters with other methods, such as an
    SVM or XGBoost classifier (see *Table 5.2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the necessary libraries and design a plotting function to visually
    show the effect of the regularization parameter values. We also load the digit
    dataset from `scikit-learn` to use for model training and evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `GridSearchCV()` function to assess the effect of different
    regularization parameter values in the following models. In `scikit-learn`, the
    regularization parameter is usually named `alpha` instead of λ, and the mixing
    parameter is called `l1_ratio` instead of α. Here, we first assess the effect
    of different `alpha` values on Lasso models, with L1 regularization, trained and
    evaluated using a digit dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The optimum `alpha` is identified to be 0.1, as shown in the following plot,
    which results in the highest accuracy across the considered values. This means
    that increasing the effect of regularization after an `alpha` value of `0.1` increases
    the model bias, resulting in a model with low performance in training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso
    model](img/B16369_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso
    model
  prefs: []
  type: TYPE_NORMAL
- en: If we assess the effect of different `alpha` values in a ridge model, with L2
    regularization, we can see that the performance increases as we increase the strength
    of regularization (see *Figure 5**.11*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge
    model](img/B16369_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge
    model
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to these two methods is Elastic-Net, which combines the effect
    of L1 and L2 regularizations. In this case, the trend of the effect of `alpha`
    on the model performance is more similar to Lasso; however, the range of accuracy
    values is narrower in comparison with Lasso, which only relies on L1 regularization
    (see *Figure 5**.12*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net
    model](img/B16369_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net
    model
  prefs: []
  type: TYPE_NORMAL
- en: 'If your dataset is not very small, more complex models help you to achieve
    higher performance. It would be only in rare cases that you would consider a linear
    model your ultimate model. To assess the effect of regularization on more complex
    models, we chose the SVM classifier and examined the effect of different values
    of `C` as the regularization parameter in `sklearn.svm.SVC()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As shown next, the range of accuracy for the models is higher, between 0.92
    and 0.99, compared to linear models with an accuracy of lower than 0.6, but higher
    regularization controls overfitting and achieves better performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification
    model](img/B16369_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification
    model
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 12*](B16369_12.xhtml#_idTextAnchor320), *Going Beyond ML Debugging
    with Deep Learning*, you will also learn about regularization techniques in deep
    neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about techniques to improve the performance of
    your models and reduce their bias and variance. You learned about the different
    hyperparameters of widely used machine learning methods, other than deep learning,
    which will be covered later in the book, and Python libraries to help you in identifying
    the optimal hyperparameter sets. You learned about regularization as another technique
    to help you in training generalizable machine learning models. You also learned
    how to increase the quality of the data to be fed into the training process by
    methods such as synthetic data generation and outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about interpretability and explainability
    in machine learning modeling and how you can use the related techniques and Python
    tools to identify opportunities for improving your models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does adding more features and training data points reduce model variance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you provide examples of methods to use to combine data with different
    confidence in class labels?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could oversampling improve the generalizability of your supervised machine
    learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between DSMOTE and Borderline-SMOTE?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you need to check the effect of every single value of every hyperparameter
    of a model during hyperparameter optimization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could L1 regularization eliminate the contribution of some of the features to
    supervised model predictions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could Lasso and Ridge regression models result in the same performance on the
    same test data if trained using the same training data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bergstra, James, and Yoshua Bengio. “*Random search for hyper-parameter optimization*.”
    *Journal of machine learning research* 13.2 (2012).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra, James, et al. “*Algorithms for hyper-parameter optimization*.” *Advances
    in neural information processing systems* 24 (2011).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen, Vu. “*Bayesian optimization for accelerating hyper-parameter tuning*.”
    *2019 IEEE second international conference on artificial intelligence and knowledge
    engineering (AIKE)*. IEEE (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li, Lisha, et al. “*Hyperband: A novel bandit-based approach to hyperparameter
    optimization*.” *Journal of Machine Learning Research* 18.1 (2017): pp. 6765-6816.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falkner, Stefan, Aaron Klein, and Frank Hutter. “*BOHB: Robust and efficient
    hyperparameter optimization at scale*.” *International Conference on Machine Learning*.
    PMLR (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng, Andrew, Stanford CS229: Machine Learning Course, Autumn 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wong, Sebastien C., et al. “*Understanding data augmentation for classification:
    when to warp?*.” *2016 international conference on digital image computing: techniques
    and applications (DICTA)*. IEEE (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikołajczyk, Agnieszka, and Michał Grochowski. “*Data augmentation for improving
    deep learning in image classification problem*.” *2018 international interdisciplinary
    PhD workshop (IIPhDW)*. IEEE (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shorten, Connor, and Taghi M. Khoshgoftaar. “*A survey on image data augmentation
    for deep learning*.” *Journal of big data* 6.1 (2019): pp. 1-48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taylor, Luke, and Geoff Nitschke. “*Improving deep learning with generic data
    augmentation*.” *2018 IEEE Symposium Series on Computational Intelligence (SSCI)*.
    IEEE (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shorten, Connor, Taghi M. Khoshgoftaar, and Borko Furht. “*Text data augmentation
    for deep learning*.” *Journal of big Data* 8.1 (2021): pp. 1-34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perez, Luis, and Jason Wang. “*The effectiveness of data augmentation in image
    classification using deep learning*.” arXiv preprint arXiv:1712.04621 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashrapov, Insaf. “*Tabular GANs for uneven distribution*.” arXiv preprint arXiv:2010.00638
    (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu, Lei, et al. “*Modeling tabular data using conditional gan*.” *Advances in
    Neural Information Processing Systems* 32 (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chawla, Nitesh V., et al. “*SMOTE: synthetic minority over-sampling technique*.”
    *Journal of artificial intelligence research* 16 (2002): pp. 321-357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han, H., Wang, WY., Mao, BH. (2005). “*Borderline-SMOTE: A New Over-Sampling
    Method in Imbalanced Data Sets Learning*.” In: Huang, DS., Zhang, XP., Huang,
    GB. (eds) *Advances in Intelligent Computing*. ICIC 2005\. *Lecture Notes in Computer
    Science*, vol. 3644\. Springer, Berlin, Heidelberg.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He, Haibo, Yang Bai, E. A. Garcia, and Shutao Li, “*ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*.” *2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence)* (2008):
    pp. 1322-1328, doi: 10.1109/IJCNN.2008.4633969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'X. Xiaolong, C. Wen, and S. Yanfei, “*Over-sampling algorithm for imbalanced
    data classification*,” in *Journal of Systems Engineering and Electronics*, vol.
    30, no. 6, pp. 1182-1191, Dec. 2019, doi: 10.21629/JSEE.2019.06.12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last, Felix, Georgios Douzas, and Fernando Bacao. “*Oversampling for imbalanced
    learning based on k-means and smote*.” arXiv preprint arXiv:1711.00837 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emmott, Andrew F., et al. “*Systematic construction of anomaly detection benchmarks
    from real data*.” *Proceedings of the ACM SIGKDD workshop on outlier detection
    and* *description*. 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emmott, Andrew, et al. “*A meta-analysis of the anomaly detection problem*.”
    arXiv preprint arXiv:1503.01158 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “*Isolation forest*.” *2008
    eighth IEEE international conference on data mining*. IEEE (2008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pevný, Tomáš. “*Loda: Lightweight on-line detector of anomalies*.” *Machine
    Learning 102* (2016): pp. 275-304.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breunig, Markus M., et al. “*LOF: identifying density-based local outliers*.”
    *Proceedings of the 2000 ACM SIGMOD international conference on Management of*
    *data* (2000).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kriegel, Hans-Peter, Matthias Schubert, and Arthur Zimek. “*Angle-based outlier
    detection in high-dimensional data*.” *Proceedings of the 14th ACM SIGKDD international
    conference on Knowledge discovery and data* *mining* (2008).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joo Seuk Kim and C. Scott, “*Robust kernel density estimation*.” *2008 IEEE
    International Conference on Acoustics, Speech and Signal Processing*, Las Vegas,
    NV, USA (2008): pp. 3381-3384, doi: 10.1109/ICASSP.2008.4518376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schölkopf, Bernhard, et al. “*Support vector method for novelty detection*.”
    *Advances in neural information processing systems* 12 (1999).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weiss, Karl, Taghi M. Khoshgoftaar, and DingDing Wang. “*A survey of transfer
    learning*.” *Journal of Big data* 3.1 (2016): pp. 1-40.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonekaboni, Seyed Ali Madani, et al. “*Learning across label confidence distributions
    using Filtered Transfer Learning*.” *2020 19th IEEE International Conference on
    Machine Learning and Applications (ICMLA)*. IEEE (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tibshirani, Robert. “*Regression shrinkage and selection via the lasso*.” *Journal
    of the Royal Statistical Society: Series B (Methodological)* 58.1 (1996): pp.
    267-288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie, Trevor, et al. *The elements of statistical learning: data mining,
    inference, and prediction*. vol. 2\. New York: Springer, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou, Hui, and Trevor Hastie. “*Regularization and variable selection via the
    elastic net*.” *Journal of the Royal Statistical Society: Series B (Statistical
    Methodology)* 67.2 (2005): pp. 301-320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
