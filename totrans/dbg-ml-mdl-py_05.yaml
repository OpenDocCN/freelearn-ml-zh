- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Improving the Performance of Machine Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高机器学习模型的性能
- en: In the previous chapter, you learned about different techniques for properly
    validating and assessing the performance of your machine learning models. The
    next step is to extend your knowledge of those techniques for improving the performance
    of your models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了关于正确验证和评估你的机器学习模型性能的不同技术。下一步是扩展你对这些技术的了解，以改善你模型的性能。
- en: In this chapter, you will learn about techniques to improve the performance
    and generalizability of your models by working on the data or algorithm you select
    for machine learning modeling.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将通过处理你为机器学习建模选择的数据或算法来学习提高模型性能和泛化性的技术。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Options for improving model performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高模型性能的选项
- en: Synthetic data generation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成合成数据
- en: Improving pre-training data processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进预训练数据预处理
- en: Regularization to improve model generalizability
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过正则化提高模型泛化性
- en: By the end of this chapter, you will be familiar with different techniques to
    improve the performance and generalizability of your models and you will know
    how you can benefit from Python in implementing them for your projects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将熟悉不同的技术来提高你模型的性能和泛化能力，你将了解如何从Python中受益，将这些技术应用于你的项目。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements are needed for this chapter as they help you better
    understand the concepts and enable you to use them in your projects and practice
    with the provided code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要求对于本章是必需的，因为它们有助于你更好地理解概念，并使你能够在你的项目和实践中使用它们，以及使用提供的代码：
- en: 'Python library requirements:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`ray` >= 2.3.1'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ray` >= 2.3.1'
- en: '`tune_sklearn` >= 0.4.5'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tune_sklearn` >= 0.4.5'
- en: '`bayesian_optimization` >= 1.4.2'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bayesian_optimization` >= 1.4.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`imblearn`'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imblearn`'
- en: '`matplotlib` >= 3.7.1'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` >= 3.7.1'
- en: Knowledge of machine learning validation techniques such as *k*-fold cross-validation
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解机器学习验证技术，如*k*-折交叉验证
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，网址为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter05)。
- en: Options for improving model performance
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高模型性能的选项
- en: The changes we can make to improve the performance of our models could be related
    to the algorithms we use or the data we feed them to train our models (see *Table
    5.1*). Adding more data points could reduce the variance of the model, for example,
    by adding data close to the decision boundaries of classification models to increase
    confidence in the identified boundaries and reduce overfitting. Removing outliers
    could reduce both bias and variance by eliminating the effect of distant data
    points. Adding more features could help the model to become better at the training
    stage (that is, lower model bias), but it might result in higher variance. There
    could also be features that cause overfitting and their removal could help to
    increase model generalizability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做出的改变以提高我们模型的性能可能与我们所使用的算法或我们提供给它们训练模型的数据有关（见*表5.1*）。添加更多的数据点可以通过添加接近分类模型决策边界的数据来减少模型的方差，例如，增加对识别边界有信心并减少过拟合。移除异常值可以通过消除远离数据点的效果来减少偏差和方差。添加更多特征可以帮助模型在训练阶段表现得更好（即降低模型偏差），但它可能导致更高的方差。也可能存在导致过拟合的特征，它们的移除可以帮助提高模型的可泛化性。
- en: '| **Change** | **Potential effect** | **Description** |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **改变** | **潜在影响** | **描述** |'
- en: '| Adding more training data points | Reducing variance | We could add new data
    points randomly or try to add data points with specific feature values, output
    values, or labels. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 增加更多训练数据点 | 降低方差 | 我们可以随机添加新的数据点，或者尝试添加具有特定特征值、输出值或标签的数据点。 |'
- en: '| Removing outliers with lower stringency | Reducing bias and variance | Removing
    outliers could reduce errors in the training set but it could also help in training
    a more generalizable model (that is, a model with lower variance). |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 以较低的标准移除异常值 | 降低偏差和方差 | 移除异常值可以减少训练集中的错误，但它也有助于训练一个更具泛化性的模型（即具有更低方差的模型）。
    |'
- en: '| Adding more features | Reducing bias | We could add features that provide
    unknown information to the model. For example, adding the crime rate of a neighborhood
    for house price prediction could improve model performance if that info is not
    already captured by existing features. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 添加更多特征 | 降低偏差 | 我们可以添加提供未知信息的特征给模型。例如，为房价预测添加一个社区的犯罪率，如果该信息尚未被现有特征捕获，可能会提高模型性能。
    |'
- en: '| Removing features | Reducing variance | Each feature could have a positive
    effect on training performance but it might add information that is not generalizable
    to new data points and result in higher variance. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 移除特征 | 降低方差 | 每个特征都可能对训练性能有积极影响，但它可能添加了无法推广到新数据点的信息，从而导致更高的方差。 |'
- en: '| Running optimization process for more iterations | Reducing bias | Optimizing
    for more iteration reduces training error but might result in overfitting. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 运行更多迭代的优化过程 | 降低偏差 | 增加迭代次数以优化可以减少训练误差，但可能导致过拟合。 |'
- en: '| Using more complex models | Reducing bias | Increasing the depth of decision
    trees is an example of an increase in model complexity that could result in lower
    model bias but potentially a higher chance of overfitting. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 使用更复杂的模型 | 降低偏差 | 增加决策树的深度是增加模型复杂性的一个例子，这可能导致降低模型偏差，但可能增加过拟合的风险。 |'
- en: Table 5.1 – Some of the options to reduce the bias and/or variance of machine
    learning models
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 – 减少机器学习模型偏差和/或方差的一些选项
- en: Increasing model complexity could help to reduce bias, as we discussed in the
    previous chapter, but a model can have many hyperparameters that affect its complexity
    or result in improving or lowering model bias and generalizability. Some of the
    hyperparameters that you could start with in the optimization process for the
    widely used supervised and unsupervised learning methods are provided in *Table
    5.2*. These hyperparameters should help you to improve the performance of your
    models, but you don’t need to write new functions or classes for hyperparameter
    optimization.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 增加模型复杂度可以帮助降低偏差，正如我们在上一章所讨论的，但模型可以有多个影响其复杂度或导致提高或降低模型偏差和泛化能力的超参数。在优化过程中，对于广泛使用的监督学习和无监督学习方法，您可以从中开始的一些超参数在*表5.2*中提供。这些超参数可以帮助您提高模型性能，但您不需要编写新的函数或类来进行超参数优化。
- en: '| **Method** | **Hyperparameter** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **方法** | **超参数** |'
- en: '| Logistic regression`sklearn.linear_model.LogisticRegression()` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归`sklearn.linear_model.LogisticRegression()` |'
- en: '`penalty`: Choosing regularization between `l1`, `l2`, `elasticnet`, and `None`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`penalty`: 在`l1`、`l2`、`elasticnet`和`None`之间选择正则化'
- en: '`class_weight`: Associating weights to classes'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight`: 将权重与类别关联'
- en: '`l1_ratio`: The Elastic-Net mixing parameter'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_ratio`: 弹性网络混合参数'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| K-Nearest Neighbors`sklearn.neighbors.KNeighborsClassifier()` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| K-最近邻`sklearn.neighbors.KNeighborsClassifier()` |'
- en: '`n_neighbors`: Number of neighbors'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_neighbors`: 邻居的数量'
- en: '`weights`: Choosing between `uniform` or `distance` to use neighbors equally
    or assign weights to them based on their distance'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights`: 在`uniform`或`distance`之间选择，以平等使用邻居或根据它们的距离分配权重'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Support Vector Machine** (**SVM**) classifier or regressor`sklearn.svm.SVC()``sklearn.svm.SVR()`
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **支持向量机**（**SVM**）分类器或回归器`sklearn.svm.SVC()``sklearn.svm.SVR()` |'
- en: '`C`: Inverse strength of regularization with the `l2` penalty'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`: 使用`l2`惩罚的正则化强度的倒数'
- en: '`kernel`: An SVM kernel with prebuilt kernels including `linear`, `poly`, `rbf`,
    `sigmoid`, and `precomputed`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel`: 包含预构建核的SVM核，包括`linear`、`poly`、`rbf`、`sigmoid`和`precomputed`'
- en: '`degree` (degree of polynomial): Degree of the polynomial kernel function (`poly`)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`degree`（多项式度）：多项式核函数（`poly`）的度'
- en: '`class_weight` (only for classification): Associating weights with classes'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight`（仅用于分类）：将权重与类别关联'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Random forest classifier or regressor`sklearn.ensemble.RandomForestClassifier()``sklearn.ensemble.RandomForestRegressor()`
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林分类器或回归器`sklearn.ensemble.RandomForestClassifier()``sklearn.ensemble.RandomForestRegressor()`
    |'
- en: '`n_estimators`: Number of trees in the forest'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`: 森林中的树的数量'
- en: '`max_depth`: Maximum depth of the trees'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 树的最大深度'
- en: '`class_weight`: Associating weights with classes'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_weight`: 将权重与类别关联'
- en: '`min_samples_split`: The minimum number of samples required to be at a leaf
    node'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`: 成为叶节点所需的最小样本数'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| XGBoost classifier or regressor`xgboost.XGBClassifier()``xgboost.XGBRegressor()`
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost分类器或回归器`xgboost.XGBClassifier()``xgboost.XGBRegressor()` |'
- en: '`booster` (`gbtree`, `gblinear`, or `dart`)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`booster`（`gbtree`、`gblinear`或`dart`）'
- en: 'For a tree booster:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于树增强器：
- en: '`eta`: Step size shrinkage to prevent overfitting.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`: 步长缩减以防止过拟合。'
- en: '`max_depth`: Maximum depth of the trees'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 树的最大深度'
- en: '`min_child_weight`: The minimum sum of data point weights needed to continue
    partitioning'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_child_weight`: 继续分区所需的最小数据点权重总和'
- en: '`lambda`: L2 regularization factor'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda`: L2正则化因子'
- en: '`alpha`: L1 regularization factor'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`: L1正则化因子'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LightGBM classifier or regressor`Lightgbm.LGBMClassifier()``Lightgbm.LGBMRegressor()`
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| LightGBM分类器或回归器`Lightgbm.LGBMClassifier()``Lightgbm.LGBMRegressor()` |'
- en: '`boosting_type` (`gbdt`, `dart`, or `rf`)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boosting_type` (`gbdt`、`dart`或`rf`)'
- en: '`num_leaves`: Maximum tree leaves'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_leaves`: 树的最大叶子数'
- en: '`max_depth`: Maximum tree depth'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 树的最大深度'
- en: '`n_estimators`: Number of boosted trees'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`: 增强树的数目'
- en: '`reg_alpha`: L1 regularization term on weights'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_alpha`: 权重的L1正则化项'
- en: '`reg_lambda`: L2 regularization term on weights'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_lambda`: 权重的L2正则化项'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| K-Means clustering`sklearn.cluster.KMeans()` |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| K-Means聚类`sklearn.cluster.KMeans()` |'
- en: '`n_clusters`: Number of clusters'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_clusters`: 聚类数'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Agglomerative clustering`sklearn.cluster.AgglomerativeClustering()` |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 聚类层次聚类`sklearn.cluster.AgglomerativeClustering()` |'
- en: '`n_clusters`: Number of clusters'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_clusters`: 聚类数'
- en: '`metric`: Distance measures with prebuilt measures including `euclidean`, `l1`,
    `l2`, `manhattan`, `cosine`, or `precomputed`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric`: 包含`euclidean`、`l1`、`l2`、`manhattan`、`cosine`或`precomputed`等预建度量的距离度量'
- en: '`linkage`: Linkage criterion with prebuilt methods including `ward`, `complete`,
    `average`, and `single`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linkage`: 包含`ward`、`complete`、`average`和`single`等预建方法的链接标准'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DBSCAN clustering`sklearn.cluster.DBSCAN()` |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| DBSCAN聚类`sklearn.cluster.DBSCAN()` |'
- en: '`eps`: Maximum allowed distance between data points for them to be considered
    neighbors'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps`: 数据点之间允许的最大距离，以被视为邻居'
- en: '`min_samples`: The minimum number of neighbors a data point needs to be considered
    a core point'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples`: 数据点需要作为核心点考虑的最小邻居数'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| UMAP`umap.UMAP()` |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| UMAP`umap.UMAP()` |'
- en: '`n_neighbors`: Constraining the size of the local neighborhood for the learning
    data structure'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_neighbors`: 限制学习数据结构的局部邻域大小'
- en: '`min_dist`: Controlling the compactness of groups in low-dimensional space'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_dist`: 控制低维空间中组的紧凑性'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 5.2 – Some of the most important hyperparameters of widely used supervised
    and unsupervised machine learning methods to start hyperparameter optimization
    with
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 – 一些广泛使用的监督和未监督机器学习方法的最重要的超参数，以开始超参数优化
- en: The Python libraries listed in *Table 5.3* have modules dedicated to different
    hyperparameter optimization techniques such as *grid search*, *random search*,
    *Bayesian search*, and *successive halving*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 列在*表5.3*中的Python库具有针对不同超参数优化技术的模块，例如*网格搜索*、*随机搜索*、*贝叶斯搜索*和*逐次减半*。
- en: '| **Library** | **URL** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **库** | **URL** |'
- en: '| `scikit-optimize` | [https://pypi.org/project/scikit-optimize/](https://pypi.org/project/scikit-optimize/)
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `scikit-optimize` | [https://pypi.org/project/scikit-optimize/](https://pypi.org/project/scikit-optimize/)
    |'
- en: '| `Optuna` | [https://pypi.org/project/optuna/](https://pypi.org/project/optuna/)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `Optuna` | [https://pypi.org/project/optuna/](https://pypi.org/project/optuna/)
    |'
- en: '| `GpyOpt` | [https://pypi.org/project/GPyOpt/](https://pypi.org/project/GPyOpt/)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `GpyOpt` | [https://pypi.org/project/GPyOpt/](https://pypi.org/project/GPyOpt/)
    |'
- en: '| `Hyperopt` | [https://hyperopt.github.io/hyperopt/](https://hyperopt.github.io/hyperopt/)
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `Hyperopt` | [https://hyperopt.github.io/hyperopt/](https://hyperopt.github.io/hyperopt/)
    |'
- en: '| `ray.tune` | [https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `ray.tune` | [https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)
    |'
- en: Table 5.3 – Commonly used Python libraries for hyperparameter optimization
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3 – 常用的Python库用于超参数优化
- en: Let’s talk about each method in detail.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论每种方法。
- en: Grid search
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格搜索
- en: This method is about determining a series of hyperparameter nomination sets
    to be tested one by one to find the optimum combination. The cost of grid-searching
    to find an optimal combination is high. Also, considering there would be a specific
    set of hyperparameters that mattered for each problem, grid search with a predetermined
    set of hyperparameter combinations for all problems is not an effective approach.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法涉及确定一系列要逐一测试的超参数提名集，以找到最佳组合。网格搜索以找到最佳组合的成本很高。此外，考虑到对于每个问题都存在一组特定的超参数，为所有问题预定义一组超参数组合的网格搜索并不是一个有效的方法。
- en: 'Here is an example of grid search hyperparameter optimization using `sklearn.model_selection.GridSearchCV()`
    for a random forest classifier model. 80% of the data is used for hyperparameter
    optimization and the performance of the model is assessed using stratified 5-fold
    CV:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用`sklearn.model_selection.GridSearchCV()`对随机森林分类器模型进行网格搜索超参数优化的示例。80%的数据用于超参数优化，模型的性能使用分层5折交叉验证进行评估：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, 10 estimators are used and different `min_samples_split` and `max_depth`
    values are considered for the hyperparameter optimization process. You can specify
    different performance metrics according to what you learned in the previous chapter
    using the scoring parameter, as one of the parameters of `sklearn.model_selection.GridSearchCV()`.
    A combination of `max_depth` of `10` and `min_samples_split` of `7` was identified
    as the best hyperparameter set in this case, which resulted in 0.948 accuracy
    using the stratified 5-fold CV. We can extract the best hyperparameter and corresponding
    score using `sklearn_gridsearch.best_params_` and `sklearn_gridsearch.best_score_`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，使用了10个估计器，并在超参数优化过程中考虑了不同的`min_samples_split`和`max_depth`值。您可以使用评分参数指定不同的性能指标，评分参数是`sklearn.model_selection.GridSearchCV()`的一个参数，根据您在上一章中学到的知识。在这种情况下，`max_depth`为`10`和`min_samples_split`为`7`的组合被确定为最佳超参数集，使用分层5折交叉验证得到了0.948的准确率。我们可以使用`sklearn_gridsearch.best_params_`和`sklearn_gridsearch.best_score_`提取最佳超参数和相应的分数。
- en: Random search
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机搜索
- en: This method is an alternative to grid search. It randomly tries different combinations
    of hyperparameter values. For the same high enough computational budget, it is
    shown that random search can achieve a higher performance model compared to grid
    search, as it can search a larger space (Bergstra and Bengio, 2012).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法是对网格搜索的一种替代。它随机尝试不同的超参数值组合。对于相同的足够高的计算预算，研究表明，与网格搜索相比，随机搜索可以实现性能更高的模型，因为它可以搜索更大的空间（Bergstra和
    Bengio，2012）。
- en: 'Here is an example of random search hyperparameter optimization using `sklearn.model_selection.RandomizedSearchCV()`
    for the same model and data used in the previous code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用`sklearn.model_selection.RandomizedSearchCV()`对相同模型和数据进行的随机搜索超参数优化的示例：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With only five iterations, this random search resulted in 0.942 CV accuracy
    with less than one-third of running time, which could depend on your local or
    cloud system configuration. In this case, a combination of `max_depth` of `15`
    and `min_samples_split` of `7` was identified as the best hyperparameter set.
    Comparing the results of grid search and random search, we can conclude that models
    with different `max_depth` values could result in similar CV accuracies for this
    specific case of random forest modeling with 10 estimators using the digit dataset
    from `scikit-learn`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 只需五次迭代，这个随机搜索就得到了0.942的CV准确率，运行时间不到三分之一，这可能会取决于您的本地或云系统配置。在这种情况下，`max_depth`为`15`和`min_samples_split`为`7`的组合被确定为最佳超参数集。比较网格搜索和随机搜索的结果，我们可以得出结论，具有不同`max_depth`值的模型可能在这个特定情况下对使用`scikit-learn`的数字数据集进行随机森林建模的CV准确率产生相似的结果。
- en: Bayesian search
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贝叶斯搜索
- en: 'In Bayesian optimization, instead of randomly selecting hyperparameter combinations
    without checking the value of previous combination sets, each combination of hyperparameter
    sets gets selected in an iteration based on the history of previously tested hyperparameter
    sets. This process helps to reduce the computational cost compared to grid search
    but it doesn’t always beat random search. We want to use Ray Tune (`ray.tune`)
    here for this approach. You can read more about different functionalities available
    in Ray Tune such as *logging tune runs*, *how to stop and resume*, *analyzing
    tune experiment results*, and *deploying tune in the cloud* on the tutorial page:
    [https://docs.ray.io/en/latest/tune/tutorials/overview.html](https://docs.ray.io/en/latest/tune/tutorials/overview.html).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯优化中，与随机选择超参数组合而不检查先前组合集的值不同，每个超参数组合集的每个组合都是基于先前测试的超参数集的历史记录在迭代中选择的。这个过程有助于降低与网格搜索相比的计算成本，但它并不总是优于随机搜索。我们在这里想使用Ray
    Tune (`ray.tune`)来实现这种方法。您可以在教程页面上了解更多关于Ray Tune中可用的不同功能，例如*记录tune运行*、*如何停止和恢复*、*分析tune实验结果*和*在云中部署tune*：[https://docs.ray.io/en/latest/tune/tutorials/overview.html](https://docs.ray.io/en/latest/tune/tutorials/overview.html)。
- en: 'The following implementation of Bayesian hyperparameter optimization using
    `ray.tune.sklearn.TuneSearchCV()` for the same random forest model, as explained
    previously, achieves 0.942 CV accuracy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用`ray.tune.sklearn.TuneSearchCV()`对相同的随机森林模型进行贝叶斯超参数优化，实现了0.942的CV准确率：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Successive halving
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连续减半法
- en: The idea behind successive having is to not invest in all hyperparameters equally.
    Candidate hyperparameter sets get evaluated using limited resources, for example,
    using only a fraction of training data or a limited number of trees in a random
    forest model in an iteration, and some of them pass to the next iteration. In
    later iterations, more resources get used until the last iteration in which all
    resources, for example, all training data, get used to evaluate the remaining
    hyperparameter sets. You can use `HalvingGridSearchCV()`and `HalvingRandomSearchCV()`as
    part of `sklearn.model_selection` to try out successive halving. You can read
    more about these two Python modules at [https://scikit-learn.org/stable/modules/grid_search.html#id3](https://scikit-learn.org/stable/modules/grid_search.html#id3).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 连续减半法的理念在于不是对所有的超参数进行同等投资。候选超参数集使用有限的资源进行评估，例如，使用训练数据的一部分或在随机森林模型的一个迭代中使用有限数量的树，其中一些会进入下一个迭代。在后续的迭代中，使用更多的资源，直到最后一个迭代，在最后一个迭代中，使用所有资源，例如，所有训练数据，来评估剩余的超参数集。您可以使用`HalvingGridSearchCV()`和`HalvingRandomSearchCV()`作为`sklearn.model_selection`的一部分来尝试连续减半。您可以在[https://scikit-learn.org/stable/modules/grid_search.html#id3](https://scikit-learn.org/stable/modules/grid_search.html#id3)了解更多关于这两个Python模块的信息。
- en: There are other hyperparameter optimization techniques, such as **Hyperband**
    (Li et al., 2017) and **BOHB** (Falkner et al., 2018) that you can try out, but
    the general idea behind most advancements in hyperparameter optimization is to
    minimize the computational resources necessary to achieve an optimum hyperparameter
    set. There are also techniques and libraries for hyperparameter optimization in
    deep learning, which we will cover in [*Chapter 12*](B16369_12.xhtml#_idTextAnchor320),
    *Going Beyond ML Debugging with Deep Learning*, and [*Chapter 13*](B16369_13.xhtml#_idTextAnchor342),
    *Advanced Deep Learning Techniques*. Although hyperparameter optimization helps
    us to get better models, using the provided data for model training and the selected
    machine learning method, we can improve model performance with other approaches,
    such as generating synthetic data for model training, which is our next topic.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他超参数优化技术，例如**Hyperband**（Li等人，2017）和**BOHB**（Falkner等人，2018），您可以尝试这些技术，但大多数超参数优化进步背后的通用理念是尽量减少达到最佳超参数集所需的计算资源。还有用于深度学习超参数优化的技术和库，我们将在[*第12章*](B16369_12.xhtml#_idTextAnchor320)，*使用深度学习超越ML调试*和[*第13章*](B16369_13.xhtml#_idTextAnchor342)，*高级深度学习技术*中介绍。尽管超参数优化有助于我们获得更好的模型，但使用提供的数据进行模型训练和选定的机器学习方法，我们可以通过其他方法来提高模型性能，例如为模型训练生成合成数据，这是我们接下来要讨论的主题。
- en: Synthetic data generation
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成合成数据
- en: The data we have access to for training and evaluating our machine learning
    models may be limited. For example, in the case of classification models, we might
    have classes with a limited number of data points, resulting in lower performance
    of our models for unseen data points of the same classes. We will go through a
    few methods here to help you improve the performance of your models in these situations.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可用于训练和评估机器学习模型的数据可能有限。例如，在分类模型的情况下，我们可能有一些数据点数量有限的类别，导致我们的模型在相同类别的未见数据点上的性能降低。在这里，我们将介绍一些方法来帮助您在这些情况下提高模型的性能。
- en: Oversampling for imbalanced data
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对不平衡数据进行过采样
- en: Imbalanced data classification is challenging due to the dominating effect of
    majority classes during training as well as in model performance reporting. For
    model performance reporting, we discussed different performance metrics in the
    previous chapter and how you can select a reliable metric even in the case of
    imbalanced data classification. Here, we want to talk about the concept of oversampling
    to help you improve the performance of your models by synthetically improving
    your training data. The concept of oversampling is to increase the number of data
    points in your minority classes using the real data points you have in your dataset.
    The simplest way of thinking about it is to duplicate some of the data points
    in minority classes, which is not a good approach as they will not provide complementary
    information to real data in the training process. There are techniques designed
    for oversampling processes, such as the **Synthetic Minority Oversampling Technique**
    (**SMOTE**) and its variations for tabular data, which we will present here.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在训练过程中以及模型性能报告中的多数类的支配效应，不平衡数据分类具有挑战性。在模型性能报告方面，我们在上一章讨论了不同的性能指标以及如何在不平衡数据分类的情况下选择一个可靠的指标。在这里，我们想要讨论过采样的概念，以帮助您通过合成改善训练数据来提高您模型的性能。过采样的概念是使用您数据集中已有的真实数据点来增加少数类数据点的数量。最简单的方式来思考它就是复制少数类中的某些数据点，但这不是一个好的方法，因为它们在训练过程中不会提供与真实数据互补的信息。有一些技术是为过采样过程设计的，例如**合成少数类过采样技术**（**SMOTE**）及其用于表格数据的变体，我们将在下面介绍。
- en: Undersampling
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 降采样
- en: In classifying imbalanced data, an alternative to oversampling is to decrease
    the imbalance by sampling the majority class. This process reduces the ratio of
    the majority-class to minority-class data points. As not all the data points get
    included in one set of sampling, multiple models can be built by sampling different
    subsets of majority-class data points and the output of those models can be combined,
    for example, through majority voting between the models. This process is called
    **undersampling**. Oversampling usually results in higher performance improvement
    compared to undersampling.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在对不平衡数据进行分类时，除了过采样之外，还可以通过采样多数类来减少不平衡。这个过程会降低多数类与少数类数据点的比例。由于并非所有数据点都会包含在一个采样集中，因此可以通过采样多数类数据点的不同子集来构建多个模型，并将这些模型的输出结合起来，例如通过模型间的多数投票。这个过程被称为**降采样**。与降采样相比，过采样通常会导致更高的性能提升。
- en: SMOTE
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SMOTE
- en: SMOTE is an old yet widely used approach to oversampling the minority class,
    for continuous feature sets, using the distribution of neighboring data points
    (Chawla et al., 2022; see *Figure 5**.1*).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE 是一种古老但广泛使用的方法，用于通过使用邻近数据点的分布（Chawla 等人，2022；参见 *图 5**.1*）对连续特征集进行少数类过采样。
- en: '![Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE,
    Borderline-SMOTE, and ADASYN](img/B16369_05_01.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 使用 SMOTE、Borderline-SMOTE 和 ADASYN 生成合成数据的示意图](img/B16369_05_01.jpg)'
- en: Figure 5.1 – Schematic illustration of synthetic data generation using SMOTE,
    Borderline-SMOTE, and ADASYN
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 使用 SMOTE、Borderline-SMOTE 和 ADASYN 生成合成数据的示意图
- en: 'The steps in generating any synthetic data point using SMOTE can be summarized
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SMOTE 生成任何合成数据点的步骤可以总结如下：
- en: Choose a random data point from a minority class.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从少数类中随机选择一个数据点。
- en: Identify the K-Nearest Neighbors for that data point.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定该数据点的 K-最近邻。
- en: Choose one of the neighbors randomly.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个邻居。
- en: Generate a synthetic data point at a randomly selected point between the two
    data points in the feature space.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特征空间中两个数据点之间随机选择一个点生成合成数据点。
- en: SMOTE and two of its variations, **Borderline-SMOTE** and **Adaptive synthetic**
    (**ADASYN**), are shown in *Figure 5**.1*. *Steps 2* to *4* of SMOTE, Borderline-SMOTE,
    and ADASYN are similar. However, Borderline-SMOTE focuses on the real data points
    that divide the classes and ADASYN focuses on the data points of the minority
    class in regions of the feature space dominated by the majority classes. In this
    way, Borderline-SMOTE increases the confidence in decision boundary identification
    to avoid overfitting and ADASYN improves generalizability for minority-class prediction
    in the parts of the space dominated by majority classes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE及其两种变体**Borderline-SMOTE**和**自适应合成**（**ADASYN**）在*图5.1*中展示。SMOTE、Borderline-SMOTE和ADASYN的步骤2到步骤4是相似的。然而，Borderline-SMOTE专注于分割类别的真实数据点，而ADASYN专注于特征空间中由多数类主导区域的数据点。通过这种方式，Borderline-SMOTE增加了决策边界识别的置信度，以避免过拟合，而ADASYN提高了在多数类主导的空间部分对少数类预测的泛化能力。
- en: 'You can use the `imblearn` Python library for synthetic data generation using
    SMOTE, Borderline-SMOTE, and ADASYN. However, before getting into using these
    functionalities, we need to write a plotting function for later use to show the
    data before and after the oversampling process:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`imblearn` Python库通过SMOTE、Borderline-SMOTE和ADASYN生成合成数据。然而，在进入使用这些功能之前，我们需要编写一个绘图函数供以后使用，以显示过采样过程之前和之后的数据：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we generate a synthetic dataset with two classes and only two features
    (that is, two-dimensional data) and consider it our real dataset. We consider
    100 data points in one of the classes as the majority class, and 10 data points
    in another class as the minority class:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们生成一个包含两个类别和仅两个特征（即二维数据）的合成数据集，并将其视为我们的真实数据集。我们将一个类别的100个数据点视为多数类，另一个类别的10个数据点视为少数类：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resulting data points are shown in the following scatter plot with red and
    black data points representing the minority and majority classes, respectively.
    We are using this synthetic data instead of a real dataset to visually show you
    how different synthetic data generation methods work.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据点在以下散点图中显示，红色和黑色数据点分别代表少数类和多数类。我们使用这个合成数据而不是真实数据集，以直观地展示不同的合成数据生成方法是如何工作的。
- en: '![Figure 5.2 – Example dataset with two features (that is, dimensions), generated
    synthetically, to use for practicing with SMOTE and its alternatives](img/B16369_05_02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 用于练习SMOTE及其替代方法的示例数据集，包含两个特征（即维度），通过合成生成](img/B16369_05_02.jpg)'
- en: Figure 5.2 – Example dataset with two features (that is, dimensions), generated
    synthetically, to use for practicing with SMOTE and its alternatives
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 用于练习SMOTE及其替代方法的示例数据集，包含两个特征（即维度），通过合成生成
- en: 'We now use SMOTE via `imblearn.over_sampling.SMOTE()`, as shown in the following
    code snippet, to generate synthetic data points for the minority class only:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用SMOTE通过`imblearn.over_sampling.SMOTE()`，如以下代码片段所示，只为少数类生成合成数据点：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see in the following figure, the new oversampled data points will
    be within the gaps between the original data points of the minority class (that
    is, red data points). However, many of these new data points don’t help to identify
    a reliable decision boundary as they are grouped in the very top-right corner,
    far from the black data points and potential decision boundaries.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在以下图中可以看到，新的过采样数据点将位于少数类原始数据点（即红色数据点）之间的间隙中。然而，许多这些新的数据点并没有帮助识别一个可靠的决策边界，因为它们被分组在非常右上角，远离黑色数据点和潜在的决策边界。
- en: '![Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing
    SMOTE](img/B16369_05_03.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 实施SMOTE后图5.2所示数据集的可视化](img/B16369_05_03.jpg)'
- en: Figure 5.3 – Visualization of the dataset shown in Figure 5.2 after implementing
    SMOTE
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 实施SMOTE后图5.2所示数据集的可视化
- en: 'We use Borderline-SMOTE instead via `imblearn.over_sampling.BorderlineSMOTE()`
    as follows for synthetic data generation:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过`imblearn.over_sampling.BorderlineSMOTE()`使用Borderline-SMOTE来生成合成数据，如下所示：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can see that the new synthetically generated data points are closer to the
    black data points of the majority class, which helps with identifying a generalizable
    decision boundary:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，新的合成数据点更接近黑色的大多数类数据点，这有助于识别一个可泛化的决策边界：
- en: '![Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing
    Borderline-SMOTE](img/B16369_05_04.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 实施Borderline-SMOTE后图5.2所示数据集的可视化](img/B16369_05_04.jpg)'
- en: Figure 5.4 – Visualization of the dataset shown in Figure 5.2 after implementing
    Borderline-SMOTE
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 实施Borderline-SMOTE后，图5.2所示数据集的可视化
- en: 'We can also use ADASYN via `imblearn.over_sampling.ADASYN()`, which also generates
    more of the new synthetic data close to the black data points as it focuses on
    the regions with more majority-class samples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过`imblearn.over_sampling.ADASYN()`使用ADASYN，因为它专注于具有更多多数类样本的区域，因此生成更多接近黑色数据点的新合成数据：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The data including original and synthetically generated data points using ADASYN
    are shown in *Figure 5**.5*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 包括原始和合成数据点在内的数据在*图5.5*中显示。
- en: '![Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing
    ADASYN](img/B16369_05_05.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 实施ADASYN后，图5.2所示数据集的可视化](img/B16369_05_05.jpg)'
- en: Figure 5.5 – Visualization of the dataset shown in Figure 5.2 after implementing
    ADASYN
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 实施ADASYN后，图5.2所示数据集的可视化
- en: There have been more recent methods built upon SMOTE for synthetic data generation
    such as **density-based synthetic minority over-sampling technique** (**DSMOTE**)
    (Xiaolong et al., 2019) and **k-means SMOTE** (Felix et al., 2017). Both of these
    methods try to capture groupings of data points either within the target minority
    class or the whole dataset. In DSMOTE, **Density-based spatial clustering of applications
    with noise** (**DBSCAN**) is used to divide data points of the minority class
    into three groups of core samples, borderline samples, and noise (i.e., outlying)
    samples, and then the core and borderline samples only get used for oversampling.
    This approach is shown to work better than SMOTE and Borderline-SMOTE (Xiaolong
    et al., 2019). K-means SMOTE is another recent alternative to SMOTE (Last et al.,
    2017) that relies on clustering of the whole dataset using a k-means clustering
    algorithm before oversampling (see *Figure 5**.6*).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SMOTE的合成数据生成方法近年来有了更多的发展，例如**基于密度的合成少数类过采样技术**（**DSMOTE**）（Xiaolong等人，2019年）和**k-means
    SMOTE**（Felix等人，2017年）。这两种方法都试图捕捉数据点在目标少数类或整个数据集中的分组。在DSMOTE中，使用**基于密度的空间聚类应用噪声**（**DBSCAN**）将少数类的数据点划分为三个组：核心样本、边界样本和噪声（即异常）样本，然后仅使用核心和边界样本进行过采样。这种方法被证明比SMOTE和Borderline-SMOTE（Xiaolong等人，2019年）更有效。K-means
    SMOTE是SMOTE的另一种近期替代方案（Last等人，2017年），它依赖于在过采样之前使用k-means聚类算法对整个数据集进行聚类（见*图5.6*）。
- en: '![Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE
    (Last et al. 2017))](img/B16369_05_06.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – k-means SMOTE的四个主要步骤的示意图（Last等人，2017年）](img/B16369_05_06.jpg)'
- en: Figure 5.6 – Schematic illustration of the four main steps of k-means SMOTE
    (Last et al. 2017))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – k-means SMOTE的四个主要步骤的示意图（Last等人，2017年）
- en: 'Here are the steps in the k-means SMOTE method for data generation, which you
    can use via the `kmeans-smote` Python library:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据生成k-means SMOTE方法中的步骤，您可以通过`kmeans-smote` Python库使用它们：
- en: Identify the decision boundary based on the original data.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据原始数据确定决策边界。
- en: Cluster data points into k clusters using k-means clustering.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-means聚类将数据点聚类到k个簇中。
- en: Oversample using SMOTE for clusters with an **Imbalance Ratio** (**IR**) greater
    than the **Imbalance Ratio** **Threshold** (**IRT**).
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**不平衡率**（**IR**）大于**不平衡率阈值**（**IRT**）的簇使用SMOTE进行过采样。
- en: 'Repeat the decision boundary identification process. (Note: IRT can be chosen
    by the user or optimized like a hyperparameter.)'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复决策边界识别过程。（注意：IRT可以由用户选择或像超参数一样优化。）
- en: You can practice with different variations of SMOTE and find out which one works
    best for your datasets, but Borderline-SMOTE and K-means SMOTE could be good starting
    points.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用SMOTE的不同变体进行练习，找出最适合您数据集的版本，但Borderline-SMOTE和K-means SMOTE可以作为良好的起点。
- en: Next, you will learn about techniques that help you in improving the quality
    of your data before getting into model training.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习一些技术，这些技术有助于在模型训练之前提高您数据的质量。
- en: Improving pre-training data processing
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进预训练数据处理
- en: Data processing in the early stages of a machine learning life cycle, before
    model training and evaluation, determines the quality of the data we feed into
    the training, validation, and testing process, and consequently our success in
    achieving a high-performance and reliable model.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习生命周期的早期阶段，在模型训练和评估之前进行的数据处理决定了我们输入训练、验证和测试过程的数据质量，从而决定了我们实现高性能和可靠模型的成功。
- en: Anomaly detection and outlier removal
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常检测和异常值移除
- en: 'Anomalies and outliers in your data could decrease the performance and reliability
    of your models in production. The existence of outliers in training data, the
    data you use for model evaluation, and unseen data in production could have different
    impacts:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers in model training**: The existence of outliers in the training data
    for supervised learning models could result in lower model generalizability. It
    could cause unnecessarily complex decision boundaries in classification or unnecessary
    nonlinearity in regression models.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers in model evaluation**: Outliers in validation and test data could
    lower the model performance. As the models are not necessarily designed for outlying
    data points, they cause the model performance assessment to be unreliable by impacting
    the performance of the models, which cannot predict their labels or continuous
    values properly. This issue could make the process of model selection unreliable.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers in production**: Unseen data points in production could be far from
    the distribution of training or even test data. Our model may have been designed
    to identify those anomalies, as in the case of fraud detection, but if that is
    not the objective, then we should tag those data points as samples, which our
    model is not confident doing or designed for. For example, if we designed a model
    to suggest drugs to cancer patients based on the genetic information of their
    tumors, our model should report low confidence for patients that need to be considered
    as outlier samples, as wrong medication could have life-threatening consequences.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 5.4* provides a summary of some of the anomaly detection methods you
    can use to identify anomalies in your data and remove outliers if necessary:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Article** **and URL** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| **Isolation** **Forest** (**iForest**) | Liu et al. 2008[https://ieeexplore.ieee.org/abstract/document/4781136](https://ieeexplore.ieee.org/abstract/document/4781136)
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| **Lightweight on-line detector of** **anomalies** (**Loda**) | Penvy, 2016[https://link.springer.com/article/10.1007/s10994-015-5521-0](https://link.springer.com/article/10.1007/s10994-015-5521-0)
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| **Local outlier** **factor** (**LOF**) | Breunig et al., 2000[https://dl.acm.org/doi/abs/10.1145/342009.335388](https://dl.acm.org/doi/abs/10.1145/342009.335388)
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| **Angle-Based Outlier** **Detection** (**ABOD**) | Kriegel et al., 2008[https://dl.acm.org/doi/abs/10.1145/1401890.1401946](https://dl.acm.org/doi/abs/10.1145/1401890.1401946)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| **Robust kernel density** **estimation** (**RKDE**) | Kim and Scott, 2008[https://ieeexplore.ieee.org/document/4518376](https://ieeexplore.ieee.org/document/4518376)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| Support Vector Method for Novelty Detection | Schölkopf et al., 1999[https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html](https://proceedings.neurips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776-Abstract.html)
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: Table 5.4 – Widely used anomaly detection techniques (Emmott et al., 2013 and
    2015)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the effective methods for anomaly detection is `scikit-learn`. To try
    it out, we first generate a synthetic training dataset as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we use `IsolationForest()` from `scikit-learn`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We used 10 decision trees in the previous code using `n_estimator = 10` when
    initializing `IsolationForest()`. This is one of the hyperparameters of iForest
    and we can play with it to get better results. You can see the resulting boundaries
    for `n_estimator = 10` and `n_estimator =` `100` next.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Decision boundaries of iForest using different numbers of estimators](img/B16369_05_07.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Decision boundaries of iForest using different numbers of estimators
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: If you accept the result of an anomaly detection method such as iForest without
    further investigation, you might decide to use only the data within the shown
    boundaries. However, there could be issues with these techniques, as with any
    other machine method. Although iForest is not a supervised learning method, the
    boundaries for identifying anomalies could be prone to overfitting and not generalizable
    for further evaluation or use on unseen data in production. Also, the choice of
    hyperparameters could result in considering a large fraction of the data points
    as outliers mistakenly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Benefitting from data of lower quality or relevance
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When doing supervised machine learning, we would like to ideally have access
    to a large quantity of high-quality data. However, features or output values do
    not have the same level of certainty across the data points we have access to.
    For example, in the case of classification, labels might not all have the same
    level of validity. In other words, our confidence in the labels of different data
    points could be different. Some of the commonly used labeling processes for data
    points are conducted by averaging experimental measurements (for example, as in
    biological or chemical contexts), or by using the annotations of multiple experts
    (or non-experts).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: You could also have a problem such as predicting the response of breast cancer
    patients to specific drugs where you have access to data on patients’ response
    to the same or similar drugs in other cancer types. Then, part of your data has
    a lower level of relevance to the objective of breast cancer patients’ responses
    to the drug.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'We preferably want to rely on high-quality data, or highly confident annotations
    and labels in these cases. However, we might have access to large quantities of
    data points that are either of lower quality, or lower relevance to the objective
    we have in mind. There are a few methods we could use to benefit from these data
    points of lower quality or relevance, although they are not successful all the
    time:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '`scikit-learn`, after initializing a random forest model such as `rf_model
    = RandomForestClassifier(random_state = 42)`, you can specify the weight of each
    datapoint in the fitting step as `rf_model.fit(X_train,y_train, sample_weight
    = weights_array)`, where `weights_array` is an array of weights for each data
    point in the training set. These weights could be the confidence scores you have
    for each data point according to their relevance to the objective in mind or their
    quality. For example, if you use 10 different expert annotators for assigning
    labels to a series of data points, you can use a fraction of them to agree on
    a class label as the weight of each data point. If there is a data point with
    a class of 1 but only 7 out of 10 annotators agreed on this class, it will receive
    a lower weight compared to another class-1 data point for which all 10 annotators
    agreed on its label.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble learning**: If you consider a distribution of the quality of or
    confidence score for each data point, then you can build different models using
    data points of each part of this distribution and then combine the predictions
    of the models, for example, using their weighted average (see *Figure 5**.8*).
    The weights assigned to each model could be a number, representative of the quality
    of the data points used for its training.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transfer learning**: In transfer learning, we can train a model on a reference
    task, typically with many more data points, and then fine-tune it on a smaller
    task to come up with the task-specific predictions (Weiss et al., 2016, Madani
    Tonekaboni et al., 2020). This method can be used on data with different levels
    of confidence (Madani Tonekaboni et al., 2020). You can train a model on a large
    dataset with different levels of label confidence (see *Figure 5**.8*), excluding
    very low-confidence data and then fine-tune it on the very high-confidence part
    of your dataset.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Techniques for using data points of different quality or relevance
    to the target problem in training machine learning models](img/B16369_05_08.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Techniques for using data points of different quality or relevance
    to the target problem in training machine learning models
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: These methods could help you reduce the need to generate more high-quality data.
    However, having more high-quality and highly relevant data is preferred, if possible.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: As the final approach we want to go through in this chapter, we will talk about
    regularization as a technique to control overfitting and help in generating models
    with a higher chance of generalizability.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Regularization to improve model generalizability
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned in the previous chapter that high model complexity could cause overfitting.
    One of the approaches to controlling the model complexity and reducing the effect
    of features that affect model generalizability is **regularization**. In the regularization
    process, we consider a regularization or penalty term in the loss function to
    be optimized during the training process. Regularization, in the simple case of
    linear modeling, can be added as follows to the loss function during the optimization
    process:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: where the first term is the loss and Ω(W) is the regularization term as a function
    of model weights, or parameters, W. However, regularization could be used with
    different machine learning methods such as SVMs or **LightGBM** (refer to *Table
    5.2*). Three of the common regularization terms are shown in the following table
    including **L1 regularization**, **L2 regularization**, and their combination.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '| **Method** | **Regularization term** | **Parameters** |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| L2 regularization | Ω(W) = λ∑ j=1 p w j 2 | λ: The regularization parameter
    that determines the strength of regularization |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| L1 regularization | Ω(W) = λ∑ j=1 p &#124;w p&#124; | λ: As in L2 regularization
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| L2 and L1 | Ω(W) = λ( 1 − α _ 2  ∑ j=1 p w j 2 + α∑ j=1 p &#124;w j&#124;)
    | λ: As in L1 and L2 regularizationα: A missing parameter to determine the effect
    of L1 versus L2 in the regularization process |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: Table 5.5 – Commonly used regularization methods for machine learning modeling
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'We can consider the process of optimization with regularization as the process
    of getting as close as possible to the optimal parameter set  ˆ β  while keeping
    the parameters bound to a constrained region, as shown in *Figure 5**.9*:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for
    controlling overfitting in a two-dimensional feature space](img/B16369_05_09.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Schematic representation of L1 and L2 norm regularizations for
    controlling overfitting in a two-dimensional feature space
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Corners in parameter-constrained regions of L1 regularization result in the
    elimination of some of the parameters, or making their associated weights zero.
    However, the convexity of the constrained parameter region for L2 regularization
    only results in lowering the effect of parameters by decreasing their weights.
    This difference usually results in a higher robustness of L1 regularization to
    outliers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Linear classification models with L1 regularization and L2 regularization are
    called **Lasso** and **Ridge** regression, respectively (Tibshirani, 1996). Elastic-Net
    was proposed later using a combination of both L1 regularization and L2 regularization
    terms (Zou et al., 2005). Here, we want to practice using these three methods,
    but you can use regularization hyperparameters with other methods, such as an
    SVM or XGBoost classifier (see *Table 5.2*).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the necessary libraries and design a plotting function to visually
    show the effect of the regularization parameter values. We also load the digit
    dataset from `scikit-learn` to use for model training and evaluation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can use the `GridSearchCV()` function to assess the effect of different
    regularization parameter values in the following models. In `scikit-learn`, the
    regularization parameter is usually named `alpha` instead of λ, and the mixing
    parameter is called `l1_ratio` instead of α. Here, we first assess the effect
    of different `alpha` values on Lasso models, with L1 regularization, trained and
    evaluated using a digit dataset:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The optimum `alpha` is identified to be 0.1, as shown in the following plot,
    which results in the highest accuracy across the considered values. This means
    that increasing the effect of regularization after an `alpha` value of `0.1` increases
    the model bias, resulting in a model with low performance in training.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso
    model](img/B16369_05_10.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Accuracy versus the regularization parameter alpha for a Lasso
    model
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: If we assess the effect of different `alpha` values in a ridge model, with L2
    regularization, we can see that the performance increases as we increase the strength
    of regularization (see *Figure 5**.11*).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge
    model](img/B16369_05_11.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Accuracy versus the regularization parameter alpha for a ridge
    model
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to these two methods is Elastic-Net, which combines the effect
    of L1 and L2 regularizations. In this case, the trend of the effect of `alpha`
    on the model performance is more similar to Lasso; however, the range of accuracy
    values is narrower in comparison with Lasso, which only relies on L1 regularization
    (see *Figure 5**.12*).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net
    model](img/B16369_05_12.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Accuracy versus the regularization parameter alpha for an Elastic-Net
    model
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'If your dataset is not very small, more complex models help you to achieve
    higher performance. It would be only in rare cases that you would consider a linear
    model your ultimate model. To assess the effect of regularization on more complex
    models, we chose the SVM classifier and examined the effect of different values
    of `C` as the regularization parameter in `sklearn.svm.SVC()`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As shown next, the range of accuracy for the models is higher, between 0.92
    and 0.99, compared to linear models with an accuracy of lower than 0.6, but higher
    regularization controls overfitting and achieves better performance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification
    model](img/B16369_05_13.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Accuracy versus regularization parameter C for an SVM classification
    model
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 12*](B16369_12.xhtml#_idTextAnchor320), *Going Beyond ML Debugging
    with Deep Learning*, you will also learn about regularization techniques in deep
    neural network models.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about techniques to improve the performance of
    your models and reduce their bias and variance. You learned about the different
    hyperparameters of widely used machine learning methods, other than deep learning,
    which will be covered later in the book, and Python libraries to help you in identifying
    the optimal hyperparameter sets. You learned about regularization as another technique
    to help you in training generalizable machine learning models. You also learned
    how to increase the quality of the data to be fed into the training process by
    methods such as synthetic data generation and outlier detection.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about interpretability and explainability
    in machine learning modeling and how you can use the related techniques and Python
    tools to identify opportunities for improving your models.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does adding more features and training data points reduce model variance?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you provide examples of methods to use to combine data with different
    confidence in class labels?
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could oversampling improve the generalizability of your supervised machine
    learning models?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between DSMOTE and Borderline-SMOTE?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do you need to check the effect of every single value of every hyperparameter
    of a model during hyperparameter optimization?
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could L1 regularization eliminate the contribution of some of the features to
    supervised model predictions?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could Lasso and Ridge regression models result in the same performance on the
    same test data if trained using the same training data?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bergstra, James, and Yoshua Bengio. “*Random search for hyper-parameter optimization*.”
    *Journal of machine learning research* 13.2 (2012).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra, James, et al. “*Algorithms for hyper-parameter optimization*.” *Advances
    in neural information processing systems* 24 (2011).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen, Vu. “*Bayesian optimization for accelerating hyper-parameter tuning*.”
    *2019 IEEE second international conference on artificial intelligence and knowledge
    engineering (AIKE)*. IEEE (2019).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li, Lisha, et al. “*Hyperband: A novel bandit-based approach to hyperparameter
    optimization*.” *Journal of Machine Learning Research* 18.1 (2017): pp. 6765-6816.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Falkner, Stefan, Aaron Klein, and Frank Hutter. “*BOHB: Robust and efficient
    hyperparameter optimization at scale*.” *International Conference on Machine Learning*.
    PMLR (2018).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ng, Andrew, Stanford CS229: Machine Learning Course, Autumn 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wong, Sebastien C., et al. “*Understanding data augmentation for classification:
    when to warp?*.” *2016 international conference on digital image computing: techniques
    and applications (DICTA)*. IEEE (2016).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikołajczyk, Agnieszka, and Michał Grochowski. “*Data augmentation for improving
    deep learning in image classification problem*.” *2018 international interdisciplinary
    PhD workshop (IIPhDW)*. IEEE (2018).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shorten, Connor, and Taghi M. Khoshgoftaar. “*A survey on image data augmentation
    for deep learning*.” *Journal of big data* 6.1 (2019): pp. 1-48.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taylor, Luke, and Geoff Nitschke. “*Improving deep learning with generic data
    augmentation*.” *2018 IEEE Symposium Series on Computational Intelligence (SSCI)*.
    IEEE (2018).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shorten, Connor, Taghi M. Khoshgoftaar, and Borko Furht. “*Text data augmentation
    for deep learning*.” *Journal of big Data* 8.1 (2021): pp. 1-34.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perez, Luis, and Jason Wang. “*The effectiveness of data augmentation in image
    classification using deep learning*.” arXiv preprint arXiv:1712.04621 (2017).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ashrapov, Insaf. “*Tabular GANs for uneven distribution*.” arXiv preprint arXiv:2010.00638
    (2020).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu, Lei, et al. “*Modeling tabular data using conditional gan*.” *Advances in
    Neural Information Processing Systems* 32 (2019).
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chawla, Nitesh V., et al. “*SMOTE: synthetic minority over-sampling technique*.”
    *Journal of artificial intelligence research* 16 (2002): pp. 321-357.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han, H., Wang, WY., Mao, BH. (2005). “*Borderline-SMOTE: A New Over-Sampling
    Method in Imbalanced Data Sets Learning*.” In: Huang, DS., Zhang, XP., Huang,
    GB. (eds) *Advances in Intelligent Computing*. ICIC 2005\. *Lecture Notes in Computer
    Science*, vol. 3644\. Springer, Berlin, Heidelberg.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He, Haibo, Yang Bai, E. A. Garcia, and Shutao Li, “*ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*.” *2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence)* (2008):
    pp. 1322-1328, doi: 10.1109/IJCNN.2008.4633969.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'X. Xiaolong, C. Wen, and S. Yanfei, “*Over-sampling algorithm for imbalanced
    data classification*,” in *Journal of Systems Engineering and Electronics*, vol.
    30, no. 6, pp. 1182-1191, Dec. 2019, doi: 10.21629/JSEE.2019.06.12.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last, Felix, Georgios Douzas, and Fernando Bacao. “*Oversampling for imbalanced
    learning based on k-means and smote*.” arXiv preprint arXiv:1711.00837 (2017).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emmott, Andrew F., et al. “*Systematic construction of anomaly detection benchmarks
    from real data*.” *Proceedings of the ACM SIGKDD workshop on outlier detection
    and* *description*. 2013.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emmott, Andrew, et al. “*A meta-analysis of the anomaly detection problem*.”
    arXiv preprint arXiv:1503.01158 (2015).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu, Fei Tony, Kai Ming Ting, and Zhi-Hua Zhou. “*Isolation forest*.” *2008
    eighth IEEE international conference on data mining*. IEEE (2008).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pevný, Tomáš. “*Loda: Lightweight on-line detector of anomalies*.” *Machine
    Learning 102* (2016): pp. 275-304.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Breunig, Markus M., et al. “*LOF: identifying density-based local outliers*.”
    *Proceedings of the 2000 ACM SIGMOD international conference on Management of*
    *data* (2000).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kriegel, Hans-Peter, Matthias Schubert, and Arthur Zimek. “*Angle-based outlier
    detection in high-dimensional data*.” *Proceedings of the 14th ACM SIGKDD international
    conference on Knowledge discovery and data* *mining* (2008).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Joo Seuk Kim and C. Scott, “*Robust kernel density estimation*.” *2008 IEEE
    International Conference on Acoustics, Speech and Signal Processing*, Las Vegas,
    NV, USA (2008): pp. 3381-3384, doi: 10.1109/ICASSP.2008.4518376.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schölkopf, Bernhard, et al. “*Support vector method for novelty detection*.”
    *Advances in neural information processing systems* 12 (1999).
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weiss, Karl, Taghi M. Khoshgoftaar, and DingDing Wang. “*A survey of transfer
    learning*.” *Journal of Big data* 3.1 (2016): pp. 1-40.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonekaboni, Seyed Ali Madani, et al. “*Learning across label confidence distributions
    using Filtered Transfer Learning*.” *2020 19th IEEE International Conference on
    Machine Learning and Applications (ICMLA)*. IEEE (2020).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tibshirani, Robert. “*Regression shrinkage and selection via the lasso*.” *Journal
    of the Royal Statistical Society: Series B (Methodological)* 58.1 (1996): pp.
    267-288.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hastie, Trevor, et al. *The elements of statistical learning: data mining,
    inference, and prediction*. vol. 2\. New York: Springer, 2009.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zou, Hui, and Trevor Hastie. “*Regularization and variable selection via the
    elastic net*.” *Journal of the Royal Statistical Society: Series B (Statistical
    Methodology)* 67.2 (2005): pp. 301-320.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
