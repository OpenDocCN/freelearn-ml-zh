- en: '*Chapter 8*: Azure Machine Learning Pipelines'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about advanced preprocessing techniques,
    such as category embeddings and NLP, to extract semantic meaning from text features.
    In this chapter, you will learn how to use these preprocessing and transformation
    techniques to build reusable ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: First, you will understand the benefits of splitting your code into individual
    steps and wrapping those into a pipeline. Not only can you make your code blocks
    reusable through modularization and parameters, but you can also control the compute
    targets for individual steps. This helps to optimally scale your computations,
    save costs, and improve performance at the same time. Lastly, you can parameterize
    and trigger your pipelines through an HTTP endpoint or through a recurring or
    reactive schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will build a complex Azure Machine Learning pipeline in a couple of
    steps. We will start with a simple pipeline, add data inputs, outputs, and connections
    between the steps, and deploy the pipeline as a web service. You will also learn
    about advanced scheduling, based on the frequency and changing data, as well as
    how to parallelize pipeline steps for large data.
  prefs: []
  type: TYPE_NORMAL
- en: In the last part, you will learn how to integrate Azure Machine Learning pipelines
    into other Azure services such as Azure Machine Learning designer, Azure Data
    Factory, and Azure DevOps. This will help you to understand the commonalities
    and differences between the different pipeline and workflow services and how you
    can trigger ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Using pipelines in ML workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and publishing an ML pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating pipelines with other Azure services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the following Python libraries and versions to
    create pipelines and pipeline steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`azureml-core 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`azureml-sdk 1.34.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning. However,
    all scripts need to be scheduled to execute in Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08).'
  prefs: []
  type: TYPE_NORMAL
- en: Using pipelines in ML workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Separating your workflow into reusable configurable steps and combining these
    steps into an end-to-end pipeline provides many benefits for implementing end-to-end
    ML processes. Multiple teams can own and iterate on individual steps to improve
    the pipeline over time, while others can easily integrate each version of the
    pipeline into their current setup.
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline itself doesn't only split code from execution; it also splits the
    execution from orchestration. Hence, you can configure individual compute targets
    that can be used to optimize your execution and provide parallel execution while
    you don't have to touch the ML code.
  prefs: []
  type: TYPE_NORMAL
- en: We will take a quick look at Azure Machine Learning pipelines and why they are
    your tool of choice when implementing ML workflows in Azure. In the following
    section, *Building and publishing an ML pipeline*, we will dive a lot deeper and
    explore the individual features by building such a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Why build pipelines?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a single developer doing mostly experimentation and working simultaneously
    on data, infrastructure, and modeling, pipelines don't add a ton of benefits to
    the developer's workflow. However, as soon as you perform enterprise-grade development
    across multiple teams that iterate on different parts of the ML system, then you
    will greatly benefit from splitting your code into a pipeline of individual execution
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: This modularization will give you great flexibility, and multiple teams will
    be able to collaborate efficiently. Teams can integrate your models and pipelines
    while you are iterating and building new versions of your pipeline at the same
    time. By using versioned pipelines and pipeline parameters, you can control how
    your data or model service pipeline should be called and ensure auditing and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Another important benefit of using workflows instead of running everything inside
    a single file is execution speed and cost improvements. Instead of running a single
    script on the same compute instance, you can run and scale the steps individually
    on different compute targets. This gives you greater control over potential cost
    savings and better optimization for performance, and you only ever have to retry
    the parts of the pipeline that failed and never the whole pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Through scheduling pipelines, you can make sure that all your pipeline runs
    are executed without your manual intervention. You simply define triggers, such
    as the existence of new training data, that should execute your pipeline. Decoupling
    your code execution from triggering the execution gives you a ton of benefits,
    such as easy integration into many other services.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the modularity of your code allows for great reusability. By splitting
    your script into functional steps such as cleaning, preprocessing, feature engineering,
    training, and hyperparameter tuning, you can version and reuse these steps for
    other projects as well.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, as soon as you want to benefit from one of these advantages, you
    can start organizing your code in pipelines, which can be deployed, scheduled,
    versioned, scaled, and reused. Let's find out how you can achieve this in Azure
    Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: What are Azure Machine Learning pipelines?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Azure Machine Learning pipelines** are workflows of executable steps in Azure
    Machine Learning that compose a complete ML workflow. Hence, you can combine data
    import, data transformations, feature engineering, model training, optimization,
    and also deployment as your pipeline steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines are resources in your Azure Machine Learning workspace that you can
    create, manage, version, trigger, and deploy. They integrate with all other Azure
    Machine Learning workspace resources such as datasets and datastores for loading
    data, compute instances, models, and endpoints. Each pipeline run is executed
    as an experiment on your Azure Machine Learning workspace and gives you the same
    benefits that we covered in the previous chapters, such as tracking files, logs,
    models, artifacts, and images while running on flexible compute clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning pipelines should be your first choice when implementing
    flexible and reusable ML workflows. By using pipelines, you can modularize your
    code into blocks of functionality and versions and share those blocks with other
    projects. This makes it easy to collaborate with other teams on complex end-to-end
    ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Another great integration of Azure Machine Learning pipelines is the integration
    with endpoints and triggers in your workspace. With a single line of code, you
    can publish a pipeline as a web service or web service endpoint and use this endpoint
    to configure and trigger the pipeline from anywhere. This opens the door for integrating
    Azure Machine Learning pipelines with many other Azure and third-party services.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you need a more complex trigger, such as continuous scheduling or
    reactive triggering based on changes in the source data, you can easily configure
    this as well. The added benefit of using pipelines is that all orchestration functionality
    is completely decoupled from your training code.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you get a lot of benefits by using Azure Machine Learning pipelines
    for your ML workflows. However, it's worth noting that this functionality does
    come with some extra overhead, namely wrapping each computation in a pipeline
    step, adding pipeline triggers, configuring environments and compute targets for
    each step, and exposing parameters as pipeline options. Let's start by building
    our first pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building and publishing an ML pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go ahead and use all we have learned from the previous chapters and build
    a pipeline for data processing. We will use the Azure Machine Learning SDK for
    Python to define all the pipeline steps as Python code so that it can be easily
    managed, reviewed, and checked into version control as an authoring script.
  prefs: []
  type: TYPE_NORMAL
- en: We will define a pipeline as a linear sequence of pipeline steps. Each step
    will have an input and output defined as pipeline data sinks and sources. Each
    step will be associated with a compute target that defines both the execution
    environment as well as the compute resource for execution. We will set up an execution
    environment as a Docker container with all the required Python libraries and run
    the pipeline steps on a training cluster in Azure Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: A pipeline runs as an experiment in your Azure Machine Learning workspace. We
    can either submit the pipeline as part of the authoring script, deploy it as a
    web service and hence trigger it through a webhook, schedule it as a published
    pipeline similar to cron jobs, or trigger it from a third-party service such as
    Logic Apps.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, running a linear sequential pipeline is good enough. However,
    when the amount of data increases and pipeline steps become slower and slower,
    we need to find a way to speed up these large computations. A common solution
    for speeding up data transformations, model training, and scoring is through parallelization.
    Hence, we will add a parallel execution step to our data transformation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in the first section of this chapter, one of the main reasons
    for decoupling ML workflows into pipelines is modularity and reusability. By splitting
    a workflow into individual steps, we build the foundation for reusable computational
    blocks for common ML tasks, be it data analysis through visualizations and feature
    importance, feature engineering through NLP and third-party data, or simply the
    scoring of common ML tasks such as automatic image tagging through object detection.
  prefs: []
  type: TYPE_NORMAL
- en: In Azure Machine Learning pipelines, we can use modules to create reusable computational
    steps from a pipeline. A module is a management layer on top of a pipeline step
    that allows you to version, deploy, load, and reuse pipeline steps with ease.
    The concept is very similar to to versioning source code or versioning datasets
    in ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: For any enterprise-grade ML workflow, the usage of pipelines is essential. Not
    only does it help you decouple, scale, trigger, and reuse individual computational
    steps, but it also provides auditability and monitorability to your end-to-end
    workflow. On top, splitting computational blocks into pipeline steps will set
    you up for a successful transition to MLOps – a **Continuous Integration and Continuous
    Deployment (CI/CD)** process for ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started and implement our first Azure Machine Learning pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Azure Machine Learning pipeline is a sequence of individual computational
    steps that can be executed in parallel or a series. Azure Machine Learning provides
    additional features on top of the pipeline, such as visualization of the computational
    graph, data transfer between steps, and publishing pipelines either as an endpoint
    or published pipeline. In this section, we will create a simple pipeline step
    and execute the pipeline to explore the Azure Machine Learning pipeline capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the type of computation, you can schedule jobs on different compute
    targets such as Azure Machine Learning, Azure Batch, Databricks, Azure Synapse,
    and more, or run *automated ML* or *HyperDrive* experiments. Depending on the
    execution type, you need to provide additional configuration to each step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a simple pipeline that consists only of a single step. We
    will incrementally add more functionality and steps in the subsequent sections.
    First, we need to define the type of execution for our pipeline step. While `PipelineStep`
    is the base class for any execution we can run in the pipeline, we need to choose
    one of the step implementations. The following steps are available at the time
    of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AutoMLStep`: Runs an automated ML experiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AzureBatchStep`: Runs a script on Azure Batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DatabricksStep`: Runs a Databricks notebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataTransferStep`: Transfers data between Azure storage accounts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HyperDriveStep`: Runs a HyperDrive experiment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ModuleStep`: Runs a module'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MpiStep`: Runs an **Message Passing Interface (MPI)** job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ParallelRunStep`: Runs a script in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PythonScriptStep`: Runs a Python script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RScriptStep`: Runs an R script'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SynapseSparkStep`: Runs a Spark script on Synapse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CommandStep`: Runs a script or command'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KustoStep`: Runs a Kusto query on Azure Data Explorer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our simple example, we want to run a single Python data preprocessing script
    in our pipeline, so we''ll choose `PythonScriptStep` from the preceding list.
    We are building on the same examples and code samples that we saw in the previous
    chapters. In this first pipeline, we will execute a single step that will load
    the data directly from the script – and hence doesn''t require any input or output
    to the pipeline step. We will add these separately in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline steps are all attached to an Azure Machine Learning workspace.
    Hence, we start by loading the workspace configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need a compute target that we can execute our pipeline step on. Let''s
    create an auto-scaling Azure Machine Learning training cluster as a compute target,
    similar to what we have created in previous chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition, we will need a run configuration that defines our training environment
    and Python libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now define `PythonScriptStep`, which provides all the required configuration
    and entry points for a target ML training script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in the preceding code, we are configuring `script_name` and the
    `source_directory` parameter, which contain the preprocessing script. We also
    pass the `runconfig` runtime configuration and the `compute_target` compute target
    to `PythonScriptStep`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall from previous chapters, we previously submitted the `ScriptRunConfig`
    objects as an experiment to the Azure Machine Learning workspace. In the case
    of pipelines, we first need to wrap the pipeline step in `Pipeline` and instead
    submit the pipeline as an experiment. While this seems counterintuitive at first,
    we will see how we can then parametrize the pipeline and add more computational
    steps to it. In the next code snippet, we define the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see, the pipeline is defined simply through a series of pipeline
    steps and linked to a workspace. In our example, we only define a single execution
    step. Let''s also check that we didn''t make any mistakes configuring our pipeline
    through the built-in pipeline validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the pipeline is validated successfully, we are ready for execution. The
    pipeline can be executed by submitting it as an experiment to the Azure Machine
    Learning workspace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You just ran your first very simple Azure Machine Learning
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find many complete and up-to-date examples for using Azure Machine
    Learning pipelines in the official Azure repository: [https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a pipeline is submitted, it is shown under the **Pipelines** section as
    well as under the **Experiments** section, as shown in *Figure 8.1*. A pipeline
    is treated as an experiment, where each pipeline run is like an experiment run.
    Each step of a pipeline, as well as its logs, figures, and metrics, can be accessed
    as a child run of the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning ](img/B17928_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: While this simple pipeline doesn't add a ton of benefits to directly submitting
    the script as an experiment, we can now add additional steps to the pipeline and
    configure data input and output. Let's take a look!
  prefs: []
  type: TYPE_NORMAL
- en: Connecting data inputs and outputs between steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pipeline steps are computational blocks, whereas the pipeline defines the sequence
    of step executions. In order to control the data flow, we need to define input
    and output for the pipeline as well as wire up data input and output for individual
    steps. The data flow between the computational blocks will ultimately define the
    execution order for the blocks, and hence turns a sequence of steps into a directed
    acyclic execution graph. This is exactly what we are going to explore in this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In most cases, a pipeline needs external input, connections between the individual
    blocks, as well as persisted output. In Azure Machine Learning pipelines, we will
    use the following building blocks to configure this data flow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-persisted pipeline input: `Dataset`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data between pipeline steps: `PipelineData`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Persisting pipeline output: `PipelineData.as_dataset()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will look at all three types of data input and output. First,
    we will look at how we pass data as input into a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Input data to pipeline steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start with adding a data input to the first step in a pipeline. To do
    so – or to pass any pre-persisted data to a pipeline step – we will use a **dataset**,
    which we saw previously in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*. In Azure Machine Learning, a dataset is
    an abstract reference for data stored in a specified path with specified encoding
    on a specified data storage system. The storage system itself is abstracted as
    a **datastore** object, a reference to the physical system with information about
    location, protocol, and access permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall from the previous chapters, we can access a dataset that was
    previously registered in our Azure Machine Learning workspace by simply referencing
    it by name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is very convenient when your data was initially organized
    and registered as a dataset. As pipeline developers, we don't need to know the
    underlying data format (for example, CSV, ZIP, Parquet, and JSON) and on which
    Azure Blob storage or Azure SQL database the data is stored. Pipeline developers
    can consume the specified data and instead focus on pre-processing, feature engineering,
    and model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when passing new data into an Azure Machine Learning pipeline, we
    often don''t have the data registered as datasets. In these cases, we can create
    a new dataset reference. Here is an example of how to create `Dataset` from publicly
    available data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: There are multiple ways to transform files and tabular data into `Dataset`.
    While this seems like a bit of complicated extra work instead of passing absolute
    paths to your pipelines directly, you will gain many benefits from following this
    convention. Most importantly, all compute instances in your Azure Machine Learning
    workspace will be able to access, read, and parse the data without any additional
    configuration. In addition, Azure Machine Learning will reference and track the
    dataset used for each experiment run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have obtained a reference to `Dataset`, we can pass the dataset to
    the pipeline step as input. When passing a dataset to the computational step,
    we can configure additional configurations such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A name for the dataset reference in the script – `as_named_input()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An access type for `FileDataset` – `as_download()` or `as_mount()`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we configure the tabular dataset as the named input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use `PythonScriptStep`, which will allow us to pass arguments
    to the pipeline step. We need to pass the dataset to two parameters – as an argument
    to the pipeline script and as an input dependency for the step. The former will
    allow us to pass the dataset to the Python script, whereas the latter will track
    the dataset as a dependency of this pipeline step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the preceding example, we can pass one (or multiple) datasets
    to the pipeline step as the `inputs` parameter, as well as an argument to the
    script. Using a specific name for this dataset will help us to differentiate between
    multiple inputs in the pipeline. We will update the preprocessing script to parse
    the dataset from the command-line arguments, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_input.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, the dataset gets passed as a dataset name
    to the Python script. We can use the `Dataset` API to retrieve the data at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we submit the pipeline for execution, we can see the pipeline visualized
    in the Azure Machine Learning Studio interface, as shown in *Figure 8.2*. We can
    see that the dataset is passed as the **titanic** named input to the **Preprocessing**
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – The dataset as a pipeline step input ](img/B17928_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The dataset as a pipeline step input
  prefs: []
  type: TYPE_NORMAL
- en: This is a great way to decouple a block of functionality from its input and
    helps you to build reusable blocks. We will see in the subsequent section, *Reusing
    pipeline steps through modularization*, how we can turn these reusable blocks
    into shared modules.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Instead of passing datasets as input arguments to the pipeline step, we can
    also access named inputs from the run context using the following property on
    the run context object – `Run.get_context().input_datasets['titanic']`. However,
    setting up datasets as input and output arguments makes it easier to reuse pipeline
    steps and code snippets across pipelines and other experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's find out how to set up a data flow between individual pipeline steps.
  prefs: []
  type: TYPE_NORMAL
- en: Passing data between steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we define input to a pipeline step, we often want to configure the output
    for the computations. By passing in input and output definitions, we separate
    the pipeline step from predefined data storage and avoid having to move data around
    as part of the computation step.
  prefs: []
  type: TYPE_NORMAL
- en: 'While pre-persisted inputs were defined as `Dataset` objects, data connections
    (input and output) between pipeline steps are defined through `PipelineData` objects.
    Let''s look at an example of a `PipelineData` object used as output for one pipeline
    step and input for another step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the previous example, we pass the dataset as arguments and reference
    them as `outputs`. The former will allow us to retrieve the dataset in the script,
    whereas the latter defines the step dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we pass the expected output path to the scoring file, we need to parse
    the command-line arguments to retrieve the path. The scoring file looks like the
    following snippet in order to read the output path and output a pandas DataFrame
    to the desired output location. We first need to parse the command-line arguments
    in the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_output.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `PipelineData` arguments get interpolated at runtime and replaced with
    the local path for the mounted dataset directory. Therefore, we can simply write
    the data to this local directory, and it will be automatically registered in the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: preprocess_output.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we output data to a `PipelineData` dataset, we can pass these datasets
    to the next pipeline step. Passing the datasets works exactly the same as we saw
    in the previous section – we pass them as arguments and register them as `inputs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can load the data in the training script. If you remember from the
    previous step, `PipelineData` is interpolated as paths on the local execution
    environment. Hence, we can read the data from the path that got interpolated in
    the command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: train.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can wrap both steps as a `Pipeline` object by passing the steps
    using the pipeline `steps` keyword. The `pipeline` object can be passed as an
    experiment to Azure Machine Learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the previous example, we can read the output path from the
    command-line arguments and use it in the Python script as a standard file path.
    Hence, we need to make sure that the file path exists and output some tabular
    data into the location. Next, we define the input for the second validation step
    that reads the newly created data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Passing data between pipeline steps ](img/B17928_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Passing data between pipeline steps
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will take a look at how to persist the output of a pipeline step
    for usage outside of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting data outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this last section, we will learn how to persist the output data of a pipeline.
    A common task for pipelines is building data transformations – and hence we often
    expect pipelines to output data.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we learned about creating outputs from pipeline steps
    with `PipelineData`, mainly to connect these outputs to inputs of subsequent steps.
    We can use the same method to define a final persisted output of a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Doing so is very simple once you understood how to create, persist, and version
    datasets. The reason for this is that we can convert a `PipelineData` object into
    a dataset using the `as_dataset()` method. Once we have a reference to the `Dataset`
    object, we can go ahead and either export it to a specific datastore or register
    it as a dataset in the workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a snippet of how to convert a `PipelineData` object defined as output
    in a pipeline step to a dataset and register it in the Azure Machine Learning
    workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'By calling the preceding authoring code, you will be able to access the resulting
    predictions as a dataset in any compute instance connected with your workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – A dataset as a pipeline step output ](img/B17928_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – A dataset as a pipeline step output
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will take a look at the different ways to trigger a pipeline execution.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing, triggering, and scheduling a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After you have created your first simple pipeline, you have multiple ways of
    running the pipeline. One example that we already saw was submitting the pipeline
    as an experiment to Azure Machine Learning. This would simply execute the pipeline
    from the same authoring script where the pipeline was configured. While this is
    a good start at first to execute a pipeline, there are other ways to trigger,
    parametrize, and execute it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Common ways to execute a pipeline are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Publish the pipeline as a web service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trigger the published pipeline using a webhook.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schedule the pipeline to run periodically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will look at all three methods to help you trigger and execute
    your pipelines with ease. Let's first start by publishing and versioning your
    pipeline as a web service.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing a pipeline as a web service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A common reason to split an ML workflow into a reusable pipeline is that you
    can parametrize and trigger it for various tasks whenever needed. Good examples
    are common pre-processing tasks, feature engineering steps, and batch scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, turning a pipeline into a parametrizable web service that we can trigger
    from any other application is a great way of deploying our ML workflow. Let's
    get started and wrap and deploy the previously built pipeline as a web service.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we want our published pipeline to be configurable through HTTP parameters,
    we need to first create these parameter references. Let''s create a parameter
    to control the learning rate of our training pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we link the pipeline parameter with the pipeline step by passing it as
    an argument to the training script. We extend the step from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we added the learning rate as a parameter to the
    list of command-line arguments. In the training script, we can parse the command-line
    arguments and read the parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: score.py
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the only step left is to publish the pipeline. To do so, we create a pipeline
    and call the `publish()` method. We need to pass a name and version to the pipeline,
    which will now be a versioned published pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That's all the code you need to expose a pipeline as a parametrized web service
    with authentication. If you want to abstract your published pipeline from a specific
    endpoint – for example, to iterate on the development process of your pipeline
    while letting other teams integrate the web service into their application – you
    can also deploy pipeline webhooks as endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example where we take the previously created pipeline service
    and expose it through a separate endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We have deployed and decoupled the pipeline and the pipeline endpoint. We can
    finally call and trigger the endpoint through the service endpoint. Let's look
    at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering a published pipeline with a webhook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The published pipeline web service requires authentication. Hence, let''s first
    retrieve an Azure Active Directory token before we call the web service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the authentication token, we can now trigger and parametrize the pipeline
    by calling the service endpoint. Let''s look at an example using the `requests`
    library. We can configure the learning rate through the `lr_arg` parameter defined
    in the previous section as well as the experiment name by sending a custom JSON
    body. If you recall, the pipeline will still run as an experiment in your Azure
    Machine Learning workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We can see in the preceding code snippet that we call the pipeline webhook using
    a `POST` request and configure the pipeline run by sending a custom JSON body.
    For authentication, we also need to pass the authentication as an HTTP header.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used a Python script to trigger the web service endpoint.
    However, you can use any other Azure service for triggering this pipeline now
    through the webhook, such as Azure Logic Apps, CI/CD pipelines in Azure DevOps,
    or any other custom application. If you'd prefer your pipeline to run periodically
    instead of triggering it manually, you can set up a pipeline schedule. Let's take
    a look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling a published pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setting up continuous triggers for workflows is a common use case when building
    pipelines. These triggers can run a pipeline and retrain a model every week or
    every day if new data is available. Azure Machine Learning pipelines support two
    types of scheduling techniques – continuous scheduling through a pre-defined frequency,
    and reactive scheduling and data change detection through a polling interval.
    In this section, we will take a look at both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start scheduling a pipeline, we will first explore a way to list
    all the previously defined pipelines of a workspace. To do so, we can use the
    `PublishedPipeline.list()` method, similar to the `list()` method from our Azure
    Machine Learning workspace resources. Let''s print the name and ID of every published
    pipeline in the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: To set up a schedule for a published pipeline, we need to pass the pipeline
    ID as an argument. We can retrieve the desired pipeline ID from the preceding
    code snippet and plug it into the schedule declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will look at continuous schedules that re-trigger a pipeline with
    a predefined frequency, similar to cron jobs. To define the scheduling frequency,
    we need to create a `ScheduleRecurrence` object. Here is an example snippet to
    create a recurring schedule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is all you need to set up a recurring schedule that continuously
    triggers your pipeline. The pipeline will run as the defined experiment in your
    Azure Machine Learning workspace. Using the `pipeline_parameters` argument, you
    can pass additional parameters to the pipeline runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure Machine Learning pipelines also support another type of recurring scheduling,
    namely polling for changes in a datastore. This type of schedule is referred to
    as a reactive schedule and requires a connection to a datastore. It will trigger
    your pipeline whenever data changes in your datastore. Here is an example of setting
    up a reactive schedule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in this example, we set up the reactive schedule using a datastore
    reference and a polling interval in minutes. Hence, the schedule will check each
    polling interval to see which blobs have changed, if any and use those to trigger
    the pipeline. The blob names will be passed to the pipeline using the `data_path_parameter_name`
    parameter. Similar to the previous schedule, you can also send additional parameters
    to the pipeline using the `pipeline_parameters` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s take a look at how to programmatically stop a schedule once
    it is enabled. To do so, we need a reference to the schedule object. We can get
    this, similar to any other resource in Azure Machine Learning, by fetching the
    schedules for a specific workplace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can filter this list using all the available attributes on the schedule
    object. Once we have found the desired schedule, we can simply disable it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using the additional `wait_for_provisioning` argument, we ensure that we block
    code execution until the schedule is really disabled. You can easily re-enable
    the schedule using the `Schedule.enable` method. Now, you can create recurring
    and reactive schedules, continuously run your Azure Machine Learning pipelines,
    and disable them if not needed anymore. Next, we will take a look at parallelizing
    execution steps.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing steps to speed up large pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's inevitable in many cases that the pipeline will process more and more data
    over time. In order to parallelize a pipeline, you can run pipeline steps in parallel
    or sequence, or parallelize a single pipeline step computation by using `ParallelRunConfig`
    and `ParallelRunStep`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we jump into parallelizing a single step execution, let''s first discuss
    the control flow of a simple pipeline. We will start with a simple pipeline that
    is constructed using multiple steps, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'When we submit this pipeline, how will these four steps be executed – in series,
    in parallel, or even in an undefined order? In order to answer the question, we
    need to look at the definitions of the individual steps. If all steps are independent
    and the compute target for each step is large enough, all steps are executed in
    parallel. However, if we define `PipelineData` as the output of `step1` and input
    it into the other steps, these steps will only be executed after `step1` has finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – A pipeline with parallel steps ](img/B17928_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – A pipeline with parallel steps
  prefs: []
  type: TYPE_NORMAL
- en: The data connections between the pipeline steps implicitly define the execution
    order of the steps. If no dependencies exist between the steps, all steps are
    scheduled in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one exception to the preceding statement, which is enforcing a specific
    execution order of pipeline steps without a dedicated data object as a dependency.
    In order to do this, you can define these dependencies manually, as shown in the
    next code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding configuration will first execute `step1` and `step2` in parallel
    before scheduling `step3`, thanks to your explicitly configured dependencies.
    This can be useful when you are accessing state or data in resources outside of
    the Azure Machine Learning workspace; hence, the pipeline cannot implicitly create
    a dependency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A pipeline with a custom step order ](img/B17928_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – A pipeline with a custom step order
  prefs: []
  type: TYPE_NORMAL
- en: Once we have answered the question of step execution order, we want to learn
    how we can execute a single step in parallel rather than multiple steps. A great
    use case for this is batch scoring a large amount of data. Rather than partitioning
    your input data as input for multiple steps, you want the data as input for a
    single step. However, to speed up the scoring process, you want a parallel execution
    of the scoring for the single step.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Azure Machine Learning pipelines, you can use a `ParallelRunStep` step to
    configure a parallel execution for a single step. To configure the data partitions
    and parallelization of the computation, you need to create a `ParallelRunConfig`
    object. The parallel run step is a great choice for any type of parallelized computation
    that helps us to split the input data into smaller partitions (also called batches
    or mini-batches) of data. Let''s walk through an example for setting up parallel
    execution for a single pipeline step. We will configure both batch sizes as a
    pipeline parameter that can be set when calling the pipeline step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet defines the run configuration for parallelizing the computation
    by splitting the input into mini-batches. We configure the batch size as a pipeline
    parameter, `batch_size`. We also configure the compute target and parallelism
    by the `node_count` and `process_count_per_node` parameters. Using these settings,
    we can score four mini-batches in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: The `score.py` script is a deployment file that needs to contain an `init()`
    and `run(batch)` method. The `batch` argument contains a list of filenames that
    will get extracted from the input argument of the step configuration. We will
    learn more about this file structure in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: The `run` method in the `score.py` script should return the scoring results
    or write the data to an external datastore. Depending on this, the `output_action`
    argument needs to be set to either `append_row`, which means that all values will
    be collected as run in a result file, or `summary_only`, which means that the
    user will deal with storing the results. You can define the result file in which
    all the rows will get appended using the `append_row_file_name` argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, setting up the run configuration for a parallel batch execution
    is not very simple and requires a bit of fiddling. However, once set up and configured
    properly, it can be used to scale out a computational step and run many tasks
    in parallel. Hence, we can now define `ParallelRunStep` with all required input
    and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we read from the input dataset that references all files on
    the datastore. We write the results to the `mnist_results` folder in our custom
    datastore. Finally, we can start the run and look at the results. To do so, we
    submit the pipeline as an experiment run to Azure Machine Learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Splitting a step execution into multiple partitions will help you to speed up
    the computation of large amounts of data. It pays off as soon as the time of computation
    is significantly longer than the overhead of scheduling a step execution on a
    compute target. Therefore, `ParallelRunStep` is a great choice for speeding up
    your pipeline, with only a few changes in your pipeline configuration required.
    Next, we will take a look into better modularization and the reusability of pipeline
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing pipeline steps through modularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By splitting your workflow into pipeline steps, you are laying the foundation
    for reusable ML and data processing building blocks. However, instead of copying
    and pasting your pipelines, pipeline steps, and code into other projects, you
    might want to abstract your functionality into functional high-level modules.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example. Suppose you are building a pipeline step that takes
    in a dataset of user and item ratings and outputs a recommendation of the top
    five items for each user. However, while you are fine-tuning the recommendation
    engine, you want to enable your colleagues to integrate the functionality into
    their pipeline. A great way would be to separate the implementation and usage
    of the code, define the input and output data formats, and modularize and version
    it. That's exactly what modules do in the scope of the Azure Machine Learning
    pipeline steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a module, the container that will hold a reference to the computational
    step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define input and output for the module using the `InputPortDef` and
    `OutputPortDef` bindings. These are input and output references that later need
    to be bound to data references. We use these bindings to abstract all of our input
    and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can define the module functionality by publishing a Python script
    for this module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: That's all you need to do to enable others to reuse your recommendation block
    in their Azure Machine Learning pipelines. By using versioning and default versions,
    you can ensure exactly which code is pulled by your users. As we can see, you
    can define multiple inputs and outputs for each module and define configurable
    parameters for this module. In addition to publishing functionality as Python
    code, we can also publish an Azure Data Lake Analytics or Azure batch step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will take a look at how the module can be integrated into an Azure
    Machine Learning pipeline and executed together with custom steps. To do so, we
    will first load the module that was previously created using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, the great thing about this is that the preceding code will work in any
    Python interpreter or execution engine that has access to your Azure Machine Learning
    workspace. This is huge – no copying of code, no need for checking out dependencies,
    and no need for defining any additional access permissions for your application
    – everything is integrated with your workspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to write up the input and output for this pipeline step. Let''s
    pass the input from the pipeline directly to the recommendation module and output
    everything to the pipeline output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we parametrize the module with the use of pipeline parameters. This lets
    us configure a parameter in the pipeline that we can pass through to the recommendation
    module. In addition, we can define a default parameter for the parameter when
    used in this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We already defined the input and output for this pipeline as well as the input
    parameters for the pipeline step. The only thing we are missing is bringing everything
    together and defining a pipeline step. Similar to the previous section, we can
    define a pipeline step that will execute the modularized recommendation block.
    To do so, instead of `PythonScriptStep`, we now use `ModuleStep`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can execute the pipeline by submitting it as an experiment to our
    Azure Machine Learning workspace. This code is very similar to what we saw already
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The preceding step executes the modularized pipeline as an experiment in your
    Azure Machine Learning workspace. However, you can also choose any of the other
    publishing methods that we discussed in the previous sections, such as publishing
    as a web service or scheduling the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting pipeline steps into reusable modules is extremely helpful when working
    with multiple teams on the same ML projects. All teams can work in parallel, and
    the results can be easily integrated with a single Azure Machine Learning workspace.
    Let's take a look at how Azure Machine Learning pipelines integrate with other
    Azure services.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating pipelines with other Azure services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's rare that users only use a single service to manage data flows, experimentation,
    training, deployment, and CI/CD in the cloud. Other services provide specific
    features that make them a better fit for a task, such as Azure Data Factory for
    loading data into Azure and Azure Pipelines for CI/CD for running automated tasks
    in Azure DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: The strongest argument for choosing a cloud provider is the strong integration
    of its individual services. In this section, we will see how Azure Machine Learning
    pipelines integrate with other Azure services. The list for this section would
    be a lot longer if we were to cover every possible service for integration. As
    we learned in this chapter, you can trigger a published pipeline by calling a
    REST endpoint and submitting a pipeline using standard Python code. This means
    that you can integrate pipelines anywhere where you can call HTTP endpoints or
    run Python code.
  prefs: []
  type: TYPE_NORMAL
- en: We will first look into integration with Azure Machine Learning designer. The
    designer lets you build pipelines using graphical blocks, and these pipelines,
    published pipelines, and pipeline runs will show up in the workspace just like
    any other pipeline that we built in this chapter. Therefore, it is practical to
    take a quick look at the commonalities and differences.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will take a quick look at integrating Azure Machine Learning pipelines
    with Azure Data Factory, arguably an integration that is used the most. It's a
    very natural instinct to include ML pipelines with ETL pipelines for scoring,
    enriching, or enhancing data during the ETL process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will compare Azure Machine Learning pipelines with Azure Pipelines
    for CI/CD in Azure DevOps. While Azure DevOps was used mainly for application
    code and app orchestration, it is now transitioning to provide fully end-to-end
    MLOps workflows. Let's start with the designer and jump right in.
  prefs: []
  type: TYPE_NORMAL
- en: Building pipelines with Azure Machine Learning designer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Azure Machine Learning designer** is a graphical interface for creating complex
    ML pipelines through a drag and drop interface. You can choose a functionality
    represented as blocks for importing data, which will use a datastore and a dataset
    under the hood.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a simple pipeline to train and score a Boosted Decision
    Tree Regression model. As you can see, the block-based programming style requires
    less knowledge about the individual blocks, and it allows you to build complex
    pipelines without writing any code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The Azure Machine Learning designer pipeline ](img/B17928_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – The Azure Machine Learning designer pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Some actions, such as connecting the output of one computation to the input
    of the next, are arguably more convenient to create in the visual UI than with
    code. It's also easier to understand the data flow by visualizing the pipeline.
    Other actions, such as creating parallel executions of large data batches, are
    a bit easier to handle and maintain in code. However, due to our code-first approach
    for reproducibility, testability, and version control, we usually prefer code
    for authoring and execution.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that the functionality of pipelines in the designer and pipelines
    using code are not the same. While you have a broad set of preconfigured abstract
    functional blocks, such as the **Boosted Decision Tree Regression** block in the
    previous *Figure 8.7*, you can't access these functionalities in code. However,
    you can use scikit-learn, PyTorch, TensorFlow, and so on to reuse an existing
    functionality or build your own in code.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the first-class integration of the designer into the workspace, you
    can access all of the files, models, and datasets of the workspace from within
    the designer. An important takeaway is that all the resources that are created
    in the workspace such as pipelines, published pipelines, real-time endpoints,
    models, datasets, and so on are stored in a common system – independently of where
    they were created.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning pipelines in Azure Data Factory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When moving data, ETL, and trigger computations in various Azure services, you
    will most likely come across **Azure Data Factory**. It is a very popular service
    to move large amounts of data into Azure, perform processing and transformations,
    build workflows, and trigger many other Azure or third-party services.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning pipelines integrate very well with Azure Data Factory,
    and you can easily configure and trigger the execution of a published pipeline
    through Data Factory. To do so, you need to drag the **ML Execute Pipeline** activity
    to your Data Factory canvas and specify the pipeline ID of the published pipeline.
    In addition, you can also specify pipeline parameters as well as the experiment
    name for the pipeline run.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how the **ML Execute Pipeline** step can be configured
    in Azure Data Factory. It uses a linked service to connect to your Azure Machine
    Learning workspace, which allows you to select the desired pipeline from a drop-down
    box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Azure Data Factory with Azure Machine Learning activity ](img/B17928_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Azure Data Factory with Azure Machine Learning activity
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are configuring the computational steps using JSON, you can use the
    following snippet to create an **ML Execute Pipeline** activity with Azure Machine
    Learning as a linked service. Again, you must specify the pipeline ID and can
    pass an experiment name, as well as pipeline parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Finally, you can trigger the step by adding triggers or output into the **ML
    Execute Pipeline** activity. This will finally trigger your published Azure Machine
    Learning pipeline and start the execution in your workspace. This is a great addition
    and makes it easy for other teams to re-use your ML pipelines during classical
    ETL and data transformation processes.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Pipelines for CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure Pipelines is a feature of Azure DevOps that lets you run, build, test,
    and deploy code as a **Continuous Integration** (**CI**) and **Continuous Deployment**
    (**CD**) process. Hence, they are flexible pipelines for code and app orchestration
    with many advanced features, such as approval queues and gated phases.
  prefs: []
  type: TYPE_NORMAL
- en: By allowing you to run multiple blocks of code, the best way to integrate Azure
    Machine Learning into Azure DevOps is by using Python script blocks. If you have
    followed this book and used a code-first approach to author your experiments and
    pipelines, then this integration is very easy. Let's take a look at a small example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write a utility function that returns a published pipeline, given
    a workspace and pipeline ID as parameters. We will need this function in this
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can go ahead and implement a very simple Python script that allows
    us to configure and trigger a pipeline run in Azure. We will initialize the workspace,
    retrieve the published pipeline, and submit the pipeline as an experiment to the
    Azure Machine Learning workspace. It''s all configurable and all with only a few
    lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows us how we can integrate a pipeline trigger into an
    Azure pipeline for CI/CD. We can see that once the workspace is initialized, the
    code follows the exact same pattern as if we had submitted the published pipeline
    from our local development environment. In addition, we can configure the pipeline
    run through environment variables and command-line parameters. We will see this
    functionality in action in [*Chapter 16*](B17928_16_ePub.xhtml#_idTextAnchor252),
    *Bringing Models into Production with MLOps*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use and configure Azure Machine Learning
    pipelines for splitting an ML workflow into multiple steps using pipeline and
    pipeline steps for estimators, Python execution, and parallel execution. You configured
    pipeline input and output using `Dataset` and `PipelineData` and managed to control
    the execution flow of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As another milestone, you deployed the pipeline as `PublishedPipeline` to an
    HTTP endpoint. This lets you configure and trigger the pipeline execution with
    a simple HTTP call. Next, you implemented automatic scheduling based on a time
    frequency, as well as a reactive schedule based on changes in the underlying dataset.
    Now, the pipeline can rerun your workflow when the input data changes without
    any manual interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also modularized and versioned a pipeline step so that it can be
    reused in other projects. We used `InputPortDef` and `OutputPortDef` to create
    virtual bindings for data sources and sinks. In the last step, we looked into
    the integration of pipelines into other Azure services, such as Azure Machine
    Learning designer, Azure Data Factory, and Azure DevOps.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into building ML models in Azure using decision
    tree-based ensemble models.
  prefs: []
  type: TYPE_NORMAL
