- en: '*Chapter 8*: Azure Machine Learning Pipelines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：Azure机器学习管道'
- en: In the previous chapter, we learned about advanced preprocessing techniques,
    such as category embeddings and NLP, to extract semantic meaning from text features.
    In this chapter, you will learn how to use these preprocessing and transformation
    techniques to build reusable ML pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了高级预处理技术，如类别嵌入和NLP，从文本特征中提取语义意义。在本章中，你将学习如何使用这些预处理和转换技术来构建可重用的机器学习管道。
- en: First, you will understand the benefits of splitting your code into individual
    steps and wrapping those into a pipeline. Not only can you make your code blocks
    reusable through modularization and parameters, but you can also control the compute
    targets for individual steps. This helps to optimally scale your computations,
    save costs, and improve performance at the same time. Lastly, you can parameterize
    and trigger your pipelines through an HTTP endpoint or through a recurring or
    reactive schedule.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将了解将你的代码分解成单个步骤并将它们包装成管道的好处。不仅可以通过模块化和参数化使你的代码块可重用，而且还可以控制单个步骤的计算目标。这有助于优化你的计算，节省成本，并同时提高性能。最后，你可以通过HTTP端点或通过定期或反应式调度来参数化和触发你的管道。
- en: Then, we will build a complex Azure Machine Learning pipeline in a couple of
    steps. We will start with a simple pipeline, add data inputs, outputs, and connections
    between the steps, and deploy the pipeline as a web service. You will also learn
    about advanced scheduling, based on the frequency and changing data, as well as
    how to parallelize pipeline steps for large data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将分几个步骤构建一个复杂的Azure机器学习管道。我们将从一个简单的管道开始，添加数据输入、输出以及步骤之间的连接，并将管道作为Web服务部署。你还将了解基于频率和变化数据的先进调度，以及如何并行化管道步骤以处理大量数据。
- en: In the last part, you will learn how to integrate Azure Machine Learning pipelines
    into other Azure services such as Azure Machine Learning designer, Azure Data
    Factory, and Azure DevOps. This will help you to understand the commonalities
    and differences between the different pipeline and workflow services and how you
    can trigger ML pipelines.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，你将学习如何将Azure机器学习管道集成到其他Azure服务中，例如Azure机器学习设计器、Azure数据工厂和Azure DevOps。这将帮助你了解不同管道和工作流程服务之间的共性和差异，以及你如何触发机器学习管道。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Using pipelines in ML workflows
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习工作流程中使用管道
- en: Building and publishing an ML pipeline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和发布机器学习管道
- en: Integrating pipelines with other Azure services
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将管道与其他Azure服务集成
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create pipelines and pipeline steps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下Python库和版本来创建管道和管道步骤：
- en: '`azureml-core 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk 1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk 1.34.0`'
- en: Similar to previous chapters, you can run this code using either a local Python
    interpreter or a notebook environment hosted in Azure Machine Learning. However,
    all scripts need to be scheduled to execute in Azure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的章节类似，你可以使用本地Python解释器或托管在Azure机器学习中的笔记本环境运行此代码。然而，所有脚本都需要在Azure中安排执行。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的GitHub存储库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08)。
- en: Using pipelines in ML workflows
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习工作流程中使用管道
- en: Separating your workflow into reusable configurable steps and combining these
    steps into an end-to-end pipeline provides many benefits for implementing end-to-end
    ML processes. Multiple teams can own and iterate on individual steps to improve
    the pipeline over time, while others can easily integrate each version of the
    pipeline into their current setup.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的工作流程分解成可重用和可配置的步骤，并将这些步骤组合成一个端到端管道，为实施端到端机器学习过程提供了许多好处。多个团队可以拥有并迭代单个步骤以改进管道，同时其他人可以轻松地将管道的每个版本集成到他们的当前设置中。
- en: The pipeline itself doesn't only split code from execution; it also splits the
    execution from orchestration. Hence, you can configure individual compute targets
    that can be used to optimize your execution and provide parallel execution while
    you don't have to touch the ML code.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 管道本身不仅将代码与执行分开，还将执行与编排分开。因此，你可以配置单个计算目标，用于优化你的执行并提供并行执行，而你无需接触ML代码。
- en: We will take a quick look at Azure Machine Learning pipelines and why they are
    your tool of choice when implementing ML workflows in Azure. In the following
    section, *Building and publishing an ML pipeline*, we will dive a lot deeper and
    explore the individual features by building such a pipeline.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速浏览Azure机器学习管道，并探讨为什么它们是实现Azure中ML工作流程的首选工具。在下一节“构建和发布ML管道”中，我们将深入探讨通过构建这样一个管道来探索其单个功能。
- en: Why build pipelines?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么构建管道？
- en: As a single developer doing mostly experimentation and working simultaneously
    on data, infrastructure, and modeling, pipelines don't add a ton of benefits to
    the developer's workflow. However, as soon as you perform enterprise-grade development
    across multiple teams that iterate on different parts of the ML system, then you
    will greatly benefit from splitting your code into a pipeline of individual execution
    steps.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个主要进行实验、同时处理数据、基础设施和建模的单个开发者来说，管道并没有给开发者的工作流程带来很多好处。然而，一旦你在多个团队中进行企业级开发，这些团队在不同的ML系统部分进行迭代，那么你将极大地从将代码拆分为单个执行步骤的管道中受益。
- en: This modularization will give you great flexibility, and multiple teams will
    be able to collaborate efficiently. Teams can integrate your models and pipelines
    while you are iterating and building new versions of your pipeline at the same
    time. By using versioned pipelines and pipeline parameters, you can control how
    your data or model service pipeline should be called and ensure auditing and reproducibility.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化将为你带来极大的灵活性，多个团队能够高效协作。当你在迭代和构建管道的新版本时，团队能够集成你的模型和管道。通过使用版本化管道和管道参数，你可以控制如何调用你的数据或模型服务管道，并确保审计和可重复性。
- en: Another important benefit of using workflows instead of running everything inside
    a single file is execution speed and cost improvements. Instead of running a single
    script on the same compute instance, you can run and scale the steps individually
    on different compute targets. This gives you greater control over potential cost
    savings and better optimization for performance, and you only ever have to retry
    the parts of the pipeline that failed and never the whole pipeline.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作流而不是在单个文件中运行所有内容的另一个重要好处是执行速度和成本的改进。你可以在不同的计算目标上单独运行和扩展步骤，而不是在同一个计算实例上运行单个脚本。这让你对潜在的成本节约有更大的控制，并能够更好地优化性能，你只需重试管道中失败的部分，而无需重试整个管道。
- en: Through scheduling pipelines, you can make sure that all your pipeline runs
    are executed without your manual intervention. You simply define triggers, such
    as the existence of new training data, that should execute your pipeline. Decoupling
    your code execution from triggering the execution gives you a ton of benefits,
    such as easy integration into many other services.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调度管道，你可以确保所有管道运行都无需手动干预即可执行。你只需定义触发器，例如新训练数据的存在，以执行你的管道。将代码执行与触发执行解耦为你带来了许多好处，例如轻松集成到许多其他服务中。
- en: Finally, the modularity of your code allows for great reusability. By splitting
    your script into functional steps such as cleaning, preprocessing, feature engineering,
    training, and hyperparameter tuning, you can version and reuse these steps for
    other projects as well.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你代码的模块化特性使得代码具有很高的可重用性。通过将脚本拆分为功能步骤，如清理、预处理、特征工程、训练和超参数调整，你可以为其他项目版本化和重用这些步骤。
- en: Therefore, as soon as you want to benefit from one of these advantages, you
    can start organizing your code in pipelines, which can be deployed, scheduled,
    versioned, scaled, and reused. Let's find out how you can achieve this in Azure
    Machine Learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦你想从这些优势中获益，你就可以开始组织你的代码到管道中，这些管道可以部署、调度、版本化、扩展和重用。让我们看看如何在Azure机器学习中实现这一点。
- en: What are Azure Machine Learning pipelines?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure机器学习管道是什么？
- en: '**Azure Machine Learning pipelines** are workflows of executable steps in Azure
    Machine Learning that compose a complete ML workflow. Hence, you can combine data
    import, data transformations, feature engineering, model training, optimization,
    and also deployment as your pipeline steps.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure 机器学习流水线**是 Azure 机器学习中的可执行步骤工作流程，它构成了完整的机器学习工作流程。因此，您可以将数据导入、数据转换、特征工程、模型训练、优化以及部署作为流水线步骤。'
- en: Pipelines are resources in your Azure Machine Learning workspace that you can
    create, manage, version, trigger, and deploy. They integrate with all other Azure
    Machine Learning workspace resources such as datasets and datastores for loading
    data, compute instances, models, and endpoints. Each pipeline run is executed
    as an experiment on your Azure Machine Learning workspace and gives you the same
    benefits that we covered in the previous chapters, such as tracking files, logs,
    models, artifacts, and images while running on flexible compute clusters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线是您 Azure 机器学习工作空间中的资源，您可以创建、管理、版本控制、触发和部署。它们与所有其他 Azure 机器学习工作空间资源集成，例如用于加载数据的数据集和数据存储、计算实例、模型和端点。每个流水线运行都在您的
    Azure 机器学习工作空间中作为一个实验执行，并为您提供我们在上一章中介绍过的相同好处，例如在灵活的计算集群上运行时跟踪文件、日志、模型、工件和图像。
- en: Azure Machine Learning pipelines should be your first choice when implementing
    flexible and reusable ML workflows. By using pipelines, you can modularize your
    code into blocks of functionality and versions and share those blocks with other
    projects. This makes it easy to collaborate with other teams on complex end-to-end
    ML workflows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现灵活和可重用的机器学习工作流程时，Azure 机器学习流水线应该是您的首选。通过使用流水线，您可以模块化代码为功能块和版本，并将这些块与其他项目共享。这使得与其他团队协作进行复杂的端到端机器学习工作流程变得容易。
- en: Another great integration of Azure Machine Learning pipelines is the integration
    with endpoints and triggers in your workspace. With a single line of code, you
    can publish a pipeline as a web service or web service endpoint and use this endpoint
    to configure and trigger the pipeline from anywhere. This opens the door for integrating
    Azure Machine Learning pipelines with many other Azure and third-party services.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习流水线的另一个伟大集成是与工作空间中的端点和触发器的集成。通过一行代码，您可以将流水线发布为 Web 服务或 Web 服务端点，并使用此端点从任何地方配置和触发流水线。这为将
    Azure 机器学习流水线与其他许多 Azure 和第三方服务集成打开了大门。
- en: However, if you need a more complex trigger, such as continuous scheduling or
    reactive triggering based on changes in the source data, you can easily configure
    this as well. The added benefit of using pipelines is that all orchestration functionality
    is completely decoupled from your training code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您需要一个更复杂的触发器，例如基于源数据变化的持续调度或响应式触发，您也可以轻松地配置这些功能。使用流水线的额外好处是，所有编排功能都与您的训练代码完全解耦。
- en: As you can see, you get a lot of benefits by using Azure Machine Learning pipelines
    for your ML workflows. However, it's worth noting that this functionality does
    come with some extra overhead, namely wrapping each computation in a pipeline
    step, adding pipeline triggers, configuring environments and compute targets for
    each step, and exposing parameters as pipeline options. Let's start by building
    our first pipeline.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，使用 Azure 机器学习流水线为您的机器学习工作流程带来了许多好处。然而，值得注意的是，这项功能确实带来了一些额外的开销，即在每个计算步骤中包装流水线步骤，添加流水线触发器，为每个步骤配置环境和计算目标，以及将参数作为流水线选项公开。让我们从构建我们的第一个流水线开始。
- en: Building and publishing an ML pipeline
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和发布机器学习流水线
- en: Let's go ahead and use all we have learned from the previous chapters and build
    a pipeline for data processing. We will use the Azure Machine Learning SDK for
    Python to define all the pipeline steps as Python code so that it can be easily
    managed, reviewed, and checked into version control as an authoring script.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用之前章节中学到的所有知识，构建一个数据处理流水线。我们将使用 Azure 机器学习 SDK for Python 定义所有流水线步骤为 Python
    代码，以便它可以轻松管理、审查和作为编写脚本存入版本控制。
- en: We will define a pipeline as a linear sequence of pipeline steps. Each step
    will have an input and output defined as pipeline data sinks and sources. Each
    step will be associated with a compute target that defines both the execution
    environment as well as the compute resource for execution. We will set up an execution
    environment as a Docker container with all the required Python libraries and run
    the pipeline steps on a training cluster in Azure Machine Learning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个管道为一系列管道步骤的线性序列。每个步骤都将有一个输入和输出，分别定义为管道数据汇和源。每个步骤都将关联到一个计算目标，该目标定义了执行环境以及执行所需的计算资源。我们将设置一个执行环境，作为一个包含所有必需Python库的Docker容器，并在Azure
    Machine Learning的训练集群上运行管道步骤。
- en: A pipeline runs as an experiment in your Azure Machine Learning workspace. We
    can either submit the pipeline as part of the authoring script, deploy it as a
    web service and hence trigger it through a webhook, schedule it as a published
    pipeline similar to cron jobs, or trigger it from a third-party service such as
    Logic Apps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 管道作为你的Azure Machine Learning工作空间中的一个实验运行。我们可以将管道作为创作脚本的一部分提交，将其部署为Web服务并通过webhook触发，将其作为已发布的管道进行安排，类似于cron作业，或者从第三方服务（如Logic
    Apps）触发。
- en: In many cases, running a linear sequential pipeline is good enough. However,
    when the amount of data increases and pipeline steps become slower and slower,
    we need to find a way to speed up these large computations. A common solution
    for speeding up data transformations, model training, and scoring is through parallelization.
    Hence, we will add a parallel execution step to our data transformation pipeline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，运行线性顺序的管道已经足够好。然而，当数据量增加且管道步骤变得越来越慢时，我们需要找到一种方法来加速这些大型计算。加快数据转换、模型训练和评分的常见解决方案是通过并行化。因此，我们将向我们的数据转换管道添加一个并行执行步骤。
- en: As we learned in the first section of this chapter, one of the main reasons
    for decoupling ML workflows into pipelines is modularity and reusability. By splitting
    a workflow into individual steps, we build the foundation for reusable computational
    blocks for common ML tasks, be it data analysis through visualizations and feature
    importance, feature engineering through NLP and third-party data, or simply the
    scoring of common ML tasks such as automatic image tagging through object detection.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章的第一节中学到的，将ML工作流程解耦到管道中的主要原因是模块化和可重用性。通过将工作流程拆分为单个步骤，我们为常见ML任务的可重用计算块奠定了基础，无论是通过可视化和特征重要性进行数据分析，还是通过NLP和第三方数据进行特征工程，或者简单地评分常见的ML任务，如通过目标检测进行自动图像标记。
- en: In Azure Machine Learning pipelines, we can use modules to create reusable computational
    steps from a pipeline. A module is a management layer on top of a pipeline step
    that allows you to version, deploy, load, and reuse pipeline steps with ease.
    The concept is very similar to to versioning source code or versioning datasets
    in ML projects.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure Machine Learning管道中，我们可以使用模块从管道创建可重用的计算步骤。模块是在管道步骤之上的一个管理层，它允许你轻松地对管道步骤进行版本控制、部署、加载和重用。这个概念与对ML项目中的源代码或数据集进行版本控制非常相似。
- en: For any enterprise-grade ML workflow, the usage of pipelines is essential. Not
    only does it help you decouple, scale, trigger, and reuse individual computational
    steps, but it also provides auditability and monitorability to your end-to-end
    workflow. On top, splitting computational blocks into pipeline steps will set
    you up for a successful transition to MLOps – a **Continuous Integration and Continuous
    Deployment (CI/CD)** process for ML projects.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何企业级ML工作流程，管道的使用是必不可少的。它不仅帮助你解耦、扩展、触发和重用单个计算步骤，而且还为你的端到端工作流程提供了可审计性和可监控性。此外，将计算块拆分为管道步骤将为你成功过渡到MLOps——ML项目的**持续集成和持续部署（CI/CD）**过程打下基础。
- en: Let's get started and implement our first Azure Machine Learning pipeline.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始并实现我们的第一个Azure Machine Learning管道。
- en: Creating a simple pipeline
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个简单的管道
- en: An Azure Machine Learning pipeline is a sequence of individual computational
    steps that can be executed in parallel or a series. Azure Machine Learning provides
    additional features on top of the pipeline, such as visualization of the computational
    graph, data transfer between steps, and publishing pipelines either as an endpoint
    or published pipeline. In this section, we will create a simple pipeline step
    and execute the pipeline to explore the Azure Machine Learning pipeline capabilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习管道是一系列可以并行或顺序执行的独立计算步骤。Azure机器学习在管道之上提供了额外的功能，例如计算图的可视化、步骤之间的数据传输以及将管道作为端点或已发布管道发布。在本节中，我们将创建一个简单的管道步骤并执行管道以探索Azure机器学习管道的功能。
- en: Depending on the type of computation, you can schedule jobs on different compute
    targets such as Azure Machine Learning, Azure Batch, Databricks, Azure Synapse,
    and more, or run *automated ML* or *HyperDrive* experiments. Depending on the
    execution type, you need to provide additional configuration to each step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 根据计算类型，您可以在不同的计算目标上安排作业，例如Azure机器学习、Azure Batch、Databricks、Azure Synapse等，或者运行*自动机器学习*或*HyperDrive*实验。根据执行类型，您需要为每个步骤提供额外的配置。
- en: 'Let''s start with a simple pipeline that consists only of a single step. We
    will incrementally add more functionality and steps in the subsequent sections.
    First, we need to define the type of execution for our pipeline step. While `PipelineStep`
    is the base class for any execution we can run in the pipeline, we need to choose
    one of the step implementations. The following steps are available at the time
    of writing:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从只包含一个步骤的简单管道开始。我们将在后续章节中逐步添加更多功能和步骤。首先，我们需要定义我们的管道步骤的执行类型。虽然`PipelineStep`是管道中可以运行的任何执行的基类，但我们需要选择一个步骤实现。以下是在编写时可用的一些步骤：
- en: '`AutoMLStep`: Runs an automated ML experiment'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AutoMLStep`: 运行一个自动机器学习实验'
- en: '`AzureBatchStep`: Runs a script on Azure Batch'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AzureBatchStep`: 在Azure Batch上运行一个脚本'
- en: '`DatabricksStep`: Runs a Databricks notebook'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DatabricksStep`: 运行一个Databricks笔记本'
- en: '`DataTransferStep`: Transfers data between Azure storage accounts'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataTransferStep`: 在Azure存储账户之间传输数据'
- en: '`HyperDriveStep`: Runs a HyperDrive experiment'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HyperDriveStep`: 运行一个HyperDrive实验'
- en: '`ModuleStep`: Runs a module'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ModuleStep`: 运行一个模块'
- en: '`MpiStep`: Runs an **Message Passing Interface (MPI)** job'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MpiStep`: 运行一个**消息传递接口 (MPI)**作业'
- en: '`ParallelRunStep`: Runs a script in parallel'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParallelRunStep`: 并行运行一个脚本'
- en: '`PythonScriptStep`: Runs a Python script'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PythonScriptStep`: 运行一个Python脚本'
- en: '`RScriptStep`: Runs an R script'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RScriptStep`: 运行一个R脚本'
- en: '`SynapseSparkStep`: Runs a Spark script on Synapse'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SynapseSparkStep`: 在Synapse上运行一个Spark脚本'
- en: '`CommandStep`: Runs a script or command'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CommandStep`: 运行一个脚本或命令'
- en: '`KustoStep`: Runs a Kusto query on Azure Data Explorer'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KustoStep`: 在Azure数据探索器上运行一个Kusto查询'
- en: 'For our simple example, we want to run a single Python data preprocessing script
    in our pipeline, so we''ll choose `PythonScriptStep` from the preceding list.
    We are building on the same examples and code samples that we saw in the previous
    chapters. In this first pipeline, we will execute a single step that will load
    the data directly from the script – and hence doesn''t require any input or output
    to the pipeline step. We will add these separately in the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单示例中，我们想在管道中运行一个单独的Python数据预处理脚本，因此我们将从前面列表中选择`PythonScriptStep`。我们正在构建与我们在前面的章节中看到的相同示例和代码示例。在这个第一个管道中，我们将执行一个步骤，该步骤将直接从脚本中加载数据——因此不需要将任何输入或输出传递给管道步骤。我们将在以下步骤中单独添加这些：
- en: 'The pipeline steps are all attached to an Azure Machine Learning workspace.
    Hence, we start by loading the workspace configuration:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道步骤都附加到Azure机器学习工作区。因此，我们首先加载工作区配置：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need a compute target that we can execute our pipeline step on. Let''s
    create an auto-scaling Azure Machine Learning training cluster as a compute target,
    similar to what we have created in previous chapters:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个计算目标，我们可以在其上执行我们的管道步骤。让我们创建一个自动扩展的Azure机器学习训练集群作为计算目标，类似于我们在前面的章节中创建的：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In addition, we will need a run configuration that defines our training environment
    and Python libraries:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们还需要一个运行配置，它定义了我们的训练环境和Python库：
- en: '[PRE2]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now define `PythonScriptStep`, which provides all the required configuration
    and entry points for a target ML training script:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义`PythonScriptStep`，它为目标的机器学习训练脚本提供了所有必需的配置和入口点：
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the preceding code, we are configuring `script_name` and the
    `source_directory` parameter, which contain the preprocessing script. We also
    pass the `runconfig` runtime configuration and the `compute_target` compute target
    to `PythonScriptStep`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面代码中所见，我们正在配置`script_name`和包含预处理脚本的`source_directory`参数。我们还传递了`runconfig`运行时配置和`compute_target`计算目标到`PythonScriptStep`。
- en: 'If you recall from previous chapters, we previously submitted the `ScriptRunConfig`
    objects as an experiment to the Azure Machine Learning workspace. In the case
    of pipelines, we first need to wrap the pipeline step in `Pipeline` and instead
    submit the pipeline as an experiment. While this seems counterintuitive at first,
    we will see how we can then parametrize the pipeline and add more computational
    steps to it. In the next code snippet, we define the pipeline:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您还记得前面的章节，我们之前将`ScriptRunConfig`对象作为实验提交到Azure机器学习工作区。在管道的情况下，我们首先需要将管道步骤包装在`Pipeline`中，然后将管道作为实验提交。虽然一开始这似乎有些反直觉，但我们将看到我们如何参数化管道并添加更多的计算步骤。在下一个代码片段中，我们定义了管道：
- en: '[PRE4]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, the pipeline is defined simply through a series of pipeline
    steps and linked to a workspace. In our example, we only define a single execution
    step. Let''s also check that we didn''t make any mistakes configuring our pipeline
    through the built-in pipeline validation:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，管道通过一系列管道步骤简单地定义，并与工作区相关联。在我们的例子中，我们只定义了一个执行步骤。让我们也检查一下，我们是否在配置管道时没有犯任何错误，通过内置的管道验证：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the pipeline is validated successfully, we are ready for execution. The
    pipeline can be executed by submitting it as an experiment to the Azure Machine
    Learning workspace:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦管道成功验证，我们就可以准备执行了。可以通过将管道作为实验提交到Azure机器学习工作区来执行管道：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Congratulations! You just ran your first very simple Azure Machine Learning
    pipeline.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您刚刚运行了第一个非常简单的Azure机器学习管道。
- en: Important Note
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'You can find many complete and up-to-date examples for using Azure Machine
    Learning pipelines in the official Azure repository: [https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方Azure存储库中找到许多使用Azure机器学习管道的完整和最新示例：[https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines)。
- en: 'Once a pipeline is submitted, it is shown under the **Pipelines** section as
    well as under the **Experiments** section, as shown in *Figure 8.1*. A pipeline
    is treated as an experiment, where each pipeline run is like an experiment run.
    Each step of a pipeline, as well as its logs, figures, and metrics, can be accessed
    as a child run of the experiment:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提交了管道，它将在**管道**部分以及**实验**部分中显示，如图8.1所示。管道被视为一个实验，其中每个管道运行就像一个实验运行。管道的每个步骤，以及其日志、图表和指标，都可以作为实验的子运行来访问：
- en: '![Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning ](img/B17928_08_01.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – Azure机器学习中的管道运行作为实验](img/B17928_08_01.jpg)'
- en: Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – Azure机器学习中的管道运行作为实验
- en: While this simple pipeline doesn't add a ton of benefits to directly submitting
    the script as an experiment, we can now add additional steps to the pipeline and
    configure data input and output. Let's take a look!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个简单的管道在直接将脚本作为实验提交时并没有带来很多好处，但现在我们可以向管道中添加额外的步骤并配置数据输入和输出。让我们看看吧！
- en: Connecting data inputs and outputs between steps
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在步骤之间连接数据输入和输出
- en: Pipeline steps are computational blocks, whereas the pipeline defines the sequence
    of step executions. In order to control the data flow, we need to define input
    and output for the pipeline as well as wire up data input and output for individual
    steps. The data flow between the computational blocks will ultimately define the
    execution order for the blocks, and hence turns a sequence of steps into a directed
    acyclic execution graph. This is exactly what we are going to explore in this
    section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 管道步骤是计算块，而管道定义了步骤执行的顺序。为了控制数据流，我们需要为管道定义输入和输出，并为单个步骤连接数据输入和输出。计算块之间的数据流最终将定义块的执行顺序，从而将一系列步骤转换为一个有向无环执行图。这正是我们将在本节中探讨的内容。
- en: 'In most cases, a pipeline needs external input, connections between the individual
    blocks, as well as persisted output. In Azure Machine Learning pipelines, we will
    use the following building blocks to configure this data flow:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，管道需要外部输入、各个块之间的连接以及持久化输出。在Azure Machine Learning管道中，我们将使用以下构建块来配置此数据流：
- en: 'Pre-persisted pipeline input: `Dataset`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预持久化管道输入：`Dataset`
- en: 'Data between pipeline steps: `PipelineData`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道步骤之间的数据：`PipelineData`
- en: 'Persisting pipeline output: `PipelineData.as_dataset()`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续管道输出：`PipelineData.as_dataset()`
- en: In this section, we will look at all three types of data input and output. First,
    we will look at how we pass data as input into a pipeline.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看所有三种类型的数据输入和输出。首先，我们将查看如何将数据作为输入传递到管道中。
- en: Input data to pipeline steps
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管道步骤的输入数据
- en: Let's start with adding a data input to the first step in a pipeline. To do
    so – or to pass any pre-persisted data to a pipeline step – we will use a **dataset**,
    which we saw previously in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*. In Azure Machine Learning, a dataset is
    an abstract reference for data stored in a specified path with specified encoding
    on a specified data storage system. The storage system itself is abstracted as
    a **datastore** object, a reference to the physical system with information about
    location, protocol, and access permissions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向管道的第一步添加数据输入开始。为此 – 或者将任何预持久化数据传递给管道步骤 – 我们将使用**数据集**，这在[*第4章*](B17928_04_ePub.xhtml#_idTextAnchor071)，*数据摄取和管理数据集*中我们已经看到。在Azure
    Machine Learning中，数据集是在指定路径上存储的指定编码数据的抽象引用。存储系统本身被抽象为**数据存储**对象，它是物理系统的引用，包含有关位置、协议和访问权限的信息。
- en: 'If you recall from the previous chapters, we can access a dataset that was
    previously registered in our Azure Machine Learning workspace by simply referencing
    it by name:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得前面的章节，我们可以通过简单地按名称引用来访问之前在Azure Machine Learning工作区中注册的数据集：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code is very convenient when your data was initially organized
    and registered as a dataset. As pipeline developers, we don't need to know the
    underlying data format (for example, CSV, ZIP, Parquet, and JSON) and on which
    Azure Blob storage or Azure SQL database the data is stored. Pipeline developers
    can consume the specified data and instead focus on pre-processing, feature engineering,
    and model training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据最初被组织并注册为数据集时，前面的代码非常方便。作为管道开发者，我们不需要知道底层的数据格式（例如，CSV、ZIP、Parquet和JSON），以及数据存储在哪个Azure
    Blob存储或Azure SQL数据库上。管道开发者可以消费指定的数据，并专注于预处理、特征工程和模型训练。
- en: 'However, when passing new data into an Azure Machine Learning pipeline, we
    often don''t have the data registered as datasets. In these cases, we can create
    a new dataset reference. Here is an example of how to create `Dataset` from publicly
    available data:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当将新数据传递到Azure Machine Learning管道时，我们通常没有将数据注册为数据集。在这些情况下，我们可以创建一个新的数据集引用。以下是如何从公开数据创建`Dataset`的示例：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There are multiple ways to transform files and tabular data into `Dataset`.
    While this seems like a bit of complicated extra work instead of passing absolute
    paths to your pipelines directly, you will gain many benefits from following this
    convention. Most importantly, all compute instances in your Azure Machine Learning
    workspace will be able to access, read, and parse the data without any additional
    configuration. In addition, Azure Machine Learning will reference and track the
    dataset used for each experiment run.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件和表格数据转换为`Dataset`有多种方式。虽然这看起来像是额外的复杂工作，而不是直接将绝对路径传递给管道，但遵循此约定将带来许多好处。最重要的是，Azure
    Machine Learning工作区中的所有计算实例都将能够访问、读取和解析数据，而无需任何额外配置。此外，Azure Machine Learning将引用和跟踪每个实验运行所使用的数据集。
- en: 'Once we have obtained a reference to `Dataset`, we can pass the dataset to
    the pipeline step as input. When passing a dataset to the computational step,
    we can configure additional configurations such as the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了`Dataset`的引用，我们就可以将数据集作为输入传递给管道步骤。当将数据集传递给计算步骤时，我们可以配置以下附加配置：
- en: A name for the dataset reference in the script – `as_named_input()`
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在脚本中对数据集引用的名称 – `as_named_input()`
- en: An access type for `FileDataset` – `as_download()` or `as_mount()`
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FileDataset`的访问类型 – `as_download()`或`as_mount()`'
- en: 'First, we configure the tabular dataset as the named input:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将表格数据集配置为命名输入：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will use `PythonScriptStep`, which will allow us to pass arguments
    to the pipeline step. We need to pass the dataset to two parameters – as an argument
    to the pipeline script and as an input dependency for the step. The former will
    allow us to pass the dataset to the Python script, whereas the latter will track
    the dataset as a dependency of this pipeline step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `PythonScriptStep`，这将允许我们将参数传递给管道步骤。我们需要将数据集传递给两个参数——作为管道脚本的参数以及作为步骤的输入依赖。前者将允许我们将数据集传递给Python脚本，而后者将跟踪数据集作为此管道步骤的依赖项：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see in the preceding example, we can pass one (or multiple) datasets
    to the pipeline step as the `inputs` parameter, as well as an argument to the
    script. Using a specific name for this dataset will help us to differentiate between
    multiple inputs in the pipeline. We will update the preprocessing script to parse
    the dataset from the command-line arguments, as shown in the following snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述示例所示，我们可以将一个（或多个）数据集作为`inputs`参数传递给管道步骤，以及作为脚本的参数。为这个数据集指定一个特定的名称将帮助我们区分管道中的多个输入。我们将更新预处理脚本以从命令行参数解析数据集，如下面的代码片段所示：
- en: preprocess_input.py
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: preprocess_input.py
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see in the preceding code, the dataset gets passed as a dataset name
    to the Python script. We can use the `Dataset` API to retrieve the data at runtime.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，数据集作为数据集名称传递给Python脚本。我们可以使用 `Dataset` API在运行时检索数据。
- en: 'Once we submit the pipeline for execution, we can see the pipeline visualized
    in the Azure Machine Learning Studio interface, as shown in *Figure 8.2*. We can
    see that the dataset is passed as the **titanic** named input to the **Preprocessing**
    step:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提交管道以执行，我们可以在Azure Machine Learning Studio界面中看到管道的可视化，如图8.2所示。我们可以看到数据集作为**titanic**命名的输入传递给**预处理**步骤：
- en: '![Figure 8.2 – The dataset as a pipeline step input ](img/B17928_08_02.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 数据集作为管道步骤输入](img/B17928_08_02.jpg)'
- en: Figure 8.2 – The dataset as a pipeline step input
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 数据集作为管道步骤输入
- en: This is a great way to decouple a block of functionality from its input and
    helps you to build reusable blocks. We will see in the subsequent section, *Reusing
    pipeline steps through modularization*, how we can turn these reusable blocks
    into shared modules.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种将功能块与其输入解耦的绝佳方式。我们将在下一节，*通过模块化重用管道步骤*中看到，如何将这些可重用块转换为共享模块。
- en: Important Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Instead of passing datasets as input arguments to the pipeline step, we can
    also access named inputs from the run context using the following property on
    the run context object – `Run.get_context().input_datasets['titanic']`. However,
    setting up datasets as input and output arguments makes it easier to reuse pipeline
    steps and code snippets across pipelines and other experiments.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将数据集作为输入参数传递给管道步骤外，我们还可以使用运行上下文对象上的以下属性从运行上下文中访问命名输入——`Run.get_context().input_datasets['titanic']`。然而，将数据集设置为输入和输出参数可以更容易地在管道和其他实验中重用管道步骤和代码片段。
- en: Next, let's find out how to set up a data flow between individual pipeline steps.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解如何设置单个管道步骤之间的数据流。
- en: Passing data between steps
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤间传递数据
- en: When we define input to a pipeline step, we often want to configure the output
    for the computations. By passing in input and output definitions, we separate
    the pipeline step from predefined data storage and avoid having to move data around
    as part of the computation step.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为管道步骤定义输入时，我们通常希望配置计算输出的输出。通过传递输入和输出定义，我们将管道步骤与预定义的数据存储分离，并避免在计算步骤中移动数据。
- en: 'While pre-persisted inputs were defined as `Dataset` objects, data connections
    (input and output) between pipeline steps are defined through `PipelineData` objects.
    Let''s look at an example of a `PipelineData` object used as output for one pipeline
    step and input for another step:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预先持久化的输入被定义为`Dataset`对象，但管道步骤之间的数据连接（输入和输出）是通过`PipelineData`对象定义的。让我们看看一个`PipelineData`对象作为某个管道步骤的输出和另一个步骤的输入的示例：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similar to the previous example, we pass the dataset as arguments and reference
    them as `outputs`. The former will allow us to retrieve the dataset in the script,
    whereas the latter defines the step dependencies:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个示例类似，我们将数据集作为参数传递，并作为`outputs`引用它们。前者将允许我们在脚本中检索数据集，而后者定义了步骤依赖项：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once we pass the expected output path to the scoring file, we need to parse
    the command-line arguments to retrieve the path. The scoring file looks like the
    following snippet in order to read the output path and output a pandas DataFrame
    to the desired output location. We first need to parse the command-line arguments
    in the training script:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将预期的输出路径传递给评分文件，我们需要解析命令行参数以检索路径。评分文件看起来如下，以便读取输出路径并将pandas DataFrame输出到期望的位置。我们首先需要在训练脚本中解析命令行参数：
- en: preprocess_output.py
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: preprocess_output.py
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `PipelineData` arguments get interpolated at runtime and replaced with
    the local path for the mounted dataset directory. Therefore, we can simply write
    the data to this local directory, and it will be automatically registered in the
    dataset:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`PipelineData`参数在运行时被解释，并用挂载的数据集目录的本地路径替换。因此，我们可以简单地写入这个本地目录，数据将被自动注册到数据集中：'
- en: preprocess_output.py
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: preprocess_output.py
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once we output data to a `PipelineData` dataset, we can pass these datasets
    to the next pipeline step. Passing the datasets works exactly the same as we saw
    in the previous section – we pass them as arguments and register them as `inputs`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据输出到`PipelineData`数据集，我们就可以将这些数据集传递给下一个管道步骤。传递数据集的方式与我们之前看到的完全相同 – 我们将它们作为参数传递并注册为`inputs`：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can load the data in the training script. If you remember from the
    previous step, `PipelineData` is interpolated as paths on the local execution
    environment. Hence, we can read the data from the path that got interpolated in
    the command-line arguments:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在训练脚本中加载数据。如果你还记得前一个步骤，`PipelineData`在本地执行环境中被解释为路径。因此，我们可以从命令行参数中解释的路径读取数据：
- en: train.py
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: train.py
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we can wrap both steps as a `Pipeline` object by passing the steps
    using the pipeline `steps` keyword. The `pipeline` object can be passed as an
    experiment to Azure Machine Learning:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过使用`steps`关键字传递步骤来将这两个步骤包装成一个`Pipeline`对象。这个`pipeline`对象可以被传递给Azure
    Machine Learning作为一个实验：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we can see in the previous example, we can read the output path from the
    command-line arguments and use it in the Python script as a standard file path.
    Hence, we need to make sure that the file path exists and output some tabular
    data into the location. Next, we define the input for the second validation step
    that reads the newly created data:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个示例所示，我们可以从命令行参数中读取输出路径，并在Python脚本中将其用作标准文件路径。因此，我们需要确保文件路径存在，并将一些表格数据输出到该位置。接下来，我们定义第二个验证步骤的输入，该步骤读取新创建的数据：
- en: '![Figure 8.3 – Passing data between pipeline steps ](img/B17928_08_03.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 在管道步骤间传递数据](img/B17928_08_03.jpg)'
- en: Figure 8.3 – Passing data between pipeline steps
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 在管道步骤间传递数据
- en: Finally, we will take a look at how to persist the output of a pipeline step
    for usage outside of the pipeline.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何将管道步骤的输出持久化以供管道外使用。
- en: Persisting data outputs
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久化数据输出
- en: In this last section, we will learn how to persist the output data of a pipeline.
    A common task for pipelines is building data transformations – and hence we often
    expect pipelines to output data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的最后，我们将学习如何持久化管道的输出数据。对于管道来说，一个常见的任务是构建数据转换 – 因此我们通常期望管道输出数据。
- en: In the previous section, we learned about creating outputs from pipeline steps
    with `PipelineData`, mainly to connect these outputs to inputs of subsequent steps.
    We can use the same method to define a final persisted output of a pipeline.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何使用`PipelineData`从管道步骤创建输出，主要是为了将这些输出连接到后续步骤的输入。我们可以使用相同的方法来定义管道的最终持久化输出。
- en: Doing so is very simple once you understood how to create, persist, and version
    datasets. The reason for this is that we can convert a `PipelineData` object into
    a dataset using the `as_dataset()` method. Once we have a reference to the `Dataset`
    object, we can go ahead and either export it to a specific datastore or register
    it as a dataset in the workspace.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了如何创建、持久化和版本化数据集，这样做就非常简单了。原因是我们可以使用`as_dataset()`方法将`PipelineData`对象转换为数据集。一旦我们有了`Dataset`对象的引用，我们就可以继续将其导出到特定的数据存储或在工作区中将其注册为数据集。
- en: 'Here is a snippet of how to convert a `PipelineData` object defined as output
    in a pipeline step to a dataset and register it in the Azure Machine Learning
    workspace:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个如何将作为管道步骤输出的`PipelineData`对象转换为数据集并在Azure Machine Learning工作区中注册的示例：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'By calling the preceding authoring code, you will be able to access the resulting
    predictions as a dataset in any compute instance connected with your workspace:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用前面的作者代码，您将能够将结果预测作为数据集访问任何与您的工作空间连接的计算实例：
- en: '![Figure 8.4 – A dataset as a pipeline step output ](img/B17928_08_04.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 作为管道步骤输出的数据集](img/B17928_08_04.jpg)'
- en: Figure 8.4 – A dataset as a pipeline step output
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 作为管道步骤输出的数据集
- en: Next, we will take a look at the different ways to trigger a pipeline execution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨触发管道执行的不同方法。
- en: Publishing, triggering, and scheduling a pipeline
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布、触发和安排管道
- en: After you have created your first simple pipeline, you have multiple ways of
    running the pipeline. One example that we already saw was submitting the pipeline
    as an experiment to Azure Machine Learning. This would simply execute the pipeline
    from the same authoring script where the pipeline was configured. While this is
    a good start at first to execute a pipeline, there are other ways to trigger,
    parametrize, and execute it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在您创建了第一个简单的管道之后，您有多种运行管道的方式。我们已经看到的一个例子是将管道作为实验提交给Azure Machine Learning。这将简单地从配置管道的同一作者脚本中执行管道。虽然这最初执行管道是一个好的开始，但还有其他触发、参数化和执行管道的方法。
- en: 'Common ways to execute a pipeline are the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 执行管道的常见方法如下：
- en: Publish the pipeline as a web service.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将管道发布为Web服务。
- en: Trigger the published pipeline using a webhook.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用webhook触发已发布的管道。
- en: Schedule the pipeline to run periodically.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排管道定期运行。
- en: In this section, we will look at all three methods to help you trigger and execute
    your pipelines with ease. Let's first start by publishing and versioning your
    pipeline as a web service.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨所有三种方法，以帮助您轻松触发和执行您的管道。让我们首先从将管道作为Web服务发布和版本化开始。
- en: Publishing a pipeline as a web service
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将管道发布为Web服务
- en: A common reason to split an ML workflow into a reusable pipeline is that you
    can parametrize and trigger it for various tasks whenever needed. Good examples
    are common pre-processing tasks, feature engineering steps, and batch scoring.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习工作流程拆分成可重用管道的常见原因是你可以根据需要对其进行参数化和触发，以执行各种任务。好的例子包括常见的预处理任务、特征工程步骤和批量评分。
- en: Hence, turning a pipeline into a parametrizable web service that we can trigger
    from any other application is a great way of deploying our ML workflow. Let's
    get started and wrap and deploy the previously built pipeline as a web service.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将管道转变为可参数化的Web服务，我们可以从任何其他应用程序中触发它，这是一种很好的部署我们的机器学习工作流程的方式。让我们开始，并将之前构建的管道作为Web服务进行打包和部署。
- en: 'As we want our published pipeline to be configurable through HTTP parameters,
    we need to first create these parameter references. Let''s create a parameter
    to control the learning rate of our training pipeline:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望我们的已发布管道可以通过HTTP参数进行配置，我们需要首先创建这些参数引用。让我们创建一个参数来控制训练管道的学习率：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we link the pipeline parameter with the pipeline step by passing it as
    an argument to the training script. We extend the step from the previous section:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过将参数作为训练脚本的参数传递来将管道参数与管道步骤链接起来。我们扩展了上一节中的步骤：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding example, we added the learning rate as a parameter to the
    list of command-line arguments. In the training script, we can parse the command-line
    arguments and read the parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们将学习率作为一个参数添加到了命令行参数列表中。在训练脚本中，我们可以解析命令行参数并读取参数：
- en: score.py
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: score.py
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, the only step left is to publish the pipeline. To do so, we create a pipeline
    and call the `publish()` method. We need to pass a name and version to the pipeline,
    which will now be a versioned published pipeline:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，唯一剩下的步骤就是发布管道。为此，我们创建一个管道并调用`publish()`方法。我们需要为管道传递一个名称和版本，这样它现在将是一个版本化的已发布管道：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: That's all the code you need to expose a pipeline as a parametrized web service
    with authentication. If you want to abstract your published pipeline from a specific
    endpoint – for example, to iterate on the development process of your pipeline
    while letting other teams integrate the web service into their application – you
    can also deploy pipeline webhooks as endpoints.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您需要公开的代码，以将管道作为具有身份验证的参数化Web服务。如果您想将您的已发布管道从特定的端点抽象出来——例如，在迭代管道的开发过程的同时，让其他团队将Web服务集成到他们的应用程序中——您还可以部署管道webhooks作为端点。
- en: 'Let''s look at an example where we take the previously created pipeline service
    and expose it through a separate endpoint:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例，其中我们使用先前创建的管道服务并通过单独的端点公开它：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We have deployed and decoupled the pipeline and the pipeline endpoint. We can
    finally call and trigger the endpoint through the service endpoint. Let's look
    at this in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经部署并解耦了管道和管道端点。我们最终可以通过服务端点调用和触发端点。让我们在下一节中看看这个例子。
- en: Triggering a published pipeline with a webhook
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用webhook触发已发布的管道
- en: 'The published pipeline web service requires authentication. Hence, let''s first
    retrieve an Azure Active Directory token before we call the web service:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 已发布的管道web服务需要认证。因此，在我们调用web服务之前，让我们首先检索一个Azure Active Directory令牌：
- en: '[PRE25]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using the authentication token, we can now trigger and parametrize the pipeline
    by calling the service endpoint. Let''s look at an example using the `requests`
    library. We can configure the learning rate through the `lr_arg` parameter defined
    in the previous section as well as the experiment name by sending a custom JSON
    body. If you recall, the pipeline will still run as an experiment in your Azure
    Machine Learning workspace:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用认证令牌，我们现在可以通过调用服务端点来触发和参数化管道。让我们通过使用`requests`库来查看一个示例。我们可以通过在上一节中定义的`lr_arg`参数配置学习率，并通过发送自定义JSON体来设置实验名称。如果你还记得，管道仍然会在你的Azure机器学习工作区中以实验的形式运行：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can see in the preceding code snippet that we call the pipeline webhook using
    a `POST` request and configure the pipeline run by sending a custom JSON body.
    For authentication, we also need to pass the authentication as an HTTP header.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们可以看到我们使用`POST`请求调用管道webhook，并通过发送自定义JSON体来配置管道运行。对于认证，我们还需要通过HTTP头传递认证信息。
- en: In this example, we used a Python script to trigger the web service endpoint.
    However, you can use any other Azure service for triggering this pipeline now
    through the webhook, such as Azure Logic Apps, CI/CD pipelines in Azure DevOps,
    or any other custom application. If you'd prefer your pipeline to run periodically
    instead of triggering it manually, you can set up a pipeline schedule. Let's take
    a look at this in the next section.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用Python脚本来触发web服务端点。然而，你现在可以通过webhook使用任何其他Azure服务来触发此管道，例如Azure Logic
    Apps、Azure DevOps中的CI/CD管道或任何其他自定义应用程序。如果你希望你的管道定期运行而不是手动触发，你可以设置管道计划。让我们在下一节中看看这个例子。
- en: Scheduling a published pipeline
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安排已发布的管道
- en: Setting up continuous triggers for workflows is a common use case when building
    pipelines. These triggers can run a pipeline and retrain a model every week or
    every day if new data is available. Azure Machine Learning pipelines support two
    types of scheduling techniques – continuous scheduling through a pre-defined frequency,
    and reactive scheduling and data change detection through a polling interval.
    In this section, we will take a look at both approaches.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建管道时，为工作流设置连续触发器是一个常见用例。这些触发器可以在每周或每天运行管道并重新训练模型，如果可用新数据。Azure机器学习管道支持两种类型的调度技术——通过预定义频率的连续调度，以及通过轮询间隔的响应式调度和数据变更检测。在本节中，我们将探讨这两种方法。
- en: 'Before we start scheduling a pipeline, we will first explore a way to list
    all the previously defined pipelines of a workspace. To do so, we can use the
    `PublishedPipeline.list()` method, similar to the `list()` method from our Azure
    Machine Learning workspace resources. Let''s print the name and ID of every published
    pipeline in the workspace:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始安排管道之前，我们将首先探索一种列出工作区中所有先前定义的管道的方法。为此，我们可以使用`PublishedPipeline.list()`方法，类似于我们的Azure机器学习工作区资源中的`list()`方法。让我们打印工作区中每个已发布管道的名称和ID：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: To set up a schedule for a published pipeline, we need to pass the pipeline
    ID as an argument. We can retrieve the desired pipeline ID from the preceding
    code snippet and plug it into the schedule declaration.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 要为已发布的管道设置计划，我们需要将管道ID作为参数传递。我们可以从前面的代码片段中检索所需的管道ID并将其插入到计划声明中。
- en: 'First, we will look at continuous schedules that re-trigger a pipeline with
    a predefined frequency, similar to cron jobs. To define the scheduling frequency,
    we need to create a `ScheduleRecurrence` object. Here is an example snippet to
    create a recurring schedule:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨连续计划，这些计划会以预定义的频率重新触发管道，类似于cron作业。为了定义计划频率，我们需要创建一个`ScheduleRecurrence`对象。以下是一个创建重复计划的示例片段：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code is all you need to set up a recurring schedule that continuously
    triggers your pipeline. The pipeline will run as the defined experiment in your
    Azure Machine Learning workspace. Using the `pipeline_parameters` argument, you
    can pass additional parameters to the pipeline runs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码就是设置一个持续触发你的管道的重复调度的全部所需。管道将在你的Azure Machine Learning工作区中定义的实验中运行。使用`pipeline_parameters`参数，你可以将额外的参数传递给管道运行。
- en: 'Azure Machine Learning pipelines also support another type of recurring scheduling,
    namely polling for changes in a datastore. This type of schedule is referred to
    as a reactive schedule and requires a connection to a datastore. It will trigger
    your pipeline whenever data changes in your datastore. Here is an example of setting
    up a reactive schedule:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning管道还支持另一种类型的重复调度，即轮询数据存储库中的更改。这种类型的调度被称为反应式调度，需要连接到数据存储库。它将在你的数据存储库中的数据更改时触发你的管道。以下是一个设置反应式调度的示例：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see in this example, we set up the reactive schedule using a datastore
    reference and a polling interval in minutes. Hence, the schedule will check each
    polling interval to see which blobs have changed, if any and use those to trigger
    the pipeline. The blob names will be passed to the pipeline using the `data_path_parameter_name`
    parameter. Similar to the previous schedule, you can also send additional parameters
    to the pipeline using the `pipeline_parameters` argument.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如此例所示，我们使用数据存储库引用和分钟级的轮询间隔来设置反应式调度。因此，调度将检查每个轮询间隔，以查看是否有任何blob已更改，并使用这些更改来触发管道。blob名称将通过`data_path_parameter_name`参数传递给管道。类似于之前的调度，你也可以使用`pipeline_parameters`参数将额外的参数发送到管道。
- en: 'Finally, let''s take a look at how to programmatically stop a schedule once
    it is enabled. To do so, we need a reference to the schedule object. We can get
    this, similar to any other resource in Azure Machine Learning, by fetching the
    schedules for a specific workplace:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何一旦启用就程序性地停止一个调度。为此，我们需要一个调度对象的引用。我们可以通过获取特定工作区的调度来获取这个引用，类似于Azure
    Machine Learning中的任何其他资源：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can filter this list using all the available attributes on the schedule
    object. Once we have found the desired schedule, we can simply disable it:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用调度对象上所有可用的属性来过滤这个列表。一旦我们找到了所需的调度，我们只需简单地禁用它：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using the additional `wait_for_provisioning` argument, we ensure that we block
    code execution until the schedule is really disabled. You can easily re-enable
    the schedule using the `Schedule.enable` method. Now, you can create recurring
    and reactive schedules, continuously run your Azure Machine Learning pipelines,
    and disable them if not needed anymore. Next, we will take a look at parallelizing
    execution steps.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用额外的`wait_for_provisioning`参数，我们确保在调度真正禁用之前阻塞代码执行。你可以使用`Schedule.enable`方法轻松重新启用调度。现在，你可以创建重复和反应式调度，持续运行你的Azure
    Machine Learning管道，并在不再需要时禁用它们。接下来，我们将看看如何并行化执行步骤。
- en: Parallelizing steps to speed up large pipelines
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行化步骤以加快大型管道
- en: It's inevitable in many cases that the pipeline will process more and more data
    over time. In order to parallelize a pipeline, you can run pipeline steps in parallel
    or sequence, or parallelize a single pipeline step computation by using `ParallelRunConfig`
    and `ParallelRunStep`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，随着时间的推移，管道不可避免地会处理越来越多的数据。为了并行化管道，你可以并行或顺序运行管道步骤，或者通过使用`ParallelRunConfig`和`ParallelRunStep`来并行化单个管道步骤的计算。
- en: 'Before we jump into parallelizing a single step execution, let''s first discuss
    the control flow of a simple pipeline. We will start with a simple pipeline that
    is constructed using multiple steps, as shown in the following example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论单个步骤执行的并行化之前，让我们首先讨论简单管道的控制流。我们将从一个使用多个步骤构建的简单管道开始，如下面的示例所示：
- en: '[PRE32]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'When we submit this pipeline, how will these four steps be executed – in series,
    in parallel, or even in an undefined order? In order to answer the question, we
    need to look at the definitions of the individual steps. If all steps are independent
    and the compute target for each step is large enough, all steps are executed in
    parallel. However, if we define `PipelineData` as the output of `step1` and input
    it into the other steps, these steps will only be executed after `step1` has finished:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提交此管道时，这四个步骤将如何执行——是顺序执行、并行执行，还是甚至是无定义的顺序？为了回答这个问题，我们需要查看各个步骤的定义。如果所有步骤都是独立的，并且每个步骤的计算目标足够大，则所有步骤都会并行执行。然而，如果我们定义`PipelineData`为`step1`的输出并将其输入到其他步骤中，则这些步骤只有在`step1`完成后才会执行：
- en: '![Figure 8.5 – A pipeline with parallel steps ](img/B17928_08_05.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 带有并行步骤的管道](img/B17928_08_05.jpg)'
- en: Figure 8.5 – A pipeline with parallel steps
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 带有并行步骤的管道
- en: The data connections between the pipeline steps implicitly define the execution
    order of the steps. If no dependencies exist between the steps, all steps are
    scheduled in parallel.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 管道步骤之间的数据连接隐式定义了步骤的执行顺序。如果没有步骤之间存在依赖关系，则所有步骤都会并行安排。
- en: 'There is one exception to the preceding statement, which is enforcing a specific
    execution order of pipeline steps without a dedicated data object as a dependency.
    In order to do this, you can define these dependencies manually, as shown in the
    next code snippet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述说法有一个例外，即在没有专用数据对象作为依赖项的情况下强制执行管道步骤的特定执行顺序。为了做到这一点，你可以手动定义这些依赖项，如下面的代码片段所示：
- en: '[PRE33]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding configuration will first execute `step1` and `step2` in parallel
    before scheduling `step3`, thanks to your explicitly configured dependencies.
    This can be useful when you are accessing state or data in resources outside of
    the Azure Machine Learning workspace; hence, the pipeline cannot implicitly create
    a dependency:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 上述配置将首先并行执行`step1`和`step2`，然后再安排`step3`，这要归功于你明确配置的依赖项。这在当你访问Azure机器学习工作区外的资源中的状态或数据时非常有用；因此，管道不能隐式创建依赖项：
- en: '![Figure 8.6 – A pipeline with a custom step order ](img/B17928_08_06.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 带有自定义步骤顺序的管道](img/B17928_08_06.jpg)'
- en: Figure 8.6 – A pipeline with a custom step order
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 带有自定义步骤顺序的管道
- en: Once we have answered the question of step execution order, we want to learn
    how we can execute a single step in parallel rather than multiple steps. A great
    use case for this is batch scoring a large amount of data. Rather than partitioning
    your input data as input for multiple steps, you want the data as input for a
    single step. However, to speed up the scoring process, you want a parallel execution
    of the scoring for the single step.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们解决了步骤执行顺序的问题，我们想要了解如何并行执行单个步骤，而不是多个步骤。这个用例非常适合批量评分大量数据。你不想将输入数据分割成多个步骤的输入，而是希望将数据作为单个步骤的输入。然而，为了加快评分过程，你希望对单个步骤的评分进行并行执行。
- en: 'In Azure Machine Learning pipelines, you can use a `ParallelRunStep` step to
    configure a parallel execution for a single step. To configure the data partitions
    and parallelization of the computation, you need to create a `ParallelRunConfig`
    object. The parallel run step is a great choice for any type of parallelized computation
    that helps us to split the input data into smaller partitions (also called batches
    or mini-batches) of data. Let''s walk through an example for setting up parallel
    execution for a single pipeline step. We will configure both batch sizes as a
    pipeline parameter that can be set when calling the pipeline step:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习管道中，你可以使用`ParallelRunStep`步骤来配置单个步骤的并行执行。为了配置数据分区和计算的并行化，你需要创建一个`ParallelRunConfig`对象。并行运行步骤是一个很好的选择，适用于任何类型的并行化计算，帮助我们将输入数据分割成更小的数据分区（也称为批量或小批量）。让我们通过一个示例来了解如何为单个管道步骤设置并行执行。我们将配置批量大小作为管道参数，该参数可以在调用管道步骤时设置：
- en: '[PRE34]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding snippet defines the run configuration for parallelizing the computation
    by splitting the input into mini-batches. We configure the batch size as a pipeline
    parameter, `batch_size`. We also configure the compute target and parallelism
    by the `node_count` and `process_count_per_node` parameters. Using these settings,
    we can score four mini-batches in parallel.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段定义了通过将输入分割成小批量来并行化计算的运行配置。我们将批量大小配置为管道参数`batch_size`。我们还通过`node_count`和`process_count_per_node`参数配置计算目标和并行性。使用这些设置，我们可以并行评分四个小批量。
- en: The `score.py` script is a deployment file that needs to contain an `init()`
    and `run(batch)` method. The `batch` argument contains a list of filenames that
    will get extracted from the input argument of the step configuration. We will
    learn more about this file structure in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`score.py` 脚本是一个部署文件，需要包含一个 `init()` 和 `run(batch)` 方法。`batch` 参数包含一个文件名列表，这些文件将从步骤配置的输入参数中提取出来。我们将在
    [*第 11 章*](B17928_11_ePub.xhtml#_idTextAnchor178)，*超参数调整和自动化机器学习* 中了解更多关于此文件结构的信息。'
- en: The `run` method in the `score.py` script should return the scoring results
    or write the data to an external datastore. Depending on this, the `output_action`
    argument needs to be set to either `append_row`, which means that all values will
    be collected as run in a result file, or `summary_only`, which means that the
    user will deal with storing the results. You can define the result file in which
    all the rows will get appended using the `append_row_file_name` argument.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`score.py` 脚本中的 `run` 方法应返回评分结果或将数据写入外部数据存储。根据这一点，`output_action` 参数需要设置为 `append_row`，这意味着所有值都将收集到一个结果文件中，或者设置为
    `summary_only`，这意味着用户将负责存储结果。您可以使用 `append_row_file_name` 参数定义所有行都将附加到的结果文件。'
- en: 'As you can see, setting up the run configuration for a parallel batch execution
    is not very simple and requires a bit of fiddling. However, once set up and configured
    properly, it can be used to scale out a computational step and run many tasks
    in parallel. Hence, we can now define `ParallelRunStep` with all required input
    and output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，设置并行批量执行的运行配置并不简单，需要一些调整。然而，一旦设置并正确配置，它就可以用来扩展计算步骤并并行运行多个任务。因此，我们现在可以定义具有所有必需输入和输出的
    `ParallelRunStep`：
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As you can see, we read from the input dataset that references all files on
    the datastore. We write the results to the `mnist_results` folder in our custom
    datastore. Finally, we can start the run and look at the results. To do so, we
    submit the pipeline as an experiment run to Azure Machine Learning:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们从引用数据存储中所有文件的输入数据集中读取。我们将结果写入我们自定义数据存储中的 `mnist_results` 文件夹。最后，我们可以开始运行并查看结果。为此，我们将管道作为实验运行提交到
    Azure Machine Learning：
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Splitting a step execution into multiple partitions will help you to speed up
    the computation of large amounts of data. It pays off as soon as the time of computation
    is significantly longer than the overhead of scheduling a step execution on a
    compute target. Therefore, `ParallelRunStep` is a great choice for speeding up
    your pipeline, with only a few changes in your pipeline configuration required.
    Next, we will take a look into better modularization and the reusability of pipeline
    steps.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 将步骤执行拆分为多个分区将有助于您加快大量数据的计算速度。一旦计算时间显著长于在计算目标上调度步骤执行的开销，这种方法就会带来回报。因此，`ParallelRunStep`
    是加快您管道的好选择，只需对您的管道配置进行少量更改。接下来，我们将探讨更好的模块化和管道步骤的可重用性。
- en: Reusing pipeline steps through modularization
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过模块化重用管道步骤
- en: By splitting your workflow into pipeline steps, you are laying the foundation
    for reusable ML and data processing building blocks. However, instead of copying
    and pasting your pipelines, pipeline steps, and code into other projects, you
    might want to abstract your functionality into functional high-level modules.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将您的流程拆分为管道步骤，您正在为可重用的 ML 和数据处理构建块奠定基础。然而，而不是将您的管道、管道步骤和代码复制粘贴到其他项目中，您可能希望将您的功能抽象为功能性的高级模块。
- en: Let's look at an example. Suppose you are building a pipeline step that takes
    in a dataset of user and item ratings and outputs a recommendation of the top
    five items for each user. However, while you are fine-tuning the recommendation
    engine, you want to enable your colleagues to integrate the functionality into
    their pipeline. A great way would be to separate the implementation and usage
    of the code, define the input and output data formats, and modularize and version
    it. That's exactly what modules do in the scope of the Azure Machine Learning
    pipeline steps.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设您正在构建一个管道步骤，该步骤接受用户和项目评分的数据集，并为每个用户输出前五个项目的推荐。然而，当您正在微调推荐引擎时，您希望允许您的同事将此功能集成到他们的管道中。一种很好的方法是将代码的实现和使用分离，定义输入和输出数据格式，并对其进行模块化和版本控制。这正是模块在
    Azure Machine Learning 管道步骤中的作用。
- en: 'Let''s create a module, the container that will hold a reference to the computational
    step:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个模块，这个容器将包含对计算步骤的引用：
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we define input and output for the module using the `InputPortDef` and
    `OutputPortDef` bindings. These are input and output references that later need
    to be bound to data references. We use these bindings to abstract all of our input
    and output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`InputPortDef`和`OutputPortDef`绑定来定义模块的输入和输出。这些是输入和输出引用，稍后需要将它们绑定到数据引用。我们使用这些绑定来抽象化所有的输入和输出：
- en: '[PRE38]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we can define the module functionality by publishing a Python script
    for this module:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过发布此模块的Python脚本来定义模块功能：
- en: '[PRE39]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: That's all you need to do to enable others to reuse your recommendation block
    in their Azure Machine Learning pipelines. By using versioning and default versions,
    you can ensure exactly which code is pulled by your users. As we can see, you
    can define multiple inputs and outputs for each module and define configurable
    parameters for this module. In addition to publishing functionality as Python
    code, we can also publish an Azure Data Lake Analytics or Azure batch step.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是您需要做的所有事情，以便其他人可以在他们的Azure机器学习管道中重用您的推荐块。通过使用版本控制和默认版本，您可以确保用户拉取的确切代码。正如我们所看到的，您可以为每个模块定义多个输入和输出，并为该模块定义可配置参数。除了以Python代码发布功能外，我们还可以发布Azure
    Data Lake Analytics或Azure批处理步骤。
- en: 'Next, we will take a look at how the module can be integrated into an Azure
    Machine Learning pipeline and executed together with custom steps. To do so, we
    will first load the module that was previously created using the following command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何将模块集成到Azure机器学习管道中，并与自定义步骤一起执行。为此，我们将首先使用以下命令加载之前创建的模块：
- en: '[PRE40]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, the great thing about this is that the preceding code will work in any
    Python interpreter or execution engine that has access to your Azure Machine Learning
    workspace. This is huge – no copying of code, no need for checking out dependencies,
    and no need for defining any additional access permissions for your application
    – everything is integrated with your workspace.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的伟大之处在于，前面的代码将在任何具有访问您的Azure机器学习工作区权限的Python解释器或执行引擎中工作。这是一个巨大的进步——无需复制代码，无需检查依赖项，也无需为您的应用程序定义任何额外的访问权限——一切都与工作区集成在一起。
- en: 'First, we need to write up the input and output for this pipeline step. Let''s
    pass the input from the pipeline directly to the recommendation module and output
    everything to the pipeline output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要编写这个管道步骤的输入和输出。让我们将管道的输入直接传递到推荐模块，并将所有输出传递到管道输出：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we parametrize the module with the use of pipeline parameters. This lets
    us configure a parameter in the pipeline that we can pass through to the recommendation
    module. In addition, we can define a default parameter for the parameter when
    used in this pipeline:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用管道参数来参数化模块。这使得我们可以在管道中配置一个参数，并将其传递到推荐模块。此外，我们还可以为在管道中使用时定义该参数的默认值：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We already defined the input and output for this pipeline as well as the input
    parameters for the pipeline step. The only thing we are missing is bringing everything
    together and defining a pipeline step. Similar to the previous section, we can
    define a pipeline step that will execute the modularized recommendation block.
    To do so, instead of `PythonScriptStep`, we now use `ModuleStep`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了此管道的输入和输出，以及管道步骤的输入参数。我们唯一缺少的是将所有这些整合起来并定义一个管道步骤。类似于前一个章节，我们可以定义一个将执行模块化推荐块的管道步骤。为此，我们不再使用`PythonScriptStep`，而是现在使用`ModuleStep`：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, we can execute the pipeline by submitting it as an experiment to our
    Azure Machine Learning workspace. This code is very similar to what we saw already
    in the previous section:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过将管道作为实验提交到我们的Azure机器学习工作区来执行管道。此代码与之前章节中看到的内容非常相似：
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding step executes the modularized pipeline as an experiment in your
    Azure Machine Learning workspace. However, you can also choose any of the other
    publishing methods that we discussed in the previous sections, such as publishing
    as a web service or scheduling the pipeline.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个步骤在您的Azure机器学习工作区中以实验的形式执行模块化管道。然而，您也可以选择之前章节中讨论的任何其他发布方法，例如作为Web服务发布或安排管道。
- en: Splitting pipeline steps into reusable modules is extremely helpful when working
    with multiple teams on the same ML projects. All teams can work in parallel, and
    the results can be easily integrated with a single Azure Machine Learning workspace.
    Let's take a look at how Azure Machine Learning pipelines integrate with other
    Azure services.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当与多个团队在同一 ML 项目上工作时，将管道步骤拆分为可重用模块非常有帮助。所有团队都可以并行工作，并且结果可以轻松地集成到单个 Azure 机器学习工作区中。让我们看看
    Azure 机器学习管道如何与其他 Azure 服务集成。
- en: Integrating pipelines with other Azure services
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将管道与其他 Azure 服务集成
- en: It's rare that users only use a single service to manage data flows, experimentation,
    training, deployment, and CI/CD in the cloud. Other services provide specific
    features that make them a better fit for a task, such as Azure Data Factory for
    loading data into Azure and Azure Pipelines for CI/CD for running automated tasks
    in Azure DevOps.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 用户仅使用单个服务来管理云中的数据流、实验、训练、部署和 CI/CD 是很少见的。其他服务提供特定的功能，使它们更适合特定任务，例如，Azure Data
    Factory 用于将数据加载到 Azure 中，Azure Pipelines 用于在 Azure DevOps 中运行自动化任务。
- en: The strongest argument for choosing a cloud provider is the strong integration
    of its individual services. In this section, we will see how Azure Machine Learning
    pipelines integrate with other Azure services. The list for this section would
    be a lot longer if we were to cover every possible service for integration. As
    we learned in this chapter, you can trigger a published pipeline by calling a
    REST endpoint and submitting a pipeline using standard Python code. This means
    that you can integrate pipelines anywhere where you can call HTTP endpoints or
    run Python code.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 选择云提供商的最有力的论据是其个别服务的强大集成。在本节中，我们将看到 Azure 机器学习管道如何与其他 Azure 服务集成。如果我们要涵盖所有可能的集成服务，这个列表将会很长。正如我们在本章中学到的，您可以通过调用
    REST 端点并使用标准 Python 代码提交管道来触发发布的管道。这意味着您可以在任何可以调用 HTTP 端点或运行 Python 代码的地方集成管道。
- en: We will first look into integration with Azure Machine Learning designer. The
    designer lets you build pipelines using graphical blocks, and these pipelines,
    published pipelines, and pipeline runs will show up in the workspace just like
    any other pipeline that we built in this chapter. Therefore, it is practical to
    take a quick look at the commonalities and differences.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将探讨与 Azure 机器学习设计师的集成。设计师允许您通过拖放界面构建管道，这些管道、发布的管道和管道运行将像我们在本章中构建的任何其他管道一样显示在工作区中。因此，快速查看共同点和差异是实用的。
- en: Next, we will take a quick look at integrating Azure Machine Learning pipelines
    with Azure Data Factory, arguably an integration that is used the most. It's a
    very natural instinct to include ML pipelines with ETL pipelines for scoring,
    enriching, or enhancing data during the ETL process.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将快速查看 Azure 机器学习管道与 Azure Data Factory 的集成，这可能是最常用的集成之一。将 ML 管道与 ETL 管道一起包含，用于在
    ETL 过程中对数据进行评分、丰富或增强，这是一种非常自然的本能。
- en: Finally, we will compare Azure Machine Learning pipelines with Azure Pipelines
    for CI/CD in Azure DevOps. While Azure DevOps was used mainly for application
    code and app orchestration, it is now transitioning to provide fully end-to-end
    MLOps workflows. Let's start with the designer and jump right in.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将比较 Azure 机器学习管道与 Azure DevOps 中的 Azure Pipelines 用于 CI/CD。虽然 Azure DevOps
    主要用于应用程序代码和应用程序编排，但它现在正在过渡到提供完整的端到端 MLOps 工作流程。让我们从设计师开始，直接进入正题。
- en: Building pipelines with Azure Machine Learning designer
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Azure 机器学习设计师构建管道
- en: '**Azure Machine Learning designer** is a graphical interface for creating complex
    ML pipelines through a drag and drop interface. You can choose a functionality
    represented as blocks for importing data, which will use a datastore and a dataset
    under the hood.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure 机器学习设计师**是一个图形界面，通过拖放界面创建复杂的 ML 管道。您可以选择表示为块的函数来导入数据，这些块将使用底层的存储库和数据集。'
- en: 'The following figure shows a simple pipeline to train and score a Boosted Decision
    Tree Regression model. As you can see, the block-based programming style requires
    less knowledge about the individual blocks, and it allows you to build complex
    pipelines without writing any code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个简单的管道，用于训练和评分一个提升决策树回归模型。如您所见，基于块的编程风格需要较少的关于单个块的知识，并且它允许您在不编写任何代码的情况下构建复杂的管道：
- en: '![Figure 8.7 – The Azure Machine Learning designer pipeline ](img/B17928_08_07.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – Azure 机器学习设计师管道](img/B17928_08_07.jpg)'
- en: Figure 8.7 – The Azure Machine Learning designer pipeline
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – Azure Machine Learning设计器管道
- en: Some actions, such as connecting the output of one computation to the input
    of the next, are arguably more convenient to create in the visual UI than with
    code. It's also easier to understand the data flow by visualizing the pipeline.
    Other actions, such as creating parallel executions of large data batches, are
    a bit easier to handle and maintain in code. However, due to our code-first approach
    for reproducibility, testability, and version control, we usually prefer code
    for authoring and execution.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作，例如将一个计算的结果连接到下一个计算的输入，在视觉UI中创建可能比使用代码更方便。通过可视化管道也更容易理解数据流。其他操作，例如创建大型数据批次的并行执行，在代码中处理和维护可能更容易一些。然而，由于我们首先使用代码的方法来保证可重复性、可测试性和版本控制，我们通常更倾向于使用代码进行编写和执行。
- en: It's worth noting that the functionality of pipelines in the designer and pipelines
    using code are not the same. While you have a broad set of preconfigured abstract
    functional blocks, such as the **Boosted Decision Tree Regression** block in the
    previous *Figure 8.7*, you can't access these functionalities in code. However,
    you can use scikit-learn, PyTorch, TensorFlow, and so on to reuse an existing
    functionality or build your own in code.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，设计器中的管道和代码中使用的管道的功能并不相同。虽然你有一系列预先配置的抽象功能块，例如之前*图8.7*中的**Boosted Decision
    Tree Regression**块，但你无法在代码中访问这些功能。然而，你可以使用scikit-learn、PyTorch、TensorFlow等来重用现有功能或在代码中构建自己的功能。
- en: Thanks to the first-class integration of the designer into the workspace, you
    can access all of the files, models, and datasets of the workspace from within
    the designer. An important takeaway is that all the resources that are created
    in the workspace such as pipelines, published pipelines, real-time endpoints,
    models, datasets, and so on are stored in a common system – independently of where
    they were created.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设计器与工作空间的一级集成，你可以在设计器内部访问工作空间中的所有文件、模型和数据集。一个重要的启示是，在工作空间中创建的所有资源，如管道、发布的管道、实时端点、模型、数据集等，都存储在公共系统中——无论它们是在哪里创建的。
- en: Azure Machine Learning pipelines in Azure Data Factory
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Data Factory中的Azure Machine Learning管道
- en: When moving data, ETL, and trigger computations in various Azure services, you
    will most likely come across **Azure Data Factory**. It is a very popular service
    to move large amounts of data into Azure, perform processing and transformations,
    build workflows, and trigger many other Azure or third-party services.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动数据、ETL以及触发各种Azure服务中的计算时，你很可能会遇到**Azure Data Factory**。这是一个非常流行的服务，可以将大量数据移动到Azure，执行处理和转换，构建工作流，并触发许多其他Azure或第三方服务。
- en: Azure Machine Learning pipelines integrate very well with Azure Data Factory,
    and you can easily configure and trigger the execution of a published pipeline
    through Data Factory. To do so, you need to drag the **ML Execute Pipeline** activity
    to your Data Factory canvas and specify the pipeline ID of the published pipeline.
    In addition, you can also specify pipeline parameters as well as the experiment
    name for the pipeline run.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning管道与Azure Data Factory集成得非常好，你可以轻松配置并通过Data Factory触发已发布的管道的执行。为此，你需要将**ML
    Execute Pipeline**活动拖放到你的Data Factory画布中，并指定已发布管道的管道ID。此外，你还可以指定管道参数以及管道运行的实验名称。
- en: 'The following figure shows how the **ML Execute Pipeline** step can be configured
    in Azure Data Factory. It uses a linked service to connect to your Azure Machine
    Learning workspace, which allows you to select the desired pipeline from a drop-down
    box:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了如何在Azure Data Factory中配置**ML Execute Pipeline**步骤。它使用链接服务连接到你的Azure Machine
    Learning工作空间，这允许你从下拉框中选择所需的管道：
- en: '![Figure 8.6 – Azure Data Factory with Azure Machine Learning activity ](img/B17928_08_08.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – Azure Data Factory与Azure Machine Learning活动](img/B17928_08_08.jpg)'
- en: Figure 8.6 – Azure Data Factory with Azure Machine Learning activity
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – Azure Data Factory与Azure Machine Learning活动
- en: 'If you are configuring the computational steps using JSON, you can use the
    following snippet to create an **ML Execute Pipeline** activity with Azure Machine
    Learning as a linked service. Again, you must specify the pipeline ID and can
    pass an experiment name, as well as pipeline parameters:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用JSON配置计算步骤，你可以使用以下片段创建一个与Azure Machine Learning作为链接服务的**ML Execute Pipeline**活动。同样，你必须指定管道ID，并可以传递实验名称以及管道参数：
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Finally, you can trigger the step by adding triggers or output into the **ML
    Execute Pipeline** activity. This will finally trigger your published Azure Machine
    Learning pipeline and start the execution in your workspace. This is a great addition
    and makes it easy for other teams to re-use your ML pipelines during classical
    ETL and data transformation processes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以通过添加触发器或输出到**ML 执行管道**活动来触发步骤。这将最终触发您已发布的 Azure Machine Learning 管道，并在工作区中开始执行。这是一个很好的补充，使得其他团队在经典的
    ETL 和数据转换过程中重用您的 ML 管道变得容易。
- en: Azure Pipelines for CI/CD
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure Pipelines for CI/CD
- en: Azure Pipelines is a feature of Azure DevOps that lets you run, build, test,
    and deploy code as a **Continuous Integration** (**CI**) and **Continuous Deployment**
    (**CD**) process. Hence, they are flexible pipelines for code and app orchestration
    with many advanced features, such as approval queues and gated phases.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Pipelines 是 Azure DevOps 的一个功能，允许您作为一个持续集成（**CI**）和持续部署（**CD**）过程来运行、构建、测试和部署代码。因此，它们是具有许多高级功能（如审批队列和门控阶段）的灵活的代码和应用程序编排管道。
- en: By allowing you to run multiple blocks of code, the best way to integrate Azure
    Machine Learning into Azure DevOps is by using Python script blocks. If you have
    followed this book and used a code-first approach to author your experiments and
    pipelines, then this integration is very easy. Let's take a look at a small example.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许您运行多个代码块，将 Azure Machine Learning 集成到 Azure DevOps 的最佳方式是使用 Python 脚本块。如果您已经按照这本书的内容，并使用以代码优先的方法来编写您的实验和管道，那么这种集成非常简单。让我们来看一个小例子。
- en: 'First, let''s write a utility function that returns a published pipeline, given
    a workspace and pipeline ID as parameters. We will need this function in this
    example:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个实用函数，该函数根据工作区和管道 ID 参数返回一个已发布的管道。在这个例子中，我们需要这个函数：
- en: '[PRE46]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we can go ahead and implement a very simple Python script that allows
    us to configure and trigger a pipeline run in Azure. We will initialize the workspace,
    retrieve the published pipeline, and submit the pipeline as an experiment to the
    Azure Machine Learning workspace. It''s all configurable and all with only a few
    lines of code:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以继续实现一个非常简单的 Python 脚本，允许我们在 Azure 中配置和触发管道运行。我们将初始化工作区，检索已发布的管道，并将管道作为实验提交到
    Azure Machine Learning 工作区。这一切都是可配置的，而且只需要几行代码：
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The preceding code shows us how we can integrate a pipeline trigger into an
    Azure pipeline for CI/CD. We can see that once the workspace is initialized, the
    code follows the exact same pattern as if we had submitted the published pipeline
    from our local development environment. In addition, we can configure the pipeline
    run through environment variables and command-line parameters. We will see this
    functionality in action in [*Chapter 16*](B17928_16_ePub.xhtml#_idTextAnchor252),
    *Bringing Models into Production with MLOps*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了我们如何将管道触发器集成到 Azure 管道中进行 CI/CD。我们可以看到，一旦工作区初始化完成，代码将遵循与从本地开发环境提交已发布的管道完全相同的模式。此外，我们还可以通过环境变量和命令行参数来配置管道运行。我们将在[*第
    16 章*](B17928_16_ePub.xhtml#_idTextAnchor252)中看到这一功能的应用，*使用 MLOps 将模型投入生产*。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to use and configure Azure Machine Learning
    pipelines for splitting an ML workflow into multiple steps using pipeline and
    pipeline steps for estimators, Python execution, and parallel execution. You configured
    pipeline input and output using `Dataset` and `PipelineData` and managed to control
    the execution flow of the pipeline.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用和配置 Azure Machine Learning 管道，通过管道和管道步骤将 ML 工作流程拆分为多个步骤，用于估计量、Python
    执行和并行执行。您使用 `Dataset` 和 `PipelineData` 配置了管道输入和输出，并成功控制了管道的执行流程。
- en: As another milestone, you deployed the pipeline as `PublishedPipeline` to an
    HTTP endpoint. This lets you configure and trigger the pipeline execution with
    a simple HTTP call. Next, you implemented automatic scheduling based on a time
    frequency, as well as a reactive schedule based on changes in the underlying dataset.
    Now, the pipeline can rerun your workflow when the input data changes without
    any manual interaction.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个里程碑，您将管道作为 `PublishedPipeline` 部署到了 HTTP 端点。这允许您通过简单的 HTTP 调用来配置和触发管道执行。接下来，您实现了基于时间频率的自动调度，以及基于底层数据集变化的反应式调度。现在，当输入数据发生变化时，管道可以自动重新运行工作流程，无需任何手动交互。
- en: Finally, we also modularized and versioned a pipeline step so that it can be
    reused in other projects. We used `InputPortDef` and `OutputPortDef` to create
    virtual bindings for data sources and sinks. In the last step, we looked into
    the integration of pipelines into other Azure services, such as Azure Machine
    Learning designer, Azure Data Factory, and Azure DevOps.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还模块化和版本化了管道步骤，以便在其他项目中重用。我们使用了`InputPortDef`和`OutputPortDef`来为数据源和汇点创建虚拟绑定。在最后一步，我们探讨了将管道集成到其他Azure服务中，例如Azure机器学习设计器、Azure数据工厂和Azure
    DevOps。
- en: In the next chapter, we will look into building ML models in Azure using decision
    tree-based ensemble models.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨在Azure中使用基于决策树集成模型构建机器学习模型。
