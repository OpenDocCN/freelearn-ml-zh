["```py\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.applications.vgg16 import VGG16\n```", "```py\ndef create_model_fcn32(nb_class, input_w=256):\n \"\"\"\n Create FCN-32s model for segmentaiton. \n Input:\n nb_class: number of detection categories\n input_w: input width, using square image\n\n Returns model created for training. \n \"\"\"\n input = Input(shape=(input_w, input_w, 3))\n\n # initialize feature extractor excuding fully connected layers\n # here we use VGG model, with pre-trained weights. \n vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input)\n # create further network\n x = Conv2D(4096, kernel_size=(7,7), use_bias=False,\n activation='relu', padding=\"same\")(vgg.output)\n x = Dropout(0.5)(x)\n x = Conv2D(4096, kernel_size=(1,1), use_bias=False,\n activation='relu', padding=\"same\")(x)\n x = Dropout(0.5)(x)\n x = Conv2D(nb_class, kernel_size=(1,1), use_bias=False, \n padding=\"same\")(x)\n # upsampling to image size using transposed convolution layer\n x = Conv2DTranspose(nb_class , \n kernel_size=(64,64), \n strides=(32,32), \n use_bias=False, padding='same')(x)\n x = Activation('softmax')(x)\n model = Model(input, x)\n model.summary()\n return model\n\n# Create model for pascal voc image segmentation for 21 classes\nmodel = create_model_fcn32(21)\n```", "```py\n def correlate(self, img):\n \"\"\"\n Correlation of input image with the kernel\n \"\"\"\n\n # get response in fourier domain\n C = cv2.mulSpectrums(cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT), \n self.H, 0, conjB=True)\n\n # compute inverse to get image domain output\n resp = cv2.idft(C, flags=cv2.DFT_SCALE | cv2.DFT_REAL_OUTPUT)\n\n # max location of the response\n h, w = resp.shape\n _, mval, _, (mx, my) = cv2.minMaxLoc(resp)\n side_resp = resp.copy()\n cv2.rectangle(side_resp, (mx-5, my-5), (mx+5, my+5), 0, -1)\n smean, sstd = side_resp.mean(), side_resp.std()\n psr = (mval-smean) / (sstd+eps)\n\n # displacement of max location from center is displacement for  \n        tracker\n return resp, (mx-w//2, my-h//2), psr\n```", "```py\ndef update(self, frame, rate = 0.125):\n # compute current state and window size\n (x, y), (w, h) = self.pos, self.size\n\n # compute and update rectangular area from new frame\n self.last_img = img = cv2.getRectSubPix(frame, (w, h), (x, y))\n\n # pre-process it by normalization\n img = self.preprocess(img)\n\n # apply correlation and compute displacement\n self.last_resp, (dx, dy), self.psr = self.correlate(img)\n\n self.good = self.psr > 8.0\n if not self.good:\n return\n\n # update pos\n self.pos = x+dx, y+dy\n self.last_img = img = cv2.getRectSubPix(frame, (w, h), self.pos)\n img = self.preprocess(img)\n\n A = cv2.dft(img, flags=cv2.DFT_COMPLEX_OUTPUT)\n H1 = cv2.mulSpectrums(self.G, A, 0, conjB=True)\n H2 = cv2.mulSpectrums( A, A, 0, conjB=True)\n\n self.H1 = self.H1 * (1.0-rate) + H1 * rate\n self.H2 = self.H2 * (1.0-rate) + H2 * rate\n self.update_kernel()\n```", "```py\ngit clone https://github.com/nwojke/deep_sort.git\n```", "```py\ncurl -O https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8/download\n```", "```py\nwget https://owncloud.uni-koblenz.de/owncloud/s/f9JB0Jr7f3zzqs8/download\n```", "```py\ncurl -O https://motchallenge.net/data/MOT16.zip\n```", "```py\nwget https://motchallenge.net/data/MOT16.zip\n```", "```py\npython deep_sort_app.py \\\n --sequence_dir=./MOT16/test/MOT16-06 \\\n --detection_file=./deep_sort_data/resources/detections/MOT16_POI_test/MOT16-06.npy \\\n --min_confidence=0.3 \\\n --display=True\n```"]