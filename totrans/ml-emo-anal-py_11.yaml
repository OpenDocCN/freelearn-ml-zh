- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Case Study – The Qatar Blockade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at what happens when we apply one of our classifiers
    to real data that has not been carefully curated, that we don’t have a Gold Standard
    for, and that was not the data that we trained the classifier on. This is a real-life
    situation. You’ve trained a classifier; now, you want to use it. How well do the
    classifiers that we have looked at so far work in this situation? In this chapter,
    we will compare the output of a classifier on data collected over an extended
    period with events in an ongoing news story to see whether changes in the pattern
    of emotions can be linked to developments in the story and whether it is possible
    to detect long-term changes in public attitudes as well as immediate responses
    to key events. This analysis will be divided into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at how specific events give rise to short-term changes in the pattern
    of emotions expressed in tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will investigate whether it is possible to detect long-term changes in public
    attitudes by tracking trends in the way emotions change over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at the proportionality scores for several classifiers over the
    period to see whether proportionality is a good metric – if the proportionality
    scores for all our classifiers show the same trends, then it is likely to be safe
    to use this as a metric when looking for spikes and long-term trends in the emotions
    expressed by tweets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have an appreciation of the extent to which
    emotion analysis tools can tell you interesting things about public attitudes,
    even when the data they have been trained on is not closely aligned with the data
    they are being applied to.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we have seen that it is possible to use machine learning
    techniques to train tools that can assign emotions to tweets. These algorithms
    can perform well on data where one emotion is assigned to each tweet, with comparatively
    little variation in performance on this kind of data, but have more difficulty
    with datasets where zero or more labels can be assigned to a tweet. In the previous
    chapter, we suggested that proportionality might provide a useful metric for assessing
  prefs: []
  type: TYPE_NORMAL
- en: whether a classifier provided a reliable overall view of a set of tweets even
    when there were large numbers of tweets with zero or multiple labels. Using multiple
    classifiers with either LEX or Naive Bayes for the base-level classifiers, and
    using **neutral** as an explicit label, gave acceptable F1 scores and quite high
    proportionality scores for the key datasets.
  prefs: []
  type: TYPE_NORMAL
- en: However, this analysis was carried out on carefully curated datasets, where
    all the tweets were rigorously labeled and the training and test sets were drawn
    from the same overall collection split into sets of 10 separate 90:10 folds. The
    labels were assigned by multiple annotators, with a label assigned to a tweet
    if either all the annotators for that tweet agreed or the majority did. The folds
    were carefully constructed so that the tweets were randomly shuffled before being
    split into training and test sets, with no overlap between the training and test
    sets for a given fold and with every tweet appearing exactly once in a test set.
    Therefore, the results were as reliable as they could be for the relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: They do not, however, tell us how reliable the classifiers will be when applied
    to data other than the given training and test sets. It is almost inevitable that
    data collected using other criteria will have different characteristics. The overall
    distribution of emotions is likely to vary – after all, the obvious application
    of this technology is to analyze how attitudes to some event vary over time, which
    will affect, for instance, the prior probabilities used by Naive Bayes and hence
    is likely to affect the assignment of various labels. Suppose, for instance, that
    10% of the tweets in the training set have the label **anger**, but that on some
    specific day, there is an upswell of anger over some issue and 50% of the tweets
    that day are angry. Consider some tweets containing the words *T1, …, Tn*. The
    formula for Naive Bayes says that *p(anger | T1 & … & Tn) = p(T1 | anger)×...
    × p(Tn | anger) × p(anger)/p(T1)×... × p(Tn)*. The term *p(anger)* here will come
    from the original data, where only 10% of the tweets express anger; but on the
    day in question, 50% do, so this part of the formula will lead to considerable
    underestimates for this emotion. If *T1, … Tn* are words that are linked to **anger**,
    *p(T1), …, P(Tn)* will also probably be underestimates of their frequency on this
    day, which may to some degree compensate for the error in *p(anger)*, but it is
    not reasonable to assume that they will exactly compensate for it.
  prefs: []
  type: TYPE_NORMAL
- en: The other algorithms are likely to suffer from similar issues, though it is
    less easy to see exactly how this will play out for other cases. It is clear,
    however, that performance on test data extracted from one collection cannot be
    assumed to carry over to data collected from another.
  prefs: []
  type: TYPE_NORMAL
- en: It is also not feasible to annotate all the new data to check the accuracy of
    a classifier. The whole point of building a classifier is that we want to collect
    information from new data that we do not have the resources to label by hand –
    if we could afford to label it by hand, then we would almost certainly get better
    results than can be achieved by any classifier since assigning labels to tweets
    is an essentially subjective task.
  prefs: []
  type: TYPE_NORMAL
- en: The case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To investigate the links between emotions expressed in texts and real-world
    events, we needed a set of tweets collected over an extended period, along with
    a picture of the major events that occurred during that period that could reasonably
    be expected to affect public opinion. We had two goals here:'
  prefs: []
  type: TYPE_NORMAL
- en: To see whether emotions expressed in tweets could be matched to events that
    would be likely to affect public opinion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To see whether classification algorithms that perform well on carefully curated
    datasets (where the test sets are drawn from the same collection of tweets as
    the training sets) continue to give good results on brand-new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we carried out a longitudinal test on historic data to see how variations
    in the daily pattern of assignments matched real-world events. This provides a
    test of the utility of the overall strategy. If a classifier detects that labels
    corresponding to some emotion show variations that are explicable in terms of
    real-world events, then it is reasonable to use it for that kind of task, irrespective
    of how it performs on the data that it was originally trained and tested on. A
    classifier that performs well on test data but does not find changes in data collected
    as events unfold is less useful than one that does not do so well on the tests
    but does detect changes corresponding to real-world events.
  prefs: []
  type: TYPE_NORMAL
- en: For our test set, we collected around 160,000 Arabic tweets that originated
    from Qatar between June 2017 and March 2018\. This was a period during which Qatar
    was subject to a blockade by neighboring countries, with decisions made by the
    Qatari governments and those of its neighbors leading to substantial rapid changes
    in public opinion. We applied a version of LEX with a tokenizer and stemmer for
    Arabic and the standard set of 11 emotions (**sadness**, **anger**, **fear**,
    **disgust**, **pessimism**, **love**, **joy**, **anticipation**, **trust**, **surprise**,
    and **optimism**) to this data and looked at how the distribution changed in response
    to these decisions. The results revealed that the residents of Qatar experienced
    an emotional rollercoaster during the first 9 months of the blockade, with opinions
    as expressed in tweets following very rapidly after public events. This provides
    some validation of the use of a classifier trained on a specific dataset as a
    tool for extracting information from data that may be linguistically different
    from the training data (the training data for the classifier was general Gulf
    Arabic, which is not identical to Qatari dialect, even for Modern Standard Arabic)
    and where the distribution of emotions may be quite different from that in the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Short-term changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to look for is correlations between spikes in the various emotions
    and events in the real world. Even if the actual distribution of labels is less
    than 100% accurate, if divergences from the baseline scores can be shown to correspond
    to real-world events, then the tool can be used to track public opinion. Being
    able to tell that people are angrier or more fearful today than they were yesterday
    is probably as useful as knowing that 56.7% of today’s tweets are angry where
    the level is usually 43.6% – messages on social media are, after all, not an accurate
    picture of public opinion in general. Twitter and similar outlets are echo chambers
    that tend to overestimate the strength of feeling in the general population. If
    56.7% of tweets about some topic are angry, about 5% of the general population
    are likely mildly annoyed about it. But if twice as many tweets are angry about
    it today compared to yesterday, then the general population may well be twice
    as angry about it today than it was yesterday.
  prefs: []
  type: TYPE_NORMAL
- en: Due to this, we looked for spikes in positive and negative sentiments and mapped
    them to the most relevant news in Qatar on that day. To identify the most relevant
    news, we looked at Google News (news.google.com) and used Google’s sorting feature
    to identify the most important blockade event of the day based on news relevance.
    We also looked at Al Jazeera’s dedicated page ([https://www.aljazeera.com/news/2018/8/2/qatar-gulf-crisis-all-the-latest-updates](https://www.aljazeera.com/news/2018/8/2/qatar-gulf-crisis-all-the-latest-updates))
    on events related to the blockade to identify the main events of the day. Our
    results showed that the spikes in sentiments that our algorithm identified from
    the collected tweets corresponded to significant blockade-related events, simultaneously
    confirming that the results that were obtained by controlled testing in the SemEval
    experiments transfer effectively to real-world scenarios for at least some classifiers
    and allowing us to probe the way that people reacted to these events.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.1* shows a spike in optimism and a small drop in pessimism on
    August 17, 2017\. The main event from the news outlook was that Saudi Arabia granted
    Qatari pilgrims permission to perform Hajj, which indicates a lessening of tension
    between the two countries. Accordingly, our data showed a spike in **optimism**
    (from 7% of tweets showing optimistic sentiments to 22%) and a decline in **pessimism**
    (from 5% to 2%). Note that while the change from 7% to 22% looks bigger than the
    change from 5% to 2%, they are both roughly threefold changes, which is reasonable
    given that these two are direct opposites. The spike is very short-lived, with
    both emotions returning to their baseline levels after a couple of days. The decline
    in **optimism** is slower than the initial spike, probably reflecting the fact
    that the initial tweets were retweeted and commented on so that the emotions that
    they expressed continue to be present even after the significance of the actual
    event has faded (the **echo** **chamber** effect):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Change of sentiments due to Saudi Arabia allowing pilgrims
    to perform Hajj](img/B18714_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Change of sentiments due to Saudi Arabia allowing pilgrims to
    perform Hajj
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example of a radical change in sentiment (July 3) relates to the change
    of position of the US government concerning Qatar. More specifically, President
    Trump reviewed his earlier position suggesting that Qatar supports terrorism and
    said that Qatar is a partner in fighting terrorism and encouraged GCC unity. Harmoniously,
    the sentiments associated with **joy** spiked from 7% to 20%, while **sadness**
    declined from 4% to 2% (see *Figure 11**.2*). A similar radical change occurred
    on July 9, when the chief prosecutor of the International Criminal Court praised
    Qatar and regretted the actions of the Quartet. Simultaneously joyful tweets increased
    to 14% and sad tweets declined to 2%. Again, the spikes are short-lived and are
    slightly steeper on the way up than on the way down, and the percentage changes
    in the upward and downward directions are similar (joy goes up between two- and
    three-fold, sadness goes down two-fold):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Change of sentiments due to Trump’s review of position and
    the ICC chief prosecutor’s comments](img/B18714_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Change of sentiments due to Trump’s review of position and the
    ICC chief prosecutor’s comments
  prefs: []
  type: TYPE_NORMAL
- en: 'In these cases, the spike in one emotion (**optimism**, **joy**) was matched
    by a proportionally similar drop in the opposite emotion (**pessimism**, **sadness**).
    This does not always happen. If we look at two days when there is a substantial
    spike in fear, we will find that there is also a spike in one of the positive
    emotions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**November 1, 2017**: This day was observed to be the most fearful day for
    the residents of Qatar when 33% of tweets exhibited fearful sentiments (see *Figure
    11**.3*). It was when Bahrain imposed entry visas on Qatar nationals and residents,
    which divided many families that had members in both counties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**November 4, 2017**: This day was observed to be the third most fearful day
    for the residents of Qatar when 28% of tweets exhibited fearful sentiments (see
    *Figures 11.3* and *11.4*). It was when the foreign ministers of Saudi Arabia,
    Bahrain, UAE, and Egypt met in Abu Dhabi and reiterated freezing the membership
    of Qatar at the Gulf Cooperation Council.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 11**.3* shows that Qatari residents expressed **love** simultaneously
    with **fear** on November 1 and 4\. It is hard to be sure about how to interpret
    this, especially given the large spike in **love** on October 28, which corresponds
    to a drop in **fear**. There are several possible explanations: it may be that
    there was some other significant event around this time that independently led
    to an increase in tweets expressing **love**; it may be a consequence of the echo-chamber
    effect, though that seems unlikely since the initial spike on October 28 died
    away by October 31; or it may be that the rise in tweets expressing **fear** leads
    directly to a rise in ones expressing **love** (possibly, in fact, to tweets expressing
    both at the same time) as people contact their loved ones in times of stress.
    Whatever the underlying reason, this figure shows that conflicting emotions can
    interact in surprising ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Fear and love cooccurring](img/B18714_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Fear and love cooccurring
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the spikes in tweets expressing **love** over this period, Qatari
    residents also expressed **joy** at the same time as they expressed the most **fear**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Fear and joy cooccurring](img/B18714_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Fear and joy cooccurring
  prefs: []
  type: TYPE_NORMAL
- en: The spikes in **joy** in *Figure 11**.4* match those in love from the previous
    figure, again including spikes in **joy** alongside the spikes in **fear**. **love**
    continues to increase up to November 7, where **joy** tails off at this point,
    which may again be an echo-chamber effect. What is clear is that, unsurprisingly,
    major real-world events are followed very rapidly by spikes in the emotions expressed
    in tweets, though the emotions that are affected can be more surprising and can
    occur in surprising combinations. Interpreting exactly what is going on may require
    detailed analysis (in the same way that gross measures of accuracy such as F1-measure
    and proportionality may require detailed analysis through finer-grained tools
    such as confusion matrices), but changes in the distribution of emotions in tweets
    do indicate changes in public attitudes.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Significant events, then, produce spikes in the emotions expressed on social
    media. This is hardly surprising, but it is worth confirming that even quite simple
    classification algorithms can detect these spikes and that, sometimes, they occur
    in unexpected combinations. What about long-term trends?
  prefs: []
  type: TYPE_NORMAL
- en: It is harder to interpret these because they do not correspond to easily identifiable
    changes in the real world. The following figure shows general trends over the
    nine months of the blockade. Interpreting these is a challenge – do they track
    the progress of the political situation, or do they reflect a general weary acceptance
    of a situation that has not changed very much?
  prefs: []
  type: TYPE_NORMAL
- en: 'These figures show eight of the emotions in matched pairs (**anticipation**,
    **trust**, and **surprise** tend to have low scores and not to change very much,
    either in response to specific events or over the long term, so we have omitted
    them). The given pairs seem reasonable – disgust and fear seem likely to form
    a matched pair; joy versus sadness, optimism versus pessimism, and anger versus
    love seem likely to behave as opposites – though as we have seen, opposites can
    show spikes at the same time, and it may be that pairing them off differently
    would give us a different overall picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18714_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Adaption over an extended period?
  prefs: []
  type: TYPE_NORMAL
- en: The trend lines in these figures are heavily smoothed – the data, as discussed
    previously, contains very large spikes, and fitting a trend line that obscures
    these may also obscure other gentler changes. Nonetheless, it is worth trying
    to see whether any general lessons can be learned from them.
  prefs: []
  type: TYPE_NORMAL
- en: It does seem that there is a general upward trend in the positive emotions –
    optimism, joy, and love all increase fairly steadily over the period – and a downward
    trend in the negative ones, with anger and pessimism both falling steadily and
    disgust and fear both decreasing slightly. Exactly how to interpret this is unclear.
    It could be that the overall situation was improving over the period covered by
    the dataset and that these trends are simply a reasonable response to the changes
    in the political situation. Alternatively, it could reflect the fact that people
    adjust to whatever situation they find themselves in, so over a period of stress
    and anxiety, their attitude to the situation becomes more relaxed, and positive
    emotions simply become more common.
  prefs: []
  type: TYPE_NORMAL
- en: The most surprising thing in this figure is that joy and sadness both show an
    increase toward the end of the period. It is hard to see why this would be the
    case – it cannot easily be explained in terms of growing adaptation to an unchanging
    situation, but at the same time, it is hard to see what changes in the situation
    could lead to an increase in both joy and sadness. Looking at trends over an extended
    period can be revealing, but it can also be confusing!
  prefs: []
  type: TYPE_NORMAL
- en: Proportionality revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding curves were obtained by running the multi-LEX classifier on the
    date-stamped tweets about the Qatar blockade. In the previous chapter, we looked
    at the notion that proportionality might provide a useful measure of how accurately
    a classifier that assigned a large number of false positives or false negatives
    might nonetheless provide an accurate general picture, even if its accuracy on
    individual tweets was flawed. We can approach this question by plotting the outputs
    of a range of different classifiers on the individual emotions: if these are generally
    similar, then we can at least hope that the places where they show peaks and troughs
    are reasonably reliable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the F1 and proportionality scores for the various classifiers,
    when trained and tested using 10-fold cross-validation on the SEM11-AR datasets,
    were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **DNN** | **LEX** | **MULTI-DNN** | **MULTI-LEX** | **MULTI-NB** | **MULTI-SVM**
    | **NB** | **SVM** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.360 (0.742) | 0.549 (0.940) | 0.415 (0.971) | 0.520 (0.869)
    | 0.543 (0.996) | 0.321 (0.878) | 0.413 (0.770) | 0.379 (0.817) |'
  prefs: []
  type: TYPE_TB
- en: Figure 11.6 – F1 and proportionality for a range of classifiers on SEM11-AR
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we have plotted the proportional scores for various
    emotions obtained by LEX, MULTI-LEX, and MULTI-NB, which are the classifiers with
    the best F1, scores over the period – that is, what proportion of the tweets at
    each date were labeled with the given emotion. Some of the data around the end
    of October 2017 is missing, and the plots are erratic over this period, but in
    general, there is a close correspondence between the plots for these three classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Correlation between proportionality scores](img/B18714_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Correlation between proportionality scores
  prefs: []
  type: TYPE_NORMAL
- en: The other plots for these classifiers are similar, with closely matching shapes,
    albeit sometimes with one lower than the other. It is inevitable that if one classifier
    assigns a higher proportion of tweets to a specific emotion than the others, then
    it will assign a lower proportion to another – MULTI-NB assigns more tweets to
    disgust and pessimism than the other two, and hence it has to assign fewer to
    the other labels. This also explains why all three have troughs for both joy and
    sadness around the beginning of October 2017 – they all have large peaks for disgust
    and pessimism at this point, and there simply is no room for either sadness or
    joy.
  prefs: []
  type: TYPE_NORMAL
- en: For these classifiers to follow each other as closely as they do, they must
    be tracking something similar. Are they tracking the proportion of tweets that
    express each emotion day by day, or are they just tracking the same words? They
    are all lexicon-based, and LEX and MULTI-LEX collect their lexicons in much the
    same way, so it seems very likely that they are tracking words. But the fact that
    we can correlate the major peaks to specific blockade-related events suggests
    that the words they are paying attention to correspond to emotions – that is,
    they are tracking emotions. By looking at the words that are most strongly linked
    to **anger** and **joy** in these classifiers, they do look like words that express
    these emotions (the translations are taken word by word from Google Translate,
    so they might be a bit strange).
  prefs: []
  type: TYPE_NORMAL
- en: '**ANGER**'
  prefs: []
  type: TYPE_NORMAL
- en: MULTI-LEX
  prefs: []
  type: TYPE_NORMAL
- en: الغضب (anger) الكهرب (electrification) المسلمين (Muslims) 😠 (😠) حسبنا (we counted)
    ونعم (and yes) استياء (discontent) خيانة (betrayal) الوكيل (agent)
  prefs: []
  type: TYPE_NORMAL
- en: LEX
  prefs: []
  type: TYPE_NORMAL
- en: الغضب (anger) الكهرب (electrification) 😠 (😠) 😑 (😑) ونعم (and yes) وترهيب (intimidation)
    حقير (despicable) جاب (gap) نرفزة (willies)
  prefs: []
  type: TYPE_NORMAL
- en: MULTI-NB
  prefs: []
  type: TYPE_NORMAL
- en: 😠 (😠) غضب (anger) 😡 (😡) قبحهم (ugliness) نرفزة (willies) الوكيل (agent) ونعم
    (and yes) حانق (grouchy) الدول (countries) غضبي (my anger)
  prefs: []
  type: TYPE_NORMAL
- en: '**JOY**'
  prefs: []
  type: TYPE_NORMAL
- en: MULTI-LEX
  prefs: []
  type: TYPE_NORMAL
- en: 💃 (💃) اعلان (advertisement) 🙈 (🙈) 💪 (💪) 💝 (💝) 💘 (💘) 🎈(🎈) يعكرها (disturb) وسعاده
    (happiness)مبسوط (happy)
  prefs: []
  type: TYPE_NORMAL
- en: LEX
  prefs: []
  type: TYPE_NORMAL
- en: '💝 (💝) #بهجة_أمل (#joy_of_hope) 💃 (💃) الفرحه (joy) #ريح_المدام (#the_wind_of_madam)
    #السعادة (#happiness) مبسوط (happy)💪 (💪)'
  prefs: []
  type: TYPE_NORMAL
- en: MULTI-NB
  prefs: []
  type: TYPE_NORMAL
- en: 'عام (general) #بهجة_أمل (#joy_of_hope) #السعادة (#happiness) 🎈(🎈) اعلان (advertisement)
    سعيد (happy) يعكرها (disturb) 😍 (😍) 💝 (💝)'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several things to note here:'
  prefs: []
  type: TYPE_NORMAL
- en: Emojis appear quite high on the list of emotion markers. This really should
    not be surprising since that is exactly what emojis are for, but it is at least
    interesting to note that they do appear in the top 10 words for both sentiments
    for all the classifiers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of words are shared by the three classifiers, though they are not always
    ranked the same. However, some words are emotion-bearing that are found by one
    but not the others (استياء (discontent), حانق (grouchy)), so they are not just
    finding the same sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some words are not emotion-bearing – المسلمين (Muslims) seems unlikely to be
    a term for expressing anger, but if there are a large number of angry tweets about
    the way that Muslims are treated, then this word will come to be associated with
    anger.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we have several independent ways of checking that classifiers are doing
    what they are supposed to do in addition to re-annotating all the data:'
  prefs: []
  type: TYPE_NORMAL
- en: If several classifiers assign the same peaks and troughs, they are likely to
    be identifying the same patterns in the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If these peaks and troughs can be aligned with external events, then the patterns
    probably do correspond to emotions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the words and emojis that are strongly linked to an emotion include words
    that would be expected to express that emotion, then the outputs probably do correspond
    to the given emotions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Not all classifiers behave as nicely as this. If we add, for instance, SVM
    and simple Naive Bayes to the mix, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Naive Bayes and SVM don’t correlate with the others](img/B18714_11_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Naive Bayes and SVM don’t correlate with the others
  prefs: []
  type: TYPE_NORMAL
- en: What has happened here is that the training of both Naive Bayes and SVM is dominated
    by the first label that is assigned to each tweet. So, if a tweet is assigned
    anger and disgust, it is simply treated as an instance of anger since there is
    no scope for assigning multiple labels **during training** for these classifiers,
    even if it is possible to assign thresholds to allow multiple labels in the test/application
    data. So, we get very high scores for **anger** and very low ones for other negative
    emotions. There are some peaks and troughs in roughly the right places for these
    classifiers, but they are swamped by the fact that they both assign anger to 90%
    or more of all tweets and less than 5% to all the other negative emotions.
  prefs: []
  type: TYPE_NORMAL
- en: A number of the other classifiers (DNN, SVM) simply assign everything to neutral,
    which shows up as a flat line in every plot. This is despite the fact that they
    score moderately well on the test data (which is drawn from the same source as
    the training data, though of course the two are kept apart on each fold). This
    suggests that these classifiers are more easily influenced by properties of the
    training data that are specific to the way it was obtained and presented, rather
    than just to the gross distribution of words. In a sense, what makes them perform
    well in the original experiments – namely that they are sensitive to things other
    than the raw word counts – is exactly what makes them less robust when applied
    to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two main lessons from this case study are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at longitudinal data can be extremely revealing. By plotting the proportion
    of tweets that are assigned different labels and comparing these plots to external
    events and each other, we can obtain information about their reliability as indicators
    of emotions in tweets drawn from entirely new sources. This is not easily available
    if we just look at a single source. When testing with single sources, we obtained
    very high scores for classifiers that did not transfer well to the new data, despite
    the care that we took to carry out 10-fold cross-validation with properly separated
    training and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The very high scores that were obtained by the more sophisticated classifiers
    fell away sharply when they were applied to the new data. It seems likely that
    these classifiers were responding to characteristics of the original data that
    are not replicated in the new material. The simple classifiers, which rely very
    heavily on simple statistics about the probability of a particular word being
    linked to a particular emotion, are more robust since that is all that they are
    sensitive to and that is typically carried over into new text types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s summarize everything we’ve covered so far.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked at the performance of a collection of classifiers
    on a range of datasets, with datasets with varying numbers of emotions, varying
    sizes, and varying kinds of text and, most importantly, with some datasets that
    assigned exactly one label to each tweet and some that allowed zero or more labels
    per tweet. The conclusion at the end of [*Chapter 10*](B18714_10.xhtml#_idTextAnchor193)*,
    Multiclassifier* was that “different tasks require
  prefs: []
  type: TYPE_NORMAL
- en: different classifiers.” This holds even more strongly now that we have tried
    our classifiers on data that does not match the data they were trained on, with
    the DNN and SVM classifiers that performed well on some of the previous datasets
    doing extremely poorly on the case study data.
  prefs: []
  type: TYPE_NORMAL
- en: These two classifiers seem to have assigned **neutral** to almost all the tweets
    in this dataset. This seems likely because the clues that these classifiers are
    sensitive to are missing from, or at any rate rare in, the data, and hence they
    are not assigning any labels. It looks as though these classifiers assign very
    strong weights to specific cues, and if those cues are not present in the data
    that has been collected from different sources or following different guidelines,
    then the classifiers cannot make use of the information that *is* present.
  prefs: []
  type: TYPE_NORMAL
- en: It is, of course, difficult to assess the performance of data for which you
    do not have a Gold Standard. It is infeasible to construct a fresh Gold Standard
    every time you want to try your classifier on a new dataset – the data used in
    this case study, for instance, contains 160K tweets, and annotating all of these
    would be an extremely time-consuming task. Given that we do not have a Gold Standard
    for this data, how can we be sure that DNN and SVM are not correct in deciding
    that virtually none of the tweets in it express any emotion?
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two checks on this:'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that the other classifiers shadow each very closely suggests that they
    are indeed tracking something – it would be beyond coincidence that they all show
    peaks and troughs at the same points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The correlation between the major peaks and troughs and real-world events relating
    to the blockade further suggests that what they are tracking is public opinion.
    Again, it would be beyond coincidence that the largest peaks in **fear** occur
    following events such as the ban on entry visas to Bahrain and the meeting at
    which Qatar’s membership of the Gulf Cooperation Council was frozen, and the largest
    peaks in **joy** follow events such as permission being granted for pilgrims to
    perform Hajj.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, yet again, you can see that you should be wary of assuming that a classifier
    that performs well on some dataset will perform equally well on another. Always,
    always, always try out different classifiers, with different settings, and make
    your own decision about which works best for your task on your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The correlation between the scores for the various classifiers does, however,
    also suggest a further strategy: combine them. There are several ensemble learning
    strategies for using the results of a combination of classifiers – for example,
    by assigning a label if the majority of classifiers in the ensemble recommend
    assigning it, by assigning a label if the sum of the individual scores for that
    label exceeds some threshold, or even by training a new classifier whose input
    is the predictions of the members of the ensemble. Just as there is a wide range
    of classifiers to choose from, there is also a wide range of strategies for combining
    the results of classifiers. So, again, you should try different ensemble strategies
    and don’t believe what people say about their favorites.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Always, always, always, try out different classifiers, with different settings,
    and make your own decision about which works best for your task on* *your data.*'
  prefs: []
  type: TYPE_NORMAL
