- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning, Generative AI, and LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The world is changing. Fast. At the time of writing in mid-2023, **machine learning**
    (**ML**) and **artificial intelligence** (**AI**) have entered the public consciousness
    in a way that even a few months ago seemed impossible. With the rollout of ChatGPT
    in late 2022, as well as a wave of new tools from labs and organizations across
    the world, hundreds of millions of people are now using ML solutions every day
    to create, analyze, and develop. On top of this, innovation seems to only be speeding
    up, with what seems like a new announcement of a record-beating model or new tool
    every day. ChatGPT is only one example of a solution that uses what is now known
    as **generative artificial intelligence** (**generative AI or GenAI**). While
    ChatGPT, Bing AI, and Google Bard are examples of text-based generative AI tools,
    there is also DALL-E and Midjourney in the image space and now a whole suite of
    multi-modal models combining these and other types of data. Given the complexity
    of the ecosystem that is evolving and the models that are being developed by leading
    AI labs around the world, it would be easy to feel overwhelmed. But fear not,
    as this chapter is all about answering the question, “What does this mean for
    me as a budding ML engineer?”
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take the same strategy as in the other chapters of
    the book and focus on the core concepts and on building solid foundations that
    you can use in your own projects for years to come. We will start with the fundamental
    algorithmic approach that has been at the heart of many cutting-edge developments
    in ML since the 2010s with a review of **deep learning**. We will then discuss
    how you can build and host your own deep learning models before moving on to GenAI,
    where we will explore the general landscape before going into a deep dive into
    the approach behind ChatGPT and other powerful text models, **Large Language Models**
    (**LLMs**).
  prefs: []
  type: TYPE_NORMAL
- en: This will then transition smoothly into an exploration of how ML engineering
    and MLOps can be applied to LLMs, including a discussion of the new challenges
    this brings. This is such a new area that much of what we will discuss in this
    chapter will reflect my views and understanding at the time of writing. As an
    ML community, we are only now starting to define what best practice means for
    these models, so we are going to be contributing to this brave new world together
    in the next few pages. I hope you enjoy the ride!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover all of this in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Going deep with deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going big with LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the future with LLMOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going deep with deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have worked with relatively “classical” ML models so far, which
    rely on a variety of different mathematical and statistical approaches to learn
    from data. These algorithms in general are not modeled on any biological theory
    of learning and are at their heart motivated by finding procedures to explicitly
    optimize the loss function in different ways. A slightly different approach that
    the reader will likely be aware of, and that we met briefly in the section on
    *Learning about learning* in *Chapter 3*, *From Model to Model Factory*, is that
    taken by **Artificial Neural Networks** (**ANNs**), which originated in the 1950s
    and were based on idealized models of neuronal activity in the brain. The core
    concept of an ANN is that through connecting relatively simple computational units
    called neurons or nodes (modeled on biological neurons), we can build systems
    that can effectively model any mathematical function (see the information box
    below for more details). The neuron in this case is a small component of the system
    that will return an output based on an input and the transformation of that input
    using some pre-determined mathematical formula. They are inherently non-linear
    and when acting in combination can very quickly begin to model quite complex data.
    Artificial neurons can be thought of as being arranged in layers, where neurons
    from one layer have connections to neurons in the next layer. At the level of
    small neural networks with not many neurons and not many layers, many of the techniques
    we have discussed in this book around retraining and drift detection still apply
    without modification. When we get to ANNs with many layers and neurons, so-called
    **Deep Neural Networks** (**DNNs**), then we have to consider some additional
    concepts, which we will cover in this section.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of neural networks to represent a huge variety of functions has
    a theoretical basis in what are known as **Universal Approximation Theorems**.
    These are rigorous mathematical results that prove that multilayer neural networks
    can approximate classes of mathematical functions to arbitrary levels of precision.
    These results don’t say which specific neural networks will do this, but they
    tell us that with enough hidden neurons or nodes, we can be sure that with enough
    data we should be able to represent our target function. Some of the most important
    results for these theorems were established in the late 1980s in papers like *Hornik,
    K., Stinchcombe, M. and White, H. (1989) “Multilayer feedforward networks are
    universal approximators”, Neural Networks, 2(5), pp. 359–366* and *Cybenko, G.
    (1989) “Approximation by superpositions of a sigmoidal function”, Mathematics
    of Control, Signals, and Systems, 2(4), pp. 303–314*.
  prefs: []
  type: TYPE_NORMAL
- en: DNNs have taken the world by storm in the last few years. From computer vision
    to natural language processing and from StableDiffusion to ChatGPT, there are
    now countless amazing examples of DNNs doing what was once considered the sole
    purview of humans. The in-depth mathematical details of deep learning models are
    covered in so much literature elsewhere, such as in the classic *Deep Learning*
    by Goodfellow, Bengio, Courville, MIT Press, 2016, that we would never be able
    to do them justice here. Although covering the detailed theory is far beyond the
    scope of this chapter, I will attempt to provide an overview of the main concepts
    and techniques you need in order to have a good working knowledge and to be able
    to start using these models in your ML engineering projects.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, ANNs are based on ideas borrowed from biology, and just like in
    a biological brain, the ANN is built up of many individual *neurons*. Neurons
    can be thought of as providing the unit of computation in the ANN. The neuron
    works by taking multiple inputs and then combining them in a specified recipe
    to produce a single output, which can then act as one of the inputs for another
    neuron or as part of the overall model’s output. The inputs to the neurons in
    a biological setting flow along *dendrites* and the outputs are channeled along
    *axons*.
  prefs: []
  type: TYPE_NORMAL
- en: But how do the inputs get transformed into the outputs? There are a few concepts
    we need to bring together to understand this process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Weight**: Assigned to each connection between the neurons in the network
    is a numerical value that can be thought of as the “strength” of the connection.
    During the training of the neural network, the weights are one of the sets of
    values that are altered to minimize the loss. This is in line with the explanation
    of model training provided in *Chapter 3*, *From Model to Model Factory*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias**: Every neuron in the network is given an another parameter that acts
    as an offset to the activation (defined below). This number is also updated during
    training and it gives the neural network more *degrees of freedom* to fit to the
    data. You can think of the bias as shifting the level at which the neuron will
    “fire” (or produce a certain output) and so having this as a variable value means
    there is more adaptability to the neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inputs**: These can be thought of as the raw data points that are fed to
    the neuron before we take into account the weights or the bias. If the neuron
    is being fed features based on the data, then the inputs are the feature values;
    if the neuron is being fed outputs from other neurons, then those are the values
    in that case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation**: The neuron in an ANN receives multiple inputs; the activation
    is the linear combination of the inputs multiplied by the appropriate weights
    plus the bias term. This translates the multiple pieces of incoming data into
    a single number that can then be used to determine what the neuron’s output should
    be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: The activation is just a number, but the activation
    function is how we decide what that number means for the neuron. There is a variety
    of activation functions that are very popular in deep learning today but the important
    characteristic is that when this function acts on the activation value, it produces
    a number that is the output of the neuron or node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These concepts are brought together diagrammatically in *Figure 7.1*. Deep
    learning models do not have a strict definition, but for our purposes, we can
    consider an ANN as deep as soon as it consists of three or more layers. This then
    means we must define some of the important characteristics of these layers, which
    we will do now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This is the first layer of neurons that has as its input the
    raw data or prepared features created from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layers**: These are the layers between the input and output layers,
    and can be thought of as where the main bulk of non-linear transformations of
    the data are performed. This is often simply because there are lots of hidden
    layers with lots of neurons! The way the neurons in the hidden layers are organized
    and connected are key parts of the *neural network architecture*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The output layer is the one responsible for translating the
    outcome of the transformations that have been carried out in the neural network
    into a result that can be interpreted appropriately for the problem at hand. As
    an example, if we are using a neural network to classify an image, we need the
    final layer to output either a 1 or 0 for the specified class or we could have
    it output a series of probabilities for different classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These concepts are useful background, but how do we start working with them
    in Python? The two most popular deep learning frameworks in the world are Tensorflow,
    released by Google Brain in 2015, and PyTorch, released by Meta AI in 2016\. In
    this chapter, we will focus on examples using PyTorch, but many of the concepts
    apply equally well to TensorFlow with some modifications.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: A schematic representation of a “neuron” in an ANN and how this
    takes input data, x, and transforms it into output, y.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, if you haven’t already, install PyTorch. You can do this by following
    the PyTorch documentation at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/),
    for installing locally on Macbook, or use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are some importance concepts and features of the PyTorch API that are
    useful to bear in mind when using PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.Tensor`: Tensors are mathematical objects that can be represented by
    multi-dimensional arrays and are core components of any modern deep learning framework.
    The data we feed into the network should be cast as a tensor, for example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`torch.nn`: This is the main module used to define our neural network models.
    For example, we can use this to define a basic classification neural network containing
    three hidden layers, each with a **Rectified Linear Unit** (**ReLU**) activation
    function. When defining a model in PyTorch using this method, you should also
    write a method called `forward`, which defines how data is passed through the
    network during training. The following code shows how you can build a basic neural
    network inside a class that inherits from the `torch.nn.Module` object. This network
    has four linear layers with ReLU activation functions and a simple forward-pass
    function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Loss functions**: In the `torch.nn` module, there is a series of loss functions
    that can be used for training the network. A popular choice is the cross-entropy
    loss, but there are many more to choose from in the documentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`torch.optim.Optimizer`: This is the base class for all optimizers in PyTorch.
    This allows for the implementation of most of the optimizers discussed in *Chapter
    3*, *From Model to Model Factory*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When defining the optimizer in PyTorch, in most cases, you pass in the instantiated
    model’s parameters and then the relevant parameters for the particular optimizer.
    For example, if we define an Adam optimizer with a learning rate of `0.001`, this
    is as simple as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`torch.autograd`: Recall that training an ML model is really an optimization
    process that leverages a combination of linear algebra, calculus and some statistics.
    PyTorch performs model optimization using *automatic differentiation*, a method
    for converting the problem of finding partial derivatives of a function into the
    application of a series of primitives that is easy to compute but still results
    in calculating the differentiation to good precision. This is not to be confused
    with *finite differences or symbolic differentiation*. You call this implicitly
    by using a loss function and calling the `backward` method, which uses autograd
    to calculate the gradients for the weight updates in each epoch; this is then
    used in the optimizer by calling `optimizer.step()`. During a training run, it
    is important to reset any input tensors as tensors in PyTorch are mutable (operations
    change their data), and it is important to reset any gradients calculated in the
    optimizer as well using `optimizer.zero_grad()`. Given this, an example training
    run with five hundred epochs would look like the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`torch.save and torch.load`: You can probably guess what these methods do from
    their names! But it is still important to show how to save and load your PyTorch
    models. When training deep learning models, it is also important to save the model
    periodically during the training process, as this often takes a long time. This
    is called “checkpointing” and means that you can pick up where you left off if
    anything goes wrong during the training runs. To save a PyTorch checkpoint, we
    can add syntax like the following to the training loop:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To then load in the model, you need to initialize another instance of your
    neural network class and of an optimizer object before reading in their states
    from the `checkpoint` object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`model.eval()`and`model.train()`: Once you have loaded in a PyTorch checkpoint,
    you need to set the model to the appropriate mode for the task you want to perform
    or there may be downstream issues. For example, if you want to perform testing
    and validation or you want to perform inference on new data with your model, then
    you need to call `model.eval()` before using it. This freezes any batch normalization
    or dropout layers you have included as they calculate statistics and perform updates
    during training that you do not want to be active during testing. Similarly, `model.train()`
    ensures these layers are ready to continue performing updates as expected during
    a training run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It should be noted that there is a more extreme setting than `model.eval()`
    where you can entirely turn off any autograd functionality in your context by
    using the following syntax:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This can give you added performance on inference but should only be used if
    you are certain that you do not need any gradient or tensor updates tracked or
    performed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Evaluation**: If you wanted to test the model we have just trained in the
    example above you could calculate an accuracy using something like the syntax
    below, but any of the methods we have discussed in this book for model validation
    apply!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And with that, you can now build, train, save, load and evaluate your first
    PyTorch model. We will now discuss how we take this further by considering some
    of the challenges of taking deep learning models into production.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling and taking deep learning into production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will move on to how to run deep learning models in a production system.
    To do this, we need to consider a few specific points that mark out DNNs from
    the other classical ML algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**They are data-hungry**: DNNs often require relatively large amounts of data
    compared to other ML algorithms, due to the fact that they are performing an extremely
    complex multi-dimensional optimization, with the parameters of each neuron adding
    degrees of freedom. This means that for you to consider training a DNN from scratch,
    you have to do the leg work upfront to make sure you have enough data and that
    it is of a suitable variety to adequately train the model. The data requirements
    also typically mean that you need to be able to store a lot of data in memory
    as well, so this often has to be thought about ahead of time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training is more complex**: This point is related to the above but is subtly
    different. The very complex non-linear optimization problem we are solving means
    that during training, there are often many ways that the model can “get lost”
    and reach a sub-optimal local minimum. Techniques like the *checkpointing* example
    we described in the previous section are widespread in the deep learning community
    as you will often have to stop training at some step when the loss is not going
    in the right direction or stagnating, roll back, and try something different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You have a new choice to make, the model architecture**: DNNs are also very
    different from classical ML algorithms because now you do not just have to worry
    about a few hyperparameters, but you also need to decide the architecture or shape
    of your neural network. This is often a non-trivial exercise and can require detailed
    knowledge of neural networks. Even if you are working with a standard architecture
    like the Transformer architecture (see *Figure 7.2*), you should still have a
    solid grasp of what all the components are doing in order to effectively diagnose
    and resolve any issues. Techniques like automated architecture search as was discussed
    in *Chapter 3* in the section on *Learning about learning* can help speed up architecture
    design but sound foundational knowledge is still important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainability is inherently harder**: A criticism that has been leveled
    at DNNs over the past few years is that their results can be very hard to explain.
    This is to be expected since the point is very much that DNN abstracts away a
    lot of the specifics of any problem into a more abstract approach. This can be
    fine in many scenarios but has now led to several high-profile cases of DNNs exhibiting
    undesired behavior like racial or gender bias, which can then be harder to explain
    and remediate. A challenge also arises in heavily regulated industries, like healthcare
    or finance, where your organization may have a legal duty to be able to evidence
    why a specific decision was made. If you used a DNN to help make this decision,
    this can often be quite challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19525_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: The Transformer architecture as originally published in the paper
    “Attention is all you need” by Google Brain, https://arxiv.org/abs/1706.03762.'
  prefs: []
  type: TYPE_NORMAL
- en: Given all of this, what are some of the main things we should consider when
    using deep learning models for our ML-engineered systems? Well, one of the first
    things you can do is use existing pre-trained models, rather than train your own.
    This obviously comes with some risks around ensuring that the model and the data
    it was fed were of sufficient quality for your application, so always proceed
    with caution and do your due diligence.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, however, this approach is absolutely fine as we may be using
    a model that has been put through its paces in quite a public way and it may be
    known to be performant on the tasks we wish to use it for. Furthermore, we may
    have a use case where we are willing to accept the operational risk of importing
    and using this pre-existing model, contingent on our own testing. Let us assume
    we are in such an example now, and we want to build a basic pipeline to summarize
    some text conversations between clients and employees of a fictional organization.
    We can do this using an off-the-shelf transformer model, like that shown in *Figure
    7.2*, from the Hugging Face `transformers` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'All you need to get started is to know the name of the model you want to download
    from the Hugging Face model server; in this case, we will use the Pegasus text
    summarization model. Hugging Face provides a “`pipeline`" API to wrap around the
    model and make it easy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Performing our first deep learning model inference is then as easy as just
    passing in some inputs to this pipeline. So, for the fictional bot-human interaction
    described above, we can just pass in some example text and see what this returns.
    Let’s do this to summarize a fictional conversation between a customer and a chatbot,
    where the customer is trying to get more information on an order they have placed.
    The conversation is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then feed this conversation into the summarizer `pipeline` object,
    and print the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the model has actually given a good summary of the nature
    of this interaction, highlighting how easy it is to start to do something that
    would probably have been very difficult or even impossible before the deep learning
    revolution.
  prefs: []
  type: TYPE_NORMAL
- en: We have just seen an example of using a pre-trained transformer model to perform
    some specific task, in this case text summarization, without any need for the
    model to be updated based on exposure to new data. In the next section, we will
    explore what to do when you want to update the model based on your own data.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning and transfer learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we showed how easy it was to get started building solutions
    with existing deep learning models if ones could be found that were appropriate
    to your task. A good question to ask ourselves, however, is “What can I do if
    these models are not exactly right for my specific problem?” This is where the
    concepts of **fine-tuning** and **transfer learning** come in. Fine-tuning is
    when we take an existing deep learning model and then continue training the model
    on some new data. This means we are not starting from scratch and so can arrive
    at an optimized network far faster. Transfer learning is when we freeze most of
    the neural network’s state and retrain the last layer(s) with new data in order
    to perform some slightly different task or perform the same task in a way that
    is more appropriate to our problem. In both of these cases, this usually means
    that we can keep many of the powerful features of the original model, such as
    its feature representations, but start to adapt it for our specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: To make this more concrete, we will now walk through an example of transfer
    learning in action. Fine-tuning can follow a similar process but just does not
    involve the adaptations to the neural network that we will implement. We will
    use the Hugging Face `datasets` and `evaluate` packages in this example, which
    will show how we can use a base **Bidirectional Encoder Representations from Transformers**
    (**BERT**) model and then use transfer learning to create a classifier that will
    estimate the star rating of reviews written in English on the Multilingual Amazon
    Reviews Corpus ([https://registry.opendata.aws/amazon-reviews-ml/](https://registry.opendata.aws/amazon-reviews-ml/)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows an example rating from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: This shows an example review and star rating from the Multilingual
    Amazon Reviews Corpus.'
  prefs: []
  type: TYPE_NORMAL
- en: Although we have used the BERT model in the following example, there are many
    variants that will work with the same example, such as DistilBERT or AlBERT, which
    are smaller models which aim to be quicker to train and to retain most of the
    performance of the original BERT model. You can play around with all of these,
    and may even find these models are faster to download due to their reduced size!
  prefs: []
  type: TYPE_NORMAL
- en: 'To start our transfer learning example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can use the `datasets` package to retrieve the dataset. We will use
    the concept of “configurations” and “splits” available for Hugging Face datasets,
    which specify specific subsets of the data and whether you want the train, test,
    or validate splits of the data. For this case, we want the English reviews and
    we will use the train split of the data initially. *Figure 7.3* shows an example
    record from the dataset. The data is retrieved with the following syntax:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to tokenize the dataset. To do this, we will use the `AutoTokenizer`
    that pairs with the BERT model we will use. Before we pull in that specific tokenizer,
    let’s write a function that will use the selected tokenizer to transform the dataset.
    We will also define the logic to take the dataset and make it of the right form
    for use in later PyTorch processes. I have also added an option to downsample
    the data for testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to create the PyTorch `dataloader` for feeding the data into
    the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before we define the logic for training the model, it will be useful to write
    a helper function for defining the learning scheduler and the optimizer for the
    training run. This can then be called in our training function, which we will
    define in the next step. We will use the AdamW optimizer in this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now define the model that we want to train using transfer learning.
    The `transformers` library from Hugging Face provides a very helpful wrapper to
    help you alter the classification head of a neural network based on a core BERT
    model. We instantiate this model and pass in the number of classes, which implicitly
    creates an update to the neural network architecture to give the logits for each
    of the classes upon running a prediction. When running inference, we will then
    take the class corresponding to the maximum of these logits as the inferred class.
    First, let’s define the logic for training the model in a function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can call all of these methods together to grab the tokenizer, pull
    in the dataset, transform it, define the model, configure the learning scheduler
    and optimizer, and finally perform the transfer learning to create the final model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then evaluate the performance of the model on the test split of the
    data using the Hugging Face `evaluate` package or any method we like. Note that
    in the example below, we call `model.eval()` so that the model is in evaluation
    mode as discussed previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will return a dictionary with the value of the calculated metric like
    that shown below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And that is how you can use PyTorch and the Hugging Face `transformers` library
    to perform transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hugging Face `transformers` library also now provides a very powerful Trainer
    API to help you perform fine-tuning in a more abstract way. If we take the same
    tokenizer and model from the previous examples, to use the Trainer API, we can
    just do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the Trainer API, you need to define a `TrainingArguments` object,
    which can include hyperparameters and a few other flags. Let’s just accept all
    of the default values but supply a path for checkpoints to be outputted to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then use the same `evaluate` package we used in the previous example
    to define a function for calculating any specified metrics, which we will pass
    into the main `trainer` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You then define the `trainer` object with all the relevant input objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You train the model with these specified configurations and objects by calling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And that is how you perform your own training on an existing model hosted on
    Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also useful to note that the Trainer API provides a really nice way to
    use a tool like **Optuna**, which we met in *Chapter 3*, *From Model to Model
    Factory*, in order to perform hyperparameter optimization. You can do this by
    specifying an Optuna trial search space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And then defining a function for initializing the neural network during every
    state of the hyperparameter search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You then just need to pass this into the `Trainer` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can then run the hyperparameter search and retrieve the best run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: And that concludes our example of transfer learning and fine-tuning of PyTorch
    deep learning models using the tools from Hugging Face. An important point to
    note is that both fine-tuning and transfer learning are still training processes
    and so can still be applied to the model factory methodology that was laid out
    in *Chapter 3*, *From Model to Model Factory*. For example, when we say “train”
    in the “train-run” process outlined in *Chapter 3*, this may now refer to the
    fine-tuning or transfer learning of a pre-trained deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: As we have covered extensively already, deep learning models can be very powerful
    tools for solving a variety of problems. One of the trends that has been explored
    aggressively in recent years by many groups and organizations is the question
    of what is possible as these models get larger and larger. In the next section,
    we are going to start answering that question by exploring what happens when deep
    learning models get extremely large. It is time to enter the world of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Living it large with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, GPT-4 has been released only a few months previously,
    in March 2023, by OpenAI. This model is potentially the largest ML model ever
    developed, with a reported one trillion parameters, although OpenAI has not confirmed
    the exact number. Since then, Microsoft and Google have announced advanced chat
    capabilities using similarly large models in their product suites and a raft of
    open-source packages and toolkits have been released. All of these solutions leverage
    some of the largest neural network models ever developed, LLMs. LLMs are part
    of an even wider class of models known as **foundation models**, which span not
    just text applications but video and audio as well. These models are roughly classified
    by the author as being too large for most organizations to consider training from
    scratch. This will mean organizations will either consume these models as third-party
    services or host and then fine-tune existing models. Solving this integration
    challenge in a safe and reliable way represents one of the main challenges in
    modern ML engineering. There is no time to lose, as new models and capabilities
    seem to be released every day; so let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main focus of LLM-based systems is to create human-like responses to a wide
    range of text-based inputs. LLMs are based on transformer architectures, which
    we have already met. This enables these models to process input in parallel, significantly
    reducing the amount of time for training on the same volume of data compared to
    other deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of LLMs, as for any transformer, consists of a series of encoders
    and decoders that leverages self-attention and feed-forward neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, you can think of the encoders as being responsible for processing
    the input, transforming it into an appropriate numerical representation, and then
    feeding this into the decoders, from which the output can be generated. The magic
    of transformers comes from the use of **self-attention**, which is a mechanism
    for capturing the contextual relationships between words in a sentence. This results
    in **attention vectors** that represent this numerically, and when multiples of
    these are being calculated, it is called **multi-headed attention**. Both the
    encoder and decoder use self-attention mechanisms to capture the contextual dependencies
    of the input and output sequences.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular transformer-based models used in LLMs is the BERT model.
    BERT was developed by Google and is a pre-trained model that can be fine-tuned
    for various natural language tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular architecture is the **Generative Pre-trained Transformer** (**GPT**),
    created by OpenAI. The ChatGPT system, released by OpenAI in November 2022, apparently
    utilized a third-generation GPT model when it took the world by storm. At the
    time of writing in March 2023, these models are up to their fourth generation
    and are incredibly powerful. Although GPT-4 is still relatively new, it is already
    sparking a heated debate about the future of AI and whether or not we have reached
    **artificial general intelligence** (**AGI**). The author does not believe we
    have, but what an exciting time to be in this space anyway!
  prefs: []
  type: TYPE_NORMAL
- en: The thing that makes LLMs infeasible to train anew in every new business context
    or organization is that they are trained on colossal datasets. GPT-3, which was
    released in 2020, was trained on almost 500 billion tokens of text. A token in
    this instance is a small fragment of a word used for the training and inferences
    process in LLMs, roughly around 4 characters in English. That is a lot of text!
    The costs for training these models are therefore concomitantly large and even
    inference can be hugely costly. This means that organizations whose sole focus
    is not producing these models will likely fail to see the economies of scale and
    the returns required to justify investing in them at this scale. This is before
    you even consider the need for specialized skill sets, optimized infrastructure,
    and the ability to grab all of that data. There are a lot of parallels with the
    advent of the public cloud several years ago, where organizations no longer had
    to invest in as much on-premises infrastructure or expertise and instead started
    to pay on a “what you use” basis. The same thing is now happening with the most
    sophisticated ML models. This is not to say that smaller, more domain-specific
    models have been ruled out. In fact, I think that this will remain one of the
    ways that organizations can leverage their own unique datasets to drive advantage
    over competitors and build out better products. The most successful teams will
    be those that combine this approach with the approach from the largest models
    in a robust way.
  prefs: []
  type: TYPE_NORMAL
- en: Scale is not the only important component though. ChatGPT and GPT-4 were not
    only trained on huge amounts of data but they were also then fine-tuned using
    a technique called **Reinforcement Learning with Human Feedback** (**RLHF**).
    During this process, the model is presented with a prompt, such as a conversational
    question, and generates a series of potential responses. The responses are then
    presented to a human evaluator who provides feedback on the quality of the response,
    usually by ranking them, which is then used to train a **reward model**. This
    model is then used to fine-tune the underlying language model through techniques
    like **Proximal Policy Optimization** (**PPO**). The details of all of this are
    well beyond the scope of this book but hopefully, you are gaining an intuition
    for how this is not run-of-the-mill data science that any team can quickly scale
    up. And since this is the case, we have to learn how to work with these tools
    as more of a “black box” and consume them as third-party solutions. We will cover
    this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming LLMs via API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous sections, the main change in our way of thinking
    as ML engineers who want to interact with LLMs, and foundation models in general,
    is that we can no longer assume we have access to the model artifact, the training
    data, or testing data. We have to instead treat the model as a third-party service
    that we should call out to for consumption. Luckily, there are many tools and
    techniques for implementing this.
  prefs: []
  type: TYPE_NORMAL
- en: The next example will show you how to build a pipeline that leverages LLMs by
    using the popular **LangChain** package. The name comes from the fact that to
    leverage the power of LLMs, we often have to chain together many interactions
    with them with calls to other systems and sources of information. LangChain also
    provides a wide variety of functionality that is useful when dealing with NLP
    and text-based applications more generally. For example, there are utilities for
    text splitting, working with vector databases, document loading and retrieval,
    and conversational state persistence, among others. This makes it a worthwhile
    package to check out even in projects where you are not necessarily working with
    LLMs specifically.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we walk through a basic example of calling the OpenAI API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `langchain` and `openai` Python bindings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We assume the user has set up an OpenAI account and has access to an API key.
    You can set this as an environment variable or use a secrets manager for storage,
    like the one that GitHub provides. We will assume the key is accessible as an
    environment variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, in our Python script or module, we can define the model we will call using
    the OpenAI API as accessed via the `langchain` wrapper. Here we will work with
    the `gpt-3.5-turbo` model, which is the most advanced of the GPT-3.5 chat models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'LangChain then facilitates the building up of pipelines using LLMs via prompt
    templates, which allow you to standardize how we will prompt and parse the response
    of the models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then create our first “chain,” which is the mechanism for pulling together
    related steps in `langchain`. This first chain is a simple one that takes a prompt
    template and the input to create an appropriate prompt to the LLM API, before
    returning an appropriately formatted response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then run this question and print the result to the terminal as a test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Given that the I am an ML engineer employed by a large bank and am based in
    Glasgow, United Kingdom, you can see that even the most sophisticated LLMs will
    get things wrong. This is an example of what we term a *hallucination*, where
    an LLM gives an incorrect but plausible answer. We will return to the topic of
    when LLMs get things wrong in the section on Building the future with *LLMOps*.
    This is still a good example of building a basic mechanism through which we can
    programmatically interact with LLMs in a standardized way.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain also provides the ability to pull multiple prompts together using
    a method in the chain called `generate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The response from this series of questions is rather verbose, but here is the
    first element of the returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Again, not *quite* right. You get the idea though! With some prompt engineering
    and better conversation design, this could quite easily be a lot better. I’ll
    leave you to play around and have some fun with it.
  prefs: []
  type: TYPE_NORMAL
- en: This quick introduction to LangChain and LLMs only scratches the surface, but
    hopefully gives you enough to fold in calls to these models into your ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to discuss another important way that LLMs are becoming part of
    the ML engineering toolkit, as we explore software development using AI assistants.
  prefs: []
  type: TYPE_NORMAL
- en: Coding with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are not only useful for creating and analyzing natural language; they can
    also be applied to programming languages. This is the purpose of the OpenAI Codex
    family of models, which has been trained on millions of code repositories with
    the aim of being able to produce reasonable-looking and performing code when prompted.
    Since GitHub Copilot, an AI assistant for coding, was launched, the concept of
    an AI assistant helping you code has entered the mainstream. Many people have
    argued that these solutions provide massive productivity boosts and improved enjoyment
    when executing their own work. GitHub has published some of its own research suggesting
    that 60-75% of 2,000 developers asked reported less frustration and improved satisfaction
    when developing software. On a far smaller cohort of 95 developers, with 50 being
    in the control group, they also showed a speedup when developing an HTTP server
    in JavaScript with a given specification. I believe there should be much more
    work done on this topic before we declare that AI coding assistants are obviously
    making us all happier and faster, but the GitHub survey and test results definitely
    suggest they are a useful tool to try out. These results are published at [https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/](https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/).
    To this previous point, an interesting pre-print paper on the arXiv by researchers
    from Stanford University, `arXiv:2211.03622` **[cs.CR]**, seems to show that developers
    using an AI coding assistant based on the OpenAI `codex-davinci-002` model were
    more likely to introduce security vulnerabilities into their code and that users
    of the model would feel more confident in their work even though it contained
    these issues! It should be noted that the model they used is relatively old in
    terms of the family of LLMs that OpenAI offers now, so again more research is
    needed. This does raise the interesting possibility that AI coding assistants
    may provide a speed boost but also introduce more bugs. Time will tell. This area
    has also started to heat up with the introduction of powerful open-source contenders.
    A particular one to call out is StarCoder, which was developed through a collaboration
    between Hugging Face and ServiceNow [https://huggingface.co/blog/starcoder](https://huggingface.co/blog/starcoder).
    The one thing that is certain is that these assistants are not going anywhere
    and they are only going to improve with time. In this section, we will start to
    explore the possibilities of working with these AI assistants in a variety of
    guises. Learning to work with AI is likely going to be a critical part of the
    ML engineering workflow in the near future, so let’s get learning!
  prefs: []
  type: TYPE_NORMAL
- en: First, when would I want to use an AI coding assistant as an ML engineer? The
    consensus in the community, and in the GitHub research, looks to be that these
    assistants help with the development of boilerplate code on established languages,
    Python among them. They do not seem to be ideal for when you want to do something
    particularly innovative or different; however, we will explore this also.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do you actually work with an AI to help you code? At the time of writing,
    there seem to be two main methods (but given the pace of innovation, it could
    be that you are working with an AI through a brain-computer interface in no time;
    who knows?), each of which has its own advantages and disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Direct editor or IDE integration**: In Copilot-supported code editors and
    IDEs, which at the time of writing include the PyCharm and VS Code environments
    we have used in this book, then you can enable Copilot to offer autocomplete suggestions
    for your code as you type it. You can also provide prompt information for the
    LLM model in your comments in the code. This mode of integration will likely always
    be there as long as developers are using these environments, but I can foresee
    a large number of AI assistant offerings in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chat interface**: If you are not using Copilot but instead another LLM, for
    example, OpenAI’s GPT-4, then you will likely need to work in some sort of chat
    interface and copy and paste relevant information between your coding environment
    and the chat. This may seem a bit more clunky but is definitely far more versatile
    as it means you can easily switch between the models of your choice, or even combine
    multiples. You could actually build your own code to feed in your code with these
    models if you have the relevant access permissions and APIs to hit, but at that
    point, you are just redeveloping a tool like Copilot!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will walk through an example showing both and highlighting how this may potentially
    help you in your future ML engineering projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you navigate to the GitHub Copilot web page, you can sign up for an individual
    subscription for a monthly fee and they offer a free trial. Once you have done
    that, you can follow the setup instructions for your chosen code editor here:
    [https://docs.github.com/en/copilot/getting-started-with-github-copilot](https://docs.github.com/en/copilot/getting-started-with-github-copilot).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have this setup, as I have done for VS Code, then you can start to
    use Copilot straight away. For example, I opened a new Python file and started
    typing some typical imports. When I started to write my first function, Copilot
    kicked in with a suggestion to complete the entire function, as shown in *Figure
    7.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: A suggested autocompletion from GitHub Copilot in VS Code.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned above, this is not the only way you can provide input to Copilot;
    you can also use comments to provide more information to the model. In *Figure
    7.5*, we can see that providing some commentary in a leading comment line helps
    define the logic we want to be contained in the function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: By providing a leading comment, you can help Copilot suggest the
    logic you want for your code.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few things that help get the best out of Copilot that are useful
    to bear in mind as you use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be very modular**: The more modular you can make your code, the better. We’ve
    already discussed why this benefits maintenance and quicker development, but here
    it also helps the Codex model create more appropriate auto-completion suggestions.
    If your functions are going to be long, complex objects, then the likelihood is
    that the Copilot suggestion wouldn’t be great.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write clear comments**: This is always a good practice of course but it really
    helps Copilot understand what code you need. It can help to write longer comments
    at the top of the file describing what you want the solution to do and then write
    shorter but very precise comments before your functions. The example in *Figure
    7.5* shows a comment that specifies the way in which I wanted the function to
    perform the feature preparation, but if the comment simply said “*standardize
    features*,” the suggestion would likely not be as complete.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Write the interfaces and function signatures**: As in *Figure 7.5*, it helps
    if you start the piece of code off by providing the function signature and the
    types or the first line of the class definition if this was a class. This acts
    to prime the model to complete the rest of the code block.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, this is enough to get you started on your journey working with AI
    to build your solutions. I think that as these tools become more ubiquitous, there
    are going to be many opportunities to use them to supercharge your development
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to build some pipelines using LLMs and we know how to start
    leveraging them to aid our own development, we can turn to what I think is one
    of the most important topics in this field. I also think it is the one with the
    most unanswered questions and so it is a very exciting one to explore. This all
    relates to the question of the operational implications of leveraging LLMs, now
    being termed **LLMOps**.
  prefs: []
  type: TYPE_NORMAL
- en: Building the future with LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the rise in interest in LLMs recently, there has been no shortage of people
    expressing the desire to integrate these models into all sorts of software systems.
    For us as ML engineers, this should immediately trigger us to ask the question,
    “What will that mean operationally?” As discussed throughout this book, the marrying
    together of operations and development of ML systems is termed MLOps. Working
    with LLMs is likely to lead to its own interesting challenges, however, and so
    a new term, LLMOps, has arisen to give this sub-field of MLOps some good marketing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Is this really any different? I don’t think it is *that* different, but should
    be viewed as a sub-field of MLOps with its own additional challenges. Some of
    the main challenges that I see in this area are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Larger infrastructure, even for fine-tuning**: As discussed previously, these
    models are far too large for typical organizations or teams to consider training
    their own, so instead teams will have to leverage third-party models, be they
    open-source or proprietary, and fine-tune them. Fine-tuning models of this scale
    will still be very expensive and so there will be a higher premium on building
    very efficient data ingestion, preparation, and training pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management is different**: When you train your own models, as we showed
    several times in *Chapter 3*, *From Model to Model Factory*, effective ML engineering
    requires us to define good practices for versioning our models and storing metadata
    that provide lineage of the experiments and training runs we have gone through
    to produce these models. In a world where models are more often hosted externally,
    this is slightly harder to do, as we do not have access to the training data,
    to the core model artifacts, and probably not even to the detailed model architecture.
    Versioning metadata will then likely default to the publicly available metadata
    for the model, think along the lines of `gpt-4-v1.3` and similar-sounding names.
    That is not a lot of information to go on, and so you will likely have to think
    of ways to enrich this metadata, perhaps with your own example runs and test results
    in order to understand how that model behaved in certain scenarios. This then
    also links to the next point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rollbacks become more challenging**: If your model is hosted externally by
    a third party, you do not control the roadmap of that service. This means that
    if there is an issue with version 5 of a model and you want to roll back to version
    4, that option might not be available to you. This is a different kind of “drift”
    from the model performance drift we’ve discussed at length in this book but it
    is going to become increasingly important. This will mean that you should have
    your own model, perhaps with nowhere near the same level of functionality or scale,
    ready as a last resort default to switch to in case of issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance is more of a challenge**: As mentioned in the previous
    point, with foundation models being served as externally hosted services, you
    are no longer in as much control as you were. This then means that if you do detect
    any issues with the model you are consuming, be they drift or some other bugs,
    you are very limited in what you can do and you will need to consider that default
    rollback we just discussed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applying your own guardrails will be key**: LLMs hallucinate, they get things
    wrong, they can regurgitate training data, and they might even inadvertently offend
    the person interacting with them. All of this means that as these models are adopted
    by more organizations, there will be a growing need to develop methods for applying
    bespoke guardrails to systems utilizing them. As an example, if an LLM was being
    used to power a next-generation chatbot, you could envisage that between the LLM
    service and the chat interface, you could have a system layer that checked for
    abrupt sentiment changes and important keywords or data that should be obfuscated.
    This layer could utilize simpler ML models and a variety of other techniques.
    At its most sophisticated, it could try and ensure that the chatbot did not lead
    to a violation of ethical or other norms established by the organization. If your
    organization has made the climate crisis an area of focus, you may want to screen
    the conversation in real time for information that goes against critical scientific
    findings in this area as an example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the era of foundation models has only just begun, it is likely that more
    and more complex challenges will arise to keep us busy as ML engineers for a long
    time to come. To me, this is one of the most exciting challenges we face as a
    community, how we harness one of the most sophisticated and cutting-edge capabilities
    ever developed by the ML community in a way that still allows the software to
    run safely, efficiently, and robustly for users day in and day out. Are you ready
    to take on that challenge?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into some of these topics in a bit more detail, first with a discussion
    of LLM validation.
  prefs: []
  type: TYPE_NORMAL
- en: Validating LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The validation of generative AI models is inherently different from and seemingly
    more complex than the same for other ML models. The main reasons for this are
    that when you are *generating* content, you are often creating very complex data
    in your results that has never existed! If an LLM returns a paragraph of generated
    text when asked to help summarize and analyze some document, how do you determine
    if the answer is “good”? If you ask an LLM to reformat some data into a table,
    how can you build a suitable metric that captures if it has done this correctly?
    In a generative context, what do “model performance” and “drift” really mean and
    how do I calculate them? Other questions may be more use case dependent, for example,
    if you are building an information retrieval or Retrieval-Augmented Generation
    (see *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*, [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf))
    solution, how do you evaluate the truthfulness of the text generated by the LLM?
  prefs: []
  type: TYPE_NORMAL
- en: There are also important considerations around how we screen the LLM-generated
    outputs for any potential biased or toxic outputs that may cause harm or reputational
    damage to the organization running the model. The world of LLM validation is complex!
  prefs: []
  type: TYPE_NORMAL
- en: 'What can we do? Thankfully, this has not all happened in a vacuum and there
    have been several benchmarking tools and datasets released that can help us on
    our journey. Things are so young that there are not many worked examples of these
    tools yet, but we will discuss the key points so that you are aware of the landscape
    and can keep on top of how things are evolving. Let’s list some of the higher-profile
    evaluation frameworks and datasets for LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI Evals**: This is a framework whereby OpenAI allows for the crowdsourced
    development of tests against proposed text completions generated by LLMs. The
    core concept at the heart of evals is the “Completion Function Protocol,” which
    is a mechanism for standardizing the testing of the strings returned when interacting
    with an LLM. The framework is available on GitHub at [https://github.com/openai/evals](https://github.com/openai/evals).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Holistic Evaluation of Language Models** (**HELM**): This project, from Stanford
    University, styles itself as a “living benchmark” for LLM performance. It gives
    you a wide variety of datasets, models, and metrics and shows the performance
    across these different combinations. It is a very powerful resource that you can
    use to base your own test scenarios on, or indeed just to use the information
    directly to understand the risks and potential benefits of using any specific
    LLM for your use case. The HELM benchmarks are available at [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guardrails AI**: This is a Python package that allows you to do validation
    on LLM outputs in the same style as `pydantic`, which is a very powerful idea!
    You can also use it to build control flows with the LLM for when issues arise
    like a response to a prompt not meeting your set criteria; in this case, you can
    use Guardrails AI to re-prompt the LLM in the hope of getting a different response.
    To use Guardrails AI, you specify a **Reliable AI Markup Language** (**RAIL**)
    file that defines the prompt format and expected behavior in an XML-like file.
    Guardrails AI is available on GitHub at [https://shreyar.github.io/guardrails/](https://shreyar.github.io/guardrails/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several more of these frameworks being created all the time, but getting
    familiar with the core concepts and datasets out there will become increasingly
    important as more organizations want to take LLM-based systems from fun proofs-of-concept
    to production solutions. In the penultimate section of this chapter, we will briefly
    discuss some specific challenges I see around the management of “prompts” when
    building LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: PromptOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with generative AI that takes text inputs, the data we input is
    often referred to as “prompts” to capture the conversational origin of working
    with these models and the concept that an input demands a response, the same way
    a prompt from a person would. For simplicity, we will call any input data that
    we feed to an LLM a prompt, whether this is in a user interface or via an API
    call and irrespective of the nature of the content we provide to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts are often quite different beasts from the data we typically feed into
    an ML model. They can be effectively freeform, have a variety of lengths, and,
    in most cases, express the intent for how we want the model to act. In other ML
    modeling problems, we can certainly feed in unstructured textual data, but this
    intent piece is missing. This all leads to some important considerations for us
    as ML engineers working with these models.
  prefs: []
  type: TYPE_NORMAL
- en: First, the shaping of prompts is important. The term **prompt engineering**
    has become popular in the data community recently and refers to the fact that
    there is often a lot of thought that goes into designing the content and format
    of these prompts. This is something we need to bear in mind when designing our
    ML systems with these models. We should be asking questions like “Can I standardize
    the prompt formats for my application or use case?”, “Can I provide appropriate
    additional formatting or content on top of what a user or input system provides
    to get a better outcome?”, and similar questions. I will stick with calling this
    prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, prompts are not your typical ML input, and tracking and managing them
    is a new, interesting challenge. This challenge is compounded by the fact that
    the same prompt may give very different outputs for different models, or even
    with different versions of the same model. We should think carefully about tracking
    the lineage of our prompts and the outputs they generate. I term this challenge
    **prompt management**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have a challenge that is not necessarily unique to prompts but definitely
    becomes a more pertinent one if we allow users of a system to feed in their own
    prompts, for example in chat interfaces. In this case, we need to apply some sort
    of screening and obfuscation rules to data coming in and coming out of the model
    to ensure that the model is not “jailbroken” in some way to evade any guardrails.
    We would also want to guard against adversarial attacks that may be designed to
    extract training data from these systems, thereby gaining personally identifiable
    or other critical information that we do not wish to be shared.
  prefs: []
  type: TYPE_NORMAL
- en: As you begin to explore this brave new world of LLMOps with the rest of the
    world, it will be important to keep these prompt-related challenges in mind. We
    will now conclude the chapter with a brief summary of what we have covered.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on deep learning. In particular, we covered the
    key theoretical concepts behind deep learning, before moving on to discuss how
    to build and train your own neural networks. We walked through examples of using
    off-the-shelf models for inference and then adapting them to your specific use
    cases through fine-tuning and transfer learning. All of the examples shown were
    based on heavy use of the PyTorch deep learning framework and the Hugging Face
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to the topical question of the largest models ever built, LLMs,
    and what they mean for ML engineering. We explored a little of their important
    design principles and behaviors before showing how to interact with them in pipelines
    using the popular LangChain package and OpenAI APIs. We also explored the potential
    for using LLMs to help with improving software development productivity, and what
    this will mean for you as an ML engineer.
  prefs: []
  type: TYPE_NORMAL
- en: We finished the chapter with an exploration of the new topic of LLMOps, which
    is all about applying the principles of ML engineering and MLOps that we have
    been discussing throughout this book to LLMs. This covered the core components
    of LLMOps and also some new capabilities, frameworks, and datasets that can be
    used to validate your LLMs. We concluded with some pointers on managing your LLM
    prompts and how the concepts around experiment tracking we covered in *Chapter
    3*, *From Model to Model Factory*, should be translated to apply in this case.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will begin the final section of the book and will cover a detailed
    end-to-end example where we will build an ML microservice using Kubernetes. This
    will allow us to apply many of the skills we have learned throguh the book.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  prefs: []
  type: TYPE_IMG
