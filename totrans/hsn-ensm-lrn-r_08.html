<html><head></head><body>
<div class="book" title="Chapter&#xA0;8.&#xA0;Ensemble Diagnostics" id="1MBG21-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08" class="calibre1"/>Chapter 8. Ensemble Diagnostics</h1></div></div></div><p class="calibre7">In earlier chapters, ensemble methods were found to be effective. In the previous chapter, we looked at scenarios in which ensemble methods increase the overall accuracy of a prediction. It has previously been assumed that different base learners are independent of each other. However, unless we have a very large sample and the base models are learners that use a distinct set of observations, such an assumption is very impractical. Even if we had a large enough sample to believe that the partitions are nonoverlapping, each base model is built on a different partition, and each partition carries with it the same information as any other partition. However, it is difficult to test validations such as this, so we need to employ various techniques in order to validate the independence of the base models on the same dataset. To do this, we will look at various different methods. A brief discussion of the need for ensemble diagnostics will kick off this chapter, and the importance of diversity in base models will be covered in the next section. For the classification problem, the classifiers can be compared with each other. We can then further evaluate the similarity and accuracy of the ensemble. Statistical tests that achieve this task will be introduced in the third section. Initially, a base learner will be compared with another one, and then we will look at all the models of the ensemble in a single step.</p><p class="calibre7">The topics that will be covered in this chapter are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Ensemble diagnostics</li><li class="listitem">Ensemble diversity</li><li class="listitem">Pairwise comparison</li><li class="listitem">Interrater agreement</li></ul></div></div>

<div class="book" title="Chapter&#xA0;8.&#xA0;Ensemble Diagnostics" id="1MBG21-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch08lvl1sec60" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">We will be using the following libraries in the chapter:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">rpart</code></li></ul></div></div></div>
<div class="book" title="What is ensemble diagnostics?" id="1NA0K1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec61" class="calibre1"/>What is ensemble diagnostics?</h1></div></div></div><p class="calibre7">The<a id="id310" class="calibre1"/> power of ensemble methods was demonstrated in the preceding chapters. An ensemble with decision trees forms a homogeneous ensemble, and this was the main topic of <a class="calibre1" title="Chapter 3. Bagging" href="part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee">Chapter 3</a>, <span class="strong"><em class="calibre9">Bagging</em></span>, to <a class="calibre1" title="Chapter 6. Boosting Refinements" href="part0045_split_000.html#1AT9A1-2006c10fab20488594398dc4871637ee">Chapter 6</a>, <span class="strong"><em class="calibre9">Boosting Refinements</em></span>. In <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>, and <a class="calibre1" title="Chapter 7. The General Ensemble Technique" href="part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee">Chapter 7</a>, <span class="strong"><em class="calibre9">The General Ensemble Technique</em></span>, we had a peek at stacked ensembles. A central assumption in an ensemble is that the models are independent of one another. However, this assumption is seldom true, and we know that the same data partition is used over and over again. This does not mean that ensembling is bad; we have every reason to use the ensembles while previewing the concerns in an ensemble application. Consequently, we need to see how close the base models are to each other and overall in their predictions. If the predictions are close to each other, then we might need those base models in the ensemble. Here, we will build logistic regression, Naïve Bayes, SVM, and a decision tree for the German credit dataset as the base models. The analysis and program is slightly repetitive here as it is carried over from earlier chapters:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/GC2.RData")
&gt; table(GC2$good_bad)
 bad good 
 300  700 
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(GC2),replace = 
+ TRUE,prob = c(0.7,0.3))
&gt; head(Train_Test)
[1] "Test"  "Test"  "Test"  "Test"  "Train" "Train"
&gt; GC2_Train &lt;- GC2[Train_Test=="Train",]
&gt; GC2_TestX &lt;- within(GC2[Train_Test=="Test",],rm(good_bad))
&gt; GC2_TestY &lt;- GC2[Train_Test=="Test","good_bad"]
&gt; GC2_TestY_numeric &lt;- as.numeric(GC2_TestY)
&gt; GC2_Formula &lt;- as.formula("good_bad~.")
&gt; p &lt;- ncol(GC2_TestX)
&gt; ntr &lt;- nrow(GC2_Train) 
&gt; nte &lt;- nrow(GC2_TestX) 
&gt; # Logistic Regression
&gt; LR_fit &lt;- glm(GC2_Formula,data=GC2_Train,family = binomial())
&gt; LR_Predict_Train &lt;- predict(LR_fit,newdata=GC2_Train,
+ type="response")
&gt; LR_Predict_Train &lt;- as.factor(ifelse(LR_Predict_Train&gt;0.5,
+ "good","bad"))
&gt; LR_Accuracy_Train &lt;- sum(LR_Predict_Train==GC2_Train$good_bad)/
+ ntr
&gt; LR_Accuracy_Train
[1] 0.78
&gt; LR_Predict_Test &lt;- predict(LR_fit,newdata=GC2_TestX,
+ type="response")
&gt; LR_Predict_Test_Bin &lt;- ifelse(LR_Predict_Test&gt;0.5,2,1)
&gt; LR_Accuracy_Test &lt;- sum(LR_Predict_Test_Bin==
+ GC2_TestY_numeric)/nte
&gt; LR_Accuracy_Test
[1] 0.757
&gt; # Naive Bayes
&gt; NB_fit &lt;- naiveBayes(GC2_Formula,data=GC2_Train)
&gt; NB_Predict_Train &lt;- predict(NB_fit,newdata=GC2_Train)
&gt; NB_Accuracy_Train &lt;- sum(NB_Predict_Train==
+ GC2_Train$good_bad)/ntr
&gt; NB_Accuracy_Train
[1] 0.767
&gt; NB_Predict_Test &lt;- predict(NB_fit,newdata=GC2_TestX)
&gt; NB_Accuracy_Test &lt;- sum(NB_Predict_Test==GC2_TestY)/nte
&gt; NB_Accuracy_Test
[1] 0.808
&gt; # Decision Tree
&gt; CT_fit &lt;- rpart(GC2_Formula,data=GC2_Train)
&gt; CT_Predict_Train &lt;- predict(CT_fit,newdata=GC2_Train,
+ type="class")
&gt; CT_Accuracy_Train &lt;- sum(CT_Predict_Train==
+ GC2_Train$good_bad)/ntr
&gt; CT_Accuracy_Train
[1] 0.83
&gt; CT_Predict_Test &lt;- predict(CT_fit,newdata=GC2_TestX,
+ type="class")
&gt; CT_Accuracy_Test &lt;- sum(CT_Predict_Test==GC2_TestY)/nte
&gt; CT_Accuracy_Test
[1] 0.706
&gt; # Support Vector Machine
&gt; SVM_fit &lt;- svm(GC2_Formula,data=GC2_Train)
&gt; SVM_Predict_Train &lt;- predict(SVM_fit,newdata=GC2_Train,
+ type="class")
&gt; SVM_Accuracy_Train &lt;- sum(SVM_Predict_Train==
+ GC2_Train$good_bad)/ntr
&gt; SVM_Accuracy_Train
[1] 0.77
&gt; SVM_Predict_Test &lt;- predict(SVM_fit,newdata=GC2_TestX,
+ type="class")
&gt; SVM_Accuracy_Test &lt;- sum(SVM_Predict_Test==GC2_TestY)/nte
&gt; SVM_Accuracy_Test
[1] 0.754</pre></div><p class="calibre7">In <a id="id311" class="calibre1"/>the next section, we will emphasize the need for diversity in ensembling.</p></div>

<div class="book" title="Ensemble diversity"><div class="book" id="1O8H62-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec62" class="calibre1"/>Ensemble diversity</h1></div></div></div><p class="calibre7">In an ensemble, we<a id="id312" class="calibre1"/> have many base models—say <span class="strong"><em class="calibre9">L</em></span> number of them. For the classification problem, we have base models as classifiers. If we have a regression problem, we have the base models as learners. Since the diagnostics are performed on the training dataset only, we will drop the convention of train and valid partitions. For simplicity, during the rest of the discussion, we will assume that we have <span class="strong"><em class="calibre9">N</em></span> observations. The <span class="strong"><em class="calibre9">L</em></span> number of models implies that we have <span class="strong"><em class="calibre9">L</em></span> predictions for each of the <span class="strong"><em class="calibre9">N</em></span> observations, and thus the number of predictions is <span class="strong"><img src="../images/00327.jpeg" alt="Ensemble diversity" class="calibre15"/></span>. It is in these predictions that we try to find the diversity of the ensemble. The diversity of the ensemble is identified depending on the type of problem we are dealing with. First, we will take the regression problem.</p></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Numeric prediction"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec37" class="calibre1"/>Numeric prediction</h2></div></div></div><p class="calibre7">In<a id="id313" class="calibre1"/> the case of <a id="id314" class="calibre1"/>regression problems, the predicted values of the observations can be compared directly with their actual values. We can easily see which base models' predictions are closer to the actual value of the observation and which are far away from it. If all the predictions are closer to each other, the base models are not diverse. In this case, one of the predictions might suffice all the same. If the predictions exhibit some variance, combining them by using the average might provide stability. In the assessment of the diversity, it is also important to know how close the ensemble prediction is to the true observation value.</p><p class="calibre7">Let's consider a hypothetical scenario in which we have six observations, their actual values, three base learners, predictions by the learners, and the ensemble prediction. A sample dataset that will help you to understand the intricacies of ensemble diversity is given in the following table:</p><div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Observation Number</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Actual</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">E1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">E2</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">E3</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">EP</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">1</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">15</p>
</td><td class="calibre25">
<p class="calibre22">20</p>
</td><td class="calibre25">
<p class="calibre22">25</p>
</td><td class="calibre25">
<p class="calibre22">20</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">2</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">40</p>
</td><td class="calibre25">
<p class="calibre22">50</p>
</td><td class="calibre25">
<p class="calibre22">60</p>
</td><td class="calibre25">
<p class="calibre22">50</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">3</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">25</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">35</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">4</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">28</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">32</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">5</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">20</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">40</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">6</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td><td class="calibre25">
<p class="calibre22">10</p>
</td><td class="calibre25">
<p class="calibre22">15</p>
</td><td class="calibre25">
<p class="calibre22">65</p>
</td><td class="calibre25">
<p class="calibre22">30</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote1"><p class="calibre27">Table 1: Six observations, three base learners, and the ensemble</p></blockquote></div><p class="calibre7">For<a id="id315" class="calibre1"/> ease of comparison, all the observations' true values are<a id="id316" class="calibre1"/> kept at 30 in <span class="strong"><em class="calibre9">Table 1</em></span>. The ensemble predictions for the six observations/cases range from 10–65, while the ensemble prediction—the average of the base learner's prediction—ranges from 20–50. As a first step to understanding the diversity of the ensemble for specific observations and the associated predictions, we will visualize the data using the following program block:</p><div class="informalexample"><pre class="programlisting">&gt; DN &lt;- read.csv("../Data/Diverse_Numeric.csv")
&gt; windows(height=100,width=100)
&gt; plot(NULL,xlim=c(5,70),ylim=c(0,7),yaxt='n',xlab="X-values",ylab="")
&gt; points(DN[1,2:6],rep(1,5),pch=c(19,1,1,1,0),cex=2)
&gt; points(DN[2,2:6],rep(2,5),pch=c(19,1,1,1,0),cex=2)
&gt; points(DN[3,2:6],rep(3,5),pch=c(19,1,1,1,0),cex=2)
&gt; points(DN[4,2:6],rep(4,5),pch=c(19,1,1,1,0),cex=2)
&gt; points(DN[5,2:6],rep(5,5),pch=c(19,1,1,1,0),cex=2)
&gt; points(DN[6,2:6],rep(6,5),pch=c(19,1,1,1,0),cex=2)
&gt; legend(x=45,y=7,c("Actual","Model","Ensemble"),pch=c(19,1,0))
&gt; axis(2,at=1:6,labels=paste("Case",1:6),las=1)</pre></div><p class="calibre7">The program's explanation is here. The first line of code imports the <code class="literal">Diverse_Numeric.csv</code> data from the code bundle folder. The <code class="literal">windows (X11)</code> function sets up a new graphical device in the Windows (Ubuntu) operating system. The <code class="literal">plot</code> function then sets up an empty plot and the axes' ranges specification is given by <code class="literal">xlim</code> and <code class="literal">ylim</code>. Each row of data from <span class="strong"><em class="calibre9">Table 1</em></span> is embossed using the plot and the <code class="literal">points</code> function. Choosing <code class="literal">pch</code> needs further clarification. If we were to choose <code class="literal">pch</code> at, for example, <code class="literal">19</code>, <code class="literal">1</code>, and <code class="literal">0</code>, then this means that we are selecting a filled circle, a circle, and a square. The three shapes will denote the actual value, the model predictions, and the ensemble prediction respectively. The axis command helps us to get the labels in the right display. The result of the preceding R code block is the following plot:</p><div class="mediaobject"><img src="../images/00328.jpeg" alt="Numeric prediction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Understanding ensemble diversity for a regression problem</p></div></div><p class="calibre11"> </p><p class="calibre7">We <a id="id317" class="calibre1"/>have six observations, each labeled as a <span class="strong"><strong class="calibre8">Case</strong></span>. Consider <span class="strong"><strong class="calibre8">Case 1</strong></span> first. The<a id="id318" class="calibre1"/> filled circle for each observation is the actual value—<span class="strong"><strong class="calibre8">30</strong></span>, in this case—and this is the same across the dataset. For this observation, the ensemble prediction is <span class="strong"><strong class="calibre8">20</strong></span>. The empty square and the value predicted by the three base models of <span class="strong"><strong class="calibre8">15</strong></span>, <span class="strong"><strong class="calibre8">20</strong></span>, and <span class="strong"><strong class="calibre8">25</strong></span> are depicted in the blank circles. The ensemble forecast—the average of the base learner's prediction—is <span class="strong"><strong class="calibre8">20</strong></span> and is denoted by a blank square. Now, the three values are less spread out, which is interpreted as indicating that the ensemble is less diverse for this observation. Consequently, this is an example of low diversity. The estimate of <span class="strong"><strong class="calibre8">20</strong></span> is also far from the actual, and we can see that this is a poor estimate. Consequently, this is a <span class="strong"><em class="calibre9">low diversity–poor estimate</em></span> case.</p><p class="calibre7">In<a id="id319" class="calibre1"/> the second case of <span class="strong"><em class="calibre9">Table 1</em></span>, the three predictions are well spread out and have high diversity. However, the ensemble estimate of <span class="strong"><strong class="calibre8">50</strong></span> is too far from the actual value<a id="id320" class="calibre1"/> of <span class="strong"><strong class="calibre8">30</strong></span>, and we refer to this as a case of <span class="strong"><em class="calibre9">high diversity–poor estimate</em></span>. <span class="strong"><strong class="calibre8">Case 3</strong></span> and <span class="strong"><strong class="calibre8">Case 4</strong></span> are thus seen as <span class="strong"><em class="calibre9">low diversity–good estimate</em></span> since the ensemble prediction matches the actual value and the three ensemble predictions are close to each other. <span class="strong"><strong class="calibre8">Case 5</strong></span> makes a fine balance between diversity and accuracy, and so we can label this as an example of <span class="strong"><em class="calibre9">high diversity–good estimate</em></span>. The final case has good accuracy, though the diversity is too high to make the ensemble prediction any good. You can refer to Kuncheva (2014) for further details on the dilemma of diversity–accuracy of ensemble learners.</p><p class="calibre7">We will consider the diversity–accuracy problem for the classification problem next.</p><div class="book" title="Class prediction"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch08lvl2sec38" class="calibre1"/>Class prediction</h3></div></div></div><p class="calibre7">The <a id="id321" class="calibre1"/>previous section looked at the problem of diversity–accuracy for the <a id="id322" class="calibre1"/>regression problem. In the case of the classification problem, we can clearly mark whether or not the prediction of the classifier matches the actual output/label. Furthermore, we only have two potential predictions: 0 or 1. Consequently, we can compare how close two classifiers are with respect to each other over all observations. For instance, with two possible outcomes for classifier <span class="strong"><img src="../images/00329.jpeg" alt="Class prediction" class="calibre15"/></span> and two possible outcomes for <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span>, we have four possible scenarios for a given observation <span class="strong"><img src="../images/00331.jpeg" alt="Class prediction" class="calibre15"/></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><img src="../images/00332.jpeg" alt="Class prediction" class="calibre15"/></span> predicts the label as 1; <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span> predicts it as 1
</li><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Class prediction" class="calibre15"/></span> predicts the label as 1; <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span> predicts it as 0
</li><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Class prediction" class="calibre15"/></span> predicts the label as 0; <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span> predicts it as 1
</li><li class="listitem"><span class="strong"><img src="../images/00329.jpeg" alt="Class prediction" class="calibre15"/></span> predicts the label as 0; <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span> predicts it as 0
</li></ul></div><p class="calibre7">In scenarios 1 and 4, the two classifiers <span class="strong"><em class="calibre9">agree</em></span> with each other, and in 2 and 3, they <span class="strong"><em class="calibre9">disagree</em></span>. If<a id="id323" class="calibre1"/> we have <span class="strong"><em class="calibre9">N</em></span> observations, each observation that is <a id="id324" class="calibre1"/>predicted with the two models will fall into one of the four preceding scenarios. Before we consider the formal measures of agreement or disagreement of two or more models, we will consider two simpler cases in the forthcoming discussion.</p><p class="calibre7">There is a popular saying that if two people agree with each other all the time, one of them is not needed. This is similar to the way in which classifiers work. Similarly, say that a pair of geese are known to be very loyal; they stick with each other, facing problems together. Now, if we have two models that behave in the same way as these geese in all observations, then the diversity is lost for good. Consequently, in any given ensemble scenario, we need to eliminate the pair of geese and keep only one of them. Suppose then that we have a matrix of <span class="strong"><em class="calibre9">L</em></span> predictions where the column corresponds to the classifier and the row to the <span class="strong"><em class="calibre9">N</em></span> observations. In this case, we will define a function named <code class="literal">GP</code>, and an abbreviation for the geese pair, which will tell us which classifiers have a geese pair classifier agreeing with them across all observations:</p><div class="informalexample"><pre class="programlisting">&gt; # Drop the Geese Pair
&gt;GP&lt;- function(Pred_Matrix) {
+   L&lt;- ncol(Pred_Matrix) # Number of classifiers
+   N&lt;- nrow(Pred_Matrix)
+   GP_Matrix &lt;- matrix(TRUE,nrow=L,ncol=L)
+   for(i in 1:(L-1)){
+     for(j in (i+1):L){
+       GP_Matrix[i,j] &lt;- ifelse(sum(Pred_Matrix[,i]==Pred_Matrix[,j])==N,
+                                TRUE,FALSE)
+       GP_Matrix[j,i] &lt;- GP_Matrix[i,j]
+     }
+   }
+   return(GP_Matrix)
+ }</pre></div><p class="calibre7">How does the geese pair <code class="literal">GP</code> function work? We give a <code class="literal">matrix</code> of predictions as the input to this function, with the columns for the classifiers and the rows for the observations. This function first creates a logical matrix of order <span class="strong"><img src="../images/00334.jpeg" alt="Class prediction" class="calibre15"/></span>with default logical values as <code class="literal">TRUE</code>. Since a classifier will obviously agree with itself, we accept the default value. Furthermore, since a <span class="strong"><img src="../images/00333.jpeg" alt="Class prediction" class="calibre15"/></span> classifier agrees/disagrees with <span class="strong"><img src="../images/00330.jpeg" alt="Class prediction" class="calibre15"/></span> in the same way that <span class="strong"><img src="../images/00333.jpeg" alt="Class prediction" class="calibre15"/></span> agrees/disagrees with <span class="strong"><img src="../images/00333.jpeg" alt="Class prediction" class="calibre15"/></span>, we make use of this fact to compute the lower matrix through a symmetrical relationship. In the two nested loops, we compare the predictions of a classifier with every other classifier. The <code class="literal">ifelse</code> function checks whether all the predictions of a classifier match with another classifier, and if the condition does not hold even for a single observation, we say that the two classifiers under consideration are not the geese pair, or that they disagree on at least one occasion:</p><p class="calibre7">Next, the <code class="literal">GP</code> function is<a id="id325" class="calibre1"/> applied to 500 classifiers that are set up for a <a id="id326" class="calibre1"/>classification problem. The <code class="literal">CART_Dummy</code> dataset is taken from the <code class="literal">RSADBE</code> package. The <code class="literal">CART_DUMMY</code> dataset and related problem description can be found in Chapter 9 of Tattar (2017). We adapt the code and the resulting output from the same source:</p><div class="informalexample"><pre class="programlisting">&gt; data(CART_Dummy)
&gt; CART_Dummy$Y &lt;- as.factor(CART_Dummy$Y)
&gt; attach(CART_Dummy)
&gt; windows(height=100,width=200)
&gt; par(mfrow=c(1,2))
&gt; plot(c(0,12),c(0,10),type="n",xlab="X1",ylab="X2")
&gt; points(X1[Y==0],X2[Y==0],pch=15,col="red")
&gt; points(X1[Y==1],X2[Y==1],pch=19,col="green")
&gt; title(main="A Difficult Classification Problem")
&gt; plot(c(0,12),c(0,10),type="n",xlab="X1",ylab="X2")
&gt; points(X1[Y==0],X2[Y==0],pch=15,col="red")
&gt; points(X1[Y==1],X2[Y==1],pch=19,col="green")
&gt; segments(x0=c(0,0,6,6),y0=c(3.75,6.25,2.25,5),
+          x1=c(6,6,12,12),y1=c(3.75,6.25,2.25,5),lwd=2)
&gt; abline(v=6,lwd=2)
&gt; title(main="Looks a Solvable Problem Under Partitions")</pre></div><p class="calibre7">As can be seen from the program, we have three variables here: <code class="literal">X1</code>, <code class="literal">X2</code>, and <code class="literal">Y</code>. The variable denoted by <code class="literal">Y</code> is a binary variable—one class is denoted by green and another by red. Using the information provided by the <code class="literal">X1</code> and <code class="literal">X2</code> variables, the goal is to predict the class of <code class="literal">Y</code>. The red and green color points are intermingled, and so a single linear classifier won't suffice here to separate the reds from the greens. However, if we recursively partition the data space by <code class="literal">X1</code> and <code class="literal">X2</code>, as shown on the right side of the resulting plots, as shown in <span class="strong"><em class="calibre9">Figure 2</em></span>, the reds and greens look separable. The previous R code block results in the following diagram:</p><div class="mediaobject"><img src="../images/00335.jpeg" alt="Class prediction" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: A typical classification problem</p></div></div><p class="calibre11"> </p><p class="calibre7">A<a id="id327" class="calibre1"/> random forest with <code class="literal">500</code> trees is set up for the <code class="literal">CART_DUMMY</code> dataset. The <a id="id328" class="calibre1"/>fixed seed ensures that the output here is reproducible on any execution. Using the fitted random forest, we next predict the output of all observations using the <code class="literal">500</code> trees. The options of <code class="literal">type="class"</code> and <code class="literal">predict.all=TRUE</code> are central to this code block. The <code class="literal">GP</code> function is then applied to the matrix of predictions for the <code class="literal">500</code> trees. Note that the diagonal elements of the <code class="literal">GP</code> matrix will always be <code class="literal">TRUE</code>. Consequently, if there is any classifier with which it has perfect agreement over all observations, the value of that cell will be <code class="literal">TRUE</code>. If the row sum then exceeds the count by 2, we have a geese classifier for that classifier. The following code captures the entire computation:</p><div class="informalexample"><pre class="programlisting">&gt; CD &lt;- CART_Dummy 
&gt; CD$Y &lt;- as.factor(CD$Y)
&gt; set.seed(1234567)
&gt; CD_RF &lt;- randomForest(Y~.,data=CD,ntree=500)
&gt; CD_RF_Predict &lt;- predict(CD_RF,newdata=CD,
+                           type="class",predict.all=TRUE)
&gt; CD_RF_Predict_Matrix &lt;- CD_RF_Predict$individual
&gt; CD_GP &lt;- GP(CD_RF_Predict_Matrix)
&gt; CD_GP[1:8,1:8]
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[2,] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE
[3,] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
[4,] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE
[5,] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
[6,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
[7,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE
[8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
&gt; rowSums(CD_ST)
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

 [149] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1
[186] 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1
[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1
[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1
[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[334] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1
[371] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[408] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 <span class="strong"><strong class="calibre8">2</strong></span> 1 1

[482] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</pre></div><p class="calibre7">The <a id="id329" class="calibre1"/>reader should note that the bold and larger font of 2 in the <a id="id330" class="calibre1"/>preceding output is not given by R. It has been modified by the software processing the text matter. Consequently, we have a lot of classifiers that have a geese classifier matching each of their own predictions. Using the which function, we first find all the classifier indexes that meet the criteria, and then, by applying the which function for the rows of the <code class="literal">CD_GP</code> matrix, we get the associated geese classifier:</p><div class="informalexample"><pre class="programlisting">&gt; which(rowSums(CD_GP)&gt;1)
 [1]  21  42 176 188 206 221 256 278 290 363 365 385 424 442
&gt; which(CD_GP[21,]==TRUE)
[1]  21 188
&gt; which(CD_GP[42,]==TRUE)
[1]  42 290
&gt; which(CD_GP[176,]==TRUE)
[1] 176 363
&gt; which(CD_GP[206,]==TRUE)
[1] 206 256
&gt; which(CD_GP[221,]==TRUE)
[1] 221 278
&gt; which(CD_GP[365,]==TRUE)
[1] 365 424
&gt; which(CD_GP[385,]==TRUE)
[1] 385 442</pre></div><p class="calibre7">As a<a id="id331" class="calibre1"/> result of running the preceding code, we are able to identify the geese<a id="id332" class="calibre1"/> classifier associated with the classifier. We can choose to remove any one member of the geese pair. In the next example, we will apply this method to the German credit data. The program tries to identify the geese classifier as follows:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(12345)
&gt; GC2_RF3 &lt;- randomForest(GC2_Formula,data=GC2_Train,mtry=10,
+                         parms = list(split="information",
+                                      loss=matrix(c(0,1,1000,0),byrow = TRUE,nrow=2)),
+                         ntree=1000)
&gt; GC2_RF_Train_Predict &lt;- predict(GC2_RF3,newdata=GC2_Train,
+                                 type="class",predict.all=TRUE)
&gt; GC2_RF_Train_Predict_Matrix &lt;- GC2_RF_Train_Predict$individual
&gt; GC2_GP &lt;- GP(GC2_RF_Train_Predict_Matrix)
&gt; rowSums(GC2_GP)
   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
  [37] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

[973] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
&gt; which(rowSums(GC2_GP)&gt;1)
integer(0)</pre></div><p class="calibre7">Since none of the classifiers have a corresponding geese classifier, we don't have to eliminate any of the trees.</p><p class="calibre7">In Kuncheva (2014), page 112, there is a useful metric known as <span class="strong"><em class="calibre9">oracle output</em></span>. Next, we formally define the quantity. Remember that we have <span class="strong"><em class="calibre9">L</em></span> number of classifiers and <span class="strong"><em class="calibre9">N</em></span> number of observations. The original/actual values of the label are denoted by <span class="strong"><img src="../images/00336.jpeg" alt="Class prediction" class="calibre15"/></span>. We will denote the ith predicted value using the classifier j by <span class="strong"><img src="../images/00337.jpeg" alt="Class prediction" class="calibre15"/></span>.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Oracle output</strong></span>: The oracle output <span class="strong"><img src="../images/00338.jpeg" alt="Class prediction" class="calibre15"/></span> is defined as <span class="strong"><strong class="calibre8">1</strong></span> if the predicted value <span class="strong"><img src="../images/00339.jpeg" alt="Class prediction" class="calibre15"/></span> is equal to <span class="strong"><img src="../images/00340.jpeg" alt="Class prediction" class="calibre15"/></span>; otherwise it is defined as <span class="strong"><strong class="calibre8">0</strong></span>. In mathematical terms, the oracle output is given using the following mathematical expression:</p><div class="mediaobject"><img src="../images/00341.jpeg" alt="Class prediction" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">So, what<a id="id333" class="calibre1"/> is the difference between the oracle output and the predictions? The<a id="id334" class="calibre1"/> predictions consist of the labels of the data, and the labels might be 1/0, GOOD/BAD, +1/-1, YES/NO, or some other binary pair of labels. Besides, in the case of a binary label, a prediction of 1 does not necessarily mean that the original value is 1; it might be 0 as well. The oracle output takes the value of 1 if 1 is predicted as 1, or if 0 is predicted as 0; otherwise, it takes the value of 0. A consequence of using the oracle output is that the proportion of 1s for a classifier will give us the accuracy of the classifier.</p><p class="calibre7">We will now create an R function named <code class="literal">Oracle</code>, which will give the oracle output when it is an input for the prediction matrix and the actual labels. After this, we will calculate the accuracy of the classifiers:</p><div class="informalexample"><pre class="programlisting">&gt; # Oracle Output
&gt; Oracle &lt;- function(PM,Actual){
+   # PM = Prediction Matrix, Actual = the true Y's
+   OM &lt;- matrix(0,nrow=nrow(PM),ncol=ncol(PM))
+   for(i in 1:ncol(OM)) {
+     OM[,i] &lt;- as.numeric(PM[,i]==Actual)
+   }
+   return(OM)
+ }
&gt; GC_Oracle &lt;- Oracle(PM=GC2_RF_Train_Predict$individual,
+                     Actual=GC2_Train$good_bad)
&gt; colSums(GC_Oracle)/nrow(GC_Oracle)
   [1] 0.872 0.884 0.859 0.869 0.866 0.878 0.888 0.872 0.869 0.875 0.885 0.869
  [13] 0.881 0.866 0.879 0.856 0.870 0.869 0.857 0.870 0.878 0.868 0.886 0.892
  [25] 0.881 0.863 0.866 0.856 0.886 0.876 0.873 0.879 0.875 0.885 0.872 0.872

[973] 0.860 0.873 0.869 0.888 0.863 0.879 0.882 0.865 0.891 0.863 0.878 0.879
 [985] 0.878 0.869 0.856 0.872 0.889 0.881 0.868 0.881 0.884 0.854 0.882 0.882
 [997] 0.862 0.884 0.873 0.885</pre></div><p class="calibre7">The oracle matrix helps us in obtaining the accuracy of the classifiers. In the next section, we will discuss some measures that will help us in understanding how close the classifiers are to each other.</p></div></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Pairwise measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl1sec63" class="calibre1"/>Pairwise measure</h2></div></div></div><p class="calibre7">In<a id="id335" class="calibre1"/> this section, we will propose some measures of agreement between two classifiers. The intention is to fix the notions of agreement/disagreement for two classifiers and then take the concept to the overall classifiers of the ensemble in the next section. If <span class="strong"><img src="../images/00333.jpeg" alt="Pairwise measure" class="calibre15"/></span> and <span class="strong"><img src="../images/00330.jpeg" alt="Pairwise measure" class="calibre15"/></span> are classifier models with predictions <span class="strong"><img src="../images/00342.jpeg" alt="Pairwise measure" class="calibre15"/></span>, we can then obtain a table that gives us the following:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts <span class="strong"><img src="../images/00343.jpeg" alt="Pairwise measure" class="calibre15"/></span> as 1; <span class="strong"><img src="../images/00330.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts it as 1
</li><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts <span class="strong"><img src="../images/00344.jpeg" alt="Pairwise measure" class="calibre15"/></span> as 1; <span class="strong"><img src="../images/00330.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts it as 0
</li><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts <span class="strong"><img src="../images/00344.jpeg" alt="Pairwise measure" class="calibre15"/></span> as 0; <span class="strong"><img src="../images/00330.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts it as 1
</li><li class="listitem"><span class="strong"><img src="../images/00333.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts <span class="strong"><img src="../images/00344.jpeg" alt="Pairwise measure" class="calibre15"/></span> as 0; <span class="strong"><img src="../images/00330.jpeg" alt="Pairwise measure" class="calibre15"/></span> predicts it as 0
</li></ul></div><p class="calibre7">The information across the <span class="strong"><em class="calibre9">N</em></span> observations can be put in a tabular form, as follows:</p><div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="top" class="calibre21"> </th><th valign="bottom" class="calibre21">
<p class="calibre22">M1 predicts 1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">M1 predicts 0</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">M2 predicts 1</p>
</td><td class="calibre25">
<p class="calibre22">n11</p>
</td><td class="calibre25">
<p class="calibre22">n10</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">M2 predicts 0</p>
</td><td class="calibre25">
<p class="calibre22">n01</p>
</td><td class="calibre25">
<p class="calibre22">n00</p>
</td></tr></tbody></table></div><p class="calibre7">Table 2: Contingency table for two classifiers/raters</p><p class="calibre7">The diagonal elements of the preceding table show the agreement between the two models/classifiers, while the off-diagonal elements show the disagreement. The models are sometimes <a id="id336" class="calibre1"/>referred<a id="id337" class="calibre1"/> to as <span class="strong"><em class="calibre9">raters</em></span>. The frequency table is also known<a id="id338" class="calibre1"/> as the <span class="strong"><strong class="calibre8">contingency table</strong></span>. Using this setup, we will now discuss some useful measures of <span class="strong"><em class="calibre9">agreement</em></span>. The comparisons are called pairwise measures as we take only a pair of classifiers into analysis.</p></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Disagreement measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec39" class="calibre1"/>Disagreement measure</h2></div></div></div><p class="calibre7">The<a id="id339" class="calibre1"/> disagreement measure <a id="id340" class="calibre1"/>between two classifiers/rates is defined according to the following formula:</p><div class="mediaobject"><img src="../images/00345.jpeg" alt="Disagreement measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">We will now define a <code class="literal">DM</code> function that is given the predictions for the two classifiers. The function will first prepare the contingency table for the predictions. The calculation of the disagreement measure is then straightforward, and is given in the following code block:</p><div class="informalexample"><pre class="programlisting">&gt; # Disagreement Measure
&gt; DM &lt;- function(prediction1,prediction2){
+   tp &lt;- table(prediction1,prediction2)
+   Diss &lt;- (tp[1,2]+tp[2,1])/length(prediction1)
+   return(Diss)
+ }</pre></div><p class="calibre7">In the first section, we had the predictions for the German credit data based on the logistic regression model naïve Bayes, SVM, and a classification tree. Now we apply the DM function to these predictions and see how much these classifiers disagree with each other:</p><div class="informalexample"><pre class="programlisting">&gt; DM(LR_Predict_Train,NB_Predict_Train)
[1] 0.121
&gt; DM(LR_Predict_Train,CT_Predict_Train)
[1] 0.154
&gt; DM(LR_Predict_Train,SVM_Predict_Train)
[1] 0.153
&gt; DM(NB_Predict_Train,CT_Predict_Train)
[1] 0.179
&gt; DM(NB_Predict_Train,SVM_Predict_Train)
[1] 0.154
&gt; DM(CT_Predict_Train,SVM_Predict_Train)
[1] 0.167</pre></div><p class="calibre7">Since we had four classifiers, there will be 3 + 2 + 1 = 6 pairwise comparisons. The naïve Bayes and classification tree have the maximum disagreement, and the least disagreement is between the logistic <a id="id341" class="calibre1"/>regression and the naïve Bayes classifiers. The DM measure can be used to <a id="id342" class="calibre1"/>easily obtain the disagreement of two models.</p></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Yule's or Q-statistic"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec40" class="calibre1"/>Yule's or Q-statistic</h2></div></div></div><p class="calibre7">The Yule's coefficient<a id="id343" class="calibre1"/> is a measure of agreement, and when its value is nearly equal <a id="id344" class="calibre1"/>to zero, it will give the disagreement between the two raters. The Yule's measure is given using the following formula:</p><div class="mediaobject"><img src="../images/00346.jpeg" alt="Yule's or Q-statistic" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The Q-statistic takes the value <a id="id345" class="calibre1"/>in the range of the correlation coefficient—that is, <span class="strong"><img src="../images/00347.jpeg" alt="Yule's or Q-statistic" class="calibre15"/></span>. Consequently, if <a id="id346" class="calibre1"/>the Q values are closer to 1, this means that the two measures nearly always agree with each other, while the value closer to -1 means that the two models predict the opposite of each other. When the Q values are closer to 0, it means that there is a very weak association between the two raters. A <code class="literal">Yule</code> function is created and applied to the different model predictions in the following code block:</p><div class="informalexample"><pre class="programlisting">&gt; # Q-statistic 
&gt; Yule &lt;- function(prediction1,prediction2){
+   tp &lt;- table(prediction1,prediction2)
+   Yu &lt;- (tp[1,1]*tp[2,2]-tp[1,2]*tp[2,1])/(tp[1,1]*tp[2,2]+tp[1,2]*tp[2,1])
+   return(Yu)
+ }
&gt; Yule(LR_Predict_Train,NB_Predict_Train)
[1] 0.949
&gt; Yule(LR_Predict_Train,CT_Predict_Train)
[1] 0.906
&gt; Yule(LR_Predict_Train,SVM_Predict_Train)
[1] 0.98
&gt; Yule(NB_Predict_Train,CT_Predict_Train)
[1] 0.865
&gt; Yule(NB_Predict_Train,SVM_Predict_Train)
[1] 0.985
&gt; Yule(CT_Predict_Train,SVM_Predict_Train)
[1] 0.912</pre></div><p class="calibre7">The agreement between naïve <a id="id347" class="calibre1"/>Bayes predictions and the SVM predictions is highest. Note that if we take the<a id="id348" class="calibre1"/> complement of the disagreement measure and perform it easily using the following code, we get a measure of the following agreement:</p><div class="informalexample"><pre class="programlisting">&gt; 1-DM(LR_Predict_Train,NB_Predict_Train)
[1] 0.879
&gt; 1-DM(LR_Predict_Train,CT_Predict_Train)
[1] 0.846
&gt; 1-DM(LR_Predict_Train,SVM_Predict_Train)
[1] 0.847
&gt; 1-DM(NB_Predict_Train,CT_Predict_Train)
[1] 0.821
&gt; 1-DM(NB_Predict_Train,SVM_Predict_Train)
[1] 0.846
&gt; 1-DM(CT_Predict_Train,SVM_Predict_Train)
[1] 0.833</pre></div><p class="calibre7">However, this analysis says that the highest agreement is between the logistic regression and naïve Bayes raters. Consequently, we note that the output and comparisons might lead to different conclusions. The correlation coefficient can also be computed for two raters; we will cover this next.</p></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Correlation coefficient measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch08lvl2sec41" class="calibre1"/>Correlation coefficient measure</h2></div></div></div><p class="calibre7">The <a id="id349" class="calibre1"/>correlation coefficient between two<a id="id350" class="calibre1"/> numeric variables is very intuitive, and it is also a very useful measure of relationship when there is a linear relationship between them. If both variables are categorical in nature, then we can still obtain the correlation coefficient between them. For two raters, the correlation coefficient is calculated using the following formula:</p><div class="mediaobject"><img src="../images/00348.jpeg" alt="Correlation coefficient measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">We will define an <code class="literal">SS_Cor</code> function that will carry out the necessary computations and return the correlation coefficient:</p><div class="informalexample"><pre class="programlisting">&gt; # Correlation coefficient 
&gt; # Sneath and Sokal, 1973
&gt; SS_Cor &lt;- function(prediction1, prediction2){
+   tp &lt;- table(prediction1,prediction2)
+   a &lt;- tp[1,1]; b &lt;- tp[2,1]; c &lt;- tp[1,2]; d &lt;- tp[2,2]
+   SS &lt;- (a*d-b*c)/sqrt(exp(log(a+b)+log(a+c)+log(c+d)+log(b+d)))
+   return(SS)
+ }</pre></div><p class="calibre7">The correlation <a id="id351" class="calibre1"/>coefficient function is now applied to <a id="id352" class="calibre1"/>the predictions, as shown in the previous examples:</p><div class="informalexample"><pre class="programlisting">&gt; SS_Cor(LR_Predict_Train,NB_Predict_Train)
[1] 0.69
&gt; SS_Cor(LR_Predict_Train,CT_Predict_Train)
[1] 0.593
&gt; SS_Cor(LR_Predict_Train,SVM_Predict_Train)
[1] 0.584
&gt; SS_Cor(NB_Predict_Train,CT_Predict_Train)
[1] 0.531
&gt; SS_Cor(NB_Predict_Train,SVM_Predict_Train)
[1] 0.587
&gt; SS_Cor(CT_Predict_Train,SVM_Predict_Train)
[1] 0.493</pre></div><p class="calibre7">The results show that the logistic and naïve Bayes predictions are in more agreement than any other combination. Correlation tests can be applied for inspecting whether the predictions of the classifiers are independent of each other.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: Apply the <code class="literal">chisq.test</code> to check for the independence of the predictions of the various classifiers here.</p></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Cohen's statistic"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_6"><a id="ch08lvl2sec42" class="calibre1"/>Cohen's statistic</h2></div></div></div><p class="calibre7">The Cohen's statistic<a id="id353" class="calibre1"/> first appeared in 1960. It is based on the probability of the two <a id="id354" class="calibre1"/>raters agreeing with each other because of chance or coincidence. The probability of two raters agreeing with each other is demonstrated as follows:</p><div class="mediaobject"><img src="../images/00349.jpeg" alt="Cohen's statistic" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">However, the probability of agreeing randomly or because of chance is found as follows:</p><div class="mediaobject"><img src="../images/00350.jpeg" alt="Cohen's statistic" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Using the definition of <span class="strong"><img src="../images/00351.jpeg" alt="Cohen's statistic" class="calibre15"/></span> and <span class="strong"><img src="../images/00352.jpeg" alt="Cohen's statistic" class="calibre15"/></span>, the Cohen's statistic is defined by the following:</p><div class="mediaobject"><img src="../images/00353.jpeg" alt="Cohen's statistic" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The Cohen's kappa can take<a id="id355" class="calibre1"/> negative values as well. If its value is 1, this means that the raters agree <a id="id356" class="calibre1"/>with each other completely. The value of 0 means that the agreement is only by chance, and a negative value means that the agreement is less than the expected number by chance. First, the R function <code class="literal">Kappa</code> is created in the following code:</p><div class="informalexample"><pre class="programlisting">&gt; # Kappa-statistic 
&gt; # Cohen's Statistic
&gt; Kappa &lt;- function(prediction1, prediction2){
+   tp &lt;- table(prediction1,prediction2)
+   a &lt;- tp[1,1]; b &lt;- tp[2,1]; c &lt;- tp[1,2]; d &lt;- tp[2,2]
+   n &lt;- length(prediction1)
+   theta1 &lt;- (a+d)/n
+   theta2 &lt;- (((a+b)*(a+c))+((c+d)*(b+d)))/n^2
+   kappa &lt;- (theta1-theta2)/(1-theta2)
+   return(kappa)
+ }</pre></div><p class="calibre7">The coding part is a clear implementation of the formulas, and the choice of <code class="literal">a</code>, <code class="literal">b</code>, <code class="literal">c</code>, <code class="literal">d</code>, <code class="literal">theta1</code>, and <code class="literal">theta2</code> has been made to make the code easy to interpret and follow. Next, we apply the predictions to the German training dataset:</p><div class="informalexample"><pre class="programlisting">&gt; Kappa(LR_Predict_Train,NB_Predict_Train)
[1] 0.69
&gt; Kappa(LR_Predict_Train,CT_Predict_Train)
[1] 0.592
&gt; Kappa(LR_Predict_Train,SVM_Predict_Train)
[1] 0.524
&gt; Kappa(NB_Predict_Train,CT_Predict_Train)
[1] 0.53
&gt; Kappa(NB_Predict_Train,SVM_Predict_Train)
[1] 0.525
&gt; Kappa(CT_Predict_Train,SVM_Predict_Train)
[1] 0.453</pre></div><p class="calibre7">Again, the agreement between the logistic and naïve Bayes predictions is the highest. We now move to the final disagreement measure.</p></div></div>

<div class="book" title="Ensemble diversity">
<div class="book" title="Double-fault measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_7"><a id="ch08lvl2sec43" class="calibre1"/>Double-fault measure</h2></div></div></div><p class="calibre7">In tennis, a double fault <a id="id357" class="calibre1"/>refers to when the serve fails. The server has two opportunities<a id="id358" class="calibre1"/> to get the right serve, and if they do not, the point is conceded to the opponent. The double fault measure occurs when both classifiers get the wrong prediction:</p><div class="mediaobject"><img src="../images/00354.jpeg" alt="Double-fault measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Clearly, we need the DF to be as low as possible and close to 0. This function is easy to interpret, and so this will be left as an exercise for the reader to follow. The R function for the double fault measure and its application is given in then following code:</p><div class="informalexample"><pre class="programlisting">&gt; # Double-fault Measure
&gt; Double_Fault &lt;- function(prediction1,prediction2,actual){
+   DF &lt;- sum((prediction1!=actual)*(prediction2!=actual))/
+         length(actual)
+   return(DF)
+ }
&gt; Double_Fault(LR_Predict_Train,NB_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.166
&gt; Double_Fault(LR_Predict_Train,CT_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.118
&gt; Double_Fault(LR_Predict_Train,SVM_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.148
&gt; Double_Fault(NB_Predict_Train,CT_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.709
&gt; Double_Fault(NB_Predict_Train,SVM_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.154
&gt; Double_Fault(CT_Predict_Train,SVM_Predict_Train,
+ GC2_Train$good_bad)
[1] 0.116</pre></div><p class="calibre7">The reader should identify <a id="id359" class="calibre1"/>the best agreement by using the double fault measure.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: In the case of <a id="id360" class="calibre1"/>multilabels (more than two categories), the extension of the metrics discussed in this section becomes cumbersome. Instead, one can use the oracle matrix and repeat these metrics. The reader should apply these measures to the oracle output.</p><p class="calibre7">The methods discussed thus far apply to only one classifier pair. In the next section, we will measure the diversity of all classifiers of an ensemble.</p></div></div>

<div class="book" title="Interrating agreement"><div class="book" id="1P71O2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec64" class="calibre1"/>Interrating agreement</h1></div></div></div><p class="calibre7">A simple extension of the measures <a id="id361" class="calibre1"/>discussed in the previous section on the ensemble classifiers is to compute the measures for all possible pairs of the ensemble and then simply average over all those values. This task constitutes the next exercise.</p><p class="calibre7">
<span class="strong"><strong class="calibre8">Exercise</strong></span>: For all possible combinations of ensemble pairs, calculate the disagreement measure, Yule's statistic, correlation coefficient, Cohen's kappa, and the double-fault measure. After doing this, obtain the average of the comparisons and report them as the ensemble diversity.</p><p class="calibre7">Here, we will propose alternative measures of diversity and kick-start the discussion with the entropy measure. In all discussions in this section, we will use the oracle outputs.</p></div>

<div class="book" title="Interrating agreement">
<div class="book" title="Entropy measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch08lvl2sec44" class="calibre1"/>Entropy measure</h2></div></div></div><p class="calibre7">You may<a id="id362" class="calibre1"/> recall that <a id="id363" class="calibre1"/>we denote the oracle outputs according to <span class="strong"><img src="../images/00355.jpeg" alt="Entropy measure" class="calibre15"/></span>. For a particular instance, the ensemble is most diverse if the number of classifiers misclassifying it is <span class="strong"><img src="../images/00356.jpeg" alt="Entropy measure" class="calibre15"/></span>. This means that <span class="strong"><img src="../images/00357.jpeg" alt="Entropy measure" class="calibre15"/></span> of the <span class="strong"><img src="../images/00358.jpeg" alt="Entropy measure" class="calibre15"/></span>s are 0s, and the rest of the <span class="strong"><img src="../images/00359.jpeg" alt="Entropy measure" class="calibre15"/></span>, <span class="strong"><img src="../images/00360.jpeg" alt="Entropy measure" class="calibre15"/></span>s are 1s. The <span class="strong"><em class="calibre9">entropy measure for the ensemble</em></span> is then defined by the following:</p><div class="mediaobject"><img src="../images/00361.jpeg" alt="Entropy measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The value of the <a id="id364" class="calibre1"/>entropy measure E is in the unit interval. If the E value is closer to 0, this<a id="id365" class="calibre1"/> means that there is no diversity in the ensemble, while a value close to 1 means that the diversity is at the highest possible level. Given the oracle matrix, we can easily calculate the entropy measure as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Entropy Measure
&gt; # Page 250 of Kuncheva (2014)
&gt; Entropy_Measure &lt;- function(OM){
+   # OM = Oracle Matrix
+   N &lt;- nrow(OM); L &lt;- ncol(OM)
+   E &lt;- 0
+   for(i in 1:N){
+     E &lt;- E+min(sum(OM[i,]),L-sum(OM[i,]))
+   }
+   E &lt;- 2*E/(N*(L-1))
+   return(E)
+ }
&gt; Entropy_Measure(GC_Oracle)
[1] 0.255</pre></div><p class="calibre7">By applying<a id="id366" class="calibre1"/> the <code class="literal">Entropy_Measure</code> on the ensemble for the German credit data, we can see that the entropy measure value is <code class="literal">0.255</code>. The random forest ensemble exhibits diversity as the entropy measure is not closer to 0. However, it is also far away from 1, which implies diversity. However, there are no critical values or tests to interpret whether the diversity is too low, or even too high, for that matter.</p></div></div>

<div class="book" title="Interrating agreement">
<div class="book" title="Kohavi-Wolpert measure"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch08lvl2sec45" class="calibre1"/>Kohavi-Wolpert measure</h2></div></div></div><p class="calibre7">The Kohavi–Wolpert measure<a id="id367" class="calibre1"/> is based on the variance of the prediction as 1s or 0s. It is based<a id="id368" class="calibre1"/> on a decomposition formula for the error rate of a classifier. For the binary problem, or when using oracle input, the variance is the same as the Gini index. This is given according to the following formula:</p><div class="mediaobject"><img src="../images/00362.jpeg" alt="Kohavi-Wolpert measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The Kohavi–Wolpert measure is the average of the variance across all observations. By using the prediction probability<a id="id369" class="calibre1"/> given by the oracle matrix, or as a side product of the fitted objects, we can obtain the variance and then average it across the observations. An R function is now created and applied to some of the predictions obtained for the German credit data, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Kohavi-Wolpert variance 
&gt; # Using the predicted probability
&gt; KW &lt;- function(Prob){
+   N &lt;- nrow(Prob)
+   kw &lt;- mean(1-Prob[,1]^2-Prob[,2]^2)/2
+   return(kw)
+ }
&gt; GC2_RF_Train_Predict_Prob &lt;- predict(GC2_RF3,newdata=GC2_Train,
+                                 type="prob",predict.all=TRUE)
&gt; GC2_RF_Train_Prob &lt;- GC2_RF_Train_Predict_Prob$aggregate
&gt; KW(GC2_RF_Train_Prob)
[1] 0.104</pre></div><p class="calibre7">The Kohavi–Wolpert <a id="id370" class="calibre1"/>measure can also be obtained using the oracle output. We define a mathematical entity that will count the number of classifiers that correctly classify the observation as follows:</p><div class="mediaobject"><img src="../images/00363.jpeg" alt="Kohavi-Wolpert measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The probability of being correctly predicted is as follows:</p><div class="mediaobject"><img src="../images/00364.jpeg" alt="Kohavi-Wolpert measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Using these probabilities, the variance can be obtained as follows:</p><div class="mediaobject"><img src="../images/00365.jpeg" alt="Kohavi-Wolpert measure" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">This method is implemented in the following code using the <code class="literal">KW_OM</code> function:</p><div class="informalexample"><pre class="programlisting">&gt; # Using the Oracle matrix
&gt; KW_OM&lt;- function(OM){
+   # OM is the oracle matrix
+   N &lt;- nrow(OM); L &lt;- ncol(OM)
+   kw &lt;- 0
+   for(i in 1:N){
+     lz &lt;- sum(OM[i,])
+     kw &lt;- kw + lz*(L-lz)
+   }
+   kw &lt;- kw/(N*L^2)
+   return(kw)
+ }
&gt; KW_OM(GC_Oracle)
[1] 0.104</pre></div><p class="calibre7">From this, we can see that the<a id="id371" class="calibre1"/> two methods give us the same result. It is also clear that we <a id="id372" class="calibre1"/>don't have a great deal of diversity following the construction of the random forests.</p></div></div>

<div class="book" title="Interrating agreement">
<div class="book" title="Disagreement measure for ensemble"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch08lvl2sec46" class="calibre1"/>Disagreement measure for ensemble</h2></div></div></div><p class="calibre7">The disagreement measure<a id="id373" class="calibre1"/> between two classifiers can be defined <a id="id374" class="calibre1"/>using the following:</p><div class="mediaobject"><img src="../images/00366.jpeg" alt="Disagreement measure for ensemble" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The disagreement measure of the ensemble is given by the following:</p><div class="mediaobject"><img src="../images/00367.jpeg" alt="Disagreement measure for ensemble" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The Kohavi–Wolpert and disagreement measure are related as follows:</p><div class="mediaobject"><img src="../images/00368.jpeg" alt="Disagreement measure for ensemble" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The next R code block delivers the implementation of the Kohavi–Wolper measure using the oracle outputs as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Disagreement Measure OVerall on Oracle Matrix
&gt; DMO &lt;- function(OM){
+   # OM is the oracle matrix
+   N &lt;- nrow(OM); L &lt;- ncol(OM)
+   dmo &lt;- 0
+   for(i in 1:L){
+     for(j in c(c(1:L)[c(1:L)!=i])){
+       dmo &lt;- dmo + sum((OM[,i]-OM[,j])^2)/N
+     }
+   }
+   dmo &lt;- dmo/(L*(L-1))
+   return(dmo)
+ }
&gt; DM_GC &lt;- DMO(OM=GC_Oracle)
&gt; DM_GC
[1] 0.208
&gt; KW(GC_Oracle)
[1] 0.104
&gt; DM_GC*999/2000
[1] 0.104</pre></div><p class="calibre7">Again, we don't see much<a id="id375" class="calibre1"/> diversity displayed across the ensemble. We <a id="id376" class="calibre1"/>will now move on to looking at the final measure of an ensemble's diversity.</p></div></div>

<div class="book" title="Interrating agreement">
<div class="book" title="Measurement of interrater agreement"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch08lvl2sec47" class="calibre1"/>Measurement of interrater agreement</h2></div></div></div><p class="calibre7">In the introductory discussion of <a id="id377" class="calibre1"/>oracle output, we showed how it can be easily used to obtain the accuracy of a classifier. The average of the classifier accuracy is defined as the average individual classification accuracy, and it is denoted by <span class="strong"><img src="../images/00369.jpeg" alt="Measurement of interrater agreement" class="calibre15"/></span>. The measurement of the interrater agreement is defined by the following:</p><div class="mediaobject"><img src="../images/00370.jpeg" alt="Measurement of interrater agreement" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">This measure is related to the Kohavi–Wolpert measure as follows:</p><div class="mediaobject"><img src="../images/00371.jpeg" alt="Measurement of interrater agreement" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">The implementation of the preceding relation can be understood with the help of the following <a id="id378" class="calibre1"/>code block:</p><div class="informalexample"><pre class="programlisting">&gt; Avg_Ensemble_Acc &lt;- function(Oracle){
+   return(mean(colSums(GC_Oracle)/nrow(GC_Oracle)))
+ }
&gt; Avg_Ensemble_Acc(GC_Oracle)
[1] 0.872
&gt; Kappa &lt;- function(Oracle){
+   pbar &lt;- Avg_Ensemble_Acc(Oracle)
+   AvgL &lt;- 0
+   N &lt;- nrow(Oracle); L &lt;- ncol(Oracle)
+   for(i in 1:N){
+     lz &lt;- sum(Oracle[i,])
+     AvgL &lt;- AvgL + lz*(L-lz)
+   }
+   Avgl &lt;- AvgL/L
+   kappa &lt;- 1-Avgl/(N*(L-1)*pbar*(1-pbar))
+   return(kappa)
+ }
&gt; Kappa(GC_Oracle)
[1] 0.0657
&gt; 1-DM_GC/(2*Avg_Ensemble_Acc(GC_Oracle)*(1-
+ Avg_Ensemble_Acc(GC_Oracle)))
[1] 0.0657</pre></div><p class="calibre7">This concludes our discussion of the agreement in an ensemble.</p></div></div>
<div class="book" title="Summary" id="1Q5IA1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch08lvl1sec65" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">Ensemble methods have been found to be very effective for classification, regression, and other related problems. Any statistical and machine learning method must always be followed up with appropriate diagnostics. The assumption that all base models are independent of each other is central to the success of an ensembling method. However, this independence condition is rarely satisfied, especially because the base models are built on the same dataset. We kicked off the chapter with the simplest measure: the geese pair method. With this, we essentially searched for the models that agree with each other at all times. If such models are present in the ensemble, it is safer to remove one of them. With a large dataset and a high number of variables, it is indeed possible that there won't be any base models that speak the same language as another. However, we still need to check whether they are equal. With this in mind, we first proposed measures that compare only two base models at a time. Different measures can lead to conflicting conclusions. However, this is generally not a problem. The concept of pairwise comparison was then extended to entire ensemble base models. While we found that our base models were not too diverse, it is also important to note here that most of the values are a safe distance away from the boundary value of 0. When we are performing the diagnostics on an ensemble and find that the values are equal to zero, it is then clear that the base models are not offering any kind of diversity. In the next chapter, we will look at the specialized topic of regression data.</p></div></body></html>