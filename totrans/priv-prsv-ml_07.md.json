["```py\npip3 install python-dp\n```", "```py\npip install python-dp\n```", "```py\nBoundedMean(epsilon: float, delta: float = 0, lower_bound: int, upper_bound)\nBoundedMean(epsilon: float)\n```", "```py\nimport pydp as dp\nimport numpy as np\nfrom pydp.algorithms.laplacian import BoundedMean\n#Generate simple data number from 1 to 10 in an array\ndata = np.arange(1,10,1)\n# privacy budget as 0.6, delta as 0 and lower and upper bounds as 1 and 10\nx = BoundedMean(0.6,0,1,10)\nprivacy_mean = x.quick_result(data)\nmean = np.mean(data)\nprint(\"Data\", data)\nprint(\"Normal Mean\", mean)\nprint(\"Mean with differential privacy\",privacy_mean )\n```", "```py\nData [1 2 3 4 5 6 7 8 9]\nNormal Mean 5.0\nMean with differential privacy 6.960764656372703\n```", "```py\nimport pydp as dp\nfrom pydp.algorithms.laplacian import BoundedSum, BoundedMean, Count, Max\nimport pandas as pd\nurl = \"transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf = df_actual[['TRANSACTION_ID',\n'TX_DATETIME','CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT']]\ndf.head()\n```", "```py\ndef mean_tx_amount(tid:int) -> float:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    return statistics.mean(list(dft[\"TX_AMOUNT\"]))\nmean_tx_amount(1)\n56.22097378277154\n# calculates mean applying differential privacy\ndef private_mean_tx_amount(privacy_budget: float, tid:int) -> float:\n    x = BoundedMean(privacy_budget,1,1,300)\n    dft = df[df[\"TERMINAL_ID\"] == id]\n    return x.quick_result(list(dft[\"TX_AMOUNT\"]))\nprivate_mean_tx_amount(0.6,1)\n220.98103940917645\n```", "```py\nterminal_mean_vs_privacy_means=[]\nfor i in range (1, 100):\n    mean = mean_tx_amount(i)\n    privacy_mean = private_mean_tx_amount(0.9,i)\n    terminal_mean_vs_privacy_means.append([i, mean,privacy_mean])\nterminal_mean_vs_privacy_means_df =\npd.DataFrame(terminal_mean_vs_privacy_means,\ncolumns=['Terminal Id','Mean','privacy_mean'])\nterminal_mean_vs_privacy_means_df.head(10)\n```", "```py\ndef count_tx_amount_above(limit: float,tid) -> int:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    return dft[dft.TX_AMOUNT > limit].count()[0]\ncount_tx_amount_above(25.0,1)\n232\ndef private_tx_amount_above(privacy_budget: float, limit: float,tid:int) -> float:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    x = Count(privacy_budget, dtype=\"float\")\n    return x.quick_result(list(dft[dft.TX_AMOUNT > limit][\"TX_AMOUNT\"]))\nprivate_tx_amount_above(0.1,25.0,1)\n257\nterminal_amount_vs_privacy_amont=[]\nfor i in range (1, 100):\n    count = count_tx_amount_above(25.0,i)\n    privacy_count = private_tx_amount_above(0.1,25.0,i)\n    terminal_amount_vs_privacy_amont.append([i, count,privacy_count])\nterminal_amount_vs_privacy_amont_df =\npd.DataFrame(terminal_amount_vs_privacy_amont, columns=['Terminal Id','Count','privacy_count'])\nterminal_amount_vs_privacy_amont_df.head(10)\n```", "```py\ndef max_tx_amount(tid:int) -> int:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    return dft.max()[\"TX_AMOUNT\"]\nmax_tx_amount(1)\n87\ndef private_max_tx_amount(privacy_budget: float,tid:int) -> int:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    x = Max(epsilon = privacy_budget, lower_bound = 100.0, upper_bound = 50000.0, dtype=\"float\"\n    return x.quick_result(list(dft[\"TX_AMOUNT\"]))\nprivate_max_tx_amount(0.5,1)\n167.51941105013407\n```", "```py\ndef sum_tx_amount(tid:int) -> float:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    return dft.sum()[\"TX_AMOUNT\"]\nsum_tx_amount(1)\n15011\ndef private_sum_tx_amount(privacy_budget: float, tid:int) -> float:\n    dft = df[df[\"TERMINAL_ID\"] == tid]\n    x = BoundedSum(epsilon = privacy_budget, delta = 0,\nlower_bound= 100.0, upper_bound = 50000.0, dtype=\"float\")\n    return x.quick_result(list(dft[\"TX_AMOUNT\"]))\nprivate_sum_tx_amount(0.6,1)\n27759.46144104004\n```", "```py\nimport pandas as pd\nurl = \"2023-07-08.csv\"\ndf = pd.read_csv(url, sep=\",\")\nredact_dataset = df.copy()\nredact_dataset = redact_dataset[1:]\ndf.head()\n```", "```py\nredact_dataset.head()\n```", "```py\nsum_original_dataset = round(sum(df[\"TX_AMOUNT\"].to_list()), 2)\nsum_redact_dataset =\nround(sum(redact_dataset[\"TX_AMOUNT\"].to_list()), 2)\ntx_amount_2079 = round((sum_original_dataset - sum_redact_dataset), 2)\ntx_amount_2079\n36\n```", "```py\ndp_sum_original_dataset = BoundedSum(\n    epsilon=1, lower_bound=1.0, upper_bound=500.0, dtype=\"float\"\n)\ndp_sum_original_dataset.reset()\ndp_sum_original_dataset.add_entries(\n    df[\"TX_AMOUNT\"].to_list()\n)\ndp_sum_og = round(dp_sum_original_dataset.result(), 2)\nprint(dp_sum_og)\n1300958.19\n```", "```py\ndp_redact_dataset = BoundedSum(epsilon=1, lower_bound=1.0, upper_bound=500.0, dtype=\"float\")\ndp_redact_dataset.add_entries(redact_dataset[\"TX_AMOUNT\"].to_list())\ndp_sum_redact = round(dp_redact_dataset.result(), 2)\nprint(dp_sum_redact)\n1302153.33\n```", "```py\nround(dp_sum_og - dp_sum_redact, 2)\n-1195.14\n```", "```py\npip install PipelineDP\n```", "```py\nImport pipeline_dp\nImport pandas as pd\nImport numpy as np\nurl =\"transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf_transactions = df_actual[['TRANSACTION_ID', 'TX_DATETIME','CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT']]\ndf_transactions\n```", "```py\nrows =\n[index_row[1] for index_row in transactions_df.iterrows()]\n```", "```py\n#In this example we use a local backend, but Spark and Apache #backends also can be tried in a similar way by making use of the provided classes.\nbackend = pipeline_dp.LocalBackend()\n# Define the total budget.\nbudget_accountant = pipeline_dp.NaiveBudgetAccountant(total_epsilon=1, total_delta=1e-6)\n# Create DPEngine which will execute the logic to generate the aggregates\ndp_engine = pipeline_dp.DPEngine(budget_accountant, backend)\n# Define privacy ID, partition key, and aggregated value extractors.\n# The aggregated value extractor isn't used for Count aggregates, but this is required for SUM, AVERAGE aggregates\ndata_extractors = pipeline_dp.DataExtractors(\n   partition_extractor=lambda row: row.TERMINAL_ID,\n   privacy_id_extractor=lambda row: row.CUSTOMER_ID,\n   value_extractor=lambda row: 1)\n# Configure the aggregation parameters. Number of partitions is 10000 because the number of terminals is 10,000\nparams = pipeline_dp.AggregateParams(\n   noise_kind=pipeline_dp.NoiseKind.LAPLACE,\n   metrics=[pipeline_dp.Metrics.COUNT],\n   max_partitions_contributed=100,\n   max_contributions_per_partition=10)\npublic_partitions=list(range(1, 10000))\n#Create a computational graph for the aggregation.\ndp_result = dp_engine.aggregate(rows, params, data_extractors, public_partitions)\n#Compute budget per each DP operation.\nbudget_accountant.compute_budgets()\ndp_result = list(dp_result)\ndp_dict=dict(dp_result)\nmyKeys = list(dp_dict.keys())\nmyKeys.sort()\nsorted_dict = {i: dp_dict[i] for I in myKeys}\nprint(sorted_dict)\ndp_count = [0] * 100\nfor count_sum_per_day in dp_result:\n  index =  count_sum_per_day[0] – 1\n  dp_count[index] = count_sum_per_day[1][0]\n  print(dp_count[index])\n```", "```py\ndf_counts = df_transactions.groupby(by='TERMINAL_ID').agg('count')\ndf_counts\n```", "```py\n@software{tumultanalyticssoftware,\n    author = {Tumult Labs},\n    title = {Tumult {{Analytics}}},\n    month = dec,\n    year = 2022,\n    version = {latest},\n    url = {https://tmlt.dev}\n}\n```", "```py\npip3 install tmlt.analytics\n```", "```py\nimport os\nfrom pyspark import SparkFiles\nfrom pyspark.sql import SparkSession\nfrom tmlt.analytics.privacy_budget import PureDPBudget\nfrom tmlt.analytics.protected_change import AddOneRow\nfrom tmlt.analytics.query_builder import QueryBuilder\nfrom tmlt.analytics.session import Session\n```", "```py\nspark = SparkSession.builder.getOrCreate()\n```", "```py\nspark.sparkContext.addFile(\n    \"/downloads/transactions.csv\")\ntrans_df = spark.read.csv(\n    SparkFiles.get(\"/downloads/transactions.csv\"), header=True, inferSchema=True\n)\ntrans_df.head(5)\n[Row(_c0=0, TRANSACTION_ID=0, TX_DATETIME=datetime.datetime(2023, 2, 1, 0, 43, 37), CUSTOMER_ID=901, TERMINAL_ID=8047, TX_AMOUNT=82, TX_TIME_SECONDS=2617, TX_TIME_DAYS=0),\n Row(_c0=1, TRANSACTION_ID=1, TX_DATETIME=datetime.datetime(2023, 2, 1, 1, 20, 13), CUSTOMER_ID=2611, TERMINAL_ID=7777, TX_AMOUNT=15, TX_TIME_SECONDS=4813, TX_TIME_DAYS=0),\n Row(_c0=2, TRANSACTION_ID=2, TX_DATETIME=datetime.datetime(2023, 2, 1, 1, 22, 52), CUSTOMER_ID=4212, TERMINAL_ID=3336, TX_AMOUNT=53, TX_TIME_SECONDS=4972, TX_TIME_DAYS=0),\n Row(_c0=3, TRANSACTION_ID=3, TX_DATETIME=datetime.datetime(2023, 2, 1, 1, 26, 40), CUSTOMER_ID=1293, TERMINAL_ID=7432, TX_AMOUNT=59, TX_TIME_SECONDS=5200, TX_TIME_DAYS=0),\n Row(_c0=4, TRANSACTION_ID=4, TX_DATETIME=datetime.datetime(2023, 2, 1, 1, 52, 23), CUSTOMER_ID=2499, TERMINAL_ID=1024, TX_AMOUNT=25, TX_TIME_SECONDS=6743, TX_TIME_DAYS=0)]\n```", "```py\nSession by wrapping a DataFrame containing private data using the from_dataframe() method:\n```", "```py\nsession = Session.from_dataframe(\n    privacy_budget=PureDPBudget(3.5),\n    source_id=\"transactions\",\n    dataframe=trans_df,\n    protected_change=AddOneRow(),\n)\n```", "```py\ncount_query = QueryBuilder(\"transactions\").count()\n```", "```py\ntotal_count = session.evaluate(\n    count_query,\n  privacy_budget=PureDPBudget(epsilon=1)\n)\n```", "```py\ntotal_count.show()\n+-------+\n|  count|\n+-------+\n|4557168|\n+-------+\n```", "```py\ntotal_count = trans_df.count()\nprint(total_count)\n4557166\n```", "```py\nsession.describe()\nThe session has a remaining privacy budget of PureDPBudget(epsilon=2.5).\nThe following private tables are available:\nTable 'transactions' (no constraints):\n  Columns:\n    - '_c0'              INTEGER\n    - 'TRANSACTION_ID'   INTEGER\n    - 'TX_DATETIME'      TIMESTAMP\n    - 'CUSTOMER_ID'      INTEGER\n    - 'TERMINAL_ID'      INTEGER\n    - 'TX_AMOUNT'        INTEGER\n    - 'TX_TIME_SECONDS'  INTEGER\n    - 'TX_TIME_DAYS'     INTEGER\n```", "```py\nlow_purchagers= QueryBuilder(\"transactions\").filter(\"TX_AMOUNT < 25\").count()\nlow_purchagers_count = session.evaluate(\n    low_purchagers,\n    privacy_budget=PureDPBudget(epsilon=1),\n)\nlow_purchagers_count.show()\n+-------+\n|  count|\n+-------+\n|1024844|\n+-------+\nprint(session.remaining_privacy_budget)\nPureDPBudget(epsilon=1.5)\n```", "```py\nmed_purchagers= QueryBuilder(\"transactions\").filter(\"TX_AMOUNT >25 AND TX_AMOUNT <50\").count()\nmed_purchagers_count = session.evaluate(\n    med_purchagers,\n    privacy_budget=PureDPBudget(epsilon=1),\n)\nmed_purchagers_count.show()\n+-------+\n|  count|\n+-------+\n|1165384|\n+-------+\nprint(session.remaining_privacy_budget)\nPureDPBudget(epsilon=0.5)\n```", "```py\nhigh_purchagers= QueryBuilder(\"transactions\").filter(\"TX_AMOUNT > 50\").count()\nhigh_purchagers_count = session.evaluate(\n    high_purchagers,\n    privacy_budget=PureDPBudget(epsilon=1),\n)\nhigh_purchagers_count.show()\n```", "```py\n---------------------------------------------------------------------------\nInsufficientBudgetError                   Traceback (most recent call last)\n/usr/local/lib/python3.10/dist-packages/tmlt/analytics/session.py in evaluate(self, query_expr, privacy_budget)\n   1283             try:\n-> 1284                 answers = self._accountant.measure(\n   1285                     measurement, d_out=adjusted_budget.value\n/usr/local/lib/python3.10/dist-packages/tmlt/core/measurements/interactive_measurements.py in measure(self, measurement, d_out)\n   1343         if not self._privacy_budget.can_spend_budget(d_out):\n-> 1344             raise InsufficientBudgetError(\n   1345                 self.privacy_budget,\nInsufficientBudgetError: PrivacyAccountant's remaining privacy budget is 1/2, which is insufficient for this operation that requires privacy loss 1.\nRuntimeError: Cannot answer query without exceeding the Session privacy budget.\nRequested: ε=1.000\nRemaining: ε=0.500\nDifference: ε=0.500\n```", "```py\nPureDPBudget(epsilon=0.5)\n```", "```py\nhigh_purchagers= QueryBuilder(\"transactions\").filter(\"TX_AMOUNT > 50\").count()\nhigh_purchagers_count = session.evaluate(\n    high_purchagers,\n    privacy_budget=session.remaining_privacy_budget,\n)\nhigh_purchagers_count.show()\n+-------+\n|  count|\n+-------+\n|2271804|\n+-------+\n```", "```py\nbudget = PureDPBudget(epsilon=2.5) # maximum budget consumed in the Session\nsession = Session.from_dataframe(\n    privacy_budget=budget,\n    source_id=\"transactions\",\n    dataframe=trans_df,\n    protected_change=AddOneRow(),\n)\n```", "```py\nfrom tmlt.analytics.keyset import KeySet\nterminal_ids = KeySet.from_dict({\n    \"TERMINAL_ID\": [\n        1,2,3,4,5,6,7,8,9,10\n    ]\n})\n```", "```py\naverage_purchase_query = (\n    QueryBuilder(\"transactions\")\n    .groupby(terminal_ids)\n    .average(\"TX_AMOUNT\", low=5, high=100)\n)\naverage_purchages= session.evaluate(\n    average_purchase_query,\n    privacy_budget=PureDPBudget(1),\n)\naverage_purchages.show(truncate=False)\n+-----------+------------------+\n|TERMINAL_ID|TX_AMOUNT_average |\n+-----------+------------------+\n|1          |55.93609022556391 |\n|2          |52.93446601941748 |\n|3          |40.95974576271186 |\n|4          |52.02414486921529 |\n|5          |47.511428571428574|\n|6          |52.276595744680854|\n|7          |51.566233766233765|\n|8          |50.12273641851107 |\n|9          |52.88358208955224 |\n|10         |48.98945147679325 |\n+-----------+------------------+\n```", "```py\nfrom tmlt.analytics.protected_change import AddRowsWithID\nbudget = PureDPBudget(epsilon=2.5) # maximum budget consumed in the Session\nsession = Session.from_dataframe(\n    privacy_budget=budget,\n    source_id=\"transactions\",\n    dataframe=trans_df,\n    protected_change=AddRowsWithID(id_column=\"CUSTOMER_ID\"),\n)\n```", "```py\nkeyset = KeySet.from_dataframe(\n    trans_df.select(\"TERMINAL_ID\", \"TX_AMOUNT\")\n)\ncount_query = (\n    QueryBuilder(\"transactions\")\n    .groupby(keyset)\n    .count()\n)\nresult = session.evaluate(count_query, PureDPBudget(1))\n```", "```py\nRuntimeError: A constraint on the number of rows contributed by each ID is needed to perform this query (e.g. MaxRowsPerID).\n```", "```py\nfrom tmlt.analytics.constraints import (\n    MaxGroupsPerID,\n    MaxRowsPerGroupPerID,\n    MaxRowsPerID,\n)\nkeyset = KeySet.from_dataframe(\n    trans_df.select(\"TERMINAL_ID\", \"TX_AMOUNT\")\n)\ncount_query = (\n    QueryBuilder(\"transactions\")\n    .enforce(MaxRowsPerID(100))\n    .groupby(keyset)\n    .count()\n)\nresult = session.evaluate(count_query, PureDPBudget(1))\ntop_five = result.sort(\"count\", ascending=False).limit(5)\ntop_five.show()\n+-----------+---------+-----+\n|TERMINAL_ID|TX_AMOUNT|count|\n+-----------+---------+-----+\n|       3001|       98| 1240|\n|       3536|       42| 1217|\n|       4359|       71| 1212|\n|       9137|       97| 1145|\n|       7179|       76| 1143|\n+-----------+---------+-----+\nresult.show()\n+-----------+---------+-----+\n|TERMINAL_ID|TX_AMOUNT|count|\n+-----------+---------+-----+\n|          0|        4|  401|\n|          0|        7|  224|\n|          0|       11|   -7|\n|          0|       12|  131|\n|          0|       16|  -35|\n|          0|       18|  -68|\n|          0|       20| -126|\n|          0|       24|  -46|\n|          0|       26| -162|\n|          0|       28|  -30|\n|          0|       31|  447|\n|          0|       33|   23|\n|          0|       35|   20|\n|          0|       44|   96|\n|          0|       49|  -56|\n|          0|       51|  211|\n|          0|       58|  -88|\n|          0|       59|  -27|\n|          0|       60| -254|\n|          0|       61|  525|\n+-----------+---------+-----+\n```", "```py\nimport pandas as pd\nimport numpy as np\nurl=\"transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf_actual.head()\ndf_transactions = df_actual[['TRANSACTION_ID', 'TX_DATETIME','CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT']]\ndf_transactions\ndf_transactions.insert(5, 'TX_FRAUD', 0, True)\ndf_transactions\n```", "```py\ndf_transactions.loc[df_transactions.TX_AMOUNT>75, 'TX_FRAUD']=1\nnb_frauds=df_transactions.TX_FRAUD.sum()\nprint(\"Number of fraud transaction\",nb_frauds)\nNumber of fraud transaction 1106783\ndf_transactions.head()\n```", "```py\ndf_transactions.to_csv(\"fraud_transactions.csv\")\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport pandas as pd\nurl = \"fraud_transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf_transactions =\ndf_actual[['CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT','TX_FRAUD']]\ndf_transactions\n```", "```py\nprint('No Frauds', round(df_transactions['TX_FRAUD'].value_counts()[0]/len(df_transactions) * 100,2), '% of the dataset')\nprint('Frauds', round(df_transactions['TX_FRAUD'].value_counts()[1]/len(df_transactions) * 100,2), '% of the dataset')\nNo Frauds 75.71 % of the dataset\nFrauds 24.29 % of the dataset\nX = df_transactions.drop('TX_FRAUD', axis=1)\ny = df_transactions['TX_FRAUD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\nlogreg = LogisticRegression(random_state=0)\nlogreg.fit(X_train, y_train)\ntraining_score = cross_val_score(logreg, X_train, y_train, cv=2)\nprint('Logistic Regression Cross Validation Score: ',\nround(training_score.mean() * 100, 2).astype(str) + '%')\nLogistic Regression Cross Validation Score: 100.0%\nnp.sum(logreg.predict(X_test) == y_test)/X_test.shape[0]\n1.0\nlogreg.intercept_[0], logreg.coef_[0]\n(-1168.308115256604,\n array([-2.47724513e-05, 3.17749573e-06, 1.54748556e+01]))\n```", "```py\ndata=[79,3115,78]\nweights = [-2.47724513e-05, 3.17749573e-06, 1.54748556e+01]\nintercept = -1168.308115256604\ndef predict(data,coefficients,intercept):\n    yhat = intercept\n    for i in range(len(data)):\n        yhat += coefficients[i] * data[i]\n    return 1.0 / (1.0 + np.exp(-yhat))\nyhat = predict(data,weights,intercept)\nyhat\n1.0\n```", "```py\ndef predict(data,coefficients,intercept):\n    yhat = intercept\n    for i in range(len(data)):\n        yhat += coefficients[i] * data[i]\n    return 1.0 / (1.0 + np.exp(-yhat))\ndef final_gradients(gradients):\n    length_grads=len(grads)\n    avg_grads=[0,0,0]\n    for i in range(0,length_grads):\n        avg_grads[0]+=grads[i][0]\n        avg_grads[1]+=grads[i][1]\n        avg_grads[2]+=grads[i][2]\n    avg_grads=[i/length_grads for i in avg_grads]\n    return avg_grads\ndef sgd(train,y_train,l_rate, n_epoch):\n    coef = [0,0,0]\n    final_grads = [0,0,0]\n    intercept = 0\n    for epoch in range(n_epoch):\n        predictions=[]\n        gradients=[]\n        sum_error = 0.0\n        for i in range(len(train)):\n            yhat = predict(train[i], coef,intercept)\n            predictions.append(yhat)\n            error = y_train[i] - yhat\n            sum_error += error**2\n            intercept= intercept + l_rate * error * yhat * (1.0 - yhat)\n## intercept\n            temp=train[i]\n            for j in range(3):\n                coef[j] = coef[j] + l_rate * error * yhat * (1.0 - yhat) * temp[j]\n            gradients.append(coef)\n        final_grads = final_gradients(gradients)\n        print('>epoch=%d, lrate=%.3f, error=%.3f, intercept=%.3f '% (epoch, l_rate, sum_error,intercept))\n    return final_grads\nl_rate = 0.24\nn_epoch = 4\ncoef = sgd(X_train[:10],y_train[:10],l_rate, n_epoch)\nprint(coef)\n>epoch=0, lrate=0.240, error=2.250, intercept=-0.030\n>epoch=1, lrate=0.240, error=2.000, intercept=-0.030\n>epoch=2, lrate=0.240, error=2.000, intercept=-0.030\n>epoch=3, lrate=0.240, error=2.000, intercept=-0.030\n[-136.44000000000003, -263.88000000000005, -0.5099999999999999]\n```", "```py\ndef clip(iv, b):\n    norm = np.linalg.norm(iv, ord=2)\n    if norm > b:\n        return b * (iv / norm)\n    else:\n        return iv\nprint( clip([[4548, 8796,   17]],5.0) )\n[[2.29645183 4.44142267 0.00858392]]\nclip(X_train[:5], 5)\narray([[1.35772596e+00, 2.62589215e+00, 5.07505304e-03],\n       [3.40625619e-01, 2.83605905e-02, 1.55236917e-02],\n       [1.41504420e+00, 1.98583840e+00, 1.52251591e-02],\n       [1.06964206e+00, 1.67446897e+00, 2.08972772e-02],\n       [6.95282267e-01, 2.50737474e+00, 1.19413013e-03]])\ndef dp_final_gradients(gradients):\n    length_grads=len(grads)\n    sensitivity = 1\n    epsilon= 0.8\n    noise = np.random.laplace(loc=0, scale=sensitivity/epsilon)\n    noise_lenth = length_grads + noise\n    avg_grads=[0,0,0]\n    for i in range(0,length_grads):\n        avg_grads[0]+=grads[i][0]\n        avg_grads[1]+=grads[i][1]\n        avg_grads[2]+=grads[i][2]\n        avg_grads=[i/noise_lenth for i in avg_grads]\n    return avg_grads\ndef dp_sgd(train,y_train,l_rate, n_epoch):\n    train = clip(train, 5)\n    coef = [0,0,0]\n    final_grads = [0,0,0]\n    intercept = 0\n    for epoch in range(n_epoch):\n        predictions=[]\n        gradients=[]\n        sum_error = 0.0\n        for i in range(len(train)):\n            yhat = predict(train[i], coef,intercept)\n            predictions.append(yhat)\n            error = y_train[i] - yhat\n            sum_error += error**2\n            intercept= intercept + l_rate * error * yhat * (1.0 - yhat)\n## intercept\n            temp=train[i]\n            for j in range(3):\n                coef[j] = coef[j] + l_rate * error * yhat * (1.0 - yhat) * temp[j]\n            gradients.append(coef)\n        final_grads = dp_final_gradients(gradients)\n        print('>epoch=%d, lrate=%.3f, error=%.3f, intercept=%.3f '% (epoch, l_rate, sum_error,intercept))\n    return final_grads\nl_rate = 0.24\nn_epoch = 4\nprint(\"Gradients using Normal SGD \")\ncoef = sgd(X_train[:10],y_train[:10],l_rate, n_epoch)\nprint(\"Gradients using Differentially Private SGD \")\ncoef = dp_sgd(X_train[:10],y_train[:10],l_rate, n_epoch)\nprint(coef)\nGradients using Normal SGD\n>epoch=0, lrate=0.240, error=2.250, intercept=-0.030\n>epoch=1, lrate=0.240, error=2.000, intercept=-0.030\n>epoch=2, lrate=0.240, error=2.000, intercept=-0.030\n>epoch=3, lrate=0.240, error=2.000, intercept=-0.030\n[-136.44000000000003, -263.88000000000005, -0.5099999999999999]\nGradients using Differentially Private SGD\n>epoch=0, lrate=0.240, error=2.146, intercept=-0.127\n>epoch=1, lrate=0.240, error=1.654, intercept=-0.193\n>epoch=2, lrate=0.240, error=1.478, intercept=-0.229\n>epoch=3, lrate=0.240, error=1.396, intercept=-0.249\n[-115.01700212848986, -222.44713076565455, -0.42992283117509383]\n```", "```py\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\n# Example dataset\ndata = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n#apply clustering on this dataset and cluster the data 2 clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(data)\nclusters = kmeans.labels_\noriginal_centroids = kmeans.cluster_centers_\n# Print the original data points, clusters and centroids\nprint(\"Original Data Points:\\n\", data)\nprint(\"Clusters:\\n\", clusters)\nprint(\"Original Centroids:\\n\", original_centroids)\nOriginal Data Points:\n [[ 1.   2\\. ]\n [ 1.5  1.8]\n [ 5.   8\\. ]\n [ 8.   8\\. ]\n [ 1.   0.6]\n [ 9.  11\\. ]]\nClusters:\n [1 1 0 0 1 0]\nOriginal Centroids:\n [[7.33333333 9.        ]\n [1.16666667 1.46666667]]\n```", "```py\ndef add_noise(data, epsilon, sensitivity):\n    beta = sensitivity / epsilon\n    laplace_noise = np.random.laplace(0, beta, data.shape)\n    noisy_data = data + laplace_noise\n    return noisy_data\n# Sensitivity is the maximum change in data points due to the\naddition or removal of a single data point\nsensitivity = np.max(np.abs(data - np.mean(data, axis=0)))\n# Privacy parameter epsilon determines the amount of noise to be added\nepsilon = 0.1\n# Add noise to the data points\nnoisy_data = add_noise(data, epsilon, sensitivity)\n# Perform clustering on the noisy data\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(noisy_data)\nnoisy_clusters = kmeans.labels_\nnoise_centroids = kmeans.cluster_centers_\nprint(\"Noisy Data Points:\\n\", noisy_data)\nprint(\"Noisy Clusters:\\n\", noisy_clusters)\nprint(\"Noisy Centroids :\\n\", noise_centroids)\nNoisy Data Points:\n [[ -8.22894996 -25.09225801]\n [ 48.29852161 -93.63432789]\n [  2.61671234  86.87531981]\n [ 10.03114688   7.72529685]\n [-27.57009962  59.88763296]\n [ 16.99705384 -94.28428515]]\nNoisy Clusters:\n [1 1 0 0 0 1]\nNoisy Centroids :\n [[ -4.97408014  51.49608321]\n [ 19.0222085  -71.00362369]]\n```", "```py\n!pip3 install diffprivlib\n```", "```py\nfrom diffprivlib.models import KMeans\nepsilon = 1.0\n# Perform clustering with differential privacy\ndp_kmeans = KMeans(epsilon=epsilon, n_clusters=2)\ndp_kmeans.fit(data)\n# Get the differentially private cluster centroids\ndp_centroids = dp_kmeans.cluster_centers_\n# Print the differentially private cluster centroids\nprint(\"Differentially Private Cluster Centroids:\\n\", dp_centroids)\nDifferentially Private Cluster Centroids:\n [[8.71915573 9.51643083]\n [5.96366996 3.84980361]]\n```", "```py\nimport pandas as pd\nimport torch\nurl =\"fraud_transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf_actual.head()\ndf_transactions = df_actual[['CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT','TX_FRAUD']]\ndf_transactions=df_transactions.head(50000)\ndf_transactions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nprint(\"No of Fraud Transactions:\", df_transactions['TX_FRAUD'].value_counts()[0])\nprint(\"No of Non Fraud Transactions:\", df_transactions['TX_FRAUD'].value_counts()[1])\nprint('No Frauds', round(df_transactions['TX_FRAUD'].value_counts()[0]/len(df_transactions) * 100,2), '% of the dataset')\nprint('Frauds', round(df_transactions['TX_FRAUD'].value_counts()[1]/len(df_transactions) * 100,2), '% of the dataset')\nNo of Fraud Transactions: 37870\nNo of Non Fraud Transactions: 12130\nNo Frauds 75.74 % of the dataset\nFrauds 24.26 % of the dataset\nX = df_transactions.drop('TX_FRAUD', axis=1)\ny = df_transactions['TX_FRAUD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n# Convert into Pytorch Tensors\nx_train = torch.FloatTensor(X_train.values)\nx_test = torch.FloatTensor(X_test.values)\ny_train = torch.FloatTensor(y_train.values)\ny_test = torch.FloatTensor(y_test.values)\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\nelse:\n    DEVICE = \"cpu\"\nprint(\"Selected device is\",DEVICE)\nclass FraudDataset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        'Initialization'\n        self.x = x\n        self.y = y\n    def __len__(self):\n        'Returns the total number of samples'\n        return len(self.x)\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample index\n        if self.y is not None:\n            return self.x[index].to(DEVICE), self.y[index].to(DEVICE)\n        else:\n            return self.x[index].to(DEVICE)\ntrain_loader_params = {'batch_size': 64,\n          'shuffle': True,\n          'num_workers': 0}\ntest_loader_params = {'batch_size': 64,\n          'num_workers': 0}\n# Loaders\ntraining_set = FraudDataset(x_train, y_train)\ntesting_set = FraudDataset(x_test, y_test)\ntrain_loader = torch.utils.data.DataLoader(training_set, **train_loader_params)\ntest_loader = torch.utils.data.DataLoader(testing_set, **test_loader_params)\nclass SimpleFraudMLP(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.first_sec = torch.nn.Sequential(\n                           torch.nn.Linear(3, 450),\n                           torch.nn.ReLU(),\n        )\n        self.second_sec = torch.nn.Sequential(\n                           torch.nn.Linear(450, 450),\n                           torch.nn.ReLU(),\n                           torch.nn.Linear(450, 1),\n                           torch.nn.Sigmoid(),\n        )\n    def forward(self, x):\n        return self.second_sec(self.first_sec(x))\nfraud_nn_model = SimpleFraudMLP().to(DEVICE)\nfrom torch import nn, optim\nloss_func = torch.nn.BCELoss().to(DEVICE)\noptimizer = torch.optim.SGD(fraud_nn_model.parameters(), lr = 0.07)\ndef train(fraud_nn_mode,num_epochs):\n    fraud_nn_model.train()\n    for epoch in range(num_epochs):\n          for x_batch, y_batch in train_loader:\n            output = fraud_nn_model(x_train)\n            print(output.squeeze())\n            print(y_train)\n            loss = loss_func(output.squeeze(), y_train)\n# clear gradients for this training step\n            optimizer.zero_grad()\n# backpropagation, compute gradients\n            loss.backward()\n# apply gradients\n            optimizer.step()\n            print(epoch, loss.item())\n    pass\ntrain (fraud_nn_model, 10)\ntensor([0., 0., 0.,  ..., 0., 0., 0.], grad_fn=<SqueezeBackward0>)\ntensor([1., 1., 1.,  ..., 0., 0., 0.])\n0 24.191429138183594\n```", "```py\npip install opacus==1.1.2\n```", "```py\nimport pandas as pd\nurl=\" fraud_transactions.csv\"\ndf_actual = pd.read_csv(url, sep=\",\")\ndf_actual.head()\ndf_transactions = df_actual[['CUSTOMER_ID','TERMINAL_ID','TX_AMOUNT','TX_FRAUD']]\ndf_transactions=df_transactions.head(50000)\ndf_transactions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nprint(\"No of Fraud Transactions:\", df_transactions['TX_FRAUD'].value_counts()[0])\nprint(\"No of Non Fraud Transactions:\", df_transactions['TX_FRAUD'].value_counts()[1])\nprint('No Frauds', round(df_transactions['TX_FRAUD'].value_counts()[0]/len(df_transactions) * 100,2), '% of the dataset')\nprint('Frauds', round(df_transactions['TX_FRAUD'].value_counts()[1]/len(df_transactions) * 100,2), '% of the dataset')\nX = df_transactions.drop('TX_FRAUD', axis=1)\ny = df_transactions['TX_FRAUD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nx_train = torch.FloatTensor(X_train.values)\nx_test = torch.FloatTensor(X_test.values)\ny_train = torch.FloatTensor(y_train.values)\ny_test = torch.FloatTensor(y_test.values)\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\nelse:\n    DEVICE = \"cpu\"\nprint(\"Selected device is\",DEVICE)\nclass FraudDataset(torch.utils.data.Dataset):\n    def __init__(self, x, y):\n        'Initialization'\n        self.x = x\n        self.y = y\n    def __len__(self):\n        'Returns the total number of samples'\n        return len(self.x)\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample index\n        if self.y is not None:\n            return self.x[index].to(DEVICE), self.y[index].to(DEVICE)\n        else:\n            return self.x[index].to(DEVICE)\ntrain_loader_params = {'batch_size': 64,\n          'shuffle': True,\n          'num_workers': 0}\ntest_loader_params = {'batch_size': 64,\n          'num_workers': 0}\n# Generators\ntraining_set = FraudDataset(x_train, y_train)\ntesting_set = FraudDataset(x_test, y_test)\ntrain_loader = torch.utils.data.DataLoader(training_set, **train_loader_params)\ntest_loader = torch.utils.data.DataLoader(testing_set, **test_loader_params)\nfraud_nn_model = SimpleFraudMLP().to(DEVICE)\nimport warnings\nwarnings.simplefilter(\"ignore\")\nMAX_GRAD_NORM = 1.2\nEPSILON = 90.0\nDELTA = 1e-5\nEPOCHS = 20\nLR = 1e-3\nfrom opacus.validators import ModuleValidator\nerrors = ModuleValidator.validate(fraud_nn_model, strict=False)\nerrors[-5:]\nfrom torch import nn, optim\n#loss_func = nn.CrossEntropyLoss()\nloss_func = torch.nn.BCELoss().to(DEVICE)\noptimizer = torch.optim.SGD(fraud_nn_model.parameters(), lr = 0.07)\nfrom opacus import PrivacyEngine\nfraud_nn_model.train()\nprivacy_engine = PrivacyEngine()\nmodel, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n    module=fraud_nn_model,\n    optimizer=optimizer,\n    data_loader=train_loader,\n    epochs=EPOCHS,\n    target_epsilon=EPSILON,\n    target_delta=DELTA,\n    max_grad_norm=MAX_GRAD_NORM,\n)\nprint(f\"Using sigma={optimizer.noise_multiplier} and C={MAX_GRAD_NORM}\")\nimport numpy as np\nimport time\nn_epochs = 10\n#Setting the model in training mode\nfraud_nn_model.train()\n#Training loop\nstart_time=time.time()\nepochs_train_losses = []\nepochs_test_losses = []\nfor epoch in range(n_epochs):\n    train_loss=[]\n    train_loss1=0\n    for x_batch, y_batch in train_loader:\n        fraud_nn_model.train()\n        y_pred = fraud_nn_model(x_batch)\n        loss = loss_func(y_pred.squeeze(), y_batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        train_loss1 += loss.item()*x_batch.size(0)\n    epsilon = privacy_engine.get_epsilon(DELTA)\n    print('ε epsilon{}    : delta:{}'.format(epsilon, DELTA))\n    epochs_train_losses.append(np.mean(train_loss))\n    print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\nε epsilon33.98911164791893    : delta:1e-05\nEpoch 0: train loss: 22.66661006201338\nε epsilon38.786904746786384    : delta:1e-05\nEpoch 1: train loss: 23.087044257350552\nε epsilon42.819749628256126    : delta:1e-05\nEpoch 2: train loss: 23.234367423345226\nε epsilon46.852594509725854    : delta:1e-05\nEpoch 3: train loss: 23.257508610022786\nε epsilon50.8854393911956    : delta:1e-05\nEpoch 4: train loss: 23.949310037727983\nε epsilon54.91828427266533    : delta:1e-05\nEpoch 5: train loss: 22.498504093839657\n```"]