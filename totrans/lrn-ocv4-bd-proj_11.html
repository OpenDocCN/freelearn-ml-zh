<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Text Recognition with Tesseract</h1>
                </header>
            
            <article>
                
<p>In <a href="31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml">Chapter 10</a>, <em>Developing Segmentation Algorithms for Text Recognition</em>, we covered the very basic OCR processing functions. Although they are quite useful for scanned or photographed documents, they are almost useless when dealing with text that casually appears in a picture.</p>
<p>In this chapter, we'll explore the OpenCV 4.0 text module, which deals specifically with scene text detection. Using this API, it is possible to detect the text that appears in a webcam video, or to analyze photographed images (like the ones in Street View or taken by a surveillance camera) to extract text information in real time. This allows for a wide range of applications to be created, from accessibility, to marketing, and even robotics fields.</p>
<p>By the end of this chapter, you will be able to do the following:</p>
<ul>
<li>Understand what scene text recognition is</li>
<li>Understand how the text API works</li>
<li>Use the OpenCV 4.0 text API to detect text</li>
<li>Extract the detected text into an image</li>
<li>Use the text API and Tesseract integration to identify letters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter requires familiarity with the basic C++ programming language. All of the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_11">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_11</a>. The code can be executed on any operating system, though it is only tested on Ubuntu.</p>
<p><span>Check out the following video to see the Code in Action:</span><br/>
<a href="http://bit.ly/2Slht5A">http://bit.ly/2Slht5A</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How the text API works</h1>
                </header>
            
            <article>
                
<p>The text API implements the algorithm that was proposed by <em>Luk√°s Neumann</em> and <em>Jiri Matas</em> in the article <span class="ItalicsPACKT"><em>Real</em>-<em>Time Scene Text Localization and Recognition</em></span> during the <strong>computer vision and pattern recognition</strong> (<strong>CVPR</strong>) conference in 2012. This algorithm represented a significant increase in scene text detection, performing state-of-the art detection both in the CVPR database, as well as in the Google Street View database. Before using the API, let's take a look at how this algorithm works under to hood, and how it addresses the scene text detection problem.</p>
<div class="packt_infobox"><strong>Remember</strong>: The OpenCV 4.0 text API does not come with the standard OpenCV modules. It's an additional module that's present in the OpenCV <kbd>contrib</kbd> package. If you installed OpenCV using the Windows Installer, you should take a look back at <a href="96b225d4-84bc-4d49-b8b3-079b15f05cf0.xhtml" target="_blank">Chapter 1</a>, <em>Getting Started with OpenCV;</em> this will guide you on how to install these modules.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The scene detection problem</h1>
                </header>
            
            <article>
                
<p>Detecting text that randomly appears in a scene is a problem that's harder than it looks. There are several new variables that you need to take into account when you're comparing to identified scanned text, such as the following:</p>
<ul>
<li><strong>Tridimensionality</strong>: The text may be in any scale, orientation, or perspective. Also, the text may be partially occluded or interrupted. There are literally thousands of possible regions where it may appear in the image.</li>
<li><strong>Variety</strong>: Text can be in several different fonts and colors. The font may have outline borders. The background can be dark, light, or a complex image.</li>
<li><strong>Illumination and shadows</strong>: The sunlight's position and apparent color changes over time. Different weather conditions like fog or rain can generate noise. Illumination may be a problem even in closed spaces, since light reflects over colored objects and hits the text.</li>
<li><strong>Blurring</strong>: Text may appear in a region that's not prioritized by lens auto-focus. Blurring is also common in moving cameras, in perspective text, or in the presence of fog.</li>
</ul>
<p>The following picture, taken from Google Street View, illustrates these problems. Note how several of these situations occur simultaneously in just a single image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-483 image-border" src="assets/5692ac93-9c3f-4ac5-a8bb-c8510e741663.png" style="width:61.92em;height:27.50em;"/></div>
<p>Performing text detection to deal with such situations may prove computationally expensive, since there are <strong><span class="CodeInTextPACKT"><span>2<em><sup>n</sup></em></span></span></strong> subsets of pixels, <em><strong><span class="CodeInTextPACKT">n</span></strong></em> being the number of pixels in the image.</p>
<p>To reduce complexity, two strategies are commonly applied:</p>
<ul>
<li><strong>Use a sliding window to search just a subset of image rectangles</strong>: This strategy just reduces the number of subsets to a smaller amount. The amount of regions varies according to the complexity of text being considered. Algorithms that deal just with text rotation may use small values, compared to the ones that also deal with rotation, skewing, perspective, and so on. The advantage of this approach is its simplicity, but they are usually limited to a narrow range of fonts and often to a lexicon of specific words.</li>
<li><strong>Use of connected component analysis</strong>: This approach assumes that pixels can be grouped into regions, where pixels have similar properties. These regions are supposed to have higher chances to be identified as characters. The advantage of this approach is that it does not depend on several text properties (orientation, scale, fonts, and so on), and they also provide a segmentation region that can be used to crop text to the OCR. This was the approach that we used in <a href="31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml">Chapter 10</a>, <em>Developing Segmentation Algorithms for Text Recognition</em>. Lighting could also affect the result, for example, if a shadow is cast over the letters, creating two distinct regions. However, since scene detection is commonly used in moving vehicles (for example, drones or cars) and with videos, the text will end up being detected eventually, since these lighting conditions will differ from frame to frame.</li>
</ul>
<p>The OpenCV 4.0 algorithm uses the second strategy by performing connected component analysis and searching for extremal regions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extremal regions</h1>
                </header>
            
            <article>
                
<p>Extremal regions are connected areas that are characterized by almost uniform intensity, which is surrounded by a contrasted background. The stability of a region can be measured by calculating how resistant to thresholding variance the region is. This variance can be measured with a simple algorithm:</p>
<ol>
<li>Apply the threshold, generating an image, <em>A</em>. Detect its connected pixels regions (extremal regions).</li>
<li><span>Increase the threshold by a delta amount, generating an image, <em>B</em>. Detect its connected pixels regions (extremal regions).</span></li>
<li>Compare image <em>B</em> with <em>A</em>. If a region in image A is similar to the same region in image <em>B</em>, add it to the same branch in the tree. The criteria of similarity may vary from implementation to implementation, but it's usually related to the image area or general shape. If a region in image <em>A</em> appears to be split in image <em>B</em>, create two new branches in the tree for the new regions and associate it with the previous branch.</li>
<li>Set <em>A</em> = <em>B</em> and go back to step 2, until the maximum threshold is applied.</li>
</ol>
<p>This will assemble a tree of regions, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c9442351-9576-48c7-b6c7-46bf4361eb11.png" style="width:247.58em;height:42.67em;"/></div>
<p>The resistance to variance is determined by counting how many nodes are in the same level. By analyzing this tree, it's also possible to determine the <strong>maximally stable extremal regions</strong> (<strong>MSER</strong>s), that is, the regions where the area remains stable in a wide variety of thresholds. In the previous diagram, it is clear that these areas would contain the letters <em><strong>O</strong></em>, <em><strong>N</strong></em>, and <em><strong>Y</strong></em>. The main disadvantage of maximally extremal regions is that they are weak in the presence of blur. OpenCV provides a MSER feature detector in the <strong>feature2d</strong> module. Extremal regions are interesting because they are strongly invariant to illumination, scale, and orientation. They are good candidates for text as well, since they are also invariant, with regards to the type of font used, even when the font is styled. Each region can also be analyzed to determine its boundary ellipsis, and can have properties like affine transformation and area numerically determined. Finally, it's worth mentioning that this entire process is fast, which makes it a very good candidate for real-time applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extremal region filtering</h1>
                </header>
            
            <article>
                
<p>Although MSERs are a common approach to define which extremal regions are worth working with, the <em>Neumann</em> and <em>Matas</em> algorithm uses a different approach, by submitting all extremal regions to a sequential classifier that's been trained for character detection. This classifier works in two different stages:</p>
<ol>
<li>The first stage incrementally computes descriptors (bounding box, perimeter, area, and Euler number) for each region. These descriptors are submitted to a classifier that estimates how probable the region is to be a character in the alphabet. Then, only the regions of high probability are selected for stage 2.</li>
<li>In this stage, the features of the whole area ratio, convex hull ratio, and the number of outer boundary inflexion points are calculated. This provides more detailed information that allows the classifier to discard non-text characters, but they are also much slower to calculate.</li>
</ol>
<p>Under OpenCV, this process is implemented in a class called <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd>. It is also possible to use different image single channel projections, such as <em>R</em>, <em>G</em>, <em>B</em>, Luminance, or gray scale conversion to increase the character recognition rates. Finally, all of the characters must be grouped into text blocks (such as words or paragraphs). OpenCV 3.0 provides two algorithms for this purpose:</p>
<ul>
<li><strong>Prune exhaustive search</strong>: Also proposed by <em>Mattas</em> in 2011, this algorithm does not need any previous training or classification, but is limited to horizontally aligned text</li>
<li><strong>Hierarchical method for oriented text</strong>: This deals with text in any orientation, but needs a trained classifier</li>
</ul>
<div class="packt_infobox">Note that since these operations require classifiers, it is also necessary to provide a trained set as input. OpenCV 4.0 provides some of these trained sets in the following <span class="CodeInTextPACKT">sample</span> package: <a href="https://github.com/opencv/opencv_contrib/tree/master/modules/text/samples">https://github.com/opencv/opencv_contrib/tree/master/modules/text/samples</a>.<br/>
This also means that this algorithm is sensitive to the fonts used in classifier training.</div>
<p>A demonstration of this algorithm can be seen in the following video, which is provided by Neumann himself: <a href="https://www.youtube.com/watch?v=ejd5gGea2Fo&amp;feature=youtu.be">https://www.youtube.com/watch?v=ejd5gGea2Fo&amp;feature=youtu.be</a>. Once the text is segmented, it just needs to be sent to an OCR like Tesseract, similarly to what we did in <a href="31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml">Chapter 10</a>, <em>Developing Segmentation Algorithms for Text Recognition</em>. The only difference is that now we will use OpenCV text module classes to interface with Tesseract, since they provide a way to encapsulate the specific OCR engine we are using.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the text API</h1>
                </header>
            
            <article>
                
<p>Enough theory. It's time to see how the text module works in practice. Let's study how we can use it to perform text detection, extraction, and identification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text detection</h1>
                </header>
            
            <article>
                
<p>Let's start by creating a simple program so that we can perform text segmentation using <strong><span class="CodeInTextPACKT">ERFilters</span></strong>. In this program, we will use the trained classifiers from text API samples. You may download this from the OpenCV repository, but they are also available in this book's companion code.</p>
<p>First, we start by including all of the necessary <kbd>libs</kbd> and <kbd>usings</kbd>:</p>
<pre>#include  "opencv2/highgui.hpp" 
#include  "opencv2/imgproc.hpp" 
#include  "opencv2/text.hpp" 
 
#include  &lt;vector&gt; 
#include  &lt;iostream&gt; 
 
using namespace std; 
using namespace cv; 
using namespace cv::text; </pre>
<p>Recall from the <em>Extremal region filtering</em> section that the <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd> works separately in each image channel. Therefore, we must provide a way to separate each desired channel in a different single channel, <kbd><span class="CodeInTextPACKT">cv::Mat</span></kbd>. This is done by the <kbd><span class="CodeInTextPACKT">separateChannels</span></kbd> function:</p>
<pre>vector&lt;Mat&gt; separateChannels(const Mat&amp; src)  
{ 
   vector&lt;Mat&gt; channels; 
   //Grayscale images 
   if (src.type() == CV_8U || src.type() == CV_8UC1) { 
         channels.push_back(src); 
         channels.push_back(255-src); 
         return channels; 
   } 
 
   //Colored images 
   if (src.type() == CV_8UC3) { 
         computeNMChannels(src, channels); 
         int size = static_cast&lt;int&gt;(channels.size())-1; 
         for (int c = 0; c &lt; size; c++) 
               channels.push_back(255-channels[c]); 
         return channels; 
   } 
 
   //Other types 
   cout &lt;&lt; "Invalid image format!" &lt;&lt; endl; 
   exit(-1);    
}</pre>
<p>First, we verify whether the image is already a single channel image (grayscale image). If that's the case, we just add this image ‚Äì it does not need to be processed. Otherwise, we check if it's an <strong>RGB</strong> image. For colored images, we call the <kbd><span class="CodeInTextPACKT">computeNMChannels</span></kbd> function to split the image into several channels. This function is defined as follows:</p>
<pre>void computeNMChannels(InputArray src, OutputArrayOfArrays channels, int mode = ERFILTER_NM_RGBLGrad); </pre>
<p>The following are its parameters:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">src</span></kbd>: The source input array. It must be a colored image of type 8UC3.</li>
<li><kbd><span class="CodeInTextPACKT">channels</span></kbd>: A vector of <kbd>Mats</kbd> that will be filled with the resulting channels.</li>
<li><kbd><span class="CodeInTextPACKT">mode</span></kbd>: Defines which channels will be computed. Two possible values can be used:
<ul>
<li><kbd><span class="CodeInTextPACKT">ERFILTER_NM_RGBLGrad</span></kbd>: Indicates whether the algorithm will use RGB color, lightness, and gradient magnitude as channels (default)</li>
<li><kbd><span class="CodeInTextPACKT">ERFILTER_NM_IHSGrad</span></kbd>: Indicates whether the image will be split by its intensity, hue, saturation, and gradient magnitude</li>
</ul>
</li>
</ul>
<p class="mce-root">We also append the negative of all color components in the vector. Since the image will have three distinct channels (<em>R</em>, <em>G</em>, and <em>B</em>), this is usually enough. It's also possible to add the non-flipped channels, just like we did with the de-grayscaled image, but we'll end up with six channels, and this could be computer-intensive. Of course, you're free to test with your images if this leads to a better result. Finally, if another kind of image is provided, the function will terminate the program with an error message.</p>
<div class="packt_infobox">Negatives are appended, so the algorithms will cover both bright text in a dark background and dark text in a bright background. There is no sense in adding a negative for the gradient magnitude.</div>
<p>Let's proceed to the <span class="CodeInTextPACKT">main</span> method. We'll use this program to segment the <kbd><span class="CodeInTextPACKT">easel.png</span></kbd> image, which is provided with the source code:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-485 image-border" src="assets/c102affb-0070-4cab-9e5d-b8a0527490dd.png" style="width:22.17em;height:22.25em;"/></div>
<p>This picture was taken by a mobile phone camera while I was walking on the street. Let's code this so that you may also use a different image easily by providing its name in the first program argument:</p>
<pre>int main(int argc, const char * argv[]) 
{ 
   const char* image = argc &lt; 2 ? "easel.png" : argv[1];     
   auto input = imread(image); </pre>
<p>Next, we'll convert the image to grayscale and separate its channels by calling the <kbd><span class="CodeInTextPACKT">separateChannels</span></kbd> function:</p>
<pre>   Mat processed; 
   cvtColor(input, processed, COLOR_RGB2GRAY); 
 
   auto channels = separateChannels(processed); </pre>
<p>If you want to work with all of the channels in a colored image, just replace the two first lines of this code extract to the following:</p>
<pre>Mat processed = input;</pre>
<p>We will need to analyze six channels (RGB and inverted) instead of two (gray and inverted). Actually, the processing times will increase much more than the improvements that we can get. With the channels in hand, we need to create <kbd><span class="CodeInTextPACKT">ERFilters</span></kbd> for both stages of the algorithm. Luckily, the OpenCV text contribution module provides functions for this:</p>
<pre>// Create ERFilter objects with the 1st and 2nd stage classifiers 
<span>auto filter1 = createERFilterNM1(<br/>     loadClassifierNM1("trained_classifierNM1.xml"),  15, 0.00015f, </span> 
    0.13f, 0.2f,true,0.1f); 
 
auto filter2 = createERFilterNM2(      
     loadClassifierNM2("trained_classifierNM2.xml"),0.5); 
 </pre>
<p>For the first stage, we call the <kbd><span class="CodeInTextPACKT">loadClassifierNM1</span></kbd> <span>function</span> to load a previously trained classification model. The .xml containing the training data is its only argument. Then, we call <kbd>createERFilterNM1</kbd> to create an instance of the <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd> class that will perform the classification. The function has the following signature:</p>
<pre>Ptr&lt;ERFilter&gt; createERFilterNM1(const Ptr&lt;ERFilter::Callback&gt;&amp; cb, int thresholdDelta = 1, float minArea = 0.00025, float maxArea = 0.13, float minProbability = 0.4, bool nonMaxSuppression = true, float minProbabilityDiff = 0.1); </pre>
<p>The parameters for this function are as follows:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">cb</span></kbd>: The classification model. This is the same model we loaded with the <kbd><span class="CodeInTextPACKT">loadCassifierNM1</span></kbd> function.</li>
<li><kbd><span class="CodeInTextPACKT">thresholdDelta</span></kbd>: The amount to be summed to the threshold in each algorithm iteration. The default value is <kbd>1</kbd>, but we'll use <kbd>15</kbd> in our example.</li>
<li><kbd><span class="CodeInTextPACKT">minArea</span></kbd>: The minimum area of the <strong>extremal region</strong> (<strong>ER</strong>), where text may be found. This is measured by the percentage of the image's size. ERs with areas smaller than this are immediately discarded.</li>
<li><kbd><span class="CodeInTextPACKT">maxArea</span></kbd>: The maximum area of the ER where text may be found. This is also measured by the percentage of the image's size. ERs with areas greater than this are immediately discarded.</li>
<li><kbd><span class="CodeInTextPACKT">minProbability</span></kbd>: The minimum probability that a region must have to be a character in order to remain for the next stage.</li>
<li><kbd><span class="CodeInTextPACKT">nonMaxSupression</span></kbd>: This is used to indicate if non-maximum suppression will be done in each branch probability.</li>
<li><kbd><span class="CodeInTextPACKT">minProbabilityDiff</span></kbd>: The minimum probability difference between the minimum and maximum extreme region.</li>
</ul>
<p>The process for the second stage is similar. We call <kbd><span class="CodeInTextPACKT">loadClassifierNM</span><span class="CodeInTextPACKT">2</span></kbd> to load the classifier model for the second stage and <kbd><span class="CodeInTextPACKT">createERFilterNM2</span></kbd> to create the second stage classifier. This function only takes the input parameters of the loaded classification model and a minimum probability that a region must achieve to be considered as a character. So, let's call these algorithms in each channel to identify all possible text regions:</p>
<pre>//Extract text regions using Newmann &amp; Matas algorithm 
cout &lt;&lt; "Processing " &lt;&lt; channels.size() &lt;&lt; " channels..."; 
cout &lt;&lt; endl; 
vector&lt;vector&lt;ERStat&gt; &gt; regions(channels.size()); 
for (int c=0; c &lt; channels.size(); c++) 
{ 
    cout &lt;&lt; "    Channel " &lt;&lt; (c+1) &lt;&lt; endl; 
    filter1-&gt;run(channels[c], regions[c]); 
    filter2-&gt;run(channels[c], regions[c]);          
}     
filter1.release(); 
filter2.release(); </pre>
<p>In the previous code, we used the <kbd><span class="CodeInTextPACKT">run</span></kbd> <span>function</span> of the <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd> class. This function takes two arguments:</p>
<ul>
<li><strong>The input channel</strong>: This includes the image to be processed.</li>
<li><strong>The regions</strong>: In the first stage algorithm, this argument will be filled with the detected regions. In the second stage (performed by <kbd>filter2</kbd>), this argument must contain the regions selected in stage 1. These will be processed and filtered by stage 2.</li>
</ul>
<p>Finally, we release both filters, since they will not be needed in the program anymore. The final segmentation step is grouping all <span class="CodeInTextPACKT">ERRegions</span> into possible words and defining their bounding boxes. This is done by calling the <kbd><span class="CodeInTextPACKT">erGrouping</span></kbd> function:</p>
<pre>//Separate character groups from regions 
vector&lt; vector&lt;Vec2i&gt; &gt; groups; 
vector&lt;Rect&gt; groupRects; 
erGrouping(input, channels, regions, groups, groupRects, ERGROUPING_ORIENTATION_HORIZ); </pre>
<p>This function has the following signature:</p>
<pre>void erGrouping(InputArray img, InputArrayOfArrays channels, std::vector&lt;std::vector&lt;ERStat&gt; &gt; &amp;regions, std::vector&lt;std::vector&lt;Vec2i&gt; &gt; &amp;groups, std::vector&lt;Rect&gt; &amp;groups_rects, int method = ERGROUPING_ORIENTATION_HORIZ, const std::string&amp; filename = std::string(), float minProbablity = 0.5); </pre>
<p>Let's take a look at the meaning <span>of</span> each parameter:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">img</span></kbd>: Input image, also called the original image.</li>
<li><kbd><span class="CodeInTextPACKT">regions</span></kbd>: Vector of single channel images where regions were extracted.</li>
<li><kbd><span class="CodeInTextPACKT">groups</span></kbd>: An output vector of indexes of grouped regions. Each group region contains all extremal regions of a single word.</li>
<li><kbd><span class="CodeInTextPACKT">groupRects</span></kbd>: A list of rectangles with the detected text regions.</li>
<li><span class="CodeInTextPACKT"><kbd>method</kbd></span>: This is the method of grouping. It can be any of the following:
<ul>
<li><kbd><span class="CodeInTextPACKT">ERGROUPING_ORIENTATION_HORIZ</span></kbd>: The default value. This only generates groups with horizontally oriented text by doing an exhaustive search, as proposed originally by <em>Neumann</em> and <em>Matas</em>.</li>
<li><kbd><span class="CodeInTextPACKT">ERGROUPING_ORIENTATION_ANY</span></kbd>: This generates groups with text in any orientation, using single linkage clustering and classifiers. If you use this method, the filename of the classifier model must be provided in the next parameter.</li>
<li><kbd><span class="CodeInTextPACKT">Filename</span></kbd>: The name of the classifier model. This is only needed if <kbd><span class="CodeInTextPACKT">ERGROUPING_ORIENTATION_ANY</span></kbd> is selected.</li>
<li><kbd><span class="CodeInTextPACKT">minProbability</span></kbd>: The minimum detected probability of accepting a group. This is also only needed if <kbd><span class="CodeInTextPACKT">ERGROUPING_ORIENTATION_ANY</span></kbd> is selected.</li>
</ul>
</li>
</ul>
<p>The code also provides a call to the second method, but it's commented out. You may switch between the two to test this out. Just comment the previous call and uncomment this one:</p>
<pre>erGrouping(input, channels, regions,  
    groups, groupRects, ERGROUPING_ORIENTATION_ANY,  
    "trained_classifier_erGrouping.xml", 0.5); </pre>
<p>For this call, we also used the default trained classifier that's provided in the text module sample package. Finally, we draw the region boxes and show the results:</p>
<pre>// draw groups boxes  
for (const auto&amp; rect : groupRects) 
    rectangle(input, rect, Scalar(0, 255, 0), 3); 
 
imshow("grouping",input); 
waitKey(0);</pre>
<p>This program outputs the following result:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-486 image-border" src="assets/a069ed42-9847-4e8d-a6e9-f662b1f80337.png" style="width:30.58em;height:31.00em;"/></div>
<p>You may check the entire source code in the <kbd><span class="CodeInTextPACKT">detection.cpp</span></kbd> file.</p>
<div class="packt_infobox">While most OpenCV text module functions are written to support both grayscale and colored images as its input parameter, at the time of writing this book, there were bugs preventing us from using grayscale images in functions such as <kbd><span class="CodeInTextPACKT">erGrouping</span></kbd><span>. For more information, take a look at the following GitHub link:</span> <span class="URLPACKT"><a href="https://github.com/Itseez/opencv_contrib/issues/309">https://github.com/Itseez/opencv_contrib/issues/309</a>.<a href="https://github.com/Itseez/opencv_contrib/issues/309"><br/></a>Always remember that the OpenCV contrib modules package is not as stable as the default OpenCV packages.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text extraction</h1>
                </header>
            
            <article>
                
<p>Now that we have detected the regions, we must crop the text before submitting it to the OCR. We could simply use a function like <kbd><span class="CodeInTextPACKT">getRectSubpix</span></kbd> or <kbd><span class="CodeInTextPACKT">Mat::copy</span></kbd>, using each region rectangle as a <strong>region of interest</strong> (<strong>ROI</strong>) but, since the letters are skewed, some undesired text may be cropped as well. For example, this is what one of the regions would look like if <span>we just e</span><span>xtract the ROI based on its given rectangle:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-487 image-border" src="assets/f227149d-3d30-4a84-9a38-4af84e3ead02.png" style="width:11.08em;height:4.75em;"/></div>
<p>Fortunately, <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd> provides us with an object called <kbd><span class="CodeInTextPACKT">ERStat</span></kbd>, which contains pixels inside each extremal region. With these pixels, we could use OpenCV's <kbd><span class="CodeInTextPACKT">floodFill</span></kbd> function to reconstruct each letter. This function is capable of painting similar colored pixels based on a seed point, just like the <strong>bucket</strong> tool of most drawing applications. This is what the function signature looks like:</p>
<pre>int floodFill(InputOutputArray image, InputOutputArray mask, <span> Point seedPoint, Scalar newVal, <br/> CV_OUT Rect* rect=0,</span> Scalar loDiff = Scalar(), Scalar upDiff = Scalar(), int flags = 4 ); </pre>
<p>Let's understand these parameters and how they will be used:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">image</span></kbd>: The input image. We'll use the channel image where the extremal region was taken. This is where the function normally does the flood fill, unless <kbd><span class="CodeInTextPACKT">FLOODFILL_MASK_ONLY</span></kbd> is supplied. In this case, the image remains untouched and the drawing occurs in the mask. That's exactly what we will do.</li>
<li><kbd><span class="CodeInTextPACKT">mask</span></kbd>: The mask must be an image with two rows and two columns greater than the input image. When the flood fill draws a pixel, it verifies if the corresponding pixel in the mask is zero. In that case, it will draw and mark this pixel as one (or another value that's passed into the flags). If the pixel is not zero, the flood fill does not paint the pixel. In our case, we'll provide a blank mask so that every letter will get painted in the mask.</li>
<li><kbd><span class="CodeInTextPACKT">seedPoint</span></kbd>: The starting point. It's similar to the place you click when you want to use the <strong>bucket</strong> tool of a graphic application.</li>
<li><kbd><span class="CodeInTextPACKT">newVal</span></kbd>: The new value of the repainted pixels.</li>
<li><kbd><span class="CodeInTextPACKT">loDiff</span></kbd> and <kbd><span class="CodeInTextPACKT">upDiff</span></kbd>: These parameters represent the lower and upper differences between the pixel being processed and its neighbors. The neighbor will be painted if it falls into this range. If the <kbd><span class="CodeInTextPACKT">FLOODFILL_FIXED_RANGE</span></kbd> flag is used, the difference between the seed point and the pixels being processed will be used instead.</li>
<li><kbd><span class="CodeInTextPACKT">rect</span></kbd>: This is an optional parameter that limits the region where the flood fill will be applied.</li>
<li><kbd><span class="CodeInTextPACKT">flags</span></kbd>: This value is represented by a bit mask:
<ul>
<li>The least significant 8 bits of the flag contains a connectivity value. A value of <kbd>4</kbd> indicates that all four edge pixels will be used, and a value of <kbd>8</kbd> will indicate that the diagonal pixels must also be taken into account. We'll use <kbd>4</kbd> for this parameter.</li>
<li>The next 8 to 16 bits contains a value from <kbd>1</kbd> to <kbd>255</kbd>, which is used to fill the mask. Since we want to fill the mask with white, we'll use <kbd>255 &lt;&lt; 8</kbd> for this value.</li>
<li>There are two more bits that can be set by adding the <kbd><span class="CodeInTextPACKT">FLOODFILL_FIXED_RANGE</span></kbd> and <kbd><span class="CodeInTextPACKT">FLOODFILL_MASK_ONLY</span></kbd> flags, as we already described.</li>
</ul>
</li>
</ul>
<p>We'll create a function called <kbd><span class="CodeInTextPACKT">drawER</span></kbd>. This function will receive four parameters:</p>
<ul>
<li>A vector with all of the processed channels</li>
<li>The <kbd><span class="CodeInTextPACKT">ERStat</span></kbd> region</li>
<li>The group that must be drawn</li>
<li>The group rectangle</li>
</ul>
<p>This function will return an image with the word represented by this group. Let's start this function by creating the mask image and defining the flags:</p>
<pre>Mat out = Mat::zeros(channels[0].rows+2, channels[0].cols+2, CV_8UC1); 
 
int flags = 4                    //4 neighbors 
   + (255 &lt;&lt; 8)                        //paint mask in white (255) 
   + FLOODFILL_FIXED_RANGE       //fixed range 
   + FLOODFILL_MASK_ONLY;        //Paint just the mask </pre>
<p>Then, we'll loop through each group. It's necessary to find the region index and its status. There's a chance of this extreme region being the root, which does not contain any points. In this case, we'll just ignore it:</p>
<pre>for (int g=0; g &lt; group.size(); g++) 
{ 
   int idx = group[g][0];         
   auto er = regions[idx][group[g][1]]; 
 
//Ignore root region 
   if (er.parent == NULL)  
         continue; </pre>
<p>Now, we can read the pixel coordinate from the <kbd><span class="CodeInTextPACKT">ERStat</span></kbd> object. It's represented by the pixel number, counting from top to bottom, left to right. This linear index must be converted to a row (<em>y</em>) and column (<em>z</em>) notation, using a formula similar to the one we saw in <a href="37cf2702-b8c6-41ff-a935-fd4030f8ce64.xhtml">Chapter 2</a>, <em>An Introduction to the Basics of OpenCV</em>:</p>
<pre>int px = er.pixel % channels[idx].cols; 
int py = er.pixel / channels[idx].cols; 
Point p(px, py); </pre>
<p>Then, we can call the <kbd>floodFill</kbd> function. The <kbd><span class="CodeInTextPACKT">ERStat</span></kbd> object gives us the value to use in the <kbd>loDiff</kbd> parameter:</p>
<pre>floodFill( 
    channels[idx], out,          //Image and mask 
    p, Scalar(255),              //Seed and color 
    nullptr,                     //No rect 
    Scalar(er.level),Scalar(0),  //LoDiff and upDiff 
    flags                        //Flags </pre>
<p>After we do this for all of the regions in the group, we'll end with an image that's a little bigger than the original one, with a black background and the word in white letters. Now, let's crop just the area of the letters. Since the region rectangle was given, we start by defining it as our region of interest:</p>
<pre>out = out(rect);</pre>
<p><span>Then, we'll find all non-zero pixels. This is the value we'll use in the <kbd>minAreaRect</kbd> function to get the rotated rectangle around the letters. Finally, we will borrow the previous chapter's</span> <span class="CodeInTextPACKT"><kbd>deskewAndCrop</kbd></span> function to crop and rotate the image for us:</p>
<pre>   vector&lt;Point&gt; points;    
   findNonZero(out, points); 
   //Use deskew and crop to crop it perfectly 
   return deskewAndCrop(out, minAreaRect(points)); 
} </pre>
<p>This is the result of the process for the easel image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-488 image-border" src="assets/ae3ff958-8639-4052-aa97-c6474f119b77.png" style="width:17.67em;height:8.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text recognition</h1>
                </header>
            
            <article>
                
<p>In <a href="31f3c15b-57fb-42c6-b67b-5552dfdfa3ac.xhtml">Chapter 10</a>, <em>Developing Segmentation Algorithms for Text Recognition,</em> we used the Tesseract API directly to recognize the text regions. This time, we'll use OpenCV classes to accomplish the same goal.</p>
<p>In OpenCV, all OCR-specific classes derive from the <strong><span class="CodeInTextPACKT">BaseOCR</span></strong> virtual class. This class provides a common interface for the OCR execution method itself. Specific implementations must inherit from this class. By default, the text module provides three different implementations: <span class="CodeInTextPACKT"><strong>OCRTesseract</strong>, <strong>OCRHMMDecoder</strong></span>, and <strong><span class="CodeInTextPACKT">OCRBeamSearchDecoder</span></strong>.</p>
<p>This hierarchy is depicted in the following class diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-489 image-border" src="assets/463bba5c-a8d2-4b38-b981-7c4d7e23b74f.png" style="width:38.58em;height:10.58em;"/></div>
<p>With this approach, we can separate the part of the code where the OCR mechanism is created from the execution itself. This makes it easier to change the OCR implementation in the future.</p>
<p>So, let's start by creating a method that decides which implementation we'll use based on a string. We currently support just Tesseract, but you may take a look in this chapter's code, where a demonstration with <strong>HMMDecoder</strong> is also provided. Also, we are accepting the OCR engine name in a string parameter, but we could improve our application's flexibility by reading it from an external JSON or XML configuration file:</p>
<pre>cv::Ptr&lt;BaseOCR&gt; initOCR2(const string&amp; ocr) { if (ocr == "tesseract") { return OCRTesseract::create(nullptr, "eng+por"); } throw string("Invalid OCR engine: ") + ocr; } </pre>
<p>As you may have noticed, the function returns <kbd><span class="CodeInTextPACKT">Ptr&lt;BaseOCR&gt;</span></kbd>. Now, take a look at the highlighted code. It calls the <kbd><span class="CodeInTextPACKT">create</span></kbd> method to initialize a Tesseract OCR instance. Let's take a look at its official signature, since it allows several specific parameters:</p>
<pre><span>Ptr&lt;OCRTesseract&gt; create(const char* datapath=NULL, <br/> const char* language=NULL, <br/> const char* char_whitelist=NULL, <br/> int oem=3, int psmode=3);</span> </pre>
<p>Let's dissect each of these parameters:</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">datapath</span></kbd>: This is the path to the root directory's <kbd><span class="CodeInTextPACKT">tessdata</span></kbd> files. The path must end with a backslash <kbd><span class="CodeInTextPACKT">/</span></kbd> character. The <kbd><span class="CodeInTextPACKT">tessdata</span></kbd> directory contains the language files you installed. Passing <kbd><span class="CodeInTextPACKT">nullptr</span></kbd> to this parameter will make Tesseract search in its installation directory, which is the location where this folder is normally present. It's common to change this value to <kbd><span class="CodeInTextPACKT">args[0]</span></kbd> when deploying an application and include the <kbd>tessdata</kbd> folder in your application path.</li>
<li><kbd><span class="CodeInTextPACKT">language</span></kbd>: This is a three letter word with the language code (for example, eng for English, por for Portuguese, or hin for Hindi). Tesseract supports the loading of multiple language codes by using the <kbd>+</kbd> sign. Therefore, passing <kbd>eng+por</kbd> will load both English and Portuguese languages. Of course, you can only use languages that you have previously installed, otherwise the loading will fail. A language <kbd>config</kbd> file may specify that two or more languages must be loaded together. To prevent that, you may use a tilde <kbd>~</kbd>. For example, you can use <kbd>hin+~eng</kbd> to guarantee that English is not loaded with Hindi, even if it is configured to do so.</li>
<li><kbd><span class="CodeInTextPACKT">whitelist</span></kbd>: This is the character that's set to be considered for recognition. In the case that <kbd>nullptr</kbd> is passed, the characters will be <kbd><span class="CodeInTextPACKT">0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ</span></kbd><span class="CodeInTextPACKT">.</span></li>
<li><kbd><span class="CodeInTextPACKT">oem</span></kbd>: These are the OCR algorithms that will be used. It can have one of the following values:
<ul>
<li><kbd><span class="CodeInTextPACKT">OEM_TESSERACT_ONLY</span></kbd>: Uses just Tesseract. It's the fastest method, but it also has less precision.</li>
<li><kbd><span class="CodeInTextPACKT">OEM_CUBE_ONLY</span></kbd>: Uses Cube engine. It's slower, but more precise. This will only work if your language was trained to support this engine mode. To check if that's the case, look for <kbd>.cube</kbd> files for your language in the <kbd>tessdata</kbd> folder. The support for English language is guaranteed.</li>
<li><kbd><span class="CodeInTextPACKT">OEM_TESSERACT_CUBE_COMBINED</span></kbd>: Combines both Tesseract and Cube to achieve the best possible OCR classification. This engine has the best accuracy and the slowest execution time.</li>
<li><span class="CodeInTextPACKT"><kbd>OEM_DEFAULT</kbd></span>: Infers the strategy based in the language config file, command-line config file or, in the absence of both, use <kbd><span class="CodeInTextPACKT">OEM_TESSERACT_ONLY</span></kbd>.</li>
</ul>
</li>
<li><kbd><span class="CodeInTextPACKT">psmode</span></kbd>: This is the segmentation mode. It can be any of the following:
<ul>
<li><kbd><span class="CodeInTextPACKT">PSM_OSD_ONLY</span>:</kbd> Using this mode, Tesseract will just run its preprocessing algorithms to detect orientation and script detection.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_AUTO_OSD</span></kbd>: This tells Tesseract to do automatic page segmentation with orientation and script detection.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_AUTO_ONLY</span></kbd>: Does page segmentation, but avoids doing orientation, script detection, or OCR. This is the default value.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_AUTO</span></kbd>: Does page segmentation and OCR, but avoids doing orientation or script detection.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_COLUMN</span></kbd>: Assumes that the text of variable sizes is displayed in a single column.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_BLOCK_VERT_TEXT</span></kbd>: Treats the image as a single uniform block of vertically aligned text.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_BLOCK</span></kbd>: Assumes a single block of text. This is the default configuration. We will use this flag since our preprocessing phase guarantees this condition.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_LINE</span></kbd>: Indicates that the image contains only one line of text.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_WORD</span></kbd>: Indicates that the image contains just one word.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_WORD_CIRCLE</span></kbd>: Indicates that the image is a just one word disposed in a circle.</li>
<li><kbd><span class="CodeInTextPACKT">PSM_SINGLE_CHAR</span></kbd>: Indicates that the image contains a single character.</li>
</ul>
</li>
</ul>
<p>For the last two parameters, it's recommended that you use the <kbd><span class="CodeInTextPACKT">#include</span></kbd> Tesseract directory to use the constant names instead of directly inserting their values. The last step is to add text detection in our main function. To do this, just add the following code to the end of the main method:</p>
<pre>auto ocr = initOCR("tesseract"); <br/>for (int i = 0; i &lt; groups.size(); i++)  <br/>{ <br/>     auto wordImage = drawER(channels, regions, groups[i],  <br/>     groupRects[i]); 
 <br/>     string word; 
     ocr-&gt;run(wordImage, word); 
     cout &lt;&lt; word &lt;&lt; endl; <br/>}</pre>
<p>In this code, we started by calling our <kbd><span class="CodeInTextPACKT">initOCR</span></kbd> method to create a Tesseract instance. Note that the remaining code will not change if we chose a different OCR engine, since the <span class="CodeInTextPACKT">run</span> method signature is guaranteed by the <kbd>BaseOCR</kbd> class. Next, we iterate over each detected <kbd><span class="CodeInTextPACKT">ERFilter</span></kbd> group. Since each group represents a different word, we will do the following:</p>
<ol>
<li>Call the previously created <kbd><span class="CodeInTextPACKT">drawER</span></kbd> function to create an image with the word.</li>
<li>Create a text string called <kbd><span class="CodeInTextPACKT">word</span></kbd>, and call the <kbd><span class="CodeInTextPACKT">run</span></kbd> function to recognize the word image. The recognized word will be stored in the string.</li>
<li>Print the text string in the screen.</li>
</ol>
<p>Let's take a look at the <kbd>run</kbd> method signature. This method is defined in the <kbd>BaseOCR</kbd> class, and will be equal for all specific OCR implementations ‚Äì even the ones that might be implemented in the future:</p>
<pre><span>virtual void run(Mat&amp; image, std::string&amp; output_text, <br/> std::vector&lt;Rect&gt;* component_rects=NULL, <br/> std::vector&lt;std::string&gt;* component_texts=NULL, <br/> std::vector&lt;float&gt;* component_confidences=NULL,</span> int component_level=0) = 0; </pre>
<p>Of course, this is a pure virtual function that must be implemented by each specific class (such as the <kbd><span class="CodeInTextPACKT">OCRTesseract</span></kbd> class we just used):</p>
<ul>
<li><kbd><span class="CodeInTextPACKT">image</span></kbd>: The input image. It must be a RGB or a grayscale image.</li>
<li><kbd><span class="CodeInTextPACKT">component_rects</span></kbd>: We can provide a vector to be filled with the bounding box of each component (words or text lines) that's detected by the OCR engine.</li>
<li><kbd><span class="CodeInTextPACKT">component_texts</span></kbd>: If given, this vector will be filled with the text strings of each component detected by the OCR.</li>
<li><kbd><span class="CodeInTextPACKT">component_confidences</span></kbd>: If given, the vector will be filled with floats, with the confidence values of each component.</li>
<li><kbd><span class="CodeInTextPACKT">component_level</span></kbd>: Defines what a component is. It may have the values <kbd><span class="CodeInTextPACKT">OCR_LEVEL_WORD</span></kbd> (by default), or <kbd><span class="CodeInTextPACKT">OCR_LEVEL_TEXT_LINE</span></kbd>.</li>
</ul>
<div class="packt_tip">If necessary, you may prefer changing the component level to a word or line in the <kbd><span class="CodeInTextPACKT">run()</span></kbd> method instead of doing the same thing in the <kbd>psmode</kbd> parameter of the <kbd><span class="CodeInTextPACKT">create()</span></kbd> function. This is preferable since the <kbd>run</kbd> method will be supported by any OCR engine that decides to implement the <kbd><span class="CodeInTextPACKT">BaseOCR</span></kbd> class. Always remember that the <kbd><span class="CodeInTextPACKT">create()</span></kbd> method is where vendor-specific configurations are set.</div>
<p>This is the program's final output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-490 image-border" src="assets/2899645e-f4e4-416c-95f9-eca150d92aa0.png" style="width:32.33em;height:13.33em;"/></div>
<p>Despite a minor confusion with the <kbd><span class="CodeInTextPACKT">&amp;</span></kbd> symbol, every word was perfectly recognized. You may check the entire source code in the <kbd><span class="CodeInTextPACKT">ocr.cpp</span></kbd> file, in this chapter's code file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw that scene text recognition is a far more difficult OCR situation than working with scanned texts. We studied how the text module addresses this problem with extremal region identification using the <em>Newmann</em> and <em>Matas</em> algorithm. We also saw how to use this API with the <kbd>floodFill</kbd> function to extract the text in to an image and submit it to Tesseract OCR. Finally, we studied how the OpenCV text module integrates with Tesseract and other OCR engines, and how can we use its classes to identify what's written in an image.</p>
<p>In the next chapter, you will be introduced to deep learning in OpenCV. You will <span>learn about object detection and classification by using the</span> <strong>y<span>ou only look once</span></strong> <span>(</span><strong>YOLO</strong><span>) algorithm.</span></p>


            </article>

            
        </section>
    </body></html>