<html><head></head><body>
		<div id="_idContainer134">
			<h1 id="_idParaDest-88"><em class="italic"><a id="_idTextAnchor090"/>Chapter 5</em>: Interpreting Results</h1>
			<p>As we have seen throughout the previous chapters, Elastic ML creates extremely useful analysis as regards both anomaly detection and forecasting. But, up until this point, we've only looked at the results created by Elastic ML in a relatively superficial way. In this chapter, we will go deeper into learning about the results that are created, how they are stored, and how you can leverage those results in different ways to bring additional insight.</p>
			<p>Specifically, this chapter will cover the following topics:</p>
			<ul>
				<li>Viewing the Elastic ML results index</li>
				<li>Anomaly scores</li>
				<li>Results index schema details</li>
				<li>Multi-bucket anomalies</li>
				<li>Forecast results</li>
				<li>Results API</li>
				<li>Custom dashboards and Canvas workpads</li>
			</ul>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor091"/>Technical requirements</h1>
			<p>The information in this chapter is based on the Elastic Stack as it exists in v7.10. </p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor092"/>Viewing the Elastic ML results index</h1>
			<p>As we go through much of the discussion of how users should interpret the results from Elastic ML's <a id="_idIndexMarker313"/>anomaly detection jobs, it will be helpful to relate what is conveyed with how that information is stored within Elastic ML's internal results index. To get a quick initial peek into that index, you can either query the index pattern directly using the <strong class="source-inline">_search</strong> API in Elasticsearch, or perhaps more intuitively, add the index pattern to Kibana and view the index with native Kibana tools. In order to do this, we must first use the following procedure to expose Elastic ML's internal results index to Kibana:</p>
			<ol>
				<li>In Kibana, click on <a id="_idIndexMarker314"/>the side menu and then select <strong class="bold">Stack Management</strong> from the list:<div id="_idContainer094" class="IMG---Figure"><img src="image/B17040_05_1.jpg" alt="Figure 5.1 – Selecting Stack Management&#13;&#10;"/></div><p class="figure-caption">Figure 5.1 – Selecting Stack Management</p></li>
				<li>Select <strong class="bold">Index Patterns</strong>:<div id="_idContainer095" class="IMG---Figure"><img src="image/B17040_05_2.jpg" alt="Figure 5.2 – Selecting Index Patterns&#13;&#10;"/></div><p class="figure-caption">Figure 5.2 – Selecting Index Patterns</p></li>
				<li>Select <strong class="bold">Create index pattern</strong>:<div id="_idContainer096" class="IMG---Figure"><img src="image/B17040_05_3.jpg" alt="Figure 5.3 – Selecting the Create index pattern button&#13;&#10;"/></div><p class="figure-caption">Figure 5.3 – Selecting the Create index pattern button</p></li>
				<li>Enter <strong class="source-inline">.ml-anomalies-*</strong> for <a id="_idIndexMarker315"/>the <strong class="bold">Index pattern name</strong> and then toggle the <strong class="bold">Include system and hidden indices switch to on</strong>. Then, click the <strong class="bold">Next step</strong> button:<div id="_idContainer097" class="IMG---Figure"><img src="image/B17040_05_4.jpg" alt="Figure 5.4 – Naming the index pattern&#13;&#10;"/></div><p class="figure-caption">Figure 5.4 – Naming the index pattern</p></li>
				<li>Choose <strong class="screen-inline">timestamp</strong> for <strong class="bold">Time field</strong> and then click the <strong class="bold">Create index pattern</strong> button:<div id="_idContainer098" class="IMG---Figure"><img src="image/B17040_05_5.jpg" alt="Figure 5.5 – Defining the time field&#13;&#10;"/></div><p class="figure-caption">Figure 5.5 – Defining the time field</p></li>
				<li>Confirm <a id="_idIndexMarker316"/>that the index pattern is defined:</li>
			</ol>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B17040_05_6.jpg" alt="Figure 5.6 – Confirming that the index pattern is defined&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Confirming that the index pattern is defined</p>
			<p>Now that <a id="_idIndexMarker317"/>the index pattern for <strong class="source-inline">.ml-anomalies-*</strong> is defined, we can use Kibana's Discover to explore the contents of the results index (select <strong class="bold">Discover</strong> from the main Kibana menu):</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B17040_05_7.jpg" alt="Figure 5.7 – Viewing the results index in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Viewing the results index in Kibana Discover</p>
			<p>Now that <a id="_idIndexMarker318"/>we're able to view the results index in Kibana Discover, we can use Discover's search and filter capabilities to explore the results in any way that we want. For example, you could retrieve all record-level anomalies for a certain anomaly detection job name where the record's anomaly score is more than a certain value:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B17040_05_8.jpg" alt="Figure 5.8 – Using Kibana Discover to search and filter anomalies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Using Kibana Discover to search and filter anomalies</p>
			<p>The syntax of this query in KQL is as follows:</p>
			<p class="source-code">job_id:"web_traffic_per_country" and result_type:"record" and record_score&gt;90</p>
			<p>Here, we can <a id="_idIndexMarker319"/>see two specific occurrences that matched our query. There is a plethora of information in the results index, and we will systematically learn to decipher the bulk of that information throughout this chapter. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">While it is safe to look at and query the results within the <strong class="source-inline">.ml-anomalies-*</strong> index pattern, we should remember that the indices matched by this index pattern are system indices and it is unwise to attempt to manually modify or delete the contents of these indices.</p>
			<p>The first concept to understand is that there are different kinds of results (hence the <strong class="source-inline">result_type</strong> field) as well as different kinds of scores that reflect the analysis from different angles or levels. As such, let's start with a better understanding of the different kinds of scoring and how those scores are calculated and stored within the results index.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor093"/>Anomaly scores</h1>
			<p>Interpreting the results of Elastic ML's anomaly detection jobs first requires the ability to recognize the <a id="_idIndexMarker320"/>fact that there are several levels of scoring unusualness, expressed within the results. They are as follows:</p>
			<ul>
				<li><strong class="bold">Bucket-level</strong> (<strong class="source-inline">result_type:bucket</strong>): This level summarizes the results of the entirety of the anomaly detection job per time bucket. Essentially, it is a representation of how unusual that time bucket is, given the configuration of your job. </li>
				<li><strong class="bold">Influencer-level</strong> (<strong class="source-inline">result_type:influencer</strong>): This is used to better understand the most unusual entities (influencers) within a timespan.</li>
				<li><strong class="bold">Record-level</strong> (<strong class="source-inline">result_type:record</strong>): This is the most detailed information regarding every anomalous occurrence or anomalous entity within a time bucket. Again, depending on the job configuration (multiple detectors, splits, and so on), there can be many record-level documents per time bucket.</li>
			</ul>
			<p>Additionally, to fully appreciate how scoring is done, we also need to fully understand the following concepts:</p>
			<ul>
				<li><strong class="bold">Normalization</strong>: The concept of projecting anomalousness onto a fixed scale between 0 and 100.</li>
				<li><strong class="bold">Influencers</strong>: Entities that induce the creation of anomalies via their influential contribution to the dataset at the time that the anomaly occurs.</li>
			</ul>
			<p>Let's investigate each of these five concepts in more depth in this section.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>Bucket-level scoring</h2>
			<p>An anomaly <a id="_idIndexMarker321"/>score at the bucket level is akin to answering the question, "How unusual was this interval of time, relative to all other intervals of time for this job?", where that interval is defined by the anomaly detection job's <strong class="source-inline">bucket_span</strong>. If your job has multiple detectors or splits in the analysis resulting in results for possibly many entities simultaneously, then each bucket-level result is an aggregated representation of all of those things. The bucket-level anomaly score is viewable in a few ways, the first being the <strong class="bold">Overall</strong> swim lane at the top in the Anomaly Explorer UI:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B17040_05_9.jpg" alt="Figure 5.9 – The swim lanes of the Anomaly Explorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – The swim lanes of the Anomaly Explorer</p>
			<p>Here, we see that the <strong class="bold">Overall</strong> swim lane contains a variety of severity colors/scores, with the <a id="_idIndexMarker322"/>particular one highlighted showing a <strong class="screen-inline">Max anomaly score</strong> of <strong class="screen-inline">88</strong>. It is important to note here that the time range being shown in this view encompasses data from January 6th through February 5th; therefore, there are about 30 "tiles" horizontally in the swim lanes, with each one representing one day. The anomaly detection job was configured to have a bucket span of 15 minutes, so each tile shows the maximum score from the entire day. If we were to zoom the display to only one day using Kibana's time picker (the date/time range control near the top-right corner of the screen), we would see more detail, specifically, that the bucket-level anomalies in the <strong class="bold">Overall</strong> swim lane occurred between 02:00 A.M. and 02:30 A.M.:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B17040_05_10.jpg" alt="Figure 5.10 – The swim lanes of the Anomaly Explorer, after zooming in&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – The swim lanes of the Anomaly Explorer, after zooming in</p>
			<p>It is also important to note here how the <strong class="bold">Overall</strong> swim lane relates (or doesn't relate) to the grid <a id="_idIndexMarker323"/>of swim lanes below it. This grid of swim lanes shows influencer-level scoring (discussed in the next section) and is therefore not directly related to the bucket-level scoring. This is a common misconception, as many people think that the <strong class="bold">Overall</strong> swim lane is some type of combination (for example, a maximum score) of the columns from the grid below it. You can certainly see in <em class="italic">Figure 5.10</em> that this is obviously not true, as the group of influencer-level scores in the second row of the grid (around 07:00 A.M.) have no corresponding score at the <strong class="bold">Overall</strong> (bucket) level. Why is this? The short answer is that the <strong class="bold">Overall</strong> swim lane is a comparison of time buckets against one another. Therefore, the most unusual time buckets get the highest scores, and time buckets that are a lot less unusual (due to the number and severity of individual anomalies within that time bucket) get smaller scores, or even no score at all. The process of determining this relative scoring is called <strong class="bold">normalization</strong>. It is an important part of the scoring at all levels and deserves some independent explanation.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor095"/>Normalization</h2>
			<p>As first <a id="_idIndexMarker324"/>introduced in <a href="B17040_01_Epub_AM.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Machine Learning for IT</em>, we saw that raw probability values for specific anomalies are normalized on a scale from 0 to 100. This process is what allows there to be a relative ranking of anomalousness, while also bounding the values to a fixed interval of values that become useful in assessing severity for the purposes of triage and/or alerting.</p>
			<p>The key aspect in the last sentence is this notion of relative ranking. In other words, the normalized values take into account things that have been seen by the anomaly detection job <em class="italic">so far</em> and rank them accordingly. This also means that previously assigned normalized scores may change over time as new anomalies are discovered. As such, you will notice that scores within the results index have both an "initial" value and the current value, for example:</p>
			<ul>
				<li><strong class="source-inline">initial_anomaly_score</strong>: The bucket-level anomaly score that was recorded at the time the anomaly was created</li>
				<li><strong class="source-inline">anomaly_score</strong>: The current bucket-level normalized anomaly score</li>
			</ul>
			<p>These two values may be the same but may indeed diverge over time. The initial score is a fixed value, but the current score may be adjusted as additional, perhaps as more egregious anomalies <a id="_idIndexMarker325"/>are encountered over time. The normalization process happens every few hours during real-time operation, or spontaneously if the analytics detect drastic changes in the normalization table. It is also done if the anomaly detection job is <em class="italic">closed</em> (put into the closed state). Normalization may rescore anomalies as far back in time as whatever the <strong class="source-inline">renormalization_window_days</strong> setting is configured to (30 days or 100 bucket spans is the default value, and the value is only changeable if the job is created with the API or the Advanced job wizard by directly modifying the job's configuration JSON).</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor096"/>Influencer-level scoring</h2>
			<p>An anomaly score at the influencer level is akin to answering the question, "What are the most <a id="_idIndexMarker326"/>unusual entities during this time?", where we are now ranking these entities against one another. The influencer-level scores are viewable in a few ways, the first being the main grid of swim lanes in the middle of the Anomaly Explorer UI, and the second being the <strong class="bold">Top influencers</strong> list along the left-hand side:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B17040_05_11.jpg" alt="Figure 5.11 – Influencers in the Anomaly Explorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – Influencers in the Anomaly Explorer</p>
			<p>Here, we see that in the main grid of swim lanes, the country codes of the <strong class="source-inline">geo.src</strong> field are listed in decreasing total influencer score. Notice that despite the grid being set to show 10 rows per page, only six anomalous country codes are listed (there are no more that have <a id="_idIndexMarker327"/>significant influencer scores during this time period). Also, the top influencers are listed on the left-hand side, showing for each entity both the maximum influencer score (99 for <strong class="source-inline">geo.src:IN</strong>) as well as the sum of all influencer scores for this time range (223 for <strong class="source-inline">geo.src:IN</strong>). In this case, since there is only one influencer defined for this job, so this information may seem redundant. However, many jobs have more than one influencer defined, so the view becomes more sensible in that case. For example, if we look at a population analysis job on the <strong class="source-inline">kibana_sample_data_logs</strong> index in which we choose <strong class="source-inline">distinct_count("url.keyword") over clientip</strong> as the detector and choose both <strong class="source-inline">clientip</strong> and <strong class="source-inline">response.keyword</strong> as influencers, the Anomaly Explorer view could look like this:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B17040_05_12.jpg" alt="Figure 5.12 – Multiple influencers in the Anomaly Explorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Multiple influencers in the Anomaly Explorer</p>
			<p>Notice <a id="_idIndexMarker328"/>that the grid's <strong class="bold">View by</strong> control is selected for <strong class="screen-inline">clientip</strong>, but the <strong class="bold">Top influencers</strong> list on the left show both influencer lists.</p>
			<p>The Anomaly Explorer is interactive, so if we select the critical anomaly tile in the <strong class="bold">Overall</strong> swim lane for the day of February 19th, the influencer grid and lists change as the filter for that period is implicitly applied:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17040_05_13.jpg" alt="Figure 5.13 – Anomaly Explorer filtered for a specific day&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13 – Anomaly Explorer filtered for a specific day</p>
			<p>We now <a id="_idIndexMarker329"/>see only the relevant entities for the selected day. Now that we've gotten a little bit of a sense of what influencers are, you may ask what fields are good candidates for influencers if this is how they can be represented? Let's take a quick detour to discuss influencers in more depth.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>Influencers</h2>
			<p>Within the anomaly detection job configuration, there is the ability to define fields as an influencer. The concept of an influencer is a field that describes an entity for which you'd like to know whether it is to <a id="_idIndexMarker330"/>blame for the existence of the anomaly, or at least whether it had a significant contribution. Note that any field chosen as a candidate to be an influencer doesn't need to be part of the detection logic, although it is natural to pick fields that are used as splits or populations to also be influencers. </p>
			<p>If we revisit the example shown in <em class="italic">Figure 5.13</em>, we see that both the <strong class="source-inline">clientip</strong> and the <strong class="source-inline">response.keyword</strong> fields were declared as influencers for the job (where <strong class="source-inline">clientip</strong> was part of the detector configuration, but <strong class="source-inline">response.keyword</strong> was not). The client IP address of <strong class="source-inline">30.156.16.164</strong> is identified as a top influencer. This seems a bit of a redundant declaration, because the anomaly was for that client IP – but this is an expected situation when influencers are chosen for the fields that define the population or are the split fields. The other top influencer (<strong class="source-inline">response.keyword</strong>) has a value of <strong class="source-inline">404</strong>. This particular piece of information is extremely relevant in that it gives the user an immediate clue of whatever the <strong class="source-inline">30.156.16.164</strong> IP address was doing during the anomaly. If we investigate the anomalous IP address at the time of the anomaly, we will see that 100% of the requests made resulted in a response code of <strong class="source-inline">404</strong>:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17040_05_14.jpg" alt="Figure 5.14 – The influencer field value of 404 dominates the results at the time of the anomaly&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14 – The influencer field value of 404 dominates the results at the time of the anomaly</p>
			<p>As such, the value of <strong class="source-inline">404</strong> has a high influencer score (<strong class="screen-inline">50</strong>, as shown in <em class="italic">Figure 5.13</em>). You may think that because 100% of the requests were <strong class="source-inline">404</strong>, the influencer score should also be 100, but it is <a id="_idIndexMarker331"/>not that simple. The influencer scores are normalized against other influencer scores and the influencer score is also expressing how unusual the value of <strong class="source-inline">404</strong> has been over time. In this specific example dataset, there are hundreds more occurrences of <strong class="source-inline">404</strong> over time, but most of those have not been associated with anomalies. As such, the influencer score for this particular anomaly is tempered by that fact. There may be a compelling argument for Elastic ML to separate these two concepts – one score that expresses the unusualness of the entity over time, and another score for how much a field value influences a particular anomaly – but for the time being, those notions are blended into the influencer score.</p>
			<p>It is also key to understand that the process of finding potential influencers happens after Elastic ML finds the anomaly. In other words, it does not affect any of the probability calculations that are made as part of the detection. Once the anomaly has been determined, ML will systematically go through all instances of each candidate influencer field and remove that instance's contribution to the data in that time bucket. If, once removed, the remaining <a id="_idIndexMarker332"/>data is no longer anomalous, then via counterfactual reasoning, that instance's contribution must have been influential and is scored accordingly (with an <strong class="source-inline">influencer_score</strong> in the results).</p>
			<p>Influencers can become a very powerful thing to leverage when viewing the results of not just a single ML job, but potentially several related jobs. In <a href="B17040_07_Epub_AM.xhtml#_idTextAnchor131"><em class="italic">Chapter 7</em></a>, <em class="italic">AIOps and Root Cause Analysis</em>, we'll see how to effectively use influencers to assist with root cause analysis.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/>Record-level scoring</h2>
			<p>An anomaly score at the record level is the lowest level of abstraction in the results and contains the <a id="_idIndexMarker333"/>most amount of detail. In the Anomaly Explorer UI, the record-level results are shown in the table at the bottom:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B17040_05_15.jpg" alt="Figure 5.15 – Anomaly table showing record-level results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15 – Anomaly table showing record-level results</p>
			<p>Notice that if the <strong class="bold">Interval</strong> selector is set to <strong class="bold">Auto</strong>, then any anomalies that are adjacent serially in time will be collapsed such that only the highest score anomaly is shown. Setting the <strong class="bold">Interval</strong> field to <strong class="bold">Show all</strong> will reveal each individual anomaly, if desired.</p>
			<p>A common misconception is that the record-level anomaly score is directly related to the deviation articulated in the <strong class="bold">description</strong> column of the UI (here <strong class="screen-inline">41x higher</strong>). The score is purely driven by the probability calculation, using the same normalization process that <a id="_idIndexMarker334"/>was described earlier. The <strong class="bold">description</strong> field, and even the <strong class="bold">typical</strong> value, are simplified bits of contextual information to make the anomaly easier to understand. In fact, as you'll see later, the <strong class="bold">description</strong> field isn't stored in the results index – it is only calculated on the fly in Kibana.</p>
			<p>When looking at these different level anomaly records in the <strong class="source-inline">.ml-anomalies-*</strong> index, we can see that many fields are there for our use. Some may be obvious, and some may not be. In the next section, we'll systematically go through the schema of the results index and will describe the meaning of the important fields.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor099"/>Results index schema details</h1>
			<p>As we have already hinted, inside the results index, there are a variety of different documents, each with their own usefulness with respect to understanding the results of the anomaly <a id="_idIndexMarker335"/>detection jobs. The ones we will discuss in this section are the ones that directly relate to the three levels of abstraction that we discussed previously in this chapter. They are aptly named as follows:</p>
			<ul>
				<li><strong class="source-inline">result_type:bucket</strong>: To give bucket-level results </li>
				<li><strong class="source-inline">result_type:record</strong>: To give record-level results </li>
				<li><strong class="source-inline">result_type:influencer</strong>: To give influencer-level results</li>
			</ul>
			<p>The distribution of these document types will depend on the ML job configuration and the characteristics of the dataset being analyzed. These document types are written with the following heuristic:</p>
			<ul>
				<li><strong class="source-inline">result_type:bucket</strong>: One document is written for every bucket span's worth of time. In other words, if the bucket span is 15 minutes, then there will be one document of this type being written every 15 minutes. Its timestamp will be equal to the leading edge of the bucket. For example, for the time bucket that encompasses the range between 11:30 and 11:45, the result document of this type will have a timestamp of 11:30.</li>
				<li><strong class="source-inline">result_type:record</strong>: One document is written for every occurrence of an anomaly within a time bucket. Therefore, with big datasets encompassing many entities (IP addresses, hostnames, and so on), a particular bucket of time could have hundreds or even thousands of anomaly records in a bucket during a major anomalous event or widespread outage. This document will also have a timestamp equal to the leading edge of the bucket.</li>
				<li><strong class="source-inline">result_type:influencer</strong>: One document is written for every influencer that is found for each anomaly record. Because there can potentially be more than one influencer type found for each anomaly record, this type of document can be even more voluminous than record results. This document will also have a timestamp that is equal to the leading edge of the bucket.</li>
			</ul>
			<p>Understanding the <a id="_idIndexMarker336"/>fields within these document types is especially important when we get to <a href="B17040_06_Epub_AM.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Alerting on ML Analysis</em>, because there will inevitably be a balance between alert detail (usually, more is preferable to less) and the number of individual alerts per unit of time (usually, less is preferable to more). We will revisit this when we start writing actual alerts.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor100"/>Bucket results</h2>
			<p>At the highest level of abstraction are the results at the bucket level. Remember that this is the <a id="_idIndexMarker337"/>aggregated results for the entire job as a function of time and essentially answers the question, "How unusual was this bucket of time?"</p>
			<p>Let's look at an example document in the <strong class="source-inline">.ml-anomalies-*</strong> index by using Kibana Discover and issuing the following KQL query:</p>
			<p class="source-code">result_type :"bucket" and anomaly_score &gt;98</p>
			<p>This will yield the following output:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17040_05_16.jpg" alt="Figure 5.16 – Bucket-level result document as seen in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – Bucket-level result document as seen in Kibana Discover</p>
			<p>Clicking on <a id="_idIndexMarker338"/>the <strong class="bold">&gt;</strong> icon next to the timestamp for the document in <em class="italic">Figure 5.16</em> will expand it so that you can see all of the details: </p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B17040_05_17.jpg" alt="Figure 5.17 – Bucket-level document detail in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17 – Bucket-level document detail in Kibana Discover</p>
			<p>You can see that just one bucket-level document was returned from our query in <em class="italic">Figure 5.16</em>, a single <a id="_idIndexMarker339"/>anomalous time bucket (at timestamp <strong class="screen-inline">1613824200000</strong>, or in my time zone, February 20, 2021, 07:30:00 A.M. GMT-05:00) that has an <strong class="screen-inline">anomaly_score</strong> greater than 98. In other words, there were no other time buckets with anomalies that big in this time range. Let's look at the key fields:</p>
			<ul>
				<li><strong class="source-inline">timestamp</strong>: The timestamp of the leading edge of the time bucket. In Kibana, this field will be displayed by default in your local time zone (although it is stored in the index in epoch format with the time zone of UTC).</li>
				<li><strong class="source-inline">anomaly_score</strong>: The current normalized score of the bucket, based upon the range of probabilities seen over the entirety of the job. The value of this score may fluctuate over time as new data is processed by the job and new anomalies are found.</li>
				<li><strong class="source-inline">initial_anomaly_score</strong>: The normalized score of the bucket, that is, when that bucket was first analyzed by the analytics. This score, unlike <strong class="source-inline">anomaly_score</strong>, will not change as more data is analyzed.</li>
				<li><strong class="source-inline">event_count</strong>: The number of raw Elasticsearch documents seen by the ML algorithms during the bucket's span.</li>
				<li><strong class="source-inline">is_interim</strong>: A flag that signifies whether the bucket is finalized or whether is still waiting for all of the data within the bucket span to be received. This field is relevant for ongoing jobs that are operating in real time. For certain <a id="_idIndexMarker340"/>types of analysis, there could be interim results, even though not all of the data for the bucket has been seen.</li>
				<li><strong class="source-inline">job_id</strong>: The name of the anomaly detection job that created this result.</li>
				<li><strong class="source-inline">processing_time_ms</strong>: An internal performance measurement of how much processing time (in milliseconds) the analytics took to process this bucket's worth of data.</li>
				<li><strong class="source-inline">bucket_influencers</strong>: An array of influencers (and details on them) that have been identified for this current bucket. Even if no influencers have been chosen as part of the job configuration, or there are no influencers as part of the analysis, there will always be a default influencer of the <strong class="source-inline">influencer_field_name:bucket_time</strong> type, which is mostly an internal record-keeping device to allow for the ordering of bucket-level anomalies in cases where explicit influencers cannot be determined.</li>
			</ul>
			<p>If a job does have named and identified influencers, then the <strong class="source-inline">bucket_influencers</strong> array may look like what is shown in <em class="italic">Figure 5.17</em>. </p>
			<p>Notice that in addition to the default entry of the <strong class="source-inline">influencer_field_name:bucket_time</strong> type, in this case, there is an entry for a field name of an analytics-identified influencer for the <strong class="source-inline">geo.src</strong> field. This is a cue that <strong class="source-inline">geo.src</strong> was a relevant influencer type that was discovered at the time of this anomaly. Since multiple influencer candidates can be chosen in <a id="_idIndexMarker341"/>the job configuration, it should be noted that in this case, <strong class="source-inline">geo.src</strong> is the only influencer field and no other fields were found to be influential. It should also be noted that, at this level of detail, the particular instance of <strong class="source-inline">geo.src</strong> (that is, which one) is not disclosed; that information will be disclosed when querying at the lower levels of abstraction, which we will discuss next.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Record results</h2>
			<p>At a lower level of abstraction, there are results at the record level. Giving the most amount of detail, record results show specific instances of anomalies and essentially answer the question, "What entity was unusual and by how much?" </p>
			<p>Let's look <a id="_idIndexMarker342"/>at an example document in the <strong class="source-inline">.ml-anomalies-*</strong> index by using Kibana Discover and issuing the following KQL query:</p>
			<p class="source-code">result_type :"record" and record_score &gt;98</p>
			<p>This will result in something similar to this:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17040_05_18.jpg" alt="Figure 5.18 – Record-level result document as seen in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.18 – Record-level result document as seen in Kibana Discover</p>
			<p>Clicking <a id="_idIndexMarker343"/>on the <strong class="bold">&gt;</strong> icon next to the timestamp for the document will expand it so that you are able to see all of the details: </p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17040_05_19.jpg" alt="Figure 5.19 – Record-level document detail in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.19 – Record-level document detail in Kibana Discover</p>
			<p>You can <a id="_idIndexMarker344"/>see that a few bucket-level documents were returned from our query in <em class="italic">Figure 5.18</em>. Let's look at the key fields:</p>
			<ul>
				<li><strong class="source-inline">timestamp</strong>: The timestamp of the leading edge of the time bucket, inside which this anomaly occurred. This is similar as explained earlier.</li>
				<li><strong class="source-inline">job_id</strong>: The name of the anomaly detection job that created this result.</li>
				<li><strong class="source-inline">record_score</strong>: The current normalized score of the anomaly record, based upon the range of the probabilities seen over the entirety of the job. The value of this score may fluctuate over time as new data is processed by the job and new anomalies are found.</li>
				<li><strong class="source-inline">initial_record_score</strong>: The normalized score of the anomaly record, that is, when that bucket was first analyzed by the analytics. This score, unlike <strong class="source-inline">record_score</strong>, will not change as more data is analyzed.</li>
				<li><strong class="source-inline">detector_index</strong>: An internal counter to keep track of the detector configuration that this anomaly belongs to. Obviously, with a single-detector job, this value will be zero, but it may be non-zero in jobs with multiple detectors.</li>
				<li><strong class="source-inline">function</strong>: A reference to keep track of which detector function was used for the creation of this anomaly.</li>
				<li><strong class="source-inline">is_interim</strong>: A flag that signifies whether or not the bucket is finalized or whether the bucket is still waiting for all of the data within the bucket span to be received. This field is relevant for ongoing jobs that are operating in real time. For certain <a id="_idIndexMarker345"/>types of analysis, there could be interim results, even though not all of the data for the bucket has been seen.</li>
				<li><strong class="source-inline">actual</strong>: The actual observed value of the analyzed data in this bucket. For example, if the function is <strong class="source-inline">count</strong>, then this represents the number of documents that are encountered (and counted) in this time bucket.</li>
				<li><strong class="source-inline">typical</strong>: A representation of the expected or predicted value based upon the ML model for this dataset.</li>
				<li><strong class="source-inline">multi_bucket_impact</strong>: A measurement (on a scale from -5 to +5) that determines how much this particular anomaly was influenced by the secondary multi-bucket analysis (explained later in the chapter), from no influence (-5) to all influence (+5).</li>
				<li><strong class="source-inline">influencers</strong>: An array of which influencers (and the values of those influencers) are relevant to this anomaly record.</li>
			</ul>
			<p>If a job has splits defined (either with <strong class="source-inline">by_field_name</strong> and/or <strong class="source-inline">partition_field_name</strong>) and identified influencers, then the record results documents will have more information, such as what is seen in <em class="italic">Figure 5.19</em>:</p>
			<ul>
				<li><strong class="source-inline">partition_field_name</strong>: A cue that a partition field was defined and that an anomaly was found for one of the partition field values.</li>
				<li><strong class="source-inline">partition_field_value</strong>: The value of the partition field that this anomaly occurred for. In other words, the entity name this anomaly was found for.</li>
			</ul>
			<p>In addition to the fields mentioned here (which would have been <strong class="source-inline">by_field_name</strong> and <strong class="source-inline">by_field_value</strong> if the job had been configured to use a <strong class="source-inline">by</strong> field), we also see an explicit instance of the <strong class="source-inline">geo.src</strong> field. This is just a shortcut – every partition, <strong class="source-inline">by</strong>, or <strong class="source-inline">over_field_value</strong> in the results will also have a direct field name.</p>
			<p>If your job <a id="_idIndexMarker346"/>is doing population analysis (via the use of <strong class="source-inline">over_field_name</strong>), then the record results document will be organized slightly differently as the reporting is done with orientation as to the unusual members of the population. For example, if we look at a population analysis job on the <strong class="source-inline">kibana_sample_data_logs</strong> index in which we choose <strong class="source-inline">distinct_count("url.keyword") over clientip</strong> as the detector, then an example record-level results document will also contain a causes array:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17040_05_20.jpg" alt="Figure 5.20 – Record-level document showing the causes array for a population job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.20 – Record-level document showing the causes array for a population job</p>
			<p>The <strong class="source-inline">causes</strong> array is built to compactly express all of the anomalous things that that IP did in that bucket. Again, many things seem redundant, but it is primarily because there may be different ways of aggregating the information for results presentation in dashboards or alerts.</p>
			<p>Also, in the case of this population analysis, we see that the <strong class="source-inline">influencers</strong> array contains both the <strong class="source-inline">clientip</strong> field and the <strong class="source-inline">response.keyword</strong> field:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17040_05_21.jpg" alt="Figure 5.21 – Record-level document showing the influencers array for a population job&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.21 – Record-level document showing the influencers array for a population job</p>
			<p>Let's conclude <a id="_idIndexMarker347"/>our survey of the results index schema by looking at the influencers-level results.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor102"/>Influencer results</h2>
			<p>Yet another lens by which to view the results is via influencers. Viewing the results this way allows <a id="_idIndexMarker348"/>us to answer the question, "What were the most unusual entities in my ML job and when were they unusual?" To understand the structure and content of influencer-level results, let's look at an example document in the <strong class="source-inline">.ml-anomalies-*</strong> index by using Kibana Discover and issuing the following KQL query:</p>
			<p class="source-code">result_type :"influencer" and response.keyword:404</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B17040_05_22.jpg" alt="Figure 5.22 – Influencer-level result document as seen in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22 – Influencer-level result document as seen in Kibana Discover</p>
			<p>Notice that <a id="_idIndexMarker349"/>in this case, we didn't query on the score (<strong class="screen-inline">influencer_score</strong>), but rather on an expected entity name and value. The last document listed (with an <strong class="screen-inline">influencer_score</strong> of <strong class="screen-inline">50.174</strong>, matches what we saw back in <em class="italic">Figure 5.13</em>. </p>
			<p>Let's look at the key fields:</p>
			<ul>
				<li><strong class="source-inline">timestamp</strong>: The timestamp of the leading edge of the time bucket, inside which this influencer's anomalous activity occurred. This is similar to what was explained earlier.</li>
				<li><strong class="source-inline">job_id</strong>: The name of the anomaly detection job that created this result.</li>
				<li><strong class="source-inline">influencer_field_name</strong>: The name of the field that was declared as an influencer in the job configuration.</li>
				<li><strong class="source-inline">influencer_field_value</strong>: The value of the influencer field for which this result is relevant.</li>
				<li><strong class="source-inline">influencer_score</strong>: The current normalized score of how unusual and contributory the influencer was to anomalies at this point.</li>
				<li><strong class="source-inline">initial_influencer_score</strong>: The normalized score of the influencer when that bucket was first analyzed by the analytics. This score, unlike <strong class="source-inline">influencer_score</strong>, will not change as more data is analyzed.</li>
				<li><strong class="source-inline">is_interim</strong>: A flag that signifies whether or not the bucket is finalized or whether the bucket is still waiting for all of the data within the bucket span to be received. This field is relevant for ongoing jobs that are operating in real time. For certain types of analysis, there could be interim results, even though not all of the data for the bucket has been seen.</li>
			</ul>
			<p>Now that we <a id="_idIndexMarker350"/>have exhaustively explained the relevant fields that are available to the user, we can file that information away for when we build custom dashboards, visualizations, and sophisticated alerting in subsequent sections and chapters. But, before we exit this chapter, we still have a few important concepts to explore. Next up is a discussion on a special kind of anomaly – the multi-bucket anomaly.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor103"/>Multi-bucket anomalies</h1>
			<p>Almost everything that we've studied so far with anomalies being generated by Elastic ML's anomaly <a id="_idIndexMarker351"/>detection jobs has been with respect to looking at a specific anomaly being raised at a specific time, but quantized at the interval of <strong class="source-inline">bucket_span</strong>. However, we can certainly have situations in which a particular observation within a bucket span may not be that unusual, but an extended window of time, taken collectively together, might be more significantly unusual than any single observation. Let's see an example.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor104"/>Multi-bucket anomaly example</h2>
			<p>First shown in the example in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>, in <em class="italic">Figure 3.17</em>, we repeat the <a id="_idIndexMarker352"/>figure here to show how multi-bucket anomalies exhibit themselves in the Elastic ML UI:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17040_05_23.jpg" alt="Figure 5.23 – Multi-bucket anomalies first shown in Chapter 3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.23 – Multi-bucket anomalies first shown in Chapter 3</p>
			<p>As we discussed in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>, multi-bucket anomalies are designated with a different symbol in the UI (a cross instead of a dot). They denote cases in which the actual singular value may not necessarily be anomalous, but that there is a trend that is occurring in a sliding window of 12 consecutive buckets. Here, you can see that there is a noticeable slump spanning several adjacent buckets.</p>
			<p>Note, however, that some of the multi-bucket anomaly markers are placed on the data at times after the data has "recovered." This can be somewhat confusing to users until you realize <a id="_idIndexMarker353"/>that because the determination of multi-bucket anomalies is a secondary analysis (in addition to the bucket-by-bucket analysis) and because this analysis is a sliding window looking in arrears, the leading edge of that window, when the anomaly is recorded, might be after the situation has recovered.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Multi-bucket scoring</h2>
			<p>As mentioned, multi-bucket analysis is a secondary analysis. Therefore, two probabilities are calculated for <a id="_idIndexMarker354"/>each bucket span – the probability of the observation seen in the current bucket, and the probability of a multi-bucket feature – a kind of weighted average of the current bucket and the previous 11. If those two probabilities are roughly the same order of magnitude, then <strong class="source-inline">multi_bucket_impact</strong> will be low (on the negative side of the -5 to +5 scale). If, on the other hand, the multi-bucket feature probability is wildly lower (thus more unusual), then <strong class="source-inline">multi_bucket_impact</strong> will be high.</p>
			<p>In the example shown in <em class="italic">Figure 5.23,</em> the UI will show the user the multi-bucket impact as being <strong class="screen-inline">high</strong>, but will not give you the actual scoring:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17040_05_24.jpg" alt="Figure 5.24 – Multi-bucket anomalies, with impact scoring shown&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24 – Multi-bucket anomalies, with impact scoring shown</p>
			<p>However, if you look <a id="_idIndexMarker355"/>at the raw record-level result, you will see that <strong class="screen-inline">multi_bucket_impact</strong> has indeed been given a value of +5:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17040_05_25.jpg" alt="Figure 5.25 – Multi-bucket anomaly record, with the raw score shown&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25 – Multi-bucket anomaly record, with the raw score shown</p>
			<p>Multi-bucket anomalies give you a different perspective on the behavior of your data. You will want to keep in mind how they are signified and scored via the <strong class="source-inline">multi_bucket_impact</strong> field in order for you to include or exclude them, as required, from your reporting or alerting logic.</p>
			<p>Let's now <a id="_idIndexMarker356"/>look forward (yes, pun intended) to how results from forecasts are represented in the results index.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor106"/>Forecast results</h1>
			<p>As explained in depth in <a href="B17040_04_Epub_AM.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, <em class="italic">Forecasting</em>, we can get Elastic ML to extrapolate into the future <a id="_idIndexMarker357"/>the trends of the data that has been analyzed. Recall what we showed in <em class="italic">Figure 4.21</em>:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17040_05_26.jpg" alt="Figure 5.26 – Forecast results first shown in Chapter 4&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.26 – Forecast results first shown in Chapter 4</p>
			<p>Remember that the prediction value is the value with the highest likelihood (probability), and that the shaded area is the range of the 95th percentile of confidence. These three key values are stored in the <strong class="source-inline">.ml-anomalies-*</strong> results indices with the following names:</p>
			<ul>
				<li><strong class="source-inline">forecast_prediction</strong></li>
				<li><strong class="source-inline">forecast_upper</strong></li>
				<li><strong class="source-inline">forecast_lower</strong></li>
			</ul>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor107"/>Querying for forecast results</h2>
			<p>When querying for the forecast results in the <strong class="source-inline">.ml-anomalies-*</strong> results indices, it is important to <a id="_idIndexMarker358"/>remember that forecast results are transient – they have a default lifespan of 14 days following creation, especially if they are created from the UI in Kibana. If a different expiration duration is desired, then the forecast will have to be invoked via the <strong class="source-inline">_forecast</strong> API endpoint and explicitly setting the <strong class="source-inline">expires_in</strong> duration.</p>
			<p>Another thing to remember is that multiple forecasts may have been invoked at different moments in time on the same dataset. As shown back in <em class="italic">Figure 4.4</em> and repeated here, multiple forecast invocations produce multiple forecast results:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17040_05_27.jpg" alt="Figure 5.27 – A symbolic representation of invoking multiple forecasts at different times&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.27 – A symbolic representation of invoking multiple forecasts at different times</p>
			<p>As such, we need a way to discern between the results. In the Kibana UI, they are discernable simply by looking at the <strong class="bold">Created</strong> date:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B17040_05_28.jpg" alt="Figure 5.28 – Viewing multiple previously run forecasts&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.28 – Viewing multiple previously run forecasts</p>
			<p>However, when looking <a id="_idIndexMarker359"/>at the results index, it should be noted that each invoked forecast has a unique <strong class="screen-inline">forecast_id</strong>:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B17040_05_29.jpg" alt="Figure 5.29 – Viewing forecast results in Kibana Discover&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.29 – Viewing forecast results in Kibana Discover</p>
			<p>This <strong class="source-inline">forecast_id</strong> is only obvious when invoking the forecast using the <strong class="source-inline">_forecast</strong> API because <strong class="source-inline">forecast_id</strong> is returned as part of the payload of the API call.</p>
			<p>Therefore, if multiple <a id="_idIndexMarker360"/>forecasts were created spanning a common time frame, there would be more than one result with different IDs.</p>
			<p>When querying the forecast results, you can think of two possible orientations:</p>
			<ul>
				<li><strong class="bold">Value-focused</strong>: The query supplies a date and time, and the result is a particular value for that time is returned. The question, "What is my utilization 5 days from now?" would be a good example.</li>
				<li><strong class="bold">Time-focused</strong>: The query supplies a value, and the result is a time at which that value is realized. The question, "When does my utilization reach 80%?" would be a good example.</li>
			</ul>
			<p>Obviously, either type of query is possible. To satisfy the time-focused inquiry, for example, we need to re-orient the query a little to ask it to return the date (or dates) on which the predicted values meet certain criteria. The user can query for the forecast results using other traditional query methods (KQL, Elasticsearch DSL), but to mix it up a little, we'll submit the query using Elastic SQL in the Kibana Dev Tools Console:</p>
			<p class="source-code">POST _sql?format=txt</p>
			<p class="source-code">{</p>
			<p class="source-code">   "query": "SELECT forecast_prediction,timestamp FROM \".ml-anomalies-*\" WHERE job_id='forecast_example' AND forecast_id='Fm5EiHcBpc7Wt6MbaGcw' AND result_type='model_forecast' AND forecast_prediction&gt;'16890' ORDER BY forecast_prediction DESC"</p>
			<p class="source-code">}</p>
			<p>Here, we are asking <a id="_idIndexMarker361"/>whether there are any times during which the predicted value exceeds our limit of the value of 16,890. The response is as follows:</p>
			<p class="source-code">forecast_prediction|       timestamp        </p>
			<p class="source-code">-------------------+------------------------</p>
			<p class="source-code">16893.498325784924 |2017-03-17T09:45:00.000Z</p>
			<p>In other words, we may breach the threshold on March 17 at 9:45 A.M. GMT (although remember from <a href="B17040_04_Epub_AM.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, <em class="italic">Forecasting</em>, that the sample data used is from the past and therefore forecast predictions are also in the past). Now that we have a good understanding of how to query for forecast results, we could include them in dashboards and visualizations, which we will cover later in this chapter – or even in alerts, as we'll see in <a href="B17040_06_Epub_AM.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Alerting on ML Analysis</em>. </p>
			<p>But, before we look at including results in custom dashboards and visualizations, let's cover one last brief topic – the Elastic ML results API.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor108"/>Results API</h1>
			<p>If programmatic <a id="_idIndexMarker362"/>access to the results is your thing, in addition to querying the results indices directly, you could opt to instead query Elastic ML's results API. Some parts of the API are redundant to what we've already explored, and some parts are unique. We will now check them out in the upcoming sections.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor109"/>Results API endpoints</h2>
			<p>There are <a id="_idIndexMarker363"/>five different results API endpoints available:</p>
			<ul>
				<li>Get buckets</li>
				<li>Get influencers</li>
				<li>Get records</li>
				<li>Get overall buckets</li>
				<li>Get categories</li>
			</ul>
			<p>The first three API endpoints give results that are redundant in light of what we've already covered in this chapter by way of querying the results index directly (through Kibana or using the Elasticsearch <strong class="source-inline">_search</strong> API), and that method actually allows more flexibility, so we really won't bother discussing them here. However, the last two API endpoints are novel, and each deserves an explanation.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor110"/>Getting the overall buckets API</h2>
			<p>The overall buckets API call is a means by which to return summarized results across multiple <a id="_idIndexMarker364"/>anomaly detection jobs in a programmatic way. We're not going to explore every argument of the request body, nor will we describe every field in the response body, as you can reference the documentation. But we will discuss here the important function of this API call, which is to request the results from an arbitrary number of jobs, and to receive a single result score (called <strong class="source-inline">overall_score</strong>) that encapsulates the <strong class="source-inline">top_n</strong> average of the maximum bucket <strong class="source-inline">anomaly_score</strong> for each job requested. As shown in the documentation, an example call is one that asks for the top two jobs (in the set of jobs that begin with the name <strong class="source-inline">job-</strong>) whose bucket anomaly score, when averaged together, is higher than <strong class="source-inline">50.0</strong>, starting from a specific timestamp:</p>
			<p class="source-code">GET _ml/anomaly_detectors/job-*/results/overall_buckets</p>
			<p class="source-code">{</p>
			<p class="source-code">  "top_n": 2,</p>
			<p class="source-code">  "overall_score": 50.0,</p>
			<p class="source-code">  "start": "1403532000000"</p>
			<p class="source-code">}</p>
			<p>This will <a id="_idIndexMarker365"/>result in the following sample return:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "count": 1,</p>
			<p class="source-code">  "overall_buckets": [</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "timestamp" : 1403532000000,</p>
			<p class="source-code">      "bucket_span" : 3600,</p>
			<p class="source-code">      "overall_score" : 55.0,</p>
			<p class="source-code">      "jobs" : [</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "job_id" : "job-1",</p>
			<p class="source-code">          "max_anomaly_score" : 30.0</p>
			<p class="source-code">        },</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "job_id" : "job-2",</p>
			<p class="source-code">          "max_anomaly_score" : 10.0</p>
			<p class="source-code">        },</p>
			<p class="source-code">        {</p>
			<p class="source-code">          "job_id" : "job-3",</p>
			<p class="source-code">          "max_anomaly_score" : 80.0</p>
			<p class="source-code">        }</p>
			<p class="source-code">      ],</p>
			<p class="source-code">      "is_interim" : false,</p>
			<p class="source-code">      "result_type" : "overall_bucket"</p>
			<p class="source-code">    }</p>
			<p class="source-code">  ]</p>
			<p class="source-code">}</p>
			<p>Notice that <strong class="source-inline">overall_score</strong> is the average of the two highest scores in this case (the result of <strong class="source-inline">overall_score</strong> of <strong class="source-inline">55.0</strong> is the average of the <strong class="source-inline">job-3</strong> score of <strong class="source-inline">80.0</strong> and the <strong class="source-inline">job-1</strong> score of <strong class="source-inline">30.0</strong>), even though three anomaly detection jobs match the query pattern of <strong class="source-inline">job-*</strong>. While this <a id="_idIndexMarker366"/>is certainly interesting, perhaps for building a composite alert, you should realize the limitations in this reporting, especially if you can only access the bucket-level anomaly score and not anything from the record or influencer level. In <a href="B17040_06_Epub_AM.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>, <em class="italic">Alerting on ML Analysis</em>, we will explore some options regarding composite alerting. </p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor111"/>Getting the categories API</h2>
			<p>The <strong class="source-inline">categories</strong> API call is only relevant for jobs that leverage categorization, as described <a id="_idIndexMarker367"/>in detail in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em>. The <strong class="source-inline">categories</strong> API returns some interesting internal definitions of the categories found during the textual analysis of the documents. If we run the API on the categorization job that we created back in <a href="B17040_03_Epub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Anomaly Detection</em> (abbreviated to only return one record for brevity), the output is as follows:</p>
			<p class="source-code">GET _ml/anomaly_detectors/secure_log/results/categories</p>
			<p class="source-code">{</p>
			<p class="source-code">  "page":{</p>
			<p class="source-code">    "size": 1</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>We will see the following response:</p>
			<p class="source-code">{</p>
			<p class="source-code">  "count" : 23,</p>
			<p class="source-code">  "categories" : [</p>
			<p class="source-code">    {</p>
			<p class="source-code">      "job_id" : "secure_log",</p>
			<p class="source-code">      "category_id" : 1,</p>
			<p class="source-code">      "terms" : "localhost sshd Received disconnect from port",</p>
			<p class="source-code">      "regex" : ".*?localhost.+?sshd.+?Received.+?disconnect.+?from.+?port.*",</p>
			<p class="source-code">      "max_matching_length" : 122,</p>
			<p class="source-code">      "examples" : [</p>
			<p class="source-code">        "Oct 22 15:02:19 localhost sshd[8860]: Received disconnect from 58.218.92.41 port 26062:11:  [preauth]",</p>
			<p class="source-code">        "Oct 22 22:27:20 localhost sshd[9563]: Received disconnect from 178.33.169.154 port 53713:11: Bye [preauth]",</p>
			<p class="source-code">        "Oct 22 22:27:22 localhost sshd[9565]: Received disconnect from 178.33.169.154 port 54877:11: Bye [preauth]",</p>
			<p class="source-code">        "Oct 22 22:27:24 localhost sshd[9567]: Received disconnect from 178.33.169.154 port 55723:11: Bye [preauth]"</p>
			<p class="source-code">      ],</p>
			<p class="source-code">      "grok_pattern" : ".*?%{SYSLOGTIMESTAMP:timestamp}.+?localhost.+?sshd.+?%{NUMBER:field}.+?Received.+?disconnect.+?from.+?%{IP:ipaddress}.+?port.+?%{NUMBER:field2}.+?%{NUMBER:field3}.*",</p>
			<p class="source-code">      "num_matches" : 595</p>
			<p class="source-code">    }</p>
			<p class="source-code">  ]</p>
			<p class="source-code">}</p>
			<p>Several <a id="_idIndexMarker368"/>elements are part of the reply:</p>
			<ul>
				<li><strong class="source-inline">category_id</strong>: This is the number of the category of message (incremented from 1). It corresponds to the value of the <strong class="source-inline">mlcategory</strong> field in the results index.</li>
				<li><strong class="source-inline">terms</strong>: This is a list of the static, non-mutable words extracted from the message.</li>
				<li><strong class="source-inline">examples</strong>: An array of complete, unaltered sample log lines that fall into this category. These are used to show the users what some of the real log lines look like. </li>
				<li><strong class="source-inline">grok_pattern</strong>: A regexp-style pattern match that could be leveraged for Logstash or an ingest pipeline that you could use to match this message category.</li>
				<li><strong class="source-inline">num_matches</strong>: A count of the number of times this message category was seen in the logs throughout the anomaly detection job running on this dataset.</li>
			</ul>
			<p>Perhaps the most interesting use of this API is not for anomaly detection, but rather around merely understanding the unique number of category types and the distribution of those types <a id="_idIndexMarker369"/>in your unstructured logs – to answer questions such as, "What kinds of messages are in my logs and how many of each type?" Some of this capability may be leveraged in the future to create a "data preparation" pipeline to assist users in ingesting unstructured logs into Elasticsearch more easily.</p>
			<p>Let's now explore how results gleaned from Elastic ML's anomaly detection and forecasting jobs can be leveraged in custom dashboards, visualizations, and Canvas workpads.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor112"/>Custom dashboards and Canvas workpads</h1>
			<p>It's clear that now that we know the ins and outs of the results index, which stores all the goodness <a id="_idIndexMarker370"/>that comes out of Elastic ML's anomaly detection <a id="_idIndexMarker371"/>and forecast analytics, our imagination is the limit concerning how we can then express those results in a way that is meaningful for our own goals. This section will briefly explore some of the concepts and ideas that you can use to bring Elastic ML's results to a big screen near you!</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor113"/>Dashboard "embeddables"</h2>
			<p>One recent addition to the capabilities of Elastic ML is the ability to embed the Anomaly Explorer <a id="_idIndexMarker372"/>timeline ("swim lanes") into existing custom dashboards. To accomplish this, simply click the "three dots" menu at the top right of the Anomaly timeline and select the <strong class="bold">Add to dashboard</strong> option:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B17040_05_30.jpg" alt="Figure 5.30 – Adding the Anomaly timeline to another dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.30 – Adding the Anomaly timeline to another dashboard</p>
			<p>At this point, select which part of the swim lane views you want to include and select which dashboard(s) you wish to add them to:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B17040_05_31.jpg" alt="Figure 5.31 – Adding the Anomaly timeline to a specific dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.31 – Adding the Anomaly timeline to a specific dashboard</p>
			<p>Clicking <a id="_idIndexMarker373"/>on the <strong class="bold">Add and edit dashboard</strong> button will then transport the user to the target dashboard and allow them to move and resize the embedded panels. For example, we can have the anomalies side by side with the other visualizations:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B17040_05_32.jpg" alt="Figure 5.32 – New dashboard now containing Anomaly swim lane visualizations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32 – New dashboard now containing Anomaly swim lane visualizations</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor114"/>Anomalies as annotations in TSVB</h2>
			<p>The <strong class="bold">Time</strong> <strong class="bold">Series</strong> <strong class="bold">Visual</strong> <strong class="bold">Builder</strong> (<strong class="bold">TSVB</strong>) component in Kibana is an extremely flexible <a id="_idIndexMarker374"/>visualization builder that allows users to <a id="_idIndexMarker375"/>not only plot their time series data but also annotate that data with information from other indices. This is a perfect recipe for plotting some raw data, but then overlaying anomalies from an anomaly detection job on top of that raw data. For example, you could create a TSVB for <strong class="source-inline">kibana_sample_data_logs</strong> with the following panel options:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B17040_05_33.jpg" alt="Figure 5.33 – Creating a new TSVB visualization – Panel options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.33 – Creating a new TSVB visualization – Panel options</p>
			<p>Then, there <a id="_idIndexMarker376"/>is the following configuration for <a id="_idIndexMarker377"/>the <strong class="bold">Data</strong> tab to do a terms aggregation for the top five origin countries (<strong class="screen-inline">geo.src</strong>):</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B17040_05_34.jpg" alt="Figure 5.34 – Creating a new TSVB visualization – Data options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.34 – Creating a new TSVB visualization – Data options</p>
			<p>Then, we have the following configuration for the <strong class="bold">Annotations</strong> tab to overlay the results of a previously created anomaly detection job named <strong class="source-inline">web_traffic_per_country</strong> to select anomalies with record scores over <strong class="source-inline">90</strong>:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B17040_05_35.jpg" alt="Figure 5.35 – Creating a new TSVB visualization – Annotation options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.35 – Creating a new TSVB visualization – Annotation options</p>
			<p>Note the <strong class="bold">Query string</strong> entry as something pretty recognizable given what we've learned in this chapter. The TSVB <a id="_idIndexMarker378"/>also requires a comma-separated <a id="_idIndexMarker379"/>list of fields to report on for the annotations (here we enter <strong class="source-inline">record_score</strong> and <strong class="source-inline">partition_field_value</strong>) and <strong class="bold">Row template</strong>, which defines how the information is formatted in the annotation (here we define it to be <strong class="source-inline">Anomaly:{{record_score}} for {{partition_field_value}}</strong>). Once this is done, we have the final result:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B17040_05_36.jpg" alt="Figure 5.36 – Creating a new TSVB visualization complete with anomaly annotations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.36 – Creating a new TSVB visualization complete with anomaly annotations</p>
			<p>We now have a nice visualization panel with anomalies superimposed on the original raw data.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor115"/>Customizing Canvas workpads</h2>
			<p>Kibana Canvas is <a id="_idIndexMarker380"/>the ultimate tool for creating pixel-perfect infographics that are data-driven from Elasticsearch. You can create highly custom-tailored reports with a set of customizable elements. The experience in Canvas is very different from standard Kibana dashboards. Canvas presents you with a workspace where you can build sets of slides (similar in concept to Microsoft PowerPoint) called the <strong class="bold">workpad</strong>.</p>
			<p>To leverage anomaly detection and/or forecast results in a Canvas workpad, there isn't anything special that needs to be done – everything that has been learned so far in this chapter is applicable. This is because it is very easy to use the <strong class="source-inline">essql</strong> command in Canvas to query the <strong class="source-inline">.ml-anomalies-*</strong> index pattern and extract the information we care about.</p>
			<p>When we <a id="_idIndexMarker381"/>install the Kibana sample data, we also get a few sample Canvas workpads to enjoy: </p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B17040_05_37.jpg" alt="Figure 5.37 – Sample Canvas workpads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.37 – Sample Canvas workpads</p>
			<p>Clicking on the <strong class="bold">[Logs] Web Traffic</strong> sample workpad opens it for us to edit:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B17040_05_38.jpg" alt="Figure 5.38 – Sample web traffic workpad&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.38 – Sample web traffic workpad</p>
			<p>Selecting one of <a id="_idIndexMarker382"/>the elements on the page (perhaps the <strong class="bold">TOTAL</strong> <strong class="bold">VISITORS</strong> counter at the bottom, which currently shows the value of <strong class="screen-inline">324</strong>) and then selecting <strong class="bold">Expression</strong> <strong class="bold">editor</strong> at the bottom-right corner of Canvas will reveal the details of the element:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B17040_05_39.jpg" alt="Figure 5.39 – Editing a Canvas element in the Expression editor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.39 – Editing a Canvas element in the Expression editor</p>
			<p>Notice that the real "magic" of obtaining live data is embedded in the <strong class="source-inline">essql</strong> command – the rest of the expression is merely formatting. As a simple example, we can adjust the SQL with the following syntax:</p>
			<p class="source-code">SELECT COUNT(timestamp) as critical_anomalies FROM \".ml-anomalies-*\" WHERE job_id='web_logs_rate' AND result_type='record' AND record_score&gt;'75'</p>
			<p>One thing to note is that because the <strong class="source-inline">.ml-anomalies-*</strong> index pattern's name begins with a non-alphabet character, the name needs to be enclosed in double-quotes, and those double-quotes need to be escaped with the backslash character.</p>
			<p>This will return <a id="_idIndexMarker383"/>the total number of critical anomalies (those that have a <strong class="source-inline">record_score</strong> larger than <strong class="screen-inline">75</strong>) for a particular anomaly detection job on that dataset:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B17040_05_40.jpg" alt="Figure 5.40 – Displaying the count of critical anomalies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.40 – Displaying the count of critical anomalies</p>
			<p>In short, it is quite easy to use Canvas to create very beautiful and meaningful data visualizations and leverage information from either anomaly detection results or forecast results.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor116"/>Summary</h1>
			<p>Elastic ML's anomaly detection and forecasting analytics creates wonderful and meaningful results that are explorable via the rich UI that is provided in Kibana, or programmatically via direct querying of the results indices and the API. Understanding the results of your anomaly detection and forecasting jobs and being able to appropriately leverage that information for further custom visualizations or alerts makes those custom assets even more powerful.</p>
			<p>In the next chapter, we'll leverage the results to create sophisticated and useful proactive alerts to further increase the operational value of Elastic ML.</p>
		</div>
	</body></html>