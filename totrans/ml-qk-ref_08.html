<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Probabilistic Graphical Models</h1>
                </header>
            
            <article>
                
<p>Before we get into <strong>Bayesian network</strong> (<strong>BN</strong>) concepts, we should be aware of the theories of probability. So, we will try to touch upon them and build the foundation of BNs.</p>
<p>We already know that probability is the degree of certainty/uncertainty of an event occurring. However, it can be also termed as the degree of belief, which is more commonly used when we talk about BN.</p>
<p>When we toss a fair coin, we say that the degree of belief around the event of heads/tails happening is <em>0.5</em>. It implies that our belief of heads happening is as strong as tails. The probability can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>p(Heads)=p(tails)=0.5</em></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Bayesian rules</li>
<li>Bayesian networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Key concepts</h1>
                </header>
            
            <article>
                
<p>We will cover a few key concepts before moving on to the body of the chapter:</p>
<ul>
<li>In the case of discrete distribution, a probability mass function is used to find out the probability, <em>p(X= x),</em> where <em>X</em> is a discrete random variable and <em>x</em> is a real value number.</li>
<li>In the case of continuous distribution, probability density function is used to find out the probability <em>p(X &lt;= x)</em>. In this scenario, a probability curve is plotted and the area under the curve (integration) helps us with the probability.</li>
</ul>
<ul>
<li>Conditional probability is to understand this, a cricket match can be the perfect example. Suppose there is a game scheduled between India and Australia and we are trying to pass on our belief of India triumphing. Do you think that the probability will be impacted by the team selected by India? Will the probability of India winning the match be impacted if <em>Virat Kohli</em> and <em>Rohit Sharma</em> are part of the team? So, <em>p(India winning|Rohit and Virat are playing)</em> denotes the probability of India winning, given that <em>Rohit</em> and <em>Virat</em> are playing. Essentially, it means that the probability of one event is dependent on the probability of another event. It is called <strong>conditional probability</strong>.</li>
</ul>
<p style="padding-left: 60px">The probability of <em>x</em>, given <em>y</em>, can be expressed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d915c396-9cd1-4e99-a87b-87d09498b0a7.png" style="width:8.58em;height:2.83em;"/> </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2d7ed028-8228-43d2-9fa1-cc9eef4f53bf.png" style="width:11.83em;height:1.33em;"/></p>
<ul>
<li class="CDPAlignLeft CDPAlign">The chain rule computes the joint distribution of a set of random variables using their conditional probabilities. From conditional probability, we know that <img class="fm-editor-equation" src="assets/0d8baa2a-6492-4518-a44b-2a2b63b30ea9.png" style="width:9.83em;height:1.25em;"/>.</li>
</ul>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">It implies that if there are <img class="fm-editor-equation" src="assets/0672cc86-ade2-4aaf-836a-b708c703e6a4.png" style="width:8.92em;height:1.42em;"/> events. The joint probability distribution turns out like this:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/46cd6ec1-a45e-40fa-8b9c-b1a4a9599281.png" style="width:30.25em;height:1.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes rule</h1>
                </header>
            
            <article>
                
<p>Bayes rule is one of the building blocks of probability theory. It stems from conditional probability and joint probability and extends beyond.</p>
<p>We will explain this in a simple way by again taking an example from cricket. In cricket, pitch condition varies as you go from one place to another and it is one of the factors that can be significant when deciding the team. The outcome can also be dependent upon it.</p>
<p>Let's say the Indian team goes to Australia for a game and we have to predict the belief of an Indian player scoring a century (100 runs) in the game. If that player has got experience of playing in that country, we might say with strong belief that he might score a century. But, there is another player who is a first-timer in this country. What would the the prior belief be for him? Of course, many would have less belief that he would score a century.</p>
<p>However, our prior belief will change as we see the way the player is performing. That is, more data about the player will be at our disposal as more games are played by that player. Based on that, posterior belief will keep getting updated. It changes a lot, largely due to the observations or more data (which is called <strong>likelihood</strong>). Bayes rule is based on these concepts.</p>
<p>Let's say that <em>A<sub>i</sub></em> forms a mutually exhaustive event with <em>B</em>:</p>
<p class="CDPAlignCenter CDPAlign">           <img class="fm-editor-equation" src="assets/5c0b99e6-fbc9-4a8a-82a8-ed1d6c118885.png" style="width:7.50em;height:3.25em;"/></p>
<p>The probability of <em>B</em> will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dc060e81-1231-4d98-9a89-0ea9f32c00dc.png" style="width:11.75em;height:3.50em;"/></p>
<p>We get the probability of B from conditional probability like so:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/47e9c056-9487-4eef-8f0a-4b9e52d70eee.png" style="width:14.50em;height:1.42em;"/></p>
<p>Hence:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/83f0e512-2d1b-41a9-ab86-40bab786697f.png" style="width:13.58em;height:2.92em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/49120dc1-9adc-4b5c-9077-4d270ce66994.png" style="width:12.75em;height:2.75em;"/></p>
<p class="CDPAlignLeft CDPAlign">Now, extracting the value of <img class="fm-editor-equation" src="assets/263394a6-45d1-4214-aa22-754050d46121.png" style="width:5.42em;height:1.42em;"/><strong> </strong>from equation 2 and putting it in equation 1, we get this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c5f4ca75-60d9-4d37-9aed-e8230fa8d3a8.png" style="width:13.75em;height:2.42em;"/></p>
<p><span>After replacing the value of <em>P(B)</em> from the preceding equation, we get this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea3cba87-2548-4ee7-a977-c03b76bd40c1.png" style="width:15.58em;height:2.75em;"/></p>
<p>Have a look at equation 3 first. This is called <strong>Bayes rule</strong>.</p>
<p><em>P(A|B)</em> is called <strong>posterior</strong>, which needs to be estimated. In the preceding example, this would be the probability of scoring a century given that the player has got the earlier experience of playing there.</p>
<p><em>P(B|A)</em> is called the <strong>likelihood</strong>, which is the probability of observing the new evidence, given our initial hypothesis. For example, the probability of a player having previous experience in playing cricket get to score a century.</p>
<p><em>P(A)</em> is called the <strong>prior</strong>, which is the probability of our hypothesis without any additional prior information.</p>
<p><em>P(B)</em> is called the <strong>marginal likelihood</strong>, which is the total probability of observing the evidence.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes network</h1>
                </header>
            
            <article>
                
<p>Bayes network is a type of probabilistic graphical model that can be used to build models to address business problems. Applications of this are quite wide. For example, it can be used in anomaly detection, predictive modeling, diagnostics, automated insights, and many other applications.</p>
<p>It is totally understandable that a few words used here would have been alien to you till now. For example, what do we mean by graphical here?</p>
<p>A graph forms out of a set of nodes and edges. Nodes are represented by <em>N={N1,N2…..Nn}</em>, where independent variables are sitting at every node. Edges are the connectors between nodes. Edges can be denoted by <em>E={E1, E2…..En}</em> and can be of two types:</p>
<ul>
<li>Directed, represented by <img class="fm-editor-equation" src="assets/843f2b04-9067-42f3-8aac-5703ddad0bd4.png" style="width:5.92em;height:1.17em;"/></li>
<li>Undirected, represented by:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"> <img src="assets/dc8df9c5-ea52-4fe6-a967-8a6cbe2d18fa.png" style="width:9.42em;height:2.83em;"/></p>
<p>With the help of nodes and edges, a relationship between the variables is exhibited. It can be a conditional independence relationship or a conditional dependence relationship. BN is one a techniques that can introduce causality amongst variables. Although causality is not an essential part of it, having this (causality) in the network can make the structure quite compact.</p>
<p>Let's see it through an example. There are a number of variables, such as waking up late, an accident on the highway, a rainy day, a traffic jam, they will be late for work, and being late for a meeting. If an individual has got up late, it means being late for work. An accident on the highway can cause a traffic jam and, in turn, this will result in being late for work. On a rainy day, the roads can be more prone to accidents and, also, there can be slow-moving traffic that will cause a traffic jam and, in turn, this will result in being late for work. The following diagram explains the example:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-903 image-border" src="assets/8b493814-db85-499c-adc3-b85469f6d528.png" style="width:33.50em;height:30.42em;"/></p>
<p>This kind of network is called a <strong>directed acyclic graph</strong>. Acyclic means that there is no cycle in the network. We are talking about a relationship between variables here. For example, waking up late and being late for a meeting are typically not independent. But they are conditionally independent, given being late for work.</p>
<p>Also, it might seem that waking up late has no connection and relationship with an accident on the highway. That is, they may appear to be independent of each other. However, if you know the value of being late for work, then these two can be called conditionally independent.</p>
<p>So, BN allows conditional independence between nodes. At the same time, it is an efficient representation of joint probability distribution, which is enabled by a chain rule.</p>
<p>Let's say that X represents n independent variables or nodes. Arcs or a directed arrow represents the probabilistic dependence or independence amongst variables. An absence of an arc would mean probabilistic independence. The network is a directed acyclic graph wherein the local probability distribution is kept at each node, which is also called the <strong>conditional probability table</strong> (<strong>CPT</strong>).</p>
<p>If we talk about the previous network, then we need the probability distribution required to address the whole network. For the purpose of <span>simplicity</span>, we will keep all the nodes as Boolean.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Probabilities of nodes</h1>
                </header>
            
            <article>
                
<p>Let's look at the probability at each node and find out how many probabilities would appear there.</p>
<p>The nodes carrying <strong>Late Wake-up</strong> and <strong>Rainy Day</strong> are the parent nodes as there are no nodes leading to such nodes. The different nodes can be seen in the following points:</p>
<ol>
<li><strong>Node (Late Wake-up)</strong>: Being one of the parent nodes, we will be looking just to find out the probability of waking up late. Hence, the count of probability to be found out is 1 here.</li>
<li><strong>Node (Rainy Day)</strong>: Like the late wake-up node, the count of probability is 1 here as well.</li>
<li><strong>Node (Accident on the highway)</strong>: As it is a child node of rainy day, it talks about the probability of the accident given the rainy day and the probability of the accident given it's not a rainy day. So, the count of probability is 2 here.</li>
<li><strong>Node (Traffic Jam)</strong>: It has got two parents (rainy day and accident). Rainy day has got two values, which are true and false, the same as accident. Combining both will yield four different combinations. Hence, the count of probability will be 4.</li>
<li><strong>Node (Late for work) and Node (Late for meeting)</strong>: A similar explanation applies to these two nodes as well. The count for the probabilities of these is 4:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-901 image-border" src="assets/4ed44c12-fbe1-4882-b3c3-8e62ccabe38a.png" style="width:32.75em;height:31.08em;"/></p>
<p>The total number of probabilities are 1 + 2 + 1 + 4 + 4 + 4 = 16.</p>
<p>Had it been just a normal joint probability distribution instead of BN, we would have had 2<sup>6</sup>-1 probabilities. Hence, BN makes the network quite compact. Also, another more basic assumption we have to be mindful of is that each node is conditionally independent of its non-descendants given its immediate parents. For example, waking up late and being late for a meeting are conditionally independent in the case that <strong>late for work</strong> is also there. Generally, we can express BN in the following manner, which displays how joint distribution can be translated into a compact structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/422192d3-e973-47a9-a21d-db60c6679cf0.png" style="width:24.92em;height:2.75em;"/></p>
<p>If <em>G</em> is the graph, <em>X<sub>i</sub> </em>is a node in the graph <em>G</em>, and <em>P</em> are the parents of the <em>X<sub>i</sub> </em>node.</p>
<p class="mce-root"/>
<p>Here are a few notes about the equations:</p>
<ul>
<li>The right-hand side of the equation is the application of the chain rule, which exhibits conditional independence relations. It is a graph-structured approximation of the joint probability distribution.</li>
<li>Of course, the graph has to be acyclic.</li>
<li>It can provide the convenience to display the relationship among various events.</li>
</ul>
<p>Now, let's take a simple scenario to showcase the CPT. The following is the combination of three events as shown:</p>
<p>If it rains, the dog starts barking and the man skips work:</p>
<ul>
<li>Probability of rain (yes/no)</li>
<li>Probability that the dog will bark (yes/no)</li>
<li>Probability that the man will skip work (yes/no)</li>
</ul>
<p>Let's have the network prepared as a directed acyclic graph. All these nodes reflect an event, and directed arrows are conditional probabilities. We will see here how to read this graph:</p>
<ul>
<li>Connector 1 indicates the probability of the dog barking if it rains</li>
<li>Connector 2 indicates the probability of the man skipping his work if the dog barks</li>
</ul>
<p>The following diagram shows the flow chart for both the probabilities:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-902 image-border" src="assets/db88ed17-07f6-443a-951d-a166d5bd2681.png" style="width:30.92em;height:20.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CPT</h1>
                </header>
            
            <article>
                
<p>Let's do the CPT for connector 1:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 113px">
<p> </p>
</td>
<td style="width: 111px">
<p><strong>The dog barks</strong></p>
</td>
<td style="width: 166px">
<p><strong>The dog doesn't bark</strong></p>
</td>
<td style="width: 218.355px">
<p><strong>Aggregate</strong></p>
</td>
</tr>
<tr>
<td style="width: 113px">
<p>It rains</p>
</td>
<td style="width: 111px">
<p>               10</p>
</td>
<td style="width: 166px">
<p>              4</p>
</td>
<td style="width: 218.355px">
<p>          14</p>
</td>
</tr>
<tr>
<td style="width: 113px">
<p>It doesn't rain</p>
</td>
<td style="width: 111px">
<p>                8</p>
</td>
<td style="width: 166px">
<p>              5</p>
</td>
<td style="width: 218.355px">
<p>          13</p>
</td>
</tr>
<tr>
<td style="width: 113px">
<p>Aggregate</p>
</td>
<td style="width: 111px">
<p>              18</p>
</td>
<td style="width: 166px">
<p>              9</p>
</td>
<td style="width: 218.355px">
<p>          27</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, we are talking about the following scenarios:</p>
<ul>
<li>Probability <em>(Dog barks|It rains) = 10/14</em></li>
<li>Probability <em>(Dog doesn't bark | It rain) = 4/14</em></li>
<li>Probability <em>(Dog barks | It doesn't rain) = 8/13</em></li>
<li>Probability <em>(Dog doesn't bark | It doesn't rain) = 5/13</em></li>
</ul>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 162.386px">
<p> </p>
</td>
<td style="width: 181.477px">
<p><strong> The dog barks</strong></p>
</td>
<td style="width: 246.932px">
<p><strong> The dog doesn't bark</strong></p>
</td>
</tr>
<tr>
<td style="width: 162.386px">
<p>It rains</p>
</td>
<td style="width: 181.477px">
<p>               10/14</p>
</td>
<td style="width: 246.932px">
<p>              4/14</p>
</td>
</tr>
<tr>
<td style="width: 162.386px">
<p>It doesn't rain</p>
</td>
<td style="width: 181.477px">
<p>                8/13</p>
</td>
<td style="width: 246.932px">
<p>              5/13</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The following diagram shows the probabilities in detail:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-905 image-border" src="assets/538aa2c0-2f3d-4f2a-8474-75084cc619ce.png" style="width:36.75em;height:21.92em;"/></p>
<p>Let's say if the probability of <em>rain = P(rain) =0.6</em> then the probability of <em>no rain = P(no rain) = 0.4</em>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's say that the CPT for the man skipping work is as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong> The man skips work</strong></p>
</td>
<td>
<p><strong> The man doesn't skip work</strong></p>
</td>
</tr>
<tr>
<td>
<p>The dog barks</p>
</td>
<td>
<p>               0.8</p>
</td>
<td>
<p>              0.2</p>
</td>
</tr>
<tr>
<td>
<p>The dog doesn't bark</p>
</td>
<td>
<p>               0.3</p>
</td>
<td>
<p>              0.7</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_infobox"><span>The probability of every event has to be calculated with respect to the parent node.</span></div>
<p>And now, we are supposed to find out the probability of <em>the man skipping work and the dog barks but it doesn't rain = P (Man skips work, the dog barks, it doesn't rain)</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>= P (Man skips work|the dog barks) *P (the dog barks|it doesn't rain) *P(it doesn't rain)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>=0.8 * (8/13) *0.4</em></p>
<p class="CDPAlignCenter CDPAlign"><em>=0.1969</em></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of the training and test set</h1>
                </header>
            
            <article>
                
<p><span>Let's take a use case and work it out in Python. We are going to use Titanic data from Kaggle.</span><br/>
The data has been split into two groups:</p>
<ul>
<li>Training set (<kbd>train.csv</kbd>)</li>
<li>Test set (<kbd>test.csv</kbd>)</li>
</ul>
<p>The data is about the passengers who traveled on the Titanic. It captures their features:</p>
<ul>
<li><span><kbd>pclass</kbd>: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd</span></li>
<li><span><kbd>gender</kbd>: Gender</span></li>
<li><span><kbd>Age</kbd>: Age in years</span></li>
<li><span><kbd>sibsp</kbd>: Number of siblings/spouses aboard the Titanic</span></li>
<li><span><kbd>parch</kbd>: </span><span>Number of parents/children aboard the Titanic</span></li>
<li><span><kbd>ticket</kbd>: Ticket number</span></li>
<li><span><kbd>fare Passenger</kbd>: Fare</span></li>
</ul>
<ul>
<li><span><kbd>cabin</kbd>: Cabin number</span></li>
<li><span><kbd>embarked</kbd>: Port of embarkation <kbd>C = Cherbourg</kbd>, <kbd>Q = Queenstown</kbd>, and <kbd>S = Southampton</kbd></span></li>
</ul>
<p>We have got to build the model to predict whether or not they survived the sinking of the Titanic. Initially, import the parameters as shown:</p>
<pre>import pandas as pd<br/>import numpy as np</pre>
<p>We are loading the datasets here:</p>
<pre>traindf= pd.read_csv("train.csv")<br/>testdf= pd.read_csv("test.csv")</pre>
<p><span>We have to look for the number of unique values for each variable since BNs are discrete models:</span></p>
<pre>for k in traindf.keys():<br/> print('{0}: {1}'.format(k, len(traindf[k].unique())))</pre>
<p>The output is as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-644 image-border" src="assets/369f0c2b-19d4-460a-b90e-1ccaef31c735.png" style="width:10.00em;height:15.00em;"/></p>
<p>In order to save our system from too much computation and to avoid load on it, we will reduce the number of variables:</p>
<pre>for k in traindf.keys():<br/> if len(traindf[k].unique())&lt;=10:<br/> print(k)</pre>
<p>We get the following  output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-645 image-border" src="assets/71bcddd7-91e1-4575-8709-56c819730aef.png" style="width:7.25em;height:9.33em;"/></p>
<p>Now, we are left with six variables.</p>
<p><span>Also, we have to discretize continuous variables in case they needs to be made part of the model:</span></p>
<pre>import math<br/>def forAge(row):<br/> if row['Age'] &lt; 10:<br/>    return '&lt;10'<br/> elif math.isnan(row['Age']):<br/>    return "nan"<br/> else:<br/>    dec = str(int(row['Age']/10))<br/>    return "{0}0's".format(dec)<br/> decade=traindf.apply(forAge, axis=1)<br/> print("Decade: {1}".format(k, len(decade.unique())))</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7ead451e-3999-4bf8-ae9b-fc81d5e89ff8.png" style="width:7.50em;height:1.83em;"/></p>
<p><span><span>Let's do the pre-processing now:</span></span></p>
<pre>def preprocess(df):<br/> # create a dataframe with discrete variables (len&lt;10)<br/> filt=[k for k in df.keys() if len(df[k].unique())&lt;=10]<br/> filtr2=df[filt].copy()<br/> forAge = lambda row: int(row['Age']/10) if not math.isnan(row['Age']) else np.nan<br/> filtr2['Decade']=df.apply(forAge, axis=1)<br/> filtr2=filtr2.dropna()<br/> filtr2['Decade']=filtr2['Decade'].astype('int32')<br/> return filtr2</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>For <kbd>traindf</kbd> and <kbd>testdf</kbd>, we use the following:</p>
<pre>ptraindf= preprocess(traindf)<br/>ptestdf=preprocess(testdf)</pre>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to save this data, since the <kbd>pyAgrum</kbd> <span>library</span> accepts only files as inputs:</p>
</div>
</div>
</div>
<pre class="mce-root">ptraindf.to_csv('post_train.csv', index=False)<br/>ptestdf.to_csv( 'post_test.csv', index=False)<br/><br/>df=pd.read_csv('post_train.csv')<br/>for k in df.keys():<br/>  print("{} : {}".format(k, df[k].unique()))</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-646 image-border" src="assets/951fb491-5531-4d0a-96ff-1048878d600b.png" style="width:15.33em;height:8.75em;"/></p>
<pre>import pyAgrum as gum<br/>import pyAgrum.lib.notebook as gnb</pre>
<p>Now, it's time to build the model. Here, you need to be watchful while choosing the <kbd>RangeVariable</kbd> and <kbd>LabelizedVariable</kbd> variables:</p>
<pre>template=gum.BayesNet()<br/>template.add(gum.RangeVariable("Survived", "Survived",0,1))<br/>template.add(gum.RangeVariable("Pclass", "Pclass",1,3))<br/>template.add(gum.LabelizedVariable("Gender", "Gender",0).addLabel("female").addLabel("male"))<br/>template.add(gum.RangeVariable("SibSp", "SibSp",0,8))<br/>template.add(gum.RangeVariable("Parch", "Parch",0,9))<br/>template.add(gum.LabelizedVariable("Embarked", "Embarked",0).addLabel('').addLabel('C').addLabel('Q').addLabel('S'))<br/>template.add(gum.RangeVariable("Decade", "Calculated decade", 0,9))<br/>gnb.showBN(template)</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-669 image-border" src="assets/bc93a989-6af8-416f-9bdb-bcee069b4e22.png" style="width:47.58em;height:4.83em;"/></p>
<p>For <kbd>learnBN()</kbd>, we use the following:</p>
<pre>learner = gum.BNLearner('post_train.csv', template)<br/>bn = learner.learnBN()<br/>bn</pre>
<p>The following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1024 image-border" src="assets/025f896e-13c7-4d3e-b6e7-c642d2518f37.png" style="width:23.92em;height:26.75em;"/></p>
<p>Now that we have the model, let's try to extract the information from it:</p>
<pre>gnb.showInformation(bn,{},size="20")</pre>
<p class="mce-root"/>
<p>We get the output as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-670 image-border" src="assets/8cc11cf8-4fbc-4674-af45-8c6f08f34bdc.png" style="width:33.92em;height:34.58em;"/></p>
<p>The entropy of a variable means that the greater the value, the more uncertain the variable's marginal probability distribution is. The lower the value of entropy, the lower the uncertainty. The <kbd>Decade</kbd> variable has got the highest entropy, which means that it is evenly distributed. Parch has got low entropy and distribution is non-even.</p>
<p>A consequence of how entropy is calculated is that entropy tends to get bigger if the random variable has many modalities.</p>
<p>Finding the inference gives us a view of the marginal probability distribution here:</p>
<pre>gnb.showInference(bn)</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-672 image-border" src="assets/1187b06c-9572-4108-ad35-05dbd51037e7.png" style="width:35.08em;height:46.67em;"/></p>
<p>Now, let's see how classification can be done:</p>
<pre>gnb.showPosterior(bn,evs={},target='Survived')</pre>
<p>We get the output as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-900 image-border" src="assets/fb4a872f-7093-4004-b45e-6da27d41c44e.png" style="width:12.08em;height:6.50em;"/></p>
<p>More than 40% of passengers survived here. But, we are not pushing any conditions.</p>
<p>Let's say we want to find out what the chances of a young male surviving are:</p>
<pre>gnb.showPosterior(bn,evs={"Gender": "male", "Decade": 3},target='Survived')</pre>
<p>The following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-906 image-border" src="assets/8485e3f0-faa9-4022-ac86-f8121bf48189.png" style="width:11.25em;height:6.08em;"/></p>
<p>So, the chances are 20.6%.</p>
<p>If we have to find out the chances of an old lady surviving, we go about it as follows:</p>
<pre>gnb.showPosterior(bn,evs={"Gender": "female", "Decade": 8},target='Survived')</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-904 image-border" src="assets/0ff5217b-890f-4ca3-aee7-e437280a3e31.png" style="width:13.50em;height:7.33em;"/></p>
<p>Now, in order to evaluate the model to find out how good it is, we will plot the ROC curve:</p>
<pre>from pyAgrum.lib.bn2roc import showROC<br/> showROC(bn, 'post_train.csv','Survived',"1",True,True)</pre>
<p>The following is the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-907 image-border" src="assets/8c257983-9e0b-4b3d-bada-0acb872a5b45.png" style="width:26.08em;height:18.50em;"/></p>
<p>Here, <strong>AUC</strong> comes out to be <strong>0.893508</strong> and it's quite decent.</p>
<p>We are done with the modeling part here. Also, we have learned about probability theory, Bayesian networks, the calculation of CPT, and how to execute it in Python<span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter has given us an understanding of probability theory. Also, the application of probability theory has been put into use. We got an idea of Bayes rule and BNs and how it is formed. We got our hands dirty with the calculation of a CPT.  Finally, we looked at a use case to understand how classification can be done with the help of BNs. The readers will now have the skill to have an in-depth knowledge of Bayes rules and BNs.</p>
<p>In the next chapter, we will study s<span>elected topics in deep learning.</span></p>


            </article>

            
        </section>
    </body></html>