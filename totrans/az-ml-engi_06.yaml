- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying ML Models for Real-Time Inferencing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at how data scientists and ML professionals can
    make predictions available through a REST service hosted in Azure to support real-time
    predictions. Data is sent to a REST API, and the predicted result is provided
    in the response. This allows for a variety of applications to consume and leverage
    a model created with AMLS. We will explore a variety of options for making your
    models available in real time with AML.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have leveraged AMLS to handle feature engineering and built and registered
    models. In this chapter, we will focus on providing solutions that leverage your
    model to provide predictions on datasets in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning provides several options for providing inferencing to
    business users to support batch and real-time inferencing use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding real-time inferencing and batch scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an MLflow model with managed online endpoints through AML Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an MLflow model with managed online endpoints through the Python SDK
    v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a model with managed online endpoints through the Python SDK v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a model with managed online endpoints through the Azure CLI v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to access your workspace, repeat the steps from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the workspace user interface on the left-hand side, click **Compute**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Compute** screen, select your compute instance and select **Start**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Start compute](img/B18003_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Start compute
  prefs: []
  type: TYPE_NORMAL
- en: Your compute instance will change from **Stopped** to **Starting** status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous chapter, we cloned the Git repository; if you have not already
    done so, continue to follow these steps. If you have already cloned the repository,
    skip to *step 9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal on your compute instance. Note the path will include your
    user in the directory. Type the following into the terminal to clone the sample
    notebooks into your working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Clicking on the refresh icon shown in *Figure 6**.2* will update and refresh
    the notebooks displayed on your screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.2 – The refresh icon](img/B18003_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – The refresh icon
  prefs: []
  type: TYPE_NORMAL
- en: 'Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
    This will display the files cloned into your working directory, as shown in *Figure
    6**.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Azure-Machine-Learning-Engineering](img/B18003_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Azure-Machine-Learning-Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Understanding real-time inferencing and batch scoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Models can be deployed to support different use cases and different business
    requirements. When deploying a model in production, how you choose to deploy your
    model should be based on your user requirements. If you need to have a prediction
    available in real time to support streaming or interaction with your prediction
    in other applications, then real-time inferencing will be required. Real-time
    inferencing requires compute resources to be active and available for your model
    to provide a response. If your application requires less responsive predictions
    that are stored in a file or perhaps a database, then batch inferencing would
    be the correct selection. Batch inferencing allows you to spin up and down compute
    resources.
  prefs: []
  type: TYPE_NORMAL
- en: Before model deployment, the compute for hosting a real-time web service will
    need to be selected. For real-time inferencing, **Azure Kubernetes Service** (**AKS**),
    **Azure Container Instances** (**ACI**), and **Azure** **Arc-enabled** **Kubernetes**
    are compute resources supported through your AML workspace. AKS is typically used
    to support production workloads, and ACI is typically leveraged to support lower
    environments with CPU-based workloads. Azure Arc-enabled Kubernetes is typically
    used to run inferencing with on-premises resources where clusters are managed
    in Azure Arc. In this chapter, we will also explore the option of leveraging a
    **managed online endpoint**, which will provide a level of abstraction over the
    compute resources required for model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are actually two different types of compute that can be leveraged
    for an online endpoint. One type is the aforementioned managed online endpoints,
    and the other is **Kubernetes online endpoints**. Managed online endpoints provide
    fully managed compute provisioning, scaling, and also host OS image updates. Kubernetes
    online endpoints are designed for teams that want to manage those items through
    their own Azure Kubernetes cluster. Managed online endpoints automate the provisioning
    of the compute, automatically update the host OS image, and provide automatic
    recovery in the event of a system failure. Due to these advantages, we will focus
    our efforts on managed online endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Managed online endpoints not only provide a level of abstraction around the
    compute resources required to deploy a REST endpoint; they also support multiple
    deployments to a single endpoint, often referred to as **blue/green deployment**.
    Blue/green deployment is the practice of moving traffic from one release to another.
    So, when we deploy a managed online endpoint, we can have an initial model that
    is deployed and then deploy a new model to the same endpoint. After the new model
    is available on the managed online endpoint, we can move users to the new deployment
    with the new model. This means for each managed online endpoint, we will not only
    be deploying the managed online endpoint but also the endpoint deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Managed online endpoints are required to have a unique name within an Azure
    region. Not providing a unique name will result in a failed deployment.
  prefs: []
  type: TYPE_NORMAL
- en: We will be exploring options to make your model available to support real-time
    use cases leveraging managed online endpoints and deployments in this chapter.
    In the next section, we will explore deploying your model with AMLS Studio for
    a low-code experience.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an MLflow model with managed online endpoints through AML Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to deploy a model to a web service, we will be required to define the
    environment, which includes the Conda and `pip` dependencies, our compute resources,
    and a scoring script. The **scoring script**, also called an **entry script**,
    will load the model in an initialization function, as well as handle running predictions
    with the incoming data to the web service.
  prefs: []
  type: TYPE_NORMAL
- en: With MLflow models, not only is the model packaged but AML also understands
    how to consume the model, so there is no need to configure an environment or entry
    script for the model deployment with managed online endpoints; AML understands
    these models natively. This makes deploying the model very easy from the UI and
    through code.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we leveraged MLflow to create and register models. Proceed
    to the `Chapter 6``, Prep-Model Creation & Registration.ipynb` notebook to create
    and register a model to leverage MLflow, as we did in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: This notebook will take you through the process of creating a model and registering
    it to the workspace, as discussed in previous chapters. However, there are a few
    points to review before creating the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'When leveraging MLflow, we can use `autolog` to generate the model and environment
    information for us, but in the notebook, we actually set `log_models=False`, as
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Disable the logging of the model](img/B18003_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Disable the logging of the model
  prefs: []
  type: TYPE_NORMAL
- en: 'We set logging the model to `false`, but in the training script, we explicitly
    package up the model and log it, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – MLflow modeling logging](img/B18003_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – MLflow modeling logging
  prefs: []
  type: TYPE_NORMAL
- en: This provides us with control over the packages used for deployment. When MLflow
    releases a new version, there may be issues between the latest version of MLflow
    and Azure Machine Learning managed online endpoints. Ideally, this will not happen;
    however, as open source continues to progress, it is a good practice to include
    versions when packaging up your model to ensure you are in total control of the
    model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you run your `Chapter 6` `Model Creation Prep & Registration.ipynb` notebook,
    the model will be registered in the workspace. Once we have an MLflow model registered
    to the workspace, we can leverage AMLS Studio to deploy the model. For this process,
    proceed with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the model from **Model List**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Select the model from Model List](img/B18003_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Select the model from Model List
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on an MLflow model, select the **Deploy** option and then the
    first option, **Deploy to real-time endpoint**, as shown in the following figure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Deploy an MLflow model](img/B18003_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Deploy an MLflow model
  prefs: []
  type: TYPE_NORMAL
- en: Since the model is selected and was created with MLflow, AMLS Studio understands
    this is an MLflow model. After clicking on the **Deploy to real-time endpoint**
    option, you will be guided through creating a deployment for your model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Configure an endpoint](img/B18003_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Configure an endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a deployment for your model, follow the options in *Figure 6**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: For `azure-ui-endpoint`, adding a prefix or suffix to the name to make it unique
    in your region.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For `UI` `created endpoint`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select **Managed** for **Compute type**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select **Key-based authentication**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Click **Next** and review the selected model, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Model selection](img/B18003_06_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Model selection
  prefs: []
  type: TYPE_NORMAL
- en: Since you started the deployment by selecting the model, there is no additional
    selection that needs to occur. As shown in *Figure 6**.9*, you have configured
    the endpoint as well as the model selection, so the next step is to move on to
    deployment configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'By clicking the **Next** icon on the **Model** screen, you are brought to a
    screen to configure the deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Configure deployment](img/B18003_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Configure deployment
  prefs: []
  type: TYPE_NORMAL
- en: As part of the deployment configuration, a deployment name is provided. For
    a given endpoint, as mentioned in the *Understanding Real-Time Inferencing and
    Batch Scoring* section, multiple deployments can be created. This allows you to
    configure the traffic pattern to test out a deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the deployment name, you can configure the scoring timeout. This
    is the timeout to enforce scoring calls. You can also enable application insights
    diagnostics and data collection for the deployment. This will enable you to use
    the **Monitoring** tab to view activity for the managed online endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Once the deployment has been configured, you will be brought to the environment
    selection tab by again clicking **Next**. Given this model was created with MLflow,
    no environment selection is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The environment was already created for the model, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Environment selection](img/B18003_06_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Environment selection
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the environment has been established, the compute resources should
    be selected for the model. In the `1`, as shown in the following figure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Compute selection](img/B18003_06_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Compute selection
  prefs: []
  type: TYPE_NORMAL
- en: Now that the compute has been selected, the traffic allocation for this deployment
    should be configured. If you have updated your model, deploy it initially with
    traffic set to `0`, confirm it is working correctly, and then, as shown in the
    following figure, update the traffic to `100`% on the new model, given this is
    our first model deployment. This should provide a seamless experience for consumers
    of the REST API from the old model to the new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Configure deployment traffic](img/B18003_06_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Configure deployment traffic
  prefs: []
  type: TYPE_NORMAL
- en: After configuring the traffic, we can review the deployment that is about to
    occur. By clicking **Next**, AMLS Studio will bring you to the **Review** stage
    of the guided model deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Review the model deployment](img/B18003_06_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Review the model deployment
  prefs: []
  type: TYPE_NORMAL
- en: The information shown on the screen will reflect the input you provided during
    the guided online managed deployment. After you have confirmed the settings displayed
    in the previous screenshot, click on the **Create** button to create the endpoint
    as well as the deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following figure shows the deployment progress. The managed online endpoint
    is being provisioned, given that the status of **Provisioning state** is **Creating**,
    and the blue deployment is being provisioned as well. Currently, the traffic is
    set to **0** for the **blue** deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Managed online endpoint deployment in progress](img/B18003_06_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Managed online endpoint deployment in progress
  prefs: []
  type: TYPE_NORMAL
- en: Once the deployment has been completed, the endpoint will show that the status
    of **Provisioning state** is **Succeeded**, and **Traffic allocation** will be
    **100%** of the deployment, as shown in the following figure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16 – AMLS Studio deployment completed](img/B18003_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – AMLS Studio deployment completed
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully deployed a managed online endpoint for
    consumption. Recall that the model has columns – `Embarked`, `Loc`, `Sex`, `Pclass`,
    `Age`, `Fare`, and `GroupSize` – as input parameters. We can send JSON to the
    REST endpoint, specifying the input data columns and the data we would like to
    receive predictions to leverage the JSON schema leveraged by AMLS.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Test the managed online endpoint](img/B18003_06_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Test the managed online endpoint
  prefs: []
  type: TYPE_NORMAL
- en: A sample of this request file is located in the `prepped_data` folder under
    the `chapter 6` folder in the Git repository. You can copy and paste this text
    into the studio test screen shown in *Figure 6**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the managed online endpoint has been tested from the AMLS Studio experience,
    we will next deploy a managed online endpoint through the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying an MLflow model with managed online endpoints through the Python SDK
    V2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we leveraged AMLS Studio to deploy our MLflow model.
    In this section, we will explore code to deploy an MLflow model to a managed online
    endpoint through the SDK v2.
  prefs: []
  type: TYPE_NORMAL
- en: In order to leverage the SDK v2 for model deployment, we will leverage the `Chapter
    6` `MLFlow Model Deployment SDK V2.ipynb` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deploy a managed online endpoint through the SDK V2, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To deploy the model, we will create `ManagedOnlineEndpoint` with the appropriate
    configuration. In the case of an MLflow model, we will need to specify `name`
    and `auth_mode`. In addition, we will provide `description` as well as `tags`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Configure ManagedOnlineEndpoint](img/B18003_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Configure ManagedOnlineEndpoint
  prefs: []
  type: TYPE_NORMAL
- en: After the endpoint has been configured, we are able to call the `create` or
    `update` method, passing in the endpoint to create the endpoint in the workspace
    with the `create_or_update` command, as shown here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Leveraging ml_client to create a managed online endpoint](img/B18003_06_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Leveraging ml_client to create a managed online endpoint
  prefs: []
  type: TYPE_NORMAL
- en: Once the endpoint has been created, you are ready to create a deployment. For
    the deployment, we will pass a name, the endpoint name, which was created in the
    previous figure, the model to deploy, the instance type of the VM to leverage
    for compute resources, as well as the instance count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The model can be retrieved directly from the workspace by its name and version,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Getting the model from the registry based on name and version](img/B18003_06_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Getting the model from the registry based on name and version
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is required for the managed online deployment. Now that the model
    has been retrieved from the workspace, it can be passed into `ManagedOnlineDeployment`,
    as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Configure deployment](img/B18003_06_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Configure deployment
  prefs: []
  type: TYPE_NORMAL
- en: Note `instance_type`, which is the compute being leveraged by our managed online
    endpoint. We specified here the use of `Standard_F4s_v2`, as we have great flexibility
    in the type of compute resources we can use to serve up our real-time prediction
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the deployment has been configured, leveraging `ml_client`, the managed
    online endpoint can be deployed with initial traffic set to `0` through the `begin_create_or_update`
    method, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Create a deployment](img/B18003_06_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Create a deployment
  prefs: []
  type: TYPE_NORMAL
- en: After the deployment has succeeded, the traffic for the endpoint can be set
    to 100% using the deployment name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Update deployment traffic](img/B18003_06_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Update deployment traffic
  prefs: []
  type: TYPE_NORMAL
- en: Now that the endpoint has been deployed, the URI for the endpoint and the primary
    key can be retrieved to make a REST API call to retrieve a prediction. For the
    online endpoint, we can easily retrieve both the scoring URI as well as the primary
    key, as shown in the following code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Retrieve the URI and primary key](img/B18003_06_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Retrieve the URI and primary key
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a call can be made to the REST endpoint to retrieve predictions. The
    following code below takes a dataframe into the `make_predictions` function, prepares
    the request, and returns the results from the managed online endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Making predictions](img/B18003_06_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Making predictions
  prefs: []
  type: TYPE_NORMAL
- en: Running the preceding code allows a dataframe to be passed and prediction results
    to be returned. This model can be tested to leverage the sample input provided
    in the file located in the `prepped_data` folder under the `chapter 6` folder
    in the Git repository. Regardless of whether the managed online endpoint was deployed
    through the SDK or AMLS Studio, the functionality is the same.
  prefs: []
  type: TYPE_NORMAL
- en: You have been able to deploy a model through AMLS Studio and the SDK, as it
    was created as an MLflow model. In the event that MLflow is not leveraged to create
    a model, one can easily be deployed, but additional configuration is required
    to deploy it. In the next section, we will continue to deploy a model, specifying
    the environment and the script required for inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model with managed online endpoints through the Python SDK v2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we deployed an MLflow model, but when you create a
    model that does not leverage MLflow, you need to provide two additional details
    for a successful managed online endpoint deployment. In this section, we will
    focus on adding functionality so that we can deploy our model without relying
    on MLflow to provide the environment and scoring script.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to deploy a managed online endpoint leveraging the SDK v2 and not
    relying on MLflow to provide the environment and scoring script, we will create
    those in this section as you leverage the notebook: `Chapter 6` `Model Deployment`
    `SDK V2.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: Our first step is to create our `score.py` file. This is the file that will
    be used to load the model and serve a request to the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet provides the information required for the entry
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – The score.py script](img/B18003_06_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – The score.py script
  prefs: []
  type: TYPE_NORMAL
- en: In the score script shown in the previous figure, there are two required functions,
    `init` and `run`. The `init` function tells AML how to load a model. The model
    will be provided as part of the deployment configuration, the model path is set,
    and `joblib` is leveraged to load the model. The `model` global variable is leveraged
    to hold the model. When the REST endpoint is called, the `run` function will be
    called. The data from the API call will be passed into the `run` function. Here,
    a dictionary is set to retrieve the information, which is transformed into a dataframe
    that is then passed to the `model.predict` function. The results of `model.predict`
    are passed to a list and returned from the function.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the score script, we will need to have the model. In the previous
    section, we retrieved the registered model leveraging the SDK v2 based on its
    name and version, but we can also search across the experiments and retrieve the
    model from them. The sample code provided here shows searching for the model from
    the experiment and downloading it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Find and download the model](img/B18003_06_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Find and download the model
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the scoring script, we will need to provide an environment to
    use for deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.28 – A deployment environment](img/B18003_06_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – A deployment environment
  prefs: []
  type: TYPE_NORMAL
- en: In the previous Jupyter notebook when we created the model, we already created
    an environment, but we need to add the `azureml-defaults` package for successful
    deployment to a managed online endpoint. Therefore, instead of using the already
    registered environment, we are creating a new environment to pass to the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to deploy a managed online endpoint that was not created with MLflow,
    the key difference is in the deployment configuration. Note that *Figure 6**.21*
    provided the code snippet required to deploy a model created with MLflow. Compare
    that code to the code found in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.29 – Managed online endpoint deployment with score script and environment](img/B18003_06_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – Managed online endpoint deployment with score script and environment
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the previous code block, the `score.py` file is specified as well
    as the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'This managed online endpoint can again be tested. As the `score.py` file highlighted,
    this REST API will be expecting a different schema for its input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You now have an understanding of how powerful tool-managed online endpoints
    are for model deployment. In addition to a guided AMLS Studio experience as well
    as full capabilities within the SDK, we can also leverage the Azure CLI v2 to
    manage the deployment process. In the next section, we will explore leveraging
    the Azure CLI v2 for this capability.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model for real-time inferencing with managed online endpoints through
    the Azure CLI v2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will leverage a managed online endpoint and deploy it with
    the Azure Machine Learning CLI v2\. The CLI v2 will leverage YAML files holding
    the configuration required for our deployment in the commands we call. Remember
    the requirement for a unique managed online endpoint name, so when running the
    code, be sure to update your managed online endpoint name in both the YAML files
    and the CLI command.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the new Azure CLI v2 extension, we are required to have an Azure CLI
    version greater than 2.15.0\. This can easily be checked by using the `az version`
    command to check your Azure CLI version:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On your compute instance, navigate to the terminal and type the following command:
    `az version`. After typing that command, you should see that the Azure CLI v2
    is installed on your compute instance, as shown in the following figure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.30 – The Azure CLI v2 with the ml extension installed](img/B18003_06_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – The Azure CLI v2 with the ml extension installed
  prefs: []
  type: TYPE_NORMAL
- en: 'You can update your `ml` extension through the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After installing the new extensions, we can again run `az version` to confirm
    that the extension is installed and up to date so that we can proceed to log in.
  prefs: []
  type: TYPE_NORMAL
- en: You will need to log in with the Azure CLI to handle your deployment by typing
    `az login`. You will be prompted to open a browser and type a device login to
    authenticate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After authenticating, you should set your default Azure subscription. Your Azure
    subscription ID can be easily retrieved by finding an Azure Machine Learning workspace
    within the portal and copying the guide displayed in the **Subscription ID** section
    of the **Overview** tab, as shown here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Azure Subscription ID](img/B18003_06_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Azure Subscription ID
  prefs: []
  type: TYPE_NORMAL
- en: 'Back in the terminal on your compute instance, set the account for the Azure
    CLI to leverage by typing `az account set -s XXXX- XXXX - XXXX - XXXX – XXXX`,
    replacing `XXXX- XXXX - XXXX - XXXX – XXXX` with your subscription ID. Then, set
    variables to hold your resource group name, the location of your AML workspace,
    and your workspace name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on where your AMLS workspace is deployed, the `LOCATION` value can be
    found here: [https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations](https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the variables have been set, you can run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Navigate to the [*Chapter 6*](B18003_06.xhtml#_idTextAnchor086) directory in
    your AML workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create the required files, run the `Chapter 6` `Model Deployment CLI v2 -
    Create Scripts.ipynb` notebook, which will create an `endpoint.yml` file, a `deployment.yml`
    file, and a `score.py` file for inferencing. The files will be generated in a
    folder called `CLI_v2_ManagedOnlineEndpoint`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the notebook, we will create the directory by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Directory creation](img/B18003_06_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Directory creation
  prefs: []
  type: TYPE_NORMAL
- en: For the managed online endpoint, we will also create a `score.py` file. The
    `score` script will leverage our model and provide the `init` and `run` functions
    that our previous `score.py` file provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.33 – The score.py file](img/B18003_06_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – The score.py file
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create the endpoint .`yml` file. We can create a basic file that
    contains the required attributes of the name and the authorization mode. The authorization
    mode can be specified as a key or `aml_token`. Key-based authentication will not
    expire, but the Azure ML token-based authentication will. We will proceed with
    key-based authentication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using `aml_token`, you can get a new token by running the `az ml online-endpoint`
    `get-credentials` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the command to generate the `endpoint.yml` file with the same key value
    as `auth_mode`, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Managed online endpoint configuration](img/B18003_06_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.34 – Managed online endpoint configuration
  prefs: []
  type: TYPE_NORMAL
- en: Remember to update your name to be something unique; if `titanic-managed-online-endpoint`
    is already deployed in your region, your deployment will fail.
  prefs: []
  type: TYPE_NORMAL
- en: For the managed online endpoint deployment, we will also generate a `deployment.yml`
    file. In this file, we need to specify the model, the scoring script, the environment,
    and the instance type used for model deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Creation of the `deployment.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.35 – Creation of the deployment.yml file](img/B18003_06_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.35 – Creation of the deployment.yml file
  prefs: []
  type: TYPE_NORMAL
- en: In this file, you will specify the registered model name and version, the registered
    environment name and version, the type of compute resources leveraged for the
    deployed model, and where the scoring script is located. In the `deployment.yml`
    file, you should update `endpoint_name` to reflect the name you chose in your
    `endpoint.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: After running the notebook and selecting a unique endpoint name, we can leverage
    CLI v2 to deploy our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the command line, open the `CLI_v2_ManagedOnlineEndpoint` directory, and
    from that directory, create your `online-endpoint` with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that `titanic-online-endpoint` should be replaced with your managed online
    endpoint name.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the endpoint is created, you can now create `online-deployment`, leveraging
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once again, note that `titanic-online-endpoint` should be replaced with your
    managed online endpoint name.
  prefs: []
  type: TYPE_NORMAL
- en: Once the deployment is complete, the endpoint will be available for testing.
    The endpoint is available from the **Endpoints** section in your AML workspace,
    as shown here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.36 – An online endpoint](img/B18003_06_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.36 – An online endpoint
  prefs: []
  type: TYPE_NORMAL
- en: We can test the endpoint by clicking on its name and selecting the **Test**
    tab on the endpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the test section, we can provide data for inferencing after typing into
    the **Test** box to retrieve results from the web service, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.37 – Testing an online endpoint](img/B18003_06_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.37 – Testing an online endpoint
  prefs: []
  type: TYPE_NORMAL
- en: This sample request can be found in the `chapter 6` folder under the `prepped_data`
    folder in a file named `sample_request_cli.json`.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you were able to deploy a web service utilizing a managed online
    endpoint in AMLS through the Azure CLI v2\. This feature takes advantage of schema
    files to provide the deployment definitions in configuration files to enable deployment.
    Managed online endpoints prevent data scientists or citizen data scientists from
    having to be concerned about the infrastructure required to support hosting a
    web service, to provide real-time inferencing to support your use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations on deploying a managed online endpoint through the CLI! This
    will be especially useful in [*Chapter 9*](B18003_09.xhtml#_idTextAnchor119),
    *Productionizing Your Workload with MLOps*. After testing out your managed online
    endpoints, be sure to delete them, as they use up compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the focus was on deploying your model as a REST endpoint to
    support real-time inferencing use cases. We saw that we are able to leverage AMLS
    Studio for a low-code deployment experience. We also leveraged SDK v2 to deploy
    models to managed online endpoints. We continued by deploying models through CLI
    v2 to support model deployment for real-time inferencing. These sections demonstrated
    deploying real-time web services through low-code, code-first, and configuration-driven
    approaches. These capabilities empower you to deploy in a variety of ways.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to leverage batch-inferencing to support
    our use cases.
  prefs: []
  type: TYPE_NORMAL
