- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Deploying ML Models for Real-Time Inferencing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为实时推理部署 ML 模型
- en: In this chapter, we will look at how data scientists and ML professionals can
    make predictions available through a REST service hosted in Azure to support real-time
    predictions. Data is sent to a REST API, and the predicted result is provided
    in the response. This allows for a variety of applications to consume and leverage
    a model created with AMLS. We will explore a variety of options for making your
    models available in real time with AML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨数据科学家和 ML 专业人士如何通过在 Azure 中托管的 REST 服务提供预测，以支持实时预测。数据被发送到 REST API，预测结果在响应中提供。这允许各种应用程序消费和使用由
    AMLS 创建的模型。我们将探索使用 AML 在实时使模型可用的各种选项。
- en: So far, we have leveraged AMLS to handle feature engineering and built and registered
    models. In this chapter, we will focus on providing solutions that leverage your
    model to provide predictions on datasets in real time.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经利用 AMLS 处理特征工程并构建和注册了模型。在本章中，我们将专注于提供利用您的模型在实时数据集上提供预测的解决方案。
- en: Azure Machine Learning provides several options for providing inferencing to
    business users to support batch and real-time inferencing use cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 为业务用户提供多种推理选项，以支持批量实时推理用例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding real-time inferencing and batch scoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解实时推理和批量评分
- en: Deploying an MLflow model with managed online endpoints through AML Studio
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 AML Studio 部署具有托管在线端点的 MLflow 模型
- en: Deploying an MLflow model with managed online endpoints through the Python SDK
    v2
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Python SDK v2 部署具有托管在线端点的 MLflow 模型
- en: Deploying a model with managed online endpoints through the Python SDK v2
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Python SDK v2 部署具有托管在线端点的模型
- en: Deploying a model with managed online endpoints through the Azure CLI v2
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Azure CLI v2 部署具有托管在线端点的模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In order to access your workspace, repeat the steps from the previous chapter:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问您的 workspace，请重复上一章中的步骤：
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 [https://ml.azure.com](https://ml.azure.com)。
- en: Select your workspace name.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的工作区名称。
- en: On the workspace user interface on the left-hand side, click **Compute**.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧的工作区用户界面中，点击 **计算**。
- en: On the **Compute** screen, select your compute instance and select **Start**.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **计算** 屏幕上，选择您的计算实例并选择 **启动**。
- en: '![Figure 6.1 – Start compute](img/B18003_06_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 启动计算](img/B18003_06_001.jpg)'
- en: Figure 6.1 – Start compute
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 启动计算
- en: Your compute instance will change from **Stopped** to **Starting** status.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的计算实例将从 **停止** 状态变为 **启动** 状态。
- en: In the previous chapter, we cloned the Git repository; if you have not already
    done so, continue to follow these steps. If you have already cloned the repository,
    skip to *step 9*.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一章中，我们已克隆了 Git 仓库；如果您尚未这样做，请继续按照以下步骤操作。如果您已经克隆了仓库，请跳转到 *步骤 9*。
- en: 'Open the terminal on your compute instance. Note the path will include your
    user in the directory. Type the following into the terminal to clone the sample
    notebooks into your working directory:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算实例上打开终端。注意路径将包括您的用户目录。在终端中输入以下内容以将示例笔记本克隆到您的当前工作目录：
- en: '[PRE0]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Clicking on the refresh icon shown in *Figure 6**.2* will update and refresh
    the notebooks displayed on your screen.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 *图 6**.2* 中显示的刷新图标将更新并刷新屏幕上显示的笔记本。
- en: '![Figure 6.2 – The refresh icon](img/B18003_06_002.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 刷新图标](img/B18003_06_002.jpg)'
- en: Figure 6.2 – The refresh icon
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 刷新图标
- en: 'Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
    This will display the files cloned into your working directory, as shown in *Figure
    6**.3*:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看您的 `Azure-Machine-Learning-Engineering` 目录中的笔记本。这将显示克隆到您的当前工作目录中的文件，如图 *图
    6**.3* 所示：
- en: '![Figure 6.3 – Azure-Machine-Learning-Engineering](img/B18003_06_003.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – Azure-Machine-Learning-Engineering](img/B18003_06_003.jpg)'
- en: Figure 6.3 – Azure-Machine-Learning-Engineering
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – Azure-Machine-Learning-Engineering
- en: Understanding real-time inferencing and batch scoring
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解实时推理和批量评分
- en: Models can be deployed to support different use cases and different business
    requirements. When deploying a model in production, how you choose to deploy your
    model should be based on your user requirements. If you need to have a prediction
    available in real time to support streaming or interaction with your prediction
    in other applications, then real-time inferencing will be required. Real-time
    inferencing requires compute resources to be active and available for your model
    to provide a response. If your application requires less responsive predictions
    that are stored in a file or perhaps a database, then batch inferencing would
    be the correct selection. Batch inferencing allows you to spin up and down compute
    resources.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以部署以支持不同的用例和不同的业务需求。在生产环境中部署模型时，您选择如何部署模型应基于您的用户需求。如果您需要实时预测以支持流式传输或与其他应用程序中的预测进行交互，则需要实时推理。实时推理需要计算资源保持活跃并可用，以便您的模型提供响应。如果您的应用程序需要响应较慢的预测，这些预测存储在文件或数据库中，那么批量推理将是正确的选择。批量推理允许您启动和关闭计算资源。
- en: Before model deployment, the compute for hosting a real-time web service will
    need to be selected. For real-time inferencing, **Azure Kubernetes Service** (**AKS**),
    **Azure Container Instances** (**ACI**), and **Azure** **Arc-enabled** **Kubernetes**
    are compute resources supported through your AML workspace. AKS is typically used
    to support production workloads, and ACI is typically leveraged to support lower
    environments with CPU-based workloads. Azure Arc-enabled Kubernetes is typically
    used to run inferencing with on-premises resources where clusters are managed
    in Azure Arc. In this chapter, we will also explore the option of leveraging a
    **managed online endpoint**, which will provide a level of abstraction over the
    compute resources required for model deployment.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署之前，需要选择用于托管实时Web服务的计算资源。对于实时推理，**Azure Kubernetes服务**（**AKS**）、**Azure容器实例**（**ACI**）和**Azure
    Arc启用**的**Kubernetes**是通过您的AML工作区支持的计算资源。AKS通常用于支持生产工作负载，而ACI通常用于支持基于CPU的工作负载的较低环境。Azure
    Arc启用的Kubernetes通常用于在Azure Arc中管理的集群上运行推理。在本章中，我们还将探讨利用**托管在线端点**的选项，这将提供对模型部署所需的计算资源的一定程度的抽象。
- en: Note that there are actually two different types of compute that can be leveraged
    for an online endpoint. One type is the aforementioned managed online endpoints,
    and the other is **Kubernetes online endpoints**. Managed online endpoints provide
    fully managed compute provisioning, scaling, and also host OS image updates. Kubernetes
    online endpoints are designed for teams that want to manage those items through
    their own Azure Kubernetes cluster. Managed online endpoints automate the provisioning
    of the compute, automatically update the host OS image, and provide automatic
    recovery in the event of a system failure. Due to these advantages, we will focus
    our efforts on managed online endpoints.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，实际上有几种不同的计算类型可以用于在线端点。一种类型是前面提到的托管在线端点，另一种是**Kubernetes在线端点**。托管在线端点提供完全托管的计算资源提供、扩展以及主机操作系统镜像更新。Kubernetes在线端点是为那些希望通过自己的Azure
    Kubernetes集群来管理这些项目的团队设计的。托管在线端点自动化了计算资源的提供，自动更新主机操作系统镜像，并在系统故障的情况下提供自动恢复。由于这些优势，我们将集中精力在托管在线端点上。
- en: Managed online endpoints not only provide a level of abstraction around the
    compute resources required to deploy a REST endpoint; they also support multiple
    deployments to a single endpoint, often referred to as **blue/green deployment**.
    Blue/green deployment is the practice of moving traffic from one release to another.
    So, when we deploy a managed online endpoint, we can have an initial model that
    is deployed and then deploy a new model to the same endpoint. After the new model
    is available on the managed online endpoint, we can move users to the new deployment
    with the new model. This means for each managed online endpoint, we will not only
    be deploying the managed online endpoint but also the endpoint deployments.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在线端点不仅提供了一种围绕部署REST端点所需的计算资源的抽象级别；它们还支持将多个部署到单个端点，通常称为**蓝/绿部署**。蓝/绿部署是将流量从一个版本移动到另一个版本的做法。因此，当我们部署托管在线端点时，我们可以有一个初始模型被部署，然后在该端点部署一个新的模型。当新模型在托管在线端点上可用后，我们可以将用户转移到使用新模型的新的部署。这意味着对于每个托管在线端点，我们不仅将部署托管在线端点，还将部署端点部署。
- en: Note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Managed online endpoints are required to have a unique name within an Azure
    region. Not providing a unique name will result in a failed deployment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 托管在线端点需要在 Azure 区域内具有一个唯一名称。如果不提供唯一名称，将导致部署失败。
- en: We will be exploring options to make your model available to support real-time
    use cases leveraging managed online endpoints and deployments in this chapter.
    In the next section, we will explore deploying your model with AMLS Studio for
    a low-code experience.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用托管在线端点和部署来使你的模型可用于支持实时用例的选项。在下一节中，我们将探讨使用 AMLS Studio 部署你的模型以获得低代码体验。
- en: Deploying an MLflow model with managed online endpoints through AML Studio
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 AML Studio 部署带有托管在线端点的 MLflow 模型
- en: In order to deploy a model to a web service, we will be required to define the
    environment, which includes the Conda and `pip` dependencies, our compute resources,
    and a scoring script. The **scoring script**, also called an **entry script**,
    will load the model in an initialization function, as well as handle running predictions
    with the incoming data to the web service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将模型部署到网络服务中，我们需要定义环境，这包括 Conda 和 `pip` 依赖项、我们的计算资源以及评分脚本。**评分脚本**，也称为**入口脚本**，将在初始化函数中加载模型，并处理对网络服务的传入数据进行预测。
- en: With MLflow models, not only is the model packaged but AML also understands
    how to consume the model, so there is no need to configure an environment or entry
    script for the model deployment with managed online endpoints; AML understands
    these models natively. This makes deploying the model very easy from the UI and
    through code.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MLflow 模型时，不仅模型被打包，AML 还理解如何消费模型，因此无需为具有托管在线端点的模型部署配置环境或入口脚本；AML 本地理解这些模型。这使得从
    UI 和代码中部署模型变得非常容易。
- en: In previous chapters, we leveraged MLflow to create and register models. Proceed
    to the `Chapter 6``, Prep-Model Creation & Registration.ipynb` notebook to create
    and register a model to leverage MLflow, as we did in previous chapters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们利用 MLflow 创建并注册了模型。继续到 `第 6 章`，`Prep-Model Creation & Registration.ipynb`
    笔记本以创建和注册模型，就像我们在前面的章节中所做的那样，利用 MLflow。
- en: This notebook will take you through the process of creating a model and registering
    it to the workspace, as discussed in previous chapters. However, there are a few
    points to review before creating the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本将带你通过创建模型并将其注册到工作区的过程，正如我们在前面的章节中所讨论的那样。然而，在创建模型之前，有一些要点需要回顾。
- en: 'When leveraging MLflow, we can use `autolog` to generate the model and environment
    information for us, but in the notebook, we actually set `log_models=False`, as
    shown in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当利用 MLflow 时，我们可以使用 `autolog` 为我们生成模型和环境信息，但在笔记本中，我们实际上设置了 `log_models=False`，如图所示：
- en: '![Figure 6.4 – Disable the logging of the model](img/B18003_06_004.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 禁用模型的日志记录](img/B18003_06_004.jpg)'
- en: Figure 6.4 – Disable the logging of the model
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 禁用模型的日志记录
- en: 'We set logging the model to `false`, but in the training script, we explicitly
    package up the model and log it, as shown here:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型日志记录设置为 `false`，但在训练脚本中，我们明确打包模型并记录它，如图所示：
- en: '![Figure 6.5 – MLflow modeling logging](img/B18003_06_005.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – MLflow 模型日志记录](img/B18003_06_005.jpg)'
- en: Figure 6.5 – MLflow modeling logging
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – MLflow 模型日志记录
- en: This provides us with control over the packages used for deployment. When MLflow
    releases a new version, there may be issues between the latest version of MLflow
    and Azure Machine Learning managed online endpoints. Ideally, this will not happen;
    however, as open source continues to progress, it is a good practice to include
    versions when packaging up your model to ensure you are in total control of the
    model deployment.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了对部署所使用的包的控制。当 MLflow 发布新版本时，最新版本的 MLflow 和 Azure Machine Learning 托管在线端点之间可能存在问题。理想情况下，这种情况不会发生；然而，随着开源的持续发展，在打包模型时包含版本是一个好习惯，以确保你对模型部署有完全的控制。
- en: 'After you run your `Chapter 6` `Model Creation Prep & Registration.ipynb` notebook,
    the model will be registered in the workspace. Once we have an MLflow model registered
    to the workspace, we can leverage AMLS Studio to deploy the model. For this process,
    proceed with the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行你的 `第 6 章` `模型创建准备与注册.ipynb` 笔记本之后，模型将在工作区中注册。一旦我们在工作区中注册了 MLflow 模型，我们就可以利用
    AMLS Studio 来部署模型。为此过程，请按照以下步骤操作：
- en: 'Select the model from **Model List**:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**模型列表**中选择模型：
- en: '![Figure 6.6 – Select the model from Model List](img/B18003_06_006.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 从模型列表中选择模型](img/B18003_06_006.jpg)'
- en: Figure 6.6 – Select the model from Model List
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 从模型列表中选择模型
- en: After clicking on an MLflow model, select the **Deploy** option and then the
    first option, **Deploy to real-time endpoint**, as shown in the following figure.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在点击 MLflow 模型后，选择**部署**选项，然后选择第一个选项，**部署到实时端点**，如图所示。
- en: '![Figure 6.7 – Deploy an MLflow model](img/B18003_06_007.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 部署 MLflow 模型](img/B18003_06_007.jpg)'
- en: Figure 6.7 – Deploy an MLflow model
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 部署 MLflow 模型
- en: Since the model is selected and was created with MLflow, AMLS Studio understands
    this is an MLflow model. After clicking on the **Deploy to real-time endpoint**
    option, you will be guided through creating a deployment for your model.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于模型已被选择且是用 MLflow 创建的，AMLS Studio 理解这是 MLflow 模型。在点击**部署到实时端点**选项后，您将指导创建模型的部署。
- en: '![Figure 6.8 – Configure an endpoint](img/B18003_06_008.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 配置端点](img/B18003_06_008.jpg)'
- en: Figure 6.8 – Configure an endpoint
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 配置端点
- en: 'To create a deployment for your model, follow the options in *Figure 6**.8*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要为您的模型创建部署，请按照*图 6.8*中的选项操作：
- en: For `azure-ui-endpoint`, adding a prefix or suffix to the name to make it unique
    in your region.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`azure-ui-endpoint`，在名称前或后添加前缀或后缀，使其在您的区域中唯一。
- en: For `UI` `created endpoint`
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`UI` `创建的端点`
- en: Select **Managed** for **Compute type**
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将**计算类型**选择为**托管**
- en: Select **Key-based authentication**
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择**基于密钥的认证**
- en: 'Click **Next** and review the selected model, as shown in the following figure:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**并查看所选模型，如图所示：
- en: '![Figure 6.9 – Model selection](img/B18003_06_009.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 模型选择](img/B18003_06_009.jpg)'
- en: Figure 6.9 – Model selection
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 模型选择
- en: Since you started the deployment by selecting the model, there is no additional
    selection that needs to occur. As shown in *Figure 6**.9*, you have configured
    the endpoint as well as the model selection, so the next step is to move on to
    deployment configuration.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您是通过选择模型开始部署的，因此不需要进行其他选择。如*图 6.9*所示，您已配置了端点以及模型选择，所以下一步是进行部署配置。
- en: 'By clicking the **Next** icon on the **Model** screen, you are brought to a
    screen to configure the deployment:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击**模型**屏幕上的**下一步**图标，您将进入配置部署的屏幕：
- en: '![Figure 6.10 – Configure deployment](img/B18003_06_010.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 配置部署](img/B18003_06_010.jpg)'
- en: Figure 6.10 – Configure deployment
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 配置部署
- en: As part of the deployment configuration, a deployment name is provided. For
    a given endpoint, as mentioned in the *Understanding Real-Time Inferencing and
    Batch Scoring* section, multiple deployments can be created. This allows you to
    configure the traffic pattern to test out a deployed model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为部署配置的一部分，提供了一个部署名称。对于特定的端点，如*理解实时推理和批量评分*部分所述，可以创建多个部署。这允许您配置流量模式以测试已部署的模型。
- en: In addition to the deployment name, you can configure the scoring timeout. This
    is the timeout to enforce scoring calls. You can also enable application insights
    diagnostics and data collection for the deployment. This will enable you to use
    the **Monitoring** tab to view activity for the managed online endpoint.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了部署名称外，您还可以配置评分超时。这是强制评分调用的超时时间。您还可以为部署启用应用程序洞察诊断和数据收集。这将使您能够使用**监控**选项卡查看托管在线端点的活动。
- en: Once the deployment has been configured, you will be brought to the environment
    selection tab by again clicking **Next**. Given this model was created with MLflow,
    no environment selection is required.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦配置了部署，您将再次点击**下一步**按钮，进入环境选择选项卡。鉴于此模型是用 MLflow 创建的，因此不需要进行环境选择。
- en: 'The environment was already created for the model, as shown here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的环境已经创建，如图所示：
- en: '![Figure 6.11 – Environment selection](img/B18003_06_011.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 环境选择](img/B18003_06_011.jpg)'
- en: Figure 6.11 – Environment selection
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 环境选择
- en: 'Now that the environment has been established, the compute resources should
    be selected for the model. In the `1`, as shown in the following figure:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在环境已经建立，应选择模型的计算资源。如图所示，在`1`：
- en: '![Figure 6.12 – Compute selection](img/B18003_06_012.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 计算选择](img/B18003_06_012.jpg)'
- en: Figure 6.12 – Compute selection
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 计算选择
- en: Now that the compute has been selected, the traffic allocation for this deployment
    should be configured. If you have updated your model, deploy it initially with
    traffic set to `0`, confirm it is working correctly, and then, as shown in the
    following figure, update the traffic to `100`% on the new model, given this is
    our first model deployment. This should provide a seamless experience for consumers
    of the REST API from the old model to the new model.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在已经选择了计算资源，应配置此部署的流量分配。如果您已更新了模型，最初应将流量设置为`0`进行部署，确认其正确无误后，然后，如图所示，将新模型的流量更新为`100%`，因为这是我们第一次模型部署。这应该为旧模型到新模型的REST
    API消费者提供无缝体验。
- en: '![Figure 6.13 – Configure deployment traffic](img/B18003_06_013.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 配置部署流量](img/B18003_06_013.jpg)'
- en: Figure 6.13 – Configure deployment traffic
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 配置部署流量
- en: After configuring the traffic, we can review the deployment that is about to
    occur. By clicking **Next**, AMLS Studio will bring you to the **Review** stage
    of the guided model deployment.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置流量后，我们可以查看即将发生的部署。通过点击**下一步**，AMLS Studio将带您进入引导式模型部署的**审查**阶段。
- en: '![Figure 6.14 – Review the model deployment](img/B18003_06_014.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 检查模型部署](img/B18003_06_014.jpg)'
- en: Figure 6.14 – Review the model deployment
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 检查模型部署
- en: The information shown on the screen will reflect the input you provided during
    the guided online managed deployment. After you have confirmed the settings displayed
    in the previous screenshot, click on the **Create** button to create the endpoint
    as well as the deployment.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 屏幕上显示的信息将反映您在引导式在线托管部署过程中提供的输入。在确认了前一个屏幕中显示的设置后，点击**创建**按钮以创建端点以及部署。
- en: The following figure shows the deployment progress. The managed online endpoint
    is being provisioned, given that the status of **Provisioning state** is **Creating**,
    and the blue deployment is being provisioned as well. Currently, the traffic is
    set to **0** for the **blue** deployment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了部署进度。托管在线端点正在配置中，因为**配置状态**的**状态**为**创建中**，蓝色部署也在配置中。目前，**蓝色**部署的流量设置为**0**。
- en: '![Figure 6.15 – Managed online endpoint deployment in progress](img/B18003_06_015.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – 正在进行的托管在线端点部署](img/B18003_06_015.jpg)'
- en: Figure 6.15 – Managed online endpoint deployment in progress
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 正在进行的托管在线端点部署
- en: Once the deployment has been completed, the endpoint will show that the status
    of **Provisioning state** is **Succeeded**, and **Traffic allocation** will be
    **100%** of the deployment, as shown in the following figure.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦部署完成，端点将显示**配置状态**的**状态**为**成功**，**流量分配**将是部署的**100%**，如图所示。
- en: '![Figure 6.16 – AMLS Studio deployment completed](img/B18003_06_016.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – AMLS Studio部署完成](img/B18003_06_016.jpg)'
- en: Figure 6.16 – AMLS Studio deployment completed
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – AMLS Studio部署完成
- en: Congratulations! You have successfully deployed a managed online endpoint for
    consumption. Recall that the model has columns – `Embarked`, `Loc`, `Sex`, `Pclass`,
    `Age`, `Fare`, and `GroupSize` – as input parameters. We can send JSON to the
    REST endpoint, specifying the input data columns and the data we would like to
    receive predictions to leverage the JSON schema leveraged by AMLS.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功部署了托管在线端点以供消费。回想一下，模型有列 – `Embarked`、`Loc`、`Sex`、`Pclass`、`Age`、`Fare`和`GroupSize`
    – 作为输入参数。我们可以向REST端点发送JSON，指定输入数据列和我们希望接收预测的数据，以利用AMLS使用的JSON模式。
- en: '![Figure 6.17 – Test the managed online endpoint](img/B18003_06_017.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 测试托管在线端点](img/B18003_06_017.jpg)'
- en: Figure 6.17 – Test the managed online endpoint
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 测试托管在线端点
- en: A sample of this request file is located in the `prepped_data` folder under
    the `chapter 6` folder in the Git repository. You can copy and paste this text
    into the studio test screen shown in *Figure 6**.17*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此请求文件的样本位于Git仓库中`chapter 6`文件夹下的`prepped_data`文件夹中。您可以将此文本复制并粘贴到*图6.17*所示的工作室测试屏幕中。
- en: Now that the managed online endpoint has been tested from the AMLS Studio experience,
    we will next deploy a managed online endpoint through the Python SDK.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经从AMLS Studio体验中测试了托管在线端点，我们将接下来通过Python SDK部署托管在线端点。
- en: Deploying an MLflow model with managed online endpoints through the Python SDK
    V2
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Python SDK V2部署带有托管在线端点的MLflow模型
- en: In the previous section, we leveraged AMLS Studio to deploy our MLflow model.
    In this section, we will explore code to deploy an MLflow model to a managed online
    endpoint through the SDK v2.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们利用AMLS Studio部署了我们的MLflow模型。在本节中，我们将探索通过SDK v2将MLflow模型部署到托管在线端点的代码。
- en: In order to leverage the SDK v2 for model deployment, we will leverage the `Chapter
    6` `MLFlow Model Deployment SDK V2.ipynb` notebook.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用SDK v2进行模型部署，我们将使用`第6章` `MLFlow模型部署SDK V2.ipynb`笔记本。
- en: 'To deploy a managed online endpoint through the SDK V2, follow the next steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过SDK V2部署托管在线端点，请按照以下步骤操作：
- en: To deploy the model, we will create `ManagedOnlineEndpoint` with the appropriate
    configuration. In the case of an MLflow model, we will need to specify `name`
    and `auth_mode`. In addition, we will provide `description` as well as `tags`.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要部署模型，我们将创建具有适当配置的`ManagedOnlineEndpoint`。对于MLflow模型，我们需要指定`name`和`auth_mode`。此外，我们还将提供`description`以及`tags`。
- en: '![Figure 6.18 – Configure ManagedOnlineEndpoint](img/B18003_06_018.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 配置ManagedOnlineEndpoint](img/B18003_06_018.jpg)'
- en: Figure 6.18 – Configure ManagedOnlineEndpoint
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 配置ManagedOnlineEndpoint
- en: After the endpoint has been configured, we are able to call the `create` or
    `update` method, passing in the endpoint to create the endpoint in the workspace
    with the `create_or_update` command, as shown here.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点配置完成后，我们可以调用`create`或`update`方法，传入端点以使用`create_or_update`命令在工作区中创建端点，如下所示。
- en: '![Figure 6.19 – Leveraging ml_client to create a managed online endpoint](img/B18003_06_019.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 利用ml_client创建托管在线端点](img/B18003_06_019.jpg)'
- en: Figure 6.19 – Leveraging ml_client to create a managed online endpoint
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 利用ml_client创建托管在线端点
- en: Once the endpoint has been created, you are ready to create a deployment. For
    the deployment, we will pass a name, the endpoint name, which was created in the
    previous figure, the model to deploy, the instance type of the VM to leverage
    for compute resources, as well as the instance count.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点创建完成后，您就可以创建部署了。对于部署，我们将传递一个名称、上一图创建的端点名称、要部署的模型、用于计算资源的VM实例类型以及实例数量。
- en: 'The model can be retrieved directly from the workspace by its name and version,
    as shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以直接通过其名称和版本从工作区检索，如下所示：
- en: '![Figure 6.20 – Getting the model from the registry based on name and version](img/B18003_06_020.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 根据名称和版本从注册表中获取模型](img/B18003_06_020.jpg)'
- en: Figure 6.20 – Getting the model from the registry based on name and version
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 根据名称和版本从注册表中获取模型
- en: 'The model is required for the managed online deployment. Now that the model
    has been retrieved from the workspace, it can be passed into `ManagedOnlineDeployment`,
    as shown here:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 托管在线部署需要模型。现在模型已经从工作区检索出来，可以传递给`ManagedOnlineDeployment`，如下所示：
- en: '![Figure 6.21 – Configure deployment](img/B18003_06_021.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.21 – 配置部署](img/B18003_06_021.jpg)'
- en: Figure 6.21 – Configure deployment
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – 配置部署
- en: Note `instance_type`, which is the compute being leveraged by our managed online
    endpoint. We specified here the use of `Standard_F4s_v2`, as we have great flexibility
    in the type of compute resources we can use to serve up our real-time prediction
    requests.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`instance_type`，这是我们托管在线端点所使用的计算资源。我们在这里指定了使用`Standard_F4s_v2`，因为我们有很高的灵活性来选择用于服务实时预测请求的计算资源类型。
- en: 'Once the deployment has been configured, leveraging `ml_client`, the managed
    online endpoint can be deployed with initial traffic set to `0` through the `begin_create_or_update`
    method, as shown here:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署配置完成后，通过`ml_client`，可以将托管在线端点部署到初始流量设置为`0`，如下所示：
- en: '![Figure 6.22 – Create a deployment](img/B18003_06_022.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – 创建部署](img/B18003_06_022.jpg)'
- en: Figure 6.22 – Create a deployment
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – 创建部署
- en: After the deployment has succeeded, the traffic for the endpoint can be set
    to 100% using the deployment name.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署成功后，可以使用部署名称将端点的流量设置为100%。
- en: '![Figure 6.23 – Update deployment traffic](img/B18003_06_023.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23 – 更新部署流量](img/B18003_06_023.jpg)'
- en: Figure 6.23 – Update deployment traffic
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – 更新部署流量
- en: Now that the endpoint has been deployed, the URI for the endpoint and the primary
    key can be retrieved to make a REST API call to retrieve a prediction. For the
    online endpoint, we can easily retrieve both the scoring URI as well as the primary
    key, as shown in the following code.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在端点已经部署，可以检索端点的URI和主键来调用REST API以获取预测。对于在线端点，我们可以轻松地检索评分URI和主键，如下面的代码所示。
- en: '![Figure 6.24 – Retrieve the URI and primary key](img/B18003_06_024.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – 检索URI和主键](img/B18003_06_024.jpg)'
- en: Figure 6.24 – Retrieve the URI and primary key
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 检索URI和主键
- en: Finally, a call can be made to the REST endpoint to retrieve predictions. The
    following code below takes a dataframe into the `make_predictions` function, prepares
    the request, and returns the results from the managed online endpoint.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，可以调用REST端点以检索预测。下面的代码将dataframe传递给`make_predictions`函数，准备请求，并从受管理的在线端点返回结果。
- en: '![Figure 6.25 – Making predictions](img/B18003_06_025.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 进行预测](img/B18003_06_025.jpg)'
- en: Figure 6.25 – Making predictions
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 进行预测
- en: Running the preceding code allows a dataframe to be passed and prediction results
    to be returned. This model can be tested to leverage the sample input provided
    in the file located in the `prepped_data` folder under the `chapter 6` folder
    in the Git repository. Regardless of whether the managed online endpoint was deployed
    through the SDK or AMLS Studio, the functionality is the same.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码允许传递dataframe并返回预测结果。此模型可以通过在Git仓库中`chapter 6`文件夹下的`prepped_data`文件夹中提供的样本输入进行测试。无论受管理的在线端点是通过对SDK还是AMLS
    Studio进行部署，功能都是相同的。
- en: You have been able to deploy a model through AMLS Studio and the SDK, as it
    was created as an MLflow model. In the event that MLflow is not leveraged to create
    a model, one can easily be deployed, but additional configuration is required
    to deploy it. In the next section, we will continue to deploy a model, specifying
    the environment and the script required for inferencing.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经能够通过AMLS Studio和SDK部署模型，因为它被创建为MLflow模型。如果未使用MLflow创建模型，则可以轻松部署，但需要额外的配置来部署。在下一节中，我们将继续部署模型，指定推理所需的环境和脚本。
- en: Deploying a model with managed online endpoints through the Python SDK v2
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Python SDK v2使用受管理的在线端点部署模型
- en: In the previous section, we deployed an MLflow model, but when you create a
    model that does not leverage MLflow, you need to provide two additional details
    for a successful managed online endpoint deployment. In this section, we will
    focus on adding functionality so that we can deploy our model without relying
    on MLflow to provide the environment and scoring script.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们部署了MLflow模型，但当你创建不使用MLflow的模型时，你需要提供两个额外的细节以成功部署受管理的在线端点。在本节中，我们将专注于添加功能，以便我们可以不依赖于MLflow提供环境和评分脚本来部署我们的模型。
- en: 'In order to deploy a managed online endpoint leveraging the SDK v2 and not
    relying on MLflow to provide the environment and scoring script, we will create
    those in this section as you leverage the notebook: `Chapter 6` `Model Deployment`
    `SDK V2.ipynb`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用SDK v2部署受管理的在线端点并利用笔记本：`第6章` `模型部署` `SDK V2.ipynb`，而不依赖于MLflow提供环境和评分脚本，我们将在本节中创建这些内容：
- en: Our first step is to create our `score.py` file. This is the file that will
    be used to load the model and serve a request to the endpoint.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的第一步是创建我们的`score.py`文件。这是用于加载模型并向端点发送请求的文件。
- en: 'The following code snippet provides the information required for the entry
    script:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段提供了入口脚本所需的信息：
- en: '![Figure 6.26 – The score.py script](img/B18003_06_026.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.26 – score.py脚本](img/B18003_06_026.jpg)'
- en: Figure 6.26 – The score.py script
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – score.py脚本
- en: In the score script shown in the previous figure, there are two required functions,
    `init` and `run`. The `init` function tells AML how to load a model. The model
    will be provided as part of the deployment configuration, the model path is set,
    and `joblib` is leveraged to load the model. The `model` global variable is leveraged
    to hold the model. When the REST endpoint is called, the `run` function will be
    called. The data from the API call will be passed into the `run` function. Here,
    a dictionary is set to retrieve the information, which is transformed into a dataframe
    that is then passed to the `model.predict` function. The results of `model.predict`
    are passed to a list and returned from the function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个图中所示的评分脚本中，有两个必需的函数，`init`和`run`。`init`函数告诉 AML 如何加载模型。模型将作为部署配置的一部分提供，模型路径被设置，并利用`joblib`来加载模型。`model`全局变量被用来保存模型。当
    REST 端点被调用时，`run`函数将被调用。API 调用的数据将被传递到`run`函数。在这里，设置了一个字典来检索信息，该信息被转换成一个 dataframe，然后传递给`model.predict`函数。`model.predict`的结果被传递到一个列表中，并从函数中返回。
- en: In addition to the score script, we will need to have the model. In the previous
    section, we retrieved the registered model leveraging the SDK v2 based on its
    name and version, but we can also search across the experiments and retrieve the
    model from them. The sample code provided here shows searching for the model from
    the experiment and downloading it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除了评分脚本，我们还需要拥有模型。在上一节中，我们根据名称和版本使用 SDK v2 检索了已注册的模型，但我们也可以在实验中搜索并从中检索模型。这里提供的示例代码展示了如何从实验中搜索模型并下载它。
- en: '![Figure 6.27 – Find and download the model](img/B18003_06_027.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.27 – 查找并下载模型](img/B18003_06_027.jpg)'
- en: Figure 6.27 – Find and download the model
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.27 – 查找并下载模型
- en: In addition to the scoring script, we will need to provide an environment to
    use for deployment.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了评分脚本，我们还需要提供一个用于部署的环境。
- en: '![Figure 6.28 – A deployment environment](img/B18003_06_028.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.28 – 部署环境](img/B18003_06_028.jpg)'
- en: Figure 6.28 – A deployment environment
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28 – 部署环境
- en: In the previous Jupyter notebook when we created the model, we already created
    an environment, but we need to add the `azureml-defaults` package for successful
    deployment to a managed online endpoint. Therefore, instead of using the already
    registered environment, we are creating a new environment to pass to the deployment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 Jupyter 笔记本中，当我们创建模型时，我们已经创建了一个环境，但我们需要添加`azureml-defaults`包以确保成功部署到管理的在线端点。因此，我们不是使用已注册的环境，而是创建一个新的环境以传递给部署。
- en: 'In order to deploy a managed online endpoint that was not created with MLflow,
    the key difference is in the deployment configuration. Note that *Figure 6**.21*
    provided the code snippet required to deploy a model created with MLflow. Compare
    that code to the code found in the following snippet:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了部署一个未使用 MLflow 创建的管理在线端点，关键区别在于部署配置。请注意，*图 6**.21* 提供了部署使用 MLflow 创建的模型的代码片段。将此代码与以下片段中的代码进行比较：
- en: '![Figure 6.29 – Managed online endpoint deployment with score script and environment](img/B18003_06_029.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.29 – 使用评分脚本和环境的在线端点部署](img/B18003_06_029.jpg)'
- en: Figure 6.29 – Managed online endpoint deployment with score script and environment
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.29 – 使用评分脚本和环境的在线端点部署
- en: Note that in the previous code block, the `score.py` file is specified as well
    as the environment.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码块中，指定了`score.py`文件以及环境。
- en: 'This managed online endpoint can again be tested. As the `score.py` file highlighted,
    this REST API will be expecting a different schema for its input:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个管理在线端点又可以进行测试。正如`score.py`文件所强调的，这个 REST API 将期望其输入有不同的模式：
- en: '[PRE1]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You now have an understanding of how powerful tool-managed online endpoints
    are for model deployment. In addition to a guided AMLS Studio experience as well
    as full capabilities within the SDK, we can also leverage the Azure CLI v2 to
    manage the deployment process. In the next section, we will explore leveraging
    the Azure CLI v2 for this capability.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了工具管理的在线端点对于模型部署是多么强大的工具。除了引导式的 AMLS Studio 体验以及 SDK 中的全部功能外，我们还可以利用
    Azure CLI v2 来管理部署过程。在下一节中，我们将探讨如何利用 Azure CLI v2 来实现这一功能。
- en: Deploying a model for real-time inferencing with managed online endpoints through
    the Azure CLI v2
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 Azure CLI v2 部署模型以进行实时推理
- en: In this section, we will leverage a managed online endpoint and deploy it with
    the Azure Machine Learning CLI v2\. The CLI v2 will leverage YAML files holding
    the configuration required for our deployment in the commands we call. Remember
    the requirement for a unique managed online endpoint name, so when running the
    code, be sure to update your managed online endpoint name in both the YAML files
    and the CLI command.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用托管在线端点，并使用 Azure 机器学习 CLI v2 部署它。CLI v2 将在我们的调用命令中利用包含我们部署所需配置的 YAML
    文件。请记住，对于唯一的托管在线端点名称的要求，所以在运行代码时，务必在 YAML 文件和 CLI 命令中更新您的托管在线端点名称。
- en: 'To use the new Azure CLI v2 extension, we are required to have an Azure CLI
    version greater than 2.15.0\. This can easily be checked by using the `az version`
    command to check your Azure CLI version:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用新的 Azure CLI v2 扩展，我们需要拥有一个大于 2.15.0 的 Azure CLI 版本。这可以通过使用 `az version`
    命令来检查您的 Azure CLI 版本来轻松完成：
- en: 'On your compute instance, navigate to the terminal and type the following command:
    `az version`. After typing that command, you should see that the Azure CLI v2
    is installed on your compute instance, as shown in the following figure.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算实例上，导航到终端并输入以下命令：`az version`。在输入该命令后，您应该看到 Azure CLI v2 已安装在您的计算实例上，如图所示。
- en: '![Figure 6.30 – The Azure CLI v2 with the ml extension installed](img/B18003_06_030.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.30 – 安装了 ml 扩展的 Azure CLI v2](img/B18003_06_030.jpg)'
- en: Figure 6.30 – The Azure CLI v2 with the ml extension installed
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.30 – 安装了 ml 扩展的 Azure CLI v2
- en: 'You can update your `ml` extension through the following command:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过以下命令更新您的 `ml` 扩展：
- en: '[PRE2]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After installing the new extensions, we can again run `az version` to confirm
    that the extension is installed and up to date so that we can proceed to log in.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 安装新扩展后，我们再次运行 `az version` 以确认扩展已安装并更新，以便我们可以继续登录。
- en: You will need to log in with the Azure CLI to handle your deployment by typing
    `az login`. You will be prompted to open a browser and type a device login to
    authenticate.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要使用 Azure CLI 登录来处理您的部署，通过输入 `az login`。您将被提示打开浏览器并输入设备登录以进行认证。
- en: After authenticating, you should set your default Azure subscription. Your Azure
    subscription ID can be easily retrieved by finding an Azure Machine Learning workspace
    within the portal and copying the guide displayed in the **Subscription ID** section
    of the **Overview** tab, as shown here.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在认证后，您应该设置您的默认 Azure 订阅。您可以通过在门户中找到一个 Azure 机器学习工作区并复制显示在 **概述** 选项卡的 **订阅 ID**
    部分的指南来轻松检索您的 Azure 订阅 ID，如图所示。
- en: '![Figure 6.31 – Azure Subscription ID](img/B18003_06_031.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.31 – Azure 订阅 ID](img/B18003_06_031.jpg)'
- en: Figure 6.31 – Azure Subscription ID
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.31 – Azure 订阅 ID
- en: 'Back in the terminal on your compute instance, set the account for the Azure
    CLI to leverage by typing `az account set -s XXXX- XXXX - XXXX - XXXX – XXXX`,
    replacing `XXXX- XXXX - XXXX - XXXX – XXXX` with your subscription ID. Then, set
    variables to hold your resource group name, the location of your AML workspace,
    and your workspace name:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算实例的终端中，通过输入 `az account set -s XXXX- XXXX - XXXX - XXXX – XXXX` 来设置 Azure
    CLI 以利用的帐户，用您的订阅 ID 替换 `XXXX- XXXX - XXXX - XXXX – XXXX`。然后，设置变量以保存您的资源组名称、AML
    工作区位置和您的工区名称：
- en: '[PRE3]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Based on where your AMLS workspace is deployed, the `LOCATION` value can be
    found here: [https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations](https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的 AMLS 工作区部署的位置，`LOCATION` 值可以在以下位置找到：[https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations](https://github.com/microsoft/azure-pipelines-extensions/blob/master/docs/authoring/endpoints/workspace-locations)。
- en: 'Once the variables have been set, you can run the following:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置了变量，您就可以运行以下命令：
- en: '[PRE6]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Navigate to the [*Chapter 6*](B18003_06.xhtml#_idTextAnchor086) directory in
    your AML workspace.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到您的 AML 工作区中的 [*第 6 章*](B18003_06.xhtml#_idTextAnchor086) 目录。
- en: To create the required files, run the `Chapter 6` `Model Deployment CLI v2 -
    Create Scripts.ipynb` notebook, which will create an `endpoint.yml` file, a `deployment.yml`
    file, and a `score.py` file for inferencing. The files will be generated in a
    folder called `CLI_v2_ManagedOnlineEndpoint`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建所需的文件，请运行 `第 6 章` `模型部署 CLI v2 - 创建脚本.ipynb` 笔记本，这将创建一个 `endpoint.yml` 文件、一个
    `deployment.yml` 文件和一个 `score.py` 文件用于推理。这些文件将在名为 `CLI_v2_ManagedOnlineEndpoint`
    的文件夹中生成。
- en: 'In the notebook, we will create the directory by running the following code:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中，我们将通过运行以下代码来创建目录：
- en: '![Figure 6.32 – Directory creation](img/B18003_06_032.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图6.32 – 目录创建](img/B18003_06_032.jpg)'
- en: Figure 6.32 – Directory creation
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.32 – 目录创建
- en: For the managed online endpoint, we will also create a `score.py` file. The
    `score` script will leverage our model and provide the `init` and `run` functions
    that our previous `score.py` file provided.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于托管在线端点，我们还将创建一个`score.py`文件。`score`脚本将利用我们的模型，并提供之前`score.py`文件提供的`init`和`run`函数。
- en: '![Figure 6.33 – The score.py file](img/B18003_06_033.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图6.33 – score.py文件](img/B18003_06_033.jpg)'
- en: Figure 6.33 – The score.py file
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.33 – score.py文件
- en: Next, we will create the endpoint .`yml` file. We can create a basic file that
    contains the required attributes of the name and the authorization mode. The authorization
    mode can be specified as a key or `aml_token`. Key-based authentication will not
    expire, but the Azure ML token-based authentication will. We will proceed with
    key-based authentication.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建端点`.yml`文件。我们可以创建一个包含名称和授权模式所需属性的基文件。授权模式可以指定为密钥或`aml_token`。基于密钥的认证不会过期，但Azure
    ML基于令牌的认证会。我们将继续使用基于密钥的认证。
- en: Note
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using `aml_token`, you can get a new token by running the `az ml online-endpoint`
    `get-credentials` command.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`aml_token`时，您可以通过运行`az ml online-endpoint` `get-credentials`命令来获取一个新的令牌。
- en: 'Run the command to generate the `endpoint.yml` file with the same key value
    as `auth_mode`, as shown here:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行命令以生成与`auth_mode`相同的键值的`endpoint.yml`文件，如下所示：
- en: '![Figure 6.34 – Managed online endpoint configuration](img/B18003_06_034.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图6.34 – 托管在线端点配置](img/B18003_06_034.jpg)'
- en: Figure 6.34 – Managed online endpoint configuration
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.34 – 托管在线端点配置
- en: Remember to update your name to be something unique; if `titanic-managed-online-endpoint`
    is already deployed in your region, your deployment will fail.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住将您的名称更新为唯一的名称；如果`titanic-managed-online-endpoint`已在您的区域部署，您的部署将失败。
- en: For the managed online endpoint deployment, we will also generate a `deployment.yml`
    file. In this file, we need to specify the model, the scoring script, the environment,
    and the instance type used for model deployment.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于托管在线端点部署，我们还将生成一个`deployment.yml`文件。在此文件中，我们需要指定模型、评分脚本、环境和用于模型部署的实例类型。
- en: 'Creation of the `deployment.yml` file:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`deployment.yml`文件：
- en: '![Figure 6.35 – Creation of the deployment.yml file](img/B18003_06_035.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图6.35 – 创建`deployment.yml`文件](img/B18003_06_035.jpg)'
- en: Figure 6.35 – Creation of the deployment.yml file
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.35 – 创建`deployment.yml`文件
- en: In this file, you will specify the registered model name and version, the registered
    environment name and version, the type of compute resources leveraged for the
    deployed model, and where the scoring script is located. In the `deployment.yml`
    file, you should update `endpoint_name` to reflect the name you chose in your
    `endpoint.yml` file.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在此文件中，您需要指定已注册的模型名称和版本、已注册的环境名称和版本、用于部署模型的计算资源类型，以及评分脚本所在的位置。在`deployment.yml`文件中，您应更新`endpoint_name`以反映您在`endpoint.yml`文件中选择的名称。
- en: After running the notebook and selecting a unique endpoint name, we can leverage
    CLI v2 to deploy our model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 运行笔记本并选择一个唯一的端点名称后，我们可以利用CLI v2来部署我们的模型。
- en: 'In the command line, open the `CLI_v2_ManagedOnlineEndpoint` directory, and
    from that directory, create your `online-endpoint` with the following command:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中，打开`CLI_v2_ManagedOnlineEndpoint`目录，并从该目录创建您的`online-endpoint`，使用以下命令：
- en: '[PRE7]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that `titanic-online-endpoint` should be replaced with your managed online
    endpoint name.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`titanic-online-endpoint`应替换为您的托管在线端点名称。
- en: 'After the endpoint is created, you can now create `online-deployment`, leveraging
    the following command:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 端点创建后，您现在可以创建`online-deployment`，使用以下命令：
- en: '[PRE8]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once again, note that `titanic-online-endpoint` should be replaced with your
    managed online endpoint name.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意`titanic-online-endpoint`应替换为您的托管在线端点名称。
- en: Once the deployment is complete, the endpoint will be available for testing.
    The endpoint is available from the **Endpoints** section in your AML workspace,
    as shown here.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，端点将可用于测试。端点可通过AML工作区的**端点**部分访问，如下所示。
- en: '![Figure 6.36 – An online endpoint](img/B18003_06_036.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图6.36 – 在线端点](img/B18003_06_036.jpg)'
- en: Figure 6.36 – An online endpoint
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.36 – 在线端点
- en: We can test the endpoint by clicking on its name and selecting the **Test**
    tab on the endpoint.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过单击端点名称并选择端点的**测试**选项卡来测试端点。
- en: 'In the test section, we can provide data for inferencing after typing into
    the **Test** box to retrieve results from the web service, as shown here:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试部分，我们可以在**测试**框中输入数据以进行推理，然后从网络服务中检索结果，如下所示：
- en: '[PRE9]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure 6.37 – Testing an online endpoint](img/B18003_06_037.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图6.37 – 测试在线端点](img/B18003_06_037.jpg)'
- en: Figure 6.37 – Testing an online endpoint
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.37 – 测试在线端点
- en: This sample request can be found in the `chapter 6` folder under the `prepped_data`
    folder in a file named `sample_request_cli.json`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例请求可以在`prepped_data`文件夹下的`chapter 6`文件夹中名为`sample_request_cli.json`的文件中找到。
- en: In this section, you were able to deploy a web service utilizing a managed online
    endpoint in AMLS through the Azure CLI v2\. This feature takes advantage of schema
    files to provide the deployment definitions in configuration files to enable deployment.
    Managed online endpoints prevent data scientists or citizen data scientists from
    having to be concerned about the infrastructure required to support hosting a
    web service, to provide real-time inferencing to support your use cases.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您能够通过Azure CLI v2在AMLS中利用托管在线端点部署网络服务。该功能利用模式文件在配置文件中提供部署定义，以实现部署。托管在线端点可以防止数据科学家或公民数据科学家担心支持托管网络服务所需的基础设施，以提供支持您的用例的实时推理。
- en: Congratulations on deploying a managed online endpoint through the CLI! This
    will be especially useful in [*Chapter 9*](B18003_09.xhtml#_idTextAnchor119),
    *Productionizing Your Workload with MLOps*. After testing out your managed online
    endpoints, be sure to delete them, as they use up compute resources.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您通过CLI部署了托管在线端点！这将在[*第9章*](B18003_09.xhtml#_idTextAnchor119)中特别有用，*使用MLOps生产化您的负载*。在测试您的托管在线端点后，请务必删除它们，因为它们会消耗计算资源。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, the focus was on deploying your model as a REST endpoint to
    support real-time inferencing use cases. We saw that we are able to leverage AMLS
    Studio for a low-code deployment experience. We also leveraged SDK v2 to deploy
    models to managed online endpoints. We continued by deploying models through CLI
    v2 to support model deployment for real-time inferencing. These sections demonstrated
    deploying real-time web services through low-code, code-first, and configuration-driven
    approaches. These capabilities empower you to deploy in a variety of ways.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，重点在于将您的模型作为REST端点部署，以支持实时推理用例。我们了解到，我们可以利用AMLS Studio实现低代码部署体验。我们还利用SDK
    v2将模型部署到托管在线端点。接着，我们通过CLI v2部署模型以支持实时推理的模型部署。这些部分展示了通过低代码、代码优先和配置驱动的方法部署实时网络服务。这些功能使您能够以多种方式部署。
- en: In the next chapter, we will learn how to leverage batch-inferencing to support
    our use cases.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何利用批量推理来支持我们的用例。
