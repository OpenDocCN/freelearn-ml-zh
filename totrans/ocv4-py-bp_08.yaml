- en: Learning to Recognize Facial Emotions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We previously familiarized ourselves with the concepts of object detection and
    object recognition. In this chapter, we will develop an app that does both together.
    The app will be able to detect your own face in each captured frame of a webcam
    live stream, recognize your facial emotion, and label it on the **Graphical User
    Interface** (**GUI**).
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to develop an app that combines both **face detection**
    and **face recognition**, with a focus on recognizing emotional expressions for
    the detected face. After reading the chapter, you will be able to use both face
    detection and recognition in different applications of your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about face detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collecting data for machine learning tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding facial emotion recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will touch upon two classic algorithms that come bundled with OpenCV—**Haar
    cascade classifiers** and **MLPs**. While the former can be used to rapidly detect
    (or to locate, and to answer the question *Where*?) objects of various sizes and
    orientations in an image, the latter can be used to recognize them (or to identify,
    and answer the question *What*?).
  prefs: []
  type: TYPE_NORMAL
- en: Learning MLPs is also the first step toward learning one of the most trendy
    algorithms these days—**deep neural networks** (**DNNs**). We will use PCA to
    speed up and improve the accuracy of the algorithm when we have not got a huge
    amount of data, to improve the accuracy of our model.
  prefs: []
  type: TYPE_NORMAL
- en: We will collect our training data ourselves to show you how that process is
    done, in order for you to be able to train machine learning models for tasks that
    don't have data readily available. Unfortunately, not having the right data is
    still one of the biggest obstacles to the widespread adoption of machine learning
    these days.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how to get started before we get our hands dirty.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code that we present in this chapter at our GitHub repository,
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8).
  prefs: []
  type: TYPE_NORMAL
- en: Other than that, you should download the **Haar cascade** files from the official
    OpenCV repository at [https://github.com/opencv/opencv/blob/master/data/haarcascades/](https://github.com/opencv/opencv/blob/master/data/haarcascades/),
    or copy them from the installation directory of your machine to the project repository.
  prefs: []
  type: TYPE_NORMAL
- en: Planning the app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reliable recognition of faces and facial expressions is a challenging task
    for **artificial intelligence** (**AI**), yet humans are able to perform these
    kinds of tasks with ease. To make our task feasible, we will limit ourselves to
    the following limited emotional expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: Neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Happy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surprised
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disgusted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today's state-of-the-art models range all the way from 3D deformable face models
    fitting over **convolutional neural networks** (**CNNs**), to deep learning algorithms.
    Granted, these approaches are significantly more sophisticated than our approach.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, an **MLP** is a classic algorithm that has helped transform the field of
    machine learning, so for educational purposes, we will stick to a set of algorithms
    that come bundled with OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: 'To arrive at such an app, we need to solve the following two challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Face detection**: We will use the popular Haar cascade classifier by Viola
    and Jones, for which OpenCV provides a whole range of pre-trained exemplars. We
    will make use of face cascades and eye cascades to reliably detect and align facial
    regions from frame to frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial expression recognition**: We will train an MLP to recognize the six
    different emotional expressions listed earlier, in every detected face. The success
    of this approach will crucially depend on the training set that we assemble, and
    the preprocessing that we choose to apply to each sample in the set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to improve the quality of our self-recorded training set, we will make
    sure that all data samples are aligned using **affine transformations**, and will
    reduce the dimensionality of the feature space by applying **PCA**. The resulting
    representation is sometimes also referred to as **Eigenfaces**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will combine the algorithms mentioned earlier in a single end-to-end app
    that annotates a detected face with the corresponding facial expression label
    in each captured frame of a video live stream. The end result might look something
    like the following screenshot, capturing my sample reaction when the code first
    ran:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4d5bb8e-eb59-466a-b7d4-50e341e42db1.png)'
  prefs: []
  type: TYPE_IMG
- en: The final app will consist of the main script that integrates the process flow
    end to end—that is, from face detection to facial expression recognition, as well
    as some utility functions to help along the way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the end product will require several components that are located in the
    `chapter8/` directory of the book''s GitHub repository, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chapter8.py`: This is the main script and entry point for the chapter, and
    we will use this for both data collection and demo. It will have the following
    layouts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter8.FacialExpressionRecognizerLayout`: This is a custom layout based
    on `wx_gui.BaseLayout` that will detect a face in each video frame and predict
    the corresponding class label by using a pre-trained model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter8.DataCollectorLayout`: This is a custom layout based on `wx_gui.BaseLayout`
    that will collect image frames, detect a face therein, assign a label using a
    user-selected facial expression label, and will save the frames into the `data/`
    directory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wx_gui.py`: This is a link to our `wxpython` GUI file that we developed in
    [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml), *Fun with Filters*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detectors.FaceDetector`: This is a class that will encompass all the code
    for face detection based on Haar cascades. It will have the following two methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detect_face`: This method detects faces in a grayscale image. Optionally,
    the image is downscaled for better reliability. Upon successful detection, the
    method returns the extracted head region.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`align_head`: This method preprocesses an extracted head region with affine
    transformations, such that the resulting face appears centered and upright.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params/`: This is a directory that contains the default Haar cascades that
    we use for the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data/`: We will write all the code to store and process our custom data here.
    The code is split into the following files:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store.py`: This is a file where we put all the helper functions to write the
    data to disk and to load the data from the disk into computer memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`process.py`: This is a file that will contain all the code to preprocess the
    data before saving. It will also contain the code to construct features from the
    raw data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will discuss these components in detail. First,
    let's look at the face detection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV comes preinstalled with a range of sophisticated classifiers for general-purpose
    object detection. These all have very similar APIs and are easy to use, once you
    know what you are looking for. Perhaps the most commonly known detector is the **cascade
    of Haar-based feature detectors** for face detection, which was first introduced
    by Paul Viola and Michael Jones in their paper *Rapid Object Detection using a
    Boosted Cascade of Simple Features* in 2001.
  prefs: []
  type: TYPE_NORMAL
- en: A Haar-based feature detector is a machine learning algorithm that is trained
    on a lot of positive and negative labeled samples. What will we do in our application
    is take a pre-trained classifier that comes with OpenCV (you can find the link
    in the *Getting started *section). But first, let's take a closer look at how
    the classifier works.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about Haar-based cascade classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every book on OpenCV should at least mention the Viola-Jones face detector.
    Invented in 2001, this cascade classifier disrupted the field of computer vision,
    as it finally allowed real-time face detection and face recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The classifier is based on **Haar-like features** (similar to **Haar basis functions**) that sum
    up the pixel intensities in small regions of an image, as well as capture the
    difference between adjacent image regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot visualizes four rectangle features. The visualization
    works to calculate the value of the feature applied at a location. You should
    sum up all pixel values in the dark gray rectangle and subtract this value from
    the sum of all pixel values in the white rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83c2064f-8325-4c2e-af90-ab2fdd8ded16.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous screenshot, the top row shows two examples of an edge feature
    (that is, you can detect edges with them), either vertically oriented (top left)
    or oriented at a 45º angle (top right). The bottom row shows a line feature (bottom
    left) and a center-surround feature (bottom right).
  prefs: []
  type: TYPE_NORMAL
- en: Applying these filters at all possible locations allows the algorithm to capture
    certain details of human faces, such as the fact that eye regions are usually
    darker than the region surrounding the cheeks.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a common Haar feature would have a dark rectangle (representing the eye
    region) atop a bright rectangle (representing the cheek region). Combining this
    feature with a bank of rotated and slightly more complicated **wavelets**, Viola
    and Jones arrived at a powerful feature descriptor for human faces. In an additional
    act of brilliance, these guys came up with an efficient way to calculate these
    features, making it possible for the first time to detect faces in real time.
  prefs: []
  type: TYPE_NORMAL
- en: The final classifier is a weighted sum of small weaker classifiers, each of
    whose binary classifiers are based on a single feature described previously. The
    hardest part is to figure out which combinations of features are helpful for detecting
    different types of objects. Luckily, OpenCV contains a collection of such classifiers.
    Let's take a look at some of these in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding pre-trained cascade classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even better, this approach not only works for faces but also for eyes, mouths,
    full bodies, company logos; you name it. In the following table, a number of pre-trained
    classifiers are shown that can be found under the OpenCV install path in the `data`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Cascade classifier types** | **XML filenames** |'
  prefs: []
  type: TYPE_TB
- en: '| Face detector (default) | **`haarcascade_frontalface_default.xml`** |'
  prefs: []
  type: TYPE_TB
- en: '| Face detector (fast Haar) | `haarcascade_frontalface_alt2.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Eye detector | `haarcascade_eye.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Mouth detector | `haarcascade_mcs_mouth.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Nose detector | `haarcascade_mcs_nose.xml` |'
  prefs: []
  type: TYPE_TB
- en: '| Full body detector | `haarcascade_fullbody.xml` |'
  prefs: []
  type: TYPE_TB
- en: In this chapter, we will only use `haarcascade_frontalface_default.xml` and  `haarcascade_eye.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: If you are wearing glasses, make sure to use `haarcascade_eye_tree_eyeglasses.xml` for
    eye detection instead.
  prefs: []
  type: TYPE_NORMAL
- en: We will first look at how to use a cascade classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained cascade classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A cascade classifier can be loaded and applied to an image (grayscale) using
    the following code, where we first read the image, then convert it to grayscale,
    and finally detect all the faces using a cascade classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From the previous code, the `detectMultiScale` function comes with a number
    of options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`minFeatureSize` is the minimum face size to consider—for example, 20 x 20 pixels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`searchScaleFactor` is the amount by which we rescale the image (scale pyramid).
    For example, a value of `1.1` will gradually reduce the size of the input image
    by 10 %, making it more likely for a face (image) with a larger value to be found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minNeighbors` is the number of neighbors that each candidate rectangle will
    have to retain. Typically, we choose `3` or `5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags` is an options object used to tweak the algorithm—for example, whether
    to look for all faces or just the largest face (`cv2.cv.CASCADE_FIND_BIGGEST_OBJECT`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If detection is successful, the function will return a list of bounding boxes
    (`faces`) that contain the coordinates of the detected face regions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we iterate through the returned faces and add a rectangle
    outline with a thickness of `2` pixels to each of the faces.
  prefs: []
  type: TYPE_NORMAL
- en: If your pre-trained face cascade does not detect anything, a common reason is
    usually that the path to the pre-trained cascade file could not be found. In this
    case, `CascadeClassifier` will fail silently. Thus, it is always a good idea to
    check whether the returned classifier `casc = cv2.CascadeClassifier(filename)` is
    empty, by checking `casc.empty()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what you should get if you run the code on the `Lenna.png` picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e215013d-803e-4bd0-a29f-344637650c9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0
  prefs: []
  type: TYPE_NORMAL
- en: From the previous screenshot, on the left, you see the original image, and on
    the right is the image that was passed to OpenCV, and the rectangle outline of
    the detected face.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try to wrap this detector into a class to make it usable for our
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the FaceDetector class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All relevant face detection code for this chapter can be found as part of the
    `FaceDetector` class in the `detectors` module. Upon instantiation, this class
    loads two different cascade classifiers that are needed for preprocessing—namely,
    a `face_cascade` classifier and an `eye_cascade` classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Because our preprocessing requires a valid face cascade, we make sure that
    the file can be loaded. If not, we throw a `ValueError` exception, so the program
    will terminate and notify the user what went wrong, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We do the same thing for the eye classifier as well, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Face detection works best on low-resolution grayscale images. This is why we
    also store a scaling factor (`scale_factor`) so that we can operate on downscaled
    versions of the input image if necessary, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have set up the class initialization, let's take a look at the algorithm
    that detects the faces.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting faces in grayscale images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will put what we learned in the previous section into a method that
    will take an image and return the biggest face in the image. We are returning
    the biggest face to simplify things since we know that, in our application, there
    is going to be a single user sitting in front of the webcam. As a challenge, you
    could try to expand this to work with more than one face!
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the method to detect the biggest face (`detect_face`). Let''s go through
    it step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the last section, first, we convert the argument RGB image to grayscale
    and scale it by `scale_factor` by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we detect the faces in the grayscale image, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate over the detected faces and outline if the `outline=True` keyword
    argument was passed to `detect_face`. OpenCV returns us `x, y` coordinates of
    the top-left location and `w, h` width and height of the head. So, in order to
    construct the outline, we just calculate the bottom and right coordinates of the
    outline, and call the `cv2.rectangle` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We crop the head out of the original RGB image. This will be handy if we want
    to do more processing on the head (for example, recognize the facial expression).
    Run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We return the following 4-tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Boolean value to check whether the detection was successful or not
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The original image with the outline of the faces added (if requested)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A cropped image of the head to use as needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinates of the location of the head in the original image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the case of success, we return the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of failure, we return that no head was found, and return `None` for
    anything that is undetermined, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's look at what happens after we detect the faces, to get them ready
    for machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing detected faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a face has been detected, we might want to preprocess the extracted head
    region before applying classification to it. Although the face cascade is fairly
    accurate, for the recognition, it is important that all the faces are upright
    and centered on the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what we want to accomplish:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d57ee527-7d22-4fef-bd90-e7df1f65b0a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding screenshot, as this is not a passport photo,
    the model has her head slightly tilted to the side while looking over her shoulder.
    The facial region, as extracted by the face cascade, is shown in the middle thumbnail
    in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compensate for the head orientation and position in the detected
    box, we aim to rotate, move, and scale the face so that all data samples will
    be perfectly aligned. This is the job of the `align_head` method in the `FaceDetector`
    class, shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we have hardcoded some parameters that are used to align
    the heads. We want all eyes to be 25 % below the top of the final image and 20
    % from the left and right edges, and this function is going to return a processed
    image of the head that has a fixed size of 200 x 200 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The first step of the process is to detect where the eyes are in the image,
    after which we will use their location to construct the necessary transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting the eyes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, OpenCV comes with a few eye cascades that can detect both open
    and closed eyes, such as `haarcascade_eye.xml`. This allows us to calculate the
    angle between the line that connects the center of the two eyes and the horizon
    so that we can rotate the face accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, adding eye detectors will reduce the risk of having false positives
    in our dataset, allowing us to add a data sample only if both the head and the
    eyes have been successfully detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the eye cascade from the file in the `FaceDetector` constructor,
    it is applied to the input image (`head`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If we are unsuccessful and the cascade classifier couldn't find an eye, OpenCV
    will throw a `RuntimeError`. Here, we are catching it and returning a `(False,
    head)` tuple, indicating that we failed to align the head.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we try to order the references to the eyes that the classifier has found.
    We set `left_eye` to be the eye with the lower first coordinate—that is, the one
    on the left, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the location of both of the eyes, we want to figure out what
    kind of transformation we want to make in order to put the eyes in the hardcoded
    positions—that is, 25% from the sides and 25% below the top of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transforming the face is a standard process that can be achieved by warping
    the image using `cv2.warpAffine` (recall [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*). We will follow
    the next steps to achieve this transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we calculate the angle (in degrees) between the line that connects the
    two eyes and a horizontal line, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we derive a scaling factor that will scale the distance between the two
    eyes to be exactly 50% of the image width, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'With the two parameters (`eye_angle_deg` and `eye_size_scale`) in hand, we
    can now come up with a suitable rotation matrix that will transform our image,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will make sure that the center of the eyes will be centered in the
    image, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we arrive at an upright scaled version of the facial region that looks
    like the third image (named as Training Image) in the previous screenshot (eye
    regions are highlighted only for the demonstration), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After this step, we know how to extract nicely aligned, cropped, and rotated
    images from unprocessed images. Now, it's time to take a look at how to use these
    images to identify facial expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The facial-expression-recognition pipeline is encapsulated in `chapter8.py`.
    This file consists of an interactive GUI that operates in two modes (**training**
    and **testing**), as described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our entire application is divided into parts, mentioned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the application in the `collect` mode using the following command from
    the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The previous command will pop up a GUI in the data collection mode to assemble
    a training set,
  prefs: []
  type: TYPE_NORMAL
- en: training an MLP classifier on the training set via `python train_classifier.py`.
    Because this step can take a long time, the process takes place in its own script.
    After successful training, store the trained weights in a file, so that we can
    load the pre-trained MLP in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, again running the GUI in the `demo` mode as follows, we will be able
    to see how good the facial recognition is on the real data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this mode, you will have a GUI to classify facial expressions on a live video
    stream in real time. This step involves loading several pre-trained cascade classifiers
    as well as our pre-trained MLP classifier. These classifiers will then be applied
    to every captured video frame.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how you can build an application to collect training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling a training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can train an MLP, we need to assemble a suitable training set. This
    is done because chances are that your face is not yet part of any dataset out
    there (the **National Security Agency's** (**NSA's**) private collection doesn't
    count), thus we will have to assemble our own. This is done most easily by returning
    to our GUI application from the previous chapters that can access a webcam, and
    operate on each frame of a video stream.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to subclass the `wx_gui.BaseLayout` and tweak the **user interface**
    (**UI**) to our liking. We will have two classes for the two different modes.
  prefs: []
  type: TYPE_NORMAL
- en: The GUI will present the user with the option of recording one of the following
    six emotional expressions—namely, neutral, happy, sad, surprised, angry, and disgusted.
    Upon clicking a button, the app will take a snapshot of the detected facial region
    and add it to the data collection in a file.
  prefs: []
  type: TYPE_NORMAL
- en: These samples can then be loaded from the file and used to train a machine learning
    classifier in `train_classifier.py`, as described in *Step 2* (given earlier).
  prefs: []
  type: TYPE_NORMAL
- en: Running the application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have seen in the previous chapters with a **wxpython GUI**, in order
    to run this app (`chapter8.py`), we need to set up a screen capture by using `cv2.VideoCapture`,
    and pass the handle to the `FaceLayout` class. We can do this by following the
    next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `run_layout` function that will work with any `BaseLayout`
    subclass, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the code is very similar to the code from previous chapters
    that used `wxpython`. We open the webcam, set the resolution, initialize the layout,
    and start the main loop of the application. This type of optimization is good
    when you have to use the same function multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we set up an argument parser that will figure out which of the two layouts
    needs to be run and run it with the appropriate arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make use of the `run_layout` function in both modes, we add a command-line
    argument to our script using the `argparse` module, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We have used the `argparse` module that comes with Python to set up an argument
    parser and add an argument with `collect` and `demo` options. We have also added
    an optional `--classifier` argument that we will use for `demo` mode only.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we use all the arguments that the user passed, to call the `run_layout` function
    with appropriate arguments, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the previous code, we have set it up to pass an extra `classifier_path`
    argument when we are in the `demo` mode. We will see how it is being used when
    we talk about `FacialExpresssionRecognizerLayout` in the later sections of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have established how to run our application, let's build the GUI
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the data collector GUI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Analogous to some of the previous chapters, the GUI of the app is a customized
    version of the generic `BaseLayout`, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We start building the GUI by calling the constructor of the parent class to
    make sure it''s correctly initialized, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we have added some extra arguments in the previous code. Those are
    for all extra attributes that our class has and that the parent class doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, before we go on to adding UI components, we also initialize a `FaceDetector`
    instance and a reference to the file to store data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we are using the hardcoded cascade XML files. Feel free to experiment
    with these as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at how we construct the UI using `wxpython`.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the basic layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The creation of the layout is again deferred to a method called `augment_layout`.
    We keep the layout as simple as possible. We create a panel for the acquired video
    frame and draw a row of buttons below it.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to then click one of the six radio buttons to indicate which facial
    expression you are trying to record, then place your head within the bounding
    box, and click the `Take Snapshot` button.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s have a look at how to build the six buttons, and correctly place
    them on a `wx.Panel` object. The code for this is shown in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You can see that even if there is a lot of code, what we wrote is mostly repetitive.
    We create a `RadioButton` for each emotion and add the button to a `pnl2` panel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Take Snapshot` button is placed below the radio buttons and will bind
    to the `_on_snapshot` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As the comment suggests, we created a new panel and added a regular button with
    the `Take Snapshot` label. The important part is that we bind the click on the
    button to the `self._on_snapshot` method, which will process each captured image
    once we click on the `Take Snapshot` button.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layout will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c0313e-4ee5-4ad5-8475-5b67e47d6df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make these changes take effect, the created panels need to be added to the
    list of existing panels, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the visualization pipeline is handled by the `BaseLayout` class.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we add boundary boxes to the faces once they appear in the
    video capture, using the `process_frame` method.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the current frame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `process_frame` method is called on all the images, and we''d like to show
    a frame around a face when it appears in the video feed. This is illustrated in
    the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We have just called the  `FaceDetector.detect_face` method of the `self.face_detector`
    object we created in the constructor of the layout class. Remember from the previous
    section that it detects faces in a downscaled grayscale version of the current
    frame using Haar cascades.
  prefs: []
  type: TYPE_NORMAL
- en: So, we are adding a frame if we recognize a face; that's it. Now, let's look
    at how we store training images inside the `_on_snapshot` method.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will store the data once the user clicks on the Take Snapshot button, and
    the `_on_snapshot` event listener method is called, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the code inside this method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we figure out the label of the image by finding out which of the radio
    buttons was selected, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it's very straightforward, once we realize that each radio button
    has a `GetValue()` method that returns `True` only if it was selected.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to look at the detected facial region of the current frame (stored
    in `self.head` by `detect_head`) and align it with all the other collected frames.
    That is, we want all the faces to be upright and the eyes to be aligned.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Otherwise, if we do not align the data samples, we run the risk of having the
    classifier compare eyes to noses. Because this computation can be costly, we do
    not apply it on every frame in the `process_frame` method, but instead only upon
    taking a snapshot in the `_on_snapshot` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Since this happened after `process_frame` was called, we already had access
    to `self.head`, which stored the image of the head present in the current frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, if we have successfully aligned the head (that is, if we have found the
    eyes), we will store the datum. Otherwise, we notify the user, using a `print`
    command to the Terminal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Actual saving is done in the  `save_datum` function, which we have abstracted
    away since it is not part of the UI. Also, this is handy in case you want to add
    a different dataset to your file, as illustrated in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the previous code, we use a `.csv` file to store the data,
    where each of the images is a `newline`. So, if you want to go back and delete
    an image (maybe you had forgotten to comb your hair), you just have to open the
    `.csv` file with a text editor and delete that line.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move to more interesting parts, and find out how we are going to
    use the data we collect to be able to train a machine learning model to detect
    emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding facial emotion recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will train an MLP to recognize facial emotions in the pictures.
  prefs: []
  type: TYPE_NORMAL
- en: We have previously made the point that finding the features that best describe
    the data is often an essential part of the entire learning task. We have also
    looked at common preprocessing methods, such as **mean subtraction** and **normalization**.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will look at an additional method that has a long tradition in face
    recognition—that is, PCA. We are hoping that, even if we don't collect thousands
    of training pictures, PCA will help us get good results.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *L**earning
    to Recognize Traffic Signs*, we write a new dataset parser in `data/emotions.py` that
    will parse our self-assembled training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a `load_data` function that will load the training data and `return`
    a tuple of collected `data` and their corresponding labels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This code, similar to all the processing codes, is self-contained and resides
    in the `data/process.py` file, similar to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*.
  prefs: []
  type: TYPE_NORMAL
- en: Our featurization function in this chapter is going to be the `pca_featurize` function
    that will perform PCA on all samples. But unlike [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, our featurization function takes into account the
    characteristics of the entire dataset, instead of operating on each of the images
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, instead of returning only the featurized data (as in [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*), it will return a tuple of training data, and all
    parameters necessary to apply the same function to the test data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's figure out what is PCA, and why we need it.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA is a dimensionality-reduction technique that is helpful whenever we are
    dealing with high-dimensional data. In a sense, you can think of an image as a
    point in a high-dimensional space. If we flatten a 2D image of height `m` and
    width `n` (by concatenating either all rows or all columns), we get a (feature)
    vector of length *m x n*. The value of the i^(th) element in this vector is the
    grayscale value of the i^(th) pixel in the image.
  prefs: []
  type: TYPE_NORMAL
- en: To describe every possible 2D grayscale image with these exact dimensions, we
    will need an *m x n*-dimensional vector space that contains *256^(m x n)* vectors.
    Wow!
  prefs: []
  type: TYPE_NORMAL
- en: An interesting question that comes to mind when considering these numbers is—*Could
    there be a smaller, more compact vector space (using less-than m x n features)
    that describes all these images equally well?* After all, we have previously realized
    that grayscale values are not the most informative measures of content.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where PCA comes into the picture. Consider a dataset from which we
    extracted exactly two features. These features could be the grayscale values of
    pixels at some *x* and *y* positions, but they could also be more complex than
    that. If we plot the dataset along these two feature axes, the data might be mapped
    within some multivariate Gaussian distribution, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97e27eaa-2ed0-4baf-b382-71d3d3faa250.png)'
  prefs: []
  type: TYPE_IMG
- en: What PCA does is *rotate* all data points until the data is mapped aligned with
    the two axes (the two inset vectors) that explain most of the *spread* of the
    data. PCA considers these two axes to be the most informative because, if you
    walk along with them, you can see most of the data points separated. In more technical
    terms, PCA aims to transform the data to a new coordinate system by means of an
    orthogonal linear transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The new coordinate system is chosen such that if you project the data onto these
    new axes, the first coordinate (called the **first principal component**) observes
    the greatest variance. In the preceding screenshot, the small vectors drawn correspond
    to the eigenvectors of the covariance matrix, shifted so that their tails come
    to lie at the mean of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had previously calculated a set of basis vectors (`top_vecs`) and mean
    (`center`), transforming the data would be straightforward, as stated in the previous
    paragraph—we subtract the center from each datum, then multiply those vectors
    by principal components, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the previous code will work for any number of `top_vecs`; thus,
    if we only supply a `num_components` number of top vectors, it will reduce the
    dimensionality of the data to `num_components`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s construct a `pca_featurize` function that takes only the data,
    and returns both the transformation and the list of arguments necessary to replicate
    the transformation—that is, `center` and `top_vecs`— so we can apply `_pcea_featurize`
    on the testing data as well, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Fortunately, someone else has already figured out how to do all this in Python.
    In OpenCV, performing PCA is as simple as calling `cv2.PCACompute`, but we have
    to pass correct arguments rather than re-format what we get from OpenCV. Here
    are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we convert `training_data` into a NumPy 2D array, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call `cv2.PCACompute`, which computes the center of the data, and
    the principal components, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can limit ourselves to the most informative components of `num_components` by
    running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The beauty of PCA is that the first principal component, by definition, explains
    most of the variance of the data. In other words, the first principal component
    is the most informative of the data. This means that we do not need to keep all
    of the components to get a good representation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also convert `mean` to create a new `center` variable that is a 1D vector
    that represents the center of the data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we return the training data processed by the `_pca_featurize` function,
    and the arguments necessary to pass to the `_pca_featurize` function, to replicate
    the same transformation so that the test data could be *featurized* in the exact
    same way as the train data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Now we know how to clean and featurize our data, it's time to look at the training
    method we use to learn to recognize facial emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MLPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLPs have been around for a while. MLPs are **artificial neural networks** (**ANNs**)
    used to convert a set of input data into a set of output data.
  prefs: []
  type: TYPE_NORMAL
- en: At the heart of an MLP is a **perceptron**, which resembles (yet overly simplifies)
    a biological neuron. By combining a large number of perceptrons in multiple layers,
    the MLP is able to make nonlinear decisions about its input data. Furthermore,
    MLPs can be trained with **backpropagation**, which makes them very interesting
    for supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: The following section explains the concept of a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A perceptron is a binary classifier that was invented in the 1950s by Frank
    Rosenblatt. A perceptron calculates a weighted sum of its inputs, and, if this
    sum exceeds a threshold, it outputs a `1`; else, it outputs a `0`.
  prefs: []
  type: TYPE_NORMAL
- en: In some sense, a perceptron is integrating evidence that its afferents signal
    the presence (or absence) of some object instance, and if this evidence is strong
    enough, the perceptron will be active (or silent). This is loosely connected to
    what researchers believe biological neurons are doing (or can be used to do) in
    the brain, hence the term *ANN*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sketch of a perceptron is depicted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68ef36cd-b7a3-4e7b-9f95-906df261fcea.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, a perceptron computes a weighted (`w[i]`) sum of all its inputs (`x[i]`),
    combined with a bias term (`b`). This input is fed into a nonlinear activation
    function (`θ`) that determines the output of the perceptron (`y`). In the original
    algorithm, the activation function was the **Heaviside** **step function**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In modern implementations of ANNs, the activation function can be anything
    ranging from sigmoid to hyperbolic tangent functions. The Heaviside step function
    and the sigmoid function are plotted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74c33d6e-33a1-406b-af14-2d6562369ffa.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the activation function, these networks might be able to perform
    either classification or regression. Traditionally, one only talks of MLPs when
    nodes use the Heaviside step function.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing about deep architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have the perceptron figured out, it would make sense to combine multiple
    perceptrons to form a larger network. MLPs usually consist of at least three layers,
    where the first layer has a node (or neuron) for every input feature of the dataset,
    and the last layer has a node for every class label.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layer in between the first and the last layer is called the hidden layer.
    An example of this feed-forward neural network is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16e356e4-ba08-47b3-ad1e-b69f20123da3.png)'
  prefs: []
  type: TYPE_IMG
- en: In a feed-forward neural network, some or all of the nodes in the input layer
    are connected to all the nodes in the hidden layer, and some or all of the nodes
    in the hidden layer are connected to some or all of the nodes in the output layer.
    You would usually choose the number of nodes in the input layer to be equal to
    the number of features in the dataset so that each node represents one feature.
  prefs: []
  type: TYPE_NORMAL
- en: Analogously, the number of nodes in the output layer is usually equal to the
    number of classes in the data, so that when an input sample of class `c` is presented,
    the *c^(th)* node in the output layer is active, and all others are silent.
  prefs: []
  type: TYPE_NORMAL
- en: It is also, of course, possible to have multiple hidden layers. Often, it is
    not clear beforehand what the optimal size of the network should be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, you will see the error rate on the training set decrease when you
    add more neurons to the network, as is depicted in the following screenshot (*thinner*,
    *red curve*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/971ee10c-412e-4c73-944e-54fdf9c61d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because the expressive power or complexity (also referred to as the
    **Vapnik-Chervonenkis** or **VC dimension**) of the model increases with the increasing
    size of the neural network. However, the same cannot be said for the error rate
    on the test set shown in the preceding screenshot (*thicker*, *blue curve*).
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you will find that, with increasing model complexity, the test error
    goes through its minimum, and adding more neurons to the network will not improve
    the performance on the test data anymore. Therefore, you would want to keep the
    size of the neural network to what is labeled the optimal range in the preceding
    screenshot, which is where the network achieves the best generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of it in this way—a model of weak complexity (on the far left
    of the plot) is probably too small to really understand the dataset that it is
    trying to learn, thus yielding large error rates on both the training and the
    test sets. This is commonly referred to as **underfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a model on the far right of the plot is probably so complex
    that it begins to memorize the specifics of each sample in the training data,
    without paying attention to the general attributes that make a sample stand apart
    from the others. Therefore, the model will fail when it has to predict data that
    it has never seen before, effectively yielding a large error rate on the test
    set. This is commonly referred to as **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, what you want is to develop a model that neither is underfitting nor
    overfitting. Often, this can only be achieved by *trial and error*; that is, by
    considering the network size as a hyperparameter that needs to be tweaked and
    tuned, depending on the exact task to be performed.
  prefs: []
  type: TYPE_NORMAL
- en: An MLP learns by adjusting its weights so that when an input sample of class
    `c` is presented, the *c^(th)* node in the output layer is active and all the
    others are silent. MLPs are trained by means of the **backpropagation** method,
    which is an algorithm to calculate the partial derivative of a **loss function**
    with respect to any synaptic weight or neuron bias in the network. These partial
    derivatives can then be used to update the weights and biases in the network in
    order to reduce the overall loss step by step.
  prefs: []
  type: TYPE_NORMAL
- en: A loss function can be obtained by presenting training samples to the network
    and by observing the network's output. By observing which output nodes are active
    and which are dormant, we can calculate the relative error between the output
    of the last layer and the true labels we provided with the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: We then make corrections to all the weights in the network so that the error
    decreases over time. It turns out that the error in the hidden layer depends on
    the output layer, and the error in the input layer depends on the error in both
    the hidden layer and the output layer. Thus, in a sense, the error backpropagates
    through the network. In OpenCV, backpropagation is used by specifying `cv2.ANN_MLP_TRAIN_PARAMS_BACKPROP`
    in the training parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent comes in two common flavors—that is, in **stochastic gradient
    descent** and **batch learning**.
  prefs: []
  type: TYPE_NORMAL
- en: In stochastic gradient descent, we update the weights after each presentation
    of a training example, whereas, in batch learning, we present training examples
    in batches and update the weights only after each batch is presented. In both
    scenarios, we have to make sure that we adjust the weights only slightly per sample
    (by adjusting the **learning rate**) so that the network slowly converges to a
    stable solution over time.
  prefs: []
  type: TYPE_NORMAL
- en: Now, after learning the theory behind MLPs, let's get our hands dirty and code
    this up using OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Crafting an MLP for facial expression recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, we will use the machine learning class that OpenCV
    provides, which is `ml.ANN_MLP`. Here are the steps to create and configure an
    MLP in OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate an empty `ANN_MLP` object, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the network architecture—the first layer equal to the dimensionality of
    the data, and the last layer equal to the output size of `6` required for the
    number of possible emotions, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the training algorithm to backpropagation, and use the symmetric sigmoid
    function for activation, as we discussed in the previous sections, by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set the termination criteria to either after `30` iterations or
    when the loss reaches values smaller than `0.000001`, as follows, and we are ready
    to train the MLP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In order to train the MLP, we need training data. We would also like to have
    an idea of how well our classifier is doing, so we need to split the collected
    data into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to split the data would be to make sure that we don't have near-identical
    images in the training and testing sets—for example, the user double-clicked on
    the Take Snapshot button, and we have two images that were taken milliseconds
    apart, thus are almost identical. Unfortunately, that is a tedious and manual
    process and out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the signature of the function, as follows. We want to get indices
    of an array of size `n` and we want to specify a ratio of the train data to all
    the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Geared with the signature, let''s go over the `train_test_split` function step
    by step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a list of `indices` and `shuffle` them, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we calculate the number of training points that need to be in the `N` training
    dataset, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we create a selector for the first `N` indices for the training
    data, and create a selector for the rest of `indices` to be used for the test
    data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now we have a model class and a training data generator, let's take a look at
    how to train the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Training the MLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV provides all the training and predicting methods, so we have to figure
    out how to format our data to fit the OpenCV requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we split the data into train/test, and featurize the training data,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Here, `pca_args` are the arguments that we will need to store if we wanted to
    featurize any future data (for example, live frames during the demo).
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the `train` method of the `cv2.ANN_MLP` module does not allow integer-valued
    class labels, we need to first convert `y_train` into one-hot encoding, consisting
    only of 0s and 1s, which can then be fed to the `train` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The one-hot encoding is taken care of in the `one_hot_encode` function in `train_classifiers.py`,
    in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we determine how many points there are in the data, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Each `c` class label in `all_labels` needs to be converted into a (`len(unique_labels)`)
    long vector of 0s and 1s, where all entries are zeros except the *c^(th)*, which
    is a 1\. We prepare this operation by allocating a vector of zeros, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create dictionaries mapping indices of columns to labels, and vice
    versa, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The vector elements at these indices then need to be set to `1`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We also return `index_to_label` so we are able to recover the label from the
    prediction vector, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We now move on to the testing of the MLP that we just trained.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the MLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, we will evaluate the performance of our classifier
    in terms of accuracy, precision, and recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reuse our previous code, we just need to calculate `y_hat` and pass `y_true`
    alongside it to the metric functions by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we featurize our test data using the `pca_args` we stored when we featurized
    the training data, and the `_pca_featurize` function, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we predict the new labels, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we extract the true test labels using indices we stored for testing,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The only things that are left to pass to a function are both `y_hat` and `y_true`,
    to calculate the accuracy of our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: It took me 84 pictures (10-15 of each emotion) to get to a training accuracy
    of `0.92` and have a good enough classifier to be able to show off my software
    to my friends. *Can you beat it?*
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how we run the training script, and save the output in a manner
    that the demo application will be able to use.
  prefs: []
  type: TYPE_NORMAL
- en: Running the script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MLP classifier can be trained and tested by using the `train_classifier.py`
    script, which does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script first sets up the command-line options of `--data` to the location
    of the saved data, and `--save` to the location of a directory where we want to
    save the trained model (this argument is optional), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load the saved data, and follow the training procedure described in
    the previous section, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we check if the user wants us to save the trained model, by running
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The previous code saves the trained model, the `index_to_label` dictionary so
    that we can display human-readable labels in the demo, and `pca_args` so that
    we can featurize live camera feed frames in the demo.
  prefs: []
  type: TYPE_NORMAL
- en: The saved `mlp.xml` file contains the network configuration and learned weights.
    OpenCV knows how to load it. So, let's see what the demo application looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to run our app, we will need to execute the main function routine (`chapter8.py`)
    that loads the pre-trained cascade classifier and the pre-trained MLP and applies
    them to each frame of the webcam live stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this time, instead of collecting more training samples, we will start
    the program with a different option, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start the application with a new `FacialExpressionRecognizerLayout`
    layout, which is a subclass of `BasicLayout` without any extra UI elements. Let''s
    go over the constructor first, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'It reads and initializes all the data that was stored by the training script,
    like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'It loads the pre-trained classifier using `ANN_MLP_load`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'It loads the Python variables that we want to pass from training, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'It initializes a `FaceDetector` class to be able to do face recognition, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have all the pieces from training in place, we can go ahead and put
    some code in place to add labels to faces. In this demo, we don''t have any use
    of extra buttons; so, the only method we have to implement is `process_frame`,
    which first tries to detect a face in the live feed and place a label on top of
    it, We will proceed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we try to detect if there is a face present in the video stream or not,
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'If there is no face, we do nothing and `return` an unprocessed `frame`, as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Once there is a face, we try to align the face (the same as when collecting
    the training data), like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are successful, we featurize the head and predict the label using the
    MLP, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we put the text with the label on top of the face''s bounding box
    and show that to the user, by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous method, we made use of `featurize_head`, which is a convenient
    function to call `_pca_featurize`, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The end result looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ded5d77-8f8a-4c9b-a8b6-9c27996f24e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Although the classifier has only been trained on (roughly) 100 training samples,
    it reliably detects my various facial expressions in every frame of the live stream,
    no matter how distorted my face seemed to be at the given moment.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good indication that the neural network that was trained previously
    is neither underfitting nor overfitting the data since it is capable of predicting
    the correct class labels, even for new data samples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter of the book has really rounded up our experience and made us combine
    a variety of our skills to arrive at an end-to-end app that consists of both object
    detection and object recognition. We became familiar with a range of pre-trained
    cascade classifiers that OpenCV has to offer, and we collected and created our
    very own training dataset, learned about MLPs, and trained them to recognize emotional
    expressions in faces (well, at least my face).
  prefs: []
  type: TYPE_NORMAL
- en: The classifier undoubtedly benefited from the fact that I was the only subject
    in the dataset, but, with all the knowledge and experience that you have gathered
    throughout this book, it is now time to overcome these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: After learning the techniques in this chapter, you can start with something
    smaller, and train the classifier on images of you (indoors and outdoors, at night
    and day, during summer and winter). Or, you can take a look at **Kaggle's Facial
    Expression Recognition Challenge**, which has a lot of nice data you could play
    with.
  prefs: []
  type: TYPE_NORMAL
- en: If you are into machine learning, you might already know that there is a variety
    of accessible libraries out there, such as **Pylearn**, **scikit-learn**, and
    **PyTorch**.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will start your deep learning journey and will put
    your hands on deep CNNs. You will get acquainted with multiple deep learning concepts,
    and you will create and train your own classification and localization networks
    using transfer learning. To accomplish this, you will use one of the pre-trained
    classification CNNs available in **Keras.** Throughout the chapter, you will extensively
    use **Keras** and **TensorFlow, **which are a couple of the most popular deep
    learning frameworks at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Kaggle''s Facial Expression Recognition Challenge**: [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pylearn**: [https://github.com/lisa-lab/pylearn2](https://github.com/lisa-lab/pylearn2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**scikit-learn**: [http://scikit-learn.org](http://scikit-learn.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pycaffe**: [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Theano**: [http://deeplearning.net/software/theano](http://deeplearning.net/software/theano).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Torch**: [http://torch.ch](http://torch.ch).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UC Irvine Machine Learning Repository**: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Lenna.png`—Image Lenna is available at [http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896) by
    Conor Lawless under attribution CC 2.0 Generic.'
  prefs: []
  type: TYPE_NORMAL
