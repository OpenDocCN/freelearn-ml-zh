- en: Learning to Recognize Facial Emotions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习识别面部表情
- en: We previously familiarized ourselves with the concepts of object detection and
    object recognition. In this chapter, we will develop an app that does both together.
    The app will be able to detect your own face in each captured frame of a webcam
    live stream, recognize your facial emotion, and label it on the **Graphical User
    Interface** (**GUI**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经熟悉了对象检测和对象识别的概念。在本章中，我们将开发一个能够同时执行这两项任务的应用程序。该应用程序将能够检测网络摄像头实时流中的每一帧捕获到的您的面部，识别您的面部表情，并在**图形用户界面**（**GUI**）上对其进行标记。
- en: The goal of this chapter is to develop an app that combines both **face detection**
    and **face recognition**, with a focus on recognizing emotional expressions for
    the detected face. After reading the chapter, you will be able to use both face
    detection and recognition in different applications of your own.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是开发一个结合**面部检测**和**面部识别**的应用程序，重点关注识别检测到的面部表情。阅读本章后，您将能够在自己的不同应用程序中使用面部检测和识别。
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Planning the app
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计划应用程序
- en: Learning about face detection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习面部检测
- en: Collecting data for machine learning tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集机器学习任务的数据
- en: Understanding facial emotion recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解面部表情识别
- en: Putting it all together
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整合所有内容
- en: We will touch upon two classic algorithms that come bundled with OpenCV—**Haar
    cascade classifiers** and **MLPs**. While the former can be used to rapidly detect
    (or to locate, and to answer the question *Where*?) objects of various sizes and
    orientations in an image, the latter can be used to recognize them (or to identify,
    and answer the question *What*?).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍OpenCV附带的两项经典算法——**Haar cascade分类器**和**MLPs**。前者可以快速检测（或定位，并回答问题“在哪里？”）图像中各种大小和方向的物体，而后者可以用来识别它们（或识别，并回答问题“是什么？”）。
- en: Learning MLPs is also the first step toward learning one of the most trendy
    algorithms these days—**deep neural networks** (**DNNs**). We will use PCA to
    speed up and improve the accuracy of the algorithm when we have not got a huge
    amount of data, to improve the accuracy of our model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习MLPs也是学习当今最流行算法之一——**深度神经网络**（**DNNs**）的第一步。当我们没有大量数据时，我们将使用PCA来加速并提高算法的准确性，以改善我们模型的准确性。
- en: We will collect our training data ourselves to show you how that process is
    done, in order for you to be able to train machine learning models for tasks that
    don't have data readily available. Unfortunately, not having the right data is
    still one of the biggest obstacles to the widespread adoption of machine learning
    these days.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自行收集训练数据，以向您展示这个过程是如何进行的，以便您能够为没有现成数据的学习任务训练机器学习模型。不幸的是，没有合适的数据仍然是当今机器学习广泛采用的最大障碍之一。
- en: Now, let's take a look at how to get started before we get our hands dirty.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们动手之前，让我们先看看如何开始。
- en: Getting started
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: You can find the code that we present in this chapter at our GitHub repository,
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在我们的GitHub仓库中找到本章中展示的代码，网址为[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter8)。
- en: Other than that, you should download the **Haar cascade** files from the official
    OpenCV repository at [https://github.com/opencv/opencv/blob/master/data/haarcascades/](https://github.com/opencv/opencv/blob/master/data/haarcascades/),
    or copy them from the installation directory of your machine to the project repository.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，您应该从官方OpenCV仓库下载**Haar cascade**文件，网址为[https://github.com/opencv/opencv/blob/master/data/haarcascades/](https://github.com/opencv/opencv/blob/master/data/haarcascades/)，或者从您的机器安装目录复制它们到项目仓库中。
- en: Planning the app
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计划应用程序
- en: 'The reliable recognition of faces and facial expressions is a challenging task
    for **artificial intelligence** (**AI**), yet humans are able to perform these
    kinds of tasks with ease. To make our task feasible, we will limit ourselves to
    the following limited emotional expressions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠地识别面部和面部表情是**人工智能**（**AI**）的一个具有挑战性的任务，然而人类能够轻松地完成这些任务。为了使我们的任务可行，我们将限制自己只关注以下有限的情绪表达：
- en: Neutral
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neutral
- en: Happy
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Happy
- en: Sad
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sad
- en: Surprised
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Surprised
- en: Angry
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Angry
- en: Disgusted
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Disgusted
- en: Today's state-of-the-art models range all the way from 3D deformable face models
    fitting over **convolutional neural networks** (**CNNs**), to deep learning algorithms.
    Granted, these approaches are significantly more sophisticated than our approach.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的最先进模型范围从适合**卷积神经网络（CNNs**）的3D可变形人脸模型，到深度学习算法。当然，这些方法比我们的方法要复杂得多。
- en: Yet, an **MLP** is a classic algorithm that has helped transform the field of
    machine learning, so for educational purposes, we will stick to a set of algorithms
    that come bundled with OpenCV.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**多层感知器（MLP**）是一种经典的算法，它帮助改变了机器学习的领域，因此出于教育目的，我们将坚持使用OpenCV附带的一系列算法。
- en: 'To arrive at such an app, we need to solve the following two challenges:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到这样的应用，我们需要解决以下两个挑战：
- en: '**Face detection**: We will use the popular Haar cascade classifier by Viola
    and Jones, for which OpenCV provides a whole range of pre-trained exemplars. We
    will make use of face cascades and eye cascades to reliably detect and align facial
    regions from frame to frame.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人脸检测**：我们将使用Viola和Jones的流行Haar级联分类器，OpenCV为此提供了一系列预训练的示例。我们将利用人脸级联和眼级联从帧到帧可靠地检测和校准面部区域。'
- en: '**Facial expression recognition**: We will train an MLP to recognize the six
    different emotional expressions listed earlier, in every detected face. The success
    of this approach will crucially depend on the training set that we assemble, and
    the preprocessing that we choose to apply to each sample in the set.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部表情识别**：我们将训练一个MLP来识别之前列出的六种不同的情感表达，在每一个检测到的人脸中。这种方法的成功将关键取决于我们组装的训练集，以及我们对每个样本选择的预处理。'
- en: In order to improve the quality of our self-recorded training set, we will make
    sure that all data samples are aligned using **affine transformations**, and will
    reduce the dimensionality of the feature space by applying **PCA**. The resulting
    representation is sometimes also referred to as **Eigenfaces**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们自录制的训练集质量，我们将确保所有数据样本都通过**仿射变换**进行对齐，并通过应用**主成分分析（PCA**）来降低特征空间的维度。这种表示有时也被称为**特征脸**。
- en: 'We will combine the algorithms mentioned earlier in a single end-to-end app
    that annotates a detected face with the corresponding facial expression label
    in each captured frame of a video live stream. The end result might look something
    like the following screenshot, capturing my sample reaction when the code first
    ran:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个端到端的单一应用中结合前面提到的算法，该应用将使用预训练模型在每个视频直播捕获的每一帧中为检测到的人脸标注相应的面部表情标签。最终结果可能看起来像以下截图，捕捉了我第一次运行代码时的样本反应：
- en: '![](img/b4d5bb8e-eb59-466a-b7d4-50e341e42db1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b4d5bb8e-eb59-466a-b7d4-50e341e42db1.png)'
- en: The final app will consist of the main script that integrates the process flow
    end to end—that is, from face detection to facial expression recognition, as well
    as some utility functions to help along the way.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用将包括一个集成的端到端流程的主脚本——即从人脸检测到面部表情识别，以及一些辅助函数来帮助实现这一过程。
- en: 'Thus, the end product will require several components that are located in the
    `chapter8/` directory of the book''s GitHub repository, listed as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终产品将需要位于书籍GitHub仓库的`chapter8/`目录中的几个组件，如下所示：
- en: '`chapter8.py`: This is the main script and entry point for the chapter, and
    we will use this for both data collection and demo. It will have the following
    layouts:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter8.py`: 这是本章的主要脚本和入口点，我们将用它来进行数据收集和演示。它将具有以下布局：'
- en: '`chapter8.FacialExpressionRecognizerLayout`: This is a custom layout based
    on `wx_gui.BaseLayout` that will detect a face in each video frame and predict
    the corresponding class label by using a pre-trained model.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter8.FacialExpressionRecognizerLayout`: 这是一个基于`wx_gui.BaseLayout`的自定义布局，它将在每个视频帧中检测面部，并使用预训练模型预测相应的类别标签。'
- en: '`chapter8.DataCollectorLayout`: This is a custom layout based on `wx_gui.BaseLayout`
    that will collect image frames, detect a face therein, assign a label using a
    user-selected facial expression label, and will save the frames into the `data/`
    directory.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter8.DataCollectorLayout`: 这是一个基于`wx_gui.BaseLayout`的自定义布局，它将收集图像帧，检测其中的面部，使用用户选择的表情标签进行标记，并将帧保存到`data/`目录中。'
- en: '`wx_gui.py`: This is a link to our `wxpython` GUI file that we developed in
    [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml), *Fun with Filters*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wx_gui.py`: 这是链接到我们在[第1章](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml)，“滤镜乐趣”中开发的`wxpython`
    GUI文件。'
- en: '`detectors.FaceDetector`: This is a class that will encompass all the code
    for face detection based on Haar cascades. It will have the following two methods:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detectors.FaceDetector`：这是一个将包含基于Haar级联的面部检测所有代码的类。它将有两个以下方法：'
- en: '`detect_face`: This method detects faces in a grayscale image. Optionally,
    the image is downscaled for better reliability. Upon successful detection, the
    method returns the extracted head region.'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detect_face`：这个方法用于检测灰度图像中的面部。可选地，图像会被缩小以提高可靠性。检测成功后，该方法返回提取的头区域。'
- en: '`align_head`: This method preprocesses an extracted head region with affine
    transformations, such that the resulting face appears centered and upright.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`align_head`：这个方法通过仿射变换预处理提取的头区域，使得最终的面部看起来居中且垂直。'
- en: '`params/`: This is a directory that contains the default Haar cascades that
    we use for the book.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params/`：这是一个包含我们用于本书的默认Haar级联的目录。'
- en: '`data/`: We will write all the code to store and process our custom data here.
    The code is split into the following files:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data/`：我们将在这里编写所有存储和处理自定义数据的代码。代码被分为以下文件：'
- en: '`store.py`: This is a file where we put all the helper functions to write the
    data to disk and to load the data from the disk into computer memory.'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`store.py`：这是一个包含所有将数据写入磁盘和从磁盘将数据加载到计算机内存中的辅助函数的文件。'
- en: '`process.py`: This is a file that will contain all the code to preprocess the
    data before saving. It will also contain the code to construct features from the
    raw data.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process.py`：这是一个包含所有预处理数据的代码的文件，以便在保存之前。它还将包含从原始数据构建特征的代码。'
- en: In the following sections, we will discuss these components in detail. First,
    let's look at the face detection algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论这些组件。首先，让我们看看面部检测算法。
- en: Learning about face detection
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习关于面部检测
- en: OpenCV comes preinstalled with a range of sophisticated classifiers for general-purpose
    object detection. These all have very similar APIs and are easy to use, once you
    know what you are looking for. Perhaps the most commonly known detector is the **cascade
    of Haar-based feature detectors** for face detection, which was first introduced
    by Paul Viola and Michael Jones in their paper *Rapid Object Detection using a
    Boosted Cascade of Simple Features* in 2001.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV预装了一系列复杂的通用对象检测分类器。这些分类器都具有非常相似的API，一旦你知道你在寻找什么，它们就很容易使用。可能最广为人知的检测器是用于面部检测的基于Haar特征的**级联检测器**，它最初由Paul
    Viola和Michael Jones在2001年发表的论文《使用简单特征增强级联的快速对象检测》中提出。
- en: A Haar-based feature detector is a machine learning algorithm that is trained
    on a lot of positive and negative labeled samples. What will we do in our application
    is take a pre-trained classifier that comes with OpenCV (you can find the link
    in the *Getting started *section). But first, let's take a closer look at how
    the classifier works.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Haar特征检测器是一种在大量正负标签样本上训练的机器学习算法。在我们的应用中，我们将使用OpenCV附带的一个预训练分类器（你可以在“入门”部分找到链接）。但首先，让我们更详细地看看这个分类器是如何工作的。
- en: Learning about Haar-based cascade classifiers
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习关于基于Haar级联分类器
- en: Every book on OpenCV should at least mention the Viola-Jones face detector.
    Invented in 2001, this cascade classifier disrupted the field of computer vision,
    as it finally allowed real-time face detection and face recognition.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每本关于OpenCV的书至少应该提到Viola-Jones面部检测器。这个级联分类器在2001年发明，它颠覆了计算机视觉领域，因为它最终允许实时面部检测和面部识别。
- en: The classifier is based on **Haar-like features** (similar to **Haar basis functions**) that sum
    up the pixel intensities in small regions of an image, as well as capture the
    difference between adjacent image regions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器基于**Haar-like特征**（类似于**Haar基函数**），它们在图像的小区域内求和像素强度，同时捕捉相邻图像区域之间的差异。
- en: 'The following screenshot visualizes four rectangle features. The visualization
    works to calculate the value of the feature applied at a location. You should
    sum up all pixel values in the dark gray rectangle and subtract this value from
    the sum of all pixel values in the white rectangle:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图可视化了四个矩形特征。可视化旨在计算在某个位置应用的特征的值。你应该将暗灰色矩形中的所有像素值加起来，然后从这个值中减去白色矩形中所有像素值的总和：
- en: '![](img/83c2064f-8325-4c2e-af90-ab2fdd8ded16.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83c2064f-8325-4c2e-af90-ab2fdd8ded16.png)'
- en: In the previous screenshot, the top row shows two examples of an edge feature
    (that is, you can detect edges with them), either vertically oriented (top left)
    or oriented at a 45º angle (top right). The bottom row shows a line feature (bottom
    left) and a center-surround feature (bottom right).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图上，第一行显示了两个边缘特征（即你可以用它们检测边缘）的示例，要么是垂直方向的（左上角）要么是45度角方向的（右上角）。第二行显示了线特征（左下角）和中心环绕特征（右下角）。
- en: Applying these filters at all possible locations allows the algorithm to capture
    certain details of human faces, such as the fact that eye regions are usually
    darker than the region surrounding the cheeks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些过滤器在所有可能的位置，允许算法捕捉到人类面部的一些细节，例如，眼睛区域通常比脸颊周围的区域要暗。
- en: Thus, a common Haar feature would have a dark rectangle (representing the eye
    region) atop a bright rectangle (representing the cheek region). Combining this
    feature with a bank of rotated and slightly more complicated **wavelets**, Viola
    and Jones arrived at a powerful feature descriptor for human faces. In an additional
    act of brilliance, these guys came up with an efficient way to calculate these
    features, making it possible for the first time to detect faces in real time.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个常见的Haar特征将是一个暗色的矩形（代表眼睛区域）在亮色的矩形（代表脸颊区域）之上。将这个特征与一组旋转和略微复杂的**小波**结合，Viola和Jones得到了一个强大的人脸特征描述符。在额外的智慧行为中，这些人想出了一个有效的方法来计算这些特征，这使得第一次能够实时检测到人脸。
- en: The final classifier is a weighted sum of small weaker classifiers, each of
    whose binary classifiers are based on a single feature described previously. The
    hardest part is to figure out which combinations of features are helpful for detecting
    different types of objects. Luckily, OpenCV contains a collection of such classifiers.
    Let's take a look at some of these in the following section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最终分类器是多个小型弱分类器的加权总和，每个分类器的二进制分类器都基于之前描述的单一特征。最难的部分是确定哪些特征组合有助于检测不同类型的对象。幸运的是，OpenCV包含了一系列这样的分类器。让我们在下一节中看看其中的一些。
- en: Understanding pre-trained cascade classifiers
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解预训练的级联分类器
- en: 'Even better, this approach not only works for faces but also for eyes, mouths,
    full bodies, company logos; you name it. In the following table, a number of pre-trained
    classifiers are shown that can be found under the OpenCV install path in the `data`
    folder:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，这种方法不仅适用于人脸，还适用于眼睛、嘴巴、全身、公司标志；你说的都对。在下表中，展示了一些可以在OpenCV安装路径下的`data`文件夹中找到的预训练分类器：
- en: '| **Cascade classifier types** | **XML filenames** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **级联分类器类型** | **XML文件名** |'
- en: '| Face detector (default) | **`haarcascade_frontalface_default.xml`** |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 面部检测器（默认） | **`haarcascade_frontalface_default.xml`** |'
- en: '| Face detector (fast Haar) | `haarcascade_frontalface_alt2.xml` |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 面部检测器（快速Haar） | `haarcascade_frontalface_alt2.xml` |'
- en: '| Eye detector | `haarcascade_eye.xml` |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 眼睛检测器 | `haarcascade_eye.xml` |'
- en: '| Mouth detector | `haarcascade_mcs_mouth.xml` |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 嘴巴检测器 | `haarcascade_mcs_mouth.xml` |'
- en: '| Nose detector | `haarcascade_mcs_nose.xml` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 鼻子检测器 | `haarcascade_mcs_nose.xml` |'
- en: '| Full body detector | `haarcascade_fullbody.xml` |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 全身检测器 | `haarcascade_fullbody.xml` |'
- en: In this chapter, we will only use `haarcascade_frontalface_default.xml` and  `haarcascade_eye.xml`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只使用`haarcascade_frontalface_default.xml`和`haarcascade_eye.xml`。
- en: If you are wearing glasses, make sure to use `haarcascade_eye_tree_eyeglasses.xml` for
    eye detection instead.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你戴着眼镜，请确保使用`haarcascade_eye_tree_eyeglasses.xml`进行眼睛检测。
- en: We will first look at how to use a cascade classifier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将探讨如何使用级联分类器。
- en: Using a pre-trained cascade classifier
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的级联分类器
- en: 'A cascade classifier can be loaded and applied to an image (grayscale) using
    the following code, where we first read the image, then convert it to grayscale,
    and finally detect all the faces using a cascade classifier:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码加载并应用于图像（灰度）的级联分类器：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'From the previous code, the `detectMultiScale` function comes with a number
    of options:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，`detectMultiScale`函数附带了一些选项：
- en: '`minFeatureSize` is the minimum face size to consider—for example, 20 x 20 pixels.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minFeatureSize`是考虑的最小人脸大小——例如，20 x 20 像素。'
- en: '`searchScaleFactor` is the amount by which we rescale the image (scale pyramid).
    For example, a value of `1.1` will gradually reduce the size of the input image
    by 10 %, making it more likely for a face (image) with a larger value to be found.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`searchScaleFactor`是我们重置图像（尺度金字塔）的量。例如，`1.1`的值将逐渐减小输入图像的大小10%，使得具有更大值的脸（图像）更容易被找到。'
- en: '`minNeighbors` is the number of neighbors that each candidate rectangle will
    have to retain. Typically, we choose `3` or `5`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minNeighbors`是每个候选矩形必须保留的邻居数量。通常，我们选择`3`或`5`。'
- en: '`flags` is an options object used to tweak the algorithm—for example, whether
    to look for all faces or just the largest face (`cv2.cv.CASCADE_FIND_BIGGEST_OBJECT`).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flags`是一个选项对象，用于调整算法——例如，是否寻找所有面部或仅寻找最大的面部(`cv2.cv.CASCADE_FIND_BIGGEST_OBJECT`)。'
- en: 'If detection is successful, the function will return a list of bounding boxes
    (`faces`) that contain the coordinates of the detected face regions, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测成功，函数将返回一个包含检测到的面部区域坐标的边界框列表(`faces`)，如下所示：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the previous code, we iterate through the returned faces and add a rectangle
    outline with a thickness of `2` pixels to each of the faces.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们遍历返回的面部，并为每个面部添加一个厚度为`2`像素的矩形轮廓。
- en: If your pre-trained face cascade does not detect anything, a common reason is
    usually that the path to the pre-trained cascade file could not be found. In this
    case, `CascadeClassifier` will fail silently. Thus, it is always a good idea to
    check whether the returned classifier `casc = cv2.CascadeClassifier(filename)` is
    empty, by checking `casc.empty()`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你预训练的面部级联没有检测到任何东西，一个常见的原因通常是预训练级联文件的路径找不到。在这种情况下，`CascadeClassifier`将静默失败。因此，始终检查返回的分类器`casc
    = cv2.CascadeClassifier(filename)`是否为空，通过检查`casc.empty()`。
- en: 'This is what you should get if you run the code on the `Lenna.png` picture:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这张`Lenna.png`图片上运行代码，你应该得到以下结果：
- en: '![](img/e215013d-803e-4bd0-a29f-344637650c9e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e215013d-803e-4bd0-a29f-344637650c9e.png)'
- en: Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源——Conor Lawless的Lenna.png，许可协议为CC BY 2.0
- en: From the previous screenshot, on the left, you see the original image, and on
    the right is the image that was passed to OpenCV, and the rectangle outline of
    the detected face.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图来看，在左侧，你可以看到原始图像，在右侧是传递给OpenCV的图像以及检测到的面部的矩形轮廓。
- en: Now, let's try to wrap this detector into a class to make it usable for our
    application.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将这个检测器封装成一个类，使其适用于我们的应用。
- en: Understanding the FaceDetector class
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解`FaceDetector`类
- en: 'All relevant face detection code for this chapter can be found as part of the
    `FaceDetector` class in the `detectors` module. Upon instantiation, this class
    loads two different cascade classifiers that are needed for preprocessing—namely,
    a `face_cascade` classifier and an `eye_cascade` classifier, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所有相关的面部检测代码都可以在`detectors`模块中的`FaceDetector`类中找到。在实例化时，这个类加载了两个不同的级联分类器，这些分类器用于预处理——即`face_cascade`分类器和`eye_cascade`分类器，如下所示：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Because our preprocessing requires a valid face cascade, we make sure that
    the file can be loaded. If not, we throw a `ValueError` exception, so the program
    will terminate and notify the user what went wrong, as shown in the following
    code block:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的预处理需要一个有效的面部级联，我们确保文件可以被加载。如果不能，我们将抛出一个`ValueError`异常，这样程序将终止并通知用户出了什么问题，如下面的代码块所示：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We do the same thing for the eye classifier as well, like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对眼睛分类器也做同样的事情，如下所示：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Face detection works best on low-resolution grayscale images. This is why we
    also store a scaling factor (`scale_factor`) so that we can operate on downscaled
    versions of the input image if necessary, like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 面部检测在低分辨率灰度图像上效果最佳。这就是为什么我们也存储一个缩放因子(`scale_factor`)，以便在必要时操作输入图像的缩放版本，如下所示：
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have set up the class initialization, let's take a look at the algorithm
    that detects the faces.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了类的初始化，让我们看看检测面部的算法。
- en: Detecting faces in grayscale images
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在灰度图像中检测面部
- en: Now, we will put what we learned in the previous section into a method that
    will take an image and return the biggest face in the image. We are returning
    the biggest face to simplify things since we know that, in our application, there
    is going to be a single user sitting in front of the webcam. As a challenge, you
    could try to expand this to work with more than one face!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将之前章节中学到的内容放入一个方法中，该方法将接受一个图像并返回图像中的最大面部。我们返回最大面部是为了简化事情，因为我们知道在我们的应用中，将只有一个用户坐在摄像头前。作为一个挑战，你可以尝试将其扩展以处理多个面部！
- en: 'We call the method to detect the biggest face (`detect_face`). Let''s go through
    it step by step:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检测最大面部的方法称为`detect_face`。让我们一步一步地来看：
- en: 'As in the last section, first, we convert the argument RGB image to grayscale
    and scale it by `scale_factor` by running the following code:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同上一节所述，首先，我们将输入的RGB图像转换为灰度图，并通过运行以下代码按`scale_factor`进行缩放：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we detect the faces in the grayscale image, like this:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在灰度图像中检测人脸，如下所示：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We iterate over the detected faces and outline if the `outline=True` keyword
    argument was passed to `detect_face`. OpenCV returns us `x, y` coordinates of
    the top-left location and `w, h` width and height of the head. So, in order to
    construct the outline, we just calculate the bottom and right coordinates of the
    outline, and call the `cv2.rectangle` function, as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历检测到的人脸，如果将`outline=True`关键字参数传递给`detect_face`，则会绘制轮廓。OpenCV返回给我们头部左上角的`x,
    y`坐标以及头部宽度和高度`w, h`。因此，为了构建轮廓，我们只需计算轮廓的底部和右侧坐标，然后调用`cv2.rectangle`函数，如下所示：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We crop the head out of the original RGB image. This will be handy if we want
    to do more processing on the head (for example, recognize the facial expression).
    Run the following code:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从原始RGB图像中裁剪出头部。如果我们想要对头部进行更多处理（例如，识别面部表情），可以运行以下代码：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We return the following 4-tuple:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们返回以下4元组：
- en: A Boolean value to check whether the detection was successful or not
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个布尔值，用于检查检测是否成功
- en: The original image with the outline of the faces added (if requested)
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加了人脸轮廓的原图像（如果需要）
- en: A cropped image of the head to use as needed
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要裁剪的头像
- en: Coordinates of the location of the head in the original image
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原图像中头部位置的坐标
- en: 'In the case of success, we return the following:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在成功的情况下，我们返回以下内容：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the case of failure, we return that no head was found, and return `None` for
    anything that is undetermined, like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在失败的情况下，我们返回没有找到头部的信息，并且对于任何不确定的事项，如这样返回`None`：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, let's look at what happens after we detect the faces, to get them ready
    for machine learning algorithms.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在检测到人脸之后会发生什么，以便为机器学习算法做好准备。
- en: Preprocessing detected faces
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理检测到的人脸
- en: After a face has been detected, we might want to preprocess the extracted head
    region before applying classification to it. Although the face cascade is fairly
    accurate, for the recognition, it is important that all the faces are upright
    and centered on the image.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测到人脸之后，我们可能想要在对其进行分类之前先预处理提取的头像区域。尽管人脸级联检测相当准确，但对于识别来说，所有的人脸都必须是竖直且居中于图像中。
- en: 'Here is what we want to accomplish:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们要达成的目标：
- en: '![](img/d57ee527-7d22-4fef-bd90-e7df1f65b0a1.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d57ee527-7d22-4fef-bd90-e7df1f65b0a1.png)'
- en: Image credit—Lenna.png by Conor Lawless is licensed under CC BY 2.0
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源——由Conor Lawless提供的Lenna.png，许可协议为CC BY 2.0
- en: As you can see from the preceding screenshot, as this is not a passport photo,
    the model has her head slightly tilted to the side while looking over her shoulder.
    The facial region, as extracted by the face cascade, is shown in the middle thumbnail
    in the preceding screenshot.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个截图所示，由于这不是护照照片，模型中的头部略微向一侧倾斜，同时看向肩膀。人脸区域，如图像级联提取的，显示在前一个截图的中间缩略图中。
- en: 'In order to compensate for the head orientation and position in the detected
    box, we aim to rotate, move, and scale the face so that all data samples will
    be perfectly aligned. This is the job of the `align_head` method in the `FaceDetector`
    class, shown in the following code block:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补偿检测到的盒子中头部朝向和位置，我们旨在旋转、移动和缩放面部，以便所有数据样本都能完美对齐。这是`FaceDetector`类中的`align_head`方法的工作，如下面的代码块所示：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the previous code, we have hardcoded some parameters that are used to align
    the heads. We want all eyes to be 25 % below the top of the final image and 20
    % from the left and right edges, and this function is going to return a processed
    image of the head that has a fixed size of 200 x 200 pixels.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们硬编码了一些用于对齐头部的参数。我们希望所有眼睛都在最终图像顶部下方25%，并且距离左右边缘各20%，此函数将返回一个头部处理后的图像，其固定大小为200
    x 200像素。
- en: The first step of the process is to detect where the eyes are in the image,
    after which we will use their location to construct the necessary transformation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 处理流程的第一步是检测图像中眼睛的位置，之后我们将使用这些位置来构建必要的转换。
- en: Detecting the eyes
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测眼睛
- en: Fortunately, OpenCV comes with a few eye cascades that can detect both open
    and closed eyes, such as `haarcascade_eye.xml`. This allows us to calculate the
    angle between the line that connects the center of the two eyes and the horizon
    so that we can rotate the face accordingly.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenCV自带了一些可以检测睁眼和闭眼的眼睛级联，例如`haarcascade_eye.xml`。这允许我们计算连接两个眼睛中心的线与地平线之间的角度，以便我们可以相应地旋转面部。
- en: In addition, adding eye detectors will reduce the risk of having false positives
    in our dataset, allowing us to add a data sample only if both the head and the
    eyes have been successfully detected.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，添加眼睛检测器将降低我们数据集中出现假阳性的风险，只有当头部和眼睛都成功检测到时，我们才能添加数据样本。
- en: 'After loading the eye cascade from the file in the `FaceDetector` constructor,
    it is applied to the input image (`head`), as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在`FaceDetector`构造函数中从文件加载眼睛级联后，它被应用于输入图像（`head`），如下所示：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we are unsuccessful and the cascade classifier couldn't find an eye, OpenCV
    will throw a `RuntimeError`. Here, we are catching it and returning a `(False,
    head)` tuple, indicating that we failed to align the head.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们失败，级联分类器找不到眼睛，OpenCV将抛出一个`RuntimeError`。在这里，我们正在捕获它并返回一个`(False, head)`元组，表示我们未能对齐头部。
- en: 'Next, we try to order the references to the eyes that the classifier has found.
    We set `left_eye` to be the eye with the lower first coordinate—that is, the one
    on the left, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们尝试对分类器找到的眼睛的引用进行排序。我们将`left_eye`设置为具有较低第一坐标的眼睛——即左侧的眼睛，如下所示：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we have the location of both of the eyes, we want to figure out what
    kind of transformation we want to make in order to put the eyes in the hardcoded
    positions—that is, 25% from the sides and 25% below the top of the image.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了两个眼睛的位置，我们想要弄清楚我们想要进行什么样的转换，以便将眼睛放置在硬编码的位置——即图像两侧的25%和顶部以下的25%。
- en: Transforming the face
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换面部
- en: 'Transforming the face is a standard process that can be achieved by warping
    the image using `cv2.warpAffine` (recall [Chapter 3](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml),
    *Finding Objects via Feature Matching and Perspective Transforms*). We will follow
    the next steps to achieve this transformation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 转换面部是一个标准过程，可以通过使用`cv2.warpAffine`（回忆[第3章](905b17f6-8eea-4d33-9291-17ea93371f2d.xhtml)，*通过特征匹配和透视变换查找对象*)来实现。我们将遵循以下步骤来完成此转换：
- en: 'First, we calculate the angle (in degrees) between the line that connects the
    two eyes and a horizontal line, as follows:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算连接两个眼睛的线与水平线之间的角度（以度为单位），如下所示：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we derive a scaling factor that will scale the distance between the two
    eyes to be exactly 50% of the image width, like this:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们推导出一个缩放因子，将两个眼睛之间的距离缩放到图像宽度的50%，如下所示：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the two parameters (`eye_angle_deg` and `eye_size_scale`) in hand, we
    can now come up with a suitable rotation matrix that will transform our image,
    as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了两个参数（`eye_angle_deg`和`eye_size_scale`），我们可以现在提出一个合适的旋转矩阵，将我们的图像转换，如下所示：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we will make sure that the center of the eyes will be centered in the
    image, like this:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将确保眼睛的中心将在图像中居中，如下所示：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we arrive at an upright scaled version of the facial region that looks
    like the third image (named as Training Image) in the previous screenshot (eye
    regions are highlighted only for the demonstration), as follows:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们得到了一个垂直缩放的图像，看起来像前一个截图中的第三张图像（命名为训练图像），如下所示：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After this step, we know how to extract nicely aligned, cropped, and rotated
    images from unprocessed images. Now, it's time to take a look at how to use these
    images to identify facial expressions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们知道如何从未处理图像中提取整齐、裁剪和旋转的图像。现在，是时候看看如何使用这些图像来识别面部表情了。
- en: Collecting data
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集数据
- en: The facial-expression-recognition pipeline is encapsulated in `chapter8.py`.
    This file consists of an interactive GUI that operates in two modes (**training**
    and **testing**), as described earlier.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 面部表情识别管道封装在`chapter8.py`中。此文件包含一个交互式GUI，在两种模式（**训练**和**测试**）下运行，如前所述。
- en: 'Our entire application is divided into parts, mentioned as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序被分成几个部分，如下所述：
- en: 'Running the application in the `collect` mode using the following command from
    the command line:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从命令行以`collect`模式运行应用程序：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The previous command will pop up a GUI in the data collection mode to assemble
    a training set,
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将在数据收集模式下弹出一个GUI，以组装一个训练集，
- en: training an MLP classifier on the training set via `python train_classifier.py`.
    Because this step can take a long time, the process takes place in its own script.
    After successful training, store the trained weights in a file, so that we can
    load the pre-trained MLP in the next step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`python train_classifier.py`在训练集上训练一个MLP分类器。因为这个步骤可能需要很长时间，所以这个过程在自己的脚本中执行。训练成功后，将训练好的权重存储在文件中，以便我们可以在下一步加载预训练的MLP。
- en: 'Then, again running the GUI in the `demo` mode as follows, we will be able
    to see how good the facial recognition is on the real data:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，再次以以下方式在`demo`模式下运行GUI，我们将能够看到在真实数据上面部识别的效果如何：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In this mode, you will have a GUI to classify facial expressions on a live video
    stream in real time. This step involves loading several pre-trained cascade classifiers
    as well as our pre-trained MLP classifier. These classifiers will then be applied
    to every captured video frame.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模式下，你将有一个GUI来实时对实时视频流中的面部表情进行分类。这一步涉及到加载几个预训练的级联分类器以及我们的预训练MLP分类器。然后，这些分类器将被应用于每个捕获的视频帧。
- en: Now, let's take a look at how you can build an application to collect training
    data.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何构建一个用于收集训练数据的应用程序。
- en: Assembling a training dataset
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组装训练数据集
- en: Before we can train an MLP, we need to assemble a suitable training set. This
    is done because chances are that your face is not yet part of any dataset out
    there (the **National Security Agency's** (**NSA's**) private collection doesn't
    count), thus we will have to assemble our own. This is done most easily by returning
    to our GUI application from the previous chapters that can access a webcam, and
    operate on each frame of a video stream.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练一个MLP之前，我们需要组装一个合适的训练集。这是因为你的脸可能还不是任何现有数据集的一部分（**国家安全局**（**NSA**）的私人收藏不算），因此我们将不得不自己组装。这可以通过回到前几章中的GUI应用程序来完成，该应用程序可以访问网络摄像头，并处理视频流的每一帧。
- en: We are going to subclass the `wx_gui.BaseLayout` and tweak the **user interface**
    (**UI**) to our liking. We will have two classes for the two different modes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继承`wx_gui.BaseLayout`并调整**用户界面**（**UI**）以满足我们的喜好。我们将有两个类用于两种不同的模式。
- en: The GUI will present the user with the option of recording one of the following
    six emotional expressions—namely, neutral, happy, sad, surprised, angry, and disgusted.
    Upon clicking a button, the app will take a snapshot of the detected facial region
    and add it to the data collection in a file.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: GUI将向用户展示以下六种情感表达之一的选项——即中性、快乐、悲伤、惊讶、愤怒和厌恶。点击按钮后，应用将捕捉到检测到的面部区域并添加到文件中的数据收集。
- en: These samples can then be loaded from the file and used to train a machine learning
    classifier in `train_classifier.py`, as described in *Step 2* (given earlier).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些样本然后可以从文件中加载并用于在`train_classifier.py`中训练机器学习分类器，如*步骤2*（之前给出）所述。
- en: Running the application
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行应用程序
- en: 'As we have seen in the previous chapters with a **wxpython GUI**, in order
    to run this app (`chapter8.py`), we need to set up a screen capture by using `cv2.VideoCapture`,
    and pass the handle to the `FaceLayout` class. We can do this by following the
    next steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中看到的那样，使用**wxpython GUI**，为了运行这个应用（`chapter8.py`），我们需要使用`cv2.VideoCapture`设置屏幕捕获，并将句柄传递给`FaceLayout`类。我们可以通过以下步骤来完成：
- en: 'First, we create a `run_layout` function that will work with any `BaseLayout`
    subclass, as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`run_layout`函数，它将适用于任何`BaseLayout`子类，如下所示：
- en: '[PRE22]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, the code is very similar to the code from previous chapters
    that used `wxpython`. We open the webcam, set the resolution, initialize the layout,
    and start the main loop of the application. This type of optimization is good
    when you have to use the same function multiple times.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，代码与之前章节中使用的`wxpython`代码非常相似。我们打开网络摄像头，设置分辨率，初始化布局，并启动应用程序的主循环。当你需要多次使用相同的函数时，这种类型的优化是好的。
- en: Next, we set up an argument parser that will figure out which of the two layouts
    needs to be run and run it with the appropriate arguments.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置一个参数解析器，它将确定需要运行哪两个布局之一，并使用适当的参数运行它。
- en: 'To make use of the `run_layout` function in both modes, we add a command-line
    argument to our script using the `argparse` module, like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在两种模式下都使用`run_layout`函数，我们使用`argparse`模块在我们的脚本中添加一个命令行参数，如下所示：
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We have used the `argparse` module that comes with Python to set up an argument
    parser and add an argument with `collect` and `demo` options. We have also added
    an optional `--classifier` argument that we will use for `demo` mode only.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Python附带的`argparse`模块来设置参数解析器，并添加了具有`collect`和`demo`选项的参数。我们还添加了一个可选的`--classifier`参数，我们将在`demo`模式下使用它。
- en: 'Now, we use all the arguments that the user passed, to call the `run_layout` function
    with appropriate arguments, as follows:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用用户传递的所有参数，以适当的参数调用`run_layout`函数，如下所示：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see in the previous code, we have set it up to pass an extra `classifier_path`
    argument when we are in the `demo` mode. We will see how it is being used when
    we talk about `FacialExpresssionRecognizerLayout` in the later sections of this
    chapter.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们已设置在`demo`模式下传递额外的`classifier_path`参数。我们将在本章后面的部分讨论`FacialExpresssionRecognizerLayout`时看到它是如何被使用的。
- en: Now that we have established how to run our application, let's build the GUI
    elements.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了如何运行我们的应用程序，让我们构建GUI元素。
- en: Implementing the data collector GUI
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现数据收集GUI
- en: 'Analogous to some of the previous chapters, the GUI of the app is a customized
    version of the generic `BaseLayout`, as shown in the following code block:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章的一些内容类似，该应用程序的GUI是通用`BaseLayout`的定制版本，如下所示：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We start building the GUI by calling the constructor of the parent class to
    make sure it''s correctly initialized, like this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用父类的构造函数开始构建GUI，以确保它被正确初始化，如下所示：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that we have added some extra arguments in the previous code. Those are
    for all extra attributes that our class has and that the parent class doesn't.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在之前的代码中添加了一些额外的参数。这些参数是我们类中所有额外的属性，而父类中没有的属性。
- en: 'Next, before we go on to adding UI components, we also initialize a `FaceDetector`
    instance and a reference to the file to store data, as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续添加UI组件之前，我们还初始化了一个`FaceDetector`实例和用于存储数据的文件引用，如下所示：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Notice that we are using the hardcoded cascade XML files. Feel free to experiment
    with these as well.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在使用硬编码的级联XML文件。您可以随意尝试这些文件。
- en: Now, let's take a look at how we construct the UI using `wxpython`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何使用`wxpython`构建UI。
- en: Augmenting the basic layout
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强基本布局
- en: The creation of the layout is again deferred to a method called `augment_layout`.
    We keep the layout as simple as possible. We create a panel for the acquired video
    frame and draw a row of buttons below it.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 布局的创建再次推迟到名为`augment_layout`的方法中。我们尽可能保持布局简单。我们创建一个用于获取视频帧的面板，并在其下方绘制一排按钮。
- en: The idea is to then click one of the six radio buttons to indicate which facial
    expression you are trying to record, then place your head within the bounding
    box, and click the `Take Snapshot` button.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后点击六个单选按钮之一，以指示您要记录哪种面部表情，然后将头部放在边界框内，并点击`Take Snapshot`按钮。
- en: 'So, let''s have a look at how to build the six buttons, and correctly place
    them on a `wx.Panel` object. The code for this is shown in the following block:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看如何构建六个按钮，并将它们正确地放置在`wx.Panel`对象上。相应的代码如下所示：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can see that even if there is a lot of code, what we wrote is mostly repetitive.
    We create a `RadioButton` for each emotion and add the button to a `pnl2` panel.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，尽管代码量很大，但我们所写的大部分内容都是重复的。我们为每种情绪创建一个`RadioButton`，并将按钮添加到`pnl2`面板中。
- en: 'The `Take Snapshot` button is placed below the radio buttons and will bind
    to the `_on_snapshot` method, as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`Take Snapshot`按钮放置在单选按钮下方，并将绑定到`_on_snapshot`方法，如下所示：'
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As the comment suggests, we created a new panel and added a regular button with
    the `Take Snapshot` label. The important part is that we bind the click on the
    button to the `self._on_snapshot` method, which will process each captured image
    once we click on the `Take Snapshot` button.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如注释所示，我们创建了一个新的面板，并添加了一个带有`Take Snapshot`标签的常规按钮。重要的是，我们将按钮的点击事件绑定到`self._on_snapshot`方法，这样我们点击`Take
    Snapshot`按钮后，将处理捕获的每一张图片。
- en: 'The layout will look like the following screenshot:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 布局将如下截图所示：
- en: '![](img/84c0313e-4ee5-4ad5-8475-5b67e47d6df6.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84c0313e-4ee5-4ad5-8475-5b67e47d6df6.png)'
- en: 'To make these changes take effect, the created panels need to be added to the
    list of existing panels, like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这些更改生效，需要将创建的面板添加到现有面板的列表中，如下所示：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The rest of the visualization pipeline is handled by the `BaseLayout` class.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的可视化管道由`BaseLayout`类处理。
- en: Now, let's see how we add boundary boxes to the faces once they appear in the
    video capture, using the `process_frame` method.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们是如何在视频捕获中一旦人脸出现就使用`process_frame`方法添加边界框的。
- en: Processing the current frame
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理当前帧
- en: 'The `process_frame` method is called on all the images, and we''d like to show
    a frame around a face when it appears in the video feed. This is illustrated in
    the following code block:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`process_frame`方法被调用在所有图像上，我们希望在视频流中出现人脸时显示一个围绕人脸的帧。如下所示：'
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have just called the  `FaceDetector.detect_face` method of the `self.face_detector`
    object we created in the constructor of the layout class. Remember from the previous
    section that it detects faces in a downscaled grayscale version of the current
    frame using Haar cascades.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚调用了在布局类的构造函数中创建的`self.face_detector`对象的`FaceDetector.detect_face`方法。记得从上一节中，它使用Haar级联在当前帧的降尺度灰度版本中检测人脸。
- en: So, we are adding a frame if we recognize a face; that's it. Now, let's look
    at how we store training images inside the `_on_snapshot` method.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们识别出人脸，我们就添加一个帧；就是这样。现在，让我们看看我们是如何在`_on_snapshot`方法中存储训练图像的。
- en: Storing the data
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储数据
- en: 'We will store the data once the user clicks on the Take Snapshot button, and
    the `_on_snapshot` event listener method is called, as shown in the following
    code block:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户点击“拍摄快照”按钮，并调用`_on_snapshot`事件监听器方法时，我们将存储数据，如下所示：
- en: '[PRE32]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s take a look at the code inside this method, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个方法内部的代码，如下所示：
- en: 'First, we figure out the label of the image by finding out which of the radio
    buttons was selected, like this:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过找出哪个单选按钮被选中来确定图像的标签，如下所示：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see, it's very straightforward, once we realize that each radio button
    has a `GetValue()` method that returns `True` only if it was selected.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一旦我们意识到每个单选按钮都有一个`GetValue()`方法，它仅在它被选中时返回`True`，这个过程就非常直接。
- en: Next, we need to look at the detected facial region of the current frame (stored
    in `self.head` by `detect_head`) and align it with all the other collected frames.
    That is, we want all the faces to be upright and the eyes to be aligned.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要查看当前帧中检测到的面部区域（由`detect_head`存储在`self.head`中）并将其与其他所有收集到的帧对齐。也就是说，我们希望所有的人脸都是直立的，眼睛是对齐的。
- en: 'Otherwise, if we do not align the data samples, we run the risk of having the
    classifier compare eyes to noses. Because this computation can be costly, we do
    not apply it on every frame in the `process_frame` method, but instead only upon
    taking a snapshot in the `_on_snapshot` method, as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果我们不对齐数据样本，我们面临的风险是分类器会将眼睛与鼻子进行比较。因为这个计算可能很昂贵，所以我们不在`process_frame`方法的每一帧上应用它，而是在`_on_snapshot`方法中仅在对快照进行操作时应用，如下所示：
- en: '[PRE34]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Since this happened after `process_frame` was called, we already had access
    to `self.head`, which stored the image of the head present in the current frame.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这发生在调用`process_frame`之后，我们已经有权限访问`self.head`，它存储了当前帧中头部的图像。
- en: 'Next, if we have successfully aligned the head (that is, if we have found the
    eyes), we will store the datum. Otherwise, we notify the user, using a `print`
    command to the Terminal, as follows:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，如果我们已经成功对齐了头部（也就是说，如果我们已经找到了眼睛），我们将存储数据。否则，我们将通过向终端发送一个`print`命令来通知用户，如下所示：
- en: '[PRE35]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Actual saving is done in the  `save_datum` function, which we have abstracted
    away since it is not part of the UI. Also, this is handy in case you want to add
    a different dataset to your file, as illustrated in the following code block:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的保存是在`save_datum`函数中完成的，我们将其抽象出来，因为它不是UI的一部分。此外，如果你想要向文件中添加不同的数据集，这会很有用，如下所示：
- en: '[PRE36]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see in the previous code, we use a `.csv` file to store the data,
    where each of the images is a `newline`. So, if you want to go back and delete
    an image (maybe you had forgotten to comb your hair), you just have to open the
    `.csv` file with a text editor and delete that line.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的代码所示，我们使用`.csv`文件来存储数据，其中每个图像都是一个`newline`。所以，如果你想回去删除一个图像（也许你忘记梳理头发），你只需要用文本编辑器打开`.csv`文件并删除那一行。
- en: Now, let's move to more interesting parts, and find out how we are going to
    use the data we collect to be able to train a machine learning model to detect
    emotions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向更有趣的部分，找出我们如何使用我们收集的数据来训练一个机器学习模型以检测情感。
- en: Understanding facial emotion recognition
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解面部情感识别
- en: In this section, we will train an MLP to recognize facial emotions in the pictures.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练一个MLP来识别图片中的面部情感。
- en: We have previously made the point that finding the features that best describe
    the data is often an essential part of the entire learning task. We have also
    looked at common preprocessing methods, such as **mean subtraction** and **normalization**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经指出，找到最能描述数据的特征往往是整个学习任务的一个重要部分。我们还探讨了常见的预处理方法，如**均值减法**和**归一化**。
- en: Here, we will look at an additional method that has a long tradition in face
    recognition—that is, PCA. We are hoping that, even if we don't collect thousands
    of training pictures, PCA will help us get good results.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将探讨一个在人脸识别中有着悠久传统的额外方法——那就是PCA。我们希望即使我们没有收集成千上万的训练图片，PCA也能帮助我们获得良好的结果。
- en: Processing the dataset
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据集
- en: Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *L**earning
    to Recognize Traffic Signs*, we write a new dataset parser in `data/emotions.py` that
    will parse our self-assembled training set.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第7章](0e410c47-1679-4125-9614-54ec0adfa160.xhtml)，*学习识别交通标志*，我们在`data/emotions.py`中编写了一个新的数据集解析器，该解析器将解析我们自行组装的训练集。
- en: 'We define a `load_data` function that will load the training data and `return`
    a tuple of collected `data` and their corresponding labels, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`load_data`函数，该函数将加载训练数据并`返回`一个包含收集到的`数据`及其对应标签的元组，如下所示：
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This code, similar to all the processing codes, is self-contained and resides
    in the `data/process.py` file, similar to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码，类似于所有处理代码，是自包含的，并位于`data/process.py`文件中，类似于[第7章](0e410c47-1679-4125-9614-54ec0adfa160.xhtml)，*学习识别交通标志*。
- en: Our featurization function in this chapter is going to be the `pca_featurize` function
    that will perform PCA on all samples. But unlike [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, our featurization function takes into account the
    characteristics of the entire dataset, instead of operating on each of the images
    separately.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的特征化函数将是`pca_featurize`函数，它将对所有样本执行PCA。但与[第7章](0e410c47-1679-4125-9614-54ec0adfa160.xhtml)，*学习识别交通标志*不同，我们的特征化函数考虑了整个数据集的特征，而不是单独对每张图像进行操作。
- en: 'Now, instead of returning only the featurized data (as in [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*), it will return a tuple of training data, and all
    parameters necessary to apply the same function to the test data, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，它将返回一个包含训练数据和应用于测试数据所需的所有参数的训练数据元组，如下所示：
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, let's figure out what is PCA, and why we need it.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们弄清楚PCA是什么，以及为什么我们需要它。
- en: Learning about PCA
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习PCA
- en: PCA is a dimensionality-reduction technique that is helpful whenever we are
    dealing with high-dimensional data. In a sense, you can think of an image as a
    point in a high-dimensional space. If we flatten a 2D image of height `m` and
    width `n` (by concatenating either all rows or all columns), we get a (feature)
    vector of length *m x n*. The value of the i^(th) element in this vector is the
    grayscale value of the i^(th) pixel in the image.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种降维技术，在处理高维数据时非常有用。从某种意义上说，你可以将图像视为高维空间中的一个点。如果我们通过连接所有行或所有列将高度为`m`和宽度为`n`的2D图像展平，我们得到一个长度为*m
    x n*的（特征）向量。这个向量中第i个元素的值是图像中第i个像素的灰度值。
- en: To describe every possible 2D grayscale image with these exact dimensions, we
    will need an *m x n*-dimensional vector space that contains *256^(m x n)* vectors.
    Wow!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述所有可能的具有这些精确尺寸的2D灰度图像，我们需要一个*m x n*维度的向量空间，其中包含*256^(m x n)*个向量。哇！
- en: An interesting question that comes to mind when considering these numbers is—*Could
    there be a smaller, more compact vector space (using less-than m x n features)
    that describes all these images equally well?* After all, we have previously realized
    that grayscale values are not the most informative measures of content.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑到这些数字时，一个有趣的问题浮现在脑海中——*是否可能存在一个更小、更紧凑的向量空间（使用小于m x n的特征）来同样好地描述所有这些图像？* 因为毕竟，我们之前已经意识到灰度值并不是内容的最有信息量的度量。
- en: 'This is where PCA comes into the picture. Consider a dataset from which we
    extracted exactly two features. These features could be the grayscale values of
    pixels at some *x* and *y* positions, but they could also be more complex than
    that. If we plot the dataset along these two feature axes, the data might be mapped
    within some multivariate Gaussian distribution, as shown in the following screenshot:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是主成分分析（PCA）介入的地方。考虑一个数据集，我们从其中提取了恰好两个特征。这些特征可能是某些*x*和*y*位置的像素的灰度值，但它们也可能比这更复杂。如果我们沿着这两个特征轴绘制数据集，数据可能被映射到某个多元高斯分布中，如下面的截图所示：
- en: '![](img/97e27eaa-2ed0-4baf-b382-71d3d3faa250.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/97e27eaa-2ed0-4baf-b382-71d3d3faa250.png)'
- en: What PCA does is *rotate* all data points until the data is mapped aligned with
    the two axes (the two inset vectors) that explain most of the *spread* of the
    data. PCA considers these two axes to be the most informative because, if you
    walk along with them, you can see most of the data points separated. In more technical
    terms, PCA aims to transform the data to a new coordinate system by means of an
    orthogonal linear transformation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: PCA所做的是将所有数据点旋转，直到数据映射到解释数据大部分*扩散*的两个轴（两个内嵌向量）上。PCA认为这两个轴是最有信息的，因为如果你沿着它们走，你可以看到大部分数据点分离。用更技术性的术语来说，PCA旨在通过正交线性变换将数据转换到一个新的坐标系中。
- en: The new coordinate system is chosen such that if you project the data onto these
    new axes, the first coordinate (called the **first principal component**) observes
    the greatest variance. In the preceding screenshot, the small vectors drawn correspond
    to the eigenvectors of the covariance matrix, shifted so that their tails come
    to lie at the mean of the distribution.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 新的坐标系被选择得使得如果你将数据投影到这些新轴上，第一个坐标（称为**第一个主成分**）观察到最大的方差。在前面的截图中，画的小向量对应于协方差矩阵的特征向量，它们的尾部被移动到分布的均值处。
- en: 'If we had previously calculated a set of basis vectors (`top_vecs`) and mean
    (`center`), transforming the data would be straightforward, as stated in the previous
    paragraph—we subtract the center from each datum, then multiply those vectors
    by principal components, as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们之前已经计算了一组基向量（`top_vecs`）和均值（`center`），那么转换数据将非常直接，正如前一段所述——我们从每个数据点中减去中心，然后将这些向量乘以主成分，如下所示：
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Notice that the previous code will work for any number of `top_vecs`; thus,
    if we only supply a `num_components` number of top vectors, it will reduce the
    dimensionality of the data to `num_components`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的代码将对任何数量的`top_vecs`都有效；因此，如果我们只提供`num_components`数量的顶级向量，它将降低数据的维度到`num_components`。
- en: 'Now, let''s construct a `pca_featurize` function that takes only the data,
    and returns both the transformation and the list of arguments necessary to replicate
    the transformation—that is, `center` and `top_vecs`— so we can apply `_pcea_featurize`
    on the testing data as well, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个`pca_featurize`函数，它只接受数据，并返回转换以及复制转换所需的参数列表——即`center`和`top_vecs`——这样我们就可以在测试数据上应用`_pcea_featurize`，如下所示：
- en: '[PRE40]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Fortunately, someone else has already figured out how to do all this in Python.
    In OpenCV, performing PCA is as simple as calling `cv2.PCACompute`, but we have
    to pass correct arguments rather than re-format what we get from OpenCV. Here
    are the steps:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有人已经想出了如何在Python中完成所有这些操作。在OpenCV中，执行PCA就像调用`cv2.PCACompute`一样简单，但我们必须传递正确的参数，而不是重新格式化我们从OpenCV得到的内容。以下是步骤：
- en: 'First, we convert `training_data` into a NumPy 2D array, like this:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将`training_data`转换为一个NumPy 2D数组，如下所示：
- en: '[PRE41]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, we call `cv2.PCACompute`, which computes the center of the data, and
    the principal components, as follows:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用`cv2.PCACompute`，它计算数据的中心，以及主成分，如下所示：
- en: '[PRE42]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can limit ourselves to the most informative components of `num_components` by
    running the following code:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码来限制自己只使用`num_components`中最有信息量的成分：
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The beauty of PCA is that the first principal component, by definition, explains
    most of the variance of the data. In other words, the first principal component
    is the most informative of the data. This means that we do not need to keep all
    of the components to get a good representation of the data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的美丽之处在于，根据定义，第一个主成分解释了数据的大部分方差。换句话说，第一个主成分是数据中最有信息量的。这意味着我们不需要保留所有成分来得到数据的良好表示。
- en: 'We also convert `mean` to create a new `center` variable that is a 1D vector
    that represents the center of the data, as follows:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将`mean`转换为创建一个新的`center`变量，它是一个表示数据中心的1D向量，如下所示：
- en: '[PRE44]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we return the training data processed by the `_pca_featurize` function,
    and the arguments necessary to pass to the `_pca_featurize` function, to replicate
    the same transformation so that the test data could be *featurized* in the exact
    same way as the train data, as follows:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们返回由`_pca_featurize`函数处理过的训练数据，以及传递给`_pca_featurize`函数的必要参数，以便复制相同的转换，这样测试数据就可以以与训练数据完全相同的方式被*特征化*，如下所示：
- en: '[PRE45]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Now we know how to clean and featurize our data, it's time to look at the training
    method we use to learn to recognize facial emotions.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何清理和特征化我们的数据，是时候看看我们用来学习识别面部情绪的训练方法了。
- en: Understanding MLPs
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解MLP
- en: MLPs have been around for a while. MLPs are **artificial neural networks** (**ANNs**)
    used to convert a set of input data into a set of output data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: MLP已经存在了一段时间。MLP是用于将一组输入数据转换为输出数据的**人工神经网络（ANNs**）。
- en: At the heart of an MLP is a **perceptron**, which resembles (yet overly simplifies)
    a biological neuron. By combining a large number of perceptrons in multiple layers,
    the MLP is able to make nonlinear decisions about its input data. Furthermore,
    MLPs can be trained with **backpropagation**, which makes them very interesting
    for supervised learning.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: MLP的核心是一个**感知器**，它类似于（但过于简化）生物神经元。通过在多个层中组合大量感知器，MLP能够对其输入数据进行非线性决策。此外，MLP可以通过**反向传播**进行训练，这使得它们对于监督学习非常有兴趣。
- en: The following section explains the concept of a perceptron.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了感知器的概念。
- en: Understanding a perceptron
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解感知器
- en: A perceptron is a binary classifier that was invented in the 1950s by Frank
    Rosenblatt. A perceptron calculates a weighted sum of its inputs, and, if this
    sum exceeds a threshold, it outputs a `1`; else, it outputs a `0`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是一种二分类器，由Frank Rosenblatt在20世纪50年代发明。感知器计算其输入的加权总和，如果这个总和超过阈值，它输出一个`1`；否则，它输出一个`0`。
- en: In some sense, a perceptron is integrating evidence that its afferents signal
    the presence (or absence) of some object instance, and if this evidence is strong
    enough, the perceptron will be active (or silent). This is loosely connected to
    what researchers believe biological neurons are doing (or can be used to do) in
    the brain, hence the term *ANN*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，感知器正在整合证据，其传入信号表示某些对象实例的存在（或不存在），如果这种证据足够强烈，感知器就会活跃（或沉默）。这与研究人员认为生物神经元在大脑中（或可以用来做）做的事情（或可以用来做）松散相关，因此有*ANN*这个术语。
- en: 'A sketch of a perceptron is depicted in the following screenshot:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的一个草图在以下屏幕截图中有展示：
- en: '![](img/68ef36cd-b7a3-4e7b-9f95-906df261fcea.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68ef36cd-b7a3-4e7b-9f95-906df261fcea.png)'
- en: Here, a perceptron computes a weighted (`w[i]`) sum of all its inputs (`x[i]`),
    combined with a bias term (`b`). This input is fed into a nonlinear activation
    function (`θ`) that determines the output of the perceptron (`y`). In the original
    algorithm, the activation function was the **Heaviside** **step function**.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，感知器计算所有输入（`x[i]`）的加权（`w[i]`）总和，加上一个偏置项（`b`）。这个输入被送入一个非线性激活函数（`θ`），它决定了感知器的输出（`y`）。在原始算法中，激活函数是**Heaviside**
    **阶跃函数**。
- en: 'In modern implementations of ANNs, the activation function can be anything
    ranging from sigmoid to hyperbolic tangent functions. The Heaviside step function
    and the sigmoid function are plotted in the following screenshot:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代ANN的实现中，激活函数可以是任何从Sigmoid到双曲正切函数的范围。Heaviside阶跃函数和Sigmoid函数在以下屏幕截图中有展示：
- en: '![](img/74c33d6e-33a1-406b-af14-2d6562369ffa.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/74c33d6e-33a1-406b-af14-2d6562369ffa.png)'
- en: Depending on the activation function, these networks might be able to perform
    either classification or regression. Traditionally, one only talks of MLPs when
    nodes use the Heaviside step function.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 根据激活函数的不同，这些网络可能能够执行分类或回归。传统上，只有当节点使用Heaviside阶跃函数时，人们才谈论MLP。
- en: Knowing about deep architectures
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解深度架构
- en: Once you have the perceptron figured out, it would make sense to combine multiple
    perceptrons to form a larger network. MLPs usually consist of at least three layers,
    where the first layer has a node (or neuron) for every input feature of the dataset,
    and the last layer has a node for every class label.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你搞清楚了感知器的工作原理，将多个感知器组合成更大的网络就很有意义了。MLP（多层感知器）通常至少包含三个层，其中第一层为数据集的每个输入特征都有一个节点（或神经元），而最后一层为每个类别标签都有一个节点。
- en: 'The layer in between the first and the last layer is called the hidden layer.
    An example of this feed-forward neural network is shown in the following screenshot:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层和最后一层之间的层被称为隐藏层。以下截图展示了这种前馈神经网络的例子：
- en: '![](img/16e356e4-ba08-47b3-ad1e-b69f20123da3.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16e356e4-ba08-47b3-ad1e-b69f20123da3.png)'
- en: In a feed-forward neural network, some or all of the nodes in the input layer
    are connected to all the nodes in the hidden layer, and some or all of the nodes
    in the hidden layer are connected to some or all of the nodes in the output layer.
    You would usually choose the number of nodes in the input layer to be equal to
    the number of features in the dataset so that each node represents one feature.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈神经网络中，输入层的一些或所有节点连接到隐藏层的所有节点，隐藏层的一些或所有节点连接到输出层的一些或所有节点。你通常会选择输入层的节点数等于数据集中的特征数，以便每个节点代表一个特征。
- en: Analogously, the number of nodes in the output layer is usually equal to the
    number of classes in the data, so that when an input sample of class `c` is presented,
    the *c^(th)* node in the output layer is active, and all others are silent.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，输出层的节点数通常等于数据集中的类别数，因此当输入样本为类别`c`时，输出层的第`c`个节点是活跃的，而其他所有节点都是沉默的。
- en: It is also, of course, possible to have multiple hidden layers. Often, it is
    not clear beforehand what the optimal size of the network should be.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以有多个隐藏层。通常，事先并不清楚网络的理想大小应该是多少。
- en: 'Typically, you will see the error rate on the training set decrease when you
    add more neurons to the network, as is depicted in the following screenshot (*thinner*,
    *red curve*):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你向网络中添加更多神经元时，你会看到训练集上的误差率下降，如下面的截图所示（*较细的*，*红色曲线*）：
- en: '![](img/971ee10c-412e-4c73-944e-54fdf9c61d3b.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/971ee10c-412e-4c73-944e-54fdf9c61d3b.png)'
- en: This is because the expressive power or complexity (also referred to as the
    **Vapnik-Chervonenkis** or **VC dimension**) of the model increases with the increasing
    size of the neural network. However, the same cannot be said for the error rate
    on the test set shown in the preceding screenshot (*thicker*, *blue curve*).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为模型的表达能力或复杂性（也称为**Vapnik-Chervonenkis**或**VC维度**）随着神经网络规模的增加而增加。然而，对于前面截图中所显示的测试集上的误差率（*较粗的*，*蓝色曲线*）来说，情况并非如此。
- en: Instead, you will find that, with increasing model complexity, the test error
    goes through its minimum, and adding more neurons to the network will not improve
    the performance on the test data anymore. Therefore, you would want to keep the
    size of the neural network to what is labeled the optimal range in the preceding
    screenshot, which is where the network achieves the best generalization performance.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你会发现，随着模型复杂性的增加，测试误差会达到其最小值，向网络中添加更多神经元也不再能提高测试数据的性能。因此，你希望将神经网络的规模保持在前面截图中所标记的“最佳范围”，这是网络实现最佳泛化性能的地方。
- en: You can think of it in this way—a model of weak complexity (on the far left
    of the plot) is probably too small to really understand the dataset that it is
    trying to learn, thus yielding large error rates on both the training and the
    test sets. This is commonly referred to as **underfitting**.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样想——一个弱复杂度模型（在图表的左侧）可能太小，无法真正理解它试图学习的数据集，因此训练集和测试集上的误差率都很大。这通常被称为**欠拟合**。
- en: On the other hand, a model on the far right of the plot is probably so complex
    that it begins to memorize the specifics of each sample in the training data,
    without paying attention to the general attributes that make a sample stand apart
    from the others. Therefore, the model will fail when it has to predict data that
    it has never seen before, effectively yielding a large error rate on the test
    set. This is commonly referred to as **overfitting**.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，图表右侧的模型可能过于复杂，以至于开始记住训练数据中每个样本的具体细节，而没有注意到使样本与众不同的通用属性。因此，当模型需要预测它以前从未见过的数据时，它将失败，从而在测试集上产生很大的误差率。这通常被称为**过拟合**。
- en: Instead, what you want is to develop a model that neither is underfitting nor
    overfitting. Often, this can only be achieved by *trial and error*; that is, by
    considering the network size as a hyperparameter that needs to be tweaked and
    tuned, depending on the exact task to be performed.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你想要的是一个既不过拟合也不欠拟合的模型。通常，这只能通过试错来实现；也就是说，将网络大小视为一个需要根据要执行的确切任务进行调整和微调的超参数。
- en: An MLP learns by adjusting its weights so that when an input sample of class
    `c` is presented, the *c^(th)* node in the output layer is active and all the
    others are silent. MLPs are trained by means of the **backpropagation** method,
    which is an algorithm to calculate the partial derivative of a **loss function**
    with respect to any synaptic weight or neuron bias in the network. These partial
    derivatives can then be used to update the weights and biases in the network in
    order to reduce the overall loss step by step.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: MLP通过调整其权重来学习，当展示一个类`c`的输入样本时，输出层的第*c*个节点是活跃的，而其他所有节点都是沉默的。MLP通过**反向传播**方法进行训练，这是一种计算网络中任何突触权重或神经元偏置相对于**损失函数**的偏导数的算法。这些偏导数可以用来更新网络中的权重和偏置，以逐步减少整体损失。
- en: A loss function can be obtained by presenting training samples to the network
    and by observing the network's output. By observing which output nodes are active
    and which are dormant, we can calculate the relative error between the output
    of the last layer and the true labels we provided with the loss function.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向网络展示训练样本并观察网络的输出，可以获得一个损失函数。通过观察哪些输出节点是活跃的，哪些是休眠的，我们可以计算最后一层的输出与通过损失函数提供的真实标签之间的相对误差。
- en: We then make corrections to all the weights in the network so that the error
    decreases over time. It turns out that the error in the hidden layer depends on
    the output layer, and the error in the input layer depends on the error in both
    the hidden layer and the output layer. Thus, in a sense, the error backpropagates
    through the network. In OpenCV, backpropagation is used by specifying `cv2.ANN_MLP_TRAIN_PARAMS_BACKPROP`
    in the training parameters.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对网络中的所有权重进行修正，以便随着时间的推移误差逐渐减小。结果发现，隐藏层的误差取决于输出层，输入层的误差取决于隐藏层和输出层的误差。因此，从某种意义上说，误差会反向传播通过网络。在OpenCV中，通过在训练参数中指定`cv2.ANN_MLP_TRAIN_PARAMS_BACKPROP`来使用反向传播。
- en: Gradient descent comes in two common flavors—that is, in **stochastic gradient
    descent** and **batch learning**.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有两种常见的类型——即**随机梯度下降**和**批量学习**。
- en: In stochastic gradient descent, we update the weights after each presentation
    of a training example, whereas, in batch learning, we present training examples
    in batches and update the weights only after each batch is presented. In both
    scenarios, we have to make sure that we adjust the weights only slightly per sample
    (by adjusting the **learning rate**) so that the network slowly converges to a
    stable solution over time.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机梯度下降中，我们在展示每个训练示例后更新权重，而在批量学习中，我们以批量的形式展示训练示例，并且只在每个批量展示后更新权重。在这两种情况下，我们必须确保我们只对每个样本进行轻微的权重调整（通过调整**学习率**），以便网络随着时间的推移逐渐收敛到一个稳定的解。
- en: Now, after learning the theory behind MLPs, let's get our hands dirty and code
    this up using OpenCV.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在学习了MLP的理论之后，让我们动手用OpenCV来实现它。
- en: Crafting an MLP for facial expression recognition
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计用于面部表情识别的MLP
- en: 'Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, we will use the machine learning class that OpenCV
    provides, which is `ml.ANN_MLP`. Here are the steps to create and configure an
    MLP in OpenCV:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第7章](0e410c47-1679-4125-9614-54ec0adfa160.xhtml)，*学习识别交通标志*，我们将使用OpenCV提供的机器学习类，即`ml.ANN_MLP`。以下是创建和配置OpenCV中MLP的步骤：
- en: 'Instantiate an empty `ANN_MLP` object, like this:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个空的`ANN_MLP`对象，如下所示：
- en: '[PRE46]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Set the network architecture—the first layer equal to the dimensionality of
    the data, and the last layer equal to the output size of `6` required for the
    number of possible emotions, as follows:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置网络架构——第一层等于数据的维度，最后一层等于所需的输出大小`6`（用于可能的情绪数量），如下所示：
- en: '[PRE47]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We set the training algorithm to backpropagation, and use the symmetric sigmoid
    function for activation, as we discussed in the previous sections, by running
    the following code:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将训练算法设置为反向传播，并使用对称的sigmoid函数作为激活函数，正如我们在前面的章节中讨论的那样，通过运行以下代码：
- en: '[PRE48]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we set the termination criteria to either after `30` iterations or
    when the loss reaches values smaller than `0.000001`, as follows, and we are ready
    to train the MLP:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将终止条件设置为在`30`次迭代后或当损失达到小于`0.000001`的值时，如下所示，我们就可以准备训练MLP了：
- en: '[PRE49]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In order to train the MLP, we need training data. We would also like to have
    an idea of how well our classifier is doing, so we need to split the collected
    data into training and test sets.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练MLP，我们需要训练数据。我们还想了解我们的分类器做得如何，因此我们需要将收集到的数据分为训练集和测试集。
- en: The best way to split the data would be to make sure that we don't have near-identical
    images in the training and testing sets—for example, the user double-clicked on
    the Take Snapshot button, and we have two images that were taken milliseconds
    apart, thus are almost identical. Unfortunately, that is a tedious and manual
    process and out of the scope of this book.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 分割数据最好的方式是确保训练集和测试集中没有几乎相同的图像——例如，用户双击了“捕获快照”按钮，我们有两个相隔毫秒的图像，因此几乎是相同的。不幸的是，这是一个繁琐且手动的过程，超出了本书的范围。
- en: 'We define the signature of the function, as follows. We want to get indices
    of an array of size `n` and we want to specify a ratio of the train data to all
    the data:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义函数的签名，如下。我们想要得到一个大小为`n`的数组的索引，我们想要指定训练数据与所有数据的比例：
- en: '[PRE50]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Geared with the signature, let''s go over the `train_test_split` function step
    by step, as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 带着签名，让我们一步一步地回顾`train_test_split`函数，如下所示：
- en: 'First, we create a list of `indices` and `shuffle` them, like this:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个`indices`列表并对其进行`shuffle`，如下所示：
- en: '[PRE51]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we calculate the number of training points that need to be in the `N` training
    dataset, like this:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算`N`训练数据集中需要有多少个训练点，如下所示：
- en: '[PRE52]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After that, we create a selector for the first `N` indices for the training
    data, and create a selector for the rest of `indices` to be used for the test
    data, as follows:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们为训练数据的前`N`个索引创建一个选择器，并为剩余的`indices`创建一个选择器，用于测试数据，如下所示：
- en: '[PRE53]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now we have a model class and a training data generator, let's take a look at
    how to train the MLP.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个模型类和一个训练数据生成器，让我们看看如何训练MLP。
- en: Training the MLP
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练MLP
- en: OpenCV provides all the training and predicting methods, so we have to figure
    out how to format our data to fit the OpenCV requirements.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV提供了所有的训练和预测方法，因此我们需要弄清楚如何格式化我们的数据以符合OpenCV的要求。
- en: 'First, we split the data into train/test, and featurize the training data,
    as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据分为训练/测试，并对训练数据进行特征化，如下所示：
- en: '[PRE54]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Here, `pca_args` are the arguments that we will need to store if we wanted to
    featurize any future data (for example, live frames during the demo).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`pca_args`是我们如果想要特征化任何未来的数据（例如，演示中的实时帧）需要存储的参数。
- en: 'Because the `train` method of the `cv2.ANN_MLP` module does not allow integer-valued
    class labels, we need to first convert `y_train` into one-hot encoding, consisting
    only of 0s and 1s, which can then be fed to the `train` method, as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`cv2.ANN_MLP`模块的`train`方法不允许整数值的类别标签，我们需要首先将`y_train`转换为one-hot编码，只包含0和1，然后可以将其输入到`train`方法中，如下所示：
- en: '[PRE55]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The one-hot encoding is taken care of in the `one_hot_encode` function in `train_classifiers.py`,
    in the following way:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: one-hot编码在`train_classifiers.py`中的`one_hot_encode`函数中处理，如下所示：
- en: 'First, we determine how many points there are in the data, like this:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们确定数据中有多少个点，如下所示：
- en: '[PRE56]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Each `c` class label in `all_labels` needs to be converted into a (`len(unique_labels)`)
    long vector of 0s and 1s, where all entries are zeros except the *c^(th)*, which
    is a 1\. We prepare this operation by allocating a vector of zeros, like this:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`all_labels`中的每个`c`类标签都需要转换为一个长度为`len(unique_labels)`的0和1的向量，其中所有条目都是0，除了*c^(th)*，它是一个1。我们通过分配一个全0的向量来准备这个操作，如下所示：'
- en: '[PRE57]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, we create dictionaries mapping indices of columns to labels, and vice
    versa, as follows:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建字典，将列的索引映射到标签，反之亦然，如下所示：
- en: '[PRE58]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The vector elements at these indices then need to be set to `1`, as follows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些索引处的向量元素需要设置为`1`，如下所示：
- en: '[PRE59]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We also return `index_to_label` so we are able to recover the label from the
    prediction vector, as follows:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还返回`index_to_label`，这样我们就能从预测向量中恢复标签，如下所示：
- en: '[PRE60]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We now move on to the testing of the MLP that we just trained.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续测试我们刚刚训练的MLP。
- en: Testing the MLP
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试MLP
- en: Analogous to [Chapter 7](0e410c47-1679-4125-9614-54ec0adfa160.xhtml), *Learning
    to Recognize Traffic Signs*, we will evaluate the performance of our classifier
    in terms of accuracy, precision, and recall.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[第7章](0e410c47-1679-4125-9614-54ec0adfa160.xhtml)，*学习识别交通标志*，我们将评估我们的分类器在准确率、精确率和召回率方面的性能。
- en: 'To reuse our previous code, we just need to calculate `y_hat` and pass `y_true`
    alongside it to the metric functions by doing the following:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重用我们之前的代码，我们只需要计算 `y_hat` 并通过以下方式将 `y_true` 一起传递给度量函数：
- en: 'First, we featurize our test data using the `pca_args` we stored when we featurized
    the training data, and the `_pca_featurize` function, like this:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用存储在特征化训练数据时的 `pca_args` 和 `_pca_featurize` 函数，对测试数据进行特征化，如下所示：
- en: '[PRE61]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Then, we predict the new labels, like this:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们预测新的标签，如下所示：
- en: '[PRE62]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Finally, we extract the true test labels using indices we stored for testing,
    as follows:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用存储的测试索引提取真实的测试标签，如下所示：
- en: '[PRE63]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The only things that are left to pass to a function are both `y_hat` and `y_true`,
    to calculate the accuracy of our classifier.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一要传递给函数的是 `y_hat` 和 `y_true`，以计算我们分类器的准确率。
- en: It took me 84 pictures (10-15 of each emotion) to get to a training accuracy
    of `0.92` and have a good enough classifier to be able to show off my software
    to my friends. *Can you beat it?*
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要84张图片（每种情绪10-15张）才能达到 `0.92` 的训练准确率，并拥有足够好的分类器向我的朋友们展示我的软件。*你能打败它吗？*
- en: Now, let's see how we run the training script, and save the output in a manner
    that the demo application will be able to use.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何运行训练脚本，并以演示应用程序能够使用的方式保存输出。
- en: Running the script
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行脚本
- en: 'The MLP classifier can be trained and tested by using the `train_classifier.py`
    script, which does the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `train_classifier.py` 脚本来训练和测试 MLP 分类器，该脚本执行以下操作：
- en: 'The script first sets up the command-line options of `--data` to the location
    of the saved data, and `--save` to the location of a directory where we want to
    save the trained model (this argument is optional), as follows:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本首先将 `--data` 命令行选项设置为保存数据的位置，将 `--save` 设置为我们想要保存训练模型的目录位置（此参数是可选的），如下所示：
- en: '[PRE64]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then, we load the saved data, and follow the training procedure described in
    the previous section, as follows:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载保存的数据，并按照上一节中描述的训练过程进行，如下所示：
- en: '[PRE65]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Finally, we check if the user wants us to save the trained model, by running
    the following code:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过运行以下代码检查用户是否希望我们保存训练好的模型：
- en: '[PRE66]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The previous code saves the trained model, the `index_to_label` dictionary so
    that we can display human-readable labels in the demo, and `pca_args` so that
    we can featurize live camera feed frames in the demo.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码保存了训练好的模型，`index_to_label` 字典，以便在演示中显示可读的标签，以及 `pca_args`，以便在演示中特征化实时摄像头帧。
- en: The saved `mlp.xml` file contains the network configuration and learned weights.
    OpenCV knows how to load it. So, let's see what the demo application looks like.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的 `mlp.xml` 文件包含网络配置和学习的权重。OpenCV 知道如何加载它。所以，让我们看看演示应用程序的样子。
- en: Putting it all together
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: In order to run our app, we will need to execute the main function routine (`chapter8.py`)
    that loads the pre-trained cascade classifier and the pre-trained MLP and applies
    them to each frame of the webcam live stream.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的应用程序，我们需要执行主函数例程（`chapter8.py`），该例程加载预训练的级联分类器和预训练的MLP，并将它们应用于网络摄像头的实时流中的每一帧。
- en: 'However, this time, instead of collecting more training samples, we will start
    the program with a different option, shown here:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次，我们不会收集更多的训练样本，而是以不同的选项启动程序，如下所示：
- en: '[PRE67]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This will start the application with a new `FacialExpressionRecognizerLayout`
    layout, which is a subclass of `BasicLayout` without any extra UI elements. Let''s
    go over the constructor first, as follows:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动应用程序，并使用新的 `FacialExpressionRecognizerLayout` 布局，它是 `BasicLayout` 的子类，没有任何额外的UI元素。让我们首先看看构造函数，如下所示：
- en: 'It reads and initializes all the data that was stored by the training script,
    like this:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它读取并初始化由训练脚本存储的所有数据，如下所示：
- en: '[PRE68]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'It loads the pre-trained classifier using `ANN_MLP_load`, as follows:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `ANN_MLP_load` 加载预训练的分类器，如下所示：
- en: '[PRE69]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'It loads the Python variables that we want to pass from training, like this:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它加载我们想要从训练中传递的 Python 变量，如下所示：
- en: '[PRE70]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'It initializes a `FaceDetector` class to be able to do face recognition, as
    follows:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它初始化一个 `FaceDetector` 类，以便能够进行人脸识别，如下所示：
- en: '[PRE71]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once we have all the pieces from training in place, we can go ahead and put
    some code in place to add labels to faces. In this demo, we don''t have any use
    of extra buttons; so, the only method we have to implement is `process_frame`,
    which first tries to detect a face in the live feed and place a label on top of
    it, We will proceed as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从训练中获得了所有这些部件，我们就可以继续编写代码来为面部添加标签。在这个演示中，我们没有使用任何额外的按钮；因此，我们唯一要实现的方法是`process_frame`，它首先尝试在实时流中检测人脸并在其上方放置标签，我们将按以下步骤进行：
- en: 'First, we try to detect if there is a face present in the video stream or not,
    by running the following code:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们尝试通过运行以下代码来检测视频流中是否存在人脸：
- en: '[PRE72]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'If there is no face, we do nothing and `return` an unprocessed `frame`, as
    follows:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有人脸，我们不做任何操作，并返回一个未处理的`frame`，如下所示：
- en: '[PRE73]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Once there is a face, we try to align the face (the same as when collecting
    the training data), like this:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦检测到人脸，我们尝试将人脸对齐（与收集训练数据时相同），如下所示：
- en: '[PRE74]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'If we are successful, we featurize the head and predict the label using the
    MLP, as follows:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们成功，我们将使用MLP对头部进行特征化并预测标签，如下所示：
- en: '[PRE75]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, we put the text with the label on top of the face''s bounding box
    and show that to the user, by running the following code:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过运行以下代码将带有标签的文本放在人脸的边界框上，并将其显示给用户：
- en: '[PRE76]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'In the previous method, we made use of `featurize_head`, which is a convenient
    function to call `_pca_featurize`, as shown in the following code block:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方法中，我们使用了`featurize_head`，这是一个方便的函数来调用`_pca_featurize`，如下面的代码块所示：
- en: '[PRE77]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The end result looks like the following:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果如下所示：
- en: '![](img/3ded5d77-8f8a-4c9b-a8b6-9c27996f24e7.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3ded5d77-8f8a-4c9b-a8b6-9c27996f24e7.png)'
- en: Although the classifier has only been trained on (roughly) 100 training samples,
    it reliably detects my various facial expressions in every frame of the live stream,
    no matter how distorted my face seemed to be at the given moment.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分类器只训练了大约100个训练样本，但它能够可靠地检测直播流中每一帧的我的各种面部表情，无论我的脸在那一刻看起来多么扭曲。
- en: This is a good indication that the neural network that was trained previously
    is neither underfitting nor overfitting the data since it is capable of predicting
    the correct class labels, even for new data samples.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明之前训练的神经网络既没有欠拟合也没有过拟合数据，因为它能够预测正确的类别标签，即使是对于新的数据样本。
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter of the book has really rounded up our experience and made us combine
    a variety of our skills to arrive at an end-to-end app that consists of both object
    detection and object recognition. We became familiar with a range of pre-trained
    cascade classifiers that OpenCV has to offer, and we collected and created our
    very own training dataset, learned about MLPs, and trained them to recognize emotional
    expressions in faces (well, at least my face).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的这一章真正总结了我们的经验，并使我们能够将各种技能结合起来，最终开发出一个端到端的应用程序，该应用程序包括物体检测和物体识别。我们熟悉了OpenCV提供的各种预训练的级联分类器，我们收集并创建了我们的训练数据集，学习了MLP，并将它们训练来识别面部表情（至少是我的面部表情）。
- en: The classifier undoubtedly benefited from the fact that I was the only subject
    in the dataset, but, with all the knowledge and experience that you have gathered
    throughout this book, it is now time to overcome these limitations.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器无疑受益于我是数据集中唯一的主题这一事实，但，凭借你在整本书中学到的所有知识和经验，现在是时候克服这些限制了。
- en: After learning the techniques in this chapter, you can start with something
    smaller, and train the classifier on images of you (indoors and outdoors, at night
    and day, during summer and winter). Or, you can take a look at **Kaggle's Facial
    Expression Recognition Challenge**, which has a lot of nice data you could play
    with.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习本章的技术之后，你可以从一些较小的东西开始，并在你（室内和室外，白天和夜晚，夏天和冬天）的图像上训练分类器。或者，你可以看看**Kaggle的面部表情识别挑战**，那里有很多你可以玩的数据。
- en: If you are into machine learning, you might already know that there is a variety
    of accessible libraries out there, such as **Pylearn**, **scikit-learn**, and
    **PyTorch**.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器学习感兴趣，你可能已经知道有许多可用的库，例如**Pylearn**、**scikit-learn**和**PyTorch**。
- en: In the next chapter, you will start your deep learning journey and will put
    your hands on deep CNNs. You will get acquainted with multiple deep learning concepts,
    and you will create and train your own classification and localization networks
    using transfer learning. To accomplish this, you will use one of the pre-trained
    classification CNNs available in **Keras.** Throughout the chapter, you will extensively
    use **Keras** and **TensorFlow, **which are a couple of the most popular deep
    learning frameworks at the time of writing.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将开始你的深度学习之旅，并亲手操作深度卷积神经网络。你将熟悉多个深度学习概念，并使用迁移学习创建和训练自己的分类和定位网络。为了完成这项任务，你将使用**Keras**中可用的预训练分类卷积神经网络。在整个章节中，你将广泛使用**Keras**和**TensorFlow**，它们是当时最受欢迎的深度学习框架之一。
- en: Further reading
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '**Kaggle''s Facial Expression Recognition Challenge**: [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge).'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kaggle 面部表情识别挑战**: [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge).'
- en: '**Pylearn**: [https://github.com/lisa-lab/pylearn2](https://github.com/lisa-lab/pylearn2).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pylearn**: [https://github.com/lisa-lab/pylearn2](https://github.com/lisa-lab/pylearn2).'
- en: '**scikit-learn**: [http://scikit-learn.org](http://scikit-learn.org).'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**: [http://scikit-learn.org](http://scikit-learn.org).'
- en: '**pycaffe**: [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org).'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pycaffe**: [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org).'
- en: '**Theano**: [http://deeplearning.net/software/theano](http://deeplearning.net/software/theano).'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Theano**: [http://deeplearning.net/software/theano](http://deeplearning.net/software/theano).'
- en: '**Torch**: [http://torch.ch](http://torch.ch).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Torch**: [http://torch.ch](http://torch.ch).'
- en: '**UC Irvine Machine Learning Repository**: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UC Irvine 机器学习库**: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
- en: Attributions
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贡献
- en: '`Lenna.png`—Image Lenna is available at [http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896) by
    Conor Lawless under attribution CC 2.0 Generic.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '`Lenna.png`—图像 Lenna 可在 [http://www.flickr.com/photos/15489034@N00/3388463896](http://www.flickr.com/photos/15489034@N00/3388463896)
    由 Conor Lawless 提供，授权为 CC 2.0 Generic。'
