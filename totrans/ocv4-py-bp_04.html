<html><head></head><body>
        

                            
                    <h1 class="header-title">3D Scene Reconstruction Using Structure from Motion</h1>
                
            
            
                
<p>In the previous chapter, you learned how to detect and track an object of interest in the video stream of a webcam, even if the object is viewed from different angles or distances, or under partial occlusion. Here, we will take the tracking of interesting features a step further and see what we can learn about the entire visual scene by studying similarities between image frames.</p>
<p>The goal of this chapter is to study how to reconstruct a scene in 3D by inferring the geometrical features of the scene from camera motion. This technique is sometimes referred to as <strong>structure from motion</strong>. By looking at the same scene from different angles, we will be able to infer the real-world 3D coordinates of different features in the scene. This process is known as <strong>triangulation</strong>, which allows us to <strong>reconstruct</strong> the scene as a <strong>3D point cloud</strong>.</p>
<p>If we take two pictures of the same scene from different angles, we can use <strong>feature matching</strong> or <strong>optic flow</strong> to estimate any translational and rotational movement that the camera underwent between taking the two pictures. However, in order for this to work, we will first have to calibrate our camera.</p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul>
<li>Learning about camera calibration</li>
<li>Setting up the app</li>
<li>Estimating the camera motion from a pair of images</li>
<li>Reconstructing the scene</li>
<li>Understanding 3D point cloud visualization</li>
<li>Learning about structure from motion</li>
</ul>
<p>Once you complete the app, you will understand the classical approaches that are used to make a 3D reconstruction of a scene or object given several images taken from different view points. You will be able to apply these approaches in your own apps related to constructing 3D models from camera images or videos.  </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting started</h1>
                
            
            
                
<p class="mce-root">This chapter has been tested with <strong>OpenCV 4.1.0</strong> and <strong>wxPython 4.0.4</strong> (<a href="http://www.wxpython.org/download.php">http://www.wxpython.org/download.php</a>). It also requires NumPy (<a href="http://www.numpy.org">http://www.numpy.org</a>) and Matplotlib (<a href="http://www.matplotlib.org/downloads.html">http://www.matplotlib.org/downloads.html</a>).</p>
<p class="mce-root">Note that you may have to obtain the so-called <em>extra</em> modules from <a href="https://github.com/Itseez/opencv_contrib">https://github.com/Itseez/opencv_contrib</a> and install OpenCV with the <kbd>OPENCV_EXTRA_MODULES_PATH</kbd> variable set in order to install <strong>scale-invariant feature transform</strong> (<strong>SIFT</strong>). Also, note that you may have to obtain a license to use SIFT in commercial applications.</p>
<p>You can find the code that we present in this chapter in our GitHub repository, <a href="https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4">https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition/tree/master/chapter4</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Planning the app</h1>
                
            
            
                
<p>The final app will extract and visualize structure from motion on a pair of images. We will assume that these two images have been taken with the same camera, whose internal camera parameters we know. If these parameters are not known, they need to be estimated first in a camera calibration process.</p>
<p>The final app will then consist of the following modules and scripts:</p>
<ul>
<li><kbd>chapter4.main</kbd>: This is the main function routine for starting the application.</li>
<li><kbd>scene3D.SceneReconstruction3D</kbd>: This is a class that contains a range of functionalities for calculating and visualizing structure from motion. It includes the following public methods:
<ul>
<li><kbd>__init__</kbd>: This constructor will accept the intrinsic camera matrix and the distortion coefficients.</li>
<li><kbd>load_image_pair</kbd>: This is a method used to load two images that have been taken with the camera described earlier from the file.</li>
<li><kbd>plot_optic_flow</kbd>: This is a method used to visualize the optic flow between the two image frames.</li>
<li><kbd>draw_epipolar_lines</kbd>: This method is used to draw the epipolar lines of the two images.</li>
<li><kbd>plot_rectified_images</kbd>: This method is used to plot a rectified version of the two images.</li>
<li><kbd>plot_point_cloud</kbd>: This is a method used to visualize the recovered real-world coordinates of the scene as a 3D point cloud. In order to arrive at a 3D point cloud, we will need to exploit epipolar geometry. However, epipolar geometry assumes the pinhole camera model, which no real camera follows.</li>
</ul>
</li>
</ul>
<p>The complete procedure of the app involves the following steps:</p>
<ol>
<li><strong>Camera calibration</strong>: We will use a chessboard pattern to extract the intrinsic camera matrix as well as the distortion coefficients, which are important for performing the scene reconstruction.</li>
<li><strong>Feature matching</strong>: We will match points in two 2D images of the same visual scene, either via SIFT or via optic flow, as seen in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/ba0e6aff-282a-4ab8-a0bd-ee62bebbb9b3.png" style="width:32.50em;height:22.25em;"/></p>
<ol start="3">
<li><strong>Image rectification</strong>: By estimating the camera motion from a pair of images, we will extract the <strong>essential matrix</strong> and rectify the images, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/7f66eaa1-9a22-4830-8963-46351a36fb56.png" style="width:57.17em;height:30.42em;"/></p>
<ol start="4">
<li><strong>Triangulation</strong>: We will reconstruct the 3D real-world coordinates of the image points by making use of constraints from <strong>epipolar geometry</strong>.</li>
<li class="CDPAlignLeft CDPAlign"><strong>3D point cloud visualization</strong>: Finally, we will visualize the recovered 3D structure of the scene using scatterplots in Matplotlib, which is most compelling when studied using the Pan axes button from pyplot. This button lets you rotate and scale the point cloud in all three dimensions. In the following screenshot, the color corresponds to the depth of a point in the scene:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/e3be9c94-8fb3-42f8-9285-683cdd36ea99.png" style="width:28.67em;height:24.25em;"/></p>
<p>First, we need to rectify our images to make them look as if they have come from a pinhole camera. For that, we need to estimate the parameters of the camera, which leads us to the field of camera calibration.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about camera calibration</h1>
                
            
            
                
<p>So far, we have worked with whatever image came straight out of our webcam, without questioning the way in which it was taken. However, every camera lens has unique parameters, such as focal length, principal point, and lens distortion.</p>
<p>What happens behind the covers when a camera takes a picture is this: light falls through a lens, followed by an aperture, before falling on the surface of a light sensor. This process can be approximated with the pinhole camera model. The process of estimating the parameters of a real-world lens such that it would fit the pinhole camera model is called camera calibration (or <strong>camera resectioning</strong>, and it should not be confused with <em>photometric</em> camera calibration). So, let's start by learning about the pinhole camera model in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the pinhole camera model</h1>
                
            
            
                
<p>The <strong>pinhole camera model </strong>is a simplification of a real camera in which there is no lens and the camera aperture is approximated by a single point (the pinhole). The formulas described here also hold exactly for a camera with a thin lens as well as describing the main parameters of any usual camera.</p>
<p>When viewing a real-world 3D scene (such as a tree), light rays pass through the point-sized aperture, and fall on a 2D image plane inside the camera, as seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/3ea08414-847b-4aac-aef3-b8ed24b114d7.png" style="width:32.33em;height:21.58em;"/></p>
<p>In this model, a 3D point with coordinates (<em>X</em>, <em>Y</em>, <em>Z</em>) is mapped to a 2D point with coordinates (<em>x</em>, <em>y</em>) that lies on the image plane. Note that this leads to the tree appearing upside down on the image plane.</p>
<p>The line that is perpendicular to the image plane and passes through the pinhole is called the <strong>principal ray</strong>, and its length is called the <strong>focal length</strong>. The focal length is a part of the internal camera parameters, as it may vary depending on the camera being used. In a simple camera with a lens, the <strong>pinhole</strong> is replaced with a lens and the focal plane is placed at the focal length of the lens in order to avoid blurring as much as possible.</p>
<p>Hartley and Zisserman found a mathematical formula to describe how a 2D point with coordinates (<em>x</em>, <em>y</em>) can be inferred from a 3D point with coordinates<em> </em>(<em>X, Y, Z</em>) and the camera's intrinsic parameters, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/ad7b9889-55c5-4862-a51f-cfb1ec9bcf7b.png" style="width:14.58em;height:4.50em;"/></p>
<p>The 3 x 3 matrix in the preceding formula is the <strong>intrinsic camera matrix</strong>—a matrix that compactly describes all internal camera parameters. The matrix comprises focal lengths (<em>f<sub>x </sub></em>and <em>f<sub>y</sub></em>) and optical centers <em>c<sub>x</sub></em> and <em>c<sub>y</sub></em>, which in the case of digital imaging are simply expressed in pixel coordinates. As mentioned earlier, the focal length is the distance between the pinhole and the image plane.</p>
<p>A pinhole camera has only one focal length, in which case, <em>f<sub>x</sub></em> = <em>f<sub>x </sub></em>= <em><sub> </sub></em><em>f<sub>x </sub></em>. However, in real cameras, these two values might differ, for example, due to imperfections of lenses, imperfections of the focal plane (which is represented by a digital camera sensor), or imperfections of assembly. The difference can be also intentional for some purpose, which can be achieved by simply involving a lens that has different curvature in different directions. The point at which the principal ray intersects the image plane is called the <strong>principal point</strong>, and its relative position on the image plane is captured by the optical center (or principal point offset).</p>
<p>In addition, a camera might be subject to radial or tangential distortion, leading to a <strong>fish-eye effect</strong>. This is because of hardware imperfections and lens misalignments. These distortions can be described with a list of the <strong>distortion coefficients</strong>. Sometimes, radial distortions are actually a desired artistic effect. At other times, they need to be corrected.</p>
<p>For more information on the pinhole camera model, there are many good tutorials out there on the web, such as <a href="http://ksimek.github.io/2013/08/13/intrinsic">http://ksimek.github.io/2013/08/13/intrinsic</a>.</p>
<p>Because these parameters are specific to the camera hardware (hence the name <em>intrinsic</em>), we need to calculate them only once in the lifetime of a camera. This is called <strong>camera calibration</strong>.</p>
<p>Next, we will cover the parameters of the intrinsic camera.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Estimating the intrinsic camera parameters</h1>
                
            
            
                
<p>In OpenCV, camera calibration is fairly straightforward. The official documentation provides a good overview of the topic and some sample C++ scripts at <a href="http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html">http://docs.opencv.org/doc/tutorials/calib3d/camera_calibration/camera_calibration.html</a>.</p>
<p>For educational purposes, we will develop our own calibration script in Python. We will need to present a special pattern image, with known geometry (chessboard plate or black circles on a white background), to the camera we wish to calibrate.</p>
<p>Because we know the geometry of the pattern image, we can use feature detection to study the properties of the internal camera matrix. For example, if the camera suffers from undesired radial distortion, the different corners of the chessboard pattern will appear distorted in the image and not lie on a rectangular grid. By taking about 10-20 snapshots of the chessboard pattern from different points of view, we can collect enough information to correctly infer the camera matrix and the distortion coefficients.</p>
<p>For this, we will use the <kbd>calibrate.py</kbd> script, which first imports the following modules:</p>
<pre>import cv2<br/>import numpy as np<br/>import wx<br/><br/>from wx_gui import BaseLayout</pre>
<p>Analogous to previous chapters, we will use a simple layout based on <kbd>BaseLayout</kbd> that embeds processing of the webcam video stream.</p>
<p>The <kbd>main</kbd> function of the script will generate the GUI and execute the <kbd>main</kbd> loop of the app:</p>
<pre> def main():</pre>
<p class="mce-root">The latter is accomplished with the following steps in the body of the function:</p>
<ol>
<li>First, connect to the camera and set standard VGA resolution:</li>
</ol>
<pre style="padding-left: 60px">capture = cv2.VideoCapture(0)<br/>assert capture.isOpened(), "Can not connect to camera"<br/>capture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)<br/>capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)</pre>
<ol start="2">
<li>Similarly to the previous chapters, create a <kbd>wx</kbd> application and the <kbd>layout</kbd> class, which we will compose later in this section:</li>
</ol>
<pre style="padding-left: 60px">app = wx.App()<br/>layout = CameraCalibration(capture, title='Camera Calibration', fps=2)</pre>
<ol start="3">
<li>Show the GUI and execute the <kbd>MainLoop</kbd> of the <kbd>app</kbd>:</li>
</ol>
<pre style="padding-left: 60px">layout.Show(True)<br/>app.MainLoop()</pre>
<p>In the next section, we'll prepare the camera calibration GUI, which we used in the <kbd>main</kbd> function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Defining the camera calibration GUI</h1>
                
            
            
                
<p>The GUI is a customized version of the generic <kbd>BaseLayout</kbd>:</p>
<pre>class CameraCalibration(BaseLayout): </pre>
<p>The layout consists of only the current camera frame and a single button below it. This button allows us to start the calibration process:</p>
<pre>    def augment_layout(self):<br/>        pnl = wx.Panel(self, -1)<br/>        self.button_calibrate = wx.Button(pnl, label='Calibrate Camera')<br/>        self.Bind(wx.EVT_BUTTON, self._on_button_calibrate)<br/>        hbox = wx.BoxSizer(wx.HORIZONTAL)<br/>        hbox.Add(self.button_calibrate)<br/>        pnl.SetSizer(hbox)</pre>
<p>For these changes to take effect, <kbd>pnl</kbd> needs to be added to the list of existing panels:</p>
<pre>self.panels_vertical.Add(pnl, flag=wx.EXPAND | wx.BOTTOM | wx.TOP,<br/>                                 border=1)</pre>
<p>The rest of the visualization pipeline is handled by the <kbd>BaseLayout</kbd> class. We only need to make sure that we initialize the required variables and provide <kbd>process_frame</kbd> methods.</p>
<p>Now that we have defined a GUI for camera calibration, let's initialize a camera calibration algorithm in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Initializing the algorithm</h1>
                
            
            
                
<p>In order to perform the calibration process, we need to do some bookkeeping. We will do that by following the next steps:</p>
<ol>
<li>For now, let's focus on a single 10 x 7 chessboard. The algorithm will detect all the <kbd>9</kbd> x <kbd>6</kbd> inner corners of the chessboard (referred to as <em>object points</em>) and store the detected image points of these corners in a list. So, let's first initialize the <kbd>chessboard_size</kbd> to the number of inner corners:</li>
</ol>
<pre style="padding-left: 60px">self.chessboard_size = (9, 6) </pre>
<ol start="2">
<li>Next, we need to enumerate all the object points and assign them object point coordinates so that the first point has coordinates (0,0), the second one (top row) has coordinates (1,0), and the last one has coordinates (8,5):</li>
</ol>
<pre>        # prepare object points<br/>        self.objp = np.zeros((np.prod(self.chessboard_size), 3),<br/>                             dtype=np.float32)<br/>        self.objp[:, :2] = np.mgrid[0:self.chessboard_size[0],<br/>                                    0:self.chessboard_size[1]]<br/>                                    .T.reshape(-1, 2)</pre>
<ol start="3">
<li>We also need to keep track of whether we are currently recording the object and image points or not. We will initiate this process once the user clicks on the <kbd>self.button_calibrate</kbd> button. After that, the algorithm will try to detect a chessboard in all subsequent frames until <kbd>self.record_min_num_frames</kbd> chessboards have been detected:</li>
</ol>
<pre>        # prepare recording<br/>        self.recording = False<br/>        self.record_min_num_frames = 15<br/>        self._reset_recording()</pre>
<ol start="4">
<li>Whenever the <kbd>self.button_calibrate</kbd> button is clicked on, we reset all the bookkeeping variables, disable the button, and start recording:</li>
</ol>
<pre style="padding-left: 30px">    def _on_button_calibrate(self, event):<br/>        """Enable recording mode upon pushing the button"""<br/>        self.button_calibrate.Disable()<br/>        self.recording = True<br/>        self._reset_recording()</pre>
<p style="padding-left: 60px">Resetting the bookkeeping variables involves clearing the lists of recorded object and image points (<kbd>self.obj_points</kbd> and <kbd>self.img_points</kbd>) as well as resetting the number of detected chessboards (<kbd>self.recordCnt</kbd>) to <kbd>0</kbd>:</p>
<pre style="padding-left: 60px">def _reset_recording(self): 
    self.record_cnt = 0 
    self.obj_points = [] 
    self.img_points = [] </pre>
<p class="mce-root">In the next section, we'll collect the image and object points.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Collecting image and object points</h1>
                
            
            
                
<p>The <kbd>process_frame</kbd> method is responsible for doing the hard work of the calibration technique, and we will collect images and object points by using the following steps:</p>
<ol>
<li>After the <kbd>self.button_calibrate</kbd> button has been clicked on, this method starts collecting data until a total of <kbd>self.record_min_num_frames</kbd> chessboards are detected:</li>
</ol>
<pre style="padding-left: 60px">    def process_frame(self, frame):<br/>        """Processes each frame"""<br/>        # if we are not recording, just display the frame<br/>        if not self.recording:<br/>            return frame<br/><br/>        # else we're recording<br/>        img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<br/>                   .astype(np.uint8)<br/>        if self.record_cnt &lt; self.record_min_num_frames:<br/>            ret, corners = cv2.findChessboardCorners(<br/>                               img_gray,<br/>                               self.chessboard_size,<br/>                               None)</pre>
<p style="padding-left: 60px">The <kbd>cv2.findChessboardCorners</kbd> function will parse a grayscale image (<kbd>img_gray</kbd>) to find a chessboard of size <kbd>self.chessboard_size</kbd>. If the image indeed contains a chessboard, the function will return <kbd>true</kbd> (<kbd>ret</kbd>) as well as a list of chessboard corners (<kbd>corners</kbd>).</p>
<ol start="2">
<li>Then, drawing the chessboard is straightforward:</li>
</ol>
<pre style="padding-left: 60px">            if ret:<br/>                print(f"{self.record_min_num_frames - self.record_cnt} chessboards remain")<br/>                cv2.drawChessboardCorners(frame, self.chessboard_size, corners, ret)</pre>
<ol start="3">
<li>The result looks like this (drawing the chessboard corners in color for the effect):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="img/4af66364-a9e5-48f4-a3f2-1b2d5d9db3c3.png" style="width:27.42em;height:17.67em;"/></p>
<p>We could now simply store the list of detected corners and move on to the next frame. However, in order to make the calibration as accurate as possible, OpenCV provides a function to refine the corner point measurement:</p>
<pre>criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,<br/>            30, 0.01)<br/>cv2.cornerSubPix(img_gray, corners, (9, 9), (-1, -1), criteria)</pre>
<p>This will refine the coordinates of the detected corners to subpixel precision. Now we are ready to append the object and image points to the list and advance the frame counter:</p>
<pre>self.obj_points.append(self.objp) 
self.img_points.append(corners) 
self.record_cnt += 1 </pre>
<p>In the next section, let's learn how to find the camera matrix, which will be required to accomplish an appropriate 3D reconstruction.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding the camera matrix</h1>
                
            
            
                
<p>Once we have collected enough data (that is, once <kbd>self.record_cnt</kbd> reaches the value of <kbd>self.record_min_num_frames</kbd>), the algorithm is ready to perform the calibration. This process can be performed with a single call to <kbd>cv2.calibrateCamera</kbd>:</p>
<pre>else:<br/>    print("Calibrating...")<br/>    ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(self.obj_points,<br/>                                                     self.img_points,<br/>                                                     (self.imgHeight,<br/>                                                      self.imgWidth),<br/>                                                     None, None)</pre>
<p>The function returns <kbd>true</kbd> on success (<kbd>ret</kbd>), the intrinsic camera matrix (<kbd>K</kbd>), the distortion coefficients (<kbd>dist</kbd>), as well as two rotation and translation matrices (<kbd>rvecs</kbd> and <kbd>tvecs</kbd>). For now, we are mainly interested in the camera matrix and the distortion coefficients, because these will allow us to compensate for any imperfections of the internal camera hardware.</p>
<p>We will simply <kbd>print</kbd> them on the console for easy inspection:</p>
<pre>print("K=", K)<br/>print("dist=", dist)</pre>
<p>For example, the calibration of my laptop's webcam recovered the following values:</p>
<pre>K= [[ 3.36696445e+03 0.00000000e+00 2.99109943e+02] 
    [ 0.00000000e+00 3.29683922e+03 2.69436829e+02] 
    [ 0.00000000e+00 0.00000000e+00 1.00000000e+00]] 
dist= [[ 9.87991355e-01 -3.18446968e+02 9.56790602e-02 <br/>         -3.42530800e-02 4.87489304e+03]]</pre>
<p>This tells us that the focal lengths of my webcam are <kbd>fx=3366.9644</kbd> pixels and <kbd>fy=3296.8392</kbd> pixels, with the optical center at <kbd>cx=299.1099</kbd> pixels and <kbd>cy=269.4368</kbd> pixels.</p>
<p>A good idea might be to double-check the accuracy of the calibration process. This can be done by projecting the object points onto the image using the recovered camera parameters so that we can compare them with the list of image points we collected with the <kbd>cv2.findChessboardCorners</kbd> function. If the two points are roughly the same, we know that the calibration was successful. Even better, we can calculate the <kbd>mean error</kbd> of the reconstruction by projecting every object point in the list:</p>
<pre>mean_error = 0 
for obj_point, rvec, tvec, img_point in zip(<br/>        self.obj_points, rvecs, tvecs, self.img_points):<br/>    img_points2, _ = cv2.projectPoints(<br/>        obj_point, rvec, tvec, K, dist)<br/>    error = cv2.norm(img_point, img_points2,<br/>                     cv2.NORM_L2) / len(img_points2)<br/>    mean_error += error<br/><br/>print("mean error=", mean_error)</pre>
<p>Performing this check on my laptop's webcam resulted in a mean error of 0.95 pixels, which is fairly close to 0.</p>
<p>With the internal camera parameters recovered, we can now set out to take beautiful, undistorted pictures of the world, possibly from different viewpoints so that we can extract some structure from motion. First, let's see how to set up our app.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the app</h1>
                
            
            
                
<p>Going forward, we will be using a famous open source dataset called <kbd>fountain-P11</kbd>. It depicts a Swiss fountain viewed from various angles:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/ac6bebd0-35f9-4265-97b3-6621dd489e1c.png" style="width:40.17em;height:26.75em;"/></p>
<p>The dataset consists of 11 high-resolution images and can be downloaded from <a href="https://icwww.epfl.ch/multiview/denseMVS.html">https://icwww.epfl.ch/multiview/denseMVS.html</a>. Had we taken the pictures ourselves, we would have had to go through the entire camera calibration procedure to recover the intrinsic camera matrix and the distortion coefficients. Luckily, these parameters are known for the camera that took the fountain dataset, so we can go ahead and hardcode these values in our code.</p>
<p>Let's prepare the <kbd>main</kbd> routine function in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding the main routine function</h1>
                
            
            
                
<p>Our <kbd>main</kbd> routine function will consist of creating and interacting with an instance of the <kbd>SceneReconstruction3D</kbd> class. This code can be found in the <kbd>chapter4.py</kbd> file. The dependencies of the module are <kbd>numpy</kbd> and the class itself, which are imported as follows:</p>
<pre>import numpy as np 
 
from scene3D import SceneReconstruction3D<br/><br/></pre>
<p>Next, we define the <kbd>main</kbd> function:</p>
<pre>def main():</pre>
<p>The function consists of the following steps:</p>
<ol>
<li>We define the intrinsic camera matrix for the camera that took photos of the fountain dataset (<kbd>K</kbd>) and set distortion coefficients (<kbd>d</kbd>):</li>
</ol>
<pre style="padding-left: 60px">K = np.array([[2759.48 / 4, 0, 1520.69 / 4, 0, 2764.16 / 4,<br/>               1006.81 / 4, 0, 0, 1]]).reshape(3, 3)<br/>d = np.array([0.0, 0.0, 0.0, 0.0, 0.0]).reshape(1, 5) </pre>
<p style="padding-left: 60px">According to the photographer, these images are already distortion-free, so we have set all the distortion coefficients to 0.</p>
<p>Note that if you want to run the code presented in this chapter on a dataset other than <kbd>fountain-P11</kbd>, you will have to adjust the intrinsic camera matrix and the distortion coefficients.</p>
<ol start="2">
<li>Next, we create an instance of the  <kbd>SceneReconstruction3D</kbd> class and load a pair of images, which we would like to apply to our structure-from-motion techniques. The dataset is downloaded into a subdirectory called <kbd>fountain_dense</kbd>:</li>
</ol>
<pre style="padding-left: 60px">scene = SceneReconstruction3D(K, d) 
scene.load_image_pair("fountain_dense/0004.png", <br/>     "fountain_dense/0005.png")</pre>
<ol start="3">
<li>Now we are ready to call methods from the class that perform various computations:</li>
</ol>
<pre style="padding-left: 60px">scene.plot_rectified_images()<br/>scene.plot_optic_flow()<br/>scene.plot_point_cloud()</pre>
<p style="padding-left: 60px">We will implement these methods throughout the rest of the chapter, and they will be explained in detail in the upcoming sections. </p>
<p>So now that we have prepared the main script of the app, let's start implementing the <kbd>SceneReconstruction3D</kbd> class, which does all the heavy lifting and incorporates the computations for 3D reconstruction.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementing the SceneReconstruction3D class</h1>
                
            
            
                
<p>All of the relevant 3D scene reconstruction code for this chapter can be found as part of the <kbd>SceneReconstruction3D</kbd> class in the <kbd>scene3D</kbd> module. Upon instantiation, the class stores the intrinsic camera parameters to be used in all subsequent calculations:</p>
<pre>import cv2 
import numpy as np 
import sys 
 
from mpl_toolkits.mplot3d import Axes3D 
import matplotlib.pyplot as plt 
from matplotlib import cm
 
class SceneReconstruction3D: 
    def __init__(self, K, dist): 
        self.K = K 
        self.K_inv = np.linalg.inv(K) 
        self.d = dist </pre>
<p>Then, we need to load a pair of images on which to operate.</p>
<p>In order to do it, first, we create a static method that will load an image and convert it to an RGB format if it is grayscale, as other methods expect a three-channel image. In the case of the fountain sequence, all images are of a relatively high resolution. If an optional <kbd>downscale</kbd> flag is set, the method will downscale the image to a width of roughly <kbd>600</kbd> pixels:</p>
<pre>    @staticmethod<br/>    def load_image(<br/>            img_path: str,<br/>            use_pyr_down: bool,<br/>            target_width: int = 600) -&gt; np.ndarray:<br/><br/>        img = cv2.imread(img_path, cv2.CV_8UC3)<br/>        # make sure image is valid<br/>        assert img is not None, f"Image {img_path} could not be loaded."<br/>        if len(img.shape) == 2:<br/>            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)<br/> <br/>        while use_pyr_down and img.shape[1] &gt; 2 * target_width:<br/>            img = cv2.pyrDown(img)<br/>        return img</pre>
<p>Next, we create a method that loads a pair of images and compensates them for the radial and tangential lens distortions using the distortion coefficients specified earlier (if there are any):</p>
<pre>    def load_image_pair(<br/>            self,<br/>            img_path1: str,<br/>            img_path2: str,<br/>            use_pyr_down: bool = True) -&gt; None:<br/><br/>        self.img1, self.img2 = [cv2.undistort(self.load_image(path, <br/>                                                              use_pyr_down), <br/>                                              self.K, self.d)<br/>            for path in (img_path1,img_path2)]</pre>
<p>Finally, we are ready to move on to the heart of the project—estimating the camera motion and reconstructing the scene!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Estimating the camera motion from a pair of images</h1>
                
            
            
                
<p>Now that we have loaded two images (<kbd>self.img1</kbd> and <kbd>self.img2</kbd>) of the same scene, such as two examples from the fountain dataset, we find ourselves in a similar situation as in the previous chapter. We are given two images that supposedly show the same rigid object or static scene but from different viewpoints.</p>
<p>However, this time we want to go a step further—if the only thing that changes between taking the two pictures is the location of the camera, can we infer the relative camera motion by looking at the matching features?</p>
<p>Well, of course we can. Otherwise, this chapter would not make much sense, would it? We will take the location and orientation of the camera in the first image as a given and then find out how much we have to reorient and relocate the camera so that its viewpoint matches that from the second image.</p>
<p>In other words, we need to recover the <strong>essential matrix</strong> of the camera in the second image. An essential matrix is a 4 x 3 matrix that is the concatenation of a 3 x 3 rotation matrix and a 3 x 1 translation matrix. It is often denoted by <em>[R | t]</em>. You can think of it as capturing the position and orientation of the camera in the second image relative to the camera in the first image.</p>
<p>The crucial step in recovering the essential matrix (as well as all other transformations in this chapter) is feature matching. We can either apply the SIFT detector to the two images or calculate the optic flow between the two images. The user may choose their favorite method by specifying a feature extraction mode, which will be implemented by the following private method:</p>
<pre>    def _extract_keypoints(self, feat_mode):<br/>        # extract features<br/>        if feat_mode.lower() == "sift":<br/>            # feature matching via sift and BFMatcher<br/>            self._extract_keypoints_sift()<br/>        elif feat_mode.lower() == "flow":<br/>            # feature matching via optic flow<br/>            self._extract_keypoints_flow()<br/>        else:<br/>            sys.exit(f"Unknown feat_mode {feat_mode}. Use 'SIFT' or <br/>                     'FLOW'")</pre>
<p>Let's learn how to perform point matching using rich feature descriptors in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying point matching with rich feature descriptors</h1>
                
            
            
                
<p>A robust way of extracting important features from an image is by using the SIFT detector. In this chapter, we want to use it for two images, <kbd>self.img1</kbd> and <kbd>self.img2</kbd>:</p>
<pre>    def _extract_keypoints_sift(self):<br/>        # extract keypoints and descriptors from both images<br/>        detector = cv2.xfeatures2d.SIFT_create()<br/>        first_key_points, first_desc = detector.detectAndCompute(self.img1,<br/>                                                                 None)<br/>        second_key_points, second_desc = detector.detectAndCompute(self.img2,<br/>                                                                   None)</pre>
<p>For feature matching, we will use a <kbd>BruteForce</kbd> matcher, so that other matchers (such as <strong>FLANN</strong>) can work as well:</p>
<pre>        matcher = cv2.BFMatcher(cv2.NORM_L1, True)<br/>        matches = matcher.match(first_desc, second_desc)</pre>
<p>For each of the <kbd>matches</kbd>, we need to recover the corresponding image coordinates. These are maintained in the <kbd>self.match_pts1</kbd> and <kbd>self.match_pts2</kbd> lists:</p>
<pre>        # generate lists of point correspondences<br/>        self.match_pts1 = np.array(<br/>            [first_key_points[match.queryIdx].pt for match in matches])<br/>        self.match_pts2 = np.array(<br/>            [second_key_points[match.trainIdx].pt for match in matches])</pre>
<p>The following screenshot shows an example of the feature matcher applied to two arbitrary frames of the fountain sequence:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/a6e6d350-9f24-4319-b2e6-3572e1a177c7.png" style="width:48.08em;height:16.00em;"/></p>
<p>In the next section, we'll learn about point matching using optic flow.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using point matching with optic flow</h1>
                
            
            
                
<p>An alternative to using rich features is using optic flow. Optic flow is the process of estimating motion between two consecutive image frames by calculating a displacement vector. A displacement vector can be calculated for every pixel in the image (dense) or only for selected points (sparse).</p>
<p>One of the most commonly used techniques for calculating dense optic flow is the Lukas-Kanade method. It can be implemented in OpenCV with a single line of code, by using the <kbd>cv2.calcOpticalFlowPyrLK</kbd> function.</p>
<p>But before that, we need to select some points in the image that are worth tracking. Again, this is a question of feature selection. If we are interested in getting an exact result for only a few highly salient image points, we can use Shi-Tomasi's <kbd>cv2.goodFeaturesToTrack</kbd> function. This function might recover features like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c7cca296-58d7-40ca-a677-03e7e3168037.png" style="width:31.67em;height:21.08em;"/></p>
<p>However, in order to infer structure from motion, we might need many more features and not just the most salient Harris corners. An alternative would be to detect the <strong>Features from Accelerated Segment Test</strong> (<strong>FAST</strong>) features:</p>
<pre>def _extract_keypoints_flow(self): 
    fast = cv2.FastFeatureDetector() 
    first_key_points = fast.detect(self.img1, None) </pre>
<p>We can then calculate the optic flow for these features. In other words, we want to find the points in the second image that most likely correspond to the <kbd>first_key_points</kbd> from the first image. For this, we need to convert the keypoint list into a NumPy array of (<em>x</em>, <em>y</em>) coordinates:</p>
<pre>first_key_list = [i.pt for i in first_key_points] 
first_key_arr = np.array(first_key_list).astype(np.float32) </pre>
<p>Then the optic flow will return a list of corresponding features in the second image (<kbd>second_key_arr</kbd>):</p>
<pre>second_key_arr, status, err = <br/>     cv2.calcOpticalFlowPyrLK(self.img1, self.img2, <br/>         first_key_arr)</pre>
<p>The function also returns a vector of status bits (<kbd>status</kbd>), which indicate whether the flow for a keypoint has been found or not, and a vector of estimated error values (<kbd>err</kbd>). If we were to ignore these two additional vectors, the recovered flow field could look something like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/5765bf9b-d466-4d75-92ef-ff05cfc81bf3.png" style="width:38.50em;height:25.67em;"/></p>
<p>In this image, an arrow is drawn for each keypoint, starting at the location of the keypoint in the first image and pointing to the location of the same keypoint in the second image. By inspecting the flow image, we can see that the camera moved mostly to the right, but there also seems to be a rotational component.</p>
<p>However, some of these arrows are really large, and some of them make no sense. For example, it is very unlikely that a pixel in the bottom-right image corner actually moved all the way to the top of the image. It is much more likely that the flow calculation for this particular keypoint is wrong. Thus, we want to exclude all the keypoints for which the status bit is 0 or the estimated error is larger than a certain value:</p>
<pre>condition = (status == 1) * (err &lt; 5.) 
concat = np.concatenate((condition, condition), axis=1) 
first_match_points = first_key_arr[concat].reshape(-1, 2) 
second_match_points = second_key_arr[concat].reshape(-1, 2) 
 
self.match_pts1 = first_match_points 
self.match_pts2 = second_match_points </pre>
<p>If we draw the flow field again with a limited set of keypoints, the image will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2814c59d-0556-4d6c-a49c-addee3d5b932.png" style="width:43.92em;height:29.25em;"/></p>
<p>The flow field can be drawn with the following public method, which first extracts the keypoints using the preceding code and then draws the actual arrows on the image:</p>
<pre>    def plot_optic_flow(self):<br/>        self._extract_keypoints_flow()<br/><br/>        img = np.copy(self.img1)<br/>        for pt1, pt2 in zip(self.match_pts1, self.match_pts2):<br/>            cv2.arrowedLine(img, tuple(pt1), tuple(pt2),<br/>                     color=(255, 0, 0))<br/><br/>        cv2.imshow("imgFlow", img)<br/>        cv2.waitKey()</pre>
<p>The advantage of using optic flow instead of rich features is that the process is usually faster and can accommodate the matching of many more points, making the reconstruction denser.</p>
<p>The caveat in working with the optic flow is that it works best for consecutive images taken by the same hardware, whereas rich features are mostly agnostic to this.</p>
<p>Let's learn how to find the camera matrices in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Finding the camera matrices</h1>
                
            
            
                
<p>Now that we have obtained the matches between keypoints, we can calculate two important camera matrices—the fundamental matrix and the essential matrix. These matrices will specify the camera motion in terms of rotational and translational components. Obtaining the fundamental matrix (<kbd>self.F</kbd>) is another OpenCV one-liner:</p>
<pre>def _find_fundamental_matrix(self): 
    self.F, self.Fmask = cv2.findFundamentalMat(self.match_pts1, <br/>         self.match_pts2, cv2.FM_RANSAC, 0.1, 0.99)</pre>
<p>The only difference between <kbd>fundamental_matrix</kbd> and <kbd>essential_matrix</kbd> is that the latter operates on rectified images:</p>
<pre>def _find_essential_matrix(self): 
    self.E = self.K.T.dot(self.F).dot(self.K) </pre>
<p>The essential matrix (<kbd>self.E</kbd>) can then be decomposed into rotational and translational components, denoted by <em>[R | t]</em>, using <strong>singular value decomposition</strong> (<strong>SVD</strong>):</p>
<pre>def _find_camera_matrices(self): 
    U, S, Vt = np.linalg.svd(self.E) 
    W = np.array([0.0, -1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, <br/>         1.0]).reshape(3, 3)</pre>
<p>Using the unitary matrices <em>U</em> and <em>V</em> in combination with an additional matrix, <em>W</em>, we can now reconstruct <em>[R | t]</em>. However, it can be shown that this decomposition has four possible solutions and only one of them is the valid second camera matrix. The only thing we can do is check all four possible solutions and find the one that predicts that all the imaged keypoints lie in front of both cameras.</p>
<p>But prior to that, we need to convert the keypoints from 2D image coordinates to homogeneous coordinates. We achieve this by adding a <em>z</em> coordinate, which we set to <kbd>1</kbd>:</p>
<pre>        first_inliers = []<br/>        second_inliers = []<br/>        for pt1,pt2, mask in <br/>        zip(self.match_pts1,self.match_pts2,self.Fmask):<br/>            if mask:<br/>                first_inliers.append(self.K_inv.dot([pt1[0], pt1[1], 1.0]))<br/>                second_inliers.append(self.K_inv.dot([pt2[0], pt2[1], <br/>                                      1.0]))</pre>
<p>We then iterate over the four possible solutions and choose the one that has <kbd>_in_front_of_both_cameras</kbd> returning <kbd>True</kbd>:</p>
<pre>        R = T = None<br/>        for r in (U.dot(W).dot(Vt), U.dot(W.T).dot(Vt)):<br/>            for t in (U[:, 2], -U[:, 2]):<br/>                if self._in_front_of_both_cameras(<br/>                        first_inliers, second_inliers, r, t):<br/>                    R, T = r, t<br/><br/>        assert R is not None, "Camera matricies were never found!"</pre>
<p>Now, we can finally construct the <em>[R | t]</em> matrices of the two cameras. The first camera is simply a canonical camera (no translation and no rotation):</p>
<pre>self.Rt1 = np.hstack((np.eye(3), np.zeros((3, 1)))) </pre>
<p>The second camera matrix consists of <em>[R | t]</em>, recovered earlier:</p>
<pre>self.Rt2 = np.hstack((R, T.reshape(3, 1))) </pre>
<p>The <kbd>__InFrontOfBothCameras</kbd> private method is a helper function that makes sure that every pair of keypoints is mapped to 3D coordinates that make them lie in front of both cameras:</p>
<pre>    def _in_front_of_both_cameras(self, first_points, second_points, rot,<br/>                                  trans):<br/>        """Determines whether point correspondences are in front of both<br/>           images"""<br/>        rot_inv = rot<br/>        for first, second in zip(first_points, second_points):<br/>            first_z = np.dot(rot[0, :] - second[0] * rot[2, :],<br/>                             trans) / np.dot(rot[0, :] - second[0] * rot[2, <br/>                             :],<br/>                                             second)<br/>            first_3d_point = np.array([first[0] * first_z,<br/>                                       second[0] * first_z, first_z])<br/>            second_3d_point = np.dot(rot.T, first_3d_point) - np.dot(rot.T,<br/>                                                                     trans)</pre>
<p>If the function finds any keypoint that is not in front of both cameras, it will return <kbd>False</kbd>:</p>
<pre>if first_3d_point[2] &lt; 0 or second_3d_point[2] &lt; 0: 
    return False 
return True </pre>
<p>So now that we have found the camera matrices, let's rectify an image in the next section, which is a good means to validating whenever the recovered matrices are correct.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Applying image rectification</h1>
                
            
            
                
<p>Maybe the easiest way to make sure that we have recovered the correct camera matrices is to rectify the images. If they are rectified correctly, then a point in the first image and a point in the second image that corresponds to the same 3D world point will lie on the same vertical coordinate.</p>
<p>In a more concrete example, such as in our case, since we know that the cameras are upright, we can verify that horizontal lines in the rectified image correspond to horizontal lines in the 3D scene. Thus, we follow these steps to rectify our image:</p>
<ol>
<li>First, we perform all the steps described in the previous subsections to obtain the <em>[R | t]</em> matrix of the second camera:</li>
</ol>
<pre style="padding-left: 60px">def plot_rectified_images(self, feat_mode="SIFT"): 
    self._extract_keypoints(feat_mode) 
    self._find_fundamental_matrix() 
    self._find_essential_matrix() 
    self._find_camera_matrices_rt() 
 
    R = self.Rt2[:, :3] 
    T = self.Rt2[:, 3] </pre>
<ol start="2">
<li>Then, rectification can be performed with two OpenCV one-liners that remap the image coordinates to the rectified coordinates based on the camera matrix (<kbd>self.K</kbd>), the distortion coefficients (<kbd>self.d</kbd>), the rotational component of the essential matrix (<kbd>R</kbd>), and the translational component of the essential matrix (<kbd>T</kbd>):</li>
</ol>
<pre style="padding-left: 60px">        R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(<br/>            self.K, self.d, self.K, self.d, <br/>            self.img1.shape[:2], R, T, alpha=1.0)<br/>        mapx1, mapy1 = cv2.initUndistortRectifyMap(<br/>            self.K, self.d, R1, self.K, self.img1.shape[:2],<br/>            cv2.CV_32F)<br/>        mapx2, mapy2 = cv2.initUndistortRectifyMap(<br/>            self.K, self.d, R2, self.K,<br/>            self.img2.shape[:2],<br/>            cv2.CV_32F)<br/>        img_rect1 = cv2.remap(self.img1, mapx1, mapy1, <br/>                              cv2.INTER_LINEAR)<br/>        img_rect2 = cv2.remap(self.img2, mapx2, mapy2, <br/>                              cv2.INTER_LINEAR)</pre>
<ol start="3">
<li>To make sure that the rectification is accurate, we plot the two rectified images (<kbd>img_rect1</kbd> and <kbd>img_rect2</kbd>) next to each other:</li>
</ol>
<pre style="padding-left: 60px">        total_size = (max(img_rect1.shape[0], img_rect2.shape[0]),<br/>                      img_rect1.shape[1] + img_rect2.shape[1], 3)<br/>        img = np.zeros(total_size, dtype=np.uint8)<br/>        img[:img_rect1.shape[0], :img_rect1.shape[1]] = img_rect1<br/>        img[:img_rect2.shape[0], img_rect1.shape[1]:] = img_rect2</pre>
<ol start="4">
<li>We also draw horizontal blue lines after every <kbd>25</kbd> pixels, across the side-by-side images, to further help us visually investigate the rectification process:</li>
</ol>
<pre style="padding-left: 60px">        for i in range(20, img.shape[0], 25):<br/>            cv2.line(img, (0, i), (img.shape[1], i), (255, 0, 0))<br/><br/>        cv2.imshow('imgRectified', img)<br/>        cv2.waitKey()</pre>
<p>Now we can easily convince ourselves that the rectification was successful, as shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d2f5e874-9cc4-4721-9797-407769b822ee.png" style="width:41.58em;height:22.25em;"/></p>
<p class="mce-root">Now that we have rectified our image, let's learn how to reconstruct the 3D scene in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reconstructing the scene</h1>
                
            
            
                
<p>Finally, we can reconstruct the 3D scene by making use of a process called <strong>triangulation</strong>. We are able to infer the 3D coordinates of a point because of the way <strong>epipolar geometry</strong> works. By calculating the essential matrix, we get to know more about the geometry of the visual scene than we might think. Because the two cameras depict the same real-world scene, we know that most of the 3D real-world points will be found in both images.</p>
<p>Moreover, we know that the mapping from the 2D image points to the corresponding 3D real-world points will follow the rules of geometry. If we study a sufficiently large number of image points, we can construct, and solve, a (large) system of linear equations to get the ground truth of the real-world coordinates.</p>
<p>Let's return to the Swiss fountain dataset. If we ask two photographers to take a picture of the fountain from different viewpoints at the same time, it is not hard to realize that the first photographer might show up in the picture of the second photographer, and vice versa. The point on the image plane where the other photographer is visible is called the <strong>epipole</strong> or <strong>epipolar point</strong>.</p>
<p>In more technical terms, the epipole is the point on one camera's image plane onto which the center of projection of the other camera projects. It is interesting to note that both the epipoles in their respective image planes, and both the centers of projection, lie on a single 3D line.</p>
<p>By looking at the lines between the epipoles and image points, we can limit the number of possible 3D coordinates of the image points. In fact, if the projection point is known, then the epipolar line (which is the line between the image point and the epipole) is known, and, in turn, the same point projected onto the second image must lie on that particular epipolar line. <em>Confusing?</em> I thought so.</p>
<p>Let's just look at these images:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c524cbb7-fb6e-4f76-b44a-39b9a0eb14fc.png" style="width:56.83em;height:19.50em;"/></p>
<p>Each line here is the epipolar line of a particular point in the image. Ideally, all the epipolar lines drawn in the left-hand image should intersect at a point, and that point typically lies outside the image. If the calculation is accurate, then that point should coincide with the location of the second camera as seen from the first camera.</p>
<p>In other words, the epipolar lines in the left-hand image tell us that the camera that took the right-hand image is located to our (that is, the first camera's) right-hand side. Analogously, the epipolar lines in the right-hand image tell us that the camera that took the image on the left is located to our (that is, the second camera's) left-hand side.</p>
<p>Moreover, for each point observed in one image, the same point must be observed in the other image on a known epipolar line. This is known as the <strong>epipolar constraint</strong>. We can use this fact to show that if two image points correspond to the same 3D point, then the projection lines of those two image points must intersect precisely at the 3D point. This means that the 3D point can be calculated from two image points, which is what we are going to do next.</p>
<p>Luckily, OpenCV again provides a wrapper to solve an extensive set of linear equations, which is done by following these steps:</p>
<ol>
<li>First, we have to convert our list of matching feature points into a NumPy array:</li>
</ol>
<pre style="padding-left: 60px">first_inliers = np.array(self.match_inliers1).reshape<br/>     (-1, 3)[:, :2]second_inliers = np.array(self.match_inliers2).reshape<br/>     (-1, 3)[:, :2]</pre>
<ol start="2">
<li><strong>Triangulation</strong> is performed next using the preceding two <em>[R | t]</em> matrices (<kbd>self.Rt1</kbd> for the first camera and <kbd>self.Rt2</kbd> for the second camera):</li>
</ol>
<pre style="padding-left: 60px">pts4D = cv2.triangulatePoints(self.Rt1, self.Rt2, first_inliers.T,<br/>     second_inliers.T).T</pre>
<ol start="3">
<li>This will return the triangulated real-world points using 4D homogeneous coordinates. To convert them to 3D coordinates, we need to divide the (<em>X</em>, <em>Y</em>, <em>Z</em>) coordinates by the fourth coordinate, usually referred to as <em>W</em>:</li>
</ol>
<pre style="padding-left: 60px">pts3D = pts4D[:, :3]/np.repeat(pts4D[:, 3], 3).reshape(-1, 3) </pre>
<p>So now that we have obtained the points in the 3D space, let's visualize them in the next section to see how they look.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Understanding 3D point cloud visualization</h1>
                
            
            
                
<p>The last step is visualizing the triangulated 3D real-world points. An easy way of creating 3D scatterplots is by using Matplotlib. However, if you are looking for more professional visualization tools, you may be interested in <strong>Mayavi</strong> (<a href="http://docs.enthought.com/mayavi/mayavi">http://docs.enthought.com/mayavi/mayavi</a>), <strong>VisPy</strong> (<a href="http://vispy.org">http://vispy.org</a>), or the <strong>Point Cloud Library</strong> (<a href="http://pointclouds.org">http://pointclouds.org</a>).</p>
<p>Although the last one does not have Python support for point cloud visualization yet, it is an excellent tool for point cloud segmentation, filtering, and sample consensus model fitting. For more information, head over to <strong>Strawlab</strong>'s GitHub repository at <a href="https://github.com/strawlab/python-pcl">https://github.com/strawlab/python-pcl</a>.</p>
<p>Before we can plot our 3D point cloud, we obviously have to extract the <em>[R | t]</em> matrix and perform the triangulation as explained earlier:</p>
<pre><br/>    def plot_point_cloud(self, feat_mode="SIFT"):<br/>        self._extract_keypoints(feat_mode)<br/>        self._find_fundamental_matrix()<br/>        self._find_essential_matrix()<br/>        self._find_camera_matrices_rt()<br/><br/>        # triangulate points<br/>        first_inliers = np.array(self.match_inliers1)[:, :2]<br/>        second_inliers = np.array(self.match_inliers2)[:, :2]<br/>        pts4D = cv2.triangulatePoints(self.Rt1, self.Rt2, first_inliers.T,<br/>                                      second_inliers.T).T<br/><br/>        # convert from homogeneous coordinates to 3D<br/>        pts3D = pts4D[:, :3] / pts4D[:, 3, None]</pre>
<p>Then, all we need to do is open a Matplotlib figure and draw each entry of <kbd>pts3D</kbd> in a 3D scatterplot:</p>
<pre><br/>        Xs, Zs, Ys = [pts3D[:, i] for i in range(3)]<br/><br/>        fig = plt.figure()<br/>        ax = fig.add_subplot(111, projection='3d')<br/>        ax.scatter(Xs, Ys, Zs, c=Ys, cmap=cm.hsv, marker='o')<br/>        ax.set_xlabel('X')<br/>        ax.set_ylabel('Y')<br/>        ax.set_zlabel('Z')<br/>        plt.title('3D point cloud: Use pan axes button below to inspect')<br/>        plt.show()</pre>
<p>The result is most compelling when studied using pyplot's <kbd>pan axes</kbd> button, which lets you rotate and scale the point cloud in all three dimensions. In the following screenshot, two projections are illustrated.</p>
<p>The first one is from the top and the second one is from some vertical angle from the left of the fountain. The color of a point corresponds to the depth of that point (<em>y</em> coordinate). Most of the points lie near a plane that makes an angle with the <em>XZ</em> plane (points from red to green). These points represent the wall behind the fountain. The other points (from yellow to blue) represent the rest of the structure of the fountain:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/2b3c5918-30af-4d8f-ab31-644a5aa2d12e.png" style="width:27.58em;height:19.75em;"/></p>
<p>The projection from some vertical angle from the left of the fountain is shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/368d1588-2a17-4455-836e-4cf24c4f58d4.png" style="width:27.42em;height:22.83em;"/></p>
<p>So now that you have completed your first app for 3D reconstruction, you have started to dive into a computer vision field called structure from motion. This is an intensively developing field. Let's understand what this field of research is trying to deal with within the next section. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Learning about structure from motion</h1>
                
            
            
                
<p>So far in this chapter, we have gone through some math and we can reconstruct the depth of a scene based on a couple of images taken from different angles, which is a problem of reconstruction of a 3D structure from camera motion.</p>
<p>In computer vision, the process of reconstruction of 3D structures of the scene based on the sequence of images is usually referred to as <strong>structure from motion</strong>. A similar set of problems is the structure from stereo vision—in reconstruction from stereo vision, there are two cameras, located at a certain distance from each other and in structure from motion, there are different images taken from different angles and positions. <em>There's not much difference conceptually, right?</em></p>
<p>Let's think about human vision. People are good at estimating distance and relative locations of objects. A person doesn't even need two eyes for it—we can look with one eye and estimate distances and relative locations pretty well. Moreover, stereoscopic vision only takes place when the distance between eyes is of a similar order of magnitude as the distance to an object when the projections of the scene on the eye have noticeable differences.</p>
<p>For example, if one object is a football field away, the relative location of the eyes doesn't matter, whereas if you look at your nose, the view changes a lot. To illustrate further that stereoscopy is not the essence of our vision, we could look at a photograph where we can describe the relative location of the objects pretty well, but what we are actually looking at is a flat surface.</p>
<p>People do not have such skills at infancy; observations show that infants are bad at locating the placements of the objects. So, probably, a person learns this skill during their conscious life by looking at the world and playing with objects. Next, a question arises—<em>if a person learns the 3D structure of the world, can't we make a computer to do so?</em></p>
<p>There are already interesting models that try to do so. For example, <strong>Vid2Depth</strong> (<a href="https://arxiv.org/pdf/1802.05522.pdf">https://arxiv.org/pdf/1802.05522.pdf</a>) is a deep learning model where the authors train a model that predicts depth in a single image; meanwhile, the model is trained on a sequence of video frames without any depth annotation. Similar problems are active topics for research nowadays. </p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we explored a way of reconstructing a scene in 3D by inferring the geometrical features of 2D images taken by the same camera. We wrote a script to calibrate a camera, and you learned about fundamental and essential matrices. We used this knowledge to perform triangulation. We then went on to visualize the real-world geometry of a scene in a 3D point cloud using simple 3D scatterplots in Matplotlib.</p>
<p>Going forward from here, it will be possible to store the triangulated 3D points in a file that can be parsed by the Point Cloud Library or to repeat the procedure for different image pairs so that we can generate a denser and more accurate reconstruction. Although we have covered a lot in this chapter, there is a lot more left to do.</p>
<p>Typically, when talking about a structure-from-motion pipeline, we include two additional steps that we have not talked about so far—<strong>bundle adjustment</strong> and <strong>geometry fitting</strong>. One of the most important steps in such a pipeline is to refine the 3D estimate in order to minimize reconstruction errors. Typically, we would also want to get all points that do not belong to our object of interest out of the cloud. But with the basic code in hand, you can now go ahead and write your own advanced structure-from-motion pipeline!</p>
<p>In the next chapter, we will use the concepts we learned in 3D Scene reconstruction. We will use the key points and features that we learned to extract in this chapter, and we'll apply other alignment algorithms to create panoramas. We will also dive deep into other topics in computational photography, understand the core concepts, and create <strong>High Dynamic Range</strong> (<strong>HDR</strong>) images.</p>


            

            
        
    </body></html>