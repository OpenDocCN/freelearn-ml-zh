- en: Chapter 8. Text Mining and Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) is ubiquitous today in various applications
    such as mobile apps, ecommerce websites, emails, news websites, and more. Detecting
    spam in e-mails, characterizing e-mails, speech synthesis, categorizing news,
    searching and recommending products, performing sentiment analysis on social media
    brands—these are all different aspects of NLP and mining text for information.'
  prefs: []
  type: TYPE_NORMAL
- en: There has been an exponential increase in digital information that is textual
    in content—in the form of web pages, e-books, SMS messages, documents of various
    formats, e-mails, social media messages such as tweets and Facebook posts, now
    ranges in exabytes (an exabyte is 1,018 bytes). Historically, the earliest foundational
    work relying on automata and probabilistic modeling began in the 1950s. The 1970s
    saw changes such as stochastic modeling, Markov modeling, and syntactic parsing,
    but their progress was limited during the 'AI Winter' years. The 1990s saw the
    emergence of text mining and a statistical revolution that included ideas of corpus
    statistics, supervised Machine Learning, and human annotation of text data. From
    the year 2000 onwards, with great progress in computing and Big Data, as well
    as the introduction of sophisticated Machine Learning algorithms in supervised
    and unsupervised learning, the area has received rekindled interest and is now
    among the hottest topics in research, both in academia and the R&D departments
    of commercial enterprises. In this chapter, we will discuss some aspects of NLP
    and text mining that are essential in Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter begins with an introduction to the key areas within NLP, and it
    then explains the important processing and transformation steps that make the
    documents more suitable for Machine Learning, whether supervised or unsupervised.
    The concept of topic modeling, clustering, and named entity recognition follow,
    with brief descriptions of two Java toolkits that offer powerful text processing
    capabilities. The case study for this chapter uses another widely-known dataset
    to demonstrate several techniques described here through experiments using the
    tools KNIME and Mallet.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP, subfields, and tasks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text categorization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Information extraction and named entity recognition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Coreference resolution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-sense disambiguation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic reasoning and inferencing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues with mining and unstructured data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text processing components and transformations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document collection and standardization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop words removal
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming/Lemmatization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Local/Global dictionary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction/generation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature representation and similarity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection and dimensionality reduction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Topics in text mining:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning and NLP
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools and usage:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mallet
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: KNIME
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP, subfields, and tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information about the real world exists in the form of structured data, typically
    generated by automated processes, or unstructured data, which, in the case of
    text, is created by direct human agency in the form of the written or spoken word.
    The process of observing real-world situations and using either automated processes
    or having humans perceive and convert that information into understandable data
    is very similar in both structured and unstructured data. The transformation of
    the observed world into unstructured data involves complexities such as the language
    of the text, the format in which it exists, variances among different observers
    in interpreting the same data, and so on. Furthermore, the ambiguity caused by
    the syntax and semantics of the chosen language, subtlety in expression, the context
    in the data, and so on, make the task of mining text data very difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss some high-level subfields and tasks that involve NLP and
    text mining. The subject of NLP is quite vast, and the following topics is in
    no way comprehensive.
  prefs: []
  type: TYPE_NORMAL
- en: Text categorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This field is one of the most well-established, and in its basic form classifies
    documents with unstructured text data into predefined categories. This can be
    viewed as a direct extension of supervised Machine Learning in the unstructured
    text world, learning from historic documents to predict categories of unseen documents
    in the future. Basic methods in spam detection in e-mails or news categorization
    are among some of the most prominent applications of this task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Text categorization](img/B05137_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Text Categorization showing classification into different categories'
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging (POS tagging)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another subtask in NLP that has seen a lot of success is associating parts-of-speech
    of the language—such as nouns, adjectives, verbs—to words in a text, based on
    context and relationship to adjacent words. Today, instead of manual POS tagging,
    automated and sophisticated POS taggers perform the job.
  prefs: []
  type: TYPE_NORMAL
- en: '![Part-of-speech tagging (POS tagging)](img/B05137_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: POS Tags associated with segment of text'
  prefs: []
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustering unstructured data for organization, retrieval, and groupings based
    on similarity is the subfield of text clustering. This field is also well-developed
    with advancements in different clustering and text representations suited for
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Text clustering](img/B05137_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Clustering of OS news documents to various OS specific clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Information extraction and named entity recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The task of extracting specific elements, such as time, location, organization,
    entities, and so on, comes under the topic of information extraction. Named entity
    recognition is a sub-field that has wide applications in different domains, from
    reviews of historical documents to bioinformatics with gene and drug information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Information extraction and named entity recognition](img/B05137_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Named Entity Recognition in a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis and opinion mining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another sub-field in the area of NLP involves inferring the sentiments of observers
    in order to categorize them with an understandable metric or to give insights
    into their opinions. This area is not as advanced as some of the ones mentioned
    previously, but much research is being done in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sentiment analysis and opinion mining](img/B05137_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Sentiment Analysis showing positive and negative sentiments for sentences'
  prefs: []
  type: TYPE_NORMAL
- en: Coreference resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding references to multiple entities existing in the text and disambiguating
    that reference is another popular area of NLP. This is considered as a stepping
    stone in doing more complex tasks such as question answering and summarization,
    which will be discussed later.
  prefs: []
  type: TYPE_NORMAL
- en: '![Coreference resolution](img/B05137_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Coreference resolution showing how pronouns get disambiguated'
  prefs: []
  type: TYPE_NORMAL
- en: Word sense disambiguation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a language such as English, since the same word can have multiple meanings
    based on the context, deciphering this automatically is an important part of NLP,
    and the focus of **word sense disambiguation** (**WSD**).
  prefs: []
  type: TYPE_NORMAL
- en: '![Word sense disambiguation](img/B05137_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Showing how word "mouse" is associated with right word using the
    context'
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Translating text from one language to another or from speech to text in different
    languages is broadly covered in the area of **machine translation** (**MT**).
    This field has made significant progress in the last few years, with the usage
    of Machine Learning algorithms in supervised, unsupervised, and semi-supervised
    learning. Deep learning with techniques such as LSTM has been proved to be the
    most effective technique in this area, and is widely used by Google for its translation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine translation](img/B05137_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Machine Translation showing English to Chinese conversion'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic reasoning and inferencing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reasoning, deriving logic, and inferencing from unstructured text is the next
    level of advancement in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: '![Semantic reasoning and inferencing](img/B05137_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Semantic Inferencing answering complex questions'
  prefs: []
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subfield that is growing in popularity in NLP is the automated summarization
    of large documents or passages of text to a small representative text that can
    be easily understood. This is one of the budding research areas in NLP. Search
    engines' usage of summaries, multi-document summarizations for experts, and so
    on, are some of the applications that are benefiting from this field.
  prefs: []
  type: TYPE_NORMAL
- en: Automating question and answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Answering questions posed by humans in natural language, ranging from questions
    specific to a certain domain to generic, open-ended questions is another emerging
    field in the area of NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Issues with mining unstructured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Humans can read, parse, and understand unstructured text/documents more easily
    than computer-based programs. Some of the reasons why text mining is more complicated
    than general supervised or unsupervised learning are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Ambiguity in terms and phrases. The word *bank* has multiple meanings, which
    a human reader can correctly associate based on context, yet this requires preprocessing
    steps such as POS tagging and word sense disambiguation, as we have seen. According
    to the Oxford English Dictionary, the word *run* has no fewer than 645 different
    uses in the verb form alone and we can see that such words can indeed present
    problems in resolving the meaning intended (between them, the words run, put,
    set, and take have more than a thousand meanings).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context and background knowledge associated with the text. Consider a sentence
    that uses a neologism with the suffix *gate* to signify a political scandal, as
    in, *With cries for impeachment and popularity ratings in a nosedive, Russiagate
    finally dealt a deathblow to his presidency*. A human reader can surmise what
    is being referred to by the coinage *Russiagate* as something that recalls the
    sense of high-profile intrigue, by association via an affix, of another momentous
    scandal in US political history, *Watergate*. This is particularly difficult for
    a machine to make sense of.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoning, that is, inferencing from documents is very difficult as mapping
    unstructured information to knowledge bases is itself a big hurdle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to perform supervised learning needs labeled training documents and
    based on the domain, performing labeling on the documents can be time consuming
    and costly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text processing components and transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some common preprocessing and transformation
    steps that are done in most text mining processes. The general concept is to convert
    the documents into structured datasets with features or attributes that most Machine
    Learning algorithms can use to perform different kinds of learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will briefly describe some of the most used techniques in the next section.
    Different applications of text mining might use different pieces or variations
    of the components shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text processing components and transformations](img/B05137_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Text Processing components and the flow'
  prefs: []
  type: TYPE_NORMAL
- en: Document collection and standardization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first steps in most text mining applications is the collection of
    data in the form of a body of documents—often referred to as a *corpus* in the
    text mining world. These documents can have predefined categorization associated
    with them or it can simply be an unlabeled corpus. The documents can be of heterogeneous
    formats or standardized into one format for the next process of tokenization.
    Having multiple formats such as text, HTML, DOCs, PDGs, and so on, can lead to
    many complexities and hence one format, such as XML or **JavaScript Object Notation**
    (**JSON**) is normally preferred in most applications.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inputs are vast collections of homogeneous or heterogeneous sources and outputs
    are a collection of documents standardized into one format, such as XML.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Standardization involves ensuring tools and formats are agreed upon based on
    the needs of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: Agree to a standard format, such as XML, with predefined tags that provide information
    on meta-attributes of documents `(<author>`, `<title>`, `<date>`, and so on) and
    actual content, such as `<document>`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most document processors can either be transformed into XML or transformation
    code can be written to perform this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The task of tokenization is to extract words or meaningful characters from the
    text containing a stream of these words. For example, the text *The boy stood
    up. He then ran after the dog* can be tokenized into tokens such as *{the, boy,
    stood, up, he, ran, after, the, dog}*.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An input is a collection of documents in a well-known format as described in
    the last section and an output is a document with tokens of words or characters
    as needed in the application.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any automated system for tokenization must address the particular challenges
    presented by the language(s) it is expected to handle:'
  prefs: []
  type: TYPE_NORMAL
- en: In languages such as English, tokenization is relatively simple due to the presence
    of white space, tabs, and newline for separating the words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different challenges in each language—even in English, abbreviations
    such as *Dr.*, alphanumeric characters (*B12*), different naming schemes (*O'Reilly*),
    and so on, must be tokenized appropriately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language-specific rules in the form of if-then instructions are written to extract
    tokens from the documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop words removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This involves removing high frequency words that have no discriminatory or predictive
    value. If every word can be viewed as a feature, this process reduces the dimension
    of the feature vector by a significant number. Prepositions, articles, and pronouns
    are some of the examples that form the stop words that are removed without affecting
    the performance of text mining in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An input is a collection of documents with the tokens extracted and an output
    is a collection of documents with tokens reduced by removing stop words.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various techniques that have evolved in the last few years ranging
    from manually precompiled lists to statistical elimination using either term-based
    or mutual information.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used technique for many languages is a manually precompiled
    list of stop words, including prepositions (in, for, on), articles (a, an, the),
    pronouns (his, her, they, their), and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many tools use Zipf's law (*References* [3]), where high frequency words, singletons,
    and unique terms are removed. Luhn's early work (*References* [4]), as represented
    in the following figure 11, shows thresholds of the upper bound and lower bound
    of word frequency, which give us the significant words that can be used for modeling:![How
    does it work?](img/B05137_08_012.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figure 11: Word Frequency distribution, showing how frequently used, significant
    and rare words exist in corpus'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Stemming or lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of normalizing tokens of similar words into one is known as stemming
    or lemmatization. Thus, reducing all occurrences of "talking", "talks", "talked",
    and so on in a document to one root word "talk" in the document is an example
    of stemming.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An input is documents with tokens and an output is documents with reduced tokens
    normalized to their stem or root words.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are basically two types of stemming: inflectional stemming and stemming
    to the root.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inflectional stemming generally involves removing affixes, normalizing the verb
    tenses and removing plurals. Thus, "ships" to "ship", "is", "are" and "am" to
    "be" in English.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming to the root is generally a more aggressive form than inflectional stemming,
    where words are normalized to their roots. An example of this is "applications",
    "applied", "reapply", and so on, all reduced to the root word "apply".
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lovin's stemmer was one of the first stemming algorithms (*References* [1]).
    Porter's stemming, which evolved in the 1980s with around 60 rules in 6 steps,
    is still the most widely used form of stemming (*References* [2]).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Present-day applications drive a wide range of statistical techniques based
    on stemming, including those using n-grams (a contiguous sequence of n items,
    either letters or words from a given sequence of text), **hidden Markov models**
    (**HMM**), and context-sensitive stemming.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Local/global dictionary or vocabulary?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the preprocessing task of converting documents into tokens is performed,
    the next step is the creation of a corpus or vocabulary as a single dictionary,
    using all the tokens from all documents. Alternatively, several dictionaries are
    created based on category, using specific tokens from fewer documents.
  prefs: []
  type: TYPE_NORMAL
- en: Many applications in topic modeling and text categorization perform well when
    dictionaries are created per topic/category, which is known as a local dictionary.
    On the other hand, many applications in document clustering and information extraction
    perform well when one single global dictionary is created from all the document
    tokens. The choice of creating one or many specific dictionaries depends on the
    core NLP task, as well as on computational and storage requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction/generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A key step in converting the document(s) with unstructured text is to transform
    them into datasets with structured features, similar to what we have seen so far
    in Machine Learning datasets. Extracting features from text so that it can be
    used in Machine Learning tasks such as supervised, unsupervised, and semi-supervised
    learning depends on many factors, such as the goals of the applications, domain-specific
    requirements, and feasibility. There are a wide variety of features, such as words,
    phrases, sentences, POS-tagged words, typographical elements, and so on, that
    can be extracted from any document. We will give a broad range of features that
    are commonly used in different Machine Learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Lexical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lexical features are the most frequently used features in text mining applications.
    Lexical features form the basis for the next level of features. They are the simple
    character- or word- level features constructed without trying to capture information
    about intent or the various meanings associated with the text. Lexical features
    can be further broken down into character-based features, word-based features,
    part-of-speech features, and taxonomies, for example. In the next section, we
    will describe some of them in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Character-based features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Individual characters (unigram) or a sequence of characters (n-gram) are the
    simplest forms of features that can be constructed from the text document. The
    bag of characters or unigram characters have no positional information, while
    higher order n-grams capture some amount of context and positional information.
    These features can be encoded or given numeric values in different ways, such
    as binary 0/1 values, or counts, for example, as discussed later in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider the memorable Dr. Seuss rhyme as the text content—"the Cat in
    the Hat steps onto the mat". While the bag-of-characters (1-gram or unigram features)
    will generate unique characters {"t","h", "e", "c","a","i","n","s","p","o","n","m"}
    as features, the 3-gram features are { "\sCa" ,"\sHa", "\sin" , "\sma", "\son",
    "\sst", "\sth", "Cat", "Hat", "at\s", "e\sC", "e\sH", "e\sm", "eps", "he\s", "in\s
    ", "mat", "n\st", "nto", "o\st", "ont", "ps\s", "s\so" , "ste"," t\si"," t\ss"
    , "tep", "the", "to\s "}. As can be seen, as "n" increases, the number of features
    increases exponentially and soon becomes unwieldy. The advantage of n-grams is
    that at the cost of increasing the total number of features, the assembled features
    often seem to capture combinations of characters that are more interesting than
    the individual characters themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Word-based features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of generating features from characters, features can similarly be constructed
    from words in a unigram and n-gram manner. These are the most popular feature
    generation techniques. The unigram or 1-word token is also known as the bag of
    words model. So the example of "the Cat in the Hat steps onto the mat" when considered
    as unigram features is {"the", "Cat", "in", "Hat", "steps", "onto", "mat"}. Similarly,
    bigram features on the same text would result in {"the Cat", "Cat in", "in the",
    "the Hat", "Hat step", "steps onto", "onto the", "the mat"}. As in the case of
    character-based features, by going to higher "n" in the n-grams, the number of
    features increases, but so does the ability to capture word sense via context.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The input is text with words and the output is text where every word is associated
    with the grammatical tag. In many applications, part-of-speech gives a context
    and is useful in identification of named entities, phrases, entity disambiguation,
    and so on. In the example "the Cat in the Hat steps onto the mat" , the output
    is {"the\Det", "Cat\Noun", "in\Prep", "the\Det", "Hat\Noun", "steps\Verb", "onto\Prep",
    "the\Det", "mat\Noun"}. Language specific rule-based taggers or Markov chain–based
    probabilistic taggers are often used in this process.
  prefs: []
  type: TYPE_NORMAL
- en: Taxonomy features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating taxonomies from the text data and using it to understand relationships
    between words is also useful in different contexts. Various taxonomical features
    such as hypernyms, hyponyms, is-member, member-of, is-part, part-of, antonyms,
    synonyms, acronyms, and so on, give lexical context that proves useful in searches,
    retrieval, and matching in many text mining scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next level of features that are higher than just characters or words in
    text documents are the syntax-based features. Syntactical representation of sentences
    in text is generally in the form of syntax trees. Syntax trees capture nodes as
    terms used in sentences, and relationships between the nodes are captured as links.
    Syntactic features can also capture more complex features about sentences and
    usage—such as aggregates—that can be used for Machine Learning. It can also capture
    statistics about syntax trees—such as sentences being left-heavy, right-heavy,
    or balanced—that can be used to understand signatures of different content or
    writers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two sentences can have the same characters and words in the lexical analysis,
    but their syntax trees or intent could be completely different. Breaking the sentences
    in the text into different phrases—**Noun Phrase** (**NP**), **Prepositional Phrase**
    (**PP**), **Verbal** (or Gerund) **Phrase** (**VP**), and so on—and capturing
    phrase structure trees for the sentences are part of this processing task. The
    following is the syntactic parse tree for our example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Syntactic Language Models** (**SLM**) are about determining the probability
    of a sequence of terms. Language model features are used in machine translation,
    spelling correction, speech translation, summarization, and so on, to name a few
    of the applications. Language models can additionally use parse trees and syntax
    trees in their computation as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The chain rule is applied to compute the joint probability of terms in the
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Syntactic features](img/B05137_08_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the example "the cat in the hat steps onto the mat":'
  prefs: []
  type: TYPE_NORMAL
- en: '![Syntactic features](img/B05137_08_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Generally, the estimation of the probability of long sentences based on counts
    using any corpus is difficult due to the need for many examples of such sentences.
    Most language models use the Markov assumption of independence and n-grams (2-5
    words) in practical implementations (*References* [8]).
  prefs: []
  type: TYPE_NORMAL
- en: Semantic features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semantic features attempt to capture the "meaning" of the text, which is then
    used for different applications of text mining. One of the simplest forms of semantic
    features is the process of adding annotations to the documents. These annotations,
    or metadata, can have additional information that describes or captures the intent
    of the text or documents. Adding tags using collaborative tagging to capture tags
    as keywords describing the text is a common semantic feature generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Another form of semantic feature generation is the process of ontological representation
    of the text. Generic and domain specific ontologies that capture different relationships
    between objects are available in knowledge bases and have well-known specifications,
    such as Semantic Web 2.0\. These ontological features help in deriving complex
    inferencing, summarization, classification, and clustering tasks in text mining.
    The terms in the text or documents can be mapped to "concepts" in ontologies and
    stored in knowledge bases. These concepts in ontologies have semantic properties,
    and are related to other concepts in a number of ways, such as generalization/specialization,
    member-of/isAMember, association, and so on, to name a few. These attributes or
    properties of concepts and relationships can be further used in search, retrieval,
    and even in predictive modeling. Many semantic features use the lexical and syntactic
    processes as pre-cursors to the semantic process and use the outputs, such as
    nouns, to map to concepts in ontologies, for example. Adding the concepts to an
    existing ontology or annotating it with more concepts makes the structure more
    suitable for learning. For example, in the "the cat in the .." sentence, "cat"
    has properties such as {age, eats, ...} and has different relationships, such
    as { "isA Mammal", "hasChild", "hasParent", and so on}.
  prefs: []
  type: TYPE_NORMAL
- en: Feature representation and similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lexical, syntactic, and semantic features, described in the last section, often
    have representations that are completely different from each other. Representations
    of the same feature type, that is, lexical, syntactic, or semantic, can differ
    based on the computation or mining task for which they are employed. In this section,
    we will describe the most common lexical feature-based representation known as
    vector space models.
  prefs: []
  type: TYPE_NORMAL
- en: Vector space model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **vector space model** (**VSM**) is a transformation of the unstructured
    document to a numeric vector representation where terms in the corpus form the
    dimensions of the vector and we use some numeric way of associating value with
    these dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the section on dictionaries, a corpus is formed out of unique
    words and phrases from the entire collection of documents in a domain or within
    local sub-categories of one. Each of the elements of such a dictionary are the
    dimensions of the vector. The terms—which can be single words or phrases, as in
    n-grams—form the dimensions and can have different values associated with them
    in a given text/document. The goal is to capture the values in the dimensions
    in a way that reflects the relevancy of the term(s) in the entire corpus (*References*
    [11]). Thus, each document or file is represented as a high-dimensional numeric
    vector. Due to the sparsity of terms, the numeric vector representation has a
    sparse representation in numeric space. Next, we will give some well-known ways
    of associating values to these terms.
  prefs: []
  type: TYPE_NORMAL
- en: Binary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the simplest form of associating value to the terms, or dimensions.
    In binary form, each term in the corpus is given a 0 or 1 value based on the presence
    or absence of the term in the document. For example, consider the following three
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document 1: "The Cat in the Hat steps onto the mat"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Document 2: "The Cat sat on the Hat"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Document 3: "The Cat loves to step on the mat"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After preprocessing by removing stop words {on, the, in, onto} and stemming
    {love/loves, steps/step} using a unigram or bag of words, {cat, hat, step, mat,
    sat, love} are the features of the corpus. Each document is now represented in
    a binary vector space model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Terms | cat | hat | step | mat | sat | love |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Document 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Term frequency (TF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In **term frequency** (**TF**), as the name suggests, the frequency of terms
    in the entire document forms the numeric value of the feature. The basic assumption
    is that the higher the frequency of the term, the greater the relevance of that
    term for the document. Counts of terms or normalized counts of terms are used
    as values in each column of terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf(t) = count(D, t)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table gives term frequencies for the three documents in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '| TF / Terms | cat | hat | step | mat | sat | love |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Document 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 2 | 1 | 1 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 3 | 1 | 0 | 1 | 1 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Inverse document frequency (IDF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Inverse document frequency** (**IDF**) has various flavors, but the most
    common way of computing it is using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inverse document frequency (IDF)](img/B05137_08_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Inverse document frequency (IDF)](img/B05137_08_017.jpg) ![Inverse document
    frequency (IDF)](img/B05137_08_018.jpg) IDF favors mostly those terms that occur
    relatively infrequently in the documents. Some empirically motivated improvements
    to IDF have also been proposed in the research (*References* [7]).
  prefs: []
  type: TYPE_NORMAL
- en: 'TF for our example corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Terms | cat | hat | step | mat | sat | love |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| N/nj | 3/3 | 3/2 | 3/2 | 3/2 | 3/1 | 3/1 |'
  prefs: []
  type: TYPE_TB
- en: '| IDF | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
  prefs: []
  type: TYPE_TB
- en: Term frequency-inverse document frequency (TF-IDF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Combining both term frequencies and inverse document frequencies in one metric,
    we get the term frequency-inverse document frequency values. The idea is to value
    those terms that are relatively uncommon in the corpus (high IDF), but are reasonably
    relevant for the document (high TF). TF-IDF is the most common form of value association
    in many text mining processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Term frequency-inverse document frequency (TF-IDF)](img/B05137_08_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us the TF-IDF for all the terms in each of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '| TF-IDF/Terms | cat | hat | step | mat | sat | love |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Document 1 | 0.0 | 0.40 | 0.40 | 0.40 | 1.10 | 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 2 | 0.0 | 0.40 | 0.0 | 0.0 | 1.10 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Document 3 | 0.0 | 0.0 | 0.40 | 0.40 | 0.0 | 1.10 |'
  prefs: []
  type: TYPE_TB
- en: Similarity measures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many techniques in supervised, unsupervised, and semi-supervised learning use
    "similarity" measures in their underlying algorithms to find similar patterns
    or to separate different patterns. Similarity measures are tied closely to the
    representation of the data. In the VSM representation of documents, the vectors
    are very high dimensional and sparse. This poses a serious issue in most traditional
    similarity measures for classification, clustering, or information retrieval.
    Angle-based similarity measures, such as cosine distances or Jaccard coefficients,
    are more often used in practice. Consider two vectors represented by **t**[1]
    and **t**[2] corresponding to two text documents.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is the L2 norm in the feature space of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Euclidean distance](img/B05137_08_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cosine distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This angle-based similarity measure considers orientation between vectors only
    and not their lengths. It is equal to the cosine of the angle between the vectors.
    Since the vector space model is a positive space, cosine distance varies from
    0 (orthogonal, no common terms) to 1 (all terms are common to both, but not necessarily
    with the same term frequency):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine distance](img/B05137_08_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Pairwise-adaptive similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This measure the distance in a reduced feature space by only considering the
    features that are most important in the two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pairwise-adaptive similarity](img/B05137_08_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, **t**[i,k] is a vector formed from a subset of the features of **t**[i]
    (*i* = 1, 2) containing the union of the *K* largest features appearing in **t**[1]
    and **t**[2].
  prefs: []
  type: TYPE_NORMAL
- en: Extended Jaccard coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This measure is computed as a ratio of the shared terms to the union of the
    terms between the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extended Jaccard coefficient](img/B05137_08_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Dice coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Dice coefficient is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dice coefficient](img/B05137_08_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Feature selection and dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal is the same as in [Chapter 2](ch02.html "Chapter 2. Practical Approach
    to Real-World Supervised Learning"), *Practical Approach to Real-World Supervised
    Learning* and [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning
    Techniques"), *Unsupervised Machine Learning Techniques*. The problem of the curse
    of dimensionality becomes even more pronounced with text mining and high dimensional
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most feature selection techniques are supervised techniques that depend on the
    labels or the outcomes for scoring the features. In the majority of cases, we
    perform filter-based rather than wrapper-based feature selection, due to the lower
    performance cost. Even among filter-based methods, some, such as those involving
    multivariate techniques such as **Correlation based Feature selection** (**CFS**),
    as described in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    can be quite costly or result in suboptimal performance due to high dimensionality
    (*References* [9]).
  prefs: []
  type: TYPE_NORMAL
- en: Information theoretic techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    filter-based univariate feature selection methods, such as **Information gain**
    (**IG**) and **Gain Ratio** (**GR**), are most commonly used once preprocessing
    and feature extraction is done.
  prefs: []
  type: TYPE_NORMAL
- en: In their research, Yang and Pederson (*References* [10]) clearly showed the
    benefits of feature selection and reduction using IG to remove close to 98% of
    terms and yet improve the predictive capability of the classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the information theoretic or entropy-based methods have a stronger influence
    resulting from the marginal probabilities of the tokens. This can be an issue
    when the terms have equal conditional probability P(t|class), the rarer terms
    may have better scores than the common terms.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical-based techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ?² feature selection is one of the most common statistical-based techniques
    employed to perform feature selection in text mining. ?² statistics, as shown
    in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, give the independence
    relationship between the tokens in the text and the classes.
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that ?² statistics for feature selection may not be effective
    when there are low-frequency terms (*References* [19]).
  prefs: []
  type: TYPE_NORMAL
- en: Frequency-based techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using the term frequency or the document frequency described in the section
    on feature representation, a threshold can be manually set, and only terms above
    or below a certain threshold can be allowed used for modeling in either classification
    or clustering tasks. Note that **term frequency** (**TF**) and **document frequency**
    (**DF**) methods are biased towards common words while some of the information
    theoretic or statistical-based methods are biased towards less frequent words.
    The choice of selection of features depends on the domain, the particular application
    of predictive learning, and more importantly, on how models using these features
    are evaluated, especially on the unseen dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach that we saw in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*, was
    to use unsupervised techniques to reduce the features using some form of transformation
    to decide their usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) computes a covariance or correlation
    matrix from the document-term matrix. It transforms the data into linear combinations
    of terms in the inputs in such a way that the transformed combination of features
    or terms has higher discriminating power than the input terms. PCA with cut-off
    or thresholding on the transformed features, as shown in [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, can bring down the dimensionality substantially and even
    improve or give comparable performance to the high dimensional input space. The
    only issue with using PCA is that the transformed features are not interpretable,
    and for domains where understanding which terms or combinations yield better predictive
    models, this technique has some limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent semantic analysis** (**LSA**) is another way of using the input matrix
    constructed from terms and documents and transforming it into lower dimensions
    with latent concepts discovered through combinations of terms used in documents
    (*References* [5]). The following figure captures the process using the **singular
    value decomposition** (**SVD**) method for factorizing the input document-term
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction](img/B05137_08_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: SVD factorization of input document-terms into LSA document vectors
    and LSA term vectors'
  prefs: []
  type: TYPE_NORMAL
- en: LSA has been shown to be a very effective way of reducing the dimensions and
    also of improving the predictive performance in models. The disadvantage of LSA
    is that storage of both vectors U and V is needed for performing retrievals or
    queries. Determining the lower dimension k is hard and needs some heuristics similar
    to k-means discussed in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine
    Learning Techniques"), *Unsupervised Machine Learning Techniques*.
  prefs: []
  type: TYPE_NORMAL
- en: Topics in text mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the first section, the area of text mining and performing Machine
    Learning on text spans a wide range of topics. Each topic discussed has some customizations
    to the mainstream algorithms, or there are specific algorithms that have been
    developed to perform the task called for in that area. We have chosen four broad
    topics, namely, text categorization, topic modeling, text clustering, and named
    entity recognition, and will discuss each in some detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text categorization/classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The text classification problem manifests itself in different applications,
    such as document filtering and organization, information retrieval, opinion and
    sentiment mining, e-mail spam filtering, and so on. Similar to the classification
    problem discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning*,
    the general idea is to train on the training data with labels and to predict the
    labels of unseen documents.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous section, the preprocessing steps help to transform
    the unstructured document collection into well-known numeric or categorical/binary
    structured data arranged in terms of a document-term matrix. The choice of performing
    some preprocessing steps, such as stemming or customizing stop words, depends
    on the data and applications. The feature choice is generally basic lexical features,
    n-grams of words as terms, and only in certain cases do we use the entire text
    as a string without breaking it into terms or tokens. It is common to use binary
    feature representation or frequency-based representation for document-term structured
    data. Once this transformation is complete, we do feature selection using univariate
    analysis, such as information gain or chi-square, to choose discriminating features
    above certain thresholds of scores. One may also perform feature transformation
    and dimensionality reduction such as PCA or LSA in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: There is a wide range in the choice of classifiers once we get structured data
    from the preceding process. In the research as well as in commercial applications,
    we see the use of most of the common modeling techniques, including linear (linear
    regression, logistic regression, and so on), non-linear (SVM, neural networks,
    KNN), generative (naïve bayes, bayesian networks), interpretable (decision trees,
    rules), and ensemble-based (bagging, boosting, random forest) classifiers. Many
    algorithms use similarity or distance metrics, of which cosine distance is the
    most popular choice. In certain classifiers, such as SVM, the string representation
    of the document can be used as is, with the right choice of string kernels and
    similarity-based metrics on strings to compute the dot products.
  prefs: []
  type: TYPE_NORMAL
- en: Validation and evaluation methods are similar to supervised classification methodologies—splitting
    the data into train/validation/test, training on training data, tuning parameters
    of algorithm(s) on validation data, and estimating the performance of the models
    on hold-out or test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since most of text classification involves a large number of documents, and
    the target classes are rare, the metrics used for evaluation, tuning, or choosing
    algorithms are in most cases precision, recall, and F-score measure, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text categorization/classification](img/B05137_08_039.jpg)![Text categorization/classification](img/B05137_08_040.jpg)![Text
    categorization/classification](img/B05137_08_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Topic modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A topic is a distribution over a fixed vocabulary. Topic modeling can be defined
    as an ability to capture different core ideas or themes in various documents.
    This has a wide range of applications, such as the summarization of documents,
    understanding reasons for sentiments, trends, the news, and many others. The following
    figure shows how topic modeling can discern a user-specified number *k* of topics
    from a corpus and then, for every document, assign proportions representing how
    much of each topic is found in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/B05137_08_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Probabilistic topic weights assignment for documents'
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few techniques for performing topic modeling using supervised
    and unsupervised learning in the literature (*References* [13]). We will discuss
    the most common technique known as **probabilistic latent semantic index** (**PLSI**).
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic latent semantic analysis (PLSA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of PLSA, as in the LSA for feature reduction, is to find latent concepts
    hidden in the corpus by discovering the association between co-occurring terms
    and treating the documents as mixtures of these concepts. This is an unsupervised
    technique, similar to dimensionality reduction, but the idea is to use it to model
    the mixture of topics or latent concepts in the document (*References* [12]).
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, the model may associate terms occurring together
    often in the corpus with a latent concept, and each document can then be said
    to exhibit that topic to a smaller or larger extent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probabilistic latent semantic analysis (PLSA)](img/B05137_08_044.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Latent concept of Baseball capturing the association between documents
    and related terms'
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The inputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: A collection of documents following a certain format and structure. We will
    give the notation:![Input and output](img/B05137_08_045.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of topics that need to be modeled or discovered as *k*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k* topics identified T = {T[1], T[2],…T[k]}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each document, coverage of the topic given in the document *d*[i] can be
    written as = {*p*[i1], *p*[i2], …*p*[ik]}, where *p*[ij]is the probability of
    the document *d*i covering the topic T[j].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Implementations of PLSA generally follow the steps described here:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic preprocessing steps as discussed previously, such as tokenization, stop
    words removal, stemming, dictionary of words formation, feature extraction (unigrams
    or n-grams, and so on), and feature selection (unsupervised techniques) are carried
    out, if necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problem can be reduced to estimating the distribution of terms in a document,
    and, given the distribution, choosing the topic based on the maximum terms corresponding
    to the topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing a "latent variable" *z* helps us to select whether the term belongs
    to a topic. Note that *z* is not "observed", but we assume that it is related
    to picking the term from the topic. Thus, the probability of the term *t* given
    the document *t* can be expressed in terms of this latent variable as:![How does
    it work?](img/B05137_08_053.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By using two sets of variables (?, p) the equation can be written as:![How does
    it work?](img/B05137_08_055.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, p(t|z; ?) is the probability of latent concepts in terms and p(z|d; p)
    is the probability of latent concepts in document-specific mixtures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using log-likelihood to estimate the parameters to maximize:![How does it work?](img/B05137_08_058.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since this equation involves nonconvex optimization, the EM algorithm is often
    used to find the parameters iteratively until convergence is reached or the total
    number of iterations are completed (*References* [6]):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The E-step of the EM algorithm is used to determine the posterior probability
    of the latent concepts. The probability of term t occurring in the document d,
    can be explained by the latent concept z as:![How does it work?](img/B05137_08_062.jpg)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The M-step of the EM algorithm uses the values obtained from the E-step, that
    is, p(z|d, t) and does parameter estimation as:![How does it work?](img/B05137_08_064.jpg)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_08_065.jpg) = how often the term *t* is associated
    with concept *z*:![How does it work?](img/B05137_08_067.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: '![How does it work?](img/B05137_08_068.jpg)= how often document *d* is associated
    with concept *z*.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Though widely used, PLSA has some drawbacks that have been overcome by more
    recent techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The unsupervised nature of the algorithm and its general applicability allows
    it to be used in a wide variety of similar text mining applications, such as clustering
    documents, associating topics related to authors/time, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PLSA with EM algorithms, as discussed in previous chapters, face the problem
    of getting "stuck in local optima", unlike other global algorithms, such as evolutionary
    algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PLSA algorithms can only do topic identification in known documents, but cannot
    do any predictive modeling. PLSA has been generalized and is known as **latent
    dirichlet allocation** (**LDA**) to overcome this (*References* [14]).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of clustering, as seen in [Chapter 3](ch03.html "Chapter 3. Unsupervised
    Machine Learning Techniques"), *Unsupervised Machine Learning Techniques*, is
    to find groups of data, text, or documents that are similar to one another within
    the group. The granularity of unstructured data can vary from small phrases or
    sentences, paragraphs, and passages of text to a collection of documents. Text
    clustering finds its application in many domains, such as information retrieval,
    summarization, topic modeling, and document classification in unsupervised situations,
    to name a few. Traditional techniques in clustering can be employed once the unstructured
    text data is transformed into structured data via preprocessing. The difficulty
    with traditional clustering techniques is the high-dimensional and sparse nature
    of the dataset obtained using the transformed document-term matrix representation.
    Many traditional clustering algorithms work only on numeric values of features.
    Because of this constraint, categorical or binary representation of terms cannot
    be used and often TF or TF-IDF are used for representation of the document-term
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss some of the basic processes and techniques
    in clustering. We will start with pre-processing and transformations and then
    discuss some techniques that are widely used and the modifications made to them.
  prefs: []
  type: TYPE_NORMAL
- en: Feature transformation, selection, and reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the pre-processing steps discussed in this section are normally used
    to get either unigram or n-gram representation of terms in documents. Dimensionality
    reduction techniques, such as LSA, are often employed to transform the features
    into smaller latent space.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The techniques for clustering in text include probabilistic models, as well
    as those that use distance-based methods, which are familiar to us from when we
    learned about structured data. We will also discuss **Non-negative Matrix Factorization**
    (**NNMF**) as an effective technique with good performance and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Generative probabilistic models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There is commonality between topic modeling and text clustering in generative
    methods. As shown in the following figure, clustering associates a document with
    a single cluster (generally), compared to topic modeling where each document can
    have a probability of coverage in multiple topics. Every word in topic modeling
    can be generated by multiple topics in an independent manner, whereas in clustering
    all the words are generated from the same cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generative probabilistic models](img/B05137_08_069.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Exclusive mapping of documents to K-Clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, this can be explained using two topics T = {T[1], T[2]} and
    two clusters c = {c[1], c[2]}.
  prefs: []
  type: TYPE_NORMAL
- en: 'In clustering, the likelihood of the document can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generative probabilistic models](img/B05137_08_072.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If the document has, say, L terms, this can be further expanded as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generative probabilistic models](img/B05137_08_074.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, once you "assume" a cluster, all the words come from that cluster. The
    product of all terms is computed, followed by the summation across all clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In topic modeling, the likelihood of the document can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generative probabilistic models](img/B05137_08_075.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, each term ti can be picked independently from topics and hence summation
    is done inside and the product is done outside.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The inputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A collection of documents following a certain format and structure expressed
    with the following notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D = {*d*1, *d*2, … *d*n}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The number of clusters that need to be modeled or discovered as *k*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k* clusters identified c = {c[1], c[2], … c[k]}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each document, *p(d*[i]*)* is mapped to one of the clusters *k*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic preprocessing steps as discussed previously, such as tokenization, stop
    words removal, stemming, dictionary of words formation, feature extraction (unigrams
    or n-grams, and so on) of terms, feature transformations (LSA), and even feature
    selection. Let *t* be the terms in the final feature set; they correspond to the
    dictionary or vocabulary ![How does it work?](img/B05137_08_154.jpg).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similar to PLSA, we introduce a "latent variable", *z*, which helps us to select
    whether the document belonging to the cluster falls in the range of *z* ={1, 2,
    … *k*} corresponding to the clusters. Let the *?* parameter be the parameter we
    estimate for each latent variable such that *p(?*[i]*)* corresponds to the probability
    of cluster *z = i*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The probability of a document belonging to a cluster is given by *p(?*[i]*)*,
    and every term in the document generated from that cluster is given by *p(t|?*[i]*)*.
    The likelihood equation can be written as:![How does it work?](img/B05137_08_087.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that instead of going through the documents, it is rewritten with terms
    *t* in the vocabulary ![How does it work?](img/B05137_08_154.jpg) raised to the
    number of times that term appears in the document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform the EM algorithm in a similar way to the method we used previously
    to estimate the parameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The E-step of the EM algorithm is used to infer the cluster from which the document
    was generated:![How does it work?](img/B05137_08_090.jpg)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The M-step of the EM algorithm is used to re-estimate the parameters using
    the result of the E-step, as shown here:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_08_091.jpg)![How does it work?](img/B05137_08_093.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'The final probability estimate for each document can be done using either the
    maximum likelihood or by using a Bayesian algorithm with prior probabilities,
    as shown here: ![How does it work?](img/B05137_08_094.jpg) or ![How does it work?](img/B05137_08_095.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Generative-based models have similar advantages to LSA and PLSA, where we get
    a probabilistic score for documents in clusters. By applying domain knowledge
    or priors using cluster size, the assignments can be further fine-tuned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disadvantages of the EM algorithm having to do with getting stuck in local
    optima and being sensitive to the starting point are still true here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance-based text clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most distance-based clustering algorithms rely on the similarity or the distance
    measure used to determine how far apart instances are from each other in feature
    space. Normally in datasets with numeric values, Euclidean distance or its variations
    work very well. In text mining, even after converting unstructured text to structured
    features of terms with numeric values, it has been found that the cosine and Jaccard
    similarity functions perform better.
  prefs: []
  type: TYPE_NORMAL
- en: Often, Agglomerative or Hierarchical clustering, discussed in [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, are used, which can merge documents based on similarity,
    as discussed previously. Merging the documents or groups is often done using single
    linkage, group average linkage, and complete linkage techniques. Agglomerative
    clustering also results in a structure that can be used for information retrieval
    and the searching of documents.
  prefs: []
  type: TYPE_NORMAL
- en: The partition-based clustering techniques k-means and k-medoids accompanied
    by *h* a suitable similarity or distance method are also employed. The issue with
    k-means, as indicated in the discussion on clustering techniques, is the sensitivity
    to starting conditions along with computation space and time. k-medoids are sensitive
    to the sparse data structure and also have computation space and time constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Non-negative matrix factorization (NMF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Non-negative matrix factorization is another technique used to factorize a large
    data-feature matrix into two non-negative matrices, which not only perform the
    dimensionality reduction, but are also easier to inspect. NMF has gained popularity
    for document clustering, and many variants of NMF with different optimization
    functions have now been shown to be very effective in clustering text (*References*
    [15]).
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The inputs are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A collection of documents following a certain format and structure given by
    the notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D = {d[1], d[2], … d[n]}
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Number of clusters that need to be modeled or discovered as *k*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: k clusters identified c = {c[1], c[2], … c[k]} with documents assigned to the
    clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The mathematical details and interpretation of NMF are given in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind NMF is to factorize the input matrix using low-rank approximation,
    as follows:![How does it work?](img/B05137_08_098.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A non-linear optimization function is used as:![How does it work?](img/B05137_08_099.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is convex in W or H, but not in both, resulting in no guarantees of a global
    minimum. Various algorithms that use constrained least squares, such as mean-square
    error and gradient descent, are used to solve the optimization function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The interpretation of NMF, especially in understanding the latent topics based
    on terms, makes it very useful. The input A[m x n] of terms and documents, can
    be represented in low rank approximation as W[m x k] H[k x n] matrices, where
    W[m x k] is the term-topic representation whose columns are NMF basis vectors.
    The non zero elements of column 1 of W, given by W[1], correspond to particular
    terms. Thus, the w[ij] can be interpreted as a basis vector W[i] about the terms
    j. The H[i1] can be interpreted as how much the document given by doc 1 has affinity
    towards the direction of the topic vector W[i].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the paper (*References* [18]) it was clearly shown how the basis vectors
    obtained for the medical abstracts, known as the Medlars dataset, creates highly
    interpretable basis vectors. The highest weighted terms in these basis vectors
    directly correspond to the concept, for example, W[1] corresponds to the topic
    related to "heart" and W[5] is related to "developmental disability".![How does
    it work?](img/B05137_08_112.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 16: From Langville et al (2006) showing some basis vectors for medical
    datasets for interpretability.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'NMF has been shown to be almost equal in performance with top algorithms, such
    as LSI, for information retrieval and queries:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalability, computation, and storage is better in NMF than in LSA or LSI using
    SVD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NMF has a problem with optimization not being global and getting stuck in local
    minima.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NMF generation of factors depends on the algorithms for optimization and the
    parameters chosen, and is not unique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of text clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of labeled datasets, all the external measures discussed in [Chapter
    3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised
    Machine Learning Techniques,* such as F-measure and Rand Index are useful in evaluating
    the clustering techniques. When the dataset doesn't have labels, some of the techniques
    described as the internal measures, such as the Davies–Bouldin Index, R-Squared,
    and Silhouette Index, can be used.
  prefs: []
  type: TYPE_NORMAL
- en: The general good practice is to adapt and make sure similarity between the documents,
    as discussed in this section, is used for measuring closeness, remoteness, and
    spread of the cluster when applied to text mining data. Similarly usage depends
    on the algorithm and some relevance to the problem too. In distance-based partition
    algorithms, the similarity of the document can be computed with the mean vector
    or the centroid. In hierarchical algorithms, similarity can be computed with most
    similar or dissimilar documents in the group.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Named entity recognition** (**NER**) is one of the most important topics
    in information retrieval for text mining. Many complex mining tasks, such as the
    identification of relations, annotations of events, and correlation between entities,
    use NER as the initial component or basic preprocessing step.'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, manual rules-based and regular expression-based techniques were
    used for entity recognition. These manual rules relied on basic pre processing,
    using POS tags as features, along with hand-engineered features, such as the presence
    of capital words, usage of punctuations prior to words, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical learning-based techniques are now used more for NER and its variants.
    NER can be mapped to sequence labeling and prediction problems in Machine Learning.
    BIO notation, where each entity type T has two labels B-T and I-T corresponding
    to beginning and intermediate, respectively, is labeled, and learning involves
    finding the pattern and predicting it in unseen data. The O represents an outside
    or unrelated entity in the sequence of text. The entity type T is further classified
    into Person, Organization, Data, and location in the most basic form.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the two most common algorithms used: generative-based
    hidden Markov models and discriminative-based maximum entropy models.'
  prefs: []
  type: TYPE_NORMAL
- en: Though we are discussing these algorithms in the context of Named Entity Recognition,
    the same algorithms and processes can be used for other NLP tasks such as POS
    Tagging, where tags are associated with a sequence rather than associating the
    NER classes.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov models for NER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hidden Markov models, as explained in [Chapter 6](ch06.html "Chapter 6. Probabilistic
    Graph Modeling"), *Probabilistic Graph Modeling*, are the sequence-based generative
    models that assume an underlying distribution that generates the sequences. The
    training data obtained by labeling sequences with the right NER classes can be
    used to learn the distribution and parameters, so that for unseen future sequences,
    effective predictions can be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training data consists of text sequences x ={x[1], x[2], ... x[n]} where
    each xi is a word in the text sequence and labels for each word are available
    as y ={y[1], y[2], ... y[n]}. The algorithm generates a model so that on testing
    on unseen data, the labels for new sequences can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the simplest form, a Markov assumption is made, which is that the hidden
    states and labels of the sequences are only dependent on the previous state. An
    adaptation to the sequence of words with labels is shown in the following figure:![How
    does it work?](img/B05137_08_116.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 17: Text sequence and labels corresponding to NER in Hidden Markov Chain'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The HMM formulation of the sequence classification helps in estimating the joint
    probability maximized on training data:![How does it work?](img/B05137_08_117.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each *y*[i] is assumed to be generated based on *y*[i–1] and *x*i. The first
    word in the entity is generated conditioned on current and previous labels, that
    is, *y*[i] and *y*[i–1]. If the instance is already a Named entity, then conditioning
    is only on previous instances, that is, *x*[i–1]. Outside words such as "visited"
    and "in" are considered "not a name class".
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The HMM formulation with the forward-backward algorithm can be used to determine
    the likelihood of a sequence of observations with parameters learned from the
    training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: HMMs are good for short sequences, as shown, with one word or term and independence
    assumption. For sequences with entities that have a longer span, the results will
    violate these assumptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HMM needs a large set of training data to estimate the parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum entropy Markov models for NER
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Maximum entropy Markov model** (**MEMM**) is a popular NER technique that
    uses the concept of Markov chains and maximum entropy models to learn and predict
    the named entities (*References* [16] and [17]).'
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training data consists of text sequences x ={x1, x2, ... xn} where each
    xi is a word in the text sequence and labels for each word are available as y
    ={y1, y2, ... yn}. The algorithm generates models so that, on testing on unseen
    data, the labels for new sequences can be generated.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following illustrates how the MEMM method is used for learning named entities.
  prefs: []
  type: TYPE_NORMAL
- en: The features in MEMM can be word features or other types of features, such as
    "isWordCapitalized", and so on, which gives it a bit more context and improves
    performance compared to HMM, where it is only word-based.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, let us look at a maximum entropy model known as a MaxEnt model, which
    is an exponential probabilistic model, but which can also be seen as a multinomial
    logistic regression model. In basic MaxEnt models, given the features {f[1], f[2]
    … f[N]} and classes c[1], c[2] … c[C], weights for these features are learned
    {w[c1], w[c2] … w[cN]} per class using optimization methods from the training
    data, and the probability of a particular class can be estimated as:![How does
    it work?](img/B05137_08_126.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The feature fi is formally written as f[i](c, x), which means the feature f[i]
    for class c and observation x. The f[i](c, x) is generally binary with values
    1/0 in most NER models. Thus, it can be written as:![How does it work?](img/B05137_08_131.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Maximum likelihood based on the probability of prediction across class can
    be used to select a single class:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_08_132.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: For every word, we use the current word, the features from "nearby" words, and
    the predictions on the nearby words to create a joint probability model. This
    is also called local learning as the chunks of test and distribution are learned
    around local features corresponding to the word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mathematically, we see how a discriminative model is created from current word
    and last prediction as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_08_133.jpg)![How does it work?](img/B05137_08_134.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Generalizing for the k features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_08_135.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Thus, in MEMM we compute the probability of the state, which is the class in
    NER, and even though we condition on prediction of nearby words given by y[i–1],
    in general we can use more features, and that is the advantage over the HMM model
    discussed previously:![How does it work?](img/B05137_08_137.jpg)![How does it
    work?](img/B05137_08_138.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 18 : Text Sequences and observation probabilities with labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Viterbi algorithm is used to perform the estimation of class for the word
    or decoding/inferencing in HMM, that is, to get estimates for p(y[i]|y[i–1], X[i])
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the MaxEnt model is used to estimate the weights as before using the
    optimization methods for state changes in general:![How does it work?](img/B05137_08_140.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MEMM has more flexibility in using features that are not just word-based or
    even human-engineered, giving it more richness and enabling its models to be more
    predictive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MEMM can have a range of more than just close words, giving it an advantage
    of detection over larger spans compared to HMM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning and NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the last few years, Deep Learning and its application to various areas of
    NLP has shown huge success and is considered the cutting edge of technology these
    days. The main advantage of using Deep Learning lies in a small subset of tools
    and methods, which are useful in a wide variety of NLP problems. It solves the
    basic issue of feature engineering and carefully created manual representations
    by automatically learning them, and thus solves the issue of having a large number
    of language experts dealing with a wide range of problems, such as text classification,
    sentiment analysis, POS tagging, and machine translation, to name a few. In this
    section, we will try to cover important concepts and research in the area of Deep
    Learning and NLP.
  prefs: []
  type: TYPE_NORMAL
- en: In his seminal paper, Bengio introduced one of the most important building blocks
    for deep learning known as word embedding or word vector (*References* [20]).
    Word embedding can be defined as a parameterized function that maps words to a
    high dimensional vector (usually 25 to 500 dimensions based on the application).
  prefs: []
  type: TYPE_NORMAL
- en: Formally, this can be written as ![Deep learning and NLP](img/B05137_08_141.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: For example, ![Deep learning and NLP](img/B05137_08_142.jpg) and ![Deep learning
    and NLP](img/B05137_08_143.jpg), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network (*R*) whose inputs are the words from sentences or n-grams
    of sentences with binary classification, such as whether the sequence of words
    in n-grams are valid or not, is used to train and learn the *W* and *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning and NLP](img/B05137_08_146.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: A modular neural network learning 5-gram words for valid–invalid
    classification'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R(W(cat),W(sat ),W(on),W(the),W(mat)) = 1(valid)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R(W(cat),W(sat),W(on),W(the),W(mat)) = 0 (Invalid)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The idea of training these sentences or n-grams is not only to learn the correct
    structure of the phrases, but also the right parameters for *W* and *R*. The word
    embeddings can also be projected on to a lower dimensional space, such as a 2D
    space, using various linear and non linear feature reduction/visualization techniques
    introduced in [Chapter 3](ch03.html "Chapter 3. Unsupervised Machine Learning
    Techniques"), *Unsupervised Machine Learning Techniques*, which humans can easily
    visualize. This visualization of word embeddings in two dimensions using techniques
    such as t-SNE discovers important information about the closeness of words based
    on semantic meaning and even clustering of words in the area, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning and NLP](img/B05137_08_149.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: t-SNE representation of a small section of the entire word mappings.
    Numbers in Roman numerals and words are shown clustered together on the left,
    while semantically close words are clustered together on the right.'
  prefs: []
  type: TYPE_NORMAL
- en: Further extending the concepts, both Collobert and Mikolov showed that the side
    effects of learning the word embeddings can be very useful in a variety of NLP
    tasks, such as similar phrase learning (for example, *W("the color is red"))*
    *?* *W("the color is yellow"))*, finding synonyms (for example, *W("nailed"))*
    *?* *W("smashed"))*, analogy mapping (for example, *W("man")?W("woman") then W("king")?W("queen"))*,
    and even complex relationship mapping (for example, *W("Paris")?W("France") then
    W("Tokyo")?W("Japan"))* (*References* [21 and 22]).
  prefs: []
  type: TYPE_NORMAL
- en: The extension of word embedding concepts towards a generalized representation
    that helps us reuse the representation with various NLP problems (with minor extensions)
    has been the main reason for many recent successes of Deep Learning in NLP. Socher,
    in his research, extended the word embeddings concept to produce bilingual word
    embeddings, that is, embed words from two different languages, such as Chinese
    (Mandarin) and English into a shared space (*References* [23]). By learning two
    language word embeddings independently of each other and then projecting them
    in a same space, his work gives us interesting insights into word similarities
    across languages that can be extended for Machine Translation. Socher also did
    interesting work on projecting the images learned from CNNs with the word embedding
    in to the same space for associating words with images as a basic classification
    problem (*References* [24]). Google, around the same time, has also been working
    on similar concepts, but at a larger scale for word-image matching and learning
    (*References* [26]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Extending the word embedding concept to have combiners or association modules
    that can help combine the words, words-phrases, phrases-phrases in all combinations
    to learn complex sentences has been the idea of Recursive Neural Networks. The
    following figure shows how complex association *((the cat)(sat(on (the mat))))*
    can be learned using Recursive Neural Networks. It also removes the constraint
    of a "fixed" number of inputs in neural networks because of the ability to recursively
    combine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning and NLP](img/B05137_08_155.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Recursive Neural Network showing how complex phrases can be learned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recursive Neural Networks have been showing great promise in NLP tasks, such
    as sentiment analysis, where association of one negative word at the start of
    many positive words has an overall negative impact on sentences, as shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deep learning and NLP](img/B05137_08_156.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: A complex sentence showing words with negative (as red circle),
    positive (green circle), and neutral (empty with 0) connected through RNN with
    overall negative sentiment.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of recursive neural networks is now extended using the building
    blocks of encoders and decoders to learn reversible sentence representation—that
    is, reconstructing the original sentence with roughly the same meaning from the
    input sentence (*References* [27]). This has become the central core theme behind
    Neural Machine Translation. Modeling conversations using the encoder-decoder framework
    of RNNs has also made huge breakthroughs (*References* [28]).
  prefs: []
  type: TYPE_NORMAL
- en: Tools and usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now discuss some of the most well-known tools and libraries in Java
    that are used in various NLP and text mining applications.
  prefs: []
  type: TYPE_NORMAL
- en: Mallet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mallet is a Machine Learning toolkit for text written in Java, which comes with
    several natural language processing libraries, including those some for document
    classification, sequence tagging, and topic modeling, as well as various Machine
    Learning algorithms. It is open source, released under CPL. Mallet exposes an
    extensive API (see the following screenshots) to create and configure sequences
    of "pipes" for pre-processing, vectorizing, feature selection, and so on, as well
    as to extend implementations of classification and clustering algorithms, plus
    a host of other text analytics and Machine Learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Mallet](img/B05137_08_157.jpg)![Mallet](img/B05137_08_158.jpg)'
  prefs: []
  type: TYPE_IMG
- en: KNIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KNIME is an open platform for analytics with Open GL licensing with a number
    of powerful tools for conducting all aspects of data science. The Text Processing
    module is available for separate download from KNIME Labs. KNIME has an intuitive
    drag and drop UI with downloadable examples available from their workflow server.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'KNIME: [https://www.knime.org/](https://www.knime.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'KNIME Labs: [https://tech.knime.org/knime-text-processing](https://tech.knime.org/knime-text-processing)'
  prefs: []
  type: TYPE_NORMAL
- en: The platform includes a Node repository that contains all the necessary tools
    to compose your workflow with a convenient nesting of nodes that can easily be
    reused by copying and pasting. The execution of the workflows is simple. Debugging
    errors can take some getting used to, so our recommendation is to take the text
    mining example, use a different dataset as input, and make the workflow execute
    without errors. This is the quickest way to get familiar with the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with mallet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now illustrate the usage of API and Java code to implement Topic Modeling
    to give the user an illustration on how to build a text learning pipeline for
    a problem in Java:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'ParallelTopicModel in Mallet has an API with parameters such as the number
    of topics, alpha, and beta that control the underlying parameter for tuning the
    LDA using Dirichlet distribution. Parallelization is very well supported, as seen
    by the increased number of threads available in the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Topic and term association is shown in the following screenshot as the result
    of running the ParallelTopicModel in Mallet. Clearly, the top terms and association
    of the topics are very well discovered in many cases, such as the classes of exec,
    acq, wheat, crude, corn, and earning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling with mallet](img/B05137_08_159.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Reuters corpus labels each document with one of 10 categories. The aim of
    the experiments in this case study is to employ the techniques of text processing
    learned in this chapter to give structure to these documents using vector space
    modeling. This is done in three different ways, and four classification algorithms
    are used to train and make predictions using the transformed dataset in each of
    the three cases. The open source Java analytics platform KNIME was used for text
    processing and learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the learning techniques for unstructured data, such as text or images,
    classification of the data into different categories given a training set with
    labels is a supervised learning problem. However, since the data is unstructured,
    some statistical or information theoretic means are necessary to extract learnable
    features from the data. In the design of this study, we performed feature representation
    and selection on the documents before using linear, non-linear, and ensemble methods
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset used in the experiments is a version of the Reuters-21578 Distribution
    1.0 Text Categorization Dataset available from the UCI Machine Learning Repository:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Reuters-21578 dataset: [https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection)'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is a Modified-Apte split containing 9,981 documents, each with
    a class label indicating the category of the document. There are 10 distinct categories
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After importing the data file, we performed a series of pre-processing steps
    in order to enrich and transform the data before training any models on the documents.
    These steps can be seen in the screenshot of the workflow created in KNIME. They
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: Punctuation erasure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N char filtering (removes tokens less than four characters in length)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case conversion – convert all to lower case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop word filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prior to the learning step, we sampled the data randomly into a 70-30 split
    for training and testing, respectively. We used five-fold cross-validation in
    each experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Data sampling and transformation](img/B05137_08_160.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows the workflow for the first experiment set, which
    uses a binary vector of features. Data import is followed by a series of pre processing
    nodes, after which the dataset is transformed into a document vector. After adding
    back the target vector, the workflow branches out into four classification tasks,
    each using a five-fold cross-validation setup. Results are gathered in the Scorer
    node.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis and dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conducted three sets of experiments in total. In the first set, after pre
    processing, we used binary vectorization of the terms, which adds a representation
    indicating whether or not a term appeared in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature analysis and dimensionality reduction](img/B05137_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the second experiment, we used the values for relative **Term Frequency**
    (**TF**) for each term, resulting in a value between 0 and 1\.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature analysis and dimensionality reduction](img/B05137_08_163.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the third, we performed feature selection by filtering out terms that had
    a relative TF score of less than 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each of the three sets of experiments, we used two linear classifiers (naïve
    Bayes and SVM using linear kernel) and two non-linear classifiers (Decision Tree
    and AdaBoost with Naïve Bayes as base learner). In text mining classification,
    precision/recall metrics are generally chosen as the evaluation metric over accuracy,
    which is more common in traditional, balanced classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results from the three sets of experiments are given in the tables. Scores
    are averages over all the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary Term Vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.5079 | 0.5281 | 0.5079 | 0.9634 | 0.5087 | 0.7063 | 0.6122
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.4989 | 0.5042 | 0.4989 | 0.9518 | 0.5013 | 0.7427(2) |
    0.6637(2) |'
  prefs: []
  type: TYPE_TB
- en: '| AdaBoost(NB) | 0.5118(2) | 0.5444(2) | 0.5118(2) | 0.9665(2) | 0.5219(2)
    | 0.7285 | 0.6425 |'
  prefs: []
  type: TYPE_TB
- en: '| LibSVM | 0.6032(1) | 0.5633(1) | 0.6032(1) | 0.9808(1) | 0.5768(1) | 0.8290(1)
    | 0.7766(1) |'
  prefs: []
  type: TYPE_TB
- en: 'Relative TF vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.4853 | 0.5480(2) | 0.4853 | 0.9641 | 0.5113(2) | 0.7248 |
    0.6292 |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.4947(2) | 0.4954 | 0.4947(2) | 0.9703(2) | 0.4950 | 0.7403(2)
    | 0.6612(2) |'
  prefs: []
  type: TYPE_TB
- en: '| AdaBoost(NB) | 0.4668 | 0.5326 | 0.4668 | 0.9669 | 0.4842 | 0.6963 | 0.6125
    |'
  prefs: []
  type: TYPE_TB
- en: '| LibSVM | 0.6559(1) | 0.6651(1) | 0.6559(1) | 0.9824(1) | 0.6224(1) | 0.8433(1)
    | 0.7962(1) |'
  prefs: []
  type: TYPE_TB
- en: 'Relative TF vector with threshold filtering (rel TF > 0.01):'
  prefs: []
  type: TYPE_NORMAL
- en: '| Classifier | Recall | Precision | Sensitivity | Specificity | F-measure |
    Accuracy | Cohen''s kappa |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes | 0.4689 | 0.5456(2) | 0.4689 | 0.9622 | 0.4988 | 0.7133 | 0.6117
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree | 0.5008(2) | 0.5042 | 0.5008(2) | 0.9706(2) | 0.5022(2) |
    0.7439(2) | 0.6657(2) |'
  prefs: []
  type: TYPE_TB
- en: '| AdaBoost(NB) | 0.4435 | 0.4992 | 0.4435 | 0.9617 | 0.4598 | 0.6870 | 0.5874
    |'
  prefs: []
  type: TYPE_TB
- en: '| LibSVM | 0.6438(1) | 0.6326(1) | 0.6438(1) | 0.9810(1) | 0.6118(1) | 0.8313(1)
    | 0.7806(1) |'
  prefs: []
  type: TYPE_TB
- en: Analysis of text processing results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The analysis of results obtained from our experiments on the Reuters dataset
    is presented here with some key observations:'
  prefs: []
  type: TYPE_NORMAL
- en: As seen in the first table, with the binary representation of terms, Naïve Bayes
    scores around 0.7, which indicates that the features generated have good discriminating
    power. AdaBoost on the same configuration of Naïve Bayes further improves all
    the metrics, such as precision, recall, F1-measure, and accuracy, by about 2%,
    indicating the advantage of boosting and meta-learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As seen in the first table, non-linear classifiers, such as Decision Tree, do
    only marginally better than linear Naïve Bayes in most metrics. SVM with a linear
    classifier increases accuracy by 17% over linear Naïve Bayes and has better metrics
    similarly in almost all measures. SVM and kernels, which have no issues with higher
    dimensional data, the curse of text classification, are thus one of the better
    algorithms for modeling, and the results confirm this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the representation from binary to TF improves many measures, such as
    accuracy, for linear Naïve Bayes (from 0.70 to 0.72) and SVM (0.82 to 0.84). This
    indeed confirms that TF-based representation in many numeric-based algorithms,
    such as SVM. AdaBoost performance with Naïve Bayes drops in most metrics when
    the underlying classifier Bayes gets stronger in performance, as shown in many
    theoretical and empirical results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, by reducing features using threshold TF > 0.01, as used here, we get
    almost similar or somewhat reduced performance in most classifiers, indicating
    that although certain terms seem rare, they have discriminating power, and reducing
    them has a negative impact.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large proportion of information in the digital world is textual. Text mining
    and NLP are areas concerned with extracting information from this unstructured
    form of data. Several important sub areas in the field are active topics of research
    today and an understanding of these areas is essential for data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Text categorization is concerned with classifying documents into pre-determined
    categories. Text may be enriched by annotating words, as with POS tagging, in
    order to give it more structure for subsequent processing tasks to act on. Unsupervised
    techniques such as clustering can be applied to documents as well. Information
    extraction and named entity recognition help identify information-rich specifics
    such as location, person or organization name, and so on. Summarization is another
    important application for producing concise abstracts of larger documents or sets
    of documents. Various ambiguities of language and semantics such as context, word
    sense, and reasoning make the tasks of NLP challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Transformations of the contents of text include tokenization, stop words removal,
    and word stemming, all of which prepare the corpus by standardizing the content
    so Machine Learning techniques can be applied productively. Next, lexical, semantic,
    and syntactic features are extracted so numerical values can represent the document
    structure more conventionally with a vector space model. Similarity and distance
    measures can then be applied to effectively compare documents for sameness. Dimensionality
    reduction is key due to the large number of features that are typically present.
    The details of the techniques for topic modeling, PLSA and text clustering, and
    named entity recognition are described in this chapter. Finally, the recent techniques
    employing deep learning in various fields of NLP are introduced to the readers.
  prefs: []
  type: TYPE_NORMAL
- en: Mallet and KNIME are two open source Java-based tools that provide powerful
    NLP and Machine Learning capabilities. The case study examines performance of
    different classifiers on the Reuters corpus using KNIME.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: J. B. Lovins (1968). *Development of a stemming algorithm*, Mechanical Translation
    and Computer Linguistic, vol.11, no.1/2, pp. 22-31\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Porter M.F, (1980). *An algorithm for suffix stripping*, Program; 14, 130-137.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ZIPF, H.P., (1949). *Human Behaviour and the Principle of Least Effort*, Addison-Wesley,
    Cambridge, Massachusetts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LUHN, H.P., (1958). *The automatic creation of literature abstracts*', IBM Journal
    of Research and Development, 2, 159-165.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deerwester, S., Dumais, S., Furnas, G., & Landauer, T. (1990), *Indexing by
    latent semantic analysis*, Journal of the American Society for Information Sciences,
    41, 391–407\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977), *Maximum likelihood from
    incomplete data via the EM algorithm*. Journal of the Royal Statistic Society,
    Series B, 39(1), 1–38.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Greiff, W. R. (1998). *A theory of term weighting based on exploratory data
    analysis*. In 21st Annual International ACM SIGIR Conference on Research and Development
    in Information Retrieval, New York, NY. ACM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J/ C. Lai
    (1992), *Class-based n-gram models of natural language*, Computational Linguistics,
    18, 4, 467-479.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Liu, S. Lin, Z. Chen, W.-Y. Ma (2003), *An Evaluation on Feature Selection
    for Text Clustering*, ICML Conference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Yang, J. O. Pederson (1995). *A comparative study on feature selection in
    text categorization*, ACM SIGIR Conference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Salton, G. & Buckley, C. (1998). *Term weighting approaches in automatic text
    retrieval*. Information Processing & Management, 24(5), 513–523\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hofmann, T. (2001). *Unsupervised learning by probabilistic latent semantic
    analysis*. Machine Learning Journal, 41(1), 177–196\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D. Blei, J. Lafferty (2006). *Dynamic topic models*. ICML Conference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'D. Blei, A. Ng, M. Jordan (2003). *Latent Dirichlet allocation*, Journal of
    Machine Learning Research, 3: pp. 993–1022.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: W. Xu, X. Liu, and Y. Gong (2003). *Document-Clustering based on Non-negative
    Matrix Factorization*. Proceedings of SIGIR'03, Toronto, CA, pp. 267-273,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dud´ik M. and Schapire (2006). R. E. *Maximum entropy distribution estimation
    with generalized regularization*. In Lugosi, G. and Simon, H. (Eds.), COLT, Berlin,
    pp. 123– 138, Springer-Verlag,.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: McCallum, A., Freitag, D., and Pereira, F. C. N. (2000). *Maximum Entropy Markov
    Models for Information Extraction and Segmentation*. In ICML, pp. 591–598..
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Langville, A. N, Meyer, C. D., Albright, R. (2006). *Initializations for the
    Nonnegative Factorization*. KDD, Philadelphia, USA
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dunning, T. (1993). *Accurate Methods for the Statistics of Surprise and Coincidence.
    Computational Linguistics*, 19, 1, pp. 61-74.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin (2003). *A Neural Probabilistic
    Language Model*. Journal of Machine Learning Research.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa.
    (2011). *Natural language processing (almost) from scratch*. Journal of Machine
    Learning Research, 12:2493–2537.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Mikolov, K. Chen, G. Corrado and J. Dean (2013). *Efficient Estimation of
    Word Representations in Vector Space*. arXiv:1301.3781v1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R. Socher, Christopher Manning, and Andrew Ng. (2010). *Learning continuous
    phrase representations and syntactic parsing with recursive neural networks*.
    In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. (2011).
    *Semi-supervised recursive autoencoders for predicting sentiment distributions*.
    In EMNLP.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: M. Luong, R. Socher and C. Manning (2013). *Better word representations with
    recursive neural networks for morphology*. CONLL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al (2013).
    *Devise: A deep visual-semantic embedding model*. In NIPS Proceedings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Léon Bottou (2011). From Machine Learning to Machine Reasoning. [https://arxiv.org/pdf/1102.1808v3.pdf](https://arxiv.org/pdf/1102.1808v3.pdf).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cho, Kyunghyun, et al (2014). *Learning phrase representations using rnn encoder-decoder
    for statistical machine translation*. arXiv preprint arXiv:1406.1078.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
