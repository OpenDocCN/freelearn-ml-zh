["```py\nvar images = [UIImage]()\nfor i in 1...3{\n    guard let image = UIImage(named:\"images/joshua_newnham_\\(i).jpg\")\n        else{ fatalError(\"Failed to extract features\") }\n\n    images.append(image)\n}\n\nlet faceIdx = 0 \nlet imageView = UIImageView(image: images[faceIdx])\nimageView.contentMode = .scaleAspectFit\n```", "```py\nlet faceDetectionRequest = VNDetectFaceRectanglesRequest()\nlet faceDetectionRequestHandler = VNSequenceRequestHandler()\n```", "```py\ntry? faceDetectionRequestHandler.perform(\n    [faceDetectionRequest],\n    on: images[faceIdx].cgImage!,\n    orientation: CGImagePropertyOrientation(images[faceIdx].imageOrientation)) \n```", "```py\nif let faceDetectionResults = faceDetectionRequest.results as? [VNFaceObservation]{\n    for face in faceDetectionResults{\n // ADD THE NEXT SNIPPET OF CODE HERE\n    }\n}\n```", "```py\nif let currentImage = imageView.image{\n    let bbox = face.boundingBox\n\n    let imageSize = CGSize(\n        width:currentImage.size.width,\n        height: currentImage.size.height)\n\n    let w = bbox.width * imageSize.width\n    let h = bbox.height * imageSize.height\n    let x = bbox.origin.x * imageSize.width\n    let y = bbox.origin.y * imageSize.height\n\n    let faceRect = CGRect(\n        x: x,\n        y: y,\n        width: w,\n        height: h)\n\n    let invertedY = imageSize.height - (faceRect.origin.y + faceRect.height)\n    let invertedFaceRect = CGRect(\n        x: x,\n        y: invertedY,\n        width: w,\n        height: h)\n\n    imageView.drawRect(rect: invertedFaceRect)\n}\n```", "```py\nimageView.image = images[faceIdx]\n\nlet faceLandmarksRequest = VNDetectFaceLandmarksRequest()\n\ntry? faceDetectionRequestHandler.perform(\n    [faceLandmarksRequest],\n    on: images[faceIdx].cgImage!,\n    orientation: CGImagePropertyOrientation(images[faceIdx].imageOrientation))\n```", "```py\nif let faceLandmarkDetectionResults = faceLandmarksRequest.results as? [VNFaceObservation]{\n    for face in faceLandmarkDetectionResults{\n        if let currentImage = imageView.image{\n            let bbox = face.boundingBox\n\n            let imageSize = CGSize(width:currentImage.size.width,\n                                   height: currentImage.size.height)\n\n            let w = bbox.width * imageSize.width\n            let h = bbox.height * imageSize.height\n            let x = bbox.origin.x * imageSize.width\n            let y = bbox.origin.y * imageSize.height\n\n            let faceRect = CGRect(x: x,\n                                  y: y,\n                                  width: w,\n                                  height: h)\n\n        }\n    }\n}\n```", "```py\nfunc getTransformedPoints(\n    landmark:VNFaceLandmarkRegion2D,\n    faceRect:CGRect,\n    imageSize:CGSize) -> [CGPoint]{\n\n    return landmark.normalizedPoints.map({ (np) -> CGPoint in\n        return CGPoint(\n            x: faceRect.origin.x + np.x * faceRect.size.width,\n            y: imageSize.height - (np.y * faceRect.size.height + faceRect.origin.y))\n    })\n} \n```", "```py\nlet landmarkWidth : CGFloat = 1.5\nlet landmarkColor : UIColor = UIColor.red \n```", "```py\nif let landmarks = face.landmarks?.leftEye {\n    let transformedPoints = getTransformedPoints(\n        landmark: landmarks,\n        faceRect: faceRect,\n        imageSize: imageSize)\n\n    imageView.drawPath(pathPoints: transformedPoints,\n                       closePath: true,\n                       color: landmarkColor,\n                       lineWidth: landmarkWidth,\n                       vFlip: false)\n\n    var center = transformedPoints\n        .reduce(CGPoint.zero, { (result, point) -> CGPoint in\n        return CGPoint(\n            x:result.x + point.x,\n            y:result.y + point.y)\n    })\n\n    center.x /= CGFloat(transformedPoints.count)\n    center.y /= CGFloat(transformedPoints.count)\n    imageView.drawCircle(center: center,\n                         radius: 2,\n                         color: landmarkColor,\n                         lineWidth: landmarkWidth,\n                         vFlip: false)\n}\n\nif let landmarks = face.landmarks?.rightEye {\n    let transformedPoints = getTransformedPoints(\n        landmark: landmarks,\n        faceRect: faceRect,\n        imageSize: imageSize)\n\n    imageView.drawPath(pathPoints: transformedPoints,\n                       closePath: true,\n                       color: landmarkColor,\n                       lineWidth: landmarkWidth,\n                       vFlip: false)\n\n    var center = transformedPoints.reduce(CGPoint.zero, { (result, point) -> CGPoint in\n        return CGPoint(\n            x:result.x + point.x,\n            y:result.y + point.y)\n    })\n\n    center.x /= CGFloat(transformedPoints.count)\n    center.y /= CGFloat(transformedPoints.count)\n    imageView.drawCircle(center: center,\n                         radius: 2,\n                         color: landmarkColor,\n                         lineWidth: landmarkWidth,\n                         vFlip: false)\n} \n```", "```py\nvar center = transformedPoints\n    .reduce(CGPoint.zero, { (result, point) -> CGPoint in\n    return CGPoint(\n        x:result.x + point.x,\n        y:result.y + point.y)\n})\n\ncenter.x /= CGFloat(transformedPoints.count)\ncenter.y /= CGFloat(transformedPoints.count)\nimageView.drawCircle(center: center,\n                     radius: 2,\n                     color: landmarkColor,\n                     lineWidth: landmarkWidth,\n                     vFlip: false)\n```", "```py\nif let landmarks = face.landmarks?.faceContour {\n    let transformedPoints = getTransformedPoints(\n        landmark: landmarks,\n        faceRect: faceRect,\n        imageSize: imageSize)\n\n    imageView.drawPath(pathPoints: transformedPoints,\n                       closePath: false,\n                       color: landmarkColor,\n                       lineWidth: landmarkWidth,\n                       vFlip: false)\n}\n\nif let landmarks = face.landmarks?.nose {\n    let transformedPoints = getTransformedPoints(\n        landmark: landmarks,\n        faceRect: faceRect,\n        imageSize: imageSize)\n\n    imageView.drawPath(pathPoints: transformedPoints,\n                       closePath: false,\n                       color: landmarkColor,\n                       lineWidth: landmarkWidth,\n                       vFlip: false)\n}\n\nif let landmarks = face.landmarks?.noseCrest {\n    let transformedPoints = getTransformedPoints(\n        landmark: landmarks,\n        faceRect: faceRect,\n        imageSize: imageSize)\n\n    imageView.drawPath(pathPoints: transformedPoints,\n                       closePath: false,\n                       color: landmarkColor,\n                       lineWidth: landmarkWidth,\n                       vFlip: false)\n}\n```", "```py\nimageView.image = images[faceIdx]\nlet model = ExpressionRecognitionModelRaw()\n\nif let faceDetectionResults = faceDetectionRequest.results as? [VNFaceObservation]{\n    for face in faceDetectionResults{\n        if let currentImage = imageView.image{\n            let bbox = face.boundingBox\n\n            let imageSize = CGSize(width:currentImage.size.width,\n                                   height: currentImage.size.height)\n\n            let w = bbox.width * imageSize.width\n            let h = bbox.height * imageSize.height\n            let x = bbox.origin.x * imageSize.width\n            let y = bbox.origin.y * imageSize.height\n\n            let faceRect = CGRect(x: x,\n                                  y: y,\n                                  width: w,\n                                  height: h)                        \n        }\n    }\n}\n```", "```py\nextension CIImage{\n}\n```", "```py\npublic func crop(rect:CGRect) -> CIImage?{\n    let context = CIContext()\n    guard let img = context.createCGImage(self, from: rect) else{\n        return nil\n    }\n    return CIImage(cgImage: img)\n}\n```", "```py\nlet ciImage = CIImage(cgImage:images[faceIdx].cgImage!)\n\nlet cropRect = CGRect(\n    x: max(x - (faceRect.width * 0.15), 0),\n    y: max(y - (faceRect.height * 0.1), 0),\n    width: min(w + (faceRect.width * 0.3), imageSize.width),\n    height: min(h + (faceRect.height * 0.6), imageSize.height))\n\nguard let croppedCIImage = ciImage.crop(rect: cropRect) else{\n    fatalError(\"Failed to cropped image\")\n} \n```", "```py\npublic func resize(size: CGSize) -> CIImage {\n    let scale = min(size.width,size.height) / min(self.extent.size.width, self.extent.size.height)\n\n    let resizedImage = self.transformed(\n        by: CGAffineTransform(\n            scaleX: scale,\n            y: scale))\n\n    let width = resizedImage.extent.width\n    let height = resizedImage.extent.height\n    let xOffset = (CGFloat(width) - size.width) / 2.0\n    let yOffset = (CGFloat(height) - size.height) / 2.0\n    let rect = CGRect(x: xOffset,\n                      y: yOffset,\n                      width: size.width,\n                      height: size.height)\n\n    return resizedImage\n        .clamped(to: rect)\n        .cropped(to: CGRect(\n            x: 0, y: 0,\n            width: size.width,\n            height: size.height))\n}\n```", "```py\nlet resizedCroppedCIImage = croppedCIImage.resize(\n    size: CGSize(width:48, height:48))\n```", "```py\npublic func getGrayscalePixelData() -> [UInt8]?{\n    var pixelData : [UInt8]?\n\n    let context = CIContext()\n\n    let attributes = [\n        kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,\n        kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue\n        ] as CFDictionary\n\n    var nullablePixelBuffer: CVPixelBuffer? = nil\n    let status = CVPixelBufferCreate(\n        kCFAllocatorDefault,\n        Int(self.extent.size.width),\n        Int(self.extent.size.height),\n        kCVPixelFormatType_OneComponent8,\n        attributes,\n        &nullablePixelBuffer)\n\n    guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer\n        else { return nil }\n\n    CVPixelBufferLockBaseAddress(\n        pixelBuffer,\n        CVPixelBufferLockFlags(rawValue: 0))\n\n    context.render(\n        self,\n        to: pixelBuffer,\n        bounds: CGRect(x: 0,\n                       y: 0,\n                       width: self.extent.size.width,\n                       height: self.extent.size.height),\n        colorSpace:CGColorSpaceCreateDeviceGray())\n\n    let width = CVPixelBufferGetWidth(pixelBuffer)\n    let height = CVPixelBufferGetHeight(pixelBuffer);\n\n    if let baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) {\n        pixelData = Array<UInt8>(repeating: 0, count: width * height)\n        let buf = baseAddress.assumingMemoryBound(to: UInt8.self)\n        for i in 0..<width*height{\n            pixelData![i] = buf[i]\n        }\n    }\n\n    CVPixelBufferUnlockBaseAddress(\n        pixelBuffer,\n        CVPixelBufferLockFlags(rawValue: 0))\n\n    return pixelData\n}\n```", "```py\npublic func getGrayscalePixelData() -> [UInt8]?{\n    let context = CIContext()\n\n    let attributes = [\n        kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,\n        kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue\n        ] as CFDictionary\n\n    var nullablePixelBuffer: CVPixelBuffer? = nil\n    let status = CVPixelBufferCreate(\n        kCFAllocatorDefault,\n        Int(self.extent.size.width),\n        Int(self.extent.size.height),\n        kCVPixelFormatType_OneComponent8,\n        attributes,\n        &nullablePixelBuffer)\n\n    guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer\n        else { return nil }\n\n    // Render the CIImage to our CVPixelBuffer and return it\n    CVPixelBufferLockBaseAddress(\n        pixelBuffer,\n        CVPixelBufferLockFlags(rawValue: 0))\n\n    context.render(\n        self,\n        to: pixelBuffer,\n        bounds: CGRect(x: 0,\n                       y: 0,\n                       width: self.extent.size.width,\n                       height: self.extent.size.height),\n        colorSpace:CGColorSpaceCreateDeviceGray())        \n\n    CVPixelBufferUnlockBaseAddress(\n        pixelBuffer,\n        CVPixelBufferLockFlags(rawValue: 0))\n}\n```", "```py\nlet width = CVPixelBufferGetWidth(pixelBuffer)\nlet height = CVPixelBufferGetHeight(pixelBuffer);\n\nif let baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) {\n    pixelData = Array<UInt8>(repeating: 0, count: width * height)\n    let buf = baseAddress.assumingMemoryBound(to: UInt8.self)\n    for i in 0..<width*height{\n        pixelData![i] = buf[i]\n    }\n}\n```", "```py\nguard let resizedCroppedCIImageData =\n    resizedCroppedCIImage.getGrayscalePixelData() else{\n        fatalError(\"Failed to get (grayscale) pixel data from image\")\n}\n\nlet scaledImageData = resizedCroppedCIImageData.map({ (pixel) -> Double in\n    return Double(pixel)/255.0\n})\n```", "```py\nguard let array = try? MLMultiArray(shape: [1, 48, 48], dataType: .double) else {\n    fatalError(\"Unable to create MLMultiArray\")\n}\n\nfor (index, element) in scaledImageData.enumerated() {\n    array[index] = NSNumber(value: element)\n}\n```", "```py\nDispatchQueue.global(qos: .background).async {\n    let prediction = try? model.prediction(\n        image: array)\n\n    if let classPredictions = prediction?.classLabelProbs{\n        DispatchQueue.main.sync {\n            for (k, v) in classPredictions{\n                print(\"\\(k) \\(v)\")\n            }\n        }\n    }\n} \n```", "```py\nimport UIKit\nimport Vision\n\nprotocol ImageProcessorDelegate : class{\n    func onImageProcessorCompleted(status: Int, faces:[MLMultiArray]?)\n}\n\nclass ImageProcessor{\n\n    weak var delegate : ImageProcessorDelegate?\n\n    init(){\n\n    }\n\n    public func getFaces(pixelBuffer:CVPixelBuffer){\n        DispatchQueue.global(qos: .background).async {  \n\n    }\n}\n```", "```py\nlet faceDetection = VNDetectFaceRectanglesRequest()\n\nlet faceDetectionRequest = VNSequenceRequestHandler()\n```", "```py\nlet ciImage = CIImage(cvPixelBuffer: pixelBuffer)\nlet width = ciImage.extent.width\nlet height = ciImage.extent.height\n\n// Perform face detection\ntry? self.faceDetectionRequest.perform(\n    [self.faceDetection],\n    on: ciImage) \n\nvar facesData = [MLMultiArray]()\n\nif let faceDetectionResults = self.faceDetection.results as? [VNFaceObservation]{\n    for face in faceDetectionResults{\n\n    }\n}\n```", "```py\nlet bbox = face.boundingBox\n\nlet imageSize = CGSize(width:width,\n                       height:height)\n\nlet w = bbox.width * imageSize.width\nlet h = bbox.height * imageSize.height\nlet x = bbox.origin.x * imageSize.width\nlet y = bbox.origin.y * imageSize.height\n\nlet paddingTop = h * 0.2\nlet paddingBottom = h * 0.55\nlet paddingWidth = w * 0.15\n\nlet faceRect = CGRect(x: max(x - paddingWidth, 0),\n                      y: max(0, y - paddingTop),\n                      width: min(w + (paddingWidth * 2), imageSize.width),\n                      height: min(h + paddingBottom, imageSize.height))\n```", "```py\nif let pixelData = ciImage.crop(rect: faceRect)?\n    .resize(size: CGSize(width:48, height:48))\n    .getGrayscalePixelData()?.map({ (pixel) -> Double in\n        return Double(pixel)/255.0 \n    }){\n    if let array = try? MLMultiArray(shape: [1, 48, 48], dataType: .double)     {\n        for (index, element) in pixelData.enumerated() {\n            array[index] = NSNumber(value: element)\n        }\n        facesData.append(array)\n    }\n}\n```", "```py\nDispatchQueue.main.async {\n    self.delegate?.onImageProcessorCompleted(status: 1, faces: facesData)\n}\n```", "```py\nimport UIKit\nimport Vision\nimport AVFoundation\n\nclass ViewController: UIViewController {\n\n    @IBOutlet weak var previewView: CapturePreviewView!\n\n    @IBOutlet weak var viewVisualizer: EmotionVisualizerView!\n\n    @IBOutlet weak var statusLabel: UILabel!\n\n    let videoCapture : VideoCapture = VideoCapture() \n\n    override func viewDidLoad() {\n        super.viewDidLoad()\n\n        videoCapture.delegate = self\n\n        videoCapture.asyncInit { (success) in\n            if success{\n\n                (self.previewView.layer as! AVCaptureVideoPreviewLayer).session = self.videoCapture.captureSession\n\n                (self.previewView.layer as! AVCaptureVideoPreviewLayer).videoGravity = AVLayerVideoGravity.resizeAspectFill\n\n                self.videoCapture.startCapturing()\n            } else{\n                fatalError(\"Failed to init VideoCapture\")\n            }\n        }\n\n        imageProcessor.delegate = self\n    }\n}\n\nextension ViewController : VideoCaptureDelegate{\n\n    func onFrameCaptured(\n        videoCapture: VideoCapture,\n        pixelBuffer:CVPixelBuffer?,\n        timestamp:CMTime){\n        // Unwrap the parameter pixxelBuffer; exit early if nil\n        guard let pixelBuffer = pixelBuffer else{\n            print(\"WARNING: onFrameCaptured; null pixelBuffer\")\n            return\n        }\n    }\n}\n```", "```py\nlet imageProcessor : ImageProcessor = ImageProcessor()\n\nlet model = ExpressionRecognitionModelRaw()\n```", "```py\nimageProcessor.delegate = self\n```", "```py\nextension ViewController : VideoCaptureDelegate{\n\n    func onFrameCaptured(\n        videoCapture: VideoCapture,\n        pixelBuffer:CVPixelBuffer?,\n        timestamp:CMTime){\n\n        guard let pixelBuffer = pixelBuffer else{\n            print(\"WARNING: onFrameCaptured; null pixelBuffer\")\n            return\n        }\n\n        self.imageProcessor.getFaces(\n pixelBuffer: pixelBuffer)\n    }\n} \n```", "```py\nextension ViewController : ImageProcessorDelegate{\n\n    func onImageProcessorCompleted(\n        status: Int,\n        faces:[MLMultiArray]?){\n        guard let faces = faces else{ return }\n\n        self.statusLabel.isHidden = faces.count > 0\n\n        guard faces.count > 0 else{\n            return\n        }\n\n        DispatchQueue.global(qos: .background).async {\n            for faceData in faces{\n\n                let prediction = try? self.model\n                    .prediction(image: faceData)\n\n                if let classPredictions =\n                    prediction?.classLabelProbs{\n                    DispatchQueue.main.sync {\n                        self.viewVisualizer.update(\n                            labelConference: classPredictions\n                        )\n                    }\n                }\n            }\n        }\n    }\n}\n```"]