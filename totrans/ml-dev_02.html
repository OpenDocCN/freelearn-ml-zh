<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">The Learning Process</h1>
                </header>
            
            <article>
                
<p>In the first chapter, we saw a general overview of the mathematical concepts, history, and areas of the field of machine learning. </p>
<p><span><span>As this book intends to provide a practical but formally correct way of learning, now it's time to explore the general thought process for any machine learning process. These concepts will be pervasive throughout the chapters and will help us to define a common framework of the best practices of the field. </span></span></p>
<p><span><span>The topics we will cover in this chapter are as follows:</span></span></p>
<ul>
<li>Understanding the problem and definitions</li>
<li>Dataset retrieval, preprocessing, and feature engineering</li>
<li>Model definition, training, and evaluation</li>
<li>Understanding results and metrics</li>
</ul>
<p class="mce-root">Every machine learning problem tends to have its own particularities. Nevertheless, as the discipline advances through time, there are emerging patterns of what kind of steps a machine learning process should include, and the best practices for them. The following sections will be a list of these steps, including code examples for the cases that apply.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the problem</h1>
                </header>
            
            <article>
                
<p>When solving machine learning problems, it's important to take time to analyze both the data and the possible amount of work beforehand. This preliminary step is flexible and less formal than all the subsequent ones on this list.</p>
<p>From the definition of machine learning, we know that our final goal is to make the computer learn or generalize a certain behavior or model from a sample set of data. So, the first thing we should do is understand the new capabilities we want to learn.</p>
<p>In the enterprise field, this is the time to have more practical discussions and brainstorms. The main questions we could ask ourselves during this phase could be as follows:</p>
<ul>
<li style="font-weight: 400">What is the real problem we are trying to solve?</li>
<li style="font-weight: 400">What is the current information pipeline?</li>
<li style="font-weight: 400">How can I streamline data acquisition?</li>
<li style="font-weight: 400">Is the incoming data complete, or does it have gaps?</li>
<li style="font-weight: 400">What additional data sources could we merge in order to have more variables to hand?</li>
<li style="font-weight: 400">Is the data release periodical, or can it be acquired in real time?</li>
<li style="font-weight: 400">What should be the minimal representative unit of time for this particular problem?</li>
<li>Does the behavior I try to characterize change in nature, or are its fundamentals more or less stable through time?</li>
</ul>
<p><span>Understanding the problem involves getting on the business knowledge side and looking at all the valuable sources of information that could influence the model. Once identified, the following task will generate an organized and structured set of values, which will be the input to our model. </span></p>
<p><span>Let's proceed to see an example of an initial problem definition, and the thought process of the </span><span>initial analysis</span><span>.</span></p>
<p><span><span>Let's say firm A is a retail chain that wants to be able to predict a certain product's demand on certain dates. This could be a challenging task because it involves human behavior, which has some non-deterministic components.</span></span></p>
<p><span><span>What kind of data input would be needed to build such a model? Of course, we would want the transaction listings for that kind of item. But what if the item is a commodity? If the item depends on the price of soybean or flour, the current and past harvest quantities could enrich the model. If the product is a medium-class item, current inflation and salary changes could also correlate with the current earnings.</span></span></p>
<p><span><span>Understanding the problem involves some business knowledge and looking to gather all the valuable sources of information that could influence the model. In some sense, it is more of an art form, and this doesn't change its importance a little bit.</span></span></p>
<p>Let's then assume that the basics of the problem have been analyzed, and the behavior and characteristics of the incoming data and desired output are clearer. T<span><span>he following task will generate an organized and structured set of values that will be the input to our model. This group of data, after a process of cleaning and adapting, will be called our dataset.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset definition and retrieval</h1>
                </header>
            
            <article>
                
<p>Once we have identified the data sources, the next task is to gather all the tuples or records as a homogeneous set. The format can be a tabular arrangement, a series of real values (such as audio or weather variables), and N-dimensional matrices (a set of images or cloud points), among other types.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The ETL process</h1>
                </header>
            
            <article>
                
<p>The previous stages in the big data processing field evolved over several decades under the name of data mining, and then adopted the popular name of <strong>big data</strong>.</p>
<p>One of the best outcomes of these disciplines is the specification of the <strong>Extraction</strong>, <strong>Transform</strong>, <strong>Load</strong> (<strong>ETL</strong>) process.</p>
<p>This process starts with a mix of many data sources from business systems, then moves to a system that transforms the data into a readable state, and then finishes by generating a data mart with very structured and documented data types.</p>
<p>For the sake of applying this concept, we will mix the elements of this process with the final outcome of a structured dataset, which includes in its final form an additional label column (in the case of supervised learning problems).</p>
<p>This process is depicted in the following diagram: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="355" width="824" src="assets/69cc866b-1119-49a8-b79c-930ee2bc7247.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the ETL process, from raw data to a useful dataset</div>
<p>The diagram illustrates the first stages of the data pipeline, starting with all the organization's data, whether it is commercial transactions, IoT device raw values, or other valuable data sources' information elements, which are commonly in very different types and compositions. The ETL process is in charge of gathering the raw information from them<span> using different software filters</span><span>, applying the necessary transforms to arrange the data in a useful manner, and finally, presenting the data in tabular format (we can think of this as a single database table with a last feature or result column, or a big CSV file with consolidated data). The final result can be conveniently used by the following processes without practically thinking of the many quirks of data formatting, because they have been standardized into a very clear table structure.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading datasets and doing exploratory analysis with SciPy and pandas</h1>
                </header>
            
            <article>
                
<p><span><span>In order to get a practical overview of some types</span></span> of <span><span>dataset formats, we will use</span></span> the <span><span>previously presented Python libraries (SciPy and pandas) </span></span><span>for</span><span> </span><span>this example, </span><span>given their almost universal use. </span></p>
<p><span><span>Let's begin by importing and performing a</span></span> simple <span><span>statistical analysis</span></span> of <span><span>several dataset input formats.</span></span></p>
<div class="packt_infobox"><span>The sample data files will be in the data directory inside each chapter's code directory.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working interactively with IPython</h1>
                </header>
            
            <article>
                
<p><span><span>In this section, we will introduce <strong>Python interactive console</strong>, or <strong>IPython</strong>, a command-</span></span>line <span><span>shell that allows us to explore concepts</span></span> and <span><span>methods in an interactive way.</span></span></p>
<p><span><span>To run IPython, you call it from the command line:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><img height="155" width="524" src="assets/68caabb5-7103-4b92-a604-9deb5c1b1dc4.png"/></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span><span>Here we see IPython executing, and then the initial quick help. The most interesting part is the last line - it will allow you to import libraries and execute commands and will show the resulting objects.</span></span> <span><span>An additional and convenient feature of IPython is that you can redefine variables on the fly to see how the results differ with different inputs.</span></span></p>
<p><span>In the current examples, we are using the standard Python version for the most supported Linux distribution at the time of writing (Ubuntu 16.04). The examples should be equivalent for Python 3.</span></p>
<p><span><span>First of all, let's import pandas and load a sample <kbd>.csv</kbd> file (a very common format with one row per line, and registers). It contains a very famous dataset for classification problems with the dimensions of the attributes of 150 instances of iris plants, with a numerical column indicating the class (1, 2, or 3):</span></span></p>
<pre><strong>In [1]: import pandas as pd #Import the pandas library with pd alias</strong></pre>
<p><span><span>In this line, we import pandas in the usual way, making its method available for use with the <kbd>import</kbd> statement. The <kbd>as</kbd> <span>modifier allows us to use a succinct name for all objects and methods in the library:</span></span></span></p>
<pre><strong>In [2]: df = pd.read_csv ("data/iris.csv") #import iris data as dataframe</strong></pre>
<p><span><span><span>In this line, we use the <kbd>read_csv</kbd> method, allowing pandas to guess the possible item separator for the <kbd>.csv</kbd> file, and storing it in a</span> <kbd>dataframe</kbd> <span>object.</span></span></span></p>
<p><span><span>Let's perform some simple exploration of the dataset:</span></span></p>
<pre><strong>In [3]: df.columns</strong><br/><strong>Out[3]:</strong><br/><strong>Index([u'Sepal.Length', u'Sepal.Width', u'Petal.Length', u'Petal.Width',</strong><br/><strong>u'Species'],</strong><br/><strong>dtype='object')</strong><br/><br/><strong>In [4]: df.head(3)</strong><br/><strong>Out[4]:</strong><br/><strong>5.1 3.5 1.4 0.2 setosa</strong><br/><strong>0 4.9 3.0 1.4 0.2 setosa</strong><br/><strong>1 4.7 3.2 1.3 0.2 setosa</strong><br/><strong>2 4.6 3.1 1.5 0.2 setosa</strong></pre>
<p><span><span>We are now able to see the column names of the dataset and explore the first <em>n</em> instances of it. Looking at the first registers, you can see the varying measures for the <kbd>setosa</kbd> iris class.</span></span></p>
<p><span><span>Now, let's access a particular subset of columns and display the first three elements:</span></span></p>
<pre><strong>In [19]: df[u'Sepal.Length'].head(3)</strong><br/><strong>Out[19]:</strong><br/><strong>0 5.1</strong><br/><strong>1 4.9</strong><br/><strong>2 4.7</strong><br/><strong>Name: Sepal.Length, dtype: float64</strong></pre>
<div class="packt_infobox"><span>Pandas includes many related methods for importing tabulated data formats, such as HDF5 (<kbd>read_hdf</kbd>), JSON (<kbd>read_json</kbd>), and Excel (<kbd>read_excel</kbd>). For a complete list of formats, visit <a href="http://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank">http://pandas.pydata.org/pandas-docs/stable/io.html</a><a href="http://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank">.</a><a href="http://pandas.pydata.org/pandas-docs/stable/io.html" target="_blank"/></span></div>
<p><span><span>In addition to these simple exploration methods, <span>we</span></span></span> will <span><span><span>now use pandas to get all the descriptive statistics concepts we've seen in order to characterize the distribution of the <kbd>Sepal.Length</kbd> column:</span></span></span></p>
<pre><strong>#Describe the sepal length column<br/>print "Mean: " + str (df[u'Sepal.Length'].mean())<br/>print "Standard deviation: " + str(df[u'Sepal.Length'].std())<br/>print "Kurtosis: " + str(df[u'Sepal.Length'].kurtosis())<br/>print "Skewness: " + str(df[u'Sepal.Length'].skew())</strong></pre>
<p>And here are the main metrics of this distribution:</p>
<pre><strong>Mean: 5.84333333333</strong><br/><strong>Standard deviation: 0.828066127978</strong><br/><strong>Kurtosis: -0.552064041316</strong><br/><strong>Skewness: 0.314910956637</strong></pre>
<p>Now we will graphically evaluate the accuracy of these metrics by looking at the histogram of this distribution, this time using the built-in <kbd>plot.hist</kbd> method:</p>
<pre><strong>#Plot the data histogram to illustrate the measures</strong><br/><strong>import matplotlib.pyplot as plt</strong><br/><strong>%matplotlib inline</strong><br/><strong>df[u'Sepal.Length'].plot.hist()</strong></pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="298" width="452" src="assets/8a004d2f-7e5c-456d-bd00-e237fbd9a10d.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Histogram of the Iris Sepal Length</div>
<p class="CDPAlignLeft CDPAlign">As the metrics show, the distribution is right skewed, because the skewness is positive, and it is of the plainly distributed type (has a spread much greater than 1), as the kurtosis metrics indicate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working on 2D data</h1>
                </header>
            
            <article>
                
<p><span><span>Let's stop here for tabular data, and go for 2D data structures. As images are the most commonly used type of data in popular machine learning problems, we</span></span> will <span><span>show you some useful methods included in the SciPy stack.</span></span></p>
<p><span><span>The following code is optimized to run on the Jupyter notebook with inline graphics. You</span></span> will <span><span>find the source code in the source file, <kbd>Dataset_IO.pynb</kbd>:</span></span></p>
<pre>import scipy.misc<br/>from matplotlib import pyplot as plt<br/>%matplotlib inline<br/>testimg = scipy.misc.imread("data/blue_jay.jpg")<br/>plt.imshow( testimg)</pre>
<p><span><span>Importing a single image basically consists of importing the corresponding modules, using the <kbd>imread</kbd> method to read the indicated image into a matrix, and showing it using</span></span> matplotlib<span><span>. The <kbd>%</kbd> starting line corresponds to a parameter modification and indicates that the following</span></span> <kbd>matplotlib</kbd> <span><span>graphics should be shown inline on the notebook, with the following results (the axes correspond to pixel numbers):</span></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img height="249" width="335" src="assets/860f0d8c-37b9-4c19-9172-aa151c1fe3b8.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Initial RGB image loaded </div>
<p><span><span>The testing variable will contain a height * width * channel number array, with all the red, green, and blue values for each image pixel. Let's get this information:</span></span></p>
<pre><span><span>testimg.shape</span></span></pre>
<p><span><span>The interpreter will display the following:</span></span></p>
<pre><span><span>(1416, 1920, 3)</span></span></pre>
<p>We could also try to separate the channels and represent them separately, with red, green, and blue scales, to get an idea of the color patterns in the image:</p>
<pre>plt.subplot(131)<br/>plt.imshow( testimg[:,:,0], cmap="Reds")<br/>plt.title("Red channel")<br/>plt.subplot(132)<br/>plt.imshow( testimg[:,:,1], cmap="Greens")<br/>plt.title("Green channel")<br/>plt.subplot(133)<br/>plt.imshow( testimg[:,:,2], cmap="Blues")<br/>plt.title("Blue channel")</pre>
<p><span><span>In the previous example, we create three subplots indicating the structure and position with a three-digit code. The first indicates the row number, the second indicates the column number, and the last, the plot position on that structure. The <kbd>cmap</kbd> parameter indicates the</span></span> colormap<span><span> </span></span><span>assigned</span><span> </span><span>to each graphic.</span></p>
<p><span><span>The output will be as follows:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><img height="105" width="341" src="assets/f0630ea6-bf26-47b6-b7a0-cad85fc6193e.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Depiction of the separated channels of the sample image.</div>
<div class="packt_infobox"><span>Note that red and green channels share a similar pattern, while the blue tones are predominant in this bird figure. This channel separation could be an extremely rudimentary preliminary way to detect this kind of bird in its habitat.</span></div>
<p><span><span>This section is a simplified introduction to the different methods of loading datasets. In the following chapters, we will see different advanced ways to get the datasets, including </span> <span>loading and training </span><span>the different batches of sample sets.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering</h1>
                </header>
            
            <article>
                
<p>Feature engineering is in some ways one of the most underrated parts of the machine learning process, even though it is considered the cornerstone of the learning process by many prominent figures of the community.</p>
<p>What's the purpose of this process? In short, it takes the raw data from databases, sensors, archives, and so on, and transforms it in a way that makes it easy for the model to generalize. This discipline takes criteria from many sources, including common sense. It's indeed more like an art than a rigid science. It is a manual process, even when some parts of it can be automatized via a group of techniques grouped in the feature extraction field.</p>
<p>As part of this process we also have many powerful mathematical tools and dimensionality reduction techniques, such as <strong>Principal Component Analysis</strong> (<strong>PCA</strong>) and <strong>Autoencoders</strong>, that allow data scientists to skip features that don't enrich the representation of the data in useful ways.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imputation of missing data</h1>
                </header>
            
            <article>
                
<p><span><span>When dealing with not-so-perfect or incomplete datasets, a missing register may not add value to the model in itself, but all the other elements of the row could be useful to</span></span> the <span><span>model. This is especially true when</span></span> the <span><span>model has a high percentage of incomplete values, so no row can be discarded.</span></span></p>
<p><span><span>The main question in this process is <em>"how do you interpret a missing value?"</em> There are many ways, and they usually depend on the problem itself.</span></span></p>
<p><span><span>A very naive approach could be set the value to zero, supposing that the mean of the data distribution is 0. An improved step could be to relate the missing data with the surrounding content, assigning the average of the whole column, or an interval of <em>n</em> elements of the same columns. Another option is to use the column's median or most frequent value.</span></span></p>
<p><span><span>Additionally, there are more advanced techniques, such as robust methods and even k-nearest neighbors, that we won't cover in this book.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One hot encoding</h1>
                </header>
            
            <article>
                
<p>Numerical or categorical information can easily be normally represented by integers, one for each option or discrete result. But there are situations where <kbd>bins</kbd> indicating the current option are preferred. This form of data representation is called <strong>one hot encoding</strong>. <span><span>This encoding</span></span> simply <span><span>transforms a certain input into a</span></span> binary <span><span>array containing only zeros, except for the value indicated by the value of a variable, which will be one.</span></span></p>
<p><span><span>In the simple case of an integer, this will be the representation of the list [1, 3, 2, 4] in one hot encoding:</span></span></p>
<pre><span><span>[[0 1 0 0 0]</span></span><br/> <span><span><span>[0 0 0 1 0]</span></span></span><br/> <span><span><span>[0 0 1 0 0]</span></span></span><br/> <span><span><span>[0 0 0 0 1]]</span></span></span></pre>
<p>Let's perform a simple implementation of a one hot integer encoder for integer arrays, in order to better understand the concept:</p>
<pre>import numpy as np<br/>def get_one_hot(input_vector):<br/>result=[]<br/>for i in input_vector:<br/> newval=np.zeros(max(input_vector))<br/> newval.itemset(i-1,1)<br/> result.append(newval)<br/> return result</pre>
<p><span><span>In this example, we first define the <kbd>get_one_hot</kbd> function, which takes an array as input and returns an array.</span></span></p>
<p><span><span>What we do is take the elements of the arrays one by one, and for each element in it, we generate a zero array with length equal to the maximum value of the array, in order to have space for all possible values. Then we insert <kbd>1</kbd> on the index position indicated by the current value (we subtract <kbd>1</kbd> because we go from</span></span> 1-<span><span>based indexes to</span></span> 0-<span><span>based indexes).</span></span></p>
<p><span><span>Let's try the function we just wrote:</span></span></p>
<pre>get_one_hot([1,5,2,4,3])<br/><br/>#Out:<br/>[array([ 1., 0., 0., 0., 0.]),<br/>array([ 0., 0., 0., 0., 1.]),<br/>array([ 0., 1., 0., 0., 0.]),<br/>array([ 0., 0., 0., 1., 0.]),<br/>array([ 0., 0., 1., 0., 0.])]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset preprocessing</h1>
                </header>
            
            <article>
                
<p>When we first dive into data science, a common mistake is expecting all the data to be very polished and with good characteristics from the very beginning. Alas, that is not the case for a very considerable percentage of cases, for many reasons such as null data, sensor errors that cause outliers and NAN, faulty registers, instrument-induced bias, and all kinds of defects that lead to poor model fitting and that must be eradicated.</p>
<p>The two key processes in this stage are data normalization and feature scaling. This process consists of applying simple transformations called <strong>affine</strong> that map the current unbalanced data into a more manageable shape, maintaining its integrity but providing better stochastic properties and improving the future applied model. The common goal of the standardization techniques is to bring the data distribution closer to a normal distribution, with the following techniques:</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization and feature scaling</h1>
                </header>
            
            <article>
                
<p><span><span>One very important step in </span></span><span><span>dataset preprocessing is normalization</span></span> and <span><span>feature scaling. Data normalization allows our optimization techniques, specially the iterative ones, to converge better, and makes the data more manageable.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalization or standardization</h1>
                </header>
            
            <article>
                
<p><span><span>This technique aims to give the dataset the properties of a normal distribution, that is, a mean of 0 and a standard deviation of 1.</span></span></p>
<p class="CDPAlignLeft CDPAlign"><span><span><span>The way to obtain these properties is by calculating the so-called <em>z</em> scores, based on the dataset samples, with the following formula:</span></span></span></p>
<div style="padding-left: 210px" class="mce-root"><img height="60" width="146" src="assets/72fb7b7b-e99f-4fdc-98bb-f6c65f00ab3e.png"/></div>
<p><span><span>Let's visualize and practice this new concept with the help of scikit-learn, reading a file from the <kbd>MPG</kbd> dataset, which contains city-cycle fuel consumption in miles per gallon, based on the following features: <kbd>mpg</kbd>, <kbd>cylinders</kbd>, <kbd>displacement</kbd>,  <kbd>horsepower</kbd>, <kbd>weight</kbd>, <kbd>acceleration</kbd>, <kbd>model year</kbd>, <kbd>origin</kbd>, and <kbd>car name</kbd>.</span></span></p>
<pre>from sklearn import preprocessing<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>df=pd.read_csv("data/mpg.csv")<br/>plt.figure(figsize=(10,8))<br/>print df.columns<br/>partialcolumns = df[['acceleration', 'mpg']]<br/>std_scale = preprocessing.StandardScaler().fit(partialcolumns)<br/>df_std = std_scale.transform(partialcolumns)<br/>plt.scatter(partialcolumns['acceleration'], partialcolumns['mpg'], color="grey", marker='^')<br/>plt.scatter(df_std[:,0], df_std[:,1])</pre>
<div>The following picture allows us to compare the non normalized and normalized data representations:</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eb067f14-b133-4671-8471-8c49d0dd910c.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Depiction of the original dataset, and its normalized counterpart.</div>
<div class="packt_tip CDPAlignLeft CDPAlign">It's very important to have an account of the denormalizing of the resulting data at the time of evaluation so that you do not lose the representative of the data, especially if the model is applied to regression, when the regressed data won't be useful if not scaled.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model definition</h1>
                </header>
            
            <article>
                
<p>If we wanted to summarize the machine learning process using just one word, it would certainly be models. This is because what we build with machine learning are abstractions or models representing and simplifying reality, allowing us to solve real-life problems based on a model that we have trained on.</p>
<p>The task of choosing which model to use is becoming increasingly difficult, given the increasing number of models appearing almost every day, but you can make general approximations by grouping methods by the type of task you want to perform and also the type of input data, so that the problem is simplified to a smaller set of options.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Asking ourselves the right questions</h1>
                </header>
            
            <article>
                
<p>At the risk of generalizing too much, let's try to summarize a sample decision problem for a model:</p>
<ul>
<li style="font-weight: 400">Are we trying to characterize data by simply grouping information based on its characteristics, without any or a few previous hints? This is the domain of clustering techniques.</li>
<li style="font-weight: 400">The first and most basic question: are we trying to predict the instant outcome of a variable, or to tag or classify data into groups? If the former, we are tackling a regression problem. If the latter, this is the realm of classification problems.</li>
<li style="font-weight: 400">Having the former questions resolved, and opting for any of the options of point 2, we should ask ourselves: is the data sequential, or rather, should we take the sequence in account? Recurrent neural networks should be one of the first options.</li>
<li style="font-weight: 400">Continuing with non-clustering techniques: is the data or pattern to discover spatially located? Convolutional neural networks are a common starting point for this kind of problem.</li>
<li style="font-weight: 400">In the most common cases (data without a particular arrangement), if the function can be represented by a single univariate or multivariate function, we can apply linear, polynomial, or logistic regression, and if we want to upgrade the model, a multilayer neural network will provide support for more complex non-linear solutions.</li>
<li style="font-weight: 400">How many dimensions and variables are we working on? Do we just want to extract the most useful features (and thus data dimensions), excluding the less interesting ones? This is the realm of dimensionality reduction techniques.</li>
<li style="font-weight: 400">Do we want to learn a set of strategies with a finite set of steps aiming to reach a goal? This belongs to the field of reinforcement learning. If none of these classical methods are fit for our research, a very high number of niche techniques appear and should be subject to additional analysis.</li>
</ul>
<div class="packt_infobox">In the following chapters, you will find additional information about how to base your decision on stronger criteria, and finally apply a model to your data. Also, if you see your answers don't relate well with the simple criteria explained in this section, you can check <em>Chapter 8</em>, <em>Recent Models and Developments</em>, for more advanced models.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss function definition</h1>
                </header>
            
            <article>
                
<p class="western">This machine learning process step is also very important because it provides a distinctive measure of the quality of your model, and if wrongly chosen, it could either ruin the accuracy of the model or its efficiency in the speed of convergence.</p>
<p class="western">Expressed in a simple way, the loss function is a function that measures the distance from the model's estimated value to the real expected value.</p>
<p class="western">An important fact that we have to take into account is that the objective of almost all of the models is to minimize the error function, and for this, we need it to be differentiable, and the derivative of the error function should be as simple as possible.</p>
<p class="western">Another fact is that when the model gets increasingly complex, the derivative of the error will also get more complex, so we will need to approximate solutions for the derivatives with iterative methods, one of them being the well-known gradient descent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model fitting and evaluation</h1>
                </header>
            
            <article>
                
<p class="western">In this part of the machine learning process, we have the model and data ready, and we proceed to train and validate our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset partitioning</h1>
                </header>
            
            <article>
                
<p>At the time of training the models, we usually partition all the provided data into three sets: the training set, which will actually be used to adjust the parameters of the models; the validation set, which will be used to compare alternative models applied to that data (it can be ignored if we have just one model and architecture in mind); and the test set, which will be used to measure the accuracy of the chosen model. The proportions of these partitions are normally 70/20/10.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Common training terms –  iteration, batch, and epoch</h1>
                </header>
            
            <article>
                
<p>When <span>training the </span>model, there are some common terms that indicate the different parts of the iterative optimization:</p>
<ul>
<li>An <strong>iteration </strong>defines one instance of calculating the error gradient and adjusting the model parameters. When the data is fed into groups of samples, each one of these groups is called a <strong>batch</strong>.</li>
<li>Batches can include the whole dataset (traditional batching), or include just a tiny subset until the whole dataset is fed forward, called mini-batching. The number of samples per batch is called the <strong>batch size</strong>.</li>
<li>Each pass of the whole dataset is called an <strong>epoch</strong>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of training – online and batch processing</h1>
                </header>
            
            <article>
                
<p>The training process provides many ways of iterating over the datasets and adjusting the parameters of the models according to the input data and error minimization results.</p>
<p>Of course, the dataset can and will be evaluated many times and <span>in a variety of ways </span><span>during the training phase.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameter initialization</h1>
                </header>
            
            <article>
                
<p>In order to assure a good fitting start, the model weights have to be initialized to the most effective values. Neural networks, which normally have a <em>tanh</em> activation function, are mainly sensitive to the range [-1,1], or [0,1]; for this reason, it's important to have the data normalized, and the parameters should also be within that range.</p>
<p><span>The model parameters should have useful initial values for the model to converge. One important decision at the start of training is the initialization values for the model parameters (commonly called <strong>weights</strong>). A canonical initial rule is not initializing variables at 0 because it prevents the models from optimizing, as they do not have a suitable function slope multiplier to adjust. A common sensible standard is to use a normal random distribution for all the values.</span></p>
<p><span>Using NumPy, you would normally initialize a coefficient vector with the following code:</span></p>
<pre><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 1 <br/>dist <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)<br/>&gt;&gt;&gt; dist = np.random.normal(mu, sigma, 10)<br/>&gt;&gt;&gt; print dist<br/>[ 0.32416595 1.48067723 0.23039378 -0.59140674 1.65827372 -0.8241832<br/> 0.86016434 -0.05996878 2.2855467 -0.19759244]<br/></span></pre>
<div class="western packt_tip">One particular source of problems at this stage is setting all of the model's parameters to zero. As many optimization techniques normally multiply the weights by a determinate coefficient to the approximate minimum, multiplying by zero will prevent any change in the model, excepting the bias terms.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model implementation and results interpretation</h1>
                </header>
            
            <article>
                
<p>No model is practical if it can't be used outside the training and test sets. This is when the model is deployed into production.</p>
<p>In this stage, we normally load all the model's operation and trained weights, wait for new unknown data, and when it arrives, we feed it through all the chained functions of the model, informing the outcomes of the output layer or operation via a web service, printing to standard output, and so on.</p>
<p>Then, we will have a final task - to interpret the results of the model in the real world to constantly check whether it works in the current conditions. In the case of generative models, the suitability of the predictions is easier to understand because the goal is normally the representation of a previously known entity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression metrics</h1>
                </header>
            
            <article>
                
<p>For regression metrics, a number of indicators are calculated to give a succinct idea of the fitness of the regressed model. Here is a list of the main metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean absolute error</h1>
                </header>
            
            <article>
                
<p>The <kbd>mean_absolute_error</kbd> function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss, or <em>l1-norm</em> loss.<br/>
If <em>ŷ<sub>i</sub></em> is the predicted value of the <em>i</em>th sample, and <em>y<sub>i</sub></em> is the corresponding true value, then the <strong>mean absolute error</strong> (<strong>MAE</strong>) estimated over <em>n</em> samples is defined as follows:</p>
<div style="padding-left: 150px"><img height="64" width="285" src="assets/addea822-7795-4645-8773-df4c2196e5b0.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Median absolute error</h1>
                </header>
            
            <article>
                
<p>The median absolute error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.<br/>
If <em>ŷ</em> is the predicted value of the <em>i</em>th sample and <em>y<sub>i</sub></em> is the corresponding true value, then the median absolute error estimated over <em>n</em> samples is defined as follows:</p>
<div style="padding-left: 120px"><img height="41" width="407" src="assets/01cce5da-4454-43a3-88e3-30ae07e8d931.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mean squared error</h1>
                </header>
            
            <article>
                
<p>The <strong>mean squared error</strong><span> (</span><strong>MSE</strong><span>)</span> is a risk metric equal to the expected value of the squared (quadratic) error loss.</p>
<p>If <em>ŷ<sub>i</sub></em> is the predicted value of the <em>i</em>th sample and <em>y<sub>i</sub></em> is the corresponding true value, then the MSE estimated over <em>n</em> samples is defined as follows:</p>
<div style="padding-left: 150px"><img height="62" width="271" src="assets/19b41214-110d-4bd7-87a0-6ca71a855c5c.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification metrics</h1>
                </header>
            
            <article>
                
<p>The task of classification implies different rules for the estimation of the error. The advantage we have is that the number of outputs is discrete, so it can be determined exactly whether a prediction has failed or not in a binary way. That leads us to the main indicators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Accuracy</h1>
                </header>
            
            <article>
                
<p>The accuracy calculates either the fraction or the count of correct predictions for a model.<br/>
In multi-label classification, the function returns the subset's accuracy.</p>
<p>If the entire set of predicted labels for a sample strictly matches the true set of labels, then the subset's accuracy is <em>1.0</em>; otherwise, it is <em>0.0</em>.<br/>
If <em>ŷ<sub>i</sub></em> is the predicted value of the <em>i</em>th sample and <em>y<sub>i</sub></em> is the corresponding true value, then the fraction of correct predictions over <em>n</em> samples is defined as follows:</p>
<div style="padding-left: 150px"><img height="74" width="385" src="assets/d11bb0e9-44c1-4ed6-aa2e-2908f6076d06.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Precision score, recall, and F-measure</h1>
                </header>
            
            <article>
                
<div class="math CDPAlignCenter CDPAlign">
<p><strong>Precision</strong> score is as follows:<br/></p>
</div>
<div style="padding-left: 210px"><img height="41" width="120" src="assets/d21665f9-726e-4904-bd22-7ff7a183d4ea.png"/></div>
<p>Here, <em>t<sub>p</sub></em> is the number of true positives and <em>f<sub>p</sub></em> is the number of false positives. The precision is the ability of the classifier to <span>not</span><span> </span><span>label as positive a sample that is negative. The best value is 1 and the worst value is</span> <em>0</em><span>.</span></p>
<p><strong>Recall</strong> is as follows:</p>
<div style="padding-left: 210px"><img height="43" width="101" src="assets/320e3e5d-3413-4616-a921-375e4ca896cf.png"/></div>
<div class="math CDPAlignLeft CDPAlign">
<p>Here, <em>t<sub>p</sub></em> is the number of true positives and <em>f<sub>n</sub></em> is the number of false negatives. The recall can be described as the ability of the classifier to find all the positive samples. Its values range from 1 (optimum) to zero.</p>
<p><strong>F measure</strong> <span>(</span><em>F<sub>β</sub></em> <span>and</span> F<sub>1</sub> <span>measures) can be interpreted as a special kind of mean (weighted harmonic mean) of the precision and recall. A</span>  <em>F<sub>β</sub></em> <span>measure's best value is 1 and its worst score is 0. With</span> <em>β = 1</em><span>,</span> <em>F<sub>β</sub></em> <span>and</span> <em>F<sub>1</sub></em> <span>are equivalent, and the recall and the precision are equally important:</span></p>
</div>
<div style="padding-left: 150px"><img height="45" width="229" src="assets/ca5234a8-9e44-4d0e-a76a-230ae5b8c02b.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrix</h1>
                </header>
            
            <article>
                
<p>Every classification task aims to predict a label or tag for new unknown data. A very efficient way of showing the classification's accuracy is through a confusion matrix, where we show [classified sample, ground truth] pairs and a detailed view of how the predictions are doing.</p>
<p>The expected output should be the main diagonal of the matrix with a 1.0 score; that is, all the expected values should match the real ones.</p>
<p>In the following code example, we will do a synthetic sample of predictions and real values, and generate a confusion matrix of the final data:</p>
<pre>from sklearn.metrics import confusion_matrix<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>y_true = [8,5,6,8,5,3,1,6,4,2,5,3,1,4]<br/>y_pred = [8,5,6,8,5,2,3,4,4,5,5,7,2,6]<br/>y = confusion_matrix(y_true, y_pred)<br/>print y <br/>plt.imshow(confusion_matrix(y_true, y_pred), interpolation='nearest', cmap='plasma')<br/>plt.xticks(np.arange(0,8), np.arange(1,9))<br/>plt.yticks(np.arange(0,8), np.arange(1,9))<br/>plt.show()</pre>
<p>The result will be the following:</p>
<pre>[[0 1 1 0 0 0 0 0]<br/> [0 0 0 0 1 0 0 0]<br/> [0 1 0 0 0 0 1 0]<br/> [0 0 0 1 0 1 0 0]<br/> [0 0 0 0 3 0 0 0]<br/> [0 0 0 1 0 1 0 0]<br/> [0 0 0 0 0 0 0 0]<br/> [0 0 0 0 0 0 0 2]]</pre>
<p>And the final confusion matrix graphic representation for these values will be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="357" width="351" src="assets/af7d43a7-d90f-4114-b495-f48a31597747.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Confusion matrix</div>
<p class="CDPAlignLeft CDPAlign">In the image, we see the high accuracy value in the (<em>5,5</em>) diagonal value with three correct predictions, and the (<em>8,8</em>) value with two. As we can see, the distribution of the accuracy by value can be intuitively extracted just by analyzing the graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering quality measurements</h1>
                </header>
            
            <article>
                
<p>Unsupervised learning techniques, <span>understood as the labeling of data </span>without a ground truth, makes it a bit difficult to implement significant metrics for the models. Nevertheless, there are a number of measures implemented for this kind of technique. In this section, we have a list of the most well-known ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Silhouette coefficient</h1>
                </header>
            
            <article>
                
<p>The <strong>silhouette</strong> <strong>coefficient</strong> is a metric that doesn't need to know the labeling of the dataset. It gives an idea of the separation between clusters.</p>
<p>It is composed of two different elements:</p>
<ul>
<li>The mean distance between a sample and all other points in the same class (<em>a</em>)</li>
<li>The mean distance between a sample and all other points in the nearest cluster (<em>b</em>)</li>
</ul>
<p>The formula for this coefficient <em>s</em> is defined as follows:</p>
<div style="padding-left: 210px"><img height="80" width="162" src="assets/fed0e65c-0325-4353-bcdb-012e59c3a141.png"/></div>
<div class="packt_infobox"><span>The silhouette coefficient is only defined if the number of classes is at least two, and the coefficient for a whole sample set is the mean of the coefficient for all samples.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Homogeneity, completeness, and V-measure</h1>
                </header>
            
            <article>
                
<p>Homogeneity, completeness, and V-measure are three key related indicators of the quality of a clustering operation. In the following formulas, we will use <em>K</em> for the number of clusters, <em>C</em> for the number of classes, <em>N</em> for the total number of samples, and <em>a<sub>ck</sub></em> for the <span>number of elements of class</span> <em><span class="MathJax"><span class="MJX_Assistive_MathML">c</span></span></em> <span>in cluster</span> <em><span class="MathJax"><span class="MJX_Assistive_MathML">k</span></span></em><span>.</span></p>
<p><strong>Homogeneity</strong> is a measure of the ratio of samples of a single class pertaining to a single cluster. The fewer different classes included in one cluster, the better. The lower bound should be 0.0 and the upper bound should be 1.0 (higher is better), and the formulation for it is expressed as follows:</p>
<div style="padding-left: 150px"><img height="68" width="320" src="assets/631fb8c1-f23f-4fdd-90c6-9121e272c095.png"/></div>
<p><strong>Completeness </strong>measures the ratio of the member of a given class that is assigned to the same cluster:</p>
<div style="padding-left: 150px"><img height="67" width="319" src="assets/7398ef4a-b2d6-4845-a62f-bc7f07b530d8.png"/></div>
<p><strong>V-measure</strong> is the harmonic mean of homogeneity and completeness, expressed by the following formula:</p>
<div style="padding-left: 180px"><img height="72" width="219" src="assets/7598e979-d41d-4e76-ae8a-61ed1fc8c857.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we reviewed all the main steps involved in a machine learning process. We will be, indirectly, using them throughout the book, and we hope they help you structure your future work too.</p>
<p>In the next chapter, we will review the programming languages and frameworks that we will be using to solve all our machine learning problems and become proficient with them before starting with the projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Lichman, M. (2013). UCI Machine Learning Repository (<a href="http://archive.ics.uci.edu/ml" target="_blank">http://archive.ics.uci.edu/ml</a>). Irvine, CA: University of California, School of Information and Computer Science.</span></li>
<li><span>Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In <em>Proceedings on the Tenth International Conference of Machine Learning</em>, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.</span></li>
<li><span>Townsend, James T. <em>Theoretical analysis of an alphabetic confusion matrix.</em></span> Attention, Perception, &amp; Psychophysics<span> 9.1 (1971): 40-50.</span></li>
<li>Peter J. Rousseeuw (1987). <em>Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis</em>. Computational and Applied Mathematics 20: 53-65.</li>
<li><span>Kent, Allen, et al, Machine literature searching VIII. <em>Operational criteria for designing information retrieval systems.</em></span> Journal of the Association for Information Science and Technology<span> 6.2 (1955): 93-101.</span></li>
<li><span>Rosenberg, Andrew, and Julia Hirschberg, V-Measure: <em>A Conditional Entropy-Based External Cluster Evaluation Measure</em>.</span> EMNLP-CoNLL<span>. Vol. 7. 2007.</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>