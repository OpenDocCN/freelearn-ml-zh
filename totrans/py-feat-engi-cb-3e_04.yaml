- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing Variable Discretization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discretization is the process of transforming continuous variables into discrete
    features by creating a set of contiguous intervals, also called **bins**, which
    span the range of the variable values. Subsequently, these intervals are treated
    as categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Many machine learning models, such as decision trees and Naïve Bayes, work better
    with discrete attributes. In fact, decision tree-based models make decisions based
    on discrete partitions over the attributes. During induction, a decision tree
    evaluates all possible feature values to find the best cut-point. Therefore, the
    more values the feature has, the longer the induction time of the tree is. In
    this sense, discretization can reduce the time it takes to train the models.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization has additional advantages. Data is reduced and simplified; discrete
    features can be easier to understand by domain experts. Discretization can change
    the distribution of skewed variables; when sorting observations across bins with
    equal-frequency, the values are spread more homogeneously across the range. Additionally,
    discretization can minimize the influence of outliers by placing them at lower
    or higher intervals, together with the remaining **inlier** values of the distribution.
    Overall, discretization reduces and simplifies data, making the learning process
    faster and potentially yielding more accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization can also lead to a loss of information, for example, by combining
    values that are strongly associated with different classes or target values into
    the same bin. Therefore, the aim of a discretization algorithm is to find the
    minimal number of intervals without incurring a significant loss of information.
    In practice, many discretization procedures require the user to input the number
    of intervals into which the values will be sorted. Then, the job of the algorithm
    is to find the cut points for those intervals. Among these procedures, we find
    the most widely used equal-width and equal-frequency discretization methods. Discretization
    methods based on decision trees are, otherwise, able to find the optimal number
    of partitions, as well as the cut points.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization procedures can be classified as **supervised** and **unsupervised**.
    Unsupervised discretization methods only use the variable’s distribution to determine
    the limits of the contiguous bins. On the other hand, supervised methods use target
    information to create the intervals.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss widely used supervised and unsupervised discretization
    procedures that are available in established open source libraries. Among these,
    we will cover equal-width, equal-frequency, arbitrary, k-means, and decision tree-based
    discretization. More elaborate methods, such as ChiMerge and CAIM, are out of
    the scope of this chapter, as their implementation is not yet open source available.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing equal-width discretization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing equal-frequency discretization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discretizing the variable into arbitrary intervals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing discretization with k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing feature binarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using decision trees for discretization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will use the numerical computing libraries `pandas`, `numpy`,
    `matplotlib`, `scikit-learn`, and `feature-engine`. We will also use the `yellowbrick`
    Python open source library, which you can install with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For more details about `yellowbrick`, visit the documentation here:'
  prefs: []
  type: TYPE_NORMAL
- en: https://www.scikit-yb.org/en/latest/index.html
  prefs: []
  type: TYPE_NORMAL
- en: Performing equal-width discretization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Equal-width discretization consists of dividing the range of observed values
    for a variable into *k* equally sized intervals, where *k* is supplied by the
    user. The interval width for the *X* variable is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi
    mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">d</mi><mi
    mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">h</mi><mo>=</mo><mfrac><mrow><mi
    mathvariant="bold-italic">M</mi><mi mathvariant="bold-italic">a</mi><mi mathvariant="bold-italic">x</mi><mfenced
    open="(" close=")"><mi mathvariant="bold-italic">X</mi></mfenced><mo>−</mo><mi
    mathvariant="bold-italic">M</mi><mi mathvariant="bold-italic">i</mi><mi mathvariant="bold-italic">n</mi><mo>(</mo><mi
    mathvariant="bold-italic">X</mi><mo>)</mo></mrow><mi mathvariant="bold-italic">k</mi></mfrac></mrow></mrow></math>](img/20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, if the values of the variable vary between 0 and 100, we can create five
    bins like this: *width = (100-0) / 5 = 20*. The bins will be 0–20, 20–40, 40–60,
    and 80–100\. The first and final bins (0–20 and 80–100) can be expanded to accommodate
    values smaller than 0 or greater than 100 by extending the limits to minus and
    plus infinity.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will carry out equal-width discretization using `pandas`,
    `scikit-learn`, and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the predictor and target variables of the California housing dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid data leakage, we will find the intervals’ limits by using the variables
    in the train set. Then, we will use these limits to discretize the variables in
    train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s divide the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will divide the continuous `HouseAge` variable into 10 intervals using
    `pandas` and the formula described at the beginning of the recipe.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s capture the minimum and maximum values of `HouseAge`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s determine the interval width, which is the variable’s value range divided
    by the number of bins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we execute `print(width)`, we will obtain `5`, which is the size of the intervals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we need to define the interval limits and store them in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we now execute `print(interval_limits)`, we will see the interval limits:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s expand the limits of the first and last intervals to accommodate smaller
    or greater values that we could find in the test set or in future data sources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the DataFrames so we don’t overwrite the original ones,
    which we will need for later steps in the recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s sort the `HouseAge` variable into the intervals that we defined in *step
    6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have set `include_lowest=True` to include the lowest value in the first interval.
    Note that we used the train set to find the intervals and then used those limits
    to sort the variable in both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s print the top `5` observations of the discretized and original variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the `52` value was allocated to the
    46–infinite interval, the `43` value was allocated to the 41–46 interval, and
    so on:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The parentheses and brackets in the intervals indicate whether a value is included
    in the interval or not. For example, the (41, 46] interval contains all values
    greater than 41 and smaller than or equal to 46.
  prefs: []
  type: TYPE_NORMAL
- en: Equal-width discretization allocates a different number of observations to each
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a bar plot with the proportion of observations across the intervals
    of `HouseAge` in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the proportion of observations per
    interval is approximately the same in the train and test sets, but different across
    intervals:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The proportion of observations per interval after the discretization](img/B22396_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – The proportion of observations per interval after the discretization
  prefs: []
  type: TYPE_NORMAL
- en: With `feature-engine`, we can perform equal-width discretization in fewer lines
    of code and for many variables at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the discretizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the discretizer to sort three continuous variables into eight
    intervals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`EqualWidthDiscretiser()` returns an integer indicating whether the value was
    sorted into the first, second, or eighth bin by default. That is the equivalent
    of ordinal encoding, which we described in the *Replacing categories with ordinal
    numbers* recipe of [*Chapter 2*](B22396_02.xhtml#_idTextAnchor182), *Encoding
    Categorical Variables*. To carry out a different encoding with the `feature-engine`
    or `category` `encoders` Python libraries, cast the returned variables as objects
    by setting `return_object` to `True`. Alternatively, make the transformer return
    the interval limits by setting `return_boundaries` to `True`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the discretizer to the train set so that it learns the cut points
    for each variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After fitting, we can inspect the cut points in the `binner_dict_` attribute
    by executing `print(disc.binner_dict_)`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine` will automatically extend the limits of the lower and upper
    intervals to infinite to accommodate potential outliers in future data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discretize the variables in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`EqualWidthDiscretiser()` returns a DataFrame where the selected variables
    are discretized. If we run `test_t.head()`, we will see the following output where
    the original values of `MedInc`, `HouseAge`, and `AveRooms` are replaced by the
    interval numbers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – A DataFrame with three discretized variables: HouseAge, MedInc,
    and AveRooms](img/B22396_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2 – A DataFrame with three discretized variables: HouseAge, MedInc,
    and AveRooms'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s make bar plots with the proportion of observations per interval
    to better understand the effect of equal-width discretization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The intervals contain a different number of observations, as shown in the following
    plots:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Bar plots with the proportion of observations per interval after
    the discretization](img/B22396_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Bar plots with the proportion of observations per interval after
    the discretization
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s implement equal-width discretization with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the classes from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up an equal-width discretizer by setting its `strategy` to `uniform`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`KBinsDiscretiser()` can return the bins as integers by setting `encoding`
    to `''ordinal''` or one-hot encoded by setting `encoding` to `''onehot-dense''`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use `ColumnTransformer()` to restrict the discretization to the selected
    variables from *step 13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `remainder` set to `passthrough`, `ColumnTransformer()` returns all the
    variables in the input DataFrame after the transformation. To return only the
    transformed variables, set `remainder` to `drop`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the discretizer to the train set so that it learns the interval limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s discretize the selected variables in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can inspect the cut points learned by the transformer by executing `ct.named_transformers_["discretizer"].bin_edges_`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`ColumnTransformer()` will append `discretize` to the variables that were discretized
    and `remainder` to those that were not modified.'
  prefs: []
  type: TYPE_NORMAL
- en: We can check the output by executing `test_t.head()`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we sorted the variable values into equidistant intervals. To
    perform discretization with `pandas`, we first found the maximum and minimum values
    of the `HouseAge` variable using the `max()` and `min()` methods. Then, we estimated
    the interval width by dividing the value range by the number of arbitrary bins.
    With the width and the minimum and maximum values, we determined the interval
    limits and stored them in a list. We used this list with pandas `cut()` to sort
    the variable values into the intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Pandas `cut()` sorts the variable into intervals of equal size by default. It
    will extend the variable range by .1% on each side to include the minimum and
    maximum values. The reason why we generated the intervals manually is to accommodate
    potentially smaller or larger values than those seen in the dataset in future
    data sources when we deploy our model.
  prefs: []
  type: TYPE_NORMAL
- en: After discretization, we normally treat the intervals as categorical values.
    By default, pandas `cut()` returns the interval values as ordered integers, which
    is the equivalent of ordinal encoding. Alternatively, we can return the interval
    limits by setting the `labels` parameter to `None`.
  prefs: []
  type: TYPE_NORMAL
- en: To display the number of observations per interval, we created a bar plot. We
    used the pandas `value_counts()` function to obtain the fraction of observations
    per interval, which returns the result in pandas Series, where the index is the
    interval and the counts are the values. To plot these proportions, first, we concatenated
    the train and test set series using the pandas `concat()`function in a DataFrame,
    and then we assigned the `train` and `test` column names to it. Finally, we used
    `plot.bar()` to display a bar plot. We rotated the labels with Matplotlib’s `xticks()`function,
    and added the *x* and *y* legend with `xlabels()` and `ylabel()`, as well as the
    title with `title()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform equal-width discretization with `feature-engine`, we used `EqualWidth``     Discretiser()`, which takes the number of bins and the variables to discretize
    as arguments. With `fit()`, the discretizer learned the interval limits for each
    variable. With `transform()`, it sorted the values into each bin.'
  prefs: []
  type: TYPE_NORMAL
- en: '`EqualWidthDiscretiser()` returns the bins as sorted integers by default, which
    is the equivalent of ordinal encoding. To follow up the discretization with any
    other encoding procedure available in the `feature-engine` or `category encoders`
    libraries, we need to return the bins cast as objects by setting `return_object`
    to `True` when we set up the transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`EqualWidthDiscretiser()` extends the values of the first and last interval
    to minus and plus infinity by default to automatically accommodate smaller and
    greater values than those seen in the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: We followed the discretization with bar plots to display the fraction of observations
    per interval for each of the transformed variables. We could see that if the original
    variable was skewed, the bar plot was also skewed. Note how some of the intervals
    of the `MedInc` and `AveRooms` variables, which had skewed distributions, contained
    very few observations. In particular, even though we wanted to create eight bins
    for `AveRooms`, there were only enough values to create five, and most values
    of the variables were allocated to the first interval.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discretized three continuous variables into equal-width bins with
    `KBinsDiscretizer()` from scikit-learn. To create equal-width bins, we set the
    `strategy` argument to `uniform`. With `fit()`, the transformer learned the limits
    of the intervals, and with `transform()`, it sorted the values into each interval.
  prefs: []
  type: TYPE_NORMAL
- en: We used the `ColumnTransformer()` to restrict the discretization to the selected
    variables, setting the transform output to pandas to obtain a DataFrame after
    the transformation. `KBinsDiscretizer()` can return the intervals as ordinal numbers,
    as we had it do in the recipe, or as one-hot-encoded variables. The behavior can
    be modified through the `encode` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a comparison of equal-width discretization with more sophisticated methods,
    see Dougherty J, Kohavi R, Sahami M. *Supervised and unsupervised discretization
    of continuous features*. In: Proceedings of the 12th international conference
    on machine learning. San Francisco: Morgan Kaufmann; 1995\. p. 194–202.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing equal-frequency discretization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equal-width discretization is intuitive and easy to compute. However, if the
    variables are skewed, then there will be many empty bins or bins with only a few
    values, while most observations will be allocated to a few intervals. This could
    result in a loss of information. This problem can be solved by adaptively finding
    the interval cut-points so that each interval contains a similar fraction of observations.
  prefs: []
  type: TYPE_NORMAL
- en: Equal-frequency discretization divides the values of the variable into intervals
    that carry the same proportion of observations. The interval width is determined
    by **quantiles**. Quantiles are values that divide data into equal portions. For
    example, the median is a quantile that divides the data into two halves. Quartiles
    divide the data into four equal portions, and percentiles divide the data into
    100 equal-sized portions. As a result, the intervals will most likely have different
    widths, but a similar number of observations. The number of intervals is defined
    by the user.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform equal-frequency discretization using `pandas`,
    `scikit-learn`, and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To avoid data leakage, we will determine the interval boundaries or quantiles
    from the train set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s divide the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll use pandas `qcut()`to obtain a discretized copy of the `HouseAge` variable,
    which we will store as a new column in the training set, and the limits of eight
    equal-frequency intervals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you execute `print(interval_limits)`, you’ll see the following interval
    limits: `array([ 1., 14., 18., 24., 29., 34., 37.,` `44., 52.])`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s print the top five observations of the discretized and original variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we see that the `52` value was allocated to the 44–52
    interval, the `43` value was allocated to the 37–44 interval, and so on:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: test_t["House_disc"] = pd.cut(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: x=X_test["HouseAge"],
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: bins=interval_limits,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: include_lowest=True)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a bar plot with the proportion of observations per interval in the
    train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see that the bins contain a similar fraction
    of observations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.4 – The proportion of observations per interval of HouseAge after
    equal-frequency discretization](img/B22396_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – The proportion of observations per interval of HouseAge after equal-frequency
    discretization
  prefs: []
  type: TYPE_NORMAL
- en: With `feature-engine`, we can apply equal-frequency discretization to multiple
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the discretizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the transformer to discretize three continuous variables into
    eight bins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With `return_boundaries=True`, the transformer will return the interval boundaries
    after the discretization. To return the interval number, set it to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fit the discretizer to the train set so that it learns the interval limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`feature-engine` will automatically extend the limits of the lower and upper
    intervals to infinite to accommodate potential outliers in future data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s transform the variables in the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make bar plots with the fraction of observations per interval to better
    understand the effect of equal-frequency discretization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see that the intervals have a similar fraction
    of observations:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.5 – The proportion of observations per interval after  equal-frequency\
    \ discretization of three va\uFEFFriables.](img/B22396_04_5.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The proportion of observations per interval after equal-frequency
    discretization of three variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s carry out equal-frequency discretization with scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the transformer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the discretizer to sort variables into eight equal-frequency bins:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the discretizer to a slice of the train set containing the variables
    from *step 10* so that it learns the interval limits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s `KBinsDiscretiser()` will discretize all the variables in the
    dataset. To discretize only a subset, we apply the transformer to the slice of
    the DataFrame that contains the variables of interest. Alternatively, we can restrict
    the discretization to a subset of variables by using the `ColumnTransformer()`,
    as we did in the *Performing equal-width* *discretization* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make a copy of the DataFrames where we’ll store the discretized variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s transform the variables in both the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can inspect the cut points by executing `disc.bin_edges_`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we sorted the variable values into intervals with a similar
    proportion of observations.
  prefs: []
  type: TYPE_NORMAL
- en: We used pandas `qcut()` to identify the interval limits from the train set and
    sort the values of the `HouseAge` variable into those intervals. Next, we passed
    those interval limits to pandas `cut()` to discretize `HouseAge` in the test set.
    Note that pandas `qcut()`, like pandas `cut()`, returned the interval values as
    ordered integers, which is the equivalent of ordinal encoding,
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: With equal-frequency discretization, many occurrences of values within a small
    continuous range could cause observations with very similar values, resulting
    in different intervals. The problem with this is that it can introduce artificial
    distinctions between data points that are actually quite similar in nature, biasing
    models or subsequent data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: With Feature-engine’s `EqualFrequencyDiscretiser()`, we discretized three variables
    into eight bins. With `fit()`, the discretizer learned the interval limits and
    stored them in the `binner_dict_` attribute. With `transform()`, the observations
    were allocated to the bins.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`EqualFrequencyDiscretiser()` returns an integer indicating whether the value
    was sorted into the first, second, or eighth bin by default. That is the equivalent
    of ordinal encoding, which we described in the *Replacing categories with ordinal
    numbers* recipe in [*Chapter 2*](B22396_02.xhtml#_idTextAnchor182), *Encoding*
    *Categorical Variables*.'
  prefs: []
  type: TYPE_NORMAL
- en: To follow up the discretization with a different type of encoding, we can return
    the variables cast as objects by setting `return_object` to `True` and then use
    any of the `feature-engine` or `category encoders` transformers . Alternatively,
    we can return the interval limits, as we did in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discretized variables into eight equal-frequency bins using `scikit-learn`’s
    `KBinsDiscretizer()`. With `fit()`, the transformer learned the cut points and
    stored them in its `bin_edges_` attribute. With `transform()`, it sorted the values
    into each interval. Note that, differently from `EqualFrequencyDiscretiser()`,
    `KBinsDiscretizer()` will transform all of the variables in the dataset. To avoid
    this, we only applied the discretizer on a slice of the data with the variables
    to modify.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s `KbinsDiscretizer` has the option to return the intervals as
    ordinal numbers or one-hot encoded. The behavior can be modified through the `encode`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Discretizing the variable into arbitrary intervals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In various industries, it is common to group variable values into segments that
    make sense for the business. For example, we might want to group the variable
    age in intervals representing children, young adults, middle-aged people, and
    retirees. Alternatively, we might group ratings into bad, good, and excellent.
    On occasion, if we know that the variable is in a certain scale (for example,
    logarithmic), we might want to define the interval cut points within that scale.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will discretize a variable into pre-defined user intervals
    using `pandas` and `feature-engine`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary Python libraries and get the dataset ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import Python libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot a histogram of the `Population` variable to find out its value range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Population values vary between 0 and approximately 40,000:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.6 – Histogram of the \uFEFFPopulation variable](img/B22396_04_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Histogram of the Population variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a list with arbitrary interval limits, setting the upper limit
    to infinity to accommodate bigger values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a list with the interval limits as strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a copy of the dataset and discretize the `Population` variable into
    the pre-defined limits from *step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s discretize `Population` into pre-defined intervals and name the
    intervals with the labels that we defined in *step 5* for comparison:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s inspect the first five rows of the original and discretized variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the last two columns of the DataFrame, we can see the discretized variables:
    the first one with the strings that we created in *step 5* as values, and the
    second one with the interval limits:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We only need one of the variable versions, either the one with the value range
    or the one with the interval limits. In this recipe, I created both to highlight
    the different options offered by `pandas`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can count and plot the number of observations within each interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see that the number of observations per interval
    varies:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The proportion of observations per interval after the discretization.](img/B22396_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The proportion of observations per interval after the discretization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up the recipe, let’s discretize multiple variables utilizing `feature-engine`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the transformer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s create a dictionary with the variables as keys and the interval limits
    as values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up the discretizer with the limits from *step 11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can go ahead and discretize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `X_t.head()`, we will see the following output, where the `Population`
    and `MedInc` variables have been discretized:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – A DataFrame containing the discretized variables](img/B22396_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – A DataFrame containing the discretized variables
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of using `feature-engine` is that we can discretize multiple variables
    at the same time and apply arbitrary discretization as part of a scikit-learn
    `Pipeline`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we sorted the values of a variable into user-defined intervals.
    First, we plotted a histogram of the `Population` variable to get an idea of its
    value range. Next, we arbitrarily determined the limits of the intervals and captured
    them in a list. We created intervals that included 0–200, 200–500, 500–1000, 1000–2000,
    and more than 2,000 by setting the upper limit to infinite with `np.inf`. Next,
    we created a list with the interval names as strings. Using pandas `cut()` and
    passing the list with the interval limits, we sorted the variable values into
    the pre-defined bins. We executed the command twice; in the first run, we set
    the `labels` argument to `None`, returning the interval limits as a result. In
    the second run, we set the `labels` argument to the list of strings. We captured
    the returned output in two variables: the first one displays the interval limits
    as values and the second one has strings as values. Finally, we counted the number
    of observations per variable using pandas `value_counts()`.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we automated the procedure with `feature-engine`’s `ArbitraryDiscretiser()`.
    This transformer takes a dictionary with the variables to discretize as keys and
    the interval limits in a list as values, and then uses pandas `cut()` under the
    hood to discretize the variables. With `fit()`, the transformer does not learn
    any parameters but checks that the variables are numerical. With `transform()`,
    it discretizes the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Performing discretization with k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aim of a discretization procedure is to find a set of cut points that partition
    a variable into a small number of intervals that have good class coherence. To
    create partitions that group similar observations, we can use clustering algorithms
    such as k-means.
  prefs: []
  type: TYPE_NORMAL
- en: In discretization using k-means clustering, the partitions are the clusters
    identified by the k-means algorithm. The k-means clustering algorithm has two
    main steps. In the initialization step, *k* observations are chosen randomly as
    the initial centers of the *k* clusters, and the remaining data points are assigned
    to the closest cluster. The proximity to the cluster is measured by a distance
    measure, such as the Euclidean distance. In the iteration step, the centers of
    the clusters are re-computed as the average of all of the observations within
    the cluster, and the observations are reassigned to the newly created closest
    cluster. The iteration step continues until the optimal *k* centers are found.
  prefs: []
  type: TYPE_NORMAL
- en: Discretization with k-means requires one parameter, which is *k*, the number
    of clusters. There are a few methods to determine the optimal number of clusters.
    One of them is the elbow method, which we will use in this recipe. This method
    consists of training several k-means algorithms over the data using different
    values of *k*, and then determining the explained variation returned by the clustering.
    In the next step, we plot the explained variation as a function of the number
    of clusters, *k*, and pick the *elbow* of the curve as the number of clusters
    to use. The elbow is the inflection point that indicates that increasing the number
    of *k* further does not significantly increase the variance explained by the model.
    There are different metrics to quantify the explained variation. We will use the
    sum of the square distances from each point to its assigned center.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the Python library `yellowbrick` to determine the
    optimal number of clusters and then carry out k-means discretization with scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start by importing the necessary Python libraries and get the dataset
    ready:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required Python libraries and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a `pandas` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The k-means optimal clusters should be determined using the train set, so let’s
    divide the data into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a list with the variables to transform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s set up a k-means clustering algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, using Yellowbrick’s visualizer and the elbow method, let’s find the optimal
    number of clusters for each variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plots, we see that the optimal number of clusters is six for
    the first two variables and seven for the third:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The number of clusters versus the explained variation for the
    MedInc, HouseAge, and AveRooms variables, from top to bottom](img/B22396_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The number of clusters versus the explained variation for the MedInc,
    HouseAge, and AveRooms variables, from top to bottom
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up a discretizer that uses k-means clustering to create six partitions
    and returns the clusters as one-hot-encoded variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the discretizer to the slice of the DataFrame that contains the variables
    to discretize so that it finds the clusters for each variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we sort the values of all three of the variables into six clusters.
    To discretize `MedInc` and `HouseAge` into six partitions and `AveRooms` into
    seven, we would set up one instance of the discretizer for each variable group
    and use the `ColumnTransformer()` to restrict the discretization to each group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the cut points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each array contains the cut points for the six clusters for `MedInc`, `HouseAge`,
    and `AveRooms`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s obtain the discretized form of the variables in the train test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With `print(test_features)`, we can inspect the DataFrame that is returned
    by the discretizer. It contains 18 binary variables corresponding to the one-hot-encoded
    transformation of the six clusters returned for each of the three numerical variables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can concatenate the result to the original DataFrame using `pandas` and
    then drop the original numerical variables. Alternatively, use the `ColumnTransformer()`
    class to restrict the discretization to the selected variables and add the result
    to the data by setting `remainder` to `"passthrough"`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we performed discretization with k-means clustering. First,
    we identified the optimal number of clusters utilizing the elbow method by using
    Yellowbrick’s `KElbowVisualizer()`.
  prefs: []
  type: TYPE_NORMAL
- en: To perform k-means discretization, we used scikit-learn’s `KBinsDiscretizer()`,
    setting `strategy` to `kmeans` and the number of clusters to six in the `n_bins`
    argument. With `fit()`, the transformer learned the cluster boundaries using the
    k-means algorithm. With `transform()`, it sorted the variable values to their
    corresponding cluster. We set `encode` to `"onehot-dense"`; hence, after the discretization,
    the transformer applied one-hot encoding to the clusters. We also set the output
    of the discretizer to `pandas`, and with that, the transformer returned the one-hot
    encoded version of the clustered variables as a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discretization with k-means is described in the article found in *Palaniappan
    and Hong, Discretization of Continuous Valued Dimensions in OLAP Data Cube*s.
    International Journal of Computer Science and Network Security, VOL.8 No.11, November
    2008\. [http://paper.ijcsns.org/07_book/200811/20081117.pdf](http://paper.ijcsns.org/07_book/200811/20081117.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about the elbow method, visit Yellowbrick’s documentation and
    references at [https://www.scikit-yb.org/en/latest/api/cluster/elbow.html](https://www.scikit-yb.org/en/latest/api/cluster/elbow.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For other ways of determining the fit of k-means clustering, check out the additional
    visualizers in Yellowbrick at [https://www.scikit-yb.org/en/latest/api/cluster/index.html](https://www.scikit-yb.org/en/latest/api/cluster/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing feature binarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some datasets contain sparse variables. Sparse variables are those where the
    majority of the values are 0\. The classical example of sparse variables are those
    derived from text data through the bag-of-words model, where each variable is
    a word and each value represents the number of times the word appears in a certain
    document. Given that a document contains a limited number of words, whereas the
    feature space contains the words that appear across all documents, most documents,
    that is, most rows, will show a value of 0 for most columns. However, words are
    not the sole example. If we think about house details data, the *number of saunas*
    variable will also be 0 for most houses. In summary, some variables have very
    skewed distributions, where most observations show the same value, usually 0,
    and only a few observations show different, usually higher, values.
  prefs: []
  type: TYPE_NORMAL
- en: For a simpler representation of these sparse or highly skewed variables, we
    can binarize them by clipping all values greater than 1 to 1\. In fact, binarization
    is commonly performed on text count data, where we consider the presence or absence
    of a feature rather than a quantified number of occurrences of a word.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform binarization using `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a dataset consisting of a bag of words, which is available in the
    UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Bag+of+Words).
    It is licensed under CC BY 4.0 ([https://creativecommons.org/licenses/by/4.0/legalcode](https://creativecommons.org/licenses/by/4.0/legalcode)).
  prefs: []
  type: TYPE_NORMAL
- en: 'I downloaded and prepared a small bag of words representing a simplified version
    of one of those datasets. You will find this dataset in the accompanying GitHub
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/tree/main/ch04-discretization
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing the libraries and loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries, classes, and datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the bag of words dataset, which contains words as columns and different
    texts as rows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s display histograms to visualize the sparsity of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following histograms, we can see that the different words appear zero
    times in most documents:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.10 – Histograms representing th\uFEFFe number of times each word\
    \ appears in a document](img/B22396_04_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Histograms representing the number of times each word appears
    in a document
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up `binarizer` to clip all values greater than 1 to 1 and return
    DataFrames as a result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s binarize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now we can explore the distribution of the binarized variables by displaying
    the histograms as in *step 3*, or better, by creating bar plots.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s create a bar plot with the number of observations per bin per variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following plot, we can see the binarized variables, where most occurrences
    show the `0` value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.11 – Bar plots containing the number of documents that eithe\uFEFF\
    r show each one of the words or not](img/B22396_04_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Bar plots containing the number of documents that either show
    each one of the words or not
  prefs: []
  type: TYPE_NORMAL
- en: That’s it; now we have a simpler representation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we changed the representation of sparse variables to consider
    the presence or absence of an occurrence, which, in our case, is a word. The data
    consisted of a bag of words, where each variable (column) is a word, each row
    is a document, and the values represent the number of times the word appears in
    a document. Most words do not appear in most documents; therefore, most values
    in the data are 0\. We corroborated the sparsity of our data with histograms.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn’s `Binarizer()` mapped values greater than the threshold, which,
    in our case, was 0, to the `1` value, while values less than or equal to the threshold
    were mapped to 0\. `Binarizer()` has the `fit()` and `transform()` methods, where
    `fit()` does not do anything and `transform()` binarizes the variables.
  prefs: []
  type: TYPE_NORMAL
- en: '`Binarizer()` modifies all variables in a dataset returning NumPy arrays by
    default. To return `pandas` DataFrames instead, we set the transform output to
    `pandas`.'
  prefs: []
  type: TYPE_NORMAL
- en: Using decision trees for discretization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In all previous recipes in this chapter, we determined the number of intervals
    arbitrarily, and then the discretization algorithm would find the interval limits
    one way or another. Decision trees can find the interval limits and the optimal
    number of bins automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree methods discretize continuous attributes during the learning process.
    At each node, a decision tree evaluates all possible values of a feature and selects
    the cut point that maximizes the class separation, or sample coherence, by utilizing
    a performance metric such as entropy or Gini impurity for classification, or the
    squared or absolute error for regression. As a result, the observations end up
    in certain leaves based on whether their feature values are greater or smaller
    than certain cut points.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following figure, we can see the diagram of a decision tree that is
    trained to predict house prices based on the property’s average number of rooms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – A diagram of a decision tree trained to predict house price
    based on the property’s average number of rooms](img/B22396_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – A diagram of a decision tree trained to predict house price based
    on the property’s average number of rooms
  prefs: []
  type: TYPE_NORMAL
- en: Based on this decision tree, houses with a smaller mean number of rooms than
    5.5 will go to the first leaf, houses with a mean number of rooms between 5.5
    and 6.37 will fall into the second leaf, houses with mean values between 6.37
    and 10.77 will end up in the third leaf, and houses with mean values greater than
    10.77 will land in the fourth leaf.
  prefs: []
  type: TYPE_NORMAL
- en: As you see, by design, decision trees can find the set of cut points that partition
    a variable into intervals with good class coherence.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will perform decision tree-based discretization using Feature-engine.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin by importing some libraries and loading the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s import the required Python libraries, classes, and datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the California housing dataset into a `pandas` DataFrame and then
    split it into train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s make a list with the names of the variables to discretize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we execute `print(variables)`, we’ll see the following variable names: `[''MedInc''`,
    `''HouseAge''`, `''AveRooms''`, `''AveBedrms''`, `''``Population''`, `''AveOccup'']`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s set up the transformer to discretize the variables from *step 3*. We
    want the transformer to optimize the hyperparameter’s maximum depth and minimum
    samples per leaf of each tree based on the negative mean square error metric using
    three-fold cross-validation. As the output of the discretization, we want the
    limits of the intervals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s fit the discretizer using the train set so that it finds the best decision
    trees for each of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can inspect the limits of the found intervals for each variable in the `binner_dict_`
    attribute by executing `disc.binner_dict_`. Note how the discretizer appended
    minus and plus infinity to the limits to accommodate smaller and greater values
    than those observed in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discretize the variables and then display the first five rows of the
    transformed training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see the limits of the intervals to which each
    observation was allocated:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The first five rows of the transformed training set containing
    the discretized variables](img/B22396_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The first five rows of the transformed training set containing
    the discretized variables
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to return the interval limits and want to use these datasets to
    train machine learning models, you will need to follow up the discretization with
    one-hot encoding or ordinal encoding. Check the recipes in [*Chapter 2*](B22396_02.xhtml#_idTextAnchor182),
    *Encoding Categorical Variables*, for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of returning the interval limits, we can return the interval number
    to which each observation is allocated by setting up the transformer like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now fit and then transform the training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you now execute `train_t[variables].head()`, you will see integers as a
    result instead of the interval limits:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The first five rows of the transformed training set containing
    the discretized variables](img/B22396_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The first five rows of the transformed training set containing
    the discretized variables
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up the recipe, we will make the discretizer return the predictions
    of the trees as replacement values for the discretized variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set up the transformer to return the predictions, then fit it to the
    training set, and finally transform both datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s explore the number of unique values of the `AveRooms` variable before
    and after the discretization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following output, we can see that the predictions of the decision trees
    are also discrete or finite because the trees contain a finite number of end leaves;
    `7`, while the original variable contained more than 6000 different values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To better understand the structure of the tree, we can capture it into a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When we set the transformer to return integers or bin limits, we will obtain
    the bin limits in the `binner_dict_` attribute. If we set the transformer to return
    the tree predictions, `binner_dict_` will contain the trained tree for each variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can display the tree structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following figure, we can see the values used by the tree to allocate
    samples to the different end leaves based on the mean number of rooms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The structure of the decision tree trained to discretize AveRooms](img/B22396_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The structure of the decision tree trained to discretize AveRooms
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up the recipe, we can plot the number of observations per bin for three
    of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see the number of observations per bin in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 4.16 – The proportion of observations \uFEFFper bin after discretizing\
    \ the variables with decision trees](img/B22396_04_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – The proportion of observations per bin after discretizing the
    variables with decision trees
  prefs: []
  type: TYPE_NORMAL
- en: As evidenced in the plots, discretization with decision trees returns a different
    fraction of observations at each node or bin.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform discretization with decision trees, we used f`eature-engine`’s `Decision``     TreeDiscretiser()`. This transformer fitted a decision tree using each variable
    to discretize as input and optimized the hyperparameters of the model to find
    the best partitions based on a performance metric. It automatically found the
    optimal number of intervals, as well as their limits, returning either the limits,
    the bin number, or the predictions as a result.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The implementation of `feature-engine` is inspired by the winning solution of
    the KDD 2009 data science competition. The winners created new features by obtaining
    predictions of decision trees based on continuous features. You can find more
    details in the *Winning the KDD Cup Orange Challenge with Ensemble Selection*
    article on *page 27* of the article series at [http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf](http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a review of discretization techniques, you might find the following articles
    useful:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dougherty et al, *Supervised and Unsupervised Discretization of Continuous
    Features, Machine Learning: Proceedings of the 12th International Conference*,
    1995, ([https://ai.stanford.edu/~ronnyk/disc.pdf](https://ai.stanford.edu/~ronnyk/disc.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al, *Discretization: An Enabling Technique, Data Mining, and Knowledge
    Discovery*, 6, 393–423, 2002, ([https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique](https://www.researchgate.net/publication/220451974_Discretization_An_Enabling_Technique)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garcia et al, *A Survey of Discretization Techniques: Taxonomy and Empirical
    Analysis in Supervised Learning, IEEE Transactions on Knowledge in Data Engineering
    25 (4)*, 2013, ([https://ieeexplore.ieee.org/document/6152258](https://ieeexplore.ieee.org/document/6152258)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
