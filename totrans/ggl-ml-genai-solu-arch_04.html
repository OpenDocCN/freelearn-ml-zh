<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer060" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-60"><a id="_idTextAnchor059" class="calibre6 pcalibre pcalibre1"/>3</h1>
<h1 id="_idParaDest-61" class="calibre5"><a id="_idTextAnchor060" class="calibre6 pcalibre pcalibre1"/>AI/ML Tooling and the Google Cloud AI/ML Landscape</h1>
<p class="calibre3">In this chapter, we take a look at the various tools in Google Cloud that can be used to implement AI/ML workloads. We start off with a quick overview of some of the fundamental Google Cloud services that function as the building blocks for almost all workloads on Google Cloud. We then progress toward more advanced services that are used specifically for data science and AI/ML workloads. This is the final chapter in the <em class="italic">Basics</em> part of this book, and like the previous two chapters, it provides foundational information that we build upon throughout the book. If you already have knowledge of Google Cloud’s services, this chapter may serve as a refresher for that knowledge. If you are new to Google Cloud, this chapter is an essential part of your learning process, because it introduces concepts that are assumed to be known in the rest of <span>the book.</span></p>
<p class="calibre3">To describe how each tool is used in the context of data science projects, we will refer to the steps of the ML model lifecycle, as laid out in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>. <span><em class="italic">Figure 3</em></span><em class="italic">.1</em> shows a simplified diagram of the ML model lifecycle. In reality, combinations of these steps could be repeated in cycles throughout the model lifecycle, but we will omit those details for simplicity at this point. Our simplified workflow example assumes that the outputs from each step are satisfactory, and we can move on to the next step in the process. It also includes hyperparameter optimization<a id="_idIndexMarker223" class="calibre6 pcalibre pcalibre1"/> in the <strong class="bold">Train </strong><span><strong class="bold">Model</strong></span><span> step.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer045">
<img alt="Figure 3.1: Simplified ML model lifecycle" src="image/B18143_03_1.jpg" class="calibre53"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.1: Simplified ML model lifecycle</p>
<p class="calibre3">This chapter will cover the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Why <span>Google Cloud?</span></li>
<li class="calibre8">Prerequisites for using Google <span>Cloud tools</span></li>
<li class="calibre8">Google Cloud <span>services overview</span></li>
<li class="calibre8">Google Cloud tools for <span>data processing</span></li>
<li class="calibre8">Google <span>Cloud VertexAI</span></li>
<li class="calibre8">Standard industry tools on <span>Google Cloud</span></li>
<li class="calibre8">Choosing the right tool for <span>the job</span></li>
</ul>
<p class="calibre3">Let’s begin with a discussion of why we would want to use Google Cloud for data science and AI/ML use cases in the <span>first place.</span></p>
<h1 id="_idParaDest-62" class="calibre5"><a id="_idTextAnchor061" class="calibre6 pcalibre pcalibre1"/>Why Google Cloud?</h1>
<p class="calibre3">Google has been a<a id="_idIndexMarker224" class="calibre6 pcalibre pcalibre1"/> well-known leader in the AI/ML space for a very long time. They have contributed a lot to the AI/ML industry, through countless research papers, publications, and donations of AI/ML libraries to the open source community, such as TensorFlow, one of the most widely used ML libraries of all time. Their search and advertising algorithms have been leading their respective industries for years, and their peer organizations, such as DeepMind, dedicate their entire existence to pure <span>AI/ML research.</span></p>
<p class="calibre3">Google has also been spearheading initiatives <a id="_idIndexMarker225" class="calibre6 pcalibre pcalibre1"/>such as <strong class="bold">Ethical AI</strong>, championing the concepts of fairness and explainability to ensure that AI is held accountable and used only for purposes that are beneficial to humans. AI/ML is not something that Google is trying to use, but rather it is a core tenet of <span>Google’s business.</span></p>
<p class="calibre3">A significant testament to Google Cloud’s leadership in this space was when Gartner officially recognized them as a leader in the 2022 Gartner® Magic Quadrant™ for Cloud AI Developer Services. Google Cloud provides a wide array of services and tools for implementing AI/ML use cases and embraces open source and third-party solutions in order to provide the broadest selection possible to their customers. By using Google Cloud for AI/ML workloads, you can benefit from the decades of AI/ML research performed, and expertise gained, by Google in <span>this space.</span><a id="_idTextAnchor062" class="calibre6 pcalibre pcalibre1"/></p>
<h1 id="_idParaDest-63" class="calibre5"><a id="_idTextAnchor063" class="calibre6 pcalibre pcalibre1"/>Prerequisites for using Google Cloud tools and services</h1>
<p class="calibre3">This section is<a id="_idIndexMarker226" class="calibre6 pcalibre pcalibre1"/> going to be pretty simple because Google Cloud makes it very easy to get started in trying out its services. If you have a Gmail account, then you pretty much already have everything you need to get started on Google Cloud. As a generous bonus, Google Cloud gives USD $300 in credits to new customers, plus additional free credits to new customers who verify their business email addresses. You can use those free credits to explore and evaluate Google Cloud’s various services. Also, many of Google Cloud’s services provide a <strong class="bold">Free Tier</strong>, which allows you to use those services free of charge up to their specified free usage limit, the details of which you can find in the Google Cloud <span>documentation (</span><a href="https://cloud.google.com/free/docs/free-cloud-features" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/free/docs/free-cloud-features</span></a><span>).</span></p>
<p class="calibre3">If you need to create a new Google Cloud account, you can sign up at <a href="https://console.cloud.google.com/freetrial" class="calibre6 pcalibre pcalibre1">https://console.cloud.google.com/freetrial</a>, and when you need to go beyond free usage, you can upgrade to a paid Cloud <span>Billing account.</span></p>
<p class="calibre3">After you’ve created and logged into your account, you can start using the Google Cloud services that we’ll be using in this book, and many more. When you first try to use a Google Cloud service, you may need to enable the API for that service. This is a simple, one-click action that you only need to perform once in each Google Cloud project. <span><em class="italic">Figure 3</em></span><em class="italic">.2</em> shows an example of the page that’s displayed when you first try to use the Google Filestore service (we will describe Filestore in more detail later in this chapter). You can simply click the <strong class="bold">Enable</strong> button to enable <span>the API.</span></p>
<p class="callout-heading">Definition</p>
<p class="callout">A Google Cloud project organizes all your Google Cloud resources. It consists of a set of users; a set of APIs; and billing, authentication, and monitoring settings for those APIs. All Google Cloud resources, along with user permissions for accessing them, reside in <span>a project.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer046">
<img alt="Figure 3.2: Enabling a Google Cloud API on first use" src="image/B18143_03_2.jpg" class="calibre54"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.2: Enabling a Google Cloud API on first us<a id="_idTextAnchor064" class="calibre6 pcalibre pcalibre1"/>e</p>
<h2 id="_idParaDest-64" class="calibre9"><a id="_idTextAnchor065" class="calibre6 pcalibre pcalibre1"/>Security, privacy, and compliance</h2>
<p class="calibre3">The first thing we <a id="_idIndexMarker227" class="calibre6 pcalibre pcalibre1"/>need to think about when we’ve <a id="_idIndexMarker228" class="calibre6 pcalibre pcalibre1"/>created <a id="_idIndexMarker229" class="calibre6 pcalibre pcalibre1"/>our Google Cloud account is security. This also extends to privacy and compliance, and these are hot topics in the data analytics and AI/ML industries today because your customers want to know that their data is being handled securely. Fortunately, these topics are major priorities for Google Cloud, and as a result, Google Cloud provides a plethora of default controls and dedicated services to facilitate and uphold these priorities. We will introduce some of the important concepts and related services briefly here, and we will dive deeper into these topics in later chapters of <span>this b<a id="_idTextAnchor066" class="calibre6 pcalibre pcalibre1"/>ook.</span></p>
<h3 class="calibre11">Who has access to what?</h3>
<p class="calibre3">The first topic to <a id="_idIndexMarker230" class="calibre6 pcalibre pcalibre1"/>discuss in the context of security, privacy, and compliance is identity and access management; that is, identifying and controlling who has access to which resources in your Google Cloud environments. Google Cloud provides the <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) service <a id="_idIndexMarker231" class="calibre6 pcalibre pcalibre1"/>for this purpose. This service enables you to define identities such as users and groups of users, and permissions with regard to accessing Google Cloud resources. For every attempted action on Google Cloud, whether it’s to read an object from storage or to run a piece of code as a cloud function, the permissions associated with that action, the resource upon which the action would act, and the invoking identity would be evaluated by Google Cloud IAM, and the action would only be permitted if the correct combination of permissions has been applied to all relevant identities and resources. For additional convenience, you can integrate Google Cloud IAM with external <strong class="bold">Identity Providers </strong>(<strong class="bold">IdPs</strong>) and <a id="_idIndexMarker232" class="calibre6 pcalibre pcalibre1"/>directories, such as <span>Active Dire<a id="_idTextAnchor067" class="calibre6 pcalibre pcalibre1"/>ctory.</span></p>
<h3 class="calibre11">Data security</h3>
<p class="calibre3">Google Cloud<a id="_idIndexMarker233" class="calibre6 pcalibre pcalibre1"/> encrypts all data at rest by default. You can control the keys that are used to encrypt your data by using <strong class="bold">Customer-Managed Encryption Keys</strong> (<strong class="bold">CMEKs</strong>), or you can let Google Cloud manage all<a id="_idIndexMarker234" class="calibre6 pcalibre pcalibre1"/> that functionality for you. Regarding data in transit, Google has built global networks with stringent security controls, and TLS encryption is used to protect data that is being transported throughout these global networks. Google Cloud also enables you to encrypt your data even when the data is actively in use, through Confidential Computing, which uses a hardware-based <strong class="bold">Trusted Execution Environment</strong> (<strong class="bold">TEE</strong>). TEEs are secure and isolated <a id="_idIndexMarker235" class="calibre6 pcalibre pcalibre1"/>environments that prevent unauthorized access or modification of applications and data while they a<a id="_idTextAnchor068" class="calibre6 pcalibre pcalibre1"/>re <span>in use.</span></p>
<h3 class="calibre11">Infrastructure security</h3>
<p class="calibre3">In addition to <a id="_idIndexMarker236" class="calibre6 pcalibre pcalibre1"/>Google Cloud’s state-of-the-art infrastructure security controls, Google Cloud provides tools to help prevent and detect potential security threats and vulnerabilities. For example, you can use Cloud Firewall and Cloud Armor to prevent <strong class="bold">Distributed Denial-of-Service</strong> (<strong class="bold">DDoS</strong>) and<a id="_idIndexMarker237" class="calibre6 pcalibre pcalibre1"/> common OWASP threats. You can use <a id="_idIndexMarker238" class="calibre6 pcalibre pcalibre1"/>Chronicle, Security Command Center, and Mandiant, for <strong class="bold">Security Incident and Event Monitoring</strong> (<strong class="bold">SIEM</strong>), <strong class="bold">Security Orchestration Automation and Response</strong> (<strong class="bold">SOAR</strong>), intrusion detection, and <a id="_idIndexMarker239" class="calibre6 pcalibre pcalibre1"/>threat intelligence. In addition to all of these Google Cloud services, you can use third-party<a id="_idIndexMarker240" class="calibre6 pcalibre pcalibre1"/> observability and reporting services such as Splunk o<a id="_idTextAnchor069" class="calibre6 pcalibre pcalibre1"/>n <span>Google Cloud.</span></p>
<h3 class="calibre11">Compliance</h3>
<p class="calibre3">Google Cloud provides<a id="_idIndexMarker241" class="calibre6 pcalibre pcalibre1"/> audit data that tracks the actions being performed by identities on resources in your environments, which is important for compliance reasons. Google Cloud participates in formal compliance programs such as FedRamp, SOC2, and SOC3, and supports compliance standards such<a id="_idIndexMarker242" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">Payment Card Industry Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>), and multiple ISO/IEC international standards. You can see additional details regarding Google Cloud’s compliance program participation <span>at </span><a href="https://cloud.google.com/security/compliance" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/sec<span id="_idTextAnchor070"/>urity/compliance</span></a><span>.</span></p>
<h3 class="calibre11">Interacting with Google Cloud services</h3>
<p class="calibre3">There are <a id="_idIndexMarker243" class="calibre6 pcalibre pcalibre1"/>numerous ways in which you can interact with Google Cloud services. At a high level, you can either use the <strong class="bold">Graphical User Interface</strong> (<strong class="bold">GUI</strong>), the <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>), or the API. We <a id="_idIndexMarker244" class="calibre6 pcalibre pcalibre1"/>explore <a id="_idIndexMarker245" class="calibre6 pcalibre pcalibre1"/>each of these options in more det<a id="_idTextAnchor071" class="calibre6 pcalibre pcalibre1"/>ail in <span>this section.</span></p>
<h3 class="calibre11">Console</h3>
<p class="calibre3">One of the most<a id="_idIndexMarker246" class="calibre6 pcalibre pcalibre1"/> straightforward ways to interact with Google Cloud services is via the Google Cloud console, which provides a GUI. You can access the console <span>at </span><a href="https://console.cloud.google.com/" class="calibre6 pcalibre pcalibre1"><span>https://console.cloud.google.com/</span></a><span>.</span></p>
<p class="calibre3">The console enables you to perform actions in Google Cloud by clicking around in a web-based interface in your browser. For example, you create a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) by clicking <strong class="bold">Compute Engine</strong> in the products menu, and then going to the VM instances<a id="_idIndexMarker247" class="calibre6 pcalibre pcalibre1"/> page and clicking <strong class="bold">Create an instance</strong> to specify the desired properties of your VM, as shown in <span><em class="italic">Figure 3</em></span><span><em class="italic">.3</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer047">
<img alt="Figure 3.3: Creating a VM in the Google Cloud console" src="image/B18143_03_3.jpg" class="calibre55"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.3: Creating a VM in the<a id="_idTextAnchor072" class="calibre6 pcalibre pcalibre1"/> Google Cloud console</p>
<h3 class="calibre11">The gcloud CLI</h3>
<p class="calibre3">If you prefer to <a id="_idIndexMarker248" class="calibre6 pcalibre pcalibre1"/>use a CLI, Google Cloud has built a tool<a id="_idIndexMarker249" class="calibre6 pcalibre pcalibre1"/> named <strong class="bold">gcloud</strong>, which enables you to interact with Google Cloud services by executing text-based commands. This is particularly useful if you wish to automate sequences of Google Cloud service API actions by composing scripts that run multiple commands in order. For example, you could create a Bash script that contains multiple commands, and you could execute that script either manually or on a periodic schedule if it contains actions that frequently need to be repeated. This approach would be suitable for ad hoc automation that can be implemented with little effort. There are other ways to automate more complex sequences of actions on Google Cloud, which we will explore in <span>later chapters.</span></p>
<p class="calibre3">The following is an example of a <strong class="source-inline">gcloud</strong> command. This command will enable the API for <strong class="source-inline">SERVICE_NAME</strong>, which is a placeholder for the name of the Google Cloud service with which we want <span>to interact:</span></p>
<pre class="source-code">
gcloud services enable SERVICE_NAME</pre> <p class="calibre3">For example, to enable the Filestore API, rather than clicking the <strong class="bold">Enable</strong> button in the Google Cloud console, as we described earlier in this chapter, we could instead use the following <strong class="source-inline">gcloud</strong> <span>CLI command:</span></p>
<pre class="source-code">
gcloud services enable file.googleapis.com</pre> <p class="calibre3">In this case, <strong class="source-inline">file</strong> is the command-line name for the Google Cloud Filestore service (the full service name <span>is </span><span><strong class="source-inline">file.googleapis.com</strong></span><span>).</span></p>
<p class="calibre3">To use the gcloud CLI, you can install it on any machine on which you wish to run it, as it supports many different operating systems, such as Linux, macOS, and Windows, or you can use Google <a id="_idIndexMarker250" class="calibre6 pcalibre pcalibre1"/>Clou<a id="_idTextAnchor073" class="calibre6 pcalibre pcalibre1"/>d Shell, <span>described next.</span></p>
<h4 class="calibre20">Google Cloud Shell</h4>
<p class="calibre3">Google Cloud Shell<a id="_idIndexMarker251" class="calibre6 pcalibre pcalibre1"/> is a very convenient way to use the gcloud CLI and interact with Google Cloud APIs. It’s a tool that provides a Linux-based environment in which you can issue commands to Google Cloud <span>service APIs.</span></p>
<p class="calibre3">You can open the Cloud Shell by clicking on the <img alt="" role="presentation" src="image/icon.png" class="calibre56"/> symbol in the top-right corner of the Google Cloud console screen, as shown in <span><em class="italic">Figure 3</em></span><span><em class="italic">.4</em></span><span>:</span></p>
<p class="calibre3"> </p>
<div class="calibre2">
<div class="img---figure" id="_idContainer049">
<img alt="Figure 3.4: Activating Google Cloud Shell" src="image/B18143_03_4.jpg" class="calibre57"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.4: Activating Google Cloud Shell</p>
<p class="calibre3">The terminal will <a id="_idIndexMarker252" class="calibre6 pcalibre pcalibre1"/>then appear at the bottom of the screen, as shown in <span><em class="italic">Figure 3</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer050">
<img alt="Figure 3.5: Google Cloud Shell" src="image/B18143_03_5.jpg" class="calibre58"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.5: Google Cloud Shell</p>
<p class="calibre3">When you first try to use Cloud Shell, you need to authorize it to interact with Google Cloud service<a id="_idIndexMarker253" class="calibre6 pcalibre pcalibre1"/> APIs, as depicted in <span><em class="italic">Figure 3</em></span><span><em class="italic">.6</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer051">
<img alt="Figure 3.6: Authorizing Google Cloud Shell" src="image/B18143_03_6.jpg" class="calibre59"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.6: Authoriz<a id="_idTextAnchor074" class="calibre6 pcalibre pcalibre1"/>ing Google Cloud Shell</p>
<h3 class="calibre11">API access</h3>
<p class="calibre3">The most low-level <a id="_idIndexMarker254" class="calibre6 pcalibre pcalibre1"/>method for interacting with Google Cloud services is by programmatically invoking their APIs directly. This method differs from the GUI and CLI access because it is not intended for direct human interaction, but rather it is suitable for more advanced use cases, such as interacting with Google Cloud services via your application software. As an example, let’s consider an application that saves users’ photos in the cloud. When new users sign up, we may wish to create a new Google Cloud Storage bucket to store their photos, among other signup-related activities (we will describe the Google Cloud Storage service later in this chapter). We could create the following REST API request for <span>that purpose:</span></p>
<pre class="console">
curl -X POST --data-binary @JSON_FILE_NAME \
     -H "Authorization: Bearer OAUTH2_TOKEN" \
     -H «Content-Type: application/json» \
     https://storage.googleapis.com/storage/v1/b?project=PROJECT_IDENTIFIER</pre> <p class="calibre3">Let’s break <span>this down:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">JSON_FILE_NAME</strong> is the name of the required JSON file that specifies the <span>bucket details</span></li>
<li class="calibre8"><strong class="source-inline">OAUTH2_TOKEN</strong> is an access token that is required to invoke <span>the API</span></li>
<li class="calibre8"><strong class="source-inline">PROJECT_IDENTIFIER</strong> is the ID or number of the project with which our bucket will be associated, for <span>example, </span><span><strong class="source-inline">my-project</strong></span></li>
</ul>
<p class="calibre3">The required JSON file is structured <span>as follows:</span></p>
<pre class="console">
{
  "name": "BUCKET_NAME",
  "location": "BUCKET_LOCATION",
  "storageClass": "STORAGE_CLASS",
  "iamConfiguration": {
    "uniformBucketLevelAccess": {
      "enabled": true
    },
  }
}</pre> <p class="calibre3">Here’s the<a id="_idIndexMarker255" class="calibre6 pcalibre pcalibre1"/> breakdown <span>of this:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">BUCKET_NAME</strong> is the name we want to give <span>our bucket.</span></li>
<li class="calibre8"><strong class="source-inline">BUCKET_LOCATION</strong> is the location where you want to store your bucket object data. For more information regarding Google Cloud locations, refer to the Google Cloud documentation <span>here: </span><a href="https://cloud.google.com/compute/docs/regions-zones" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/compute/docs/regions-zones</span></a><span>.</span></li>
<li class="calibre8"><strong class="source-inline">STORAGE_CLASS</strong> is the default storage class of your bucket. For more information regarding Google Cloud Storage classes, refer to the Google Cloud documentation <span>here: </span><a href="https://cloud.google.com/storage/docs/storage-classes" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/storage/docs/storage-classes</span></a><span>.</span></li>
</ul>
<p class="calibre3">In practice, it’s most common to use Google Cloud’s client <strong class="bold">Software Development Kits</strong> (<strong class="bold">SDK</strong>s) to <a id="_idIndexMarker256" class="calibre6 pcalibre pcalibre1"/>create such API calls programmatically. For example, in the following Python code, we import the Google Cloud Storage client library, and we then define a function to create<a id="_idIndexMarker257" class="calibre6 pcalibre pcalibre1"/> a new bucket, specifying the bucket name, the location, and the <span>storage class:</span></p>
<pre class="source-code">
# import the GCS client library
from google.cloud import storage
def create_bucket_class_location(bucket_name):
    """
    Create a new bucket in the US region with the coldline storage
    class
    """
    # bucket_name = "your-new-bucket-name"
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    bucket.storage_class = "COLDLINE"
    new_bucket = storage_client.create_bucket(bucket, location="us")
    return new_bucket</pre> <p class="calibre3">Now that we’ve <a id="_idIndexMarker258" class="calibre6 pcalibre pcalibre1"/>covered some of the basics of how to interact with Google Cloud services, let’s discuss the types of Google Cloud service<a id="_idTextAnchor075" class="calibre6 pcalibre pcalibre1"/>s we will use in <span>this book.</span></p>
<h1 id="_idParaDest-65" class="calibre5"><a id="_idTextAnchor076" class="calibre6 pcalibre pcalibre1"/>Google Cloud services overview</h1>
<p class="calibre3">Having covered the <a id="_idIndexMarker259" class="calibre6 pcalibre pcalibre1"/>basics of how to set up a Google Cloud account and how to enable and interact with the various services, we will now introduce the services that we are going to use in this book to create AI/ML workloads. We will first cover the fundamental cloud services upon which almost all workloads are built, and then we will cover the more advanced services relate<a id="_idTextAnchor077" class="calibre6 pcalibre pcalibre1"/>d to data science <span>and AI/ML.</span></p>
<h2 id="_idParaDest-66" class="calibre9"><a id="_idTextAnchor078" class="calibre6 pcalibre pcalibre1"/>Google Cloud computing services</h2>
<p class="calibre3">Considering that the<a id="_idIndexMarker260" class="calibre6 pcalibre pcalibre1"/> word <em class="italic">computing</em> is included directly in the term <strong class="bold">cloud computing</strong>, and the fact that computing services form the basis of all other cloud services, we will start this section with a brief overview of Googl<a id="_idTextAnchor079" class="calibre6 pcalibre pcalibre1"/>e Cloud’s <span>computing services.</span></p>
<h3 class="calibre11">Google Compute Engine (GCE)</h3>
<p class="calibre3">A few years ago, the term<a id="_idIndexMarker261" class="calibre6 pcalibre pcalibre1"/> cloud computing was pretty much synonymous with the<a id="_idIndexMarker262" class="calibre6 pcalibre pcalibre1"/> term <strong class="bold">virtualization</strong>. Traditionally, companies had physical servers on their own premises, and this was then contrasted with creating virtual servers in the cloud, either public or private. Hence, perhaps the easiest concept to understand in cloud computing is virtualization, where we simply create a virtual server instead of a physical server by introducing an abstraction layer called a hypervisor between the hardware and our server’s operating system, as depicted in <span><em class="italic">Figure 3</em></span><em class="italic">.7</em>. For most companies, if they’re already running physical servers, then their first foray into the world of the cloud is usually implemented by using VMs because this is the simplest step to transition from the physical paradigm to the cloud paradigm. <strong class="bold">Google Compute Engine</strong> (<strong class="bold">GCE</strong>) is Google Cloud’s service for running VMs in the cloud. It provides some useful features, such as auto-scaling based on demand, which is one of the well-established benefits of <span>cloud computing.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer052">
<img alt="Figure 3.7: Example VM implementation" src="image/B18143_03_7.jpg" class="calibre60"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.7: Example VM implementation</p>
<h3 class="calibre11">Google Kubernetes Engine (GKE)</h3>
<p class="calibre3">A newer type of <a id="_idIndexMarker263" class="calibre6 pcalibre pcalibre1"/>virtualization was created in the 2000s by using <a id="_idIndexMarker264" class="calibre6 pcalibre pcalibre1"/>Linux <strong class="bold">cgroups</strong> and <strong class="bold">Namespaces</strong> to isolate computing<a id="_idIndexMarker265" class="calibre6 pcalibre pcalibre1"/> resources such as CPUs, RAM, and storage resources, for specific processes within a running operating system. With containerization, the abstraction layer moves higher in the stack, whereby it exists between the operating system and our applications, as depicted in <span><em class="italic">Figure 3</em></span><span><em class="italic">.8</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer053">
<img alt="Figure 3.8: Example container implementation" src="image/B18143_03_8.jpg" class="calibre61"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.8: Example container implementation</p>
<p class="calibre3">This gives us some interesting benefits beyond those afforded by hypervisor-based virtualization. For example, containers are generally much smaller and more <em class="italic">lightweight</em> than VMs, meaning they contain much fewer software components. While a VM has to boot up an entire operating system and lots of software applications before it becomes usable, which can take a few minutes, a container usually only contains your application code and any <a id="_idIndexMarker266" class="calibre6 pcalibre pcalibre1"/>required dependencies and can therefore be loaded in seconds. This makes a big difference when it comes to auto-scaling and auto-healing cloud-based software workloads. Starting up new VMs in relation to a sudden increase in traffic may not happen quickly enough and you may lose some requests while the VMs boot up and load your application. The same applies to restarting a VM due to some kind of problem. In both of those cases, a container would usually start much more quickly. Containing fewer components also means that containers can be deployed much more quickly, and this makes them a perfect environment for microservices with DevOps CI/CD pipelines. There are also many other benefits of containers, such as portability and <span>easy manageability.</span></p>
<p class="calibre3">However, one of the challenges introduced by containerization also stems from their lighter footprint. Because they are generally smaller than VMs, it’s common to have more of them in a single application deployment. Managing lots of tiny containers can be challenging, especially in terms of application lifecycle management and orchestration; that is, determining how and where to run your workloads, and assigning adequate computing resources to them. This is where Kubernetes comes into the picture. The following are the official definitions for Kubernetes, in general, and <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), <span>in </span><span><a id="_idIndexMarker267" class="calibre6 pcalibre pcalibre1"/></span><span>particular.</span></p>
<p class="calibre3">Kubernetes, also <a id="_idIndexMarker268" class="calibre6 pcalibre pcalibre1"/>known as <strong class="bold">K8s</strong>, is an open source system for automating the deployment, scaling, and management of containerized applications. It groups containers that<a id="_idIndexMarker269" class="calibre6 pcalibre pcalibre1"/> make up an application into logical units for easy management and discovery. GKE provides a managed environment for deploying, managing, and scaling your containerized applications using <span>Google infrastructure.</span></p>
<p class="calibre3"><span><em class="italic">Figure 3</em></span><em class="italic">.9</em> shows an example of how Kubernetes organizes and orchestrates applications. It deploys your applications as Pods, which are groups of containers with similar functionality, and it deploys agents on your hardware servers or the host operating systems that keep track of resource utilization and communicate that information back to the Kubernetes master, which uses that information to manage <span>Pod deployments.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer054">
<img alt="Figure 3.9: Example GKE implementation  (source: https://kubernetes.io/docs/concepts/architecture/)" src="image/B18143_03_9.jpg" class="calibre62"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.9: Example GKE implementation (source: https://kubernetes.io/docs/concepts/architecture/)</p>
<h3 class="calibre11">Google Cloud serverless computing</h3>
<p class="calibre3">The term <strong class="bold">serverless</strong> in the<a id="_idIndexMarker270" class="calibre6 pcalibre pcalibre1"/> context of cloud <a id="_idIndexMarker271" class="calibre6 pcalibre pcalibre1"/>computing refers to the concept of running your code on a cloud provider’s infrastructure without needing to manage any of the servers that will be used to run your code. In reality, there are still servers that are being used, behind the scenes, but the cloud provider creates and manages them on your behalf so that you don’t need to perform those actions. Google Cloud has two primary services that relate to serverless computing, which are named <strong class="bold">Cloud Functions</strong>, and <strong class="bold">Cloud Run</strong>. Another Google Cloud service, named <strong class="bold">App Engine</strong>, is also often bundled under the serverless umbrella, and we will describe that service, and how it differs from Cloud Functions and Cloud Run, later in <span>this section.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Many other Google Cloud services also run in a serverless fashion, whereby the actions they perform on your behalf run on servers that are managed in the background, without any need for you to manage those servers. However, Cloud Functions and Cloud Run are the two Google Cloud services that relate to <em class="italic">serverless computing</em>, which specifically refers<a id="_idIndexMarker272" class="calibre6 pcalibre pcalibre1"/> to running your code without <a id="_idTextAnchor080" class="calibre6 pcalibre pcalibre1"/>the need to explicitly <span>manage servers.</span></p>
<h4 class="calibre20">Cloud Functions</h4>
<p class="calibre3">With Cloud<a id="_idIndexMarker273" class="calibre6 pcalibre pcalibre1"/> Functions, you simply write small <a id="_idIndexMarker274" class="calibre6 pcalibre pcalibre1"/>pieces of code — for example, a single function — and Google Cloud will run that code for you in response to events that you specify as triggers to run that code. You don’t need to manage any containers, servers, or infrastructure on which your code executes, and there are many types of triggers that you can configure. For example, whenever a file is uploaded to your Google Cloud Storage bucket, that could trigger your piece of code to execute. Your code could then process that file in some way, feed it into another Google Cloud service to be processed, or simply send a notification to inform somebody that the file has <span>been uploaded.</span></p>
<p class="calibre3">This concept is referred to<a id="_idIndexMarker275" class="calibre6 pcalibre pcalibre1"/> as <strong class="bold">Functions as a Service</strong> (<strong class="bold">FaaS</strong>) because it is usually used to execute a single function for each event trigger. This approach is suitable for when you want to simply write and run small code snippets that respond to events that occur in your environment. You can also use Cloud Functions to connect with other Google Cloud or third-party cloud services to streamline challenging <span>orchestration problems.</span></p>
<p class="calibre3">In addition to sparing you the trouble of managing servers, another advantage of using Cloud Functions is that you don’t have to pay for servers when no <a id="_idTextAnchor081" class="calibre6 pcalibre pcalibre1"/>events are happening in <span>your environment.</span></p>
<h4 class="calibre20">Cloud Run</h4>
<p class="calibre3">Cloud Run is a <a id="_idIndexMarker276" class="calibre6 pcalibre pcalibre1"/>different type of serverless<a id="_idIndexMarker277" class="calibre6 pcalibre pcalibre1"/> computing service that is more suitable for long-running application processes. While cloud functions are intended to run small pieces of code in response to specific events that occur, Cloud Run can run more complex applications. This also means that it provides more flexibility and control with regard to how your code executes. For example, it runs your code in containers, and you have more control over what executes in those containers. If your application requires custom software package dependencies, for example, you can provision those dependencies to be available in <span>your containers.</span></p>
<p class="calibre3">Cloud Run abstracts away all infrastructure management by automatically scaling up and down from zero almost instantaneously, depending on traffic, and it only ch<a id="_idTextAnchor082" class="calibre6 pcalibre pcalibre1"/>arges you for the exact resources <span>you use.</span></p>
<h3 class="calibre11">App Engine</h3>
<p class="calibre3">While App Engine <a id="_idIndexMarker278" class="calibre6 pcalibre pcalibre1"/>can also be considered as a serverless service, because it <a id="_idIndexMarker279" class="calibre6 pcalibre pcalibre1"/>manages the underlying infrastructure for you, its use cases differ from those of Cloud Functions and Cloud Run. App Engine comes in two levels of service, referred to as <strong class="bold">Standard</strong> and <strong class="bold">Flexible</strong>. In the standard environment, your application runs on a lightweight server inside a sandbox. This sandbox restricts what your application can do. For example, the sandbox only allows your app to use a limited set of software binary libraries, and your app cannot write to a permanent disk. The standard environment also limits the CPU and memory options available to your application. Because of these restrictions, most App Engine standard applications tend to be stateless web applications that respond to HTTP requests quickly. In contrast, the flexible environment runs your application in Docker containers on Google Compute Engine VMs, which have <span>fewer restrictions.</span></p>
<p class="calibre3">Also, note that the standard environment can scale from zero instances up to thousands very quickly, but the flexible environment must have at least one instance running and can take longer to scale up in response to sudden <span>traffic increases.</span></p>
<p class="calibre3">App Engine is generally suited to large web applications. Its flexible environment can be more customizable than Cloud Run. However, if you want to deploy a long-running web application without managing the underlying infrastructure, I recommend first evaluating whether Cloud Run could meet your application’s needs and comparing the costs of <a id="_idIndexMarker280" class="calibre6 pcalibre pcalibre1"/>ru<a id="_idTextAnchor083" class="calibre6 pcalibre pcalibre1"/>nning <a id="_idIndexMarker281" class="calibre6 pcalibre pcalibre1"/>your app on Cloud Run versus <span>App Engine.</span></p>
<h3 class="calibre11">Google Cloud Batch</h3>
<p class="calibre3">Some workflows are<a id="_idIndexMarker282" class="calibre6 pcalibre pcalibre1"/> intended to run for a long time without the need for human interaction. Examples of such workloads include media transcoding, computational fluid dynamics, Monte Carlo simulations, genomics processing, and drug discovery, among others. These kinds of workloads usually require large amounts of computing power and can be optimized by running tasks in parallel. Creating and running these jobs by yourself can incur overheads such as managing servers, queueing mechanisms, parallelization, and failure logic. Fortunately, the Google Cloud Batch service has been built to manage all these kinds of activities for you. As a fully managed job scheduler, it automatically scales the infrastructure required to run your batch jobs up or down and handles parallelization and retry logic that you can configure in case any errors occur <span>during execution.</span></p>
<p class="calibre3">Now that we’ve covered the primary computing services on Google Cloud, let’s review some of the services you can use to in<a id="_idTextAnchor084" class="calibre6 pcalibre pcalibre1"/>tegrate between different Google <span>Cloud services.</span></p>
<h2 id="_idParaDest-67" class="calibre9"><a id="_idTextAnchor085" class="calibre6 pcalibre pcalibre1"/>Google Cloud integration services</h2>
<p class="calibre3">In addition to<a id="_idIndexMarker283" class="calibre6 pcalibre pcalibre1"/> Google Cloud infrastructure services such as compute and storage, we often need to implement integrations between the services in order to create complex workloads. Google Cloud has created tools specifically for this purpose, and we will briefly di<a id="_idTextAnchor086" class="calibre6 pcalibre pcalibre1"/>scuss some of the relevant tools in <span>this section.</span></p>
<h3 class="calibre11">Pub/Sub</h3>
<p class="calibre3">Google Cloud Pub/Sub is<a id="_idIndexMarker284" class="calibre6 pcalibre pcalibre1"/> a messaging service that <a id="_idIndexMarker285" class="calibre6 pcalibre pcalibre1"/>can be used to pass data between components of your system architecture, whether those components are other Google Cloud services, third-party services, or components you’ve built yourself. It’s an extremely versatile service that can be used for a wide array of system integration use cases, such as decoupling microservices, or streaming data into a <span>data lake.</span></p>
<p class="calibre3">Pub/Sub relates to the system architecture concept of publishing and subscribing, whereby one system can publish a message or a piece of data to a shared space, or <strong class="bold">Topic</strong>, and other systems can then receive that piece of data by subscribing to that topic. The messages can be delivered via either a <strong class="bold">Push</strong> or <strong class="bold">Pull</strong> mechanism. In the case of a push approach, the Pub/Sub service initiates the communication with the subscriber systems and sends the message to those systems. In the case of a pull model, the subscriber systems initiate the communication to the Pub/Sub service and then request or pull the information from the <span>Pub/Sub service.</span></p>
<p class="calibre3">Pub/Sub also caters to nuanced messaging needs such as publishing messages in order (if required) and retrying failed message transmissions. Google Cloud also provides an offering called <a id="_idIndexMarker286" class="calibre6 pcalibre pcalibre1"/>Pub/Sub Lite, which is a lower-cost option <a id="_idIndexMarker287" class="calibre6 pcalibre pcalibre1"/><a id="_idTextAnchor087" class="calibre6 pcalibre pcalibre1"/>with fewer features than the regular <span>Pub/Sub product.</span></p>
<h3 class="calibre11">Google Cloud Tasks</h3>
<p class="calibre3">Similar to Pub/Sub, Google<a id="_idIndexMarker288" class="calibre6 pcalibre pcalibre1"/> Cloud Tasks is a service that can be used to implement message passing and asynchronous system integration. With Pub/Sub, the publishers and subscribers are completely decoupled, and they have no control over each other’s implementations. On the other hand, with Cloud Tasks, the <a id="_idIndexMarker289" class="calibre6 pcalibre pcalibre1"/>publisher (or <strong class="bold">task producer</strong>) fully controls the overall execution of the workload. It can be specifically used for cases where a task producer needs to control the execution timing of a specific webhook or remote procedure call. Cloud Tasks is included in this section for completeness because it’s an alternative to Pub/Sub for some u<a id="_idTextAnchor088" class="calibre6 pcalibre pcalibre1"/>se cases, but we will not use Cloud Tasks in <span>this book.</span></p>
<h3 class="calibre11">Eventarc</h3>
<p class="calibre3">Eventarc is a <a id="_idIndexMarker290" class="calibre6 pcalibre pcalibre1"/>Google Cloud service that enables <a id="_idIndexMarker291" class="calibre6 pcalibre pcalibre1"/>you to build <em class="italic">event-driven</em> workloads. This is a common pattern for companies that want their workloads to execute in response to events that happen in their environment. We touched on this topic briefly when we introduced Cloud Functions. Cloud Functions can be triggered directly by certain event sources, but Eventarc provides much more flexibility and control for implementing complex event-driven architectures, in conjunction with Cloud Functions and other Google Cloud services, as well as some <span>third-party applications.</span></p>
<p class="calibre3">Eventarc uses Pub/Sub to route messages from <strong class="bold">Event Providers</strong> to <strong class="bold">Event Destinations</strong>. As the names suggest, event providers send events to Eventarc, and Eventarc sends events to event destinations. It provides a way to standardize your event processing architectures, rather than building random, ad hoc event-drive<a id="_idTextAnchor089" class="calibre6 pcalibre pcalibre1"/>n implementations<a id="_idIndexMarker292" class="calibre6 pcalibre pcalibre1"/> between your various <span>system components.</span></p>
<h3 class="calibre11">Workflows</h3>
<p class="calibre3">While Eventarc <a id="_idIndexMarker293" class="calibre6 pcalibre pcalibre1"/>provides a mechanism for standardizing your event-driven workloads, Google Cloud Workflows, as the name suggests, is a service that has been specifically built to orchestrate complex workflows, in which the coordination of activities among various systems needs to be implemented in a specific order. With this in mind, Google Cloud Workflows and Eventarc make a great pair when used together to implement complex, event-driven workloads. Workflows can either be triggered by events, or you can create batch workflows that can be triggered in <span>different ways.</span></p>
<p class="calibre3">Workflows can orchestrate activities between various microservices and custom or third-party APIs. The Workflows service maintains the state of each step in your workload during execution, meaning that it tracks the inputs and outputs of each step, and it knows which steps have already been completed, which steps are currently executing, and which steps remain to be invoked in the workflow. It allows you to visualize all of your workflow’s steps and their dependencies, and if any step in the process fails, you can use the Workflows service to figure out which one and determine what to do next. <span><em class="italic">Figure 3</em></span><em class="italic">.10</em> shows an example of a workflow process for an online retail system, in which a customer purchases an item. Various Google Cloud computing products are used to run each of the software services in the workflow, and the coordination of each of the steps in the process is managed by the Workflows service. While this is an example of a simple order processing workflow, note that most large retail companies work with supply chains consisting of extremely complex webs of interconnected systems <span>and partners.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer055">
<img alt="Figure 3.10: Example workflow for online retail system" src="image/B18143_03_10.jpg" class="calibre63"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.10: Example workflow for online retail system</p>
<p class="calibre3">The Workflows service is best suited to orchestrating activities between services. If you want to implement an<a id="_idIndexMarker294" class="calibre6 pcalibre pcalibre1"/> orchestration workflow for data engineering, then Google Cloud Composer may be more suitab<a id="_idTextAnchor090" class="calibre6 pcalibre pcalibre1"/>le. We discuss Google Cloud Composer later in <span>this chapter.</span></p>
<h3 class="calibre11">Scheduler</h3>
<p class="calibre3">Google Cloud Scheduler <a id="_idIndexMarker295" class="calibre6 pcalibre pcalibre1"/>is a relatively simple but very useful service that can be used to execute workloads according to a schedule. For example, if you want a process to run at the same time every day, every hour, or every month, you could use Cloud Scheduler to define and kick off those executions. Any of you who are familiar with Unix-based operating systems may see a similarity with the <span>cron service.</span></p>
<p class="calibre3">Google Cloud Scheduler can be used in conjunction with many of the integration services we’ve described in this section. For example, you could schedule a message to be sent to a Pub/Sub topic every 15 minutes, and that could t<a id="_idTextAnchor091" class="calibre6 pcalibre pcalibre1"/>hen be sent to Eventarc and used to<a id="_idIndexMarker296" class="calibre6 pcalibre pcalibre1"/> invoke a <span>cloud function.</span></p>
<h2 id="_idParaDest-68" class="calibre9"><a id="_idTextAnchor092" class="calibre6 pcalibre pcalibre1"/>Networking and connectivity</h2>
<p class="calibre3">Very few workloads <a id="_idIndexMarker297" class="calibre6 pcalibre pcalibre1"/>exist without the need to set up some kind of network connectivity. For example, even if you have only a single server, you generally need to connect to it in some way in order to perform any actions on it. As you scale beyond a single server, those servers usually need to communicate with each other. In this section, we discuss the fundamental networking and connectivity concepts upon whic<a id="_idTextAnchor093" class="calibre6 pcalibre pcalibre1"/>h we will build our workloads in later chapters of <span>this book.</span></p>
<h3 class="calibre11">Virtual Private Cloud (VPC)</h3>
<p class="calibre3">The first concept <a id="_idIndexMarker298" class="calibre6 pcalibre pcalibre1"/>we introduce <a id="_idIndexMarker299" class="calibre6 pcalibre pcalibre1"/>in this section is the <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) concept. A VPC is a virtual network that can span all Google Cloud regions. The reason it is called a virtual private cloud is that it defines the boundaries of your networking infrastructure, and therefore where you run your workloads within Google Cloud. You can, however, peer or share connectivit<a id="_idTextAnchor094" class="calibre6 pcalibre pcalibre1"/>y with other VPCs in order to communicate across <span>VPC boundaries.</span></p>
<h3 class="calibre11">Hybrid networking</h3>
<p class="calibre3">If you’re working for a <a id="_idIndexMarker300" class="calibre6 pcalibre pcalibre1"/>company<a id="_idIndexMarker301" class="calibre6 pcalibre pcalibre1"/> that has its own on-premises servers and networks, and you want to connect them to the cloud, this is referred to as <strong class="bold">Hybrid Connectivity</strong>. It’s a <a id="_idIndexMarker302" class="calibre6 pcalibre pcalibre1"/>common need for many companies, and therefore, Google Cloud has created specific solutions to facilitate t<a id="_idTextAnchor095" class="calibre6 pcalibre pcalibre1"/>his kind of connectivity, which consist of the <span>following offerings.</span></p>
<h4 class="calibre20">Dedicated Interconnect</h4>
<p class="calibre3">Dedicated Interconnect<a id="_idIndexMarker303" class="calibre6 pcalibre pcalibre1"/> provides direct physical<a id="_idIndexMarker304" class="calibre6 pcalibre pcalibre1"/> connections between your on-premises network and Google’s network. It offers a guaranteed uptime of 99.99% and can connect either one or two links that each can support up to 100 <strong class="bold">gigabits per second</strong> (<strong class="bold">Gbps</strong>) in bandwidth. It requires hardware connectivity to be set up at specific Dedicated Interconnect locations, and therefore it can require non-trivial effort to set it up. This option is for companies who need high-bandwidth networking<a id="_idTextAnchor096" class="calibre6 pcalibre pcalibre1"/> <a id="_idIndexMarker305" class="calibre6 pcalibre pcalibre1"/>between <a id="_idIndexMarker306" class="calibre6 pcalibre pcalibre1"/>their premises and Google Cloud for <span>long-lived connectivity.</span></p>
<h4 class="calibre20">Partner Interconnect</h4>
<p class="calibre3">If you don’t have your<a id="_idIndexMarker307" class="calibre6 pcalibre pcalibre1"/> own infrastructure <a id="_idIndexMarker308" class="calibre6 pcalibre pcalibre1"/>built at one of the Dedicated Interconnect locations, there are Google Cloud partners that offer connectivity with up to 99.99% availability through Partner Interconnect. This option also requires some effort in working with the partners to set it up, but it doesn’t require the same amount of investment as Dedicated Interconnect. A trade-off is that the partners generally share the connections among many<a id="_idTextAnchor097" class="calibre6 pcalibre pcalibre1"/> customers, so the bandwidth is less than that of <span>Dedicated Interconnect.</span></p>
<h4 class="calibre20">Private Google Access (PGA) for on-premises</h4>
<p class="calibre3">This is a basic<a id="_idIndexMarker309" class="calibre6 pcalibre pcalibre1"/> connectivity<a id="_idIndexMarker310" class="calibre6 pcalibre pcalibre1"/> option that provides direct access to Google s<a id="_idTextAnchor098" class="calibre6 pcalibre pcalibre1"/>ervices such as Cloud Storage and BigQuery from your <span>on-premises locations.</span></p>
<h4 class="calibre20">Virtual Private Network (VPN)</h4>
<p class="calibre3">Perhaps the <a id="_idIndexMarker311" class="calibre6 pcalibre pcalibre1"/>easiest way to connect <a id="_idIndexMarker312" class="calibre6 pcalibre pcalibre1"/>your on-premises resources to your Google Cloud VPC is via a <strong class="bold">Virtual Private Network</strong> (<strong class="bold">VPN</strong>), which uses <strong class="bold">IP security</strong> (<strong class="bold">IPsec</strong>) mechanisms<a id="_idIndexMarker313" class="calibre6 pcalibre pcalibre1"/> to offer a low-cost option that delivers 1.5 – 3.0 Gbps of throughput over an encrypted public internet connection. Unlike the Interconnect services mentioned previously, this option does not require any special, hardware-related network connectivity in any <span>specific location.</span></p>
<p class="calibre3">Now that we’ve covered the fundamental Google Cloud services that underpin the workloads that we’ll build in this book, it’s time to dive into the services that we will directly u<a id="_idTextAnchor099" class="calibre6 pcalibre pcalibre1"/>se to create <a id="_idIndexMarker314" class="calibre6 pcalibre pcalibre1"/>our data processing workloads to prepare for our AI/ML <span>use cases.</span></p>
<h1 id="_idParaDest-69" class="calibre5"><a id="_idTextAnchor100" class="calibre6 pcalibre pcalibre1"/>Google Cloud tools for data storage and processing</h1>
<p class="calibre3">Since gathering data is the first major step in an AI/ML project (after establishing the business objectives of the project), we begin our exploration of Google Cloud’s AI/ML-related services by first reviewing the tools for storing and processing data. <span><em class="italic">Figure 3</em></span><em class="italic">.2</em> shows the steps in the life cycle that relate to ingesting, storing, and processing data. It should be noted that the <strong class="bold">Train Model</strong>, <strong class="bold">Evaluate Model</strong>, and <strong class="bold">Monitor Model</strong> steps would also usually create outputs that need to be <span>stored somewhere.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer056">
<img alt="Figure 3.11: Ingesting, storing, exploring, and processing data" src="image/B18143_03_11.jpg" class="calibre64"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.11: Ingesting, storing, exploring, and processing data</p>
<p class="calibre3">As can be seen in <span><em class="italic">Figure 3</em></span><em class="italic">.11</em>, and as we’ve discusse<a id="_idTextAnchor101" class="calibre6 pcalibre pcalibre1"/>d previously, working with data is a very prominent part of any <span>AI/ML project.</span></p>
<h2 id="_idParaDest-70" class="calibre9"><a id="_idTextAnchor102" class="calibre6 pcalibre pcalibre1"/>Data ingestion</h2>
<p class="calibre3">Before we can do <a id="_idIndexMarker315" class="calibre6 pcalibre pcalibre1"/>anything with data in Google Cloud, we need to get access to the data, and we often want to ingest that data into some kind of storage service on Google Cloud. In this section, we’ll discuss some of the tools that exist on Google Cloud for ingesting data, and in the next section, we will cover the Google Cloud storage systems into which the ingestion services ingest data. We will not focus on Google Cloud’s database services in this chapter, nor the related <strong class="bold">Database Migration Service</strong> (<strong class="bold">DMS</strong>), because<a id="_idIndexMarker316" class="calibre6 pcalibre pcalibre1"/> for machine learning purposes, we would usually extract data from databases and place it into one of the storage systems described in this section. An exception to this may be Google<a id="_idTextAnchor103" class="calibre6 pcalibre pcalibre1"/> Cloud Bigtable, but we will discuss that service separately in a <span>later chapter.</span></p>
<h3 class="calibre11">gsutil</h3>
<p class="calibre3">Perhaps the simplest<a id="_idIndexMarker317" class="calibre6 pcalibre pcalibre1"/> way to transfer data<a id="_idIndexMarker318" class="calibre6 pcalibre pcalibre1"/> to <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>), or <a id="_idIndexMarker319" class="calibre6 pcalibre pcalibre1"/>between GCS buckets, is via the gsutil command<a id="_idTextAnchor104" class="calibre6 pcalibre pcalibre1"/>-line tool, which can be used to transfer up to 1 TB of data with a <span>simple command.</span></p>
<h3 class="calibre11">The Data transfer service</h3>
<p class="calibre3">If you want to transfer <a id="_idIndexMarker320" class="calibre6 pcalibre pcalibre1"/>more than 1 TB <a id="_idIndexMarker321" class="calibre6 pcalibre pcalibre1"/>of data, you can use the data transfer service, which transfers data quickly and securely from on-premises systems or other public cloud providers. For large data migration projects, in which you may wish to run multiple data transfer jobs, it lets you centralize your job management to monitor the status of each job. You can transfer petabytes of data consisting of billions of files, at up to tens of Gbps of bandwidth, and the data transfer service will optimize your network bandwidth to accelerate transfers. You can inge<a id="_idTextAnchor105" class="calibre6 pcalibre pcalibre1"/>st your data into GCS and then have other Google Cloud services access it <span>from there.</span></p>
<h3 class="calibre11">The BigQuery Data Transfer Service</h3>
<p class="calibre3">The BigQuery Data <a id="_idIndexMarker322" class="calibre6 pcalibre pcalibre1"/>Transfer <a id="_idIndexMarker323" class="calibre6 pcalibre pcalibre1"/>Service automates data movement specifically into BigQuery, on a scheduled, managed basis. You can access the BigQuery Data Transfer Service using the Google Cloud console, the <strong class="bold">bq</strong> command-line tool, or the <a id="_idIndexMarker324" class="calibre6 pcalibre pcalibre1"/>BigQuery Data Transfer Service API. It supports lots of data sources<a id="_idTextAnchor106" class="calibre6 pcalibre pcalibre1"/>, such as GCS, Google Ads, YouTube, Amazon S3, Amazon Redshift, Teradata, and <span>many more.</span></p>
<h2 id="_idParaDest-71" class="calibre9"><a id="_idTextAnchor107" class="calibre6 pcalibre pcalibre1"/>Data storage</h2>
<p class="calibre3">There are many different<a id="_idIndexMarker325" class="calibre6 pcalibre pcalibre1"/> ways to store data in Google Cloud, and the types of tools and services you select for data storage will depend on your use case and what you’re trying to achieve. In this section, we’ll take a look at the different products and services provided by Googl<a id="_idTextAnchor108" class="calibre6 pcalibre pcalibre1"/>e Cloud in the data storage space, and the kinds of workloads to which they <span>best relate.</span></p>
<h3 class="calibre11">Concepts – data warehouses, data lakes, and lake houses</h3>
<p class="calibre3">Before diving<a id="_idIndexMarker326" class="calibre6 pcalibre pcalibre1"/> into each of the major Google<a id="_idIndexMarker327" class="calibre6 pcalibre pcalibre1"/> Cloud data storage services, it’s <a id="_idIndexMarker328" class="calibre6 pcalibre pcalibre1"/>important to discuss the concepts of data warehouses, data lakes, and lake houses, which are all terms that have become quite popular in the industry in <span>recent years.</span></p>
<p class="calibre3">A <strong class="bold">data warehouse</strong> usually<a id="_idIndexMarker329" class="calibre6 pcalibre pcalibre1"/> contains structured data in a format that is optimized <a id="_idIndexMarker330" class="calibre6 pcalibre pcalibre1"/>for analytics purposes, such as columnar data formats like <strong class="bold">Parquet</strong> or <strong class="bold">Optimized Row Columnar</strong> (<strong class="bold">ORC</strong>). This is because data analytics queries<a id="_idIndexMarker331" class="calibre6 pcalibre pcalibre1"/> often operate on database columns rather than rows. For example, we might run a query to find out the average age of customers who buy our products, and this query would therefore focus on the <em class="italic">age</em> column in our customer database table. Columnar data formats store all of the elements of each column near each other on the physical storage disks so queries that operate on database columns run <span>more efficiently.</span></p>
<p class="calibre3">A <strong class="bold">data lake</strong>, as the <a id="_idIndexMarker332" class="calibre6 pcalibre pcalibre1"/>name suggests, serves as a reservoir in which you can store huge amounts of data in a variety of formats, both structured and unstructured. Because there are no specific requirements regarding query optimization, data lakes can usually store much more data than data warehouses. Data lakes are a key ingredient in breaking down the problematic data silos that we described in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, and they can serve as the foundation of your data <span>management strategy.</span></p>
<p class="calibre3">The term, <strong class="bold">data lake house</strong>, refers <a id="_idIndexMarker333" class="calibre6 pcalibre pcalibre1"/>to more recently emerging patterns, in which companies utilize a combination of data warehouses and data lakes in order to get the best of both worlds and support a broader set of use cases, such as real-time<a id="_idTextAnchor109" class="calibre6 pcalibre pcalibre1"/> analytics, batch data processing, machine learning, and visualization, all from the <span>same source.</span></p>
<h3 class="calibre11">Google Cloud Storage (GCS)</h3>
<p class="calibre3">In addition to <a id="_idIndexMarker334" class="calibre6 pcalibre pcalibre1"/>Google Compute<a id="_idIndexMarker335" class="calibre6 pcalibre pcalibre1"/> Engine, <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) is one of the most fundamental services in Google Cloud. It supports what’s referred to as <strong class="bold">object</strong> storage, and it is perhaps the most versatile of all the storage services in Google Cloud, because you can store pretty much any type of data in GCS, and it can be directly accessed from most Google Cloud services that process data. It is especially suitable for large amounts of data, and it can be used as the basis for building an enterprise <span>data lake.</span></p>
<p class="calibre3">GCS provides different storage classes to optimize your usage based on cost and frequency of access. For objects that you don’t access frequently, you can put them in a lower-cost storage class, and you can even configure GCS to automatically move your objects between storage classes based on criteria such as the age of each object. For more information on each of the different storage classes, and which one works best for <a id="_idTextAnchor110" class="calibre6 pcalibre pcalibre1"/>different use<a id="_idIndexMarker336" class="calibre6 pcalibre pcalibre1"/> cases, reference the table <span>at </span><a href="https://cloud.google.com/storage/docs/storage-classes" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/storage/docs/storage-classes</span></a><span>.</span></p>
<h3 class="calibre11">Filestore</h3>
<p class="calibre3">The Google<a id="_idIndexMarker337" class="calibre6 pcalibre pcalibre1"/> Cloud Filestore <a id="_idIndexMarker338" class="calibre6 pcalibre pcalibre1"/>service is a high-performance, fully managed file storage service, used for workloads in which a structured file system is needed. This is the concept <a id="_idIndexMarker339" class="calibre6 pcalibre pcalibre1"/>of <strong class="bold">Network Attached Storage</strong>, in which your VMs and containers can <em class="italic">mount</em> a shared filesystem, and can access and operate on the files in the shared directory structure. It uses <a id="_idIndexMarker340" class="calibre6 pcalibre pcalibre1"/>the <strong class="bold">Network File System version 3</strong> (<strong class="bold">NFSv3</strong>) protocol and supports any <span>NFSv3-compatible clients.</span></p>
<p class="calibre3">Filestore is available in three <span>different formats:</span></p>
<ul class="calibre16">
<li class="calibre8">Filestore Basic, which is best for file sharing, software development, and <span>web hosting</span></li>
<li class="calibre8">Filestore Enterprise, which is best for critical applications such as <span>SAP workloads</span></li>
<li class="calibre8">Filestore High Scale, which is best for high-performance computing, including genome sequencing, financial services trading analysis, and other <span>high-performance workloads</span></li>
</ul>
<p class="calibre3">Access to the shared file system depends on the permissions you’ve configured, and the networking connectivity that you have set<a id="_idTextAnchor111" class="calibre6 pcalibre pcalibre1"/> up by using the products that we discussed in the <em class="italic">Networking and connectivity</em> section of <span>this chapter.</span></p>
<h3 class="calibre11">Persistent Disk</h3>
<p class="calibre3">So far, we’ve<a id="_idIndexMarker341" class="calibre6 pcalibre pcalibre1"/> covered<a id="_idIndexMarker342" class="calibre6 pcalibre pcalibre1"/> object storage and file storage. Another type of storage is referred to as block storage. This type of storage may be most familiar because it’s the traditional type of storage used by disks that are directly attached to computers; that is, <strong class="bold">Direct Attached Storage</strong> (<strong class="bold">DAS</strong>). For<a id="_idIndexMarker343" class="calibre6 pcalibre pcalibre1"/> example, the disk drive in your laptop uses this type of storage. In companies’ own on-premises data centers, many servers may be connected to shared block storage devices referred to as a <strong class="bold">Storage Area Network</strong> (<strong class="bold">SAN</strong>), using <a id="_idIndexMarker344" class="calibre6 pcalibre pcalibre1"/>the types of shared RAID array configurations that we briefly discussed in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. In either case, these block storage devices appear to our server operating systems as if they are directly attached disks, and they are used as such by the applications running on our servers or in <span>our containers.</span></p>
<p class="calibre3">SANs can require a lot of effort to set up and maintain, but when using Google Cloud Persistent Disk, you can simply define what kind of disk storage you want, and the required capacity, and all of the underlying infrastructure is managed for you <span>by Google.</span></p>
<p class="calibre3">At a high level, Persistent <a id="_idIndexMarker345" class="calibre6 pcalibre pcalibre1"/>Disk provides two different storage types, which are <strong class="bold">Hard Disk Drives</strong> (<strong class="bold">HDDs</strong>) and <strong class="bold">Solid State Drives</strong> (<strong class="bold">SSDs</strong>). HDDs offer low-cost<a id="_idIndexMarker346" class="calibre6 pcalibre pcalibre1"/> storage when bulk throughput is of primary<a id="_idIndexMarker347" class="calibre6 pcalibre pcalibre1"/> importance. SSDs offer high p<a id="_idTextAnchor112" class="calibre6 pcalibre pcalibre1"/>erformance and speed for both random-access<a id="_idIndexMarker348" class="calibre6 pcalibre pcalibre1"/> workloads and bulk throughput. Both types can be sized up to <span>64 TB.</span></p>
<h3 class="calibre11">BigQuery</h3>
<p class="calibre3">Google Cloud BigQuery is<a id="_idIndexMarker349" class="calibre6 pcalibre pcalibre1"/> a serverless<a id="_idIndexMarker350" class="calibre6 pcalibre pcalibre1"/> data warehouse, meaning that you can use it without needing to configure or manage any servers. As a data warehouse, it straddles both storage and processing. It can store your data in a format that’s optimized for data analytics workloads, and it provides tools that allow you to run SQL queries on that data. You can also use it to run queries on other storage systems such as GCS, Cloud SQL, Cloud Spanner, Cloud Bigtable, and even storage systems on AWS or Azure. Additionally, it provides built-in machine learning that enables you to get ML inferences from your data via SQL queries, without needing to use other services. On the other hand, if you explicitly want to use other services such as Vertex AI, which we will describe later in this chapter, it integrates easily with many other Google <span>Cloud services.</span></p>
<p class="calibre3">It supports geospatial analysis, so you can augment your analytics workflows with location data, and it supports real-time analytics on streaming data when you integrate streaming solutions such as Dataflow with BigQuery BI Engine, which is an in-memory analysis service that provides a sub-second query response time. BI Engine also natively integrates with Looker Studio and works with many business intelligence tools. BigQuery is an extremely popular service on Google Cloud, and you will learn how to use many of its features in <span>this book.</span></p>
<p class="calibre3">After you’ve ingested and stored data in Google Cloud, you will often want to organize and manage it so<a id="_idIndexMarker351" class="calibre6 pcalibre pcalibre1"/> that<a id="_idIndexMarker352" class="calibre6 pcalibre pcalibre1"/> it can easil<a id="_idTextAnchor113" class="calibre6 pcalibre pcalibre1"/>y be discovered and utilized effectively. In the next section, we will discuss Google Cloud’s data <span>management tools.</span></p>
<h2 id="_idParaDest-72" class="calibre9"><a id="_idTextAnchor114" class="calibre6 pcalibre pcalibre1"/>Data management</h2>
<p class="calibre3">The services we<a id="_idIndexMarker353" class="calibre6 pcalibre pcalibre1"/> describe in this section enable you to organize your data and make it easier to manage how your data can be discovered and accessed by users in your organization, thus breaking down or preventing data silos. These tools act as a supporting layer between the data storage services we discu<a id="_idTextAnchor115" class="calibre6 pcalibre pcalibre1"/>ssed in the previous section and the data processing services we will discuss in subsequent sections in <span>this chapter.</span></p>
<h3 class="calibre11">BigLake</h3>
<p class="calibre3">BigLake is a storage <a id="_idIndexMarker354" class="calibre6 pcalibre pcalibre1"/>engine that unifies data<a id="_idIndexMarker355" class="calibre6 pcalibre pcalibre1"/> warehouses and data lakes by enabling BigQuery and open source frameworks such as Spark to access data with fine-grained access control. For example, you could store data in GCS and make it available as a BigLake table, and then you could access that data from BigQuery or Spark. Fine-grained access control means that you can control access to the data at the table, row, and column level. As an example, you could ensure that your data scientists can see all columns except the credit card information column, or you could ensure that the sales department for a particular geographical location can only see the rows that pertain to that location, and cannot see data in any rows that relate to other <span>geographical locations.</span></p>
<p class="calibre3">BigLake allows you to perform analytics on distributed data regardless of where and how it’s stored, using your preferred analytics tools – open source or cloud native – over a single copy of the data. This is important because it means you don’t need to move the data around between your data lakes and data warehouses, which has traditionally been laborious and expensive to do. BigLake also supports open source engines such as Apache Spark, Presto, and Trino, and open formats such as Parquet, Avro, ORC, CSV, and JSON, serving multiple compute engines through Apache Arrow. You can centrally manage data security policies in one place and have them consistently enforced across multiple query engines, and across multiple clouds when using BigQuery Omni. It can also integrate with Google Cloud Dataplex, <a id="_idTextAnchor116" class="calibre6 pcalibre pcalibre1"/>which we will describe next, to enhance this functionality and provide <a id="_idIndexMarker356" class="calibre6 pcalibre pcalibre1"/>unified <a id="_idIndexMarker357" class="calibre6 pcalibre pcalibre1"/>data governance and management <span>at scale.</span></p>
<h3 class="calibre11">Dataplex</h3>
<p class="calibre3">Google refers<a id="_idIndexMarker358" class="calibre6 pcalibre pcalibre1"/> to <a id="_idIndexMarker359" class="calibre6 pcalibre pcalibre1"/>Dataplex as an “<em class="italic">intelligent data fabric that enables organizations to centrally discover, manage, monitor, and govern their data across data lakes, data warehouses, and data marts, with consistent controls</em>.” This relates to the concept of breaking down data silos. In <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, we talked about data silos being a common challenge that companies run into when they wish to perform data science tasks and the complexities of managing who can access the data securely when you have many datasets owned by various organizations throughout your company. Dataplex helps to overcome these challenges by enabling data discovery and providing a single pane of glass for data management across data silos, and centralized security and governance. This means that you can define security and governance policies in Dataplex, and have them applied to data that is stored and accessed by other systems, in a consistent manner. It integrates with other Google Cloud data management services such as BigQuery, Cloud Storage, and <span>Vertex AI.</span></p>
<p class="calibre3">With Dataplex, the idea is to create a <em class="italic">data mesh</em>, in which there are logical connections between your various data stores and data processing systems, rather than disjointed data silos. It also uses Google’s AI/ML capabilities to provide additional features such as automated data life cycle management, data quality enforcement, and lineage tracking (you may remember that, in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, we also talked about lineage tracking being a difficult, common challenge that companies face when implementing data <span>science workloads).</span></p>
<p class="calibre3">Google Cloud originally had a standalone service named Data Catalog, which could be used to store metadata about your various datasets, and therefore provide discoverability by allowing you to view and search through the metadata to understand what datasets are available. This service is now provided within Dataplex, and it can even automate data discovery, classification, and metadata enrichment. It can then logically organize data that exists across multiple storage services into business-specific domains using the concepts of Dataplex lakes and <span>data zones.</span></p>
<p class="calibre3">Dataplex also provides some data processing functionality via its <em class="italic">serverless data exploration workbench</em>, which provides one-click access to Spark SQL scripts and Jupyter notebooks, allowing you to interactively query your datasets. The workbench also allows teams to publish, share, and search for datasets, therefore enabling discoverability and collaboration <span>across teams.</span></p>
<p class="calibre3">Speaking of data<a id="_idIndexMarker360" class="calibre6 pcalibre pcalibre1"/> p<a id="_idTextAnchor117" class="calibre6 pcalibre pcalibre1"/>rocessing, our next section will<a id="_idIndexMarker361" class="calibre6 pcalibre pcalibre1"/> cover some of the primary tools and services available for processing data on <span>Google Cloud.</span></p>
<h2 id="_idParaDest-73" class="calibre9"><a id="_idTextAnchor118" class="calibre6 pcalibre pcalibre1"/>Data processing</h2>
<p class="calibre3">When you have used<a id="_idIndexMarker362" class="calibre6 pcalibre pcalibre1"/> some of the services we discussed in the previous sections to store, organize, and manage data, you may then wish to process that data in some way. Fortunatel<a id="_idTextAnchor119" class="calibre6 pcalibre pcalibre1"/>y, there are a number of tools and services in Google Cloud that can be used for this purpose, and we will explore <span>them here.</span></p>
<h3 class="calibre11">Dataproc</h3>
<p class="calibre3">Dataproc is a fully<a id="_idIndexMarker363" class="calibre6 pcalibre pcalibre1"/> managed and highly scalable<a id="_idIndexMarker364" class="calibre6 pcalibre pcalibre1"/> service for running Apache Hadoop, Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. It is therefore very popular for data processing workloads on Google Cloud; especially when open source tools are preferred. You can either manage the servers that process your data yourself, or Dataproc also provides a serverless option, in which Google will manage all of the servers for you. People sometimes want to manage the servers themselves if they want to use customized configurations or customized tools. Dataproc also integrates with other Google tools such as BigQuery, and Vertex AI to cater to flexible data management needs and data science projects, and you can enforce fine-grained row and column-level access controls with Dataproc, BigLake, and Dataplex. You can also manage and enforce user authorization and authentication using existing Kerberos and Apache Ranger policies, and it provides the built-in Dataproc Metastore, which eliminates the need to run your own Hive Metastore or <span>catalog service.</span></p>
<p class="calibre3">Managing your own on-premises Hadoop or Spark clusters can require a lot of work. One of the great things about Dataproc is that you can easily spin up clusters on demand in order to run a data processing workload, and then automatically shut them down when you’re not using them, and clusters can automatically scale up and down to meet your needs, which helps to save costs. You can also use Google Compute Engine Spot instances to further save costs for workloads that can tolerate being interrupted. You can<a id="_idTextAnchor120" class="calibre6 pcalibre pcalibre1"/> run your workloads in VMs or containers, and it also supports GPUs if you need to use those in your <a id="_idIndexMarker365" class="calibre6 pcalibre pcalibre1"/>data <span>processing workloads.</span></p>
<h3 class="calibre11">Dataprep</h3>
<p class="calibre3">Dataprep by Trifacta <a id="_idIndexMarker366" class="calibre6 pcalibre pcalibre1"/>is a tool for visually <a id="_idIndexMarker367" class="calibre6 pcalibre pcalibre1"/>exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning. It’s serverless, so there is no infrastructure to deploy or manage. It’s a very useful tool for the data exploration phase of a data science project, enabling you to explore and understand data with visual data distributions, and automatically detecting schemas, data types, possible joins, and anomalies such as missing values, outliers, and duplicates. You can then define a sequence of transformations to clean up and prepare your data for training ML models. You can do all of this visually, without needing to write any code, and it even suggests what kinds of transformations you may wish to implement, such as aggregation, pivot, unpivot, joins, union, extraction, calculation, comparison, condition, merge, regular expressions, and more. You can also apply data quality rules to ensure t<a id="_idTextAnchor121" class="calibre6 pcalibre pcalibre1"/>hat your data meets your quality requirements, and it <a id="_idIndexMarker368" class="calibre6 pcalibre pcalibre1"/>allows teams to collaborate on datasets by sharing or copying them <span>as needed.</span></p>
<h3 class="calibre11">Dataflow</h3>
<p class="calibre3">In order to discuss<a id="_idIndexMarker369" class="calibre6 pcalibre pcalibre1"/> Google Cloud Dataflow, we <a id="_idIndexMarker370" class="calibre6 pcalibre pcalibre1"/>will first take a minute to introduce Apache Beam. In previous sections and chapters of this book, we’ve referred to <em class="italic">batch</em> data processing, in which large amounts of data are processed by long-running jobs, and <em class="italic">streaming</em> data processing, in which small pieces of data are processed very quickly, usually in real time or near real time. There are generally slightly different tools for each of those processing types. For example, you might use Hadoop for batch processing and you might use Apache Flink for stream processing. Apache Beam provides a unified model that can be used for both batch and streaming workloads. In the words of the Apache Beam project management committee, this allows you to “<em class="italic">write once, run anywhere</em>.” This can be very useful as it enables your data engineers to simplify how they code their data processing workloads by using this unified model instead of using completely different tools and code for their batch and streaming <span>use cases.</span></p>
<p class="calibre3">Google Cloud Dataflow is a fully managed, serverless service for executing Apache Beam pipelines. Once you have defined your data processing steps as an Apache Beam pipeline, you can then use your preferred data processing engines, such as Spark, Flink, or Google Cloud Dataflow, to execute the pipeline steps. As a fully managed service, Dataflow can automatically provision and scale the resources required to run your data processing steps, and it integrates with other tools<a id="_idTextAnchor122" class="calibre6 pcalibre pcalibre1"/> and services such as BigQuery, enabling you to use<a id="_idIndexMarker371" class="calibre6 pcalibre pcalibre1"/> SQL to <a id="_idIndexMarker372" class="calibre6 pcalibre pcalibre1"/>access your data, and Vertex AI notebooks for ML model training <span>use cases.</span></p>
<h3 class="calibre11">Looker</h3>
<p class="calibre3">Looker is Google Cloud’s <a id="_idIndexMarker373" class="calibre6 pcalibre pcalibre1"/>business intelligence platform <a id="_idIndexMarker374" class="calibre6 pcalibre pcalibre1"/>with embedded analytics. One of its main advantages is LookML, which is a powerful SQL-based modeling language. You can use LookML to centrally define and manage business rules and definitions as a version-controlled data model, and LookML can then create efficient SQL queries on your behalf. As a business intelligence tool, Looker then provides a user interface in which you can visualize your data in graphs, charts, <span>and dashboards.</span></p>
<p class="calibre3">It comes in a few different service tiers, providing different levels of business intelligence functionality. Google originally had a business intelligence tool named Data Studio, and Looker was created by a separate company, but Google acquired that company and has integrated Looker into its Cloud Service portfolio. They also integrated their original Data Studio product into Looker, creating Looker Studio, a<a id="_idTextAnchor123" class="calibre6 pcalibre pcalibre1"/>nd added an enterprise version of that tool, named Looker Studio Pro, which provides additional functionality as well as <span>customer support.</span></p>
<h3 class="calibre11">Data Fusion</h3>
<p class="calibre3">In order to <a id="_idIndexMarker375" class="calibre6 pcalibre pcalibre1"/>discuss Data Fusion, let’s <a id="_idIndexMarker376" class="calibre6 pcalibre pcalibre1"/>first talk about the concepts of <strong class="bold">ETL</strong> and <strong class="bold">ELT</strong>, which stand for <strong class="bold">Extract, Transform, Load</strong>, and <strong class="bold">Extract, Load, Transform</strong>, respectively. The ETL <a id="_idIndexMarker377" class="calibre6 pcalibre pcalibre1"/>concept<a id="_idIndexMarker378" class="calibre6 pcalibre pcalibre1"/> has been around since the 1970s, and it’s a common pattern that’s used in data science and data engineering when you need to perform transformations on your data. The pattern used in this case is to extract your data from its source storage location, transform it in some way, and then load it into your desired destination storage location. Examples of transformations could be to change the format of the data from CSV to JSON, or to remove all rows that contain missing information. A complex data engineering project may require the creation of ETL pipelines that define multiple transformation steps. ELT, on the other hand, has gained popularity in recent years, especially in relation to cloud-based data processing workloads. The idea with ELT is that you can extract your data from source locations and load it into a data lake or data warehouse, and then perform different kinds of transformations based on your project needs. This is also often referred to as <em class="italic">data integration</em>. Using this approach can enable analysts to use SQL to get insights and value from the data without requiring data engineers to create complex <span>ETL pipelines.</span></p>
<p class="calibre3">With this in mind, Data Fusion is Google Cloud’s serverless product that allows you to run your ETL/ELT workloads without having to manage any servers or infrastructure. It provides a visual interface that enables you to define your data transformation steps without having to write code, which makes it easy for less technical analysts to process the data, and it even tracks that ever-important data lineage that we discussed in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, as your data progresses through each <span>transformation step.</span></p>
<p class="calibre3">Data Fusion integrates with other Google Cloud services such as B<a id="_idTextAnchor124" class="calibre6 pcalibre pcalibre1"/>igQuery, Dataflow, Dataproc, Datastore, Cloud<a id="_idIndexMarker379" class="calibre6 pcalibre pcalibre1"/> Storage, and Pub/Sub, making<a id="_idIndexMarker380" class="calibre6 pcalibre pcalibre1"/> it easy to perform data processing workflows across <span>those services.</span></p>
<h3 class="calibre11">Google Cloud Composer</h3>
<p class="calibre3">Composer is a<a id="_idIndexMarker381" class="calibre6 pcalibre pcalibre1"/> Google Cloud <a id="_idIndexMarker382" class="calibre6 pcalibre pcalibre1"/>orchestration service built on the open source Apache Airflow project, and it is particularly useful for data integration or data processing workloads. Like Data Fusion, it integrates with other Google Cloud services such as BigQuery, Dataflow, Dataproc, Datastore, Cloud Storage, and Pub/Sub, making it easy to orchestrate data processing workflows across <span>those services.</span></p>
<p class="calibre3">It is highly scalable, and it can also be used to implement workloads across multiple cloud providers and on-premises locations. It orchestrates workflows by using <strong class="bold">Directed Acyclic Graphs</strong> (<strong class="bold">DAGs</strong>), which represent the tasks your workflow needs to execute, as well as all of the relationships and dependencies <span>between them.</span></p>
<p class="calibre3">Similar to Google Cloud Workflows, Composer monitors the execution of the tasks in your workload and tracks whether each step is completed correctly or whether any problems have occurred. <span><em class="italic">Figure 3</em></span><em class="italic">.12</em> shows an example of a workflow being orchestrated by Google Cloud Composer, in which data regarding customer orders is ingested periodically from Cloud Bigtable into Cloud Dataproc and enriched with data regarding broader retail trends from third-party providers, which is provided via Google Cloud Storage. The outputs are then stored in BigQuery for analysis. This data could then be used to build business intelligence dashboards in Looker or to train machine learning models in Vertex AI, <span>for example.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer057">
<img alt="Figure 3.12: Data processing workflow" src="image/B18143_03_12.jpg" class="calibre65"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.12: Data processing workflow</p>
<p class="calibre3">As depicted in <span><em class="italic">Figure 3</em></span><em class="italic">.12</em>, when we’ve processed and stored our data, we may want to use it to train a machine learning model. Before we get <a id="_idTextAnchor125" class="calibre6 pcalibre pcalibre1"/>into training our own models, let’s explore <a id="_idIndexMarker383" class="calibre6 pcalibre pcalibre1"/>some <a id="_idIndexMarker384" class="calibre6 pcalibre pcalibre1"/>of the AI/ML capabilities that we can use on Google Cloud that provide models trained <span>by Google.</span></p>
<h1 id="_idParaDest-74" class="calibre5"><a id="_idTextAnchor126" class="calibre6 pcalibre pcalibre1"/>Google Cloud AI tools and AutoML</h1>
<p class="calibre3">In this section, we <a id="_idIndexMarker385" class="calibre6 pcalibre pcalibre1"/>will cover Google Cloud’s AI tools <a id="_idIndexMarker386" class="calibre6 pcalibre pcalibre1"/>that can be used to implement AI use cases without the need to understand the underlying machine learning concepts. With these services, you can simply send a request to an API, and get a response from ML models that are created and maintained by Google. There’s no need to manually preprocess data, train or hyper-tune machine learning models, or manage <span>any infrastructure.</span></p>
<p class="calibre3">We will <a id="_idIndexMarker387" class="calibre6 pcalibre pcalibre1"/>group <a id="_idIndexMarker388" class="calibre6 pcalibre pcalibre1"/>these services into the following <a id="_idIndexMarker389" class="calibre6 pcalibre pcalibre1"/>categories: <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>), <strong class="bold">Computer Vision</strong>, <span>and </span><span><strong class="bold">Discovery</strong></span><span>.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">At the time of writing this, in January 2023, Google has also announced a preview of a service <a id="_idIndexMarker390" class="calibre6 pcalibre pcalibre1"/>named <strong class="bold">Timeseries Insights API</strong>, which can be used to perform analysis and gather insights in real time from your time series datasets, for use cases such as time series forecasting, or detecting anomalies and trends in your data while they are happening. This is an interesting new service because running forecasting and anomaly detection workloads over billions of time series data points is computationally intensive, and most existing systems implement these workloads as batch<a id="_idTextAnchor127" class="calibre6 pcalibre pcalibre1"/> jobs, which limits the type of analysis you can perform online, such as deciding whether to alert based on a sudden increase or decrease in <span>data values.</span></p>
<h2 id="_idParaDest-75" class="calibre9"><a id="_idTextAnchor128" class="calibre6 pcalibre pcalibre1"/>NLP</h2>
<p class="calibre3">Natural language <a id="_idIndexMarker391" class="calibre6 pcalibre pcalibre1"/>processing, in the context of AI, refers to the use of computers to understand and process natural human language. It can be further broken down<a id="_idIndexMarker392" class="calibre6 pcalibre pcalibre1"/> into <strong class="bold">Natural Language Understanding</strong> (<strong class="bold">NLU</strong>) and <strong class="bold">Natural Language Generation</strong> (<strong class="bold">NLG</strong>). NLU is <a id="_idIndexMarker393" class="calibre6 pcalibre pcalibre1"/>concerned with understanding the content and meaning of words and sentences, as they are understood by humans. NLG takes this one step further and attempts to c<a id="_idTextAnchor129" class="calibre6 pcalibre pcalibre1"/>reate or generate words and sentences in a way that can be understood by humans. In this section, we’ll discuss some of Google Cloud’s <span>NLP-related services.</span></p>
<h2 id="_idParaDest-76" class="calibre9"><a id="_idTextAnchor130" class="calibre6 pcalibre pcalibre1"/>The Natural Language API</h2>
<p class="calibre3">You can use the<a id="_idIndexMarker394" class="calibre6 pcalibre pcalibre1"/> Google Cloud Natural Language API to understand the contents of textual inputs. This can be used for purposes such as sentiment analysis, entity analysis, content classification, and syntax analysis. With sentiment analysis, it can tell you what kinds of emotions are suggested by the content. For example, you could feed all of your product reviews into this API and get an understanding of whether people are responding positively to your product, or whether they may be frustrated by something about the product. Entity analysis can identify what kinds of content exist in the text, such as people’s names, place names, locations, addresses, and phone numb<a id="_idTextAnchor131" class="calibre6 pcalibre pcalibre1"/>ers. The content classification feature is useful if you have large amounts of textual data and you want to organize and categorize it based on <span>the contents.</span></p>
<h3 class="calibre11">Text-to-Speech</h3>
<p class="calibre3">The name of this<a id="_idIndexMarker395" class="calibre6 pcalibre pcalibre1"/> service is somewhat self-explanatory. The <a id="_idIndexMarker396" class="calibre6 pcalibre pcalibre1"/>service will take a textual input that you provide, and it will convert it to an audible spoken output. This can be very useful for accessibility use cases, whereby if somebody is visually impaired and cannot read text content, it could be automatically spoken to them. It provides the option to use many different voices to personalize the user experience, and you can even create custom voices using your own recordings. As of January 2023, it supported more than 40 lang<a id="_idTextAnchor132" class="calibre6 pcalibre pcalibre1"/>uages and variants. It also supports<strong class="bold"> Speech Synthesis Markup Language</strong> (<strong class="bold">SSML</strong>), which <a id="_idIndexMarker397" class="calibre6 pcalibre pcalibre1"/>enables you to have more control over how words and phrases <span>are pronounced.</span></p>
<h3 class="calibre11">Speech-to-Text</h3>
<p class="calibre3">This service does <a id="_idIndexMarker398" class="calibre6 pcalibre pcalibre1"/>pretty much the opposite of the previous<a id="_idIndexMarker399" class="calibre6 pcalibre pcalibre1"/> service. In this case, you can provide audio inputs, and the service will transcribe any spoken language into a textual output. This is useful for dictation purposes, accessibility use cases such as closed captioning, and other use cases such as quality control. For example, you could provide recordings of your customer service calls and it would convert them to text. Then you could feed that text into the Natural Language API to understand wheth<a id="_idTextAnchor133" class="calibre6 pcalibre pcalibre1"/>er your customers are frustrated or happy with the service they are receiving. As of January 2023, the service supported an impressive 125 languages <span>and variants.</span></p>
<h3 class="calibre11">Translation AI</h3>
<p class="calibre3">Another service<a id="_idIndexMarker400" class="calibre6 pcalibre pcalibre1"/> whose name is self-explanatory, this<a id="_idIndexMarker401" class="calibre6 pcalibre pcalibre1"/> service can be used to translate from one language to another. It can help you to internationalize your products, and engage your customers with localization of content. It can detect more than 100 languages, and you can customize the translations with industry- or domain-specific terms. It provides <strong class="bold">Translation Hub</strong>, which<a id="_idTextAnchor134" class="calibre6 pcalibre pcalibre1"/><a id="_idIndexMarker402" class="calibre6 pcalibre pcalibre1"/> allows you to manage translation workloads at scale, as well as Media Translation API, which can deliver real-time audio translation directly to <span>your applications.</span></p>
<h3 class="calibre11">Contact Center AI (CCAI)</h3>
<p class="calibre3"><strong class="bold">Contact Center AI</strong> (<strong class="bold">CCAI</strong>) provides <a id="_idIndexMarker403" class="calibre6 pcalibre pcalibre1"/>human-like<a id="_idIndexMarker404" class="calibre6 pcalibre pcalibre1"/> AI-powered contact center experiences. It consists of a number of different components, such as Dialogflow, which can be used to create chatbots that can have intelligent, human-like conversations <span>with customers.</span></p>
<p class="calibre3">Have you ever been on hold with a company’s customer service line for an hour before anybody helps you with your concerns? This, of course, happens when customer service centers are overloaded with calls. Using chatbots can offload a significant amount of cases with simple questions, freeing up humans to focus on more complex <span>customer interactions.</span></p>
<p class="calibre3">When a human does need to get involved, CCAI has another feature, named Agent Assist, that provides support to human agents while they handle customer interactions. It can recommend ready-to-send responses to customers, provide answers to customer questions from a centralized knowledge base, and transcribe calls in <span>real time.</span></p>
<p class="calibre3">CCAI also includes CCAI Insights, which uses NLP to identify customer sentiment and reasons for calls, which helps contact center managers l<a id="_idTextAnchor135" class="calibre6 pcalibre pcalibre1"/>earn about customer interactions to improve <span>call outcomes.</span></p>
<p class="calibre3">There’s also the option to use CCAI Platform, which <a id="_idIndexMarker405" class="calibre6 pcalibre pcalibre1"/>provides a <strong class="bold">Contact Center as a Service</strong> (<span><strong class="bold">CCaaS</strong></span><span>) solution.</span></p>
<h3 class="calibre11">Document AI</h3>
<p class="calibre3">Document<a id="_idIndexMarker406" class="calibre6 pcalibre pcalibre1"/> AI goes <a id="_idIndexMarker407" class="calibre6 pcalibre pcalibre1"/>beyond understanding the content of textual inputs, and also incorporates structure. It provides pre-trained models for data extraction, or you can use Document AI Workbench to create custom models, and you can use Document AI Warehouse to search and store documents. For example, if you collect information from people via forms, Document AI could be used to extract the data from those forms and store it in a database, or send it to another data processing system to process the data in some way or feed it to another ML model to perform some kind of other task. It could also be used to categorize and organize documents based on their contents. Some companies process millions of forms and contracts per year, and before these kinds of AI systems existed, all of those documents had to be processed by humans, which led to extremely laborious and error-prone work. With Document AI, you can automate that work, and you can also enrich the data using <a id="_idIndexMarker408" class="calibre6 pcalibre pcalibre1"/>Google <strong class="bold">Enterprise Knowledge Graph</strong> (<strong class="bold">EKG</strong>), or you can enhance the functionality of Document AI with human inputs by <a id="_idIndexMarker409" class="calibre6 pcalibre pcalibre1"/>using its <strong class="bold">Human-in-the-Loop</strong> (<strong class="bold">HITL</strong>) AI functionality. With HITL AI, experts can verify the outputs from Document AI, and provide corrections if needed. You can either use your own workforce of experts for this purpose or, if you don’t have such experts in your employment, <a id="_idTextAnchor136" class="calibre6 pcalibre pcalibre1"/>you can use Google’s <span>HITL workforce.</span></p>
<p class="calibre3">Document <a id="_idIndexMarker410" class="calibre6 pcalibre pcalibre1"/>AI’s <strong class="bold">Optical Character Recognition</strong> (<strong class="bold">OCR</strong>) functionality<a id="_idIndexMarker411" class="calibre6 pcalibre pcalibre1"/> flows over into the computer vision realm, which <a id="_idIndexMarker412" class="calibre6 pcalibre pcalibre1"/>we will <span>discuss next.</span></p>
<h2 id="_idParaDest-77" class="calibre9"><a id="_idTextAnchor137" class="calibre6 pcalibre pcalibre1"/>Computer vision</h2>
<p class="calibre3">You can use Google Cloud <a id="_idIndexMarker413" class="calibre6 pcalibre pcalibre1"/>Vision AI to create your own computer vision applications or get insights from images and videos with pre-trained APIs, AutoML, or custom models. It enables you to spin up video and image analytics applications in minutes, for use cases such as detecting objects, reading handwriting, or creating image metadata. It consists of three main components: Vertex AI Vision, Vision API, and custom ML models. Vertex AI Vision includes <strong class="bold">Streams</strong> to ingest real-time video data, <strong class="bold">Applications</strong> to let you create an application by combining various components, and <strong class="bold">Vision Warehouse</strong> to store model output and streaming data. The Vision API offers pre-trained ML models that you can access through REST and RPC APIs, allowing you to assign labels to images and classify them into millions of predefined categories. If you need to develop more specialized models, you can use AutoML or build your own <span>custom models.</span></p>
<p class="calibre3">Vision AI can be used to implement interesting use cases such as image search, in which you could use Vision API and AutoML Vision to make images searchable based on topics and scenes detected in the images, or product search, in which you could enable customers to find products of interest within images and visually search product catalogs using the <span>Vision API.</span></p>
<p class="calibre3">Also in the computer vision space is Google Cloud Video AI, which can analyze video content for use cases such as content discovery. Video AI can recognize over 20,000 objects, places, and actions in video, it can extract metadata at the video, shot, or frame level, and you can even create your own custom entity labels with AutoML Video Intelligence. It consists of two main components: the Video Intelligence API, which provides pre-trained ML models, and Vertex AI for AutoML video, which provides a graphical interface to train your own custom models to classify and track objects in videos<a id="_idTextAnchor138" class="calibre6 pcalibre pcalibre1"/>, without the need for ML experience. Vertex AI for AutoML video can be used for projects requiring<a id="_idIndexMarker414" class="calibre6 pcalibre pcalibre1"/> custom labels that aren’t covered by the pre-trained Video <span>Intelligence API.</span></p>
<h3 class="calibre11">Discovery AI</h3>
<p class="calibre3">Google Cloud Discovery <a id="_idIndexMarker415" class="calibre6 pcalibre pcalibre1"/>AI includes services such as search and recommendation engines. These kinds of features are essential for today’s online businesses, in which it’s important to ensure that your customers find what they want on your website as quickly and easily as possible. If this doesn’t happen, they’ll go to your competitor’s website. Imagine if your company could integrate Google-quality search into its website. This includes functionality such as image-based product searches, which we discussed in the previous section, making it easier for customers to search for products with an image, by using object recognition to provide real-time results of similar items from your <span>product catalog.</span></p>
<p class="calibre3">In addition to your customers finding products by directly searching for them, companies see a significant amount of business being driven by recommendation engines, which can make personalized recommendations for products based on customers’ previous purchasing behavior. I’ve experienced this many times myself as a consumer, whereby I’m purchasing something on a website, I see a recommendation for something else that I may be interested in purchasing, and I think “<em class="italic">Actually, yes I do also want one of those</em>,” and I go ahead and add it to my cart before checking out. These kinds of personalized experiences help to maintain customer loyalty, which is also extremely important in today’s online <span>business world.</span></p>
<p class="calibre3">Bringing these two concepts together – that is, search and personalization – makes a lot of sense, because not only will the search results intelligently match what the customer searched for, but the ranking of the results can be catered to what the specific customer is more likely to find relevant to <span>their preferences.</span></p>
<p class="calibre3">A third component of Discovery AI is Browse AI, which extends the search functionality to work on category pages in addition to text queries. Without this, retailers would mostly sort products on their category and navigation pages by historic bestsellers. This ordering doesn’t adapt well to new product additions, changes in product availability, and sales. With Browse AI, retailers can sort products on these pages in an order that is personalized to each user, and based on predicted, rather than historic, bestsellers. This ordering can rapidly adapt to new products, product stockouts, and price changes, without needing to wait for backward-looking bestseller lists to <span>catch up.</span></p>
<p class="calibre3">Now that we’ve taken a look at the high-level AI services on Google Cloud, which you can use to get inferences <a id="_idTextAnchor139" class="calibre6 pcalibre pcalibre1"/>from ML models that are trained and maintained by Google by simply calling an API, let’s<a id="_idIndexMarker416" class="calibre6 pcalibre pcalibre1"/> discuss how you can start to train your own models on Google Cloud, starting <span>with AutoML.</span></p>
<h1 id="_idParaDest-78" class="calibre5"><a id="_idTextAnchor140" class="calibre6 pcalibre pcalibre1"/>AutoML</h1>
<p class="calibre3">Some of the services we<a id="_idIndexMarker417" class="calibre6 pcalibre pcalibre1"/> discussed in the previous sections use completely pre-trained models, and others allow you to bring your own data to either train or <em class="italic">up-train</em> a model based on your data. Pre-trained models are trained on datasets provided by Google or other sources, and the term, up-train, refers to augmenting a pre-trained model with additional data. If you want to create more customized use cases than those supported by the high-level API services, you may want to train your own models. Vertex AI, which we will describe later in this chapter, provides a plethora of tools for implementing every step in the model development process. However, before we get to the level of customizing every step in the process, one way in which you can easily start getting inferences from ML models that are trained on your data in Google Cloud is to use AutoML, which enables developers with limited ML expertise to train models specific to their business needs in as little as a few minutes or hours. The actual amount of time depends on the algorithms being used, how much data is used for training, and some other factors, but in any case, AutoML saves you a lot of work and time, considering that data scientists can spend weeks on these tasks when not using AutoML. Looking at our ML model development lifecycle diagram in <span><em class="italic">Figure 3</em></span><em class="italic">.13</em>, AutoML automatically performs all of the steps in the process for us, as indicated by everything encapsulated within the <span>blue box.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer058">
<img alt="Figure 3.13: ML model lifecycle managed by AutoML" src="image/B18143_03_13.jpg" class="calibre66"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.13: ML model lifecycle managed by AutoML</p>
<p class="calibre3">How does AutoML work? If we think back to all of the steps in a typical data science project that we discussed in <a href="B18143_02.xhtml#_idTextAnchor035" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 2</em></span></a>, you may remember that each step – especially in the early stages of a project – required a lot of trial and error. For example, you might try a number of different data transformation techniques, and then try out a few different algorithms during training, as well as different combinations of hyperparameter values for those algorithms. Each of those steps could take many days or weeks of work before you find a good candidate model (i.e., a model that we believe may satisfy our business objectives and metrics). AutoML automatically runs many trial jobs very quickly – generally much more quickly than a human could achieve – and evaluates the outcomes against desired metric thresholds. Jobs whose outcomes do not meet the desired criteria are not selected as candidates, and AutoML continues to try other options until suitable candidates are found, or some other threshold is met, such as all options <span>being exhausted.</span></p>
<p class="calibre3">How can you start using <a id="_idIndexMarker418" class="calibre6 pcalibre pcalibre1"/>AutoML? Some of the AI services we discussed in the previous sections in this chapter already use AutoML in order to train the models that you access via those APIs. Examples include AutoML image, in which you can get insights from object detection and image classification, AutoML Video for streaming video analysis, AutoML Text to understand the structure, meaning, and sentiment of text, AutoML Translation to translate between languages, and AutoML Forecasting to provide forecasts based on time <span>series data.</span></p>
<p class="calibre3">Another AutoML use case that we haven’t explored yet is AutoML for tabular data (structured data that’s stored in tables). This is a very common format for storing business data, as it provides a way to organize information that is easy for humans to read and understand. AutoML Tabular supports multiple ML use cases with tabular data, such as binary classification, multi-class classification, regression, <span>and forecasting.</span></p>
<p class="calibre3">We can use AutoML to automate a lot of the trial and error steps and develop candidate models, and then we can customize further from that point if we wish. For example, we could use Vertex AI Tabular Workflows, which creates a <em class="italic">glassbox</em>-managed AutoML pipeline that lets us see and interpret each step in the model building and deployment process. We can then tweak any steps in the process as we see fit, and automate any updates via MLOps pipelines. We will be performing exactly these kinds of activities in later chapters of <span>this book.</span></p>
<p class="calibre3">Next, we’re going <a id="_idIndexMarker419" class="calibre6 pcalibre pcalibre1"/>to dive deeper into ML model customization, and we will explore Vertex AI in more detail, as it can be used to customize every step in a data <span>science project.</span></p>
<h1 id="_idParaDest-79" class="calibre5"><a id="_idTextAnchor141" class="calibre6 pcalibre pcalibre1"/>Google Cloud Vertex AI</h1>
<p class="calibre3">This is where we <a id="_idIndexMarker420" class="calibre6 pcalibre pcalibre1"/>start getting to expert-level AI/ML. Think back on all of the data science concepts we’ve covered so far in this book; all of the different types of ML approaches, algorithms, use cases, and tasks. Vertex AI is where you can accomplish all of them, and many more that we will yet explore in this book. You can use Vertex AI as your central command center for performing everything AI/ML-related. Our ML model lifecycle diagram in <span><em class="italic">Figure 3</em></span><em class="italic">.14</em> illustrates this point graphically. Traditionally, we would expect an AI/ML platform to mainly take care of training, evaluating, deploying, and monitoring models. This is what’s represented by the blue box on the right-hand side of <span><em class="italic">Figure 3</em></span><em class="italic">.14</em>, and all of these activities are of course supported by Vertex AI. However, with additional features such as notebooks and MLOps pipelines, Vertex goes beyond just those traditional ML activities, to also enable us to perform all of the tasks in our lifecycle, including data exploration and processing, as represented by the light blue dashed box on the left-hand side of the <span>following diagram:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer059">
<img alt="Figure 3.14: ML model lifecycle with Vertex AI" src="image/B18143_03_14.jpg" class="calibre67"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 3.14: ML model lifecycle with Vertex AI</p>
<p class="calibre3">Let’s take a look at Vertex AI’s features in a bit more detail. Starting with the basics, we can use Vertex AI’s Deep Learning VM Images to instantiate a VM image containing the most popular AI frameworks on a Compute Engine instance, or we can use Vertex AI Deep Learning Containers to quickly build and deploy models in a portable and consistent <span>containerized environment.</span></p>
<p class="calibre3">The VMs and container images provide building blocks on which we can develop customized ML workloads, but we can also use Vertex AI in other ways to perform and manage all of the steps in our model development lifecycle. We can use Jupyter notebooks to explore data and experiment with each of the steps in the process, while Vertex AI Data Labeling enables us to get highly accurate labels from human labelers for creating better supervised ML models, and Vertex AI Feature Store provides a fully managed feature repository for serving, sharing, and reusing ML features. Vertex AI Training provides pre-built algorithms and allows users to bring their custom code to train models in a fully managed training service for greater flexibility and customization, or for users running training on-premises or in another cloud environment. Vertex AI Vizier automates all of our hyperparameter tuning jobs for us, finding an optimal set of hyperparameter values for us, and saving us from having to do a lot of painstaking work manually! Optimized hyperparameter values lead to more accurate and more <span>efficient models.</span></p>
<p class="calibre3">When we need to<a id="_idIndexMarker421" class="calibre6 pcalibre pcalibre1"/> deploy our models, we can use Vertex AI Predictions, which will host our models on infrastructure that’s managed by Google, which auto-scales to meet our models’ traffic needs. It can host our models for batch or online use cases, and it offers a unified framework to deploy custom models built on any framework, such as TensorFlow, PyTorch, scikit-learn, or XGB, as well as BigQuery ML and AutoML models, and on a broad range of machine types and GPUs. After deployment, we can use Vertex AI Model Monitoring to provide automated alerts for data drift, concept drift, or other model performance incidents that may require supervision. We can then automate the entire process as MLOps pipelines by using Vertex Pipelines, which allows us to trigger retraining of our models when needed, and to manage updates to our models in a version-controlled way. We can also use Vertex AI TensorBoard to visualize ML experiment outcomes and compare models and model versions against each other to easily identify the best-performing models. This visualization and tracking tool for ML experimentation includes model graphs that display images, text, and <span>audio data.</span></p>
<p class="calibre3">What’s really great is that we can perform and manage all of these activities from Vertex AI Workbench, which is a Jupyter-based fully managed, scalable, enterprise-ready compute infrastructure with security controls and user management capabilities. All the while, the lineage details of our models as they progress through every step in the process can be tracked by Vertex Experiments and the Vertex ML Metadata service, which provides artifact, lineage, and execution tracking for ML workflows, with an easy-to-use <span>Python SDK.</span></p>
<p class="calibre3">Vertex AI provides even more functionality beyond all of the features we mentioned previously, such as Vertex AI Matching Engine, which is a massively scalable, low latency, and cost-efficient vector similarity matching service. Vertex AI Neural Architecture Search enables us to build new model architectures targeting application-specific needs and optimize our existing model architectures for latency, memory, and power, in an automated service, and we can use Vertex Explainable AI to understand and build trust in our model predictions with actionable explanations integrated into Vertex AI Prediction, AutoML Tables, and Vertex AI Workbench. Explainable AI provides detailed model evaluation<a id="_idIndexMarker422" class="calibre6 pcalibre pcalibre1"/> metrics and feature attributions, indicating how important each input feature is to <span>our predictions.</span></p>
<h1 id="_idParaDest-80" class="calibre5"><a id="_idTextAnchor142" class="calibre6 pcalibre pcalibre1"/>Standard industry tools on Google Cloud</h1>
<p class="calibre3">In addition to Google<a id="_idIndexMarker423" class="calibre6 pcalibre pcalibre1"/> Cloud’s own data science tools that we’ve been describing so far in this chapter, you can also use other data science tools such as open source frameworks or other popular industry solutions. There are lots of great libraries out there that make it easy to perform the various tasks in the model development life cycle. When it comes to data exploration and processing, for example, the beloved pandas library is a staple of any ML and data analysis course. You can use it for handling missing data, slicing, subsetting, reshaping, merging, and joining datasets. Matplotlib is right up there with pandas for data exploration as it allows you to visualize your data via customizable and interactive plots and charts that can be exported into various file formats. NumPy allows you to easily manipulate and play around with the kinds of <em class="italic">n</em>-dimensional arrays and vectors we find in so many ML implementations. Learning NumPy also sets you up to start using frameworks such as scikit-Learn, TensorFlow, <span>and PyTorch.</span></p>
<p class="calibre3">Speaking of scikit-learn, it is as much of a staple in any machine learning course as pandas. If you take an ML course, you will almost certainly use scikit-learn at some point in your learning process, and for good reason; it’s a framework that’s easy to use and understand and contains lots of built-in algorithms and datasets that you can use to implement ML workloads. And, it’s more than just an easy framework for learning purposes; many companies also use scikit-learn for their production <span>ML workloads.</span></p>
<p class="calibre3">While we’re still on the topic of general-purpose ML frameworks, the next framework we’ll discuss is the wildly popular TensorFlow, which was originally created by Google before they open sourced it, and is therefore very well supported on Google Cloud. TensorFlow can be used for everything from processing data to NLP and computer vision. You can use it to train, deploy, and serve models, and with <strong class="bold">TensorFlow Extended</strong> (<strong class="bold">TFX</strong>), you<a id="_idIndexMarker424" class="calibre6 pcalibre pcalibre1"/> can implement end-to-end MLOps pipelines to automate all of those steps. We’ll certainly be exploring TensorFlow in more detail in this book, as well as Keras, which is an API that provides access to TensorFlow via a Python interface that’s popular for its ease of use, and advertises itself as “<em class="italic">an API designed for human beings, </em><span><em class="italic">not machines</em></span><span>.”</span></p>
<p class="calibre3">We’ll round out our discussion of general-purpose ML frameworks with PyTorch, which was originally developed by Facebook (Meta AI) and is now open sourced under the Linux Foundation. PyTorch has been rapidly gaining popularity in recent years, especially among Python developers, and it has become a very widely used framework in addition to TensorFlow. In this book, we’re not going to get into the argument of which framework is better. There are staunch supporters on each side of this debate, and if you Google “TensorFlow versus PyTorch,” you’ll find no shortage of websites and forums highlighting how one is better than the other for particular types of use cases. We will also be <a id="_idIndexMarker425" class="calibre6 pcalibre pcalibre1"/>using PyTorch in later chapters of <span>this book.</span></p>
<p class="calibre3">Switching gears from general-purpose ML frameworks to more specialized frameworks, you might want to use something such as OpenCV for computer vision workloads, or SpaCy for NLP. OpenCV has a broad selection of algorithms for applying ML and deep learning to images and video content, to perform many of the tasks we discussed earlier in this chapter, such as object recognition, and tracking objects and actions through video frames. SpaCy has lots of pre-trained word embeddings and pipelines supporting multiple languages, and it supports custom models written in TensorFlow and PyTorch and lots of Python packages <span>for NLP.</span></p>
<p class="calibre3">The good news is that all of the tools and frameworks we’ve discussed in this section can easily be used on Google Cloud. In addition to open source tools, there are many popular third-party data science solutions that can be used on Google Cloud, providing flexibility for people to use whatever tools they prefer for the objectives they want to achieve. We’ve already talked about using Spark on Google Cloud through services such as Dataproc and Vertex AI, and you can use third-party Spark offerings such as Databricks via the Google Cloud Marketplace. The marketplace allows you to find thousands of solutions that run on Google Cloud, including deep learning solutions from companies such as <span>Hugging Face.</span></p>
<p class="calibre3">With all of these tools and all of the services provided by Google Cloud, you might wonder how to choose the right tool for your data science workloads. Let’s take a look at that discussion<a id="_idIndexMarker426" class="calibre6 pcalibre pcalibre1"/> in more detail in the <span>next section.</span></p>
<h1 id="_idParaDest-81" class="calibre5"><a id="_idTextAnchor143" class="calibre6 pcalibre pcalibre1"/>Choosing the right tool for the job</h1>
<p class="calibre3">Your choice of <a id="_idIndexMarker427" class="calibre6 pcalibre pcalibre1"/>data processing tools will depend heavily on what kind of data processing tasks you need to accomplish. If you have a bunch of raw data that you need to transform in bulk, as in an ETL/ELT task, Data Fusion would be a good place to start your assessment, whereas if you want to perform relatively simple transformations using SQL syntax, then start with BigQuery, and if you want to visualize and transform data via an easy-to-use GUI, then go with Dataprep. If you prefer to stick to using open source tools, then you might want to use something like pandas or Spark. We discussed pandas being a good starting point for people who are beginning to learn about data exploration and preprocessing, and how it’s also more than an educational tool. pandas is really great for initial data exploration and data processing at a moderate scale. However, for large-scale data processing projects, Spark’s highly parallelized functionality will be a lot more efficient, and if you want to use it without managing the infrastructure yourself, then you can run it on Dataproc. Dataflow is the recommended choice for streaming data, and it has the additional benefit of also working well for batch data processing with its unified <span>programming model.</span></p>
<p class="calibre3">When it comes to choosing a tool for AI/ML workloads, the decision may be a bit more straightforward. If you or your company has a pre-existing affinity for a specific third party such as Hugging Face, then you may be guided in that direction. The general best practice is that you should start with the highest level of abstraction available to you because then you won’t have to spend a lot of time and effort building and maintaining something that is already available as a service for you to use. If that does not meet your needs for any reason, such as specific business requirements that require some level of customization, then move to a solution that provides more control over how the workload is implemented. For example, if you’re building an application that needs to include some kind of NLP functionality, start by evaluating the Google Cloud NLP API. If, for any reason, you cannot achieve your desired objective with that solution, move on to evaluating the use of AutoML to automate training a model on your specific data. If your objective is still not met by the models created during that process, then it’s time to increase your level of customization, and potentially use Vertex AI to build a completely <span>customized model.</span></p>
<p class="calibre3">Another very important factor in your decision process is your budget. Services that do more work for you – such as the higher-level AI services – may cost more than the lower-level services, but it’s very important to factor in how much you are paying your employees to manage <a id="_idIndexMarker428" class="calibre6 pcalibre pcalibre1"/>infrastructure and perform the tasks that could be performed on your behalf by the <span>higher-level services.</span></p>
<h1 id="_idParaDest-82" class="calibre5"><a id="_idTextAnchor144" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we covered all of the foundational and primary services that you can use for implementing AI/ML workloads on Google Cloud. We started with the basic services such as Google Compute Engine, and Google Cloud networking services, upon which all other services are built. We then explored how you could use those services to set up connectivity with systems outside of Google Cloud. Next, we discussed the services that you can use to import or transfer data to Google Cloud and the various storage systems that you can use for storing that data in the cloud. Having covered the primary storage systems, we moved on to discuss Google Cloud’s data management and data processing services. The final step in our journey was to understand all of the different AI/ML services that exist in <span>Google Cloud.</span></p>
<p class="calibre3">At this point, you have already learned a lot, and with this knowledge, you could now have an intelligent discussion about AI/ML and Google Cloud. This is a significant achievement because people spend a very long time trying to learn about AI/ML, and a very long time trying to learn about Google Cloud. Right now, you know more than a lot of people out there, and you should feel proud and give yourself a pat on the back for coming <span>this far.</span></p>
<p class="calibre3">This concludes the <em class="italic">Basics</em> part of our book, and all of the knowledge you have gained in the previous chapters will form the basis of what you will learn in the rest of this book. In the next chapter and beyond, we will start performing hands-on activities, diving deeper into the services and concepts we have covered thus far, and actually start to build data science workloads on <span>Google Cloud!</span></p>
</div>
</div></body></html>