["```py\n> modelLookup(\"C5.0\") \n```", "```py\n model parameter                 label forReg forClass probModel\n1  C5.0    trials # Boosting Iterations  FALSE     TRUE      TRUE\n2  C5.0     model            Model Type  FALSE     TRUE      TRUE\n3  C5.0    winnow                Winnow  FALSE     TRUE      TRUE \n```", "```py\n> library(caret)\n> set.seed(300)\n> m <- train(default ~ ., data = credit, method = \"C5.0\") \n```", "```py\n> p <- predict(m, credit) \n```", "```py\n> table(p, credit$default) \n```", "```py\np      no yes\n  no  700   2\n  yes   0 298 \n```", "```py\n> head(predict(m, credit)) \n```", "```py\n[1] no  yes no  no  yes no \nLevels: no yes \n```", "```py\n> head(predict(m, credit, type = \"prob\")) \n```", "```py\n no        yes\n1 0.9606970 0.03930299\n2 0.1388444 0.86115560\n3 1.0000000 0.00000000\n4 0.7720279 0.22797207\n5 0.2948061 0.70519387\n6 0.8583715 0.14162853 \n```", "```py\n> ctrl <- trainControl(method = \"cv\", number = 10,\n                       selectionFunction = \"oneSE\") \n```", "```py\n> grid <- expand.grid(model = \"tree\",\n                      trials = c(1, 5, 10, 15, 20, 25, 30, 35),\n                      winnow = FALSE) \n```", "```py\n> grid \n```", "```py\n model trials winnow\n1  tree      1  FALSE\n2  tree      5  FALSE\n3  tree     10  FALSE\n4  tree     15  FALSE\n5  tree     20  FALSE\n6  tree     25  FALSE\n7  tree     30  FALSE\n8  tree     35  FALSE \n```", "```py\n> set.seed(300)\n> m <- train(default ~ ., data = credit, method = \"C5.0\",\n             metric = \"Kappa\",\n             trControl = ctrl,\n             tuneGrid = grid) \n```", "```py\n> m \n```", "```py\nC5.0 \n1000 samples\n  16 predictor\n   2 classes: 'no', 'yes' \nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \nResampling results across tuning parameters:\n  trials  Accuracy  Kappa    \n   1      0.710     0.2859380\n   5      0.726     0.3256082\n  10      0.725     0.3054657\n  15      0.726     0.3204938\n  20      0.733     0.3292403\n  25      0.732     0.3308708\n  30      0.733     0.3298968\n  35      0.738     0.3449912\nTuning parameter 'model' was held constant at a value of tree\nTuning parameter 'winnow' was held constant at a value of FALSE\nKappa was used to select the optimal model using the one SE rule.\nThe final values used for the model were trials = 5, model = tree\n and winnow = FALSE. \n```", "```py\n> library(ipred)\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> set.seed(123)\n> mybag <- bagging(default ~ ., data = credit, nbagg = 25) \n```", "```py\n> credit_pred <- predict(mybag, credit)\n> table(credit_pred, credit$default) \n```", "```py\n credit_pred  no yes\n        no  699   4\n        yes   1 296 \n```", "```py\n> library(caret)\n> credit <- read.csv(\"credit.csv\")\n> set.seed(300)\n> ctrl <- trainControl(method = \"cv\", number = 10)\n> train(default ~ ., data = credit, method = \"treebag\",\n        trControl = ctrl) \n```", "```py\nBagged CART \n1000 samples\n  16 predictor\n   2 classes: 'no', 'yes' \nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \nResampling results:\n  Accuracy  Kappa    \n  0.732     0.3319334 \n```", "```py\n> library(adabag)\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> set.seed(300)\n> m_adaboost <- boosting(default ~ ., data = credit) \n```", "```py\n> p_adaboost <- predict(m_adaboost, credit) \n```", "```py\n> head(p_adaboost$class) \n```", "```py\n[1] \"no\"  \"yes\" \"no\"  \"no\"  \"yes\" \"no\" \n```", "```py\n> p_adaboost$confusion\n               Observed Class \n```", "```py\nPredicted Class  no yes\n            no  700   0\n            yes   0 300 \n```", "```py\n> set.seed(300)\n> adaboost_cv <- boosting.cv(default ~ ., data = credit) \n```", "```py\n> adaboost_cv$confusion\n               Observed Class \n```", "```py\nPredicted Class  no yes\n            no  598 160\n            yes 102 140 \n```", "```py\n> library(vcd)\n> Kappa(adaboost_cv$confusion) \n```", "```py\n value     ASE     z  Pr(>|z|)\nUnweighted 0.3397 0.03255 10.44 1.676e-25\nWeighted   0.3397 0.03255 10.44 1.676e-25 \n```", "```py\n> library(randomForest)\n> set.seed(300)\n> rf <- randomForest(default ~ ., data = credit) \n```", "```py\n> rf \n```", "```py\nCall:\n randomForest(formula = default ~ ., data = credit) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n        OOB estimate of  error rate: 23.3%\nConfusion matrix:\n     no yes class.error\nno  638  62  0.08857143\nyes 171 129  0.57000000 \n```", "```py\n> library(vcd)\n> Kappa(rf$confusion[1:2,1:2]) \n```", "```py\n value     ASE     z  Pr(>|z|)\nUnweighted 0.381 0.03215 11.85 2.197e-32\nWeighted   0.381 0.03215 11.85 2.197e-32 \n```", "```py\n> library(ranger)\n> set.seed(300)\n> m_ranger <- ranger(default ~ ., data = credit) \n```", "```py\n> m_ranger \n```", "```py\nRanger result\nCall:\n ranger(default ~ ., data = credit) \nType:                             Classification \nNumber of trees:                  500 \nSample size:                      1000 \nNumber of independent variables:  16 \nMtry:                             4 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             23.10 % \n```", "```py\n> Kappa(m_ranger$confusion.matrix)\n           value    ASE     z  Pr(>|z|) \n```", "```py\nUnweighted 0.381 0.0321 11.87 1.676e-32\nWeighted   0.381 0.0321 11.87 1.676e-32 \n```", "```py\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> credit$default <- ifelse(credit$default == \"yes\", 1, 0)\n> set.seed(123)\n> train_sample <- sample(1000, 900)\n> credit_train <- credit[train_sample, ]\n> credit_test  <- credit[-train_sample, ]\n> library(gbm)\n> set.seed(300)\n> m_gbm <- gbm(default ~ ., data = credit_train) \n```", "```py\n> m_gbm \n```", "```py\ngbm(formula = default ~ ., data = credit_train)\nA gradient boosted model with 59 bernoulli loss function.\n100 iterations were performed.\nThere were 16 predictors of which 14 had non-zero influence. \n```", "```py\n> p_gbm <- predict(m_gbm, credit_test, type = \"response\")\n> p_gbm_c <- ifelse(p_gbm > 0.50, 1, 0)\n> table(credit_test$default, p_gbm_c) \n```", "```py\n p_gbm_c\n1\n  0 60  5\n  1 21 14 \n```", "```py\n> library(vcd)\n> Kappa(table(credit_test$default, p_gbm_c)) \n```", "```py\n value     ASE    z  Pr(>|z|)\nUnweighted 0.3612 0.09529 3.79 0.0001504\nWeighted   0.3612 0.09529 3.79 0.0001504 \n```", "```py\n> grid_gbm <- expand.grid(\n    n.trees = c(100, 150, 200),\n    interaction.depth = c(1, 2, 3),\n    shrinkage = c(0.01, 0.1, 0.3),\n    n.minobsinnode = 10\n  ) \n```", "```py\n> library(caret)\n> ctrl <- trainControl(method = \"cv\", number = 10,\n                       selectionFunction = \"best\") \n```", "```py\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> set.seed(300)\n> m_gbm_c <- train(default ~ ., data = credit, method = \"gbm\",\n                   trControl = ctrl, tuneGrid = grid_gbm,\n                   metric = \"Kappa\",\n                   verbose = FALSE) \n```", "```py\n> m_gbm_c \n```", "```py\nStochastic Gradient Boosting \n1000 samples\n  16 predictor\n   2 classes: 'no', 'yes' \nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \nResampling results across tuning parameters:\n  shrinkage  interaction.depth  n.trees  Accuracy  Kappa      \n  0.10       1                  100      0.737     0.269966697\n  0.10       1                  150      0.738     0.295886773\n  0.10       1                  200      0.742     0.320157816\n  0.10       2                  100      0.747     0.327928587\n  0.10       2                  150      0.750     0.347848347\n  0.10       2                  200      0.759     0.380641164\n  0.10       3                  100      0.747     0.342691964\n  0.10       3                  150      0.748     0.356836684\n  0.10       3                  200      0.764     0.394578005\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nKappa was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 200,\ninteraction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. \n```", "```py\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> library(Matrix)\n> credit_matrix <- sparse.model.matrix(~ . -default, data = credit) \n```", "```py\n> dim(credit_matrix) \n```", "```py\n[1] 1000   36 \n```", "```py\n> print(credit_matrix[1:5, 1:15]) \n```", "```py\n5 x 15 sparse Matrix of class \"dgCMatrix\"\n   [[ suppressing 15 column names '(Intercept)', 'checking_balance> 200 DM', 'checking_balance1 - 200 DM' ... ]]\n\n1 1 . . .  6 . . . . . . . 1 . 1169\n2 1 . 1 . 48 1 . . . . . . 1 . 5951\n3 1 . . 1 12 . . . . . . 1 . . 2096\n4 1 . . . 42 1 . . . . . . 1 . 7882\n5 1 . . . 24 . . 1 . 1 . . . . 4870 \n```", "```py\n> credit_matrix <- credit_matrix[, -1] \n```", "```py\n> set.seed(12345)\n> train_ids <- sample(1000, 900)\n> credit_train <- credit_matrix[train_ids, ]\n> credit_test <- credit_matrix[-train_ids, ] \n```", "```py\n> dim(credit_train) \n```", "```py\n[1] 900  35 \n```", "```py\n> dim(credit_test) \n```", "```py\n[1] 100  35 \n```", "```py\n> credit_train_labels <-\n    ifelse(credit[train_ids, c(\"default\")] == \"yes\", 1, 0)\n> credit_test_labels <-\n    ifelse(credit[-train_ids, c(\"default\")] == \"yes\", 1, 0) \n```", "```py\n> library(xgboost)\n> params.xgb <- list(objective   = \"binary:logistic\",\n                     max_depth   = 6,\n                     eta         = 0.3,\n                     gamma       = 0,\n                     colsample_bytree = 1,\n                     min_child_weight = 1,\n                     subsample = 1) \n```", "```py\n> set.seed(555)\n> xgb_credit <- xgboost(params  = params.xgb,\n                        data    = credit_train,\n                        label   = credit_train_labels, \n                        nrounds = 100,\n                        verbose = 1,\n                        print_every_n = 10) \n```", "```py\n[1] train-logloss:0.586271 \n[11]\ttrain-logloss:0.317767 \n[21]\ttrain-logloss:0.223844 \n[31]\ttrain-logloss:0.179252 \n[41]\ttrain-logloss:0.135629 \n[51]\ttrain-logloss:0.108353 \n[61]\ttrain-logloss:0.090580 \n[71]\ttrain-logloss:0.077314 \n[81]\ttrain-logloss:0.065995 \n[91]\ttrain-logloss:0.057018 \n[100] train-logloss:0.050837 \n```", "```py\n> prob_default <- predict(xgb_credit, credit_test) \n```", "```py\n> pred_default <- ifelse(prob_default > 0.50, 1, 0) \n```", "```py\n> table(pred_default, credit_test_labels)\n            credit_test_labels \n```", "```py\npred_default  0  1\n           0 62 13\n           1 11 14 \n```", "```py\n> library(vcd)\n> Kappa(table(pred_default, credit_test_labels)) \n```", "```py\n value    ASE     z  Pr(>|z|)\nUnweighted 0.3766 0.1041 3.618 0.0002967\nWeighted   0.3766 0.1041 3.618 0.0002967 \n```", "```py\n> grid_xgb <- expand.grid(\n    eta = c(0.3, 0.4),\n    max_depth = c(1, 2, 3),\n    colsample_bytree = c(0.6, 0.8),\n    subsample = c(0.50, 0.75, 1.00),\n    nrounds = c(50, 100, 150),\n    gamma = c(0, 1),\n    min_child_weight = 1\n  ) \n```", "```py\n> library(caret)\n>  ctrl <- trainControl(method = \"cv\", number = 10,\n                       selectionFunction = \"best\")\n> credit <- read.csv(\"credit.csv\", stringsAsFactors = TRUE)\n> set.seed(300)\n> m_xgb <- train(default ~ ., data = credit, method = \"xgbTree\",\n                      trControl = ctrl, tuneGrid = grid_xgb,\n                      metric = \"Kappa\", verbosity = 0) \n```", "```py\n> m_xgb$bestTune \n```", "```py\n nrounds max_depth eta gamma colsample_bytree\n        50         3  0.4     1              0.6\n    min_child_weight subsample\n1 \n```", "```py\n> max(m_xgb$results[\"Kappa\"]) \n```", "```py\n[1] 0.4062946 \n```"]