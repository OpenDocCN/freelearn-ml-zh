- en: Evaluating Results with TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we understood how a neural network works, what the
    various hyper parameters in a neural network are, and how they can be tweaked
    further to improve our model's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Google offers TensorBoard, a visualization of the model training logs. In this
    chapter, we show how to use TensorBoard for TensorFlow and Keras. We interpret
    the visualizations generated by TensorBoard to understand the performance of our
    models, and also understand the other functionalities in TensorBoard that can
    help visualize our dataset better.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, Keras as a framework is a wrapper on top
    of either TensorFlow or Theano. The computations that you'll use TensorFlow for,
    such as training a massive deep neural network, can be complex and confusing.
    To make it easier to understand, debug, and optimize TensorFlow programs, the
    creators of TensorFlow have included a suite of visualization tools called TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use TensorBoard to visualize your TensorFlow graph, plot quantitative
    metrics about the execution of your graph, and also to see additional data such
    as images that were given as input. When TensorBoard is fully configured, it looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f8752e6-b68a-47ca-9948-880978984c6c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this screenshot, you can note that the chart shows a reduction in mean
    cross-entropy error over an increasing number of epochs. In the later sections
    of the chapter, we will go through the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the various summary operations captured by TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ways to debug the code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we understood how Datalab can be set up. Installing
    TensorBoard in Datalab is as simple as specifying the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/511333e7-ab7a-4ca9-bea2-b029111e5df1.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we need not make any separate installations for TensorBoard and it
    comes in prebuilt within the `google.datalab.ml` package.
  prefs: []
  type: TYPE_NORMAL
- en: Once the package is imported, we need to start TensorBoard by specifying the
    location of logs that contain the summaries written by the model fitting process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tb.start` method works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c3873ed-70ef-4ff0-8a78-176c7d92b25f.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in the first step, it checks whether the user is permitted to perform
    the calculation. Next, it picks up an unused port to open TensorBoard, and finally
    it starts TensorBoard along with printing the link to open TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn more about writing to logs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of summary operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summaries provide a way to export condensed information about a model, which
    is then accessible in tools such as TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the commonly used summary functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scalar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`histogram`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`audio`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merge`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`merge_all`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `scalar` summary operation returns a scalar, that is, the value of a certain
    metric over an increasing number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: A `histogram` summary operation returns the histogram of various valuesâ€”potentially
    weights and biases at each layer.
  prefs: []
  type: TYPE_NORMAL
- en: The `image` and `audio` summary operations return images and audio, which can
    be visualized and played in TensorBoard respectively.
  prefs: []
  type: TYPE_NORMAL
- en: A `merge` operation returns the union of all the values of input summaries,
    while `merge_all` returns the union of all the summaries contained in the model
    specification.
  prefs: []
  type: TYPE_NORMAL
- en: A visualization of some of the summaries discussed here will be provided in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Ways to debug the code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand how TensorBoard helps, let''s initialize a model structure
    as follows, one that is bound not to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3e3e0a0-cdf1-4c9f-8ff0-08992cd12529.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, in this code snippet, the validation accuracy is only around 19%.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for such a low validation accuracy is that the input dataset is not
    scaled and we are performing ReLU activation on top of an unscaled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the preceding code, we are storing the logs of the model run in
    the directory `logs/tensor_new6` (the sub-directory could be named anything).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the logs are stored in this location, we start TensorBoard as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6a09942-e1d6-4b66-90f0-9298a2042a29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code starts TensorBoard, which looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d81b685-ae67-4660-b155-8dae3419df78.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, by default, the output gives a measure of the scalars, that is, the
    accuracy and loss values of both the train and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outputs can be visualized adjacent to each other using the regular expression
    `.*` in `Filter` tags, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d87aac5c-cfc5-4c46-98c1-63da4e1f35ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the first two graphs in this screenshot represent the accuracy and
    loss of train datasets, while the next two graphs represent the accuracy and loss
    of validation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we look at the histogram of weights and bias across various layers, we
    learn that the weights and biases do not change across epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ecc83cb-f675-4804-920f-46e33f2ac6e8.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an indication that no learning is happening in the network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same can be noted when we look at the distribution of weights and biases
    across epochs in a different tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4d12641-71a9-499b-9f6f-63fde2752466.png)'
  prefs: []
  type: TYPE_IMG
- en: From this screenshot, we can conclude why the accuracy of the model is so low;
    it's because the model is not able to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, by clicking on the GRAPHS tab, let us explore whether the model was initialized
    correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9593e451-3861-4eb7-b386-00c49adc8c6e.png)'
  prefs: []
  type: TYPE_IMG
- en: You should notice that the training block is connected to every other block
    in the graph. This is because, in order to compute the gradients, one needs to
    connect to every variable in the graph (as every variable contains weights that
    need to be adjusted).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us, for now, remove the training block from the graph. This is done by
    right-clicking on the training block, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e44be15-e1ee-4505-924e-aef7c22bc855.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The resultant graph after removing the training block is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc17e10a-a576-45a9-9064-6f845e13dc31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the input layer is connected to hidden layer, which in turn is connected
    to output layer, from which the metrics and loss are calculated. Let us explore
    the connections by double-clicking on the individual blocks, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca36387e-45c9-4775-83e5-2c4939cad734.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A zoom-in of these connections helps us understand the shapes at various blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/957aa933-eadd-4b45-920a-94f5db891ece.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer is (784) in dimension, as there could be any number of input
    samples but each of them are 784-dimensional. Similarly, the kernel (weight matrix)
    is 784 x 784 in dimensions and the bias would have 784 initialized values, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the preceding diagram, we take the values in input layer and perform
    matrix multiplication with the kernel that is initialized using `random_normal`
    initialization. Also note that `random_normal` initialization is not connected
    to the training block, while the kernel block is connected to the training block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us also find out whether the output layer is connected to all the relevant
    blocks per expectations. Given that the graph looks very complicated, we can use
    another functionality provided in TensorBoard: Trace inputs. Trace inputs help
    in highlighting only those blocks that are connected to any block of interest.
    It is activated by selecting the block of interest and toggling the switch in
    the left-hand pane, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92b85145-05ad-468f-9c22-cc3bffe95a54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now all the connections look fine, but the gradients are still not getting
    updated; let us change the activation function to sigmoid and then check the weight
    histograms:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We build a neural network with sigmoid activation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/546cb038-f973-42c9-8560-4998769eef5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the neural network structure is defined and compiled, let us fit the model
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81de7d25-9b97-4a62-b351-2bb125af4b68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to open TensorBoard, we will execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f94c332-4bbb-407b-936a-db78f0c9c2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At the same time, we should notice that the accuracy and loss metrics have
    improved considerably:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/108592f0-b4a0-44c1-b55a-bd973f9c8245.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One would also be able to visualize the histogram of gradients by specifying
    write_grads=True in the TensorBoard function. The output would then be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08f06dd8-d983-49f5-acf5-2c8127f65375.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting up TensorBoard from TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we have seen that there are two ways to define a model
    in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: Premade estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a custom estimator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code, we will consider one additional snippet of code that
    would enable us to visualize the various summary operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6200657-4ef8-4d03-a86f-3f3ed8fecd99.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we only need to specify the `model_dir` in the premade estimator to
    store the various log files generated from TensorFlow operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard can then be initialized by referring to the model directory, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9662f9da-adb9-48f9-8d79-d1fe88ef3c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding code would result in a TensorBoard visualization that would have
    all the summaries built in.
  prefs: []
  type: TYPE_NORMAL
- en: Summaries from custom estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at obtaining predefined summaries from premade
    estimators in TensorBoard. In this section, we will understand obtaining summaries
    in custom estimators so that they can be visualized in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary operations that need to be captured should be specified in the
    custom estimator function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0b30c65-9d1a-4d65-9e46-f6b8bdde557a.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the model function remains very similar to what we defined in the
    previous section while learning about custom estimators; however, a few lines
    of code that write summary to log files are added.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.summary.scalar` adds the accuracy metric. Similarly, we might have wanted
    to add loss (which is another scalar) to logs; however, it gets added by default
    (note that loss is displayed when we train the model).'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.summary.histogram` gives a distribution of weights within the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, we should notice the scalars and histogram/distributions
    in the TensorBoard output. The code to train the model and start TensorBoard is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35306ac3-eb5e-4b9f-a8c1-689db7b14520.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding code snippet, we have specified the model function and parameters
    and the directory to which the log files would be written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet trains the model for 1,000 batches of 1,024 (batch
    size) data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet starts TensorBoard by using the log files written in the given
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we understood visualizing neural network models in TensorBoard,
    both from Keras and TensorFlow. We also considered how to visualize the models,
    distribution of weights, and loss/accuracy metrics in both premade estimators
    and custom defined estimators. And also the various metrics in neural networks.
  prefs: []
  type: TYPE_NORMAL
