- en: 'Chapter 5: Model Evaluation and Packaging'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn in detail about ML model evaluation and interpretability
    metrics. This will enable us to have a comprehensive understanding of the performance
    of ML models after training them. We will also learn how to package the models
    and deploy them for further use (such as in production systems). We will study
    in detail how we evaluated and packaged the models in the previous chapter and
    explore new ways of evaluating and explaining the models to ensure a comprehensive
    understanding of the trained models and their potential usability in production
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin this chapter by learning various ways of measuring, evaluating, and
    interpreting the model''s performance. We look at multiple ways of testing the
    models for production and packaging ML models for production and inference. An
    in-depth study of the ML models'' evaluation will be carried out as you will be
    presented with a framework to assess any kind of ML model and package it for production.
    Get ready to build a solid foundation in terms of evaluation and get ML models
    ready for production. For this, we are going to cover the following main topics
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and interpretability metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Production testing methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why package ML models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to package ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference ready models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation and interpretability metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Acquiring data and training ML models is a good start toward creating business
    value. After training models, it is vital to measure the models' performance and
    understand why and how a model is predicting or performing in a certain way. Hence,
    model evaluation and interpretability are essential parts of the MLOps workflow.
    They enable us to understand and validate the ML models to determine the business
    value they will produce. As there are several types of ML models, there are numerous
    evaluation techniques as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at [*Chapter 2*](B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028),
    *Characterizing Your Machine Learning Problem*, where we studied various types
    of models categorized as learning models, hybrid models, statistical models, and
    **HITL** (**Human-in-the-loop**) models, we will now discuss different metrics
    to evaluate these models. Here are some of the key model evaluation and interpretability
    techniques as shown in *Figure 5.1*. These have become standard in research and
    industry for evaluating model performance and justifying model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Model evaluation and interpretation taxonomy'
  prefs: []
  type: TYPE_NORMAL
- en: (The techniques in this taxonomy can be applied to almost any business problem
    when carefully navigated, selected, and executed.)](img/B16572_05_01.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Model evaluation and interpretation taxonomy
  prefs: []
  type: TYPE_NORMAL
- en: (The techniques in this taxonomy can be applied to almost any business problem
    when carefully navigated, selected, and executed.)
  prefs: []
  type: TYPE_NORMAL
- en: Learning models' metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Learning models** are of two types – supervised learning (supervised learning
    models or algorithms are trained based on labeled data) and unsupervised learning
    (unsupervised learning models or algorithms can learn from unlabeled data).'
  prefs: []
  type: TYPE_NORMAL
- en: As we have studied in previous chapters, examples of supervised learning algorithms
    include classification (random forest, support vector machine, and so on) and
    regression (linear regression, logistic regression, and so on) algorithms. On
    the other hand, examples of unsupervised learning include clustering (k-means,
    DBSCAN, Gaussian mixture models, and more) and dimensionality reduction (PCA,
    random forest, forward and backward feature elimination, and so on) algorithms.
    In order to measure these algorithms efficiently, the following are examples of
    some commonly used and efficient metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning models train on labeled data. In the training data, the
    outcome of the input is marked or known. Hence, a model is trained to learn to
    predict the outcome when given an input based on the labeled data. After training
    the model, it is important to gauge the model's potential and performance. Here
    are some metrics to gauge supervised models.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Evaluating an ML model is vital to understanding its behaviour and this can
    be tricky. Normally, the dataset is split into two sub-sets: the training and
    the test sets. First, the training set is used to train the model, and then the
    test set is used to test the model. After this, the model''s performance is evaluated
    to determine the error using metrics such as the accuracy percentage of the model
    on test data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This methodology is not reliable and comprehensive because accuracy for one
    test set can be different from another test set. To avoid this problem, cross-validation
    provides a solution by fragmenting or splitting the dataset into folds and ensuring
    that each fold is used as a test set at some point, as shown in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – K-Fold cross-validation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – K-Fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple cross-validation methods, including stratified cross-validation,
    leave-one-out cross-validation, and K-fold cross-validation. K-fold cross-validation
    is widely used and is worth noting as this technique involves splitting the dataset
    into k folds/fragments and then using each fold as a test set in successive iterations.
    This process is useful because each iteration has a unique test set on which accuracy
    is measured. Then, the accuracy for each iteration is used to find the average
    test results (calculated by simply taking the average of all test results).
  prefs: []
  type: TYPE_NORMAL
- en: Average accuracy acquired by cross-validation is a more reliable and comprehensive
    metric than the conventional accuracy measure. For example, in *Figure 5.2*, we
    can see five iterations. Each of these iterations has a unique test set, and upon
    testing accuracy for each iteration and averaging all accuracies, we get an average
    accuracy for the model using K-fold cross-validation. It is worth noting that
    K-fold is not a good choice if you have a very large training dataset or if the
    model requires a large amount of time, CPU, and/or GPU processing for running.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When a classifier is trained, precision can be a vital metric in quantifying
    positive class predictions made by the classifier that are actually true and belong
    to the positive class. Precision quantifies the number of correct positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's say we have trained a classifier to predict cats and dogs
    from images. Upon inferring the trained model on the test images, the model is
    used for predicting/detecting dogs from images (in other words, dogs being the
    positive class). Precision, in this case, quantifies the number of correct dog
    predictions (positive predictions).
  prefs: []
  type: TYPE_NORMAL
- en: Precision is calculated as the ratio of correctly predicted positive examples
    to the total number of predicted positive examples.
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = TruePositives / (TruePositives + FalsePositives)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** focuses on minimizing false positives. High precision ranges
    from 0 to 1, and it relates to a low false positive rate. The higher the precision,
    the better it is; for example, an image classifier model that predicts whether
    a cancer patient requires chemotherapy treatment. If the model predicts that a
    patient should be submitted for chemotherapy when it is not really necessary,
    this can be very harmful as the effects of chemotherapy can be detrimental when
    not required. This case is a dangerous false positive. A high-precision score
    will result in fewer false positives, while having a low-precision score will
    result in a high number of false positives. Hence, regarding the chemotherapy
    treatment the prediction model should have a high-precision score.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When a classifier is trained, recall can be used to quantify the positive class
    predictions established from the total number of positive examples in the dataset.
    Recall measures the number of correct positive predictions made out of the total
    number of positive predictions that could have been made. Recall provides evidence
    of missed positive predictions, unlike precision, which only tells us the correct
    positive predictions out of the total number of positive predictions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, take the same example discussed earlier, where we trained a classifier
    to predict cats and dogs from images. Upon inferring the trained model on the
    test images, the model is used for predicting/detecting dogs from images (in other
    words, dogs being the positive class). Recall, in this case, quantifies the number
    of missed dog predictions (positive predictions).
  prefs: []
  type: TYPE_NORMAL
- en: In this fashion, recall provides an empirical indication of the coverage of
    the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: '*Recall = TruePositives / (TruePositives + FalseNegatives)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** focuses on minimizing false negatives. High recall relates to a
    low false negative rate. The higher the recall, the better it is. For example,
    a model that analyzes the profile data from a passenger in an airport tries to
    predict whether that passenger is a potential terrorist. In this case, it is more
    secure to have false positives than false negatives. If the models predict that
    an innocent person is a terrorist, this could be checked following a more in-depth
    investigation. But if a terrorist passes, a number of lives could be in danger.
    In this case, it is more secure to have false negatives than false positives as
    false negatives can be checked with the help of an in-depth investigation. Recall
    should be high to avoid false negatives. In this case, having a high recall score
    is prioritized over high precision.'
  prefs: []
  type: TYPE_NORMAL
- en: F-score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a case where we need to avoid both high false positives and high false negatives,
    f-score is a useful measure for reaching this state. F-measure provides a way
    to consolidate both precision and recall into single metrics that reflect both
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: Neither precision nor recall portrays the whole story.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have the best precision with terrible recall, or alternatively, F-measure
    expresses both precision and recall. It is measured according to the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F-Measure = (2 * Precision * Recall) / (Precision + Recall)*'
  prefs: []
  type: TYPE_NORMAL
- en: The harmonic mean of your precision and recall is the F-measure. In most cases,
    you must choose between precision and recall. The harmonic mean rapidly decreases
    if you optimize your classifier to favor one and disfavor the other. When both
    precision and recall are similar, it is at its best; for example, a model that
    predicts cancer early by taking as input a patient's images and blood exams. In
    this real scenario, this could bring a lot of unnecessary costs to a hospital
    and possible harm to a patient's health if the model outputs a high number of
    false positives. On the other hand, if the model fails to detect genuine cancer
    patients, a number of lives would be in danger. In such cases, we need to avoid
    both high false positives and high false negatives and here, the f-score is a
    useful measure for avoiding high false positives and false negatives. F-score
    measures between 0 and 1\. The higher the f-score, the better it is. We can expect
    a smaller number of false positives and false negatives with a high f-score.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The confusion matrix is a metric that reports the performance of classification
    models on a set of test data samples for which prediction values are pre-known.
    It is a metric in matrix form where a confusion matrix is an N X N matrix, and
    N is the number of classes being predicted. For example, let''s say we have two
    classes to predict (binary classification), then N=2, and, as a result, we will
    have a 2 X 2 matrix, like the one shown here in *Figure 5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Confusion matrix for binary classification'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Confusion matrix for binary classification
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.3* is an example of a confusion matrix for binary classification
    between diabetic and non diabetic patients. There are 181 test data samples on
    which predictions are made to classify patient data samples into diabetic and
    non diabetic categories. Using a confusion matrix, you can get critical insights
    to interpret the model''s performance. For instance, at a glance, you will know
    how many predictions made are actually true and how many are false positives.
    Such insights are invaluable for interpreting the model''s performance in many
    cases. Here are what these terms mean in the context of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): These are cases predicted to be **yes** and are
    actually **yes** as per test data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives** (**TN**): These are the cases predicted to be **no** and
    these cases are actually **no** as per test data samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives** (**FP**): The model predicted **yes**, but they are **no**
    as per test data samples. This type of error is known as a **Type I error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives** (**FN**): The model predicted **no**, but they are **yes**
    as per test data samples. This type of error is known as a **Type II error**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In *Figure 5.3*, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: The *x*-axis represents the predictions made by the ML models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *y*-axis represents the actual labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first and fourth boxes in the matrix (diagonal boxes) depict the correctly
    predicted images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second and third boxes in the matrix represent false predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first box, (**Non Diabetic** x **Non Diabetic**), 108 data samples (**True
    negatives** – **TN**) were predicted to be **Non Diabetic** (correct predictions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the fourth box, (**Diabetes** x **Diabetes**), 36 data samples (**True positives**
    – **TP**) were predicted correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of the images in the second box (**Cats** x **Dogs**) 11 images are
    false positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third box (**Dogs** x **Cats**), which has 26 images, contains false negatives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix can provide a big picture of the predictions made on the
    test data samples and such insights are significant in terms of interpreting the
    performance of the model. The confusion matrix is the *de facto* error analysis
    metric for classification problems, as most other metrics are derived from this
    one.
  prefs: []
  type: TYPE_NORMAL
- en: AUC-ROC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A different perspective for observing model performance can enable us to interpret
    model performance and fine-tune it to derive better results. ROC and AUC curves
    can enable such insights. Let''s see how the **Receiver Operating Characteristic**
    (**ROC**) curve can enable us to interpret model performance. The ROC curve is
    a graph exhibiting the performance of a classification model at all classification
    thresholds. The graph uses two parameters to depict the model''s performance:
    **True Positive Rate** (**TPR**=*TP/TP+FN*) and **False Positive Rate** (**FPR**=*FPFP+TN*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a typical ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – ROC-AUC curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – ROC-AUC curve
  prefs: []
  type: TYPE_NORMAL
- en: An ROC curve depicts the TPR versus FPR for different thresholds for classification.
    Lowering the threshold for classification enables more items to be classified
    as positive, which in turn increases both false positives and true positives.
    The **Area Under the Curve** (**AUC**) is a metric used to quantify the effectiveness
    or ability of a classifier to distinguish between classes and is used to summarize
    the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: The AUC value varies from `0` to `1`, and the classifier is able to correctly
    distinguish between all the positive and negative class points if the AUC value
    is `1`, and the classifier is unable to correctly distinguish between all the
    positive and negative class points if the AUC value is `0`. When the AUC value
    is `0.5` (without manually setting a threshold), then this is a random classifier.
  prefs: []
  type: TYPE_NORMAL
- en: AUC helps us to rank predictions according to their accuracy, but it does not
    give us absolute values. Hence it is scale-independent. Additionally, AUC is independent
    of the classification threshold. The classification threshold chosen does not
    matter when using AUC as AUC estimates the quality of the model's predictions
    irrespective of what classification threshold is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: The Matthew's Correlation Coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Brian Matthews developed the **Matthews correlation coefficient** (**MCC**)
    in 1975 as a method for model evaluation. It calculates the discrepancies between
    real and expected values. It is an extension of confusion matrix results to measure
    the inefficiency of a classifier. TP, TN, FP, and FN are the four entries in a
    confusion matrix. These entries are factored into the coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This measure results in high scores only when a prediction returns good rates
    for all these four categories. The MCC score ranges from `-1` to `+1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1` is the best agreement between actuals and predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the score is `0`, this means there is no agreement at all between actuals
    and predictions. The prediction is random with respect to actuals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, an MCC score of `0.12` suggests that the classifier is very random.
    If it is `0.93`, this suggests that the classifier is good. MCC is a useful metric
    for helping to measure the ineffectiveness of a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning models or algorithms can learn from unlabeled data. Unsupervised
    learning can be used to mine insights and identify patterns from unlabeled data.
    Unsupervised algorithms are widely used for clustering or anomaly detection without
    relying on any labels. Here are some metrics for gauging the performance of unsupervised
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The Rand index
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Rand index is a metric for evaluating the quality of the clustering technique.
    It depicts the degree of similarity between the clusters. The Rand index measures
    the percentage of correct decisions. Decisions assign a pair of data points (for
    example, documents) to the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If `N` data points exist, the total number of *decisions = N(N-1)/2*, which
    denotes the pair of data points involved in the decision.
  prefs: []
  type: TYPE_NORMAL
- en: '*Rand index = TP + TN / TP + FP + FN + TN*'
  prefs: []
  type: TYPE_NORMAL
- en: Purity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`1`, and bad clustering has a purity value close to `0`. *Figure 5.5* is a
    visual representation of an example of calculating *purity*, as explained below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Clusters after clustering'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Clusters after clustering
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity is an external evaluation criterion regarding cluster quality. In *Figure
    5.5*, the majority class and the number of members of the majority class for the
    three clusters are as follows: green drops x 5 (cluster 1), red dots x 5 (cluster
    2), and crosses x 4 (cluster 3). Hence, purity is (1/17) x (5 + 5 + 3) = ~0.76.'
  prefs: []
  type: TYPE_NORMAL
- en: The Silhouette coefficient
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For clustering algorithms, determining the quality of clusters is important.
    To determine the quality or goodness of the clusters, the silhouette score, or
    silhouette coefficient, is used as a metric. Its value ranges from `-1` to `1`.
    When clusters are clearly distinguishable or well apart from one another, then
    the silhouette score is `1`. On the contrary, `-1` means clusters are wrongly
    allocated, and `0` means clusters are indifferent from one another. This is how
    the silhouette score is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Silhouette Score = (b-a)/max(a,b)*'
  prefs: []
  type: TYPE_NORMAL
- en: '`a` = the average distance between each point within a cluster (average intra-cluster
    distance).'
  prefs: []
  type: TYPE_NORMAL
- en: '`b` = the average distance between all clusters (average inter-cluster distance).'
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid models' metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There have been rapid developments in ML by combining conventional methods
    to develop hybrid methods to solve diverse business and research problems. Hybrid
    models include semi-supervised, self-supervised, multi-instance, multi-task, reinforcement,
    ensemble, transfer, and federated learning models. To evaluate and validate these
    models, a range of metrics are used depending on the use case and model type.
    It is good to know these metrics to be able to use the right metrics as per the
    model you will develop and evaluate in the future. Here are the metrics for evaluating
    hybrid models:'
  prefs: []
  type: TYPE_NORMAL
- en: Human versus machine test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Zeal to reach human-level performance is quite common while training and testing
    ML and deep learning models. In order to validate the models and conclude that
    the models have reached or surpassed human-level performance, human versus machine
    experiments are performed on tasks. The same task is implemented using an ML model
    and the human performance is evaluated against the ML model''s performance. There
    are various metrics for evaluating human versus machine performance according
    to the context and tasks. Some examples are mentioned here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bilingual evaluation understudy** (**BLEU**) is a method for assessing the
    quality of text for the task of machine translation from one language to another.
    The quality of text generated by a machine translation algorithm is compared to
    the output of a human. The evaluation is done to observe how close a machine translation
    is to a professional human translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**) is a human
    versus machine performance evaluation metric used to evaluate tasks such as automatic
    summarization and machine translation. This metric compares an automatically generated
    summary or translation versus summary/translations produced by humans.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Turing test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Turing test was engineered by the famous Alan Turing. He referred to it
    as the imitation game in the 1950s. The Turing test is a test of a machine to
    evaluate its ability to exhibit intelligent behavior similar to that of a human.
    In another sense, the Turing test is also a test to evaluate the ability of a
    machine to fool a human into believing a task done by machine is human-like or
    done by a human. For instance, we can see the Turing test in operation in *Figure
    5.6*, where a text-based interaction is happening between the human interrogator,
    X, and a computer or machine subject (Bob), and the interrogator, X, and a human
    subject (Alice):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Turing test'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Turing test
  prefs: []
  type: TYPE_NORMAL
- en: During the Turing test, the human interrogator, X, performs a series of interactions,
    both with Bob (computer) and Alice (human) with an intent to distinguish between
    the human and the machine correctly. The machine passes the Turing test if/when
    the interrogator cannot distinguish them correctly or mistakes the machine for
    a human (Bob for Alice).
  prefs: []
  type: TYPE_NORMAL
- en: Reward per return
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement learning models are hybrid models that involve continuous learning
    mechanisms between the agent and operating environment in order to achieve pre-defined
    goals. The agent learns based on rewards earned for efficient or optimal steps
    toward reaching a goal. When the goal is optimal control, you will want to measure
    the agent by how well it does at the task. To quantify how well the agent performs
    the task, the aggregate measures of reward, such as total reward per episode (otherwise
    known as "return") or mean reward per time step, can be used to assess and optimize
    control for the agent with respect to the environment and the goals.
  prefs: []
  type: TYPE_NORMAL
- en: Regret
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regret is a commonly used metric for hybrid models such as reinforcement learning
    models. At each time step, you calculate the difference between the reward of
    the optimal decision and the decision taken by your algorithm. Cumulative regret
    is then calculated by summing this up. The minimum regret is 0 with the optimal
    policy. The smaller the regret, the better an algorithm has performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regret enables the actions of the agent to be assessed with respect to the
    best policy for the optimal performance of the agent as shown in *Figure 5.7*.
    The shaded region in red is the regret:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Regret for reinforcement learning'
  prefs: []
  type: TYPE_NORMAL
- en: Shapely Additive Explanations (SHAP)](img/B16572_05_07.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Regret for reinforcement learning
  prefs: []
  type: TYPE_NORMAL
- en: SHapley Additive exPlanations (SHAP)
  prefs: []
  type: TYPE_NORMAL
- en: 'Model interpretability and explaining why the model is making certain decisions
    or predictions can be vital in a number of business problems or industries. Using
    techniques discussed earlier, we can interpret the model''s performance, but there
    are still some gray areas, such as deep learning models, which are black-box models.
    It is noticeable in general that these models can be trained to achieve great
    results or accuracies on test data, but it is hard to say why. In such scenarios,
    **SHapley Additive exPlanations** (**SHAP**) can be useful to decode what is happening
    with the predicted results and which feature predictions correlate to the most.
    SHAP was proposed in this paper (at NIPS): [http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'SHAP works both for classification and regression models. The primary goal
    of SHAP is to explain the model output prediction by computing the contribution
    of each feature. The SHAP explanation method uses Shapley values to explain the
    feature importance for model outputs or predictions. Shapley values are computed
    from cooperative game theory, and these values range from `-1` to `1`. Shapley
    values describe the distribution of model outputs among the features, as shown
    in *Figure 5.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Shapley values bar chart depicting feature importance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Shapley values bar chart depicting feature importance
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several SHAP explainer techniques, such as SHAP Tree Explainer, SHAP
    Deep Explainer, SHAP Linear Explainer, and SHAP Kernel Explainer. Depending on
    the use case, these explainers can provide useful information on model predictions
    and help us to understand black-box models. Read more here: [https://christophm.github.io/interpretable-ml-book/shap.html](https://christophm.github.io/interpretable-ml-book/shap.html)'
  prefs: []
  type: TYPE_NORMAL
- en: MIMIC explainer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mimic explainer is an approach mimicking black-box models by training an interpretable
    global surrogate model. These trained global surrogate models are interpretable
    models that are trained to approximate the predictions of any black-box model
    as accurately as possible. By using the surrogate model, a black-box model can
    be gauged or interpreted as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps are implemented to train a surrogate model:'
  prefs: []
  type: TYPE_NORMAL
- en: To train a surrogate model, start by selecting a dataset, X. This dataset can
    be the same as the one used for training the black-box model or it can be another
    dataset of similar distributions depending on the use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the predictions of the black-box model for the selected dataset, X.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an interpretable model type (linear model, decision tree, random forest,
    and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the dataset, X, and its predictions, train the interpretable model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now you have a trained surrogate model. Kudos!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate how well the surrogate model has reproduced predictions of the black-box
    model, for example, using R-square or F-score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get an understanding of black-box model predictions by interpreting the surrogate
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following interpretable models can be used as surrogate models: **Light
    Gradient boosting model** (**LightGBM**), linear regression, stochastic gradient
    descent, or random forest and decision tree.'
  prefs: []
  type: TYPE_NORMAL
- en: Surrogate models can enable ML solution developers to gauge and understand the
    black-box model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: Permutation feature importance explainer (PFI)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Permutation Feature Importance** (**PFI**) is a technique used to explain
    classification and regression models. This technique is useful for interpreting
    and understanding a feature to model output or prediction correlation. PFI is
    an alternative to SHAP. It works by randomly assessing one feature at a time for
    the entire dataset and calculating the change in performance evaluation metrics.
    The change in performance metric is evaluated for each feature; the more significant
    the change, the more important the feature is.'
  prefs: []
  type: TYPE_NORMAL
- en: PFI can describe the overall behavior of any model, but does not explain individual
    predictions of the model. PFI is an alternative to SHAP, but is still quite different
    as PFI is based on the decrease in performance of the model, while SHAP is based
    on the magnitude of feature attributions.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical models' metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 2*](B16572_02_Final_JM_ePub.xhtml#_idTextAnchor028),
    *Characterizing Your Machine Learning Problem*, there are three types of statistical
    models: inductive learning, deductive learning, and transduction learning. Statistical
    models offer a good degree of interpretability.'
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The mean, or average, is the central value of the dataset. It is calculated
    by summing all the values and dividing the sum by the number of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*mean = x1 + x2 + x3 +.... + xn / n*'
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The standard deviation measures the dispersion of the values in the dataset.
    The lower the standard deviation, the closer the data points to the mean. A widely
    spread dataset would have a higher standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bias measures the strength (or rigidity) of the mapping function between the
    independent (input) and dependent (output) variables. The stronger the assumptions
    of the model regarding the functional form of the mapping, the greater the bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'High bias is helpful when the underlying true (but unknown) model has matching
    properties as the assumptions of the mapping function. However, you could get
    completely off-track if the underlying model does not exhibit similar properties
    as the functional form of the mapping. For example, the assumption that there
    is a linear relationship in the variables when in reality it is highly non-linear
    and it would lead to a bad fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low bias**: Weak assumptions with regard to the functional form of the mapping
    of inputs to outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High bias**: Strong assumptions with regard to the functional form of the
    mapping of inputs to outputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The bias is always a positive value. Here is an additional resource for learning
    more about bias in ML. This article offers a broader explanation: [https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/](https://kourentzes.com/forecasting/2014/12/17/the-bias-coefficient-a-new-metric-for-forecast-bias/).'
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The variance of the model is the degree to which the model's performance changes
    when it is fitted on different training data. The impact of the specifics on the
    model is captured by the variance.
  prefs: []
  type: TYPE_NORMAL
- en: A high variance model will change a lot with even small changes in the training
    dataset. On the other hand, a low variance model wouldn't change much even with
    large changes in the training dataset. The variance is always positive.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: R-squared, also known as the coefficient of determination, measures the variation
    in the dependent variable that can be explained by the model. It is calculated
    as the explained variation divided by the total variation. In simple terms, R-squared
    measures how close the data points are to the fitted regression line.
  prefs: []
  type: TYPE_NORMAL
- en: The value of R-squared lies between `0` and `1`. A low R-squared value indicates
    that most of the variation in the response variable is not explained by the model,
    but by other factors not included in it. In general, you should aim for a higher
    R-squared value because this indicates that the model better fits the data.
  prefs: []
  type: TYPE_NORMAL
- en: RMSE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **root mean square error** (**RMSE**) measures the difference between the
    predicted values of the model and the observed (true) values.
  prefs: []
  type: TYPE_NORMAL
- en: Options are many, and you need to choose the right metric for real-world production
    scenarios to have well-justified evaluations; for example, why a data scientist
    or a data science team might want to select one evaluation metric over another,
    for instance, R-squared over mean for a regression problem. It depends on the
    use case and type of data.
  prefs: []
  type: TYPE_NORMAL
- en: HITL model metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two types of **HITL** models – human reinforcement learning and active
    learning models. In these models, human-machine collaboration fosters the algorithm
    to mimic human-like behaviors and outcomes. A key driver for these ML solutions
    is the human in the loop. Humans validate, label, and retrain the models to maintain
    the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Human bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just like the human brain, ML systems are subject to cognitive bias. Human
    cognitive biases are processes that disrupt your decision making and reasoning
    ability, ending up in the production of errors. Human bias occurrences include
    stereotyping, selective perception, the bandwagon effect, priming, affirmation
    predisposition, observational selection bias, and the speculator''s false notion.
    In many cases, it is vital to avoid such biases for ML systems in order to make
    rational and optimal decisions. This will make ML systems more pragmatic than
    humans if we manage to deduce human bias and rectify it. This will be especially
    useful in HITL-based systems. Using bias testing, three types of human biases
    can be identified and worked upon to maintain the ML system''s decision making
    such that it is free from human bias. These three human biases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interaction bias**'
  prefs: []
  type: TYPE_NORMAL
- en: When an ML system is fed a dataset containing entries of one particular type,
    an interaction bias is introduced that prevents the algorithm from recognizing
    any other types of entries. This type of bias can be identified in inference testing
    for trained models. Methods such as SHAP and PFI can be useful in identifying
    feature bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latent bias**'
  prefs: []
  type: TYPE_NORMAL
- en: Latent bias is experienced when multiple examples in the training set have a
    characteristic that stands out. Then, the ones without that characteristic fail
    to be recognized by the algorithm. For example, recently, the Amazon HR algorithm
    for selecting people based on applications for roles within the company showed
    bias against women, the reason being latent bias.
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection bias**'
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias is introduced to an algorithm when the selection of data for
    analysis is not properly randomized. For example, in designing a high-performance
    face recognition system, it is vital to include all possible types of facial structures
    and shapes and from all ethnic and geographical samples, so as to avoid selection
    bias. Selection bias can be identified by methods such as SHAP or PFI to observe
    model feature bias.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the case of human reinforcement learning, the goal of the system is to maximize
    the rewards of the action in the current state. In order to maximize the rewards
    for actions, optimal policy can be used as a metric to gauge the system. The optimal
    policy is the policy where the action that maximizes the reward/return of the
    current state is chosen. The optimal policy is the metric or state that is ideal
    for a system to perform at its best. In a human reinforcement learning-based system,
    a human operator or teacher sets the optimal policy as the goal of the system
    is to reach human-level performance.
  prefs: []
  type: TYPE_NORMAL
- en: Rate of automation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automation is the process of automatically producing goods or getting a task
    done through the use of robots or algorithms with no direct human assistance.
  prefs: []
  type: TYPE_NORMAL
- en: The level of automation of an ML system can be calculated using the rate of
    automation of the total tasks. It is basically the percentage of tasks fully automated
    by the system, and these tasks do not require any human assistance. It shows what
    percentage of tasks are fully automated out of all the tasks. For example, AlphaGo,
    by DeepMind, has achieved 100% automation to operate on its own to defeat human
    world champion players.
  prefs: []
  type: TYPE_NORMAL
- en: Risk rate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The probability of an ML model performing errors is known as the error rate.
    The error rate is calculated based on the model's performance for production systems.
    The lower the error rate, the better it is for an ML system. The goal of a human
    in the loop is to reduce the error rate and teach the ML model to function at
    its most optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Production testing methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As there are various businesses in operation, so are different types of production
    systems serving these businesses. In this section, we look into the different
    types of production systems or setups commonly used and how to test them.
  prefs: []
  type: TYPE_NORMAL
- en: Batch testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batch testing validates your model by performing testing in an environment that
    is different from its training environment. Batch testing is carried out on a
    set of samples of data to test model inference using metrics of choice, such as
    accuracy, RMSE, or f1-score. Batch testing can be done in various types of computes,
    for example, in the cloud, or on a remote server or a test server. The model is
    usually served as a serialized file, and the file is loaded as an object and inferred
    on test data.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will surely have come across A/B testing. It is often used in service design
    (websites, mobile apps, and so on) and for assessing marketing campaigns. For
    instance, it is used to evaluate whether a specific change in the design or tailoring
    content to a specific audience positively affects business metrics such as user
    engagement, the click-through rate, or the sales rate. A similar technique is
    applied in testing ML models using A/B testing. When models are tested using A/B
    testing, the test will answer important questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the new model B work better in production than the current model A?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which of the two models' nominees work better in production to drive positive
    business metrics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To evaluate the results of A/B testing, statistical techniques are used based
    on the business or operations to determine which model will perform better in
    production. A/B testing is usually conducted in this manner, and real-time or
    live data is fragmented or split into two sets, Set A and Set B. Set A data is
    routed to the old model, and Set B data is routed to the new model. In order to
    evaluate whether the new model (model B) performs better than the old model (model
    A), various statistical techniques can be used to evaluate model performance (for
    example, accuracy, precision, recall, f-score, and RMSE), depending on the business
    use case or operations. Depending on a statistical analysis of model performance
    in correlation to business metrics (a positive change in business metrics), a
    decision is made to replace the new model with the old one or determine which
    model is better.
  prefs: []
  type: TYPE_NORMAL
- en: A/B testing is performed methodically using statistical hypothesis testing,
    and this hypothesis validates two sides of a coin – the null hypothesis and the
    alternate hypothesis. The null hypothesis asserts that the new model does not
    increase the average value of the monitoring business metrics. The alternate hypothesis
    asserts that the new model improves the average value of the monitoring business
    metrics. Ultimately, A/B testing is used to evaluate whether *the new model drives
    a significant boost in specific business metrics*. There are various types of
    A/B testing, depending on business use cases and operations, for example, `Z-test`,
    `G-test` (I recommend knowing about these and others), and others. Choosing the
    right A/B test and metrics to evaluate can be a win-win for your business and
    ML operations.
  prefs: []
  type: TYPE_NORMAL
- en: Stage test or shadow test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before deploying a model for production, which would then lead to making business
    decisions, it can be valuable to replicate a production-like environment (staging
    environment) to test the model's performance. This is especially important for
    testing the robustness of the model and assessing its performance on real-time
    data. It could be facilitated by deploying the develop branch or a model to be
    tested on a staging server and inferring the same data as the production pipeline.
    The only shortcoming here will be that end users will not see the results of the
    develop branch or business decisions will not be made in the staging server. The
    results of the staging environment will statistically be evaluated using suitable
    metrics (for example, accuracy, precision, recall, f-score, and RMSE) to determine
    the robustness and performance of the model in correlation to business metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Testing in CI/CD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Implementing testing as part of CI/CD pipelines can be rewarding in terms of
    automating and evaluating (based on set criteria) the model''s performance. CI/CD
    pipelines can be set up in multiple ways depending on the operations and architecture
    in place, for instance:'
  prefs: []
  type: TYPE_NORMAL
- en: Upon a successful run of an ML pipeline, CI/CD pipelines can trigger a new model's
    A/B test in the staging environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a new model is trained, it is beneficial to set up a dataset separate from
    the test set to measure its performance against suitable metrics, and this step
    can be fully automated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD pipelines can periodically trigger ML pipelines at a set time in a day
    to train a new model, which uses live or real-time data to train a new model or
    fine-tune an existing model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD pipelines can monitor the ML model's performance of the deployed model
    in production, and this can be triggered or managed using time-based triggers
    or manual triggers (initiated by team members responsible for quality assurance).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CI/CD pipelines can provision two or more staging environments to perform A/B
    testing on unique datasets to perform more diverse and comprehensive testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are a variety of scenarios, and depending on requirements, CI/CD pipelines
    offer various workflows and operations tailored to the needs of the business and
    tech requirements. Selecting an efficient architecture and CI/CD process can augment
    tech operations and team performance overall. CI/CD testing can augment and automate
    testing to great lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Why package ML models?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLOps enables a systematic approach to train and evaluate models. After models
    are trained and evaluated, the next steps are to bring them to production. As
    we know, ML doesn't work like traditional software engineering, which is deterministic
    in nature and where a piece of code or module is imported into the existing system
    and it works. Engineering ML solutions is non-deterministic and involves serving
    ML models to make predictions or analyze data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to serve the models, they need to be packed into software artifacts
    to be shipped to the testing or production environments. Usually, these software
    artifacts are packaged into a file or a bunch of files or containers. This allows
    the software to be environment- and deployment-agnostic. ML models need to be
    packaged for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Portability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Packaging ML models into software artifacts enables them to be shipped or transported
    from one environment to another. This can be done by shipping a file or bunch
    of files or a container. Either way, we can transport the artifacts and replicate
    the model in various setups. For example, a packaged model can be deployed in
    a virtual machine or serverless setup.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML inference is a process that involves processing real-time data using ML models
    to calculate an output, for example, a prediction or numerical score. The purpose
    of packaging ML models is to be able to serve the ML models in real time for ML
    inference. Effective ML model packaging (for example, a serialized model or container)
    can facilitate deployment and serve the model to make predictions and analyze
    data in real time or in batches.
  prefs: []
  type: TYPE_NORMAL
- en: Interoperability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML model interoperability is the ability of two or more models or components
    to exchange information and to use exchanged information in order to learn or
    fine-tune from each other and perform operations with efficiency. Exchanged information
    can be in the form of data or software artifacts or model parameters. Such information
    enables models to fine-tune, retrain, or adapt to various environments from the
    experience of other software artifacts in order to perform and be efficient. Packaging
    ML models is the foundation for enabling ML model interoperability.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment agnosticity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Packaging ML models into software artifacts such as serialized files or containers
    enables the models to be shipped and deployed in various runtime environments,
    such as in a virtual machine, a container serverless environment, a streaming
    service, microservices, or batch services. It opens opportunities for portability
    and deployment agnosticity using the same software artifacts that an ML model
    is packaged in.
  prefs: []
  type: TYPE_NORMAL
- en: How to package ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML models can be packaged in various ways depending on business and tech requirements
    and as per operations for ML. ML models can be packaged and shipped in three ways,
    as discussed in the following sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Serialized files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Serialization is a vital process for packaging an ML model as it enables model
    portability, interoperability, and model inference. Serialization is the method
    of converting an object or a data structure (for example, variables, arrays, and
    tuples) into a storable artefact, for example, into a file or a memory buffer
    that can be transported or transmitted (across computer networks). The main purpose
    of serialization is to reconstruct the serialized file into its previous data
    structure (for example, a serialized file into an ML model variable) in a different
    environment. This way, a newly trained ML model can be serialized into a file
    and exported into a new environment where it can de-serialized back into an ML
    model variable or data structure for ML inferencing. A serialized file does not
    save or include any of the previously associated methods or implementation. It
    only saves the data structure as it is in a storable artefact such as a file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some popular serialization formats in *figure 5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 5.1 – Popular ML model serialization formats'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Table_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 – Popular ML model serialization formats
  prefs: []
  type: TYPE_NORMAL
- en: All these serialized formats (except ONNX) have one problem in common, the problem
    of interoperability. To address that, ONNX is developed as an open source project
    supported by Microsoft, Baidu, Amazon, and other big companies. This enables a
    model to be trained using one framework (for example, in scikit-learn) and then
    retrained again using TensorFlow. This has become a game changer for industrialized
    AI as models can be rendered interoperable and framework-independent.
  prefs: []
  type: TYPE_NORMAL
- en: ONNX has unlocked new avenues, such as federated learning and transfer learning.
    Serialized models enable portability and also batch inferencing (batch inference,
    or offline inference, is the method of generating predictions on a batch of data
    points or samples) in different environments.
  prefs: []
  type: TYPE_NORMAL
- en: Packetizing or containerizing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We often encounter diverse environments for production systems. Every environment
    possesses different challenges when it comes to deploying ML models, in terms
    of compatibility, robustness, and scalability. These challenges can be avoided
    by standardizing some processes or modules and containers are a great way to standardize
    ML models and software modules.
  prefs: []
  type: TYPE_NORMAL
- en: A container is a standard unit of software made up of code and all its dependencies.
    It enables the quick and reliable operation of applications from one computing
    environment to another. It enables the software to be environment- and deployment-agnostic.
    Containers are managed and orchestrated by Docker. Docker has become an industry
    standard at developing and orchestrating containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker is an open source ([https://opensource.com/resources/what-open-source](https://opensource.com/resources/what-open-source))
    tool. It has been developed to make it convenient to build, deploy, and run applications
    by using containers. By using containers, a developer can package an application
    with its components and modules, such as files, libraries, and other dependencies,
    and deploy it as one package. Containers are a reliable way to run applications
    using a Linux OS with customized settings. Docker containers are built using Dockerfiles,
    which are used to containerize an application. After building a Docker image,
    a Docker container is built. A Docker container is an application running with
    custom settings as orchestrated by the developer. *Figure 5.8* shows the process
    of building and running a Docker container from a Dockerfile. A Dockerfile is
    built into a Docker image, which is then run as a Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Docker artifacts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_05_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Docker artifacts
  prefs: []
  type: TYPE_NORMAL
- en: 'A Dockerfile, Docker image, and a Docker container are foundational components
    for building and running containers. These are each described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dockerfile**: A Dockerfile is a text document containing a set of Docker
    commands ordered by the developer to build a Docker image. Docker is able to read
    the Dockerfile and build a Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker image**: This is a sequential collection of execution parameters to
    use within a collection of root filesystems within a container during runtime.
    Docker images are like a snapshot of containers. Containers are constructed from
    Docker images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker container**: Containers are constructed from Docker images. A container
    is a runtime instance of a Docker image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML models can be served in Docker containers for robustness, scalability, and
    deployment agnosticity. In later chapters, we will deploy ML models using Docker
    for the purpose of hands-on experience, hence, it is good to have a general understanding
    of this tool.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice generation and deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices enable the collection of services that are independently deployable.
    Each of these services is highly maintainable, testable, and loosely coupled.
    Microservices are orchestrated by architecture that is organized around business
    capabilities to enable a system to serve business needs. For example, Spotify
    has transitioned from a monolithic complex system to a microservices-based system.
    It was done by splitting the complex system into fragmented services, with specific
    goals such as a search engine, content tagging, content classification, user behavioral
    analytics for a recommendation engine, and autogenerated playlists. Fragmented
    microservices are now developed by a dedicated team. Each microservice is isolated
    and less dependent on one another. This way, it is easier to develop and maintain.
    The company can be consistent with customer service and continuously improve without
    putting the service down.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a microservice is generated by tailoring serialized files into a
    containerized Docker image. These Docker images can then be deployed and orchestrated
    into any Docker - supported environment. Deploying and managing Docker images
    can be performed using container management tools, such as Kubernetes. Docker
    enables extreme portability and interoperability, Docker images can be easily
    deployed to any popular cloud service, such as Google Cloud, Azure, or AWS. Docker
    images can be deployed and managed to any Docker corporate server or data center
    or real-time environment as long as it supports Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservices can be served in a REST API format, and this is a popular way
    to serve ML models. Some Python frameworks, such as Flask, Django, and FastAPI,
    have become popular in enabling ML models to serve as REST API microservices.
    To facilitate robust and scalable system operations, software developers can sync
    with Dockerized microservices via a REST API. To orchestrate Docker-based microservice
    deployments on Kubernetes-supported infrastructure, Kubeflow is a good option.
    It is cloud-agnostic and can be run on-premises or on local machines. Besides
    that, Kubeflow is based on Kubernetes, but keeps the Kubernetes details and difficulties
    inside a box. Kubeflow is a robust way of serving a model. This is a tool worth
    exploring: [https://www.kubeflow.org/docs/started/kubeflow-overview/](https://www.kubeflow.org/docs/started/kubeflow-overview/).'
  prefs: []
  type: TYPE_NORMAL
- en: Inference ready models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have previously worked on a business problem to predict the weather at a
    port. To build a solution for this business problem, data processing and ML model
    training were performed, followed by serializing models. Now, in this section,
    we explore how inference is done on the serialized model. This section''s code
    is available from the attached Jupyter notebook in the chapter''s corresponding
    folder in the book''s GitHub repository. Here are the instructions for running
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure portal again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From `MLOps_WS` workspace, and then click on the `MLOps_WS` workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Manage** section, click on the **Compute** section, and then select
    the machine created in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*. Click on the **Start** button to start the instance.
    When the VM is ready, click on the JupyterLab link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, in JupyterLab, navigate to the chapter's corresponding folder (`05_model_evaluation_packaging`)
    and open the notebook (`model_evaluation_packaging.ipynb`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connecting to the workspace and importing model artifacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we import the requisite packages, connect to the ML workspace using
    the `Workspace()` function, and then download the serialized scaler and model
    to perform predictions. `Scaler` will be used to scale input data into the same
    scale of data that was used for model training. The `Model` file is serialized
    in ONNX format. Both the `Scaler` and `Model` files are imported using the `Model()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After running this code, you will see new files downloaded in the left panel
    in the JupyterLab window.
  prefs: []
  type: TYPE_NORMAL
- en: Loading model artifacts for inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We open and load the `Scaler` and `Model` files into variables that can be
    used for ML model inference. `Scaler` is read and loaded into a variable using
    pickle, and the ONNX runtime is used to load the ONNX file using `InferenceSession()`
    for making ML model predictions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ML model inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To perform ML model inference, scale the test data and set it up for inference
    using the `fit_transform()` method. Now, perform inference on the test data by
    using the ONNX session and run `sess.run()` by passing the input data, `test_data`,
    in `float` `32` format. Lastly, print the results of model inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With these steps, we have successfully downloaded the serialized model, loaded
    it to a variable, and performed inference on a test data sample. The expected
    result of the block code is the value `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored the various methods to evaluate and interpret
    ML models. We have learned about production testing methods and the importance
    of packaging models, why and how to package models, and the various practicalities
    and tools for packaging models for ML model inference in production. Lastly, to
    understand the workings of packaging and de-packaging serialized models for inference,
    we performed the hands-on implementation of ML model inference using serialized
    models on test data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn more about deploying your ML models. Fasten
    your seatbelts and get ready to deploy your models to production!
  prefs: []
  type: TYPE_NORMAL
