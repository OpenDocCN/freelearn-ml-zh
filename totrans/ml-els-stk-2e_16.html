<html><head></head><body>
		<div id="_idContainer309">
			<h1 id="_idParaDest-203"><em class="italic"><a id="_idTextAnchor236"/>Chapter 13</em>: Inference</h1>
			<p>In this chapter, we will take an in-depth look at all of the fascinating things you can do with trained supervised models in the Elastic Stack. First, we will see how to use the Trained Models API to view information about the models available in our cluster, to see details about individual models, and to export models so that they can be ported to other Elasticsearch clusters. We will also take a brief look at how to use eland to import external models, such as those trained by third-party machine learning libraries, into Elasticsearch.</p>
			<p>In the second part of this chapter, we will go in-depth into how to use trained supervised models with inference in a variety of contexts to enrich data. To do this, we will learn about inference processors and ingest pipelines and how these can be combined with continuous transforms, reindexing, and at ingest time when using various beats or otherwise ingesting data into Elasticsearch. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Examining, importing, and exporting trained machine learning models using the Trained Models API and Python</li>
				<li>Understanding inference processors and ingest pipelines and how to configure and use them</li>
				<li>Importing external models into Elasticsearch using eland </li>
			</ul>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor237"/>Technical requirements</h1>
			<p>The material in this chapter will require an Elasticsearch cluster version 7.10 or later, and an installation of Python 3.7 or later with the <strong class="source-inline">eland</strong>, <strong class="source-inline">elasticsearch-py</strong>, and <strong class="source-inline">scikit-learn</strong> libraries installed. For detailed instructions on how to configure your Python installation to work with this chapter, please see the README section in the <strong class="source-inline">Chapter 13 - Inference and Advanced Transforms</strong> folder in the book's GitHub repository: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference</a>. </p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor238"/>Examining, exporting, and importing your trained models with the Trained Models API</h1>
			<p>You have <a id="_idIndexMarker850"/>prepared your dataset, trained your classification or regression model, looked at its performance, and determined that you would like to <a id="_idIndexMarker851"/>use it to enrich your production datasets. Before you can dive into ingest pipelines, inference processors, and the multitude of other components that you can configure to use your trained models, it is good to become familiar with the <strong class="bold">Trained Models API</strong> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html">https://www.elastic.co/guide/en/elasticsearch/reference/7.10/get-trained-models.html</a>), a set of REST API endpoints that you can use to find out information about your models and even export them to other clusters. Let's take a tour of this API to see what it can tell us about our models. </p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor239"/>A tour of the Trained Models API</h2>
			<p>In this section, we will <a id="_idIndexMarker852"/>take a practical look at using the Kibana Dev Console to examine things about our trained supervised models: </p>
			<ol>
				<li>Let's start in the Kibana Dev Console. Briefly, for those not yet familiar with this tool, the Kibana Dev Console, which you can access by clicking through the left-hand side slide-out menu and scrolling down to the <strong class="bold">Management</strong> menu, gives power users an easy graphical environment through which to execute REST API commands. We will take advantage of this functionality to talk to the Inference API and to examine the responses we get from Elasticsearch when querying various REST API endpoints.<p>In the Kibana Dev Console, execute the following REST request: </p><p class="source-code">GET _ml/trained_models/</p><p>The response that we receive back from Elasticsearch includes summary information about how many trained models we currently have in the cluster, as well as details about each model. The exact response you receive will, of course, vary cluster by cluster depending on what models you have been training, so the response snippet shown in <em class="italic">Figure 13.1</em> is only an example. </p><p>Looking at the <a id="_idIndexMarker853"/>response in <em class="italic">Figure 13.1</em>, you can see that we receive back from Elasticsearch the count – the number of trained models we have in the cluster, as well as a list of model objects, each of which contains numerous attributes that describe the model. </p><p>Although most of the fields in the model object can be useful at one time or another, at this stage, an important field to pay attention to is <strong class="source-inline">model_id</strong>. This field contains the unique identifier that is assigned to each model stored in the cluster and that is used to reference the model when it is later used in inference processors and ingest pipelines. </p><p>Another piece of information provided by the Trained Models API is <strong class="source-inline">analyzed_fields</strong>, a dictionary that includes a list of included or excluded fields. It is good to double-check this as a sanity check to make sure that only the fields you intended to use for training were included in the training: </p><div id="_idContainer295" class="IMG---Figure"><img src="image/B17040_13_1.jpg" alt="Figure 13.1 – A snippet of the Inference API response illustrating the number of trained models in the cluster as well as information about one of the models&#13;&#10;"/></div><p class="figure-caption">Figure 13.1 – A snippet of the Inference API response illustrating the number of trained models in the cluster as well as information about one of the models</p></li>
				<li>Our demo cluster <a id="_idIndexMarker854"/>contains just three trained models, so the amount of information returned from the API is not overwhelming, but in case you are working on a cluster with tens or hundreds of models, it can be helpful to view the details of a single model at a time using the API call with the full name of the model. Note, if you are following along and want to run the subsequent API call in your specific instance of Kibana, you will have to look up and use the <strong class="source-inline">model_id</strong> of the model located in your cluster: <p class="source-code">GET _ml/trained_models/breast-cancer-wisconsin-classification-1612270856116</p><p>Alternatively, use an API call with a wildcard:</p><p class="source-code">GET _ml/trained_models/breast-cancer-wisconsin-classification-*</p><p>A less verbose summary is available through the <strong class="source-inline">_cat</strong> API. We can use the following API call to see a brief summary of the models available:</p><p class="source-code">GET _cat/ml/trained_models</p><p>The response we receive from our cluster is displayed in <em class="italic">Figure 13.2</em>. You will notice that there are two models trained on the breast cancer dataset as well as a third model whose identifier is <strong class="source-inline">lang_ident_model_1</strong>. This is a language identification model that ships by default with Elasticsearch and can be used to identify <a id="_idIndexMarker855"/>which language a given string is likely to be in. We will look at how this language identification model works and how to use it later in this chapter: </p></li>
			</ol>
			<div>
				<div id="_idContainer296" class="IMG---Figure">
					<img src="image/B17040_13_2.jpg" alt="Figure 13.2 – The response from the _cat API &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – The response from the _cat API </p>
			<p>Now that we have taken a brief look at how we can examine and extract information about the trained models available in our cluster, let's take a closer look at one last powerful function of the Trained Models API – that of exporting the model from an Elasticsearch cluster. Since this procedure involves a few more moving parts than the last one, we have dedicated the next section to it. </p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor240"/>Exporting and importing trained models with the Trained Models API and Python </h2>
			<p>Why might <a id="_idIndexMarker856"/>you want to <a id="_idIndexMarker857"/>export a model trained in Elasticsearch? You may want to export your model so that you can <a id="_idIndexMarker858"/>either store it <a id="_idIndexMarker859"/>externally, share it with colleagues, or import it later into another Elasticsearch cluster. Since training machine learning models can be resource-intensive, you might want to provision one transient Elasticsearch cluster for training, train and evaluate the model on this cluster, and then export and re-import the model into another, smaller cluster so that you can perform inference – a less resource-intensive procedure. </p>
			<p>To export a model using Python and to follow the steps, you will need a Python installation using 3.7 or later and the <strong class="source-inline">elasticsearch-py</strong> library version 7.10.1. For detailed instructions and further resources on how to configure a Python environment and install the required dependencies, please follow the instructions in this README file (<a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference</a>) in the book's GitHub repository. All the steps and logic required to export a model from an Elasticsearch cluster will be available in the <strong class="source-inline">export_model.py</strong> Python script at, <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export</a>, in the book's GitHub repository. In this section, we will look at each of the steps in the script to understand the components necessary to export a model. Hopefully, this treatment will give you the <a id="_idIndexMarker860"/>building blocks to <a id="_idIndexMarker861"/>use the <strong class="source-inline">elasticsearch-py</strong> client to construct your own machine learning workflows in Python: </p>
			<ol>
				<li value="1">The building <a id="_idIndexMarker862"/>block of nearly all Python scripts that interact with an Elasticsearch cluster is the construction of the <a id="_idIndexMarker863"/>Elasticsearch client object. The client object's class must first be imported from the library like this:<p class="source-code">from elasticsearch import Elasticsearch</p><p>Once the class has been imported, we can create an instance of the object and assign it to the <strong class="source-inline">es_client</strong> variable: </p><p class="source-code">es_client = Elasticsearch(es_url, http_auth=(ES_USERNAME, ES_PASSWORD))</p><p>Note that we pass into the object's constructor the variable that holds the URL of the Elasticseach instance. This can either be something like <strong class="source-inline">localhost:9200</strong> if you are running an instance of Elasticsearch on your local machine, or a longer URL for cloud-based deployments. Additionally, we pass in two variables, <strong class="source-inline">ES_USERNAME</strong> and <strong class="source-inline">ES_PASSWORD</strong>, that hold the username and password of our Elasticsearch instance. This is not required if you are running an unprotected Elasticsearch cluster locally for development purposes, but please note that running an unsecured Elasticsearch cluster in production is extremely dangerous. </p></li>
				<li>To interact with the Machine Learning APIs, which we will need to export our model from the cluster, we need to import the <strong class="source-inline">MlClient</strong> class:<p class="source-code">from elasticsearch.client import MlClient</p><p>Then, create an instance of it:</p><p class="source-code">ml_client = MlClient(es_client)</p><p>Once we have these two clients created, we can proceed to export our model. To do this, we will use the <strong class="source-inline">get_trained_models</strong> method. The documentation for the method is at here, <a href="https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack">https://elasticsearch-py.readthedocs.io/en/v7.13.0/api.html#x-pack</a>, and it is good to keep this reference <a id="_idIndexMarker864"/>handy when working with this library in case you need to double-check the meaning of parameters and configuration options. There are three <a id="_idIndexMarker865"/>important parameters to pass into the method: <strong class="source-inline">model_id</strong>, which specifies the <a id="_idIndexMarker866"/>name of the <a id="_idIndexMarker867"/>model you wish to export, the <strong class="source-inline">decompress_definition</strong> flag, which should be set to <strong class="source-inline">False</strong>, and the <strong class="source-inline">include_model_definition</strong> flag, which should be set to <strong class="source-inline">True</strong>: </p><p class="source-code">model_id = breast-cancer-wisconsin-classification-1612270856116 </p><p class="source-code">compressed_model = ml.client.get_trained_models(model_id, decompress_definition=False, include_model_definition=True, for_export=True)</p><p class="callout-heading">Tip</p><p class="callout">If you are running the preceding code against an Elasticsearch cluster version 7.11 or later, you set an additional parameter, <strong class="source-inline">exclude_generated</strong>, as <strong class="source-inline">True</strong> to make sure that the response you receive from the API is in a convenient format for importing into another Elasticsearch cluster. If you are running version 7.10, use <strong class="source-inline">for_export</strong> as shown. </p><p>When we <a id="_idIndexMarker868"/>run the preceding code, the API will return a Python dictionary stored in the <strong class="source-inline">compressed_model</strong> variable, which includes information about our <a id="_idIndexMarker869"/>model as well as the <a id="_idIndexMarker870"/>compressed <a id="_idIndexMarker871"/>definition, as is shown in the snippet in <em class="italic">Figure 13.3</em>. Note that the actual response returned is much longer and is not shown to save space:</p><div id="_idContainer297" class="IMG---Figure"><img src="image/B17040_13_3.jpg" alt="Figure 13.3 – A snippet of the Python dictionary that captures the compressed definition of a model as well as metadata&#13;&#10;"/></div><p class="figure-caption">Figure 13.3 – A snippet of the Python dictionary that captures the compressed definition of a model as well as metadata</p></li>
				<li>Once we have this <a id="_idIndexMarker872"/>model definition stored in a <strong class="source-inline">compressed_model</strong> variable in our Python script, we can <a id="_idIndexMarker873"/>convert the dictionary to a JSON-formatted string and write it out to a file, which can be <a id="_idIndexMarker874"/>stored in <a id="_idIndexMarker875"/>version control or imported into another Elasticsearch cluster. <p>To convert the dictionary to JSON format, we must import the built-in Python <strong class="source-inline">json</strong> library:</p><p class="source-code"><strong class="source-inline">import json </strong></p><p>After this, we can write the exported model to a file whose path we have stored in the <strong class="source-inline">filename</strong> variable:</p><p class="source-code">with open(filename, 'w') as handle:</p><p class="source-code">    handle.write(json.dumps(compressed_model))</p></li>
			</ol>
			<p>All of the preceding steps are summarized in the <strong class="source-inline">export_model.py</strong> script, which is available in the book's GitHub repository here: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference/model_import_export</a>.</p>
			<p>Now that we have <a id="_idIndexMarker876"/>seen how to export a <a id="_idIndexMarker877"/>trained model from an Elasticsearch cluster, let's look at how to import a model from a file. As previously, we will break down the logic into steps, but <a id="_idIndexMarker878"/>the full working script will be stored in the book's GitHub repository here: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference</a>: </p>
			<ol>
				<li value="1">Many of the steps in this <a id="_idIndexMarker879"/>procedure are like the export script we stepped through in detail previously. In particular, the creation of the <strong class="source-inline">Elasticsearch</strong> and <strong class="source-inline">MlClient</strong> objects, as well as the parsing of the command-line arguments, follow similar steps as the preceding script, so we will not be explaining them in detail. Thus, the first step is to read the model file and convert the string contents into a Python dictionary using the <strong class="source-inline">loads</strong> file from the built-in <strong class="source-inline">json</strong> library:<p class="source-code">With open(filename, 'r') as handle:</p><p class="source-code">    model_definition = json.loads(handle.read())</p></li>
				<li>Once we have the compressed <strong class="source-inline">model_definition</strong> and the required metadata loaded into our Python dictionary, we can use the <strong class="source-inline">put_trained_model</strong> method to upload it into our cluster.</li>
				<li>Finally, navigate to the Kibana instance of your cluster and use the Trained Models API to double-check that the model has indeed been imported into your cluster.</li>
			</ol>
			<p>Now that we have <a id="_idIndexMarker880"/>learned how to view details about a model as well as how to export and import models, we <a id="_idIndexMarker881"/>are ready to move on to building more complicated machine learning infrastructures without models. Once <a id="_idIndexMarker882"/>you have a trained model, the possibilities for the model are nearly endless – you can combine the <a id="_idIndexMarker883"/>model with transforms, use it to enrich your data at ingest time, and much more. The building blocks of this infrastructure are inference processors and ingest pipelines. We will take a detailed look at each of these two in the next chapter to get you ready for building your own machine learning infrastructures. </p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor241"/>Understanding inference processors and ingest pipelines </h1>
			<p>You have a trained machine learning model, so now what? Remember from <a href="B17040_11_Epub_AM.xhtml#_idTextAnchor209"><em class="italic">Chapter 11</em></a>, <em class="italic">Classification Analysis</em>, and <a href="B17040_12_Epub_AM.xhtml#_idTextAnchor230"><em class="italic">Chapter 12</em></a>, <em class="italic">Regression</em>, that one of the exciting things about machine learning models is that they learn from a labeled training dataset and then, in a way, encode the knowledge so that they can be used to make predictions on previously unseen data points. This <a id="_idIndexMarker884"/>process of labeling or making predictions for previously unseen data points is what we call <strong class="bold">inference</strong>. </p>
			<p>How does this happen in practice in the Elastic Stack?</p>
			<p>There are a multitude of different architectures that you might build to make use of inference in the Elastic Stack, but the basic building blocks of all of them are inference processors and ingest pipelines. These are the main subjects of our exploration in this chapter. </p>
			<p>An <strong class="bold">ingest pipeline</strong> is a <a id="_idIndexMarker885"/>special component that lets you manipulate and transform your data in various ways before it is written to an Elasticsearch index. Ingest pipelines are normally composed of various processors, which are sub-units or configurable tasks that each perform a single type of manipulation or transformation on the data being ingested. Ingest pipelines can consist of multiple processors that are performed sequentially on each incoming data document as it is being ingested. </p>
			<p>For example, a typical ingest pipeline architecture might involve a script processor, which is able to execute a Painelss script on each document ingested through the pipeline,  followed by an inference processor followed by another script processor. For many machine learning applications, such as the ones we will look at a little bit later in this chapter, this is the perfect place to perform feature engineering or transform features into a suitable format for consumption by the machine learning model or to remove unnecessary fields before a document is ingested into Elasticsearch. </p>
			<p>There are a variety of <a id="_idIndexMarker886"/>built-in processors that can be combined and customized to create complex data transformation pipelines. For example, the <strong class="bold">GeoIP</strong> processor <a id="_idIndexMarker887"/>adds geographical information about IP addresses, the script processor allows users to write <a id="_idIndexMarker888"/>custom painless code to perform calculations and manipulations on existing document fields, and the <strong class="bold">CSV</strong> processor <a id="_idIndexMarker889"/>enables parsing and the extraction of data from CSV values to create fields. The full list of processors is available in the Elasticsearch documentation here: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/processors.html</a>. We encourage you to take a look to see the kinds of possible data architecture you can build with them.</p>
			<p>For our purposes, in terms of exploring machine learning, the most important processor to study is the inference processor. When documents pass through this processor, they are annotated with predictions by the machine learning model referenced in the processor's configuration. Let's take a look at how to configure our own inference processor and use it in an ingest pipeline with the help of a practical example. </p>
			<p>In this example, we will be using the fictitious social media dataset that we first examined in <a href="B17040_09_Epub_AM.xhtml#_idTextAnchor162"><em class="italic">Chapter 9</em></a>, <em class="italic">Introducing Data Frame Analytics</em>. This time, we will be using a language identification model to identify which language the text in these fictional microblogging site posts is written in. Let's get started!</p>
			<ol>
				<li value="1">If you have been playing around with the Trained Models API we discussed at the beginning of this chapter, you may have noticed that even if you have not trained any models in a particular Elasticsearch cluster, you will still see a single model, <strong class="source-inline">lang_ident_model_1</strong>, available in the cluster. The metadata associated with this model returned by the Trained Models API is shown in <em class="italic">Figure 13.4</em>:<div id="_idContainer298" class="IMG---Figure"><img src="image/B17040_13_4.jpg" alt="Figure 13.4 – The language identification model lang_ident_model_1&#13;&#10;"/></div><p class="figure-caption">Figure 13.4 – The language identification model lang_ident_model_1</p><p>The model <a id="_idIndexMarker890"/>ships by <a id="_idIndexMarker891"/>default with an Elasticsearch cluster and can be used in inference processors and ingest pipelines just like any other model you might train yourself on an Elasticsearch cluster! </p></li>
				<li>Next, let's see how we can create an ingest pipeline configuration with an inference processor that references this language identification model. Recall that processors are the sub-units within an ingest pipeline that process each document as it enters the pipeline and before it is written to an Elasticsearch index. Even though you can never use an inference processor as a standalone functional unit in Elasticsearch – it must always be a part of a pipeline – let's first examine the configuration of the processor in isolation and then see how it fits into an Ingest pipeline.<p>The following code <a id="_idIndexMarker892"/>snippet shows the configuration of an <a id="_idIndexMarker893"/>inference processor for our planned language identification text pipeline: </p><p class="source-code">{</p><p class="source-code">  "inference": {</p><p class="source-code">    "model_id": " lang_ident_model_1",</p><p class="source-code">    "target_field": "text_language_prediction",</p><p class="source-code">    "field_map": {</p><p class="source-code">      "post": "text"</p><p class="source-code">    },</p><p class="source-code">    "inference_config": { "classification": {} }</p><p class="source-code">  }</p><p class="source-code">}</p><p>Let's take a moment to go over the most important configuration parameters and what they mean. For a full API reference of all the available configuration options for inference processors, please look at the Elasticsearch documentation here: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html">https://www.elastic.co/guide/en/elasticsearch/reference/master/inference-processor.html</a>. </p><p>The key part of any inference processor is the trained machine learning model that will be used to make predictions on our text documents. The inference processor becomes aware of which model it should use to classify incoming documents through the <strong class="source-inline">model_id</strong> configuration field. In our case, the <strong class="source-inline">model_id</strong> (which is available in the metadata of the model that we can view by using the Trained Models API) is <strong class="source-inline">lang_ident_model_1</strong>. </p><p class="callout-heading">Tip</p><p class="callout">Remember that you can always look up the <strong class="source-inline">model_id</strong> of your supervised machine learning model by using the Trained Models API. </p><p>The next configuration item is <strong class="source-inline">target_field</strong>. Remember that once the inference processor processes a document coming through the ingest pipeline, it will generate a prediction for the document based on the result from the trained machine learning model. For the user to view this prediction, it must be written to a field in the document. This is the field that we can, optionally, specify in the <strong class="source-inline">target_field</strong> configuration. If you do not have a particular preference for the name of the field that will hold your machine learning model's predictions, you can leave this empty, and a default name will be assigned to it. </p><p>Finally, before we <a id="_idIndexMarker894"/>jump into the actual configuration of the classifier, let's <a id="_idIndexMarker895"/>take a moment to understand the <strong class="source-inline">field_map</strong> configuration parameter. Recall from our previous discussions in <a href="B17040_11_Epub_AM.xhtml#_idTextAnchor209"><em class="italic">Chapter 11</em></a>, <em class="italic">Classification Analysis</em> and <a href="B17040_12_Epub_AM.xhtml#_idTextAnchor230"><em class="italic">Chapter 12</em></a>, <em class="italic">Regression</em>, that a supervised machine learning model is trained on a labeled dataset that contains features and a dependent variable. These features should encode meaningful information about the dependent variable. Because these features are important for training the model, they are also needed when we use the trained model to perform predictions – or inference – on previously unseen data points. </p><p>However, since the process of training a machine learning model is decoupled from the process of inference, it is entirely possible that the field names we used for the features of a model in the training dataset are not the same as the field names of the features in the new data that we want to use in inference. </p><p>In this case, the language identification model is pre-trained for the user, so a natural question to ask is how we, as end users, find out what was the name of the fields that were used as features in the model and thus, what we should call the fields in the documents we wish to use in inference in order for the model to work. The answer to this question lies in the metadata for the model returned by the Trained Models API and which is displayed in <em class="italic">Figure 13.4</em>. Look to the bottom of the metadata to locate the <strong class="source-inline">input</strong> field and the configuration block that is nested beneath it:<strong class="source-inline"> </strong></p><p class="source-code">      "input" : {</p><p class="source-code">        "field_names" : [</p><p class="source-code">          "text"</p><p class="source-code">        ]</p><p class="source-code">      }</p><p>The field name <a id="_idIndexMarker896"/>listed under <strong class="source-inline">field_names</strong> in the model's metadata <a id="_idIndexMarker897"/>specifies that the name of the feature field used in the model is text, which means that the features we want to use in our inference documents need to have a field of this name that contains the text to which we wish to apply language identification.</p><p>Since the training of the model is decoupled from the inference process, it is entirely possible that the field names chosen for the features during the training process are not available in the data that we wish to pass through our inference processor. In this case, if it is not possible or desirable to alter the field names of our data, we can use the <strong class="source-inline">field_map</strong> configuration block in our inference processor to map from the field name in the data we will be using for inference to the field name that our supervised model expects. Below, we have configured a mapping between the <strong class="source-inline">post</strong> field name that contains the text in our fictional microblogging social media dataset and the <strong class="source-inline">text</strong> field name that the model expects:</p><p class="source-code">    "field_map": {</p><p class="source-code">      "post": "text"</p><p class="source-code">    },</p><p>Finally, we've come to the last part of the configuration – the <strong class="source-inline">inference_config</strong> block. The configuration options for this block determine whether we are using classification or regression. Since, in the case of language identification, we are working with multiclass classification, we will select classification and leave all the other configuration options as their default settings. Slightly later in this section, we will take a closer look at the available fields in <strong class="source-inline">inference_config</strong> and how adjusting them determines the final format of the results. </p></li>
				<li>Now that we have <a id="_idIndexMarker898"/>examined the pieces that are part of configuring the <a id="_idIndexMarker899"/>inference processor, let's move on to configuring the ingest pipeline. This is the top-level container, if you will, or component that will house our inference processor and potentially others as well. <p>The configuration for an ingest pipeline that contains the inference processor we configured looks like the following: </p><p class="source-code">PUT _ingest/pipeline/language-identification-pipeline</p><p class="source-code">{</p><p class="source-code">  "description": "Pipeline for classifying language in social media posts",</p><p class="source-code">  "processors": [</p><p class="source-code">{</p><p class="source-code">  "inference": {</p><p class="source-code">    "model_id": " lang_ident_model_1",</p><p class="source-code">    "target_field": "text_language_prediction",</p><p class="source-code">    "field_map": {</p><p class="source-code">      "post": "text"</p><p class="source-code">    },</p><p class="source-code">    "inference_config": { "classification": {} }</p><p class="source-code">  }</p><p class="source-code">}</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>The bulk of the <a id="_idIndexMarker900"/>configuration is taken up by the specifications for the inference processor that we studied in detail previously. The only <a id="_idIndexMarker901"/>additional noteworthy features in the configuration of the ingest pipeline are the name of the pipeline, which is a part of the REST API endpoint, and the <strong class="source-inline">processors</strong> array in the body of the configuration. In this case, we have chosen to call the pipeline <strong class="source-inline">language-identification-pipeline</strong>. The <strong class="source-inline">processors</strong> array contains the configuration specifications for our processors. In this case, we only have one processor. <p>You can either copy and paste the following configuration into the Kibana Dev Console or, alternatively, use the Ingest Pipelines wizard, which is available in the <strong class="bold">Stack</strong> <strong class="bold">Management</strong> panel in Kibana, as shown in <em class="italic">Figure 13.5</em>: </p><div id="_idContainer299" class="IMG---Figure"><img src="image/B17040_13_5.jpg" alt="Figure 13.5 – The Create pipeline wizard&#13;&#10;"/></div><p class="figure-caption">Figure 13.5 – The Create pipeline wizard</p></li>
				<li>Once we have our pipeline configured – either through the Dev Console or the wizard, we are ready to start using it to identify the language in our fictional social media microblogging platform posts. <p>Although normally, we would use an ingest pipeline together with a transform or a beat such as <strong class="source-inline">packetbeat</strong>, in this case, we are going to ingest documents into an index using the Kibana Dev Console since it is easier to illustrate the concepts we want to <a id="_idIndexMarker902"/>teach there. Slightly later in the chapter, we <a id="_idIndexMarker903"/>will look at more advanced and, hence, more realistic examples. </p><p>Let's index our first document through the pipeline. The REST API command that achieved this looks like the following:</p><p class="source-code">POST social-media-feed-inference/_doc?pipeline=language-identification-pipeline</p><p class="source-code">{</p><p class="source-code">  "username": "Sanna",</p><p class="source-code">    "statistics": {</p><p class="source-code">      "likes": 320,</p><p class="source-code">      "shares": 8000</p><p class="source-code">    },</p><p class="source-code">    "timestamp": "2021-01-20T23:19:06",</p><p class="source-code">    "post" : "Terveisiä Suomesta! Täällä olen talvilomalla!"</p><p class="source-code">}</p><p>We are sending a <strong class="source-inline">POST</strong> request to index the document in the body of the request and are passing the name of the pipeline we created in the preceding steps as the argument for the optional <strong class="source-inline">pipeline</strong> parameter. In this case, a user, "Sanna," has written an update (as seen in the post field in Finnish). Let's examine the <strong class="source-inline">social-media-feed-inference</strong> index to see what the ingested document looks like.</p><p>If you have not already done so, create <a id="_idIndexMarker904"/>an index pattern for the <strong class="source-inline">social-media-feed-inference</strong> index and navigate to the <a id="_idIndexMarker905"/>Discover app in Kibana. Now, the <strong class="source-inline">social-media-feed-inference</strong> index contains only the one document we indexed using the REST API call shown previously. The document is shown in <em class="italic">Figure 13.6</em>:</p><div id="_idContainer300" class="IMG---Figure"><img src="image/B17040_13_6.jpg" alt="Figure 13.6 – An ingested document in the social-media-feed-inference index &#13;&#10;"/></div><p class="figure-caption">Figure 13.6 – An ingested document in the social-media-feed-inference index </p><p>As we can see from the document in <em class="italic">Figure 13.6</em>, the inference processor has added four new fields alongside the original fields in the document. All these fields are prefixed with the name <strong class="source-inline">text_language_prediction_model</strong>, which is what we configured in our inference processor configuration. As we can see, the fields record the <strong class="source-inline">model_id</strong> of the model used to make the prediction, <strong class="source-inline">predicted_value</strong>, which in this case will contain an identifier for the language the model predicts the post to be in, <strong class="source-inline">prediction_probability</strong> as well as <strong class="source-inline">prediction_score</strong>. These were previously covered in <a href="B17040_11_Epub_AM.xhtml#_idTextAnchor209"><em class="italic">Chapter 11</em></a>, <em class="italic">Classification Analysis</em>.</p><p>As we can see, in this case, the model has determined correctly that the original post was written in the Finnish language. </p></li>
				<li>In the previous <a id="_idIndexMarker906"/>example, we created the inference processor and ingest pipeline configurations, and proceeded directly to index documents into <a id="_idIndexMarker907"/>our index through the pipeline. If we wish to first see a few dry runs of our pipeline before indexing, we can use the <strong class="source-inline">_simulate</strong> endpoint: <p class="source-code">POST _ingest/pipeline/language-identification-pipeline/_simulate</p><p class="source-code">{</p><p class="source-code">  "docs": [</p><p class="source-code">    {"_source": {</p><p class="source-code">  "username": "Sanna",</p><p class="source-code">    "statistics": {</p><p class="source-code">      "likes": 320,</p><p class="source-code">      "shares": 8000</p><p class="source-code">    },</p><p class="source-code">    "timestamp": "2021-01-20T23:19:06",</p><p class="source-code">    "post" : "Terveisiä Suomesta! Täällä olen talvilomalla!"</p><p class="source-code">}</p><p class="source-code">}</p><p class="source-code">  ]</p><p class="source-code">}</p><p>The response <a id="_idIndexMarker908"/>returned by the API for this call contains the results of the <a id="_idIndexMarker909"/>model's predictions, as you can see in the following code snippet:</p><p class="source-code">{</p><p class="source-code">      "doc" : {</p><p class="source-code">        "_index" : "_index",</p><p class="source-code">        "_type" : "_doc",</p><p class="source-code">        "_id" : "_id",</p><p class="source-code">        "_source" : {</p><p class="source-code">          "post" : "Terveisiä Suomesta! Täällä olen talvilomalla!",</p><p class="source-code">          "text_language_prediction" : {</p><p class="source-code">            "prediction_score" : 0.9999995958245499,</p><p class="source-code">            "model_id" : "lang_ident_model_1",</p><p class="source-code">            "prediction_probability" : 0.9999995958245499,</p><p class="source-code">            "predicted_value" : "fi"</p><p class="source-code">          },</p><p class="source-code">          "username" : "Sanna",</p><p class="source-code">          "statistics" : {</p><p class="source-code">            "shares" : 8000,</p><p class="source-code">            "likes" : 320</p><p class="source-code">          },</p><p class="source-code">          "timestamp" : "2021-01-20T23:19:06"</p><p class="source-code">        },</p><p class="source-code">        "_ingest" : {</p><p class="source-code">          "timestamp" : "2021-03-29T01:35:07.492629377Z"</p><p class="source-code">        }</p><p class="source-code">      }</p><p class="source-code">    }</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>Finally, it is also possible to <a id="_idIndexMarker910"/>use the Ingest Pipeline UI to test out the documents before beginning to ingest to make sure that everything is <a id="_idIndexMarker911"/>working as intended. Unfortunately, in the UI, this can only be performed during the creation of a new ingest pipeline and not with an existing one, so, for the purposes of this demonstration, you can use the wizard to start creating a clone of the <strong class="source-inline">language-identification-pipeline</strong> we created previously. <p>Then, to the right of the <strong class="bold">Processors</strong> selector in the wizard, as shown in <em class="italic">Figure 13.7</em>, locate the <strong class="bold">Test pipeline</strong> text and click on the <strong class="bold">Add documents</strong> link: </p></li>
			</ol>
			<div>
				<div id="_idContainer301" class="IMG---Figure">
					<img src="image/B17040_13_7.jpg" alt="Figure 13.7 – The Create Ingest pipeline wizard &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.7 – The Create Ingest pipeline wizard </p>
			<p>This will trigger a <a id="_idIndexMarker912"/>menu on the right-hand side of the wizard that <a id="_idIndexMarker913"/>allows you to either add a document from an index or manually specify it in the textbox provided. In this case, we are going to manually add our Finnish language test document to the textbox, as shown in <em class="italic">Figure 13.8</em>:</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B17040_13_8.jpg" alt="Figure 13.8 – A sample document in the ingest pipeline&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.8 – A sample document in the ingest pipeline</p>
			<p>Once you have configured your <a id="_idIndexMarker914"/>documents, click the <strong class="bold">Run the pipeline</strong> button and you <a id="_idIndexMarker915"/>should see a preview of your documents after they have passed through the inference pipeline. </p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor242"/>Handling missing or corrupted data in ingest pipelines </h2>
			<p>Many, or maybe <a id="_idIndexMarker916"/>even most, real-world applications will not <a id="_idIndexMarker917"/>have neat datasets. Instead, data will be missing, mislabeled, and potentially even corrupted. It's important to take a moment to look at what happens in such cases with inference processors so that you can recognize and mitigate these issues in your own pipelines:</p>
			<ol>
				<li value="1">Let's continue with the fictitious microblogging platform we used as our example previously and suppose that due to a misconfiguration error, we rename the <strong class="source-inline">post</strong> field, which <a id="_idIndexMarker918"/>contains the text string <a id="_idIndexMarker919"/>whose language we wish to detect, to <strong class="source-inline">post_text</strong>, as shown: <p class="source-code">POST social-media-feed-inference/_doc?pipeline=language-identification-pipeline</p><p class="source-code">{</p><p class="source-code">  "username": "Sanna",</p><p class="source-code">    "statistics": {</p><p class="source-code">      "likes": 320,</p><p class="source-code">      "shares": 8000</p><p class="source-code">    },</p><p class="source-code">    "timestamp": "2021-01-20T23:19:06",</p><p class="source-code">    "post_text" : "Terveisiä Suomesta! Täällä olen talvilomalla!"</p><p class="source-code">}</p><p>What happens once we send this text through the <strong class="source-inline">language-identification-pipeline</strong>? Let's perform the REST API call shown previously and then look at the ingested document in the <strong class="bold">Discover</strong> tab, as we did in the previous section. </p></li>
				<li>As we can see from the document as shown in <em class="italic">Figure 13.9</em>, the model was not able to make a correct prediction about the language in which the text in the <strong class="source-inline">post_text</strong> field was written: </li>
			</ol>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/B17040_13_9.jpg" alt="Figure 13.9 – A warning message in an ingested document&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.9 – A warning message in an ingested document</p>
			<p>Real-world use cases and datasets are <a id="_idIndexMarker920"/>often messy and contain <a id="_idIndexMarker921"/>missing and corrupt data, so be on the lookout for this message to catch and rectify potential errors in your inference setup!</p>
			<p>The first step to troubleshoot why this message appears is to compare the fields in the documents you are trying to ingest through the ingest pipeline with the analyzed_fields stored in the model’s metadata. Refer back to the section <em class="italic">Examining, Exporting and Importing your Trained Models with the Trained Models API</em> in this chapter for tips on how to view a model’s metadata.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor243"/>Using inference processor configuration options to gain more insight into your predictions</h2>
			<p>In the previous sections, we configured our inference processors in such a way that the documents processed by this processor only contained four fields: the predicted class label of the document, the <a id="_idIndexMarker922"/>probability of the prediction, the score of the prediction, and the model ID. However, what happens in cases where you can <a id="_idIndexMarker923"/>see that the model has made an incorrect prediction, or you would like to have more information about the probabilities that the model was assigned to other potential classes? This can be useful for debugging. </p>
			<p>How do we configure the inference processor to provide us with more information? Let's take a look:</p>
			<ol>
				<li value="1">Let's begin by going back to the inference processor configuration that we saw in the previous section and taking a closer look at the <strong class="source-inline">inference_config</strong> configuration block, which we left empty. In this case, since we want to see a more detailed breakdown of different probabilities, we want to add a <strong class="source-inline">num_top_classes</strong> field to the configuration block. This configuration parameter controls the <a id="_idIndexMarker924"/>number of classes for which the probabilities are written out. For example, if we set it to 3, each document will <a id="_idIndexMarker925"/>contain the probabilities for the top 3 classes it was most likely to belong to: <p class="source-code">PUT _ingest/pipeline/language-identification-pipeline-v2</p><p class="source-code">{</p><p class="source-code">  "description": "Pipeline for classifying language in social media posts",</p><p class="source-code">  "processors": [</p><p class="source-code">{</p><p class="source-code">  "inference": {</p><p class="source-code">    "model_id": " lang_ident_model_1",</p><p class="source-code">    "target_field": "text_language_prediction",</p><p class="source-code">    "field_map": {</p><p class="source-code">      "post": "text"</p><p class="source-code">    },</p><p class="source-code">    "inference_config": { "classification": {"num_top_classes": 3} }</p><p class="source-code">  }</p><p class="source-code">}</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>Next, let's ingest a document through this new pipeline, <strong class="source-inline">language-identification-pipeline-v2</strong>, using the following REST API call:<p class="source-code">POST social-media-feed-inference/_doc?pipeline=language-identification-pipeline-v2</p><p class="source-code">{</p><p class="source-code">  "username": "Sanna",</p><p class="source-code">    "statistics": {</p><p class="source-code">      "likes": 320,</p><p class="source-code">      "shares": 8000</p><p class="source-code">    },</p><p class="source-code">    "timestamp": "2021-01-20T23:19:06",</p><p class="source-code">    "post" : "Terveisiä Suomesta! Täällä olen talvilomalla!"</p><p class="source-code">}</p><p>We will see that as a <a id="_idIndexMarker926"/>result, the inference processor writes <a id="_idIndexMarker927"/>out a detailed breakdown of the possible languages (or classes if we are using classification terminology) to which the post belongs, as shown in <em class="italic">Figure 13.10</em>. Possible candidates are Finnish, denoted by the keyword <strong class="bold">fi</strong>, Swedish, denoted by the keyword <strong class="bold">sv</strong>, and Estonian, denoted by the keyword <strong class="bold">eo</strong>: </p></li>
			</ol>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<img src="image/B17040_13_10.jpg" alt="Figure 13.10 – A detailed breakdown of the potential classes a given document belongs to along with the associated probabilities &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.10 – A detailed breakdown of the potential classes a given document belongs to along with the associated probabilities </p>
			<p>We'll now <a id="_idIndexMarker928"/>move on to <a id="_idIndexMarker929"/>importing models using eland.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor244"/>Importing external models into Elasticsearch using eland</h1>
			<p>Suppose you already have a <a id="_idIndexMarker930"/>model trained using one of the other frameworks. Is it possible to re-use the building blocks we <a id="_idIndexMarker931"/>discussed in the previous section to deploy <a id="_idIndexMarker932"/>your own externally trained models? The answer is yes, with a few limitations. In this section, we will take a look at how to use the <strong class="bold">eland</strong> library, along with <strong class="bold">scikit-learn</strong>, another <a id="_idIndexMarker933"/>machine learning library for creating and training external machine learning models and importing them into Elasticsearch for inference. </p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor245"/>Learning about supported external models in eland </h2>
			<p>Unfortunately, the inference <a id="_idIndexMarker934"/>functionality in the Elastic Stack does not yet <a id="_idIndexMarker935"/>have support for importing an externally trained machine learning model from any library (though it might at some point in the future!). Instead, the eland documentation (<a href="https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model">https://eland.readthedocs.io/en/7.10.1b1/reference/api/eland.ml.MLModel.import_model.html#eland.ml.MLModel.import_model</a>) contains a list of third-party libraries that produce supported models. As it stands currently, the list of supported model types is as follows:</p>
			<ul>
				<li><strong class="source-inline">sklearn.tree.DecisionTreeClassifier</strong></li>
				<li><strong class="source-inline">sklearn.tree.DecisionTreeRegressor</strong></li>
				<li><strong class="source-inline">sklearn.ensemble.RandomForestRegressor</strong></li>
				<li><strong class="source-inline">sklearn.ensemble.RandomForestClassifier</strong></li>
				<li><strong class="source-inline">lightgbm.LGBMRegressor</strong></li>
				<li><strong class="source-inline">lightgbm.LGBMClassifier</strong></li>
				<li><strong class="source-inline">xgboost.XGBClassifier</strong></li>
				<li><strong class="source-inline">xgboost.XGBRegressor</strong><p class="callout-heading">Note</p><p class="callout">Please note that there are some additional restrictions to models generated with the preceding libraries that pertain to the type of objective function that is selected or the type of encoding that must be enforced on the features, so please do make sure you check the eland documentation for the most up-to-date information of supported third-party models. </p></li>
			</ul>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor246"/>Training a scikit-learn DecisionTreeClassifier and importing it into Elasticsearch using eland</h2>
			<p>Now that we have <a id="_idIndexMarker936"/>learned about the important preliminaries, let's hit the ground running and take a look at how we can train an external machine learning model using the scikit-learn library. All of the code examples used in this walk-through will be available in the Jupyter notebook in the book's GitHub repository here: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/tree/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models</a>: </p>
			<ol>
				<li value="1">The first step in our project to import an external model into Elasticsearch will be to retrieve some training data and use it to train a decision tree model. The scikit-learn library has a <a id="_idIndexMarker937"/>great collection of built-in datasets that can be used for learning and quick prototyping. To continue with the same data theme that we have been developing in <a href="B17040_11_Epub_AM.xhtml#_idTextAnchor209"><em class="italic">Chapter 11</em></a>, <em class="italic">Classification Analysis</em> and in this chapter, we will be using the built-in Wisconsin Breast Cancer dataset (which is a variation of the dataset that we have been using).<p>Before we begin, let's <a id="_idIndexMarker938"/>make sure <a id="_idIndexMarker939"/>that we import all of the required functions and libraries into our <a id="_idIndexMarker940"/>Python script (or Jupyter notebook):</p><p class="source-code"># import a variation of the breast cancer dataset we have been using in earlier chapters</p><p class="source-code">from sklearn.datasets import load_breast_cancer</p><p class="source-code"># import the function that trains a DecisionTreeClassifier</p><p class="source-code">from sklearn.tree import DecisionTreeClassifier</p><p class="source-code"># import a helper function to generate the test/train split</p><p class="source-code">from sklearn.model_selection import train_test_split </p></li>
				<li>Now that we have our imports, let's load the dataset by calling the <strong class="source-inline">load_breast_cancer</strong> function. <p class="source-code"># let's load the dataset and store the datapoints in the variable X and the class labels in the variable y</p><p class="source-code">X, y = load_breast_cancer(return_X_y=True)</p><p>Notice that this function returns two values, which we store in the variables <strong class="source-inline">X </strong>and <strong class="source-inline">y</strong>. The way that Elasticsearch organizes its training data is different from the conventions in scikit-learn. In Elasticsearch, our training data was stored in a single Elasticsearch index. Each document in the index represents one data point and is a combination of fields that represent the features and the field that represents the dependent variable (see <a href="B17040_11_Epub_AM.xhtml#_idTextAnchor209"><em class="italic">Chapter 11</em></a>, <em class="italic">Classification Analysis</em> and <a href="B17040_12_Epub_AM.xhtml#_idTextAnchor230"><em class="italic">Chapter 12</em></a>, <em class="italic">Regression</em>, for further information about dependent variables and why they are important in supervised learning). </p><p>In contrast with <a id="_idIndexMarker941"/>Elasticsearch's approach, scikit-learn represents each data point using a vector that <a id="_idIndexMarker942"/>contains <a id="_idIndexMarker943"/>all of the feature values. These vectors make up the matrix stored in the variable <strong class="source-inline">X</strong>. We can use Python's slicing syntax to see what a sample data point would look like. An example is shown in <em class="italic">Figure 13.11</em>:</p><div id="_idContainer305" class="IMG---Figure"><img src="image/B17040_13_11.jpg" alt="Figure 13.11 – A data point is represented as a vector of field values&#13;&#10;"/></div><p class="figure-caption">Figure 13.11 – A data point is represented as a vector of field values</p><p>The dependent variables are stored in the separate variable <strong class="source-inline">y</strong>. In a similar fashion to our previous example, we can use Python's slicing syntax to see which class the <a id="_idIndexMarker944"/>data point whose feature values (or <strong class="bold">feature vector</strong>) are depicted belongs to. This is shown in <em class="italic">Figure 13.12</em>: </p><div id="_idContainer306" class="IMG---Figure"><img src="image/B17040_13_12.jpg" alt="Figure 13.12 – The class label for the first data point is 0&#13;&#10;"/></div><p class="figure-caption">Figure 13.12 – The class label for the first data point is 0</p></li>
				<li>Now that we have imported our dataset and verified that it looks acceptable, we can move on to the next step, which is training our decision tree model. While Elasticsearch automatically splits our training data into a training and a testing dataset, in scikit-learn, we have to perform this step manually. Although it's not strictly <a id="_idIndexMarker945"/>necessary to do this in this case, since we are not overly interested in systematically <a id="_idIndexMarker946"/>measuring the <a id="_idIndexMarker947"/>performance of our model, we will still show this as an example for interested readers who will need to do this in their own projects: <p class="source-code"># while Elasticsearch performs the train/test split for us during the training process # in scikit-learn, we have to perform this step manually using the train_test_split function</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=12345)</p><p>As you can see from the preceding code snippet, we pass in the variable containing the feature vectors for every data point and the variable containing the dependent variable values or class labels into the scikit-learn function, <strong class="source-inline">train_test_split</strong>, and the function returns the feature vectors and dependent variables that belong to the training set (<strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong>) and testing set (<strong class="source-inline">X_test</strong>, <strong class="source-inline">y_test)</strong>, respectively. </p></li>
				<li>Now that we have our training and test dataset generated, we can move on to training the decision tree classifier. The first step is to create an instance of the <strong class="source-inline">DecisionTreeClassifier</strong> class and fit it with the feature vectors and class labels from the training dataset: <p class="source-code"># now, let's create the decision tree classifier</p><p class="source-code">dec_tree = DecisionTreeClassifier(random_state=12345).fit(X_train, y_train)</p><p>The trained model is referenced by the <strong class="source-inline">dec_tree</strong> variable. This is the variable we will serialize and upload to Elasticsearch using eland slightly later in this tutorial. First, however, let's run a quick check on our model by asking it to classify a data point <a id="_idIndexMarker948"/>from the <a id="_idIndexMarker949"/>testing <a id="_idIndexMarker950"/>dataset (remember, these are data points that the model has not seen previously during the training phase) as shown in <em class="italic">Figure 13.13</em>: </p><div id="_idContainer307" class="IMG---Figure"><img src="image/B17040_13_13.jpg" alt="Figure 13.13 – Predictions from the trained decision tree model&#13;&#10;"/></div><p class="figure-caption">Figure 13.13 – Predictions from the trained decision tree model</p><p>The model predicts that the first data point in our testing dataset belongs to class 1. We can double-check whether or not this is the actual label of the data point by checking the first element in the <strong class="source-inline">y_test</strong> variable, as shown in <em class="italic">Figure 13.14</em>: </p><div id="_idContainer308" class="IMG---Figure"><img src="image/B17040_13_14.jpg" alt="Figure 13.14 – The value of the dependent variable for the first data point in the testing set&#13;&#10;"/></div><p class="figure-caption">Figure 13.14 – The value of the dependent variable for the first data point in the testing set</p><p>In this case, the model's prediction matches the actual label of the dataset. </p></li>
				<li>Finally, let's prepare to upload this model into our Elasticsearch cluster using eland. First, we have to import the required <strong class="source-inline">MLModel</strong> class, as shown in the following code sample:<p class="source-code"># import the required eland class</p><p class="source-code">from eland.ml import MLModel</p><p>Once we have this class in our script or Jupyter notebook, we can proceed to the next step, which is retrieving the feature names from the original scikit-learn dataset. The steps required to do this are shown in the following code sample:</p><p class="source-code">data = load_breast_cancer() </p><p class="source-code">feature_names = data.feature_names</p><p>The interested <a id="_idIndexMarker951"/>reader <a id="_idIndexMarker952"/>can print <a id="_idIndexMarker953"/>out the <strong class="source-inline">feature_names</strong> variable (or take a look at the Jupyter notebook accompanying this walk-through) to see what kinds of features are included. In the interest of saving space, we will leave out the feature names list. </p></li>
				<li>Finally, we will call the <strong class="source-inline">import_model</strong> method on the <strong class="source-inline">MLModel</strong> class, as shown in the following code snippet:<p class="source-code">es_model = MLModel.import_model(</p><p class="source-code">    es_client,</p><p class="source-code">    model_id=model_id,</p><p class="source-code">    model=dec_tree,</p><p class="source-code">    feature_names=list(feature_names),</p><p class="source-code">    es_if_exists='replace'</p><p class="source-code">)</p><p>As you can see, this is a method that requires quite a number of parameters. The first parameter, <strong class="source-inline">es_client</strong>, is an instance of the Elasticsearch client object, which specifies how to connect to our Elasticsearch cluster. This is discussed in further detail in the <em class="italic">Exporting and importing trained models with the Inference API and Python</em> section in this chapter. </p><p>The second parameter is <strong class="source-inline">model_id</strong>, an identifier that will be used to identify the model once it has been uploaded into the Elasticsearch cluster. In this case, we have set the <strong class="source-inline">model_id</strong> variable, as shown in the following code snippet:</p><p class="source-code">model_id = "external-model_breast-cancer-decision-tree"</p><p>Of course, however, it is <a id="_idIndexMarker954"/>possible to set this identifier to be one of your <a id="_idIndexMarker955"/>choosing. Finally, we pass in the variable name that contains a reference to <a id="_idIndexMarker956"/>our trained model, <strong class="source-inline">dec_tree</strong>, the list of <strong class="source-inline">feature_names</strong> that we retrieved from the original dataset, and set the <strong class="source-inline">es_if_exists</strong> flag to <strong class="source-inline">'replace'</strong>, which means that if we run the code snippet more than once, an existing model with the same <strong class="source-inline">model_id</strong> will be overwritten. There are cases where this might not be the desired behavior, but in this case, since we are prototyping, it is a useful flag to set. </p></li>
				<li>Once the command discussed in the preceding section has been run, we can use the Trained Models API to determine whether or not this model has been imported into our cluster successfully. To do this, we will run the following command:<p class="source-code">GET _ml/trained_models/external-model_breast-cancer-decision-tree</p><p>As we can see, based <a id="_idIndexMarker957"/>on the <a id="_idIndexMarker958"/>returned API response, our model has indeed been successfully imported into the <a id="_idIndexMarker959"/>cluster and is now ready to use in ingest pipelines. </p><p class="callout-heading">Reminder</p><p class="callout">All of the code samples used in the figures are available in the Jupyter notebook that is linked in the book's GitHub repository here: <a href="https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb">https://github.com/PacktPublishing/Machine-Learning-with-Elastic-Stack-Second-Edition/blob/main/Chapter%2013%20-%20Inference%20and%20Advanced%20Transforms/external_models/importing-external-models-into-es-using-eland.ipynb</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor247"/>Summary</h1>
			<p>In this chapter, we have looked at the various options available for using supervised models trained in Elasticsearch and external libraries such as scikit-learn. We have learned about the Trained Models API, which is useful when managing and examining trained supervised learning models in an Elasticsearch cluster and how to make use of these models to make predictions on previously unseen examples with the help of inference processors and ingest pipelines. In the appendix following this chapter, we will provide some tips and tricks that make it easier to work with the Elastic Machine Learning stack.</p>
		</div>
	</body></html>