["```py\n!wget -q https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\n!tar -xzf dbpedia_csv.tar.gz\n!head dbpedia_csv/train.csv -n 3\n!cat dbpedia_csv/classes.txt\n```", "```py\n__label__latin Lorem ipsum dolor sit amet , consectetur adipiscing elit , sed do eiusmod tempor incididunt ut labore et dolore magna aliqua . \n```", "```py\npreprocess(\"dbpedia_csv/train.csv\", \"dbpedia.train\", keep=0.2)\npreprocess(\"dbpedia_csv/test.csv\", \"dbpedia.validation\")\n!head -n 1 dbpedia.train \n__label__Company automatic electric automatic electric company ( ae ) was the largest of the manufacturing units of the automatic electric group . it was a telephone equipment supplier for independent telephone companies in north america and also had a world-wide presence . with its line of automatic telephone exchanges it was also a long-term supplier of switching equipment to the bell system starting in 1919.\n```", "```py\nimage=sagemaker.image_uris.retrieve(framework='blazingtext', \n                                    region=region, \n                                    version='1')\nprint(image)\n433757028032.dkr.ecr.us-west-2.amazonaws.com/blazingtext:1\n```", "```py\nestimator = sagemaker.estimator.estimator(\n            image,\n            role,\n            instance_count=1,\n            instance_type='ml.c5.2xlarge',\n            volume_size=30,\n            max_run=360000,\n            input_mode='File',\n            enable_sagemaker_metrics=True,\n            output_path=s3_output_location,\n            hyperparameters={\n                'mode': 'supervised',\n                'epochs': 20,\n                'min_count': 2,\n                'learning_rate': 0.05,\n                'vector_dim': 10,\n                'early_stopping': True,\n                'patience': 4,\n                'min_epochs': 5,\n                'word_ngrams': 2,\n            },\n)\n```", "```py\ntrain_channel = prefix + '/train'\nvalidation_channel = prefix + '/validation'\nsess.upload_data(path='dbpedia_csv/dbpedia.train', bucket=bucket, key_prefix=train_channel)\nsess.upload_data(path='dbpedia_csv/dbpedia.validation', bucket=bucket, key_prefix=validation_channel)\ns3_train_data = f's3://{bucket}/{train_channel}'\ns3_validation_data = f's3://{bucket}/{validation_channel}'\nprint(s3_train_data)\nprint(s3_validation_data)\ndata_channels = {'train': s3_train_data, \n                 'validation': s3_validation_data}\nexp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\njobname = f'dbpedia-blazingtext-{exp_datetime}'\nestimator.fit(inputs=data_channels,\n              job_name=jobname,\n              logs=True)\n```", "```py\n    #train_accuracy: 0.9961\n    Number of train examples: 112000\n    #validation_accuracy: 0.9766\n    Number of validation examples: 70000\n    ```", "```py\n!aws s3 cp {estimator.model_data} ./dbpedia_csv/\n%%sh\ncd dbpedia_csv/\ntar -zxf model.tar.gz\n# Use the model archive with fastText\n# eg. fasttext predict ./model.bin test.txt\n```", "```py\n    !pip install -q sagemaker-experiments\n    ```", "```py\n    from smexperiments.experiment import Experiment\n    from smexperiments.trial import Trial\n    from botocore.exceptions import ClientError\n    from time import gmtime, strftime\n    import time\n    experiment_name = 'dbpedia-text-classification'\n    try:\n        experiment = Experiment.create(\n            experiment_name=experiment_name, \n            description='Training a text classification model using dbpedia dataset.')\n    except ClientError as e:\n        print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n    ```", "```py\n    def create_estimator(learning_rate):\n        hyperparameters={'mode': 'supervised',\n                         'epochs': 40,\n                         'min_count': 2,\n                         'learning_rate': learning_rate,\n                         'vector_dim': 10,\n                         'early_stopping': True,\n                         'patience': 4,\n                         'min_epochs': 5,\n                         'word_ngrams': 2}\n        estimator = sagemaker.estimator.estimator(\n                        image,\n                        role,\n                        instance_count=1,\n                        instance_type='ml.c4.4xlarge',\n                        volume_size=30,\n                        max_run=360000,\n                        input_mode='File',\n                        enable_sagemaker_metrics=True,\n                        output_path=s3_output_location,\n                        hyperparameters=hyperparameters)\n        return estimator\n    ```", "```py\n    for lr in [0.1, 0.01, 0.001]:\n        exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n        jobname = f'dbpedia-blazingtext-{exp_datetime}'\n        exp_trial = Trial.create(\n            experiment_name=experiment_name, \n            trial_name=jobname)\n        experiment_config={\n            'ExperimentName': experiment_name,\n            'TrialName': exp_trial.trial_name,\n            'TrialComponentDisplayName': 'Training'}\n        estimator = create_estimator(learning_rate=lr)    \n        estimator.fit(inputs=data_channels,\n                 job_name=jobname,\n                 experiment_config=experiment_config,\n                 wait=False)\n    ```", "```py\nembedding_layer = tf.keras.layers.Embedding(max_features,\n                                            embedding_dims,\n                                            input_length=maxlen)\nsequence_input = tf.keras.Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = tf.keras.layers.Dropout(args.drop_out_rate)(embedded_sequences)\nx = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(x)\nx = tf.keras.layers.MaxPooling1D()(x)\nx = tf.keras.layers.GlobalMaxPooling1D()(x)\nx = tf.keras.layers.Dense(hidden_dims, activation='relu')(x)\nx = tf.keras.layers.Dropout(drop_out_rate)(x)\npreds = tf.keras.layers.Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, preds)\noptimizer = tf.keras.optimizers.Adam(learning_rate)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n```", "```py\n    def parse_args():\n        parser = argparse.ArgumentParser()\n        # hyperparameters sent by the client are passed as command-line arguments to the script\n        parser.add_argument('--epochs', type=int, default=1)\n        parser.add_argument('--batch_size', type=int, default=64)\n        parser.add_argument('--learning_rate', type=float, default=0.01)\n        parser.add_argument('--drop_out_rate', type=float, default=0.2)\n        # data directories\n        parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n        parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n        # model directory /opt/ml/model default set by SageMaker\n        parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n        return parser.parse_known_args()\n    ```", "```py\n    if __name__ == \"__main__\":\n        args, _ = parse_args()\n        x_train, y_train = get_train_data(args.train)\n        x_test, y_test = get_test_data(args.test)\n        model = get_model(args)\n        history = model.fit(x_train, y_train,\n                  batch_size=args.batch_size,\n                  epochs=args.epochs,\n                  validation_data=(x_test, y_test))\n        save_history(args.model_dir + \"/history.p\", history)\n        # create a TensorFlow SavedModel for deployment to a SageMaker endpoint with TensorFlow Serving\n        model.save(args.model_dir + '/1')\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlow\n    exp_datetime = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n    jobname = f'imdb-tf-{exp_datetime}'\n    model_dir = f's3://{bucket}/{prefix}/{jobname}'\n    code_dir = f's3://{bucket}/{prefix}/{jobname}'\n    train_instance_type = 'ml.p3.2xlarge'\n    hyperparameters = {'epochs': 10, 'batch_size': 256, 'learning_rate': 0.01 , 'drop_out_rate': 0.2 }\n    estimator = TensorFlow(source_dir='code',\n                           entry_point='tensorflow_sentiment.py',\n                           model_dir=model_dir,\n                           code_location=code_dir,\n                           instance_type=train_instance_type,\n                           instance_count=1,\n                           enable_sagemaker_metrics=True,\n                           hyperparameters=hyperparameters,\n                           role=role,\n                           framework_version='2.1',\n                           py_version='py3')\n    ```", "```py\n    data_channels = {'train':train_s3, 'test': test_s3}\n    ```", "```py\n    experiment_name = 'imdb-sentiment-analysis'\n    try:\n        experiment = Experiment.create(\n            experiment_name=experiment_name, \n            description='Training a sentiment classification model using imdb dataset.')\n    except ClientError as e:\n        print(f'{experiment_name} experiment already exists! Reusing the existing experiment.')\n    # Creating a new trial for the experiment\n    exp_trial = Trial.create(\n        experiment_name=experiment_name, \n        trial_name=jobname)\n    experiment_config={\n        'ExperimentName': experiment_name,\n        'TrialName': exp_trial.trial_name,\n        'TrialComponentDisplayName': 'Training'}\n    ```", "```py\n    estimator.fit(inputs=data_channels,\n                  job_name=jobname,\n                  experiment_config=experiment_config,\n                  logs=True)\n    ```", "```py\n!mkdir ./imdb_data/model -p\n!aws s3 cp {estimator.model_data} ./imdb_data/model.tar.gz\n!tar -xzf ./imdb_data/model.tar.gz -C ./imdb_data/model/\nmy_model=tf.keras.models.load_model('./imdb_data/model/1/')\nmy_model.summary()\nloss, acc=my_model.evaluate(x_test, y_test, verbose=2)\nprint('Restored model, accuracy: {:5.2f}%'.format(100 * acc))\n782/782 - 55s - loss: 0.7448 - accuracy: 0.8713\nRestored model, accuracy: 87.13%\n```"]