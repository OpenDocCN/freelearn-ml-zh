["```py\n    CascadeClassifier faceDetector; \n    faceDetector.load(faceCascadeFilename);\n\n```", "```py\n    CascadeClassifier faceDetector; \n    try { \n      faceDetector.load(faceCascadeFilename); \n    } catch (cv::Exception e) {} \n    if ( faceDetector.empty() ) { \n      cerr << \"ERROR: Couldn't load Face Detector (\"; \n      cerr << faceCascadeFilename << \")!\" << endl; \n      exit(1); \n    }\n\n```", "```py\n    Mat gray; \n    if (img.channels() == 3) { \n      cvtColor(img, gray, CV_BGR2GRAY); \n    } \n    else if (img.channels() == 4) { \n      cvtColor(img, gray, CV_BGRA2GRAY); \n    } \n    else { \n      // Access the grayscale input image directly. \n      gray = img; \n    }\n\n```", "```py\n    const int DETECTION_WIDTH = 320; \n    // Possibly shrink the image, to run much faster. \n    Mat smallImg; \n    float scale = img.cols / (float) DETECTION_WIDTH; \n    if (img.cols > DETECTION_WIDTH) { \n      // Shrink the image while keeping the same aspect ratio. \n      int scaledHeight = cvRound(img.rows / scale); \n      resize(img, smallImg, Size(DETECTION_WIDTH, scaledHeight)); \n    } \n    else { \n      // Access the input directly since it is already small. \n      smallImg = img; \n    }\n\n```", "```py\n    // Standardize the brightness & contrast, such as \n    // to improve dark images. \n    Mat equalizedImg; \n    equalizeHist(inputImg, equalizedImg);\n\n```", "```py\n    int flags = CASCADE_SCALE_IMAGE; // Search for many faces. \n    Size minFeatureSize(20, 20);     // Smallest face size. \n    float searchScaleFactor = 1.1f;  // How many sizes to search. \n    int minNeighbors = 4;            // Reliability vs many faces. \n\n// Detect objects in the small grayscale image. \nstd::vector<Rect> faces; \nfaceDetector.detectMultiScale(img, faces, searchScaleFactor,  \n                minNeighbors, flags, minFeatureSize);\n\n```", "```py\n    // Enlarge the results if the image was temporarily shrunk. \n    if (img.cols > scaledWidth) { \n      for (int i = 0; i < (int)objects.size(); i++ ) { \n        objects[i].x = cvRound(objects[i].x * scale); \n        objects[i].y = cvRound(objects[i].y * scale); \n        objects[i].width = cvRound(objects[i].width * scale); \n        objects[i].height = cvRound(objects[i].height * scale); \n      } \n    } \n    // If the object is on a border, keep it in the image. \n    for (int i = 0; i < (int)objects.size(); i++ ) { \n      if (objects[i].x < 0) \n        objects[i].x = 0; \n      if (objects[i].y < 0) \n        objects[i].y = 0; \n      if (objects[i].x + objects[i].width > img.cols) \n        objects[i].x = img.cols - objects[i].width; \n      if (objects[i].y + objects[i].height > img.rows) \n        objects[i].y = img.rows - objects[i].height; \n    }\n\n```", "```py\n    int flags = CASCADE_FIND_BIGGEST_OBJECT |  \n                CASCADE_DO_ROUGH_SEARCH;\n\n```", "```py\n    Rect faceRect;    // Stores the result of the detection, or -1\\. \n    int scaledWidth = 320;     // Shrink the image before detection. \n    detectLargestObject(cameraImg, faceDetector, faceRect, scaledWidth); \n    if (faceRect.width > 0) \n      cout << \"We detected a face!\" << endl;\n\n```", "```py\n    // Access just the face within the camera image. \n    Mat faceImg = cameraImg(faceRect);\n\n```", "```py\n    int leftX = cvRound(face.cols * EYE_SX); \n    int topY = cvRound(face.rows * EYE_SY); \n    int widthX = cvRound(face.cols * EYE_SW); \n    int heightY = cvRound(face.rows * EYE_SH); \n    int rightX = cvRound(face.cols * (1.0-EYE_SX-EYE_SW)); \n\n    Mat topLeftOfFace = faceImg(Rect(leftX, topY, widthX, heightY)); \n    Mat topRightOfFace = faceImg(Rect(rightX, topY, widthX, heightY));\n\n```", "```py\n    CascadeClassifier eyeDetector1(\"haarcascade_eye.xml\"); \n    CascadeClassifier eyeDetector2(\"haarcascade_eye_tree_eyeglasses.xml\"); \n    ... \n    Rect leftEyeRect;    // Stores the detected eye. \n    // Search the left region using the 1st eye detector. \n    detectLargestObject(topLeftOfFace, eyeDetector1, leftEyeRect, \n    topLeftOfFace.cols); \n    // If it failed, search the left region using the 2nd eye  \n    // detector. \n    if (leftEyeRect.width <= 0) \n      detectLargestObject(topLeftOfFace, eyeDetector2,  \n                leftEyeRect, topLeftOfFace.cols); \n    // Get the left eye center if one of the eye detectors worked. \n    Point leftEye = Point(-1,-1); \n    if (leftEyeRect.width <= 0) { \n      leftEye.x = leftEyeRect.x + leftEyeRect.width/2 + leftX; \n      leftEye.y = leftEyeRect.y + leftEyeRect.height/2 + topY; \n    } \n\n    // Do the same for the right-eye \n    ... \n\n    // Check if both eyes were detected. \n    if (leftEye.x >= 0 && rightEye.x >= 0) { \n      ... \n    }\n\n```", "```py\n    // Get the center between the 2 eyes. \n    Point2f eyesCenter; \n    eyesCenter.x = (leftEye.x + rightEye.x) * 0.5f; \n    eyesCenter.y = (leftEye.y + rightEye.y) * 0.5f; \n\n    // Get the angle between the 2 eyes. \n    double dy = (rightEye.y - leftEye.y); \n    double dx = (rightEye.x - leftEye.x); \n    double len = sqrt(dx*dx + dy*dy); \n\n    // Convert Radians to Degrees. \n    double angle = atan2(dy, dx) * 180.0/CV_PI; \n\n    // Hand measurements shown that the left eye center should  \n    // ideally be roughly at (0.16, 0.14) of a scaled face image. \n    const double DESIRED_LEFT_EYE_X = 0.16; \n    const double DESIRED_RIGHT_EYE_X = (1.0f - 0.16); \n\n    // Get the amount we need to scale the image to be the desired \n    // fixed size we want. \n    const int DESIRED_FACE_WIDTH = 70; \n    const int DESIRED_FACE_HEIGHT = 70; \n    double desiredLen = (DESIRED_RIGHT_EYE_X - 0.16); \n    double scale = desiredLen * DESIRED_FACE_WIDTH / len;\n\n```", "```py\n    // Get the transformation matrix for the desired angle & size. \n    Mat rot_mat = getRotationMatrix2D(eyesCenter, angle, scale); \n    // Shift the center of the eyes to be the desired center. \n    double ex = DESIRED_FACE_WIDTH * 0.5f - eyesCenter.x; \n    double ey = DESIRED_FACE_HEIGHT * DESIRED_LEFT_EYE_Y -  \n      eyesCenter.y; \n    rot_mat.at<double>(0, 2) += ex; \n    rot_mat.at<double>(1, 2) += ey; \n    // Transform the face image to the desired angle & size & \n    // position! Also clear the transformed image background to a  \n    // default grey. \n    Mat warped = Mat(DESIRED_FACE_HEIGHT, DESIRED_FACE_WIDTH, \n      CV_8U, Scalar(128)); \n    warpAffine(gray, warped, rot_mat, warped.size());\n\n```", "```py\n    int w = faceImg.cols; \n    int h = faceImg.rows; \n    Mat wholeFace; \n    equalizeHist(faceImg, wholeFace); \n    int midX = w/2; \n    Mat leftSide = faceImg(Rect(0,0, midX,h)); \n    Mat rightSide = faceImg(Rect(midX,0, w-midX,h)); \n    equalizeHist(leftSide, leftSide); \n    equalizeHist(rightSide, rightSide);\n\n```", "```py\n    for (int y=0; y<h; y++) { \n      for (int x=0; x<w; x++) { \n        int v; \n        if (x < w/4) { \n          // Left 25%: just use the left face. \n          v = leftSide.at<uchar>(y,x); \n        } \n        else if (x < w*2/4) { \n          // Mid-left 25%: blend the left face & whole face. \n          int lv = leftSide.at<uchar>(y,x); \n          int wv = wholeFace.at<uchar>(y,x); \n          // Blend more of the whole face as it moves \n          // further right along the face. \n          float f = (x - w*1/4) / (float)(w/4); \n          v = cvRound((1.0f - f) * lv + (f) * wv); \n        } \n        else if (x < w*3/4) { \n          // Mid-right 25%: blend right face & whole face. \n          int rv = rightSide.at<uchar>(y,x-midX); \n          int wv = wholeFace.at<uchar>(y,x); \n          // Blend more of the right-side face as it moves \n          // further right along the face. \n          float f = (x - w*2/4) / (float)(w/4); \n          v = cvRound((1.0f - f) * wv + (f) * rv); \n        } \n        else { \n          // Right 25%: just use the right face. \n          v = rightSide.at<uchar>(y,x-midX); \n        } \n        faceImg.at<uchar>(y,x) = v; \n      } // end x loop \n    } //end y loop\n\n```", "```py\n    Mat filtered = Mat(warped.size(), CV_8U); \n    bilateralFilter(warped, filtered, 0, 20.0, 2.0);\n\n```", "```py\n    // Draw a black-filled ellipse in the middle of the image. \n    // First we initialize the mask image to white (255). \n    Mat mask = Mat(warped.size(), CV_8UC1, Scalar(255)); \n    double dw = DESIRED_FACE_WIDTH; \n    double dh = DESIRED_FACE_HEIGHT; \n    Point faceCenter = Point( cvRound(dw * 0.5), \n      cvRound(dh * 0.4) ); \n    Size size = Size( cvRound(dw * 0.5), cvRound(dh * 0.8) ); \n    ellipse(mask, faceCenter, size, 0, 0, 360, Scalar(0),  \n      CV_FILLED); \n\n    // Apply the elliptical mask on the face, to remove corners. \n    // Sets corners to gray, without touching the inner face. \n    filtered.setTo(Scalar(128), mask);\n\n```", "```py\n    // Check how long since the previous face was added. \n    double current_time = (double)getTickCount(); \n    double timeDiff_seconds = (current_time - \n      old_time) / getTickFrequency();\n\n```", "```py\n    double getSimilarity(const Mat A, const Mat B) { \n      // Calculate the L2 relative error between the 2 images. \n      double errorL2 = norm(A, B, CV_L2); \n      // Scale the value since L2 is summed across all pixels. \n      double similarity = errorL2 / (double)(A.rows * A.cols); \n      return similarity; \n    } \n\n    ... \n\n    // Check if this face looks different from the previous face. \n    double imageDiff = MAX_DBL; \n    if (old_prepreprocessedFaceprepreprocessedFace.data) { \n      imageDiff = getSimilarity(preprocessedFace, \n        old_prepreprocessedFace); \n    }\n\n```", "```py\n    // Only process the face if it's noticeably different from the \n    // previous frame and there has been a noticeable time gap. \n    if ((imageDiff > 0.3) && (timeDiff_seconds > 1.0)) { \n      // Also add the mirror image to the training set. \n      Mat mirroredFace; \n      flip(preprocessedFace, mirroredFace, 1); \n\n      // Add the face & mirrored face to the detected face lists. \n      preprocessedFaces.push_back(preprocessedFace); \n      preprocessedFaces.push_back(mirroredFace); \n      faceLabels.push_back(m_selectedPerson); \n      faceLabels.push_back(m_selectedPerson); \n\n      // Keep a copy of the processed face, \n      // to compare on next iteration. \n      old_prepreprocessedFace = preprocessedFace; \n      old_time = current_time; \n    }\n\n```", "```py\n    // Get access to the face region-of-interest. \n    Mat displayedFaceRegion = displayedFrame(faceRect); \n    // Add some brightness to each pixel of the face region. \n    displayedFaceRegion += CV_RGB(90,90,90);\n\n```", "```py\n    vector<string> algorithms; \n    Algorithm::getList(algorithms); \n    cout << \"Algorithms: \" << algorithms.size() << endl; \n    for (int i=0; i<algorithms.size(); i++) { \n      cout << algorithms[i] << endl; \n    }\n\n```", "```py\n    // Load the \"contrib\" module is dynamically at runtime. \n    bool haveContribModule = initModule_contrib(); \n    if (!haveContribModule) { \n      cerr << \"ERROR: The 'contrib' module is needed for \"; \n      cerr << \"FaceRecognizer but hasn't been loaded to OpenCV!\"; \n      cerr << endl; \n      exit(1); \n    }\n\n```", "```py\n    string facerecAlgorithm = \"FaceRecognizer.Fisherfaces\"; \n    Ptr<FaceRecognizer> model; \n    // Use OpenCV's new FaceRecognizer in the \"contrib\" module: \n    model = Algorithm::create<FaceRecognizer>(facerecAlgorithm); \n    if (model.empty()) { \n      cerr << \"ERROR: The FaceRecognizer [\" << facerecAlgorithm; \n      cerr << \"] is not available in your version of OpenCV. \"; \n      cerr << \"Please update to OpenCV v2.4.1 or newer.\" << endl; \n      exit(1); \n    }\n\n```", "```py\n    // Do the actual training from the collected faces. \n    model->train(preprocessedFaces, faceLabels);\n\n```", "```py\n    // Convert the matrix row or column (float matrix) to a \n    // rectangular 8-bit image that can be displayed or saved. \n    // Scales the values to be between 0 to 255\\. \n    Mat getImageFrom1DFloatMat(const Mat matrixRow, int height) \n    { \n      // Make a rectangular shaped image instead of a single row. \n      Mat rectangularMat = matrixRow.reshape(1, height); \n      // Scale the values to be between 0 to 255 and store them  \n      // as a regular 8-bit uchar image. \n      Mat dst; \n      normalize(rectangularMat, dst, 0, 255, NORM_MINMAX,  \n        CV_8UC1); \n      return dst; \n    }\n\n```", "```py\n    Mat img = ...; \n    printMatInfo(img, \"My Image\");\n\n```", "```py\nMy Image: 640w480h 3ch 8bpp, range[79,253][20,58][18,87]\n\n```", "```py\n    Mat averageFace = model->get<Mat>(\"mean\"); \n    printMatInfo(averageFace, \"averageFace (row)\"); \n    // Convert a 1D float row matrix to a regular 8-bit image. \n    averageFace = getImageFrom1DFloatMat(averageFace, faceHeight); \n    printMatInfo(averageFace, \"averageFace\"); \n    imshow(\"averageFace\", averageFace);\n\n```", "```py\n averageFace (row): 4900w1h 1ch 64bpp, range[5.21,251.47]\n averageFace: 70w70h 1ch 8bpp, range[0,255]\n\n```", "```py\n    Mat eigenvalues = model->get<Mat>(\"eigenvalues\"); \n    printMat(eigenvalues, \"eigenvalues\");\n\n```", "```py\n    eigenvalues: 1w18h 1ch 64bpp, range[4.52e+04,2.02836e+06] \n    2.03e+06  \n    1.09e+06 \n    5.23e+05 \n    4.04e+05 \n    2.66e+05 \n    2.31e+05 \n    1.85e+05 \n    1.23e+05 \n    9.18e+04 \n    7.61e+04  \n    6.91e+04 \n    4.52e+04\n\n```", "```py\n    eigenvalues: 2w1h 1ch 64bpp, range[152.4,316.6] \n    317, 152\n\n```", "```py\n    // Get the eigenvectors \n    Mat eigenvectors = model->get<Mat>(\"eigenvectors\"); \n    printMatInfo(eigenvectors, \"eigenvectors\"); \n\n    // Show the best 20 eigenfaces \n    for (int i = 0; i < min(20, eigenvectors.cols); i++) { \n      // Create a continuous column vector from eigenvector #i. \n      Mat eigenvector = eigenvectors.col(i).clone(); \n\n      Mat eigenface = getImageFrom1DFloatMat(eigenvector, \n        faceHeight); \n      imshow(format(\"Eigenface%d\", i), eigenface); \n    }\n\n```", "```py\n    int identity = model->predict(preprocessedFace);\n\n```", "```py\n    // Get some required data from the FaceRecognizer model. \n    Mat eigenvectors = model->get<Mat>(\"eigenvectors\"); \n    Mat averageFaceRow = model->get<Mat>(\"mean\"); \n\n    // Project the input image onto the eigenspace. \n    Mat projection = subspaceProject(eigenvectors, averageFaceRow, \n      preprocessedFace.reshape(1,1)); \n\n    // Generate the reconstructed face back from the eigenspace. \n    Mat reconstructionRow = subspaceReconstruct(eigenvectors, \n      averageFaceRow, projection); \n\n    // Make it a rectangular shaped image instead of a single row. \n    Mat reconstructionMat = reconstructionRow.reshape(1,  \n      faceHeight); \n\n    // Convert the floating-point pixels to regular 8-bit uchar. \n    Mat reconstructedFace = Mat(reconstructionMat.size(), CV_8U); \n    reconstructionMat.convertTo(reconstructedFace, CV_8U, 1, 0);\n\n```", "```py\n    similarity = getSimilarity(preprocessedFace, reconstructedFace); \n    if (similarity > UNKNOWN_PERSON_THRESHOLD) { \n      identity = -1;    // Unknown person. \n    }\n\n```", "```py\nmodel->save(\"trainedModel.yml\");\n\n```", "```py\n    string facerecAlgorithm = \"FaceRecognizer.Fisherfaces\"; \n    model = Algorithm::create<FaceRecognizer>(facerecAlgorithm); \n    Mat labels; \n    try { \n      model->load(\"trainedModel.yml\"); \n      labels = model->get<Mat>(\"labels\"); \n    } catch (cv::Exception &e) {} \n    if (labels.rows <= 0) { \n      cerr << \"ERROR: Couldn't load trained data from \" \n              \"[trainedModel.yml]!\" << endl; \n      exit(1); \n    }\n\n```", "```py\n    // Draw text into an image. Defaults to top-left-justified  \n    // text, so give negative x coords for right-justified text, \n    // and/or negative y coords for bottom-justified text. \n    // Returns the bounding rect around the drawn text. \n    Rect drawString(Mat img, string text, Point coord, Scalar  \n      color, float fontScale = 0.6f, int thickness = 1, \n      int fontFace = FONT_HERSHEY_COMPLEX);\n\n```", "```py\n    string msg = \"Click [Add Person] when ready to collect faces.\"; \n    // Draw it as black shadow & again as white text. \n    float txtSize = 0.4; \n    int BORDER = 10; \n    drawString (displayedFrame, msg, Point(BORDER, -BORDER-2), \n      CV_RGB(0,0,0), txtSize); \n    Rect rcHelp = drawString(displayedFrame, msg, Point(BORDER+1, \n      -BORDER-1), CV_RGB(255,255,255), txtSize);\n\n```", "```py\n    // Draw a GUI button into the image, using drawString(). \n    // Can give a minWidth to have several buttons of same width. \n    // Returns the bounding rect around the drawn button. \n    Rect drawButton(Mat img, string text, Point coord, \n      int minWidth = 0) \n    { \n      const int B = 10; \n      Point textCoord = Point(coord.x + B, coord.y + B); \n      // Get the bounding box around the text. \n      Rect rcText = drawString(img, text, textCoord,  \n        CV_RGB(0,0,0)); \n      // Draw a filled rectangle around the text. \n      Rect rcButton = Rect(rcText.x - B, rcText.y - B, \n        rcText.width + 2*B, rcText.height + 2*B); \n      // Set a minimum button width. \n      if (rcButton.width < minWidth) \n        rcButton.width = minWidth; \n      // Make a semi-transparent white rectangle. \n      Mat matButton = img(rcButton); \n      matButton += CV_RGB(90, 90, 90); \n      // Draw a non-transparent white border. \n      rectangle(img, rcButton, CV_RGB(200,200,200), 1, CV_AA); \n\n      // Draw the actual text that will be displayed. \n      drawString(img, text, textCoord, CV_RGB(10,55,20)); \n\n      return rcButton; \n    }\n\n```", "```py\n    // Create a GUI window for display on the screen. \n    namedWindow(windowName); \n\n    // Call \"onMouse()\" when the user clicks in the window. \n    setMouseCallback(windowName, onMouse, 0); \n\n    // Set the camera resolution. Only works for some systems. \n    videoCapture.set(CV_CAP_PROP_FRAME_WIDTH, 640); \n    videoCapture.set(CV_CAP_PROP_FRAME_HEIGHT, 480); \n\n    // We're already initialized, so let's start in Detection mode. \n    m_mode = MODE_DETECTION;\n\n```", "```py\n    bool gotFaceAndEyes = false; \n    if (preprocessedFace.data) \n      gotFaceAndEyes = true; \n\n    if (faceRect.width > 0) { \n      // Draw an anti-aliased rectangle around the detected face. \n      rectangle(displayedFrame, faceRect, CV_RGB(255, 255, 0), 2, \n        CV_AA); \n\n      // Draw light-blue anti-aliased circles for the 2 eyes. \n      Scalar eyeColor = CV_RGB(0,255,255); \n      if (leftEye.x >= 0) {   // Check if the eye was detected \n        circle(displayedFrame, Point(faceRect.x + leftEye.x, \n          faceRect.y + leftEye.y), 6, eyeColor, 1, CV_AA); \n      } \n      if (rightEye.x >= 0) {   // Check if the eye was detected \n        circle(displayedFrame, Point(faceRect.x + rightEye.x,  \n          faceRect.y + rightEye.y), 6, eyeColor, 1, CV_AA); \n      } \n    }\n\n```", "```py\n    int cx = (displayedFrame.cols - faceWidth) / 2; \n    if (preprocessedFace.data) { \n      // Get a BGR version of the face, since the output is BGR. \n      Mat srcBGR = Mat(preprocessedFace.size(), CV_8UC3); \n      cvtColor(preprocessedFace, srcBGR, CV_GRAY2BGR); \n\n      // Get the destination ROI. \n      Rect dstRC = Rect(cx, BORDER, faceWidth, faceHeight); \n      Mat dstROI = displayedFrame(dstRC); \n\n      // Copy the pixels from src to dst. \n      srcBGR.copyTo(dstROI); \n    } \n    // Draw an anti-aliased border around the face. \n    rectangle(displayedFrame, Rect(cx-1, BORDER-1, faceWidth+2, \n      faceHeight+2), CV_RGB(200,200,200), 1, CV_AA);\n\n```", "```py\n    // Keep a reference to the latest face of each person. \n    m_latestFaces[m_selectedPerson] = preprocessedFaces.size() - 2;\n\n```", "```py\n    m_gui_faces_left = displayedFrame.cols - BORDER - faceWidth; \n    m_gui_faces_top = BORDER; \n    for (int i=0; i<m_numPersons; i++) { \n      int index = m_latestFaces[i]; \n      if (index >= 0 && index < (int)preprocessedFaces.size()) { \n        Mat srcGray = preprocessedFaces[index]; \n        if (srcGray.data) { \n          // Get a BGR face, since the output is BGR. \n          Mat srcBGR = Mat(srcGray.size(), CV_8UC3); \n          cvtColor(srcGray, srcBGR, CV_GRAY2BGR); \n\n          // Get the destination ROI \n          int y = min(m_gui_faces_top + i * faceHeight, \n          displayedFrame.rows - faceHeight); \n          Rect dstRC = Rect(m_gui_faces_left, y, faceWidth, \n          faceHeight); \n          Mat dstROI = displayedFrame(dstRC); \n\n          // Copy the pixels from src to dst. \n          srcBGR.copyTo(dstROI); \n        } \n      } \n    }\n\n```", "```py\n    if (m_mode == MODE_COLLECT_FACES) { \n      if (m_selectedPerson >= 0 && \n        m_selectedPerson < m_numPersons) { \n        int y = min(m_gui_faces_top + m_selectedPerson *  \n        faceHeight, displayedFrame.rows - faceHeight); \n        Rect rc = Rect(m_gui_faces_left, y, faceWidth, faceHeight); \n        rectangle(displayedFrame, rc, CV_RGB(255,0,0), 3, CV_AA); \n      } \n    }\n\n```", "```py\n    // Check if there is enough data to train from. \n    bool haveEnoughData = true; \n    if (!strcmp(facerecAlgorithm, \"FaceRecognizer.Fisherfaces\")) { \n      if ((m_numPersons < 2) || \n      (m_numPersons == 2 && m_latestFaces[1] < 0) ) { \n        cout << \"Fisherfaces needs >= 2 people!\" << endl; \n        haveEnoughData = false; \n      } \n    } \n    if (m_numPersons < 1 || preprocessedFaces.size() <= 0 || \n      preprocessedFaces.size() != faceLabels.size()) { \n      cout << \"Need data before it can be learnt!\" << endl; \n      haveEnoughData = false; \n    } \n\n    if (haveEnoughData) { \n      // Train collected faces using Eigenfaces or Fisherfaces. \n      model = learnCollectedFaces(preprocessedFaces, faceLabels, \n              facerecAlgorithm); \n\n      // Now that training is over, we can start recognizing! \n      m_mode = MODE_RECOGNITION; \n    } \n    else { \n      // Not enough training data, go back to Collection mode! \n      m_mode = MODE_COLLECT_FACES; \n    }\n\n```", "```py\n    int cx = (displayedFrame.cols - faceWidth) / 2; \n    Point ptBottomRight = Point(cx - 5, BORDER + faceHeight); \n    Point ptTopLeft = Point(cx - 15, BORDER); \n\n    // Draw a gray line showing the threshold for \"unknown\" people. \n    Point ptThreshold = Point(ptTopLeft.x, ptBottomRight.y - \n      (1.0 - UNKNOWN_PERSON_THRESHOLD) * faceHeight); \n    rectangle(displayedFrame, ptThreshold, Point(ptBottomRight.x, \n    ptThreshold.y), CV_RGB(200,200,200), 1, CV_AA); \n\n    // Crop the confidence rating between 0 to 1 to fit in the bar. \n    double confidenceRatio = 1.0 - min(max(similarity, 0.0), 1.0); \n    Point ptConfidence = Point(ptTopLeft.x, ptBottomRight.y - \n      confidenceRatio * faceHeight); \n\n    // Show the light-blue confidence bar. \n    rectangle(displayedFrame, ptConfidence, ptBottomRight, \n      CV_RGB(0,255,255), CV_FILLED, CV_AA); \n\n    // Show the gray border of the bar. \n    rectangle(displayedFrame, ptTopLeft, ptBottomRight, \n      CV_RGB(200,200,200), 1, CV_AA);\n\n```", "```py\n    if (identity >= 0 && identity < 1000) { \n      int y = min(m_gui_faces_top + identity * faceHeight, \n        displayedFrame.rows - faceHeight); \n      Rect rc = Rect(m_gui_faces_left, y, faceWidth, faceHeight); \n      rectangle(displayedFrame, rc, CV_RGB(0,255,0), 3, CV_AA); \n    }\n\n```", "```py\n    void onMouse(int event, int x, int y, int, void*) \n    { \n      if (event != CV_EVENT_LBUTTONDOWN) \n        return; \n\n      Point pt = Point(x,y); \n\n      ... (handle mouse clicks) \n      ... \n    }\n\n```", "```py\n    if (pt.inside(m_btnAddPerson)) { \n      // Ensure there isn't a person without collected faces. \n      if ((m_numPersons==0) || \n         (m_latestFaces[m_numPersons-1] >= 0)) { \n          // Add a new person. \n          m_numPersons++; \n          m_latestFaces.push_back(-1); \n      } \n      m_selectedPerson = m_numPersons - 1; \n      m_mode = MODE_COLLECT_FACES; \n    }\n\n```", "```py\n    else if (pt.inside(m_btnDebug)) { \n      m_debug = !m_debug; \n    }\n\n```", "```py\n    else { \n      // Check if the user clicked on a face from the list. \n      int clickedPerson = -1; \n      for (int i=0; i<m_numPersons; i++) { \n        if (m_gui_faces_top >= 0) { \n          Rect rcFace = Rect(m_gui_faces_left,  \n          m_gui_faces_top + i * faceHeight, faceWidth, faceHeight); \n          if (pt.inside(rcFace)) { \n            clickedPerson = i; \n            break; \n          } \n        } \n      } \n      // Change the selected person, if the user clicked a face. \n      if (clickedPerson >= 0) { \n        // Change the current person & collect more photos. \n        m_selectedPerson = clickedPerson; \n        m_mode = MODE_COLLECT_FACES; \n      } \n      // Otherwise they clicked in the center. \n      else { \n        // Change to training mode if it was collecting faces. \n        if (m_mode == MODE_COLLECT_FACES) { \n            m_mode = MODE_TRAINING; \n        } \n      } \n    }\n\n```"]