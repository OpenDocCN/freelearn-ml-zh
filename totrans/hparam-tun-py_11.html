<html><head></head><body>
		<div id="_idContainer316">
			<h1 id="_idParaDest-77"><em class="italic"><a id="_idTextAnchor082"/>Chapter 9</em><span class="superscript">: Hyperparameter Tuning via Optuna</span></h1>
			<p><strong class="bold">Optuna</strong> is a Python package that provides various implementations of hyperparameter tuning methods, including but not limited to Grid Search, Random Search, and <strong class="bold">Tree-Structured Parzen Estimators</strong> (<strong class="bold">TPE</strong>). This package also enables us to create our own hyperparameter tuning method class and integrate it with other popular hyperparameter tuning packages, such as <strong class="source-inline">scikit-optimize</strong>.</p>
			<p>In this chapter, you’ll be introduced to the <strong class="source-inline">Optuna</strong> package, starting with its numerous features, how to utilize it to perform hyperparameter tuning, and all of the other important things you need to know about <strong class="source-inline">Optuna</strong>. We’ll not only learn how to utilize <strong class="source-inline">Optuna</strong> to perform hyperparameter tuning with their default configurations but also discuss the available configurations along with their usage. Moreover, we’ll also discuss how the implementation of the hyperparameter tuning methods is related to the theory that we have learned in previous chapters, since there may be some minor differences or adjustments made in the implementation.</p>
			<p>By the end of this chapter, you will be able to understand all of the important things you need to know about <strong class="source-inline">Optuna</strong> and implement various hyperparameter tuning methods available in this package. You’ll also be able to understand each of the important parameters of the classes and how they are related to the theory that we have learned in previous chapters. Finally, equipped with the knowledge from previous chapters, you will also be able to understand what’s happening if there are errors or unexpected results and understand how to set up the method configuration to match your specific problem.</p>
			<p>The following are the main topics that will be discussed in this chapter:</p>
			<ul>
				<li>Introducing Optuna</li>
				<li>Implementing TPE</li>
				<li>Implementing Random Search</li>
				<li>Implementing Grid Search</li>
				<li>Implementing Simulated Annealing</li>
				<li>Implementing Successive Halving</li>
				<li>Implementing Hyperband</li>
			</ul>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor083"/>Technical requirements</h1>
			<p>We will learn how to implement various hyperparameter tuning methods with <strong class="source-inline">Optuna</strong>. To ensure that you are able to reproduce the code examples in this chapter, you will require the following:</p>
			<ul>
				<li>Python 3 (version 3.7 or above)</li>
				<li>Installed <strong class="source-inline">pandas</strong> package (version 1.3.4 or above)</li>
				<li>Installed <strong class="source-inline">NumPy</strong> package (version 1.21.2 or above)</li>
				<li>Installed <strong class="source-inline">Matplotlib</strong> package (version 3.5.0 or above)</li>
				<li>Installed <strong class="source-inline">scikit-learn</strong> package (version 1.0.1 or above)</li>
				<li>Installed <strong class="source-inline">Tensorflow</strong> package (version 2.4.1 or above)</li>
				<li>Installed <strong class="source-inline">Optuna</strong> package (version 2.10.0 or above)</li>
			</ul>
			<p>All of the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python</a>.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor084"/>Introducing Optuna</h1>
			<p><strong class="source-inline">Optuna</strong> is a hyperparameter <a id="_idIndexMarker409"/>tuning package in Python that provides several hyperparameter tuning methods implementation, such as Grid Search, Random Search, Tree-structured Parzen Estimators (TPE), and many more. Unlike <strong class="source-inline">Hyperopt</strong>, which assumes we are always working with a minimization problem (see <a href="B18753_08_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 8</em></a><em class="italic">, Hyperparameter Tuning via Hyperopt</em>), we can tell <strong class="source-inline">Optuna</strong> the type of optimization problem we are working on: minimization or maximization. </p>
			<p><strong class="source-inline">Optuna</strong> has two main <a id="_idIndexMarker410"/>classes, namely <strong class="bold">samplers</strong> and <strong class="bold">pruners</strong>. Samplers are responsible for performing the hyperparameter tuning optimization, whereas pruners are <a id="_idIndexMarker411"/>responsible for judging whether we should prune the trials based on the reported values. In other words, pruners act like <em class="italic">early stopping methods</em> where we will stop a hyperparameter tuning iteration whenever it seems that there’s no additional benefit to continuing the process.</p>
			<p>The built-in implementation for samplers includes several hyperparameter tuning methods that we have learned in <em class="italic">Chapters 3 - 4</em>, namely Grid Search, Random Search, and TPE, and also other methods that are outside of the scope of this book, such as CMA-ES, NSGA-II, and many more. We can also define our own custom samplers, such as the Simulated Annealing (SA), which will be discussed in the upcoming section. Furthermore, <strong class="source-inline">Optuna</strong> also allows us to integrate samplers from another package, such as from the <strong class="source-inline">scikit-optimize</strong> (<strong class="source-inline">skopt</strong>) package where we can utilize many Bayesian optimization-based methods from there. </p>
			<p class="callout-heading">Integrations in Optuna</p>
			<p class="callout">Besides <strong class="source-inline">skopt</strong>, there are also many other integrations provided by <strong class="source-inline">Optuna,</strong> including but not limited, to <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">Keras</strong>, <strong class="source-inline">PyTorch</strong>, <strong class="source-inline">XGBoost</strong>, <strong class="source-inline">LightGBM</strong>, <strong class="source-inline">FastAI</strong>, <strong class="source-inline">MLflow</strong>, and many more. For more information about the available integrations, please <a id="_idIndexMarker412"/>see the official documentation (<a href="https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html">https://optuna.readthedocs.io/en/v2.10.0/reference/integration.html</a>).</p>
			<p>As for pruners, <strong class="source-inline">Optuna</strong> provides both <a id="_idIndexMarker413"/>statistics-based and multi-fidelity optimization (MFO)-based methods. There are <strong class="source-inline">MedianPruner</strong>, <strong class="source-inline">PercentilePruner</strong>, and <strong class="source-inline">ThresholdPruner</strong> for the statistics-based group. <strong class="source-inline">MedianPruner</strong> will prune the trials whenever the current trial’s best intermediate result is worse compared to the median of the result of the previous trial. <strong class="source-inline">PercentilePruner</strong> will perform pruning when the current best intermediate value is part of the bottom percentile from previous trials. <strong class="source-inline">ThresholdPruner</strong> will simply perform pruning whenever the predefined threshold is met. The MFO-based pruners implemented in <strong class="source-inline">Optuna</strong> are <strong class="source-inline">SuccessiveHalvingPruner</strong> and <strong class="source-inline">HyperbandPruner</strong>. Both of them <em class="italic">define the resource as the number of training steps or epochs</em>, not as the number of samples such as in the implementations of <strong class="source-inline">scikit-learn</strong>. We will learn how to utilize these MFO-based pruners in the upcoming sections.</p>
			<p>To perform hyperparameter tuning with <strong class="source-inline">Optuna</strong>, we can simply perform the following simple <a id="_idIndexMarker414"/>steps (more detailed steps, including the code implementation, will be given through various examples in the upcoming sections):</p>
			<ol>
				<li>Define the <strong class="source-inline">objective</strong> function along with the hyperparameter space.</li>
				<li>Initiate a <strong class="source-inline">study</strong> object via the <strong class="source-inline">create_study()</strong> function.</li>
				<li>Perform hyperparameter tuning by calling the <strong class="source-inline">optimize()</strong> method on the <strong class="source-inline">study</strong> object.</li>
				<li>Train the model on full training data using the best set of hyperparameters found.</li>
				<li>Test the final trained model on the test data.</li>
			</ol>
			<p>In <strong class="source-inline">Optuna</strong>, we can directly define the hyperparameter space within the <strong class="source-inline">objective</strong> function itself. There’s <a id="_idIndexMarker415"/>no need to define another dedicated separate object just to store the hyperparameter space. This means that implementing conditional hyperparameters in <strong class="source-inline">Optuna</strong> becomes very easy since we just need to put them within the corresponding <strong class="source-inline">if-else</strong> blocks in the <strong class="source-inline">objective</strong> function. <strong class="source-inline">Optuna</strong> also provides very handy hyperparameter sampling distribution methods including <strong class="source-inline">suggest_categorical</strong>, <strong class="source-inline">suggest_discrete_uniform</strong>, <strong class="source-inline">suggest_int</strong>, and <strong class="source-inline">suggest_float</strong>. </p>
			<p>The <strong class="source-inline">suggest_categorical</strong> method will suggest value from a categorical type of hyperparameters, which works similarly with the <strong class="source-inline">random.choice()</strong> method. The <strong class="source-inline">suggest_discrete_uniform</strong> can be utilized for a discrete type of hyperparameters, which works very similar to the <strong class="source-inline">hp.quniform</strong> in Hyperopt (see <a href="B18753_08_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 8</em></a><em class="italic">, Hyperparameter Tuning via Hyperopt</em>) by sampling uniformly from the range of <strong class="source-inline">[low, high]</strong> with a <strong class="source-inline">q</strong> step of discretization. The <strong class="source-inline">suggest_int</strong> method works similarly to the <strong class="source-inline">random.randint()</strong> method. Finally, the <strong class="source-inline">suggest_float</strong> method. This method works for a floating type of hyperparameters and is actually a wrapper of two other sampling distribution methods, namely the <strong class="source-inline">suggest_uniform</strong> and <strong class="source-inline">suggest_loguniform</strong>. To utilize <strong class="source-inline">suggest_loguniform</strong>, simply set the <strong class="source-inline">log</strong> parameter in <strong class="source-inline">suggest_float</strong> as <strong class="source-inline">True</strong>.</p>
			<p>To have a better understanding of how we can define the hyperparameter space within the <strong class="source-inline">objective</strong> function, the following code shows an example of how to define an <strong class="source-inline">objective</strong> function <a id="_idIndexMarker416"/>using <strong class="bold">TFKeras</strong>. Note that in this example, we write several functions to be called in the <strong class="source-inline">objective</strong> function, to ensure readability and to enable us to write the code in a modular fashion. However, you can also put all of the code within one single <strong class="source-inline">objective</strong> function directly. The data and preprocessing steps used <a id="_idIndexMarker417"/>in this example are the same as in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a><em class="italic">, Hyperparameter Tuning via Scikit</em>. However, in this example, we are using a <strong class="bold">neural network</strong> model instead of a random forest as follows: </p>
			<ol>
				<li value="1">Create a function to define the model architecture. Here, we create a binary classifier model where the number of hidden layers, number of units, dropout rate, and the <strong class="source-inline">activation</strong> function for each layer are part of the hyperparameter space, as follows: <p class="source-code">import optuna</p><p class="source-code">from tensorflow.keras.models import Sequential </p><p class="source-code">from tensorflow.keras.layers import Dense, Dropout</p><p class="source-code">def <strong class="bold">create_model</strong>(trial: optuna.trial.Trial, input_size: int):</p><p class="source-code">model = Sequential() </p><p class="source-code">model.add(Dense(input_size,input_shape=(input_size,),activation='relu'))</p><p class="source-code"> num_layers = <strong class="bold">trial.suggest_int</strong>('num_layers',low=0,high=3) </p><p class="source-code">for layer_i in range(num_layers): </p><p class="source-code">n_units = <strong class="bold">trial.suggest_int</strong>(f'n_units_layer_{layer_i}',low=10,high=100,step=5) </p><p class="source-code"> dropout_rate = <strong class="bold">trial.suggest_float</strong>(f'dropout_rate_layer_{layer_i}',low=0,high=0.5) </p><p class="source-code">actv_func = <strong class="bold">trial.suggest_categorical</strong>(f'actv_func _layer_{layer_i}',['relu','tanh','elu']) </p><p class="source-code">model.add(Dropout(dropout_rate)) </p><p class="source-code"> model.add(Dense(n_units,activation=actv_func)) </p><p class="source-code">model.add(Dense(1,activation='sigmoid'))</p><p class="source-code">return model</p></li>
				<li>Create a <a id="_idIndexMarker418"/>function to define the model’s optimizer. Notice that we define conditional hyperparameters in this function where we have a different set of hyperparameters for a different chosen optimizer as follows: <p class="source-code">import tensorflow as tf</p><p class="source-code">def <strong class="bold">create_optimizer</strong>(trial: optuna.trial.Trial):</p><p class="source-code">opt_kwargs = {}</p><p class="source-code">opt_selected = <strong class="bold">trial.suggest_categorical</strong>('optimizer', ['Adam','SGD'])</p><p class="source-code"><strong class="bold">if opt_selected == 'SGD':</strong></p><p class="source-code">opt_kwargs['lr'] = <strong class="bold">trial.suggest_float</strong>('sgd_lr',1e-5,1e-1,log=True)</p><p class="source-code">opt_kwargs['momentum'] = <strong class="bold">trial.suggest_float</strong>('sgd_momentum',1e-5,1e-1,log=True)</p><p class="source-code"><strong class="bold">else: #'Adam'</strong></p><p class="source-code">opt_kwargs['lr'] = <strong class="bold">trial.suggest_float</strong>('adam_lr',1e-5,1e-1,log=True)</p><p class="source-code">optimizer = getattr(tf.optimizers,opt_selected)(**opt_kwargs)</p><p class="source-code">return optimizer</p></li>
				<li>Create the <strong class="source-inline">train</strong> and <strong class="source-inline">validation</strong> functions. Note that the preprocessing code is not shown here, but you can see it in the GitHub repo mentioned in the <em class="italic">Technical requirements</em> section for the full code. As the case with the examples in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>, we are also using F1-score as the evaluation metric of the <a id="_idIndexMarker419"/>model as follows:<p class="source-code">def <strong class="bold">train</strong>(trial, df_train: pd.DataFrame, df_val: pd.DataFrame = None):</p><p class="source-code">    X_train,y_train = df_train.drop(columns=['y']), df_train['y']</p><p class="source-code">    if df_val is not None:</p><p class="source-code">        X_val,y_val = df_val.drop(columns=['y']), df_val['y'] </p><p class="source-code">   <strong class="bold"> #Apply pre-processing here... </strong></p><p class="source-code">    <strong class="bold">#...</strong></p><p class="source-code">    #Build model &amp; optimizer</p><p class="source-code">    model = <strong class="bold">create_model</strong>(trial,X_train.shape[1])</p><p class="source-code">    optimizer = <strong class="bold">create_optimizer</strong>(trial)   </p><p class="source-code">    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[f1_m])</p><p class="source-code">    history = model.fit(X_train,y_train,</p><p class="source-code">                   epochs=<strong class="bold">trial.suggest_int</strong>('epoch',15,50),</p><p class="source-code">              batch_size=64,</p><p class="source-code">              validation_data=(X_val,y_val) if df_val is not None else None)</p><p class="source-code">    if df_val is not None:</p><p class="source-code">        return np.mean(history.history['val_f1_m'])</p><p class="source-code">    else:</p><p class="source-code">        return model</p></li>
				<li>Create the <strong class="source-inline">objective</strong> function. Here, we split the original training data into training data for hyperparameter tuning, <strong class="source-inline">df_train_hp</strong>, and validation data,<strong class="source-inline"> df_val</strong>. We won’t follow the k-fold cross-validation evaluation method since it will take too much time for the neural network model to go through several folds of evaluation <a id="_idIndexMarker420"/>within each tuning trial (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">,Evaluating Machine Learning Models</em>).<p class="source-code">from sklearn.model_selection import <strong class="bold">train_test_split</strong></p><p class="source-code">def <strong class="bold">objective</strong>(trial: optuna.trial.Trial, df_train: pd.DataFrame):</p><p class="source-code">#Split into Train and Validation data</p><p class="source-code">      <strong class="bold">df_train_hp</strong>, <strong class="bold">df_val</strong> = <strong class="bold">train_test_split</strong>(df_train, test_size=0.1, random_state=0)</p><p class="source-code">      #Train and Validate Model</p><p class="source-code">      val_f1_score = <strong class="bold">train</strong>(trial, df_train_hp, df_val)  </p><p class="source-code">      return val_f1_score</p></li>
			</ol>
			<p>To perform <a id="_idIndexMarker421"/>hyperparameter tuning in <strong class="source-inline">Optuna</strong>, we need to initiate a <strong class="source-inline">study</strong> object via the <strong class="source-inline">create_study()</strong> function. The <strong class="source-inline">study</strong> object provides interfaces to run a new <strong class="source-inline">Trial</strong> object and access the trials’ history. The <strong class="source-inline">Trial</strong> object is simply an object that involves the process of evaluating an <strong class="source-inline">objective</strong> function. This object will be passed to the <strong class="source-inline">objective</strong> function and it is responsible for managing the trial’s state, providing interfaces upon receiving the parameter suggestion just as we saw earlier in the <strong class="source-inline">objective</strong> function. The following code shows how to utilize the <strong class="source-inline">create_study()</strong> function to initiate a <strong class="source-inline">study</strong> object:</p>
			<pre class="source-code">study = optuna.create_study(<strong class="bold">direction</strong>='maximize')</pre>
			<p>There are several important parameters in the <strong class="source-inline">create_study()</strong> function. The <strong class="source-inline">direction</strong> parameter allows us to tell <strong class="source-inline">Optuna</strong> what kind of optimization problem we are working on. There are two valid values for this parameter, namely <em class="italic">‘maximize’</em> and <em class="italic">‘minimize’</em>. By setting the <strong class="source-inline">direction</strong> parameter equal to <em class="italic">‘maximize’</em>, it means that we tell <strong class="source-inline">Optuna</strong> that we are currently working on a maximization problem. <strong class="source-inline">Optuna</strong> sets this parameter to <em class="italic">‘minimize’</em> by default. The <strong class="source-inline">sampler</strong> parameter refers to the hyperparameter tuning algorithm that we want to use. By default, <strong class="source-inline">Optuna</strong> will use TPE as the sampler. The <strong class="source-inline">pruner</strong> parameter refers to the pruning algorithm that we want to use, where <strong class="source-inline">MedianPruner()</strong> is used by default. </p>
			<p class="callout-heading">Pruning in Optuna</p>
			<p class="callout">Although <strong class="source-inline">MedianPruner()</strong> is chosen by default, the pruning process will not be performed unless we explicitly tell <strong class="source-inline">Optuna</strong> to do so within the <strong class="source-inline">objective</strong> function. This example shows how to perform a simple pruning procedure with the default pruner in <strong class="source-inline">Optuna</strong> at the following link: <a href="https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py">https://github.com/optuna/optuna-examples/blob/main/simple_pruning.py</a>.</p>
			<p>Besides the <a id="_idIndexMarker422"/>three preceding parameters, there are also other parameters in the <strong class="source-inline">create_study()</strong> function, namely <strong class="source-inline">storage</strong>, <strong class="source-inline">study_name</strong>, and <strong class="source-inline">load_if_exists</strong>. The <strong class="source-inline">storage</strong> parameter expects a database URL input, which will be <a id="_idIndexMarker423"/>handled with <strong class="bold">SQLAlchemy</strong> internally by <strong class="source-inline">Optuna</strong>. If we do not pass a database URL, <strong class="source-inline">Optuna</strong> will use the in-memory storage instead. The <strong class="source-inline">study_name</strong> parameter is simply the name that we want to give to the current <strong class="source-inline">study</strong> object. If we do not pass a name, <strong class="source-inline">Optuna</strong> will automatically generate a random name for us. Last but not least, the <strong class="source-inline">load_if_exists</strong> parameter is a Boolean parameter that handles cases when there might be conflicting study names. If the study name is already generated in the storage, and we set <strong class="source-inline">load_if_exists=False</strong>, then <strong class="source-inline">Optuna</strong> will raise an error. On the other hand, if the study name is already generated in the storage, but we set <strong class="source-inline">load_if_exists=True</strong>, <strong class="source-inline">Optuna</strong> will just load the existing <strong class="source-inline">study</strong> object instead of creating a new one.</p>
			<p>Once the <strong class="source-inline">study</strong> object is initiated along with the appropriate parameters, we can start performing the hyperparameter tuning by calling the <strong class="source-inline">optimize()</strong> method. The following code shows you how to do that:</p>
			<pre class="source-code">study.<strong class="bold">optimize</strong>(<strong class="bold">func</strong>=<strong class="bold">lambda </strong>trial: objective(trial, df_train),</pre>
			<pre class="source-code">               <strong class="bold">n_trials</strong>=50, <strong class="bold">n_jobs</strong>=-1)</pre>
			<p>There are several important parameters in the <strong class="source-inline">optimize()</strong> method. The first and most important method is the <strong class="source-inline">func</strong> parameter. This parameter expects a callable that implements the <strong class="source-inline">objective</strong> function. Here, we don’t directly pass the <strong class="source-inline">objective</strong> function to the <strong class="source-inline">func</strong> parameter since our <strong class="source-inline">objective</strong> function expects two inputs, while by default, <strong class="source-inline">Optuna</strong> can only handle an <strong class="source-inline">objective</strong> function with one input, which is the <strong class="source-inline">Trial</strong> object itself. That’s why we need the help of Python’s built-in <strong class="source-inline">lambda</strong> function to pass the second input to our <strong class="source-inline">objective</strong> function. You can also utilize the same <strong class="source-inline">lambda</strong> function if your <strong class="source-inline">objective</strong> function has more than two inputs.</p>
			<p>The second <a id="_idIndexMarker424"/>most important parameter is <strong class="source-inline">n_trials</strong>, which refers to the number of trials or iterations for the hyperparameter tuning process. Another implemented parameter that can be used as the stopping criteria is the <strong class="source-inline">timeout</strong> parameter. This parameter expects the stopping criteria in the unit of seconds. By default, <strong class="source-inline">Optuna</strong> sets the <strong class="source-inline">n_trials</strong> and <strong class="source-inline">timeout</strong> parameters to <strong class="source-inline">None</strong>. If we leave it be, then <strong class="source-inline">Optuna</strong> will run the hyperparameter tuning process until it receives a termination signal, such as <strong class="source-inline">Ctrl+C</strong> or <strong class="source-inline">SIGTERM</strong>.</p>
			<p>Last but not least, <strong class="source-inline">Optuna</strong> also allows us to utilize the parallel resources through a parameter called <strong class="source-inline">n_jobs</strong>. By default, Optuna will set <strong class="source-inline">n_jobs=1</strong>, meaning that it will only utilize one job. Here, we set <strong class="source-inline">n_jobs=-1</strong>, meaning that we will use all of the CPU counts in our computer to perform parallel computation.</p>
			<p class="callout-heading">Hyperparameter’s Importance in Optuna</p>
			<p class="callout"><strong class="source-inline">Optuna</strong> provides a very nice module to measure the importance of each hyperparameter <a id="_idIndexMarker425"/>in the search space. As per version 2.10.0, there are two methods implemented, namely the <strong class="bold">fANOVA</strong> and <strong class="bold">Mean Decrease Impurity</strong> methods. Please <a id="_idIndexMarker426"/>see the official documentation on how to utilize this module and the theory behind the implemented methods, available at the following link: <a href="https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html">https://optuna.readthedocs.io/en/v2.10.0/reference/importance.html</a>.</p>
			<p>In this section, we learned what <strong class="source-inline">Optuna</strong> is in general, the available features that we can utilize, and the general steps as to how to perform hyperparameter tuning with this package. <strong class="source-inline">Optuna</strong> also has various visualization modules that can help us track our hyperparameter tuning experiments, which will be discussed in <a href="B18753_13_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 13</em></a>, <em class="italic">Tracking Hyperparameter Tuning Experiments</em>. In the upcoming sections, we will learn how to perform various hyperparameter tuning methods with <strong class="source-inline">Optuna</strong> through examples.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor085"/>Implementing TPE</h1>
			<p>TPE is one of <a id="_idIndexMarker427"/>the variants of the Bayesian optimization hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>), which is the default sampler in <strong class="source-inline">Optuna</strong>. To perform hyperparameter tuning with TPE in <strong class="source-inline">Optuna</strong>, we can just simply pass the <strong class="source-inline">optuna.samplers.TPESampler()</strong> class to the sampler parameter of the <strong class="source-inline">create_study()</strong> function. The following example shows how to implement TPE in <strong class="source-inline">Optuna</strong>. We’ll use the same data as in the examples in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a> and follow the steps introduced in the preceding section as follows: </p>
			<ol>
				<li value="1">Define the <strong class="source-inline">objective</strong> function along with the hyperparameter space. Here, we’ll use the same function that we defined in the <em class="italic">Introducing Optuna</em> section. Remember that we use the train-validation split instead of the k-fold cross-validation method within the <strong class="source-inline">objective</strong> function.</li>
				<li>Initiate a <strong class="source-inline">study</strong> object via the <strong class="source-inline">create_study()</strong> function as follows:<p class="source-code">study = optuna.create_study(direction='maximize',</p><p class="source-code"><strong class="bold">sampler</strong>=optuna.samplers.<strong class="bold">TPESampler</strong>(seed=0))</p></li>
				<li>Perform hyperparameter tuning by calling the <strong class="source-inline">optimize()</strong> method on the <strong class="source-inline">study</strong> object as follows:<p class="source-code">study.optimize(lambda trial: objective(trial, df_train),</p><p class="source-code">               n_trials=50, n_jobs=-1)</p><p class="source-code">print("Best Trial:")</p><p class="source-code">best_trial = study.best_trial</p><p class="source-code">print("    Value: ", best_trial.value)</p><p class="source-code">print("    Hyperparameters: ")</p><p class="source-code">for key, value in best_trial.params.items():</p><p class="source-code">    print(f"        {key}: {value}")</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.563</strong> of F1-score evaluated in the validation data. We also get a dictionary consisting of the best set of hyperparameters as follows:</p>
			<p class="source-code">{'num_layers': 2,'n_units_layer_0': 30,'dropout_rate_layer_0': 0.14068484717257745,'actv_func_layer_0': 'relu','n_units_layer_1': 20,'dropout_rate_layer_1': 0.34708586671782293,'actv_func_layer_1': 'relu','optimizer': 'Adam','adam_lr': 0.0018287924415952158,'epoch': 41}</p>
			<ol>
				<li value="4">Train the <a id="_idIndexMarker428"/>model on full training data using the best set of hyperparameters found. Here, we define another function called <strong class="source-inline">train_and_evaluate_final()</strong> that has the purpose of training the model in the full training data based on the best set of hyperparameters found in the preceding step, as well as evaluating it on the test data. You can see the implemented function in the GitHub repo mentioned in the <em class="italic">Technical requirements</em> section. Define the function as follows:<p class="source-code">train_and_evaluate_final(df_train, df_test, **best_trial.params)</p></li>
				<li>Test the final trained model on the test data. Based on the results from the preceding step, we get around <strong class="source-inline">0.604</strong> in F1-score when testing our final trained neural network model with the best set of hyperparameters on the test set.</li>
			</ol>
			<p>There are several important parameters for the <strong class="source-inline">TPESampler</strong> class. First, there is the <strong class="source-inline">gamma</strong> parameter, which refers to the threshold used in TPE to divide good and bad samples (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>).The <strong class="source-inline">n_startup_trials</strong> parameter is responsible for controlling how many trials will utilize Random Search before starting to perform the TPE algorithm. The <strong class="source-inline">n_ei_candidates</strong> parameter is responsible for controlling how many candidate samples are used to calculate the <strong class="source-inline">expected improvement acquisition</strong> function. Last but not least, the <strong class="source-inline">seed</strong> parameter, which controls the random seed of the experiment. There are many other parameters available for the <strong class="source-inline">TPESampler</strong> class, so please see the original documentation for more information, available at the following link: <a href="https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html">https://optuna.readthedocs.io/en/v2.10.0/reference/generated/optuna.samplers.TPESampler.html</a>.</p>
			<p>In this section, we have learned how to perform hyperparameter tuning with TPE in <strong class="source-inline">Optuna</strong> using the <a id="_idIndexMarker429"/>same data as in the example in <a href="B18753_07_ePub.xhtml#_idTextAnchor062"><em class="italic">Chapter 7</em></a>. As mentioned in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em> <strong class="source-inline">Optuna</strong> also implements the multivariate TPE, which is able to capture the interdependencies among hyperparameters. To enable the multivariate TPE, we can just simply set the <strong class="source-inline">multivariate</strong> parameter in <strong class="source-inline">optuna.samplers.TPESampler()</strong> as <strong class="source-inline">True</strong>. In the next section, we will learn how to perform Random Search with <strong class="source-inline">Optuna</strong>.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor086"/>Implementing Random Search</h1>
			<p>Implementing Random Search in <strong class="source-inline">Optuna</strong> is very similar to implementing TPE in <strong class="source-inline">Optuna</strong>. We can just <a id="_idIndexMarker430"/>follow a similar procedure to the preceding section and change the <strong class="source-inline">sampler</strong> parameter in the <strong class="source-inline">optimize()</strong> method in <em class="italic">step 2</em>. The following code shows you how to do that:</p>
			<pre class="source-code">study = optuna.create_study(direction='maximize', </pre>
			<pre class="source-code">sampler=optuna.samplers.<strong class="bold">RandomSampler</strong>(seed=0))</pre>
			<p>Using the exact same data, preprocessing steps, hyperparameter space, and <strong class="source-inline">objective</strong> function, we get around <strong class="source-inline">0.548</strong> in the F1-score evaluated in the validation data. We also get a dictionary consisting of the best set of hyperparameters as follows: </p>
			<pre class="source-code">{'num_layers': 0,'optimizer': 'Adam','adam_lr': 0.05075826567070766,'epoch': 50}</pre>
			<p>After the model is trained with full data using the best set of hyperparameters, we get around <strong class="source-inline">0.596</strong> in F1-score when we test the final neural network model trained on the test data. Notice that although we have defined many hyperparameters earlier, (see the <strong class="source-inline">objective</strong> function in the preceding section), here, we do not get all of them in the results. This is because most of the hyperparameters are conditional hyperparameters. For example, since the chosen value for the <em class="italic">’num_layers’</em> hyperparameter is zero, there will be no <em class="italic">’n_units_layer_{layer_i}’</em>,  <em class="italic">’dropout_rate_layer_{layer_i}’</em>, or <em class="italic">‘actv_func _layer_{layer_i}’</em> since those hyperparameters will only exist when the <em class="italic">’num_layers’</em> hyperparameter is greater than zero.</p>
			<p>In this section, we have seen how to perform hyperparameter tuning using the Random Search method with <strong class="source-inline">Optuna</strong>. In the next section, we will learn how to implement Grid Search with the <strong class="source-inline">Optuna</strong> package.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor087"/>Implementing Grid Search</h1>
			<p>Implementing Grid Search in <strong class="source-inline">Optuna</strong> is a bit different from implementing TPE and Random Search. Here, we <a id="_idIndexMarker431"/>need to also define the search space object and pass it to <strong class="source-inline">optuna.samplers.GridSampler()</strong>. The search space object is just a Python dictionary data structure consisting of hyperparameters’ names as the keys and the possible values of the corresponding hyperparameter as the dictionary’s values. <strong class="source-inline">GridSampler</strong> will stop the hyperparameter tuning process if all of the combinations in the search space have already been evaluated, even though the number of trials, <strong class="source-inline">n_trials</strong>, passed to the <strong class="source-inline">optimize()</strong> method has not been reached yet. Furthermore, <strong class="source-inline">GridSampler</strong> will only get the value stated in the search space no matter the range we pass to the sampling distribution methods, such as <strong class="source-inline">suggest_categorical</strong>, <strong class="source-inline">suggest_discrete_uniform</strong>, <strong class="source-inline">suggest_int</strong>, and <strong class="source-inline">suggest_float</strong>.</p>
			<p>The following code shows how to perform Grid Search in <strong class="source-inline">Optuna</strong>. The overall procedure to implement Grid Search in <strong class="source-inline">Optuna</strong> is similar to the procedure stated in the <em class="italic">Implementing Tree-structured Parzen Estimators</em> section. The only difference is that we have to define the search space and change the <strong class="source-inline">sampler</strong> parameter to <strong class="source-inline">optuna.samplers.GridSampler()</strong> in the <strong class="source-inline">optimize()</strong> method in <em class="italic">step 2</em> as follows: </p>
			<pre class="source-code"><strong class="bold">search_space</strong> = {'num_layers': [0,1],</pre>
			<pre class="source-code">                'n_units_layer_0': list(range(10,50,5)),</pre>
			<pre class="source-code">                'dropout_rate_layer_0': np.linspace(0,0.5,5),</pre>
			<pre class="source-code">                'actv_func_layer_0': ['relu','elu'],</pre>
			<pre class="source-code">                'optimizer': ['Adam','SGD'],</pre>
			<pre class="source-code">                'sgd_lr': np.linspace(1e-5,1e-1,5),</pre>
			<pre class="source-code">                'sgd_momentum': np.linspace(1e-5,1e-1,5),</pre>
			<pre class="source-code">                'adam_lr': np.linspace(1e-5,1e-1,5),</pre>
			<pre class="source-code">                'epoch': list(range(15,50,5))</pre>
			<pre class="source-code">               }</pre>
			<pre class="source-code">study = optuna.create_study(direction='maximize',                  sampler=optuna.samplers.<strong class="bold">GridSampler</strong>(<strong class="bold">search_space</strong>),</pre>
			<pre class="source-code">                           )</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.574</strong> of the F1-score evaluated in the validation data. We also get a dictionary consisting of the best set of hyperparameters as follows:</p>
			<pre class="source-code">{'num_layers': 0,'optimizer': 'Adam','adam_lr': 0.05000500000000001,'epoch': 25}</pre>
			<p>After the model <a id="_idIndexMarker432"/>is trained on full data using the best set of hyperparameters, we get around <strong class="source-inline">0.610</strong> in F1-score when we test the final neural network model trained on the test data.</p>
			<p>It is worth noting that <strong class="source-inline">GridSampler</strong> will rely on the search space to perform the hyperparameter sampling. For example, in the search space, we only define the valid values for <strong class="source-inline">num_layers</strong> as <strong class="source-inline">[0,1]</strong>. So, although within the <strong class="source-inline">objective</strong> function we set <strong class="source-inline">trial.suggest_int(‘num_layers’,low=0,high=3)</strong> (see the <em class="italic">Introducing Optuna</em> section), only <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> will be tested during the tuning process. Remember that, in <strong class="source-inline">Optuna</strong>, we can specify the stopping criterion through the <strong class="source-inline">n_trials</strong> or <strong class="source-inline">timeout</strong> parameters. If we specify either one of those criteria, <strong class="source-inline">GridSampler</strong> will not test all of the possible combinations in the search space; the tuning process will stop once the stopping criterion is met. In this example, we set <strong class="source-inline">n_trials=50</strong>, just like the example in the preceding section.</p>
			<p>In this section, we have learned how to perform hyperparameter tuning using the Grid Search method with <strong class="source-inline">Optuna</strong>. In the next section, we will learn how to implement SA with the <strong class="source-inline">Optuna</strong> package.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor088"/>Implementing Simulated Annealing</h1>
			<p>SA is not part <a id="_idIndexMarker433"/>of the built-in implementation of the hyperparameter tuning method in <strong class="source-inline">Optuna</strong>. However, as mentioned in the first section of this chapter, we can define our own custom sampler in <strong class="source-inline">Optuna</strong>. When creating a custom sampler, we need to create a class that inherits from the <strong class="source-inline">BaseSampler</strong> class. The most important method that we need to define within our custom class is the <strong class="source-inline">sample_relative()</strong> method. This method is responsible for sampling the corresponding hyperparameters from the search space based on the hyperparameter tuning algorithm we chose.</p>
			<p>The complete custom <strong class="source-inline">SimulatedAnnealingSampler()</strong> class with geometric cooling annealing schedule (see <a href="B18753_05_ePub.xhtml#_idTextAnchor047"><em class="italic">Chapter 5</em></a>) has been defined and can be seen in the GitHub repo <a id="_idIndexMarker434"/>mentioned in the <em class="italic">Technical requirements</em> section. The following code shows only the implementation of the <strong class="source-inline">sample_relative()</strong> method within the class:</p>
			<pre class="source-code">class <strong class="bold">SimulatedAnnealingSampler</strong>(optuna.samplers.<strong class="bold">BaseSampler</strong>):</pre>
			<pre class="source-code">    ...</pre>
			<pre class="source-code">    def <strong class="bold">sample_relative</strong>(self, study, trial, search_space):</pre>
			<pre class="source-code">        if search_space == {}:</pre>
			<pre class="source-code">            # The relative search space is empty (it means this is the first trial of a study).</pre>
			<pre class="source-code">            return {}</pre>
			<pre class="source-code"> </pre>
			<pre class="source-code">        prev_trial = self._get_last_complete_trial(study)</pre>
			<pre class="source-code">        if self._rng.uniform(0, 1) &lt;= self._transition_probability(study, prev_trial):</pre>
			<pre class="source-code">            self._current_trial = prev_trial</pre>
			<pre class="source-code">        params = self._sample_neighbor_params(search_space)</pre>
			<pre class="source-code">        <strong class="bold">#Geometric Cooling Annealing Schedule</strong></pre>
			<pre class="source-code">        self._temperature *= self.cooldown_factor </pre>
			<pre class="source-code">        return params</pre>
			<pre class="source-code">    ...</pre>
			<p>The following code shows how to perform hyperparameter tuning with SA in <strong class="source-inline">Optuna</strong>. The overall procedure to implement SA in <strong class="source-inline">Optuna</strong> is similar to the procedure stated in the <em class="italic">Implementing Tree-structured Parzen Estimators</em> section. The only difference is that we have to change the <strong class="source-inline">sampler</strong> parameter to <strong class="source-inline">SimulatedAnnealingSampler()</strong> in the <strong class="source-inline">optimize()</strong> method in <em class="italic">step 2</em> as follows:</p>
			<pre class="source-code">study = optuna.create_study(direction='maximize',</pre>
			<pre class="source-code">                  sampler=<strong class="bold">SimulatedAnnealingSampler</strong>(seed=0),</pre>
			<pre class="source-code">                           )</pre>
			<p>Using the exact <a id="_idIndexMarker435"/>same data, preprocessing steps, hyperparameter space, and <strong class="source-inline">objective</strong> function, we get around <strong class="source-inline">0.556</strong> of the F1-score evaluated in the validation data. We also get a dictionary consisting of the best set of hyperparameters as follows:</p>
			<pre class="source-code">{'num_layers': 3,'n_units_layer_0': 30,'dropout_rate_layer_0': 0.28421697443432425,'actv_func_layer_0': 'tanh','n_units_layer_1': 20,'dropout_rate_layer_1': 0.05936385947712203,'actv_func_layer_1': 'tanh','n_units_layer_2': 25,'dropout_rate_layer_2': 0.2179324626328134,'actv_func_layer_2': 'relu','optimizer': 'Adam','adam_lr': 0.006100619734336806,'epoch': 39}</pre>
			<p>After the model is trained on full data using the best set of hyperparameters, we get around <strong class="source-inline">0.559</strong> in F1-score when we test the final neural network model trained on the test data.</p>
			<p>In this section, we have learned how to perform hyperparameter tuning using the SA algorithm with <strong class="source-inline">Optuna</strong>. In the next section, we will learn how to utilize Successive Halving as a pruning method in <strong class="source-inline">Optuna</strong>.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor089"/>Implementing Successive Halving</h1>
			<p><strong class="bold">Successive Halving</strong> (<strong class="bold">SH</strong>) is implemented <a id="_idIndexMarker436"/>as a pruner in <strong class="source-inline">Optuna</strong>, meaning that it is responsible for stopping hyperparameter tuning iterations whenever it seems that there’s no additional benefit to continuing the process. Since it is implemented as a pruner, the resource definition of SH (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>) in <strong class="source-inline">Optuna</strong> refers to the number of training steps or epochs of the model, instead of the number of samples, as it does in <strong class="source-inline">scikit-learn</strong>’s implementation. </p>
			<p>We can utilize SH as a pruner along with any sampler that we use. This example shows you how to perform hyperparameter tuning with the Random Search algorithm as the sampler and SH as the pruner. The overall procedure is similar to the procedure stated in the <em class="italic">Implementing TPE</em> section. Since we are utilizing SH as a pruner, we have to edit our <strong class="source-inline">objective</strong> function so that it will utilize the pruner during the optimization process. In this example, we can use the callback integration with TFKeras provided by <strong class="source-inline">Optuna</strong> via <strong class="source-inline">optuna.integration.TFKerasPruningCallback</strong>. We simply need to pass this class to the <strong class="source-inline">callbacks</strong> parameter when fitting the model within the <strong class="source-inline">train</strong> function <a id="_idIndexMarker437"/>as shown in the following code:</p>
			<pre class="source-code">def <strong class="bold">train</strong>(trial, df_train: pd.DataFrame, df_val: pd.DataFrame = None):</pre>
			<pre class="source-code">...</pre>
			<pre class="source-code">    history = model.fit(X_train,y_train,</pre>
			<pre class="source-code">                       epochs=trial.suggest_int('epoch',15,50),</pre>
			<pre class="source-code">                       batch_size=64,</pre>
			<pre class="source-code">                       validation_data=(X_val,y_val) if df_val is not None else None,</pre>
			<pre class="source-code">                       <strong class="bold">callbacks</strong>=[<strong class="bold">optuna.integration.TFKerasPruningCallback</strong>(trial,'val_f1_m')],</pre>
			<pre class="source-code">                   )</pre>
			<pre class="source-code">...</pre>
			<p>Once we have told <strong class="source-inline">Optuna</strong> to utilize the pruner, we also need to set the <strong class="source-inline">pruner</strong> parameter in the <strong class="source-inline">optimize()</strong> method to <strong class="source-inline">optuna.pruners.SuccessiveHalvingPruner()</strong> in <em class="italic">step 2</em> of the <em class="italic">Implementing Tree-structured Parzen Estimators </em>section<em class="italic"> </em>as follows:</p>
			<pre class="source-code">study = optuna.create_study(direction='maximize',</pre>
			<pre class="source-code">  sampler=optuna.samplers.RandomSampler(seed=0),</pre>
			<pre class="source-code">  <strong class="bold">pruner</strong>=optuna.pruners.<strong class="bold">SuccessiveHalvingPruner</strong>(reduction_factor=3, min_resource=5)</pre>
			<pre class="source-code">                           )</pre>
			<p>In this example, we also increased the number of trials from <strong class="source-inline">50</strong> to <strong class="source-inline">100</strong> since most of the trials <a id="_idIndexMarker438"/>will be pruned anyway as follows: </p>
			<pre class="source-code">study.optimize(lambda trial: objective(trial, df_train),</pre>
			<pre class="source-code">               <strong class="bold">n_trials=100</strong>, n_jobs=-1,</pre>
			<pre class="source-code">              )</pre>
			<p>Using the exact same data, preprocessing steps, and hyperparameter space, we get around <strong class="source-inline">0.582</strong> of the F1-score evaluated in the validation data. Out of <strong class="source-inline">100</strong> trials performed, there are <strong class="source-inline">87</strong> trials pruned by SH, which implies only <strong class="source-inline">13</strong> completed trials. We also get a dictionary consisting of the best set of hyperparameters as follows:</p>
			<pre class="source-code">{'num_layers': 3,'n_units_layer_0': 10,'dropout_rate_layer_0': 0.03540368984067649,'actv_func_layer_0': 'elu','n_units_layer_1': 15,'dropout_rate_layer_1': 0.008554081181978979,'actv_func_layer_1': 'elu','n_units_layer_2': 15,'dropout_rate_layer_2': 0.4887044768096681,'actv_func_layer_2': 'relu','optimizer': 'Adam','adam_lr': 0.02763126523504823,'epoch': 28}</pre>
			<p>After the model is trained on full data using the best set of hyperparameters, we get around <strong class="source-inline">0.597</strong> in F1-score when we test the final neural network model trained on the test data.</p>
			<p>It is worth noting that there are several parameters for <strong class="source-inline">SuccessiveHalvingPruner</strong> that we can customize based on our needs. The <strong class="source-inline">reduction_factor</strong> parameter refers to the multiplier factor of SH (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>). The <strong class="source-inline">min_resource</strong> parameter refers to the minimum number of resources to be used at the first trial. This parameter is set to <em class="italic">‘auto’</em>, by default, where a heuristic is utilized to calculate the most appropriate value based on the number of required steps for the first trial to be completed. In other words, <strong class="source-inline">Optuna</strong> will only be able to start the tuning process after the <strong class="source-inline">min_resource</strong> training steps or epochs have been performed.</p>
			<p><strong class="source-inline">Optuna</strong> also provides the <strong class="source-inline">min_early_stopping_rate</strong> parameter, which has the exact same meaning as we defined in <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>. Last but not least, the <strong class="source-inline">bootstrap_count</strong> parameter. This parameter is not part of the original SH algorithm. The purpose of this parameter is to control the minimum number of trials that need to be completed before the actual SH iterations start. </p>
			<p>You may wonder, what about the parameter that controls the value of maximum resources <a id="_idIndexMarker439"/>and the number of candidates in SH? Here, in <strong class="source-inline">Optuna</strong>, the maximum resources definition will be automatically derived based on the total number of training steps or epochs within the defined <strong class="source-inline">objective</strong> function. As for the parameter that controls the number of candidates, <strong class="source-inline">Optuna</strong> delegates this responsibility to the <strong class="source-inline">n_trials</strong> parameter in the <strong class="source-inline">study.optimize()</strong> method.</p>
			<p>In this section, we have learned how to utilize SH as a pruner during the hyperparameter tuning process. In the next section, we will learn how to utilize Hyperband, the extended algorithm of SH, as a pruning method in <strong class="source-inline">Optuna</strong>.</p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor090"/>Implementing Hyperband</h1>
			<p>Implementing <strong class="bold">Hyperband</strong> (<strong class="bold">HB</strong>) in <strong class="source-inline">Optuna</strong> is very similar to implementing Successive Halving as <a id="_idIndexMarker440"/>a pruner. The only difference is that we have to set the <strong class="source-inline">pruner</strong> parameter in the <strong class="source-inline">optimize()</strong> method to <strong class="source-inline">optuna.pruners.HyperbandPruner()</strong> in <em class="italic">step 2</em> in the preceding section. The following code shows you how to perform hyperparameter tuning with the Random Search algorithm as the sampler and HB as the pruner:</p>
			<pre class="source-code">study = optuna.create_study(direction='maximize',</pre>
			<pre class="source-code">  sampler=optuna.samplers.RandomSampler(seed=0),</pre>
			<pre class="source-code">  <strong class="bold">pruner</strong>=optuna.pruners.<strong class="bold">HyperbandPruner</strong>(reduction_factor=3, min_resource=5)</pre>
			<pre class="source-code">                           )</pre>
			<p>All of the parameters of <strong class="source-inline">HyperbandPruner</strong> are the same as <strong class="source-inline">SuccessiveHalvingPruner</strong>’s, except that, here, there is no <strong class="source-inline">min_early_stopping_rate</strong> parameter and there is a <strong class="source-inline">max_resource</strong> parameter. The <strong class="source-inline">min_early_stopping_rate</strong> parameter is removed since it is set automatically based on the ID of each bracket. The <strong class="source-inline">max_resource</strong> parameter is responsible for setting the maximum resource allocated to a trial. By default, this parameter is set to <em class="italic">‘auto’</em>, which means that the value will be set as the largest step in the first completed trial.</p>
			<p>Using the exact <a id="_idIndexMarker441"/>same data, preprocessing steps, and hyperparameter space, we get around <strong class="source-inline">0.580</strong> of the F1-score evaluated in the validation data. Out of <strong class="source-inline">100</strong> trials performed, there are <strong class="source-inline">79</strong> trials pruned by SH, which implies only <strong class="source-inline">21</strong> completed trials. We also get a dictionary consisting of the best set of hyperparameters as follows:</p>
			<pre class="source-code">{'num_layers': 0,'optimizer': 'Adam','adam_lr': 0.05584201313189952,'epoch': 37}</pre>
			<p>After the model is trained on full data using the best set of hyperparameters, we get around <strong class="source-inline">0.609</strong> in F1-score when we test the final neural network model trained on the test data.</p>
			<p>In this section, we have learned how to utilize HB as a pruner during the hyperparameter tuning process with <strong class="source-inline">Optuna</strong>.</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor091"/>Summary</h1>
			<p>In this chapter, we have learned all of the important aspects of the <strong class="source-inline">Optuna</strong> package. We have also learned how to implement various hyperparameter tuning methods using the help of this package, in addition to understanding each of the important parameters of the classes and how are they related to the theory that we have learned in previous chapters. From now on, you should be able to utilize the packages we have discussed in the last few chapters to implement your chosen hyperparameter tuning method, and ultimately, boost the performance of your ML model. Equipped with the knowledge from <em class="italic">Chapters 3 - 6</em>, you will also be able to debug your code if there are errors or unexpected results, and you will be able to craft your own experiment configuration to match your specific problem.</p>
			<p>In the next chapter, we will learn about the DEAP and Microsoft NNI packages and how to utilize them to perform various hyperparameter tuning methods. The goal of the next chapter is similar to this chapter, which is to be able to utilize the package for hyperparameter tuning purposes and understand each of the parameters of the implemented classes.</p>
		</div>
		<div>
			<div id="_idContainer317">
			</div>
		</div>
	</body></html>