- en: Segmentation and Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we studied different methods for feature extraction
    and image classification using **Convolutional Neural Networks** (**CNNs**) to
    detect objects in an image. Those methods work well in creating a bounding box
    around the target object. However, if our application requires a precise boundary,
    called an **instance**, around the object, we need to apply a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be focusing on object instance detection, which is
    also termed image segmentation. In the second part of the chapter, we will first
    see MOSSE tracker with OpenCV see various approaches to tracking objects in a
    sequence of image
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation and tracking are, however, not quite interlinked problems, but
    they depend heavily on the previous approaches of feature extraction and object
    detection. The application's range is quite vast, including image editing, image
    denoising, surveillance, motion capture, and so on. The chosen methods for segmentation
    and tracking are suitable for specific applications.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be continuing the use of OpenCV and NumPy for image processing. For
    deep learning, we will use Keras with the TensorFlow backend. For segmentation,
    we will be using the `Pascal VOC` dataset. This has annotations for object detection,
    as well as segmentation. For tracking, we will use the `MOT16` dataset, which
    consists of an annotated sequence of images from video. We will mention how to
    use the code in the sections where it is used.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Segmentation is often referred to as the clustering of pixels of a similar
    category. An example is as shown in the following screenshot. Here, we see that
    inputs are on the left and the segmentation results are on the right. The colors
    of an object are according to pre-defined object categories. These examples are
    taken from the `Pascal VOC` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69476ba8-0d38-46cf-83cd-6c583e42c4a1.png)'
  prefs: []
  type: TYPE_IMG
- en: In the top picture on the left, there are several small aeroplanes in the background
    and, therefore, we see small pixels colored accordingly in the corresponding image
    on the right. In the bottom-left picture, there are two pets laying together,
    therefore, their segmented image on the right has different colors for the pixels
    belonging to the cat and dog respectively. In this figure, the boundary is differently
    colored for convenience and does not imply a different category.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional segmentation techniques, the key property used is image intensity
    levels. First, different smaller regions of similar intensity values are found,
    and later they are merged into larger regions. To get the best performance, an
    initial point is chosen by the user for algorithms. Recent approaches using deep
    learning have shown better performance without the need for initialization. In
    further sections, we will see an extension of previously seen CNNs for image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting our discussion on segmentation methods, let's look at the challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The challenges in a segmentation task are greater than the previous object
    detection task, as the complexity of detection is increased:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Noisy boundaries**: Grouping pixels that belong to a category may not be
    as accurate due to the fuzzy edges of an object. As a result, objects from different
    categories are clustered together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluttered scene**: With several objects in the image frame, it becomes harder
    to classify pixels correctly. With more clutter, the chances of false positive
    classification also increase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs for segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning based segmentation approaches have recently grown, both in terms
    of accuracy as well as effectiveness, in more complex domains. One of the popular
    models using CNN for segmentation is a **fully convolutional network** (**FCN**)[5],
    which we will explore in this section. This method has the advantage of training
    an end-to-end CNN to perform pixel-wise semantic segmentation. The output is an
    image with each pixel classified as either background or into one of the predefined
    categories of objects. The overall architecture is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49f5436a-4b06-4495-bee9-349d2a6c25cc.png)'
  prefs: []
  type: TYPE_IMG
- en: As the layers are stacked hierarchically, the output from each layer gets downsampled
    yet is feature rich. In the last layer, as shown in the figure, the downsampled
    output is upsampled using a deconvolutional layer, resulting in the final output
    being the same size as that of the input.
  prefs: []
  type: TYPE_NORMAL
- en: The deconvolutional layer is used to transform the input feature to the upsampled
    feature, however, the name is a bit misleading, as the operation is not exactly
    the inverse of convolution. This acts as transposed convolution, where the input
    is convolved after a transpose, as compared to a regular convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous model, the upsampling of the feature layer was done with a
    single layer. This can, however, be extended over to a hierarchical structure,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29bbcdc6-0f64-4016-9e24-9288c303e85b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, the feature extractor is kept the same, while upsampling
    is updated with more deconvolutional layers where each of these layers upsamples
    features from the previous layer and generates an overall richer prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of FCN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn to model one of the basic segmentation models
    in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by importing Keras required modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will create an FCN model, which takes in VGG16 features
    as input and adds further layers for fine tuning them. These are then upsampled
    to give resulting output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we saw segmentation methods to compute object precise region
    in an image. The FCN method shown here uses only convolutional layers to compute
    these regions. The `upsampling` method is key to compute pixel-wise categories
    and hence different choices of upsampling methods will result in a different quality
    of results.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking is the problem of estimating the position of an object over consecutive
    image sequences. This is also further divided into single object tracking and
    multiple object tracking, however, both single and multi-object tracking require
    slightly different approaches. In this section, we will see the methods for multi-object
    tracking, as well as single-object tracking.
  prefs: []
  type: TYPE_NORMAL
- en: The methods for image-based tracking are used in several applications, such
    as action recognition, self-driving cars, security and surveillance, augmented
    reality apps, motion capture systems, and video compression techniques. In **Augmented
    Reality** (**AR**) apps, for example, if we want to draw a virtual three-dimensional
    object on a planar surface, we would want to keep track of the planar surface
    for a feasible output.
  prefs: []
  type: TYPE_NORMAL
- en: In surveillance or traffic monitoring, tracking vehicles and keeping records
    of number plates helps to manage traffic and keeps security in check. Also, in
    video compression applications, if we already know that a single object is the
    only thing changing in frames, we can perform better compression by using only
    those pixels that change, thereby optimizing video transmission and receiving.
  prefs: []
  type: TYPE_NORMAL
- en: In the setup of tracking, we will first see challenges in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is always crucial to know which challenges we need to take care of before
    building apps. As a standard computer vision method, a lot of the challenges here
    are common:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object occlusion**: If the target object is hidden behind other objects in
    a sequence of images, then it becomes not only hard to detect the object but also
    to update future images if it becomes visible again.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast movement**: Cameras, such as on smartphones, often suffers from jittery
    movement. This causes a blurring effect and, sometimes, the complete absence of
    an object from the frame. Therefore, sudden changes in the motion of cameras also
    lead to problems in tracking applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change of shape**: If we are targeting non-rigid objects, changes in shape
    or the complete deformation of an object will often lead to being unable to detect
    the object and also tracking failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives**: In a scene with multiple similar objects, it is hard to
    match which object is targeted in subsequent images. The tracker may lose the
    current object in terms of detection and start tracking a similar object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These challenges can make our applications crash suddenly or give a completely
    incorrect estimate of an object's location.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for object tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An intuitive method for tracking is to use the object detection method from
    the previous chapter and compute detection in each frame. This will result in
    a bounding box detection for every frame, but we would also like to know if a
    particular object stays in the image sequence and for how many frames, that is,
    to keep track of K-frames for the object in the scene. We would also need a matching
    strategy to say that the object found in the previous image is the same as the
    one in the current image frame.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing with this intuition, we add a predictor for the bounding box motion.
    We assume a state for the bounding box, which consists of coordinates for the
    box center as well as its velocities. This state changes as we see more boxes
    in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Given the current state of the box, we can predict a possible region for where
    it will be in the next frame by assuming some noise in our measurement. The object
    detector can search for an object similar to the previous object in the next possible
    region. The location of the newly found object box and the previous box state
    will help us to update the new state of the box. This will be used for the next
    frame. As a result, iterating this process over all of the frames will result
    in not only the tracking of the object bounding box but keeping a location check
    on particular objects over the whole sequence. This method of tracking is also
    termed as **tracking by detection. **
  prefs: []
  type: TYPE_NORMAL
- en: In tracking by detection, each frame uses an object detector to find possible
    instances of objects and matches those detections with corresponding objects in
    the previous frame.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if no object detector is to be used, we can initialize the
    target object and track it by matching it and finding a similar object in each
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will see two popular methods for tracking. The
    first method is quite fast, yet simple, while the latter is quite accurate, even
    in the case of multiple-object tracking.
  prefs: []
  type: TYPE_NORMAL
- en: MOSSE tracker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is proposed by for fast object tracking using correlation filter methods.
    Correlation filter-based tracking comprises the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming a template of a target object *T* and an input image *I*, we first
    take the **Fast Fourier Transform** (**FFT**) of both the template (*T*) and the
    image (*I*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A convolution operation is performed between template *T* and image *I*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result from step 2 is inverted to the spatial domain using **Inverse Fast
    Fourier Transform** (**IFFT**). The position of the template object in the image
    *I* is the max value of the IFFT response we get.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This correlation filter-based technique has limitations in the choice of *T*.
    As a single template image match may not observe all the variations of an object,
    such as rotation in the image sequence, Bolme, and its co-authors[1] proposed
    a more robust tracker-based correlation filter, termed as **Minimum Output Sum
    of Squared Error** (**MOSSE**) filter. In this method, the template *T* for matching
    is first learned by minimizing a sum of squared error as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0ceec79-6109-42d4-b7e0-e1f27ddcebaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *i* is the training samples and the resulting learned template is *T**.
  prefs: []
  type: TYPE_NORMAL
- en: We will see the implementation of MOSSE tracker from OpenCV, as it already has
    good implementation here: [https://github.com/opencv/opencv/blob/master/samples/python/mosse.py](https://github.com/opencv/opencv/blob/master/samples/python/mosse.py)
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the key parts of the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `update` function gets a frame from video or image sequence iteratively
    and updates the state of the tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A major advantage of using the MOSSE filter is that it is quite fast for real-time
    tracking systems. The overall algorithm is simple to implement and can be used
    in the hardware without special image processing libraries, such as embedded platforms.
    There have been several modifications to this filter and, as such, readers are
    requested to explore more about these filters.
  prefs: []
  type: TYPE_NORMAL
- en: Deep SORT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we looked at one of the simplest trackers. In this section, we will
    use richer features from CNNs to perform tracking. **Deep SORT**[2] is a recent
    algorithm for tracking that extends **Simple Online and Real-time Tracking**[3]
    and has shown remarkable results in the **Multiple Object Tracking** (**MOT**)
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the problem setting of MOT, each frame has more than one object to track.
    A generic method to solve this has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detection**: First, all the objects are detected in the frame. There can
    be single or multiple detections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Association**: Once we have detections for the frame, a matching is performed
    for similar detections with respect to the previous frame. The matched frames
    are followed through the sequence to get the tracking for an object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Deep SORT, this generic method is further divided into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To compute detections, a popular CNN-based object detection method is used.
    In the paper[2], Faster-RCNN[4] is used to perform the initial detection per frame.
    As explained in the previous chapter, this method is two-stage object detection,
    which performs well for object detection, even in cases of object transformations
    and occlusions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The intermediate step before data association consists of an estimation model.
    This uses the state of each track as a vector of eight quantities, that is, box
    center (*x*, *y*), box scale (*s*), box aspect ratio (*a*), and their derivatives
    with time as velocities. The Kalman filter is used to model these states as a
    dynamical system. If there is no detection of a tracking object for a threshold
    of consecutive frames, it is considered to be out of frame or lost. For a newly
    detected box, the new track is started.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the final step, given the predicted states from Kalman filtering using the
    previous information and the newly detected box in the current frame, an association
    is made for the new detection with old object tracks in the previous frame. This
    is computed using Hungarian algorithm on bipartite graph matching. This is made
    even more robust by setting the weights of the matching with distance formulation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is further explained in the following diagram. The tracker uses a vector
    of states to store the historical information for previous detections. If a new
    frame comes, we can either use pre-stores bounding box detections or compute them
    using object detection methods discussed in *chapter 6.*  Finally, using current
    observation of bounding box detections and previous states, the current tracking
    is estimated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52fec427-9359-4a8c-92ca-d2e9ea01434e.png)'
  prefs: []
  type: TYPE_IMG
- en: We will see an effective demo of Deep SORT using its official repository at  [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort)
  prefs: []
  type: TYPE_NORMAL
- en: 'At first, clone the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we already have TensorFlow and Keras installed, we will not be going
    through their installation. As we saw previously, it uses CNN-based object detection
    for initial detection. We can run the network and get detection or use pre-generated
    detections. To do so, let''s get pre-trained models here in the `deep_sort` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On macOS (if `wget` is not available):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These downloaded files consist of pre-detected boxes using CNN-based models
    for the `MOT challenge` dataset CC BY-NC-SA 3.0\. We need one more thing to use
    the downloaded model, that is, a dataset on which these detections were created.
    Let's get the dataset from [https://motchallenge.net/data/MOT16.zip](https://motchallenge.net/data/MOT16.zip):[](https://motchallenge.net/data/MOT16.zip)
  prefs: []
  type: TYPE_NORMAL
- en: 'On macOS:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'On Linux:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have finished setting up the code structure, we can run a demo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--sequence_dir` is the path to the MOT challenge test image sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--detection_file` is our downloaded pre-generated detection corresponding
    to the sequence directory we chose previously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--min_confidence` is the threshold to filter any detection less than this
    value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For test sequence MOT16-06, we can see the window which shows video output
    frame-by-frame. Each frame consists of the bounding box around person tracked
    and the number is the ID of the person being tracked. The number updates if a
    new person is detected and follows until the tracking stops.  In the following
    figure, a sample output is explained from the tracking window. For ease of explanation,
    background image is not shown and only tracking boxes are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e15452e-1e3e-4fa2-a7bc-d508958426fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Readers are encourages to run other test sequences too, like MOT16-07, to further
    understand effectiveness of the model with varying environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw a demo of the Deep SORT method for MOT. One of the crucial
    parts of this method is detection and the use of Faster RCNN as a good detector.
    However, to increase the speed of the overall algorithm, Faster RCNN can also
    be replaced by other fast object detectors such as the Single Shot detector, because
    the rest of the method uses detected box states and not on the feature extraction
    method and features itself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, two different computer vision problems were shown. In segmentation,
    both the pixel level as well as convolutional neural net-based methods were shown.
    FCN shows the effectiveness of segmenting an image using the feature extraction
    method and, as a result, several current applications can be based on it. In track,
    two different approaches were discussed. Tracking by detection and tracking by
    matching can both be used for applications to track objects in the video. MOSSE
    tracker is a simple tracker for fast-paced applications and can be implemented
    on small computing devices. The Deep SORT method explained in this chapter can
    be used for multi-object tracking that uses deep CNN object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin with another branch of computer vision that
    focuses on understanding geometry of the scene explicitly. We will see methods
    to compute camera position and track its trajectory using only images.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bolme David S. J. Ross Beveridge, Bruce A. Draper, and Yui Man Lui. *Visual
    object tracking using adaptive correlation filters*. In Computer Vision and Pattern
    Recognition (CVPR), 2010 IEEE Conference on, pp. 2544-2550\. IEEE, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wojke, Nicolai, Alex Bewley, and Dietrich Paulus. *Simple Online and Realtime
    Tracking with a Deep Association Metric*. arXiv preprint arXiv:1703.07402 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bewley, Alex, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. *Simple
    online and realtime tracking*. In Image Processing (ICIP), 2016 IEEE International
    Conference on, pp. 3464-3468\. IEEE, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. *Faster R-CNN: Towards
    real-time object detection with region proposal networks*. In Advances in neural
    information processing systems, pp. 91-99\. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long, Jonathan, Evan Shelhamer, and Trevor Darrell. *Fully convolutional networks
    for semantic segmentation*. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 3431-3440\. 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
