<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer154">
<h1 class="chapterNumber">5</h1>
<h1 class="chapterTitle" id="_idParaDest-118">Deployment Patterns and Tools</h1>
<p class="normal">In this chapter, we will dive into some important concepts around the deployment of your <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) solution. We will begin to close the circle of the ML development lifecycle and lay the groundwork for getting your solutions out into the world.</p>
<p class="normal">The act of deploying software, of taking it from a demo you can show off to a few stakeholders to a service that will ultimately impact customers or colleagues, is a very exhilarating but often challenging exercise. It also remains one of the most difficult aspects of any ML project and getting it right can ultimately make the difference between generating value or just hype.</p>
<p class="normal">We are going to explore some of the main concepts that will help your ML engineering team cross the chasm between a fun proof-of-concept to solutions that can run on scalable infrastructure in an automated way. This will require us to first cover questions of how to design and architect your ML systems, particularly if you want to develop solutions that can be scaled and extended seamlessly. We will then discuss the concept of containerization and how this allows your application code to be abstracted from the specific infrastructure it is being built or run on, allowing for portability in many different cases. We will then move on to a concrete example of using these ideas to deploy an ML microservice on AWS. The rest of the chapter will then return to the question of how to build effective and robust pipelines for your end-to-end ML solution, which was introduced in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>. We will introduce and explore <strong class="keyWord">Apache Airflow</strong> for building and orchestrating any generic Python process, including your data preparation and ML pipelines. Then we will finish up with a similar deep dive on <strong class="keyWord">ZenML</strong> and <strong class="keyWord">Kubeflow</strong>, two open-source advanced ML pipelining tools that are now extensively used in industry. This collection of tools means that you should finish this chapter feeling very confident that you can deploy and orchestrate quite complex ML solutions using a variety of software.</p>
<p class="normal">This will all be broken down into the following sections:</p>
<ul>
<li class="bulletList">Architecting systems</li>
<li class="bulletList">Exploring some standard ML patterns</li>
<li class="bulletList">Containerizing</li>
<li class="bulletList">Hosting your own microservice on <strong class="keyWord">Amazon Web Services</strong> (<strong class="keyWord">AWS</strong>)</li>
<li class="bulletList">Building general pipelines with Airflow</li>
<li class="bulletList">Building advanced ML pipelines</li>
</ul>
<p class="normal">The next section will kick things off with a discussion of how we can architect and design our ML systems with deployment in mind. Let’s go!</p>
<h1 class="heading-1" id="_idParaDest-119">Technical requirements</h1>
<p class="normal">As with the other chapters, you can set up your Python development environment to be able to run the examples in this chapter by using the supplied Conda environment <code class="inlineCode">yml</code> file or the <code class="inlineCode">requirements.txt</code> files from the book repository, under <em class="italic">Chapter05</em>.</p>
<pre class="programlisting con"><code class="hljs-con">conda env create –f mlewp-chapter05.yml
</code></pre>
<p class="normal">You will also require some non-Python tools to be installed to follow the examples from end to end. Please see the respective documentation for each tool:</p>
<ul>
<li class="bulletList">AWS CLI v2</li>
<li class="bulletList">Postman</li>
<li class="bulletList">Docker</li>
</ul>
<h1 class="heading-1" id="_idParaDest-120">Architecting systems</h1>
<p class="normal">No matter how you are working<a id="_idIndexMarker559"/> to build your software, it is always important to have a design in mind. This section will highlight the key considerations we must bear in mind when architecting ML systems.</p>
<p class="normal">Consider a scenario where you are contracted to organize the building of a house. We would not simply go out and hire a team of builders, buy all the supplies, hire all the equipment, and just tell everyone to <em class="italic">start building</em>. We would also not assume we knew exactly what the client who hired us wants without first speaking to them.</p>
<p class="normal">Instead, we would likely try to understand what the client wanted in detail, and then try to design the solution that would fit their requirements. We would potentially iterate this plan a few times with them and with appropriate experts who knew the details of pieces that fed into the overall design. Although we are not interested in building houses (or maybe you are, but there will not be any in this book!), we can still see the analogy with software. Before building anything, we should create an effective and clear design. This design provides the direction of travel for the solution and helps the build team know exactly what components they will work on. This means that we will be confident that what we build will solve the end user’s problem.</p>
<p class="normal">This, in a nutshell, is what software architecture is all about.</p>
<p class="normal">If we did the equivalent of the above example for our ML solution, some of the following things may happen. We could end up with a very confusing code base, with some ML engineers in our team building elements and functionality that are already covered by the work that other engineers have done. We may also build something that fundamentally cannot work later in the project; for example, if we have selected a tool that has specific environmental requirements we cannot meet due to another component. We may also struggle to anticipate what infrastructure we need to be provisioned ahead of time, leading to a disorganized scramble within the project to get the correct resource. We may also underestimate the amount of work required and miss our deadline. All of these are outcomes we wish to avoid and can be avoided if we are following a good design.</p>
<p class="normal">In order to be effective, the architecture<a id="_idIndexMarker560"/> of a piece of software should provide at least the following things to the team working on building the solution:</p>
<ul>
<li class="bulletList">It should define the functional components required to solve the problem in totality.</li>
<li class="bulletList">It should define how these functional components will interact, usually through the exchange of some form of data.</li>
<li class="bulletList">It should show how the solution can be extended in the future to include further functionality the client may require.</li>
<li class="bulletList">It should provide guidance on which tools should be selected to implement each of the components outlined in the architecture.</li>
<li class="bulletList">It should stipulate the process flow for the solution, as well as the data flow.</li>
</ul>
<p class="normal">This is what a piece of good architecture should do, but what does this actually mean in practice?</p>
<p class="normal">There is no strict definition of how an architecture has to be compiled. The key point is that it acts as a design against which building can progress. So, for example, this might take the form of a nice diagram with boxes, lines, and some text, or it could be a several-page document. It might be compiled<a id="_idIndexMarker561"/> using a formal modeling language such as <strong class="keyWord">Unified Modeling Language</strong> (<strong class="keyWord">UML</strong>), or not. This often depends on the business context in which you operate and what requirements are placed on the people writing the architecture. The key is that it checks off the points above and gives the engineers clear guidance on what to build and how it will all stick together.</p>
<p class="normal">Architecture<a id="_idIndexMarker562"/> is a vast and fascinating subject in itself, so we will not go much further into the details of this here, but we will now focus on what architecture means in an ML engineering context.</p>
<h2 class="heading-2" id="_idParaDest-121">Building with principles</h2>
<p class="normal">The field of architecture<a id="_idIndexMarker563"/> is vast but no matter where you look, like any mature discipline, there are always consistent principles that are presented. The good news is that some of them are actually the same as some of the principles we met when discussing good Python programming in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>. In this section, we will discuss some of these and how they can be used for architecting ML systems.</p>
<p class="normal"><strong class="keyWord">Separation of Concerns</strong> has already been mentioned in this book<a id="_idIndexMarker564"/> as a good way to ensure that software components inside your applications are not unnecessarily complex and that your solutions are extensible and can be easily interfaced with. This principle holds true of systems in their entirety and as such is a good architecture principle to bear in mind. In practice, this often manifests in the idea of separate “layers” within your applications that have distinct responsibilities. For example, let’s look at the architecture shown in <em class="italic">Figure 5.1</em>. This shows how to use tools to create an automated deployment<a id="_idIndexMarker565"/> and orchestration process for ML pipelines and is taken from the AWS Solutions Library, <a href="https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/"><span class="url">https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/</span></a>. We can see that there are distinct “areas” within the architecture corresponding to provisioning, pipeline deployment, and pipeline serving. These blocks show that there are distinct pieces of the solution that have specific functionalities and that the interaction between these different pieces is handled by an interface.</p>
<figure class="mediaobject"><img alt="" height="365" role="presentation" src="../Images/B19525_05_01.png" width="826"/></figure>
<p class="packt_figref">Figure 5.1: An ML workload orchestrator architecture from the AWS Solutions Library MLOps Workload Orchestrator.</p>
<p class="normal">The <strong class="keyWord">Principle of Least Surprise</strong> is a rule of thumb that essentially captures the fact that the first time<a id="_idIndexMarker566"/> any reasonably knowledgeable<a id="_idIndexMarker567"/> person in your domain, such as a developer, tester, or data scientist, encounters your architecture, it should not have anything within it that should stand out as unorthodox or surprising. This may not always be possible, but it is a good principle to keep in mind as it forces you to consider what those who are likely to be working with your architecture already know and how you can leverage that to both make a good design and have it followed. Using <em class="italic">Figure 5.1</em> as an example again, the architecture embodies the principle very nicely, as the design has clear logic building blocks for provisioning, promoting, and running the ML pipelines. At a lower level in the architecture, we can see that data is consistently being sourced from S3 buckets, that Lambdas are interacting with API gateways, and so on and so forth. This means that ML engineers, data scientists, and cloud platform engineers will both understand and leverage this architecture well when implementing it.</p>
<p class="normal">The <strong class="keyWord">Principle of Least Effort</strong> is a bit more subtle than the previous one, in that<a id="_idIndexMarker568"/> it captures the idea that developers, being human, will follow the path of least resistance and not create more work unless necessary. I interpret this principle as emphasizing the importance of taking the time to consider your architecture thoughtfully and building it with care, as it could be used for a long time after it has been developed, by engineer after engineer!</p>
<p class="normal">So far, we have only discussed high-level architecture principles. Now we will look at some design principles that – while they can still be used at the system design level – are also very powerful when used at the level of your code.</p>
<p class="normal">The <strong class="keyWord">SOLID</strong> principles (<strong class="keyWord">Single Responsibility</strong>, <strong class="keyWord">Open/Closed</strong>, <strong class="keyWord">Liskov Substitution</strong>, <strong class="keyWord">Interface Segregation</strong>, <strong class="keyWord">Dependency Inversion</strong>) are a set that is often applied<a id="_idIndexMarker569"/> to the code base but can also be extrapolated up to system design and architecture quite nicely. Once we adapt these principles to the architecture level, they can be explained in the following way:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Single Responsibility</strong>: This is very similar, perhaps identical, to the idea of separation of concerns. Specifically, this states that if a module only has one reason to change at any one time, or it only has one job to do, then this makes it more resilient and easier to maintain. If you have one box in your architecture diagram that is going to have to do ten different things, then you have violated this principle and it means that whenever any one of those processes or interfaces has to change, you have to go into that box and poke around, potentially creating more issues or drastically increasing the likelihood of downtime.</li>
<li class="bulletList"><strong class="keyWord">Open/Closed</strong>: This refers to the fact that it is a really good idea to architect in a way that components are “open for extension but closed for modification.” This also works at the level of the entire design. If you design your system so that new functionality can be tagged on and does not require going back and rewiring the core, then you will likely build something that will stand the test of time. A great example from ML would be that if we try and build our system so that if we want to add in new processing pipelines we can just do that, and we don’t have to go back into some obscure section of the code and severely modify things.</li>
<li class="bulletList"><strong class="keyWord">Liskov Substitution</strong>: When the SOLID principles were written, they originally referred to object-oriented programming in languages<a id="_idIndexMarker570"/> like Java. This principle then stated that objects should be able to be replaced by their subtypes and still maintain application behavior. At the system level, this now basically states that if two components are supposed to have the same interface and contract with other components, you can swap them for one another.</li>
<li class="bulletList"><strong class="keyWord">Interface Segregation</strong>: I interpret this one as “don’t have multiple ways for components to talk to one another.” So, in your application, try and ensure that the ways of handing off between different pieces of the solution are pretty narrow. Another way of phrasing this is that making your interfaces as client specific as possible is a good idea.</li>
<li class="bulletList"><strong class="keyWord">Dependency Inversion</strong>: This is very similar to the Liskov Substitution principle but is a bit more general. The idea here is that the communications between modules or parts of your solution should be taken care of by abstractions and not by a concrete, specific implementation. A good example would be that instead of calling an ML microservice directly from another process, you instead place the requisite job data in a queue, for example, AWS Simple Queue Service, and then the microservice picks up the work from the queue. This ensures that the client and the serving microservice do not need to know details about each other’s interface, and also that the downstream application can be extended with more services reading from the queue. This would then also embody the Open/Closed principle, and can be seen in the architecture in <em class="italic">Figure 5.1</em> through the use of the Lambda function calling into AWS CloudFormation.</li>
</ul>
<p class="normal">A final favorite<a id="_idIndexMarker571"/> of mine is the concept of <strong class="keyWord">Bounded Contexts</strong>, where we have to seek to ensure that data<a id="_idIndexMarker572"/> models, or other important<a id="_idIndexMarker573"/> data or metadata, are aligned within specific conceptual models and are not a “free-for-all.” This applies particularly well to Domain-Driven Design and applies very well to large, complex solutions. A great example would be if you have a large organization with multiple business units and they want a series of very similar services that run ML on business data stored in a database. It would be better for there to be several databases hosting the information, one for each business unit, rather than having a shared data layer across multiple applications. More concretely, your data model shouldn’t contain information specific to the sales and marketing function and the engineering function and the human resources function, and so on. Instead, each should have their own database with their own models, and there should be explicit contracts for joining any information between them later if needed. I believe that this idea can still be applied to Data Lakes, which are discussed later in this chapter. In this case, the bounded contexts could apply to specific folders within the lake, or they could actually refer to the context<a id="_idIndexMarker574"/> of entire lakes, each segregated into different domains. This is very much the idea behind the so-called Data Mesh.</p>
<p class="normal">We have just mentioned some of the most used ML patterns, so let’s now move on to explore this concept in a bit more detail as we look to apply the principles we have been discussing.</p>
<h1 class="heading-1" id="_idParaDest-122">Exploring some standard ML patterns</h1>
<p class="normal">In this book, we have already mentioned<a id="_idIndexMarker575"/> a few times that we should not attempt to <em class="italic">reinvent</em> the wheel and we should reuse, repeat, and recycle what works according to the wider software and ML community. This is also true about your deployment architectures. </p>
<p class="normal">When we discuss architectures that can be reused for a variety of different use cases with similar characteristics, we often refer to these as <em class="italic">patterns</em>. Using standard (or at least well-known) patterns can really help you speed up the time to value of your project and help you engineer your ML solution in a way that is robust and extensible.</p>
<p class="normal">Given this, we will spend the next few sections summarizing some of the most important architectural patterns that have become increasingly successful in the ML space over the past few years.</p>
<h2 class="heading-2" id="_idParaDest-123">Swimming in data lakes</h2>
<p class="normal">The single most important asset<a id="_idIndexMarker576"/> for anyone trying to use ML is, of course, the data<a id="_idIndexMarker577"/> that we can analyze and train our models on. The era of <strong class="keyWord">big data</strong> meant that the sheer size and variability in the format of this data became an increasing challenge. If you are a large organization (or even not so large), it is not viable to store all of the data you will want to use for ML applications in a structured relational database. Just the complexity of modeling the data for storage in such a format would be very high. So, what can you do?</p>
<p class="normal">Well, this problem was initially<a id="_idIndexMarker578"/> tackled with the introduction of <strong class="keyWord">data warehouses</strong>, which let you bring all of your relational data storage into one solution and create a single point of access. This helps alleviate, to some extent, the problem of data volumes, as each database can store relatively small amounts of data even if the total is large. These warehouses were designed with the integration of multiple data sources in mind. However, they are still relatively restrictive as they usually bundle together the infrastructure for compute and storage. This means they can’t be scaled very well, and they can be expensive investments that create vendor lock-in. Most importantly for ML, data warehouses cannot store raw and semi-structured or unstructured data (for example, images). This automatically rules out a lot of good ML use cases if warehouses<a id="_idIndexMarker579"/> are used as your main data store. Now, with tools such as <strong class="keyWord">Apache Spark</strong>, which we’ve already used extensively throughout this book, if we have the clusters available, we can feasibly analyze and model any size or structure of data. The question then becomes, how should we store it?</p>
<p class="normal"><strong class="keyWord">Data lakes</strong> are technologies that allow you to store any type of data at any scale you feasibly need. There are a variety<a id="_idIndexMarker580"/> of providers of data lake<a id="_idIndexMarker581"/> solutions, including the main public cloud providers, such as <strong class="keyWord">Microsoft Azure</strong>, <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>), and AWS. Since we have met AWS before, let’s focus on that.</p>
<p class="normal">The main storage solution in AWS is called the <strong class="keyWord">Simple Storage Service</strong>, or <strong class="keyWord">S3</strong>. Like all of the core data lake<a id="_idIndexMarker582"/> technologies, you can effectively load anything into it since it is based on the concept of <em class="italic">object storage</em>. This means that every instance of data you load is treated as its own object with a unique identifier and associated metadata. </p>
<p class="normal">It allows your S3 bucket to simultaneously contain photographs, JSON files, <code class="inlineCode">.txt</code> files, Parquet files, and any other number of data formats.</p>
<p class="normal">If you work in an organization<a id="_idIndexMarker583"/> that does not have a data lake, this does not automatically exclude you from doing ML, but it can definitely make it a more difficult journey since with a lake you always know how you can store the data you need for your problem, no matter the format.</p>
<h2 class="heading-2" id="_idParaDest-124">Microservices</h2>
<p class="normal">Your ML project’s code<a id="_idIndexMarker584"/> base will start small – just a few lines at first. But as your team expends more and more effort in building the solution required, this will quickly grow. If your solution has to have a few different capabilities and perform some quite distinct actions and you keep all of this in the same code base, your solution can become incredibly complex. In fact, software<a id="_idIndexMarker585"/> in which the components are all tightly coupled and non-separable like this is called <strong class="keyWord">monolithic</strong>, as it is akin to single big blocks that can exist independently of other applications. This sort of approach may fit the bill for your use case, but as the complexity of solutions continues to increase, a much more resilient and extensible design pattern is often required.</p>
<p class="normal">Microservice architectures are those in which the functional components of your solution are cleanly separated, potentially in completely different code bases or running on completely different infrastructure. For example, if we are building a user-facing web application that allows users to browse, select, and purchase products, we may have a variety of ML capabilities we wish to deploy in quick succession. We may want to recommend new products based on what they have just been looking at, we may want to retrieve forecasts of when their recently ordered items will arrive, and we may want to highlight some discounts we think they will benefit from (based on our analysis of their historic account behavior). This would be a very tall order, maybe even impossible, for a monolithic application. However, it is something that quite naturally falls into microservice architecture like that in <em class="italic">Figure 5.2</em>:</p>
<figure class="mediaobject"><img alt="Figure 5.1 – An example of some ML microservices " height="261" src="../Images/B19525_05_02.png" width="351"/></figure>
<p class="packt_figref">Figure 5.2: An example of some ML microservices.</p>
<p class="normal">The implementation of a microservice architecture can be accomplished using a few tools, some of which we will cover in the <em class="italic">Hosting your own microservice on AWS</em> section. The main idea is that you always separate out the elements of your solution into their own services that are not tightly coupled together.</p>
<p class="normal">Microservice architectures<a id="_idIndexMarker586"/> are particularly good at allowing our development teams to achieve the following:</p>
<ul>
<li class="bulletList">Independently debug, patch, or deploy individual services rather than tearing down the whole system.</li>
<li class="bulletList">Avoid a single point of failure.</li>
<li class="bulletList">Increase maintainability.</li>
<li class="bulletList">Allow separate services to be owned by distinct teams with clearer responsibilities.</li>
<li class="bulletList">Accelerate the development of complex products.</li>
</ul>
<p class="normal">Like every architecture pattern or design style, it is, of course, not a silver bullet, but we would do well to remember the microservice<a id="_idIndexMarker587"/> architecture when designing our next solution.</p>
<p class="normal">Next, we will discuss event-based designs.</p>
<h2 class="heading-2" id="_idParaDest-125">Event-based designs</h2>
<p class="normal">You do not always want to operate<a id="_idIndexMarker588"/> in scheduled batches. As we have seen, even just in the previous section, <em class="italic">Microservices</em>, not all use cases align with running a large batch prediction from a model on a set schedule, storing the results, and then retrieving them later. What happens if the data volumes you need are not there for a training run? What if no new data to run predictions on has arrived? What if other systems could make use of a prediction based on individual data points at the earliest time they become available rather than at a specific time every day?</p>
<p class="normal">In an event-based architecture, individual actions produce results that then trigger other individual actions in the system, and so on and so forth. This means that processes can happen as early as they can and no earlier. It also allows for a more dynamic or stochastic data flow, which can be beneficial if other systems are not running on scheduled batches either.</p>
<p class="normal">Event-based patterns could be mixed with others, for example, microservices or batch processing. The benefits still stand, and, in fact, event-based components allow for more sophisticated orchestration and management of your solution.</p>
<p class="normal">There are two types of event-based patterns:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Pub/sub</strong>: In this case, event data is published<a id="_idIndexMarker589"/> to a message broker or event bus to be consumed by other applications. In one variant of the pub/sub pattern, the broker or buses used are organized by some appropriate classification and are designated as <strong class="keyWord">topics</strong>. An example<a id="_idIndexMarker590"/> of a tool<a id="_idIndexMarker591"/> that does this is <strong class="keyWord">Apache Kafka</strong>.</li>
<li class="bulletList"><strong class="keyWord">Event streaming</strong>: Streaming use cases<a id="_idIndexMarker592"/> are ones where we want to process a continuous flow of data in something very close to real time. We can think of this as working with data as it <em class="italic">moves through</em> the system. This means it is not persisted <em class="italic">at rest</em> in a database but processed as it is created or received by the streaming solution. An example tool<a id="_idIndexMarker593"/> to use for event streaming applications is <strong class="keyWord">Apache Storm</strong>.</li>
</ul>
<p class="normal"><em class="italic">Figure 5.3</em> shows an example event-based architecture applied to the case of <strong class="keyWord">IoT</strong> and mobile devices that have their data passed into classification and anomaly detection algorithms:</p>
<figure class="mediaobject"><img alt="Figure 5.2 – A basic event-based architecture where a stream of data is accessed by different services via a broker " height="413" src="../Images/B19525_05_03.png" width="821"/></figure>
<p class="packt_figref">Figure 5.3: A basic event-based high-level design where a stream of data is accessed by different services via a broker.</p>
<p class="normal">The next section will touch on designs where we do the opposite of processing one data point at a time and instead work <a id="_idIndexMarker594"/>with large chunks or batches at any one time.</p>
<h2 class="heading-2" id="_idParaDest-126">Batching</h2>
<p class="normal">Batches of work may not sound<a id="_idIndexMarker595"/> like the most sophisticated concept, but it is one of the most common pattern flavors out there in the world of ML.</p>
<p class="normal">If the data you require for prediction comes in at regular time intervals in batches, it can be efficient to schedule your prediction runs with a similar cadence. This type of pattern can also be useful if you do not have to create a low-latency solution.</p>
<p class="normal">This concept can also be made to run quite efficiently for a few reasons:</p>
<ul>
<li class="bulletList">Running in scheduled batches means that we know exactly when we will need compute resources, so we can plan accordingly. For example, we may be able to shut down our clusters for most of the day or repurpose them for other activities.</li>
<li class="bulletList">Batches allow for the use of larger numbers of data points at runtime, so you can run things such as anomaly detection or clustering at the batch level if desired.</li>
<li class="bulletList">The size of your batches of data can often be chosen to optimize some criterion. For example, using large batches and running parallelized logic and algorithms on it could be more efficient.</li>
</ul>
<p class="normal">Software solutions where ML algorithms<a id="_idIndexMarker596"/> are run in batches often look very similar to classic <strong class="keyWord">Extract</strong>, <strong class="keyWord">Transform</strong>, <strong class="keyWord">Load</strong> (<strong class="keyWord">ETL</strong>) systems. These are systems where data is extracted from a source or sources, before being processed on route to a target system where it is then uploaded. In the case of an ML solution, the processing is not standard data transformation such as joins and filters but is instead the application of feature engineering<a id="_idIndexMarker597"/> and ML algorithm pipelines. This is why, in this book, we will term these designs <strong class="keyWord">Extract, Transform, Machine Learning </strong>(<strong class="keyWord">ETML</strong>) patterns. ETML will be discussed more in <em class="chapterRef">Chapter 9</em>, <em class="italic">Building an Extract, Transform, Machine Learning Use Case</em>.</p>
<p class="normal">We will now discuss a key piece<a id="_idIndexMarker598"/> of technology that is critical to making modern architectures applicable to a wide range of platforms – containers.</p>
<h1 class="heading-1" id="_idParaDest-127">Containerizing</h1>
<p class="normal">If you develop software<a id="_idIndexMarker599"/> that you want to deploy somewhere, which is the core aim of an ML engineer, then you have to be very aware of the environmental requirements of your code, and how different environments might affect the ability of your solution to run. This is particularly important for Python, which does not have a core capability<a id="_idIndexMarker600"/> for exporting programs as standalone executables (although there are options for doing this). This means that Python code needs a Python interpreter to run and needs to exist in a general Python environment where the relevant libraries and supporting packages have been installed.</p>
<p class="normal">A great way to avoid headaches from this point of view is to ask the question: <em class="italic">Why can’t I just put everything I need into something that is relatively isolated from the host environment, which I can ship and then run as a standalone application or program?</em> The answer to this question is that you can and that you do this through <strong class="keyWord">containerization</strong>. This is a process whereby an application and its dependencies can be packaged together in a standalone unit that can effectively run on any computing platform.</p>
<p class="normal">The most popular<a id="_idIndexMarker601"/> container technology is <strong class="keyWord">Docker</strong>, which is open-source and very easy to use. Let’s learn <a id="_idIndexMarker602"/>about it by using it to containerize a simple <strong class="keyWord">Flask</strong> web application that could act as an interface to a forecasting model like that created in the <em class="italic">Example 2</em>: <em class="italic">Forecasting API</em> section in <em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to ML Engineering</em>.</p>
<p class="normal">The next few sections<a id="_idIndexMarker603"/> will use a similar simple Flask application that has a forecast serving endpoint. As a proxy for a full ML model, we will first work with a skeleton application that simply returns a short list of random numbers when requested for a forecast. The detailed code for the application can be found in this book’s GitHub repo at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service</span></a>.</p>
<p class="normal">The web application creates a basic app where you can supply a store ID and forecast a start date for the system and it will return the dummy forecast. To get this, you hit the <code class="inlineCode">/forecast</code> endpoint.</p>
<p class="normal">An example is shown in <em class="italic">Figure 5.4</em>:</p>
<figure class="mediaobject"><img alt="" height="812" role="presentation" src="../Images/B19525_05_04.png" width="825"/></figure>
<p class="packt_figref">Figure 5.4: The result of querying our skeleton ML microservice.</p>
<p class="normal">Now, we’ll move on to discuss<a id="_idIndexMarker604"/> how to containerize this application. First, you need to install Docker<a id="_idIndexMarker605"/> on your platform by using the documentation at <a href="https://docs.docker.com/engine/install/"><span class="url">https://docs.docker.com/engine/install/</span></a>:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Once you have Docker installed, you need to tell it how to build the container image, which you do by creating a <code class="inlineCode">Dockerfile</code> in your project. The <code class="inlineCode">Dockerfile</code> specifies all of the build steps in text so that the process of building the image is automated and easily configurable. We will now walk through building a simple example <code class="inlineCode">Dockerfile</code>, which will be built on in the next section, <em class="italic">Hosting your own microservice on AWS</em>. First, we need to specify the base image we are working from. It usually makes sense to use one of the official Docker images as a base, so here we will use the <code class="inlineCode">python:3.10-slim</code> environment to keep things lean and mean. This base image will be used in all commands following the <code class="inlineCode">FROM</code> keyword, which signifies we are entering a build stage. We can actually name this stage for later use, calling it <code class="inlineCode">builder</code> using the <code class="inlineCode">FROM … as</code> syntax:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.10</span>-slim as builder
</code></pre>
</li>
<li class="numberedList">Then, we copy all the files we need from the current directory to a directory labeled <code class="inlineCode">src</code> in the build stage and install all of our requirements using our <code class="inlineCode">requirements.txt</code> file (if you want to run this step without specifying any requirements, you can just use an empty <code class="inlineCode">requirements.txt</code> file):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">COPY</span> . /src
<span class="hljs-keyword">RUN</span> pip install --user --no-cache-dir -r requirements.txt
</code></pre>
</li>
<li class="numberedList">The next stage involves similar steps but is aliased to the word <code class="inlineCode">app</code> since we are now creating our application. Notice the reference to the <code class="inlineCode">builder</code> stage from steps <em class="italic">1</em> and <em class="italic">2</em> here:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.10</span>-slim as app
<span class="hljs-keyword">COPY</span> --from=builder /root/.local /root/.local
<span class="hljs-keyword">COPY</span> --from=builder /src .
</code></pre>
</li>
<li class="numberedList">We can define or add to environment variables as we are used to in a bash environment:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">ENV</span> PATH=/root/.local:$PATH
</code></pre>
</li>
<li class="numberedList">Since in this example<a id="_idIndexMarker606"/> we are going to be running a simple Flask web application, we need to tell the system which port to expose:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">5000</span>
</code></pre>
</li>
<li class="numberedList">We can execute commands during the Docker build using the <code class="inlineCode">CMD</code> keyword. Here, we use this to run <code class="inlineCode">app.py</code>, which is the main entry point to the Flask app, and will start the service we will call via REST API to get ML results later:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">CMD</span> [<span class="hljs-string">"python3"</span>, <span class="hljs-string">"app.py"</span>]
</code></pre>
</li>
<li class="numberedList">Then we can build the image with the <code class="inlineCode">docker build</code> command. Here, we create an image named <code class="inlineCode">basic-ml-microservice</code> and tag it with the <code class="inlineCode">latest</code> label:
        <pre class="programlisting con"><code class="hljs-con">docker build -t basic-ml-microservice:latest
</code></pre>
</li>
<li class="numberedList">To check the build was successful, run the following command in the Terminal:
        <pre class="programlisting con"><code class="hljs-con">docker images --format "table {{.ID}}\t{{.CreatedAt}}\t{{.Repository}}"
</code></pre>
<p class="normal">You should see an output like that in <em class="italic">Figure 5.5</em>:</p>
<figure class="mediaobject"><img alt="" height="296" role="presentation" src="../Images/B19525_05_05.png" width="761"/></figure>
<p class="packt_figref">Figure 5.5: Output from the docker images command.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="9">Finally, you can run your Docker image with the following command in your Terminal:
        <pre class="programlisting con"><code class="hljs-con">docker run --rm -it -p 8080:5000 basic-ml-microservice:latest
</code></pre>
</li>
</ol>
<p class="normal">Now that you have containerized<a id="_idIndexMarker607"/> some basic applications and can run your Docker image, we need to answer the question of how we can use this to build an ML solution hosted on an appropriate platform. The next section covers how we can do this on AWS.</p>
<h1 class="heading-1" id="_idParaDest-128">Hosting your own microservice on AWS</h1>
<p class="normal">A classic way to surface<a id="_idIndexMarker608"/> your ML models is via a lightweight web service hosted on a server. This can be a very flexible pattern of deployment. </p>
<p class="normal">You can run a web service on any server with access to the internet (roughly) and, if designed well, it is often easy to add further functionality to your web service and expose it via new endpoints.</p>
<p class="normal">In Python, the two<a id="_idIndexMarker609"/> most used web<a id="_idIndexMarker610"/> frameworks have always been <strong class="keyWord">Django</strong> and <strong class="keyWord">Flask</strong>. In this section, we will focus on Flask as it is the simpler of the two and has been written about extensively for ML deployments on the web, so you will be able to find plenty of material to build on what you learn here.</p>
<p class="normal">On AWS, one of the simplest ways you can host your Flask web solution is as a containerized application on an appropriate platform. We will go through the basics of doing this here, but we will not spend time on the detailed aspects of maintaining good web security for your service. To fully discuss this may require an entire book in itself, and there are excellent, more focused resources elsewhere.</p>
<p class="normal">We will assume that you have your AWS account set up from <em class="chapterRef">Chapter 2</em>, <em class="italic">The Machine Learning Development Process</em>. If you do not, then go back and refresh yourself on what you need to do.</p>
<p class="normal">We will need the AWS <strong class="keyWord">Command Line Interface</strong> (<strong class="keyWord">CLI</strong>). You can find the appropriate commands for installing<a id="_idIndexMarker611"/> and configuring the AWS CLI, as well as a lot of other<a id="_idIndexMarker612"/> useful information, on the AWS CLI documentation pages at <a href="https://docs.aws.amazon.com/cli/index.xhtml"><span class="url">https://docs.aws.amazon.com/cli/index.xhtml</span></a>.</p>
<p class="normal">Specifically, configure<a id="_idIndexMarker613"/> your Amazon CLI by following the steps in this tutorial: <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml"><span class="url">https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml</span></a>.</p>
<p class="normal">The documentation specifies how to install the CLI for a variety of different computer architectures, so follow along for your given platform and then you will be ready to have fun with the AWS examples used in the rest of the book!</p>
<p class="normal">In the following<a id="_idIndexMarker614"/> example, we will use Amazon <strong class="keyWord">Elastic Container Registry</strong> (<strong class="keyWord">ECR</strong>) and <strong class="keyWord">Elastic Container Service</strong> (<strong class="keyWord">ECS</strong>) to host a skeleton containerized<a id="_idIndexMarker615"/> web application. In <em class="chapterRef">Chapter 8</em>, <em class="italic">Building an Example ML Microservice</em>, we will discuss how to build and scale an ML microservice in more detail and using a lower-level implementation based on Kubernetes. These two approaches complement each other nicely and will help you widen your ML engineering toolkit.</p>
<p class="normal">Deploying our service on ECS will require a few different components, which we will walk through in the next few sections:</p>
<ul>
<li class="bulletList">Our container hosted inside a repository on ECR</li>
<li class="bulletList">A cluster and service created on ECS</li>
<li class="bulletList">An application<a id="_idIndexMarker616"/> load balancer created via the <strong class="keyWord">Elastic Compute Cloud</strong> (<strong class="keyWord">EC2</strong>) service</li>
</ul>
<p class="normal">First, let’s tackle pushing<a id="_idIndexMarker617"/> the container to ECR.</p>
<h2 class="heading-2" id="_idParaDest-129">Pushing to ECR</h2>
<p class="normal">Let’s look at the following<a id="_idIndexMarker618"/> steps:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We have the following Dockerfile defined within the project directory from the <em class="italic">Containerizing</em> section:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.10</span>-slim as builder
<span class="hljs-keyword">COPY</span> . /src
<span class="hljs-keyword">RUN</span> pip install --user --no-cache-dir -r requirements.txt
<span class="hljs-keyword">FROM</span> python:<span class="hljs-number">3.10</span>-slim as app
<span class="hljs-keyword">COPY</span> --from=builder /root/.local /root/.local
<span class="hljs-keyword">COPY</span> --from=builder /src .
<span class="hljs-keyword">ENV</span> PATH=/root/.local:$PATH
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">5000</span>
<span class="hljs-keyword">CMD</span> [<span class="hljs-string">"</span><span class="hljs-string">python3"</span>, <span class="hljs-string">"app.py"</span>]
</code></pre>
</li>
<li class="numberedList">We can then use the AWS <code class="inlineCode">CLI</code> to create an ECR repository for hosting our container. We will call the repository <code class="inlineCode">basic-ml-microservice</code> and will set the region as <code class="inlineCode">eu-west-1</code>, but this should be changed to what region seems most appropriate for your account. The command below will return some metadata about your ECR repository; keep this for later steps:
        <pre class="programlisting con"><code class="hljs-con">aws ecr create-repository 
    --repository-name basic-ml-microservice
    --image-scanning-configuration scanOnPush=true 
    --region eu-west-1
</code></pre>
</li>
<li class="numberedList">We can then log in to the container registry with the following command in the Terminal. Note that the repository URI will have been in the metadata provided after running step <em class="italic">2</em>. You can also retrieve this by running <code class="inlineCode">aws ecr describe-repositories --region eu-west-1</code>:
        <pre class="programlisting con"><code class="hljs-con">aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin &lt;ECR_REPOSITORY_URI&gt;
</code></pre>
</li>
<li class="numberedList">Then, if we navigate to the directory containing the <code class="inlineCode">Dockerfile</code> (<code class="inlineCode">app</code>), we can run the following command to build the container:
        <pre class="programlisting con"><code class="hljs-con">docker build --tag basic-ml-microservice:local
</code></pre>
</li>
<li class="numberedList">The next step tags the image:
        <pre class="programlisting con"><code class="hljs-con">docker tag basic-ml-microservice:local &lt;ECR_REPOSITORY_URI&gt;
</code></pre>
</li>
<li class="numberedList">We then deploy the Docker image we have just built to the container registry with the following command:
        <pre class="programlisting con"><code class="hljs-con">docker push &lt;YOUR_AWS_ID&gt;.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest
</code></pre>
</li>
</ol>
<p class="normal">If successful, this last command<a id="_idIndexMarker619"/> will have pushed the locally built Docker image to your remotely hosted ECR repository. You can confirm this by navigating to the AWS management console, going to the ECR service, and selecting the basic-ml-microservice repository. You should then see something like what is shown in <em class="italic">Figure 5.6</em>.</p>
<figure class="mediaobject"><img alt="" height="265" role="presentation" src="../Images/B19525_05_06.png" width="825"/></figure>
<p class="packt_figref">Figure 5.6: Succesful push of the locally built Docker image to the ECR repository.</p>
<p class="normal">The steps we have just gone through are actually quite powerful in general, as you are now able to build cross-platform Docker images and share them in a central repository under your AWS account. You can share Docker containers and images via DockerHub as well, <a href="https://hub.docker.com/"><span class="url">https://hub.docker.com/</span></a>, but this gives you more control if you want to do this inside your own organization.</p>
<p class="normal">Now that we have built the container<a id="_idIndexMarker620"/> that hosts the Flask app, we will now look to deploy this on scalable infrastructure. To do this, in the next section, we will set up our cluster on ECS.</p>
<h2 class="heading-2" id="_idParaDest-130">Hosting on ECS</h2>
<p class="normal">Now, let’s start with the setup! At the time<a id="_idIndexMarker621"/> of writing in mid-2023, AWS has recently introduced a revamped ECS console that allows for a far smoother setup than previously. So, if you read the first edition of this book, you will find this a far smoother experience:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, navigate to <strong class="screenText">ECS</strong> on the AWS Management Console and click <strong class="screenText">Create Cluster</strong>. You will be provided with a form that asks for details about networking, infrastucture, monitoring, and the provision of any tags on the resources we are about to create. This should look like <em class="italic">Figure 5.7</em>.
    <figure class="mediaobject"><img alt="" height="510" role="presentation" src="../Images/B19525_05_07.png" width="602"/></figure>
<p class="packt_figref">Figure 5.7: Creating a cluster in Elastic Container Service.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2">First, we can name the cluster <code class="inlineCode">mlewp2-ecs-cluster</code>, or indeed whatever you want! Then when you expand the <strong class="screenText">Networking</strong> section, you should see that many of the <strong class="screenText">VPC</strong> and subnet details are auto-populated with defaults based on your AWS account setup. If you need to set these up, the form points to the relevant documentation. See <em class="italic">Figure 5.8</em> for an example.
    <figure class="mediaobject"><img alt="" height="369" role="presentation" src="../Images/B19525_05_08.png" width="608"/></figure>
<p class="packt_figref">Figure 5.8: Networking configuration for our cluster in AWS ECS.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">The <strong class="screenText">Infrastructure</strong> section contains<a id="_idIndexMarker622"/> three options, with the use of <strong class="screenText">AWS Fargate</strong> being the pre-selected default option. We do not need to know the details of how Fargate works but suffice it to say that this provides a very high-level abstraction for managing container workloads across multiple servers. The introduction of Fargate has meant that you do not need to worry about details of the provisioning and running of clusters of virtual machines to run your container workloads. According to the AWS documentation, the Fargate service is ideal for dynamic bursts of work or large workloads with low operational overhead. If you know you are going to be running large jobs that have to be price optimized, you can then look to the other infra options provided, for example, <strong class="screenText">EC2 instances</strong>. We will not need these for the purposes of this example. <em class="italic">Figure 5.9</em> shows the <strong class="screenText">Infrastructure</strong> section for reference.
    <figure class="mediaobject"><img alt="" height="290" role="presentation" src="../Images/B19525_05_09.png" width="762"/></figure>
<p class="packt_figref">Figure 5.9: Configuring the infrastructure options in the ECS service.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">The <strong class="screenText">Monitoring</strong> and <strong class="screenText">Tags</strong> sections<a id="_idIndexMarker623"/> are relatively self-explanatory and allow you to toggle on <strong class="screenText">container insights</strong> and provide your own string tags for the ECS resources that will be created. Let’s leave these as the default for now and click <strong class="screenText">Create</strong> at the bottom of the page. You should then see that the cluster was successfully created after a few minutes, as shown in <em class="italic">Figure 5.10</em>.
    <figure class="mediaobject"><img alt="" height="207" role="presentation" src="../Images/B19525_05_10.png" width="727"/></figure>
<p class="packt_figref">Figure 5.10: The successful creation of the ECS cluster.</p></li>
</ol>
<p class="normal">The previous steps were all about setting up the ECS cluster, the infrastructure on which our containerized application can run. To actually tell ECS how to run the solution, we need to define <strong class="keyWord">tasks</strong>, which are simply processes<a id="_idIndexMarker624"/> we wish to be executed on the cluster. There<a id="_idIndexMarker625"/> is a related concept of <strong class="keyWord">Services</strong> in ECS, which refers to a process for managing your tasks, for example, by ensuring a certain number of tasks are always running on the cluster. This is useful if you have certain uptime requirements for your solution, such as, if it needs to be available for requests 24/7. We can create the task definition in the cluster by first navigating to the cluster review page in the AWS management console, and then selecting <strong class="screenText">Task Definitions</strong> on the left-hand side. We will then click on <strong class="screenText">Create New Task Definition</strong>. Follow the steps below to create this task definition.</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We have to name the task definition family, which is just the collection of versions of the task definition. Let’s call ours <code class="inlineCode">basic-ml-microservice-tasks</code> for simplicity. We then need to provide some container details such as the URI for the image we want to use. This is the URI for the image we pushed to the ECR repository previously, which is formatted something like <code class="inlineCode">&lt;YOUR_AWS_ID&gt;.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest</code>. You can give the container a new name. Here, I have called it <strong class="screenText">mlmicro</strong>. Finally, you need<a id="_idIndexMarker626"/> to supply appropriate port mappings to allow the container and the application it contains to be accessible to external traffic. I have mapped <code class="inlineCode">port 5000</code>, which you may recall is the port we exposed in the original Dockerfile using the TCP protocol. This is all shown in <em class="italic">Figure 5.11</em>. You can leave the rest of the optional settings for this first section as the default just now and click <strong class="screenText">Next</strong> to move on to the next page of settings.
    <figure class="mediaobject"><img alt="" height="435" role="presentation" src="../Images/B19525_05_11.png" width="679"/></figure>
<p class="packt_figref">Figure 5.11: Defining the container image to use for the task definition in ECS.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2">The next page in the console asks for information about the environment and infrastructure you will be running the solution on. Based on the settings we used for the ECS cluster, we will be using Fargate as the infrastructure option, running on a <strong class="screenText">Linux x86_64</strong> environment. The tasks we are running are very small in this case (we’re just returning some numbers for demo purposes) so we can keep the default options of <strong class="screenText">1 vCPU</strong> with <strong class="screenText">3 GB</strong> memory. You can also add container-level memory and CPU requirements if necessary, but we can leave this blank for now. This is particularly useful if you have a computationally heavy service, or it contains an application that is pre-loaded with some large model or configuration data. You can see this in <em class="italic">Figure 5.12</em>.
    <figure class="mediaobject"><img alt="" height="667" role="presentation" src="../Images/B19525_05_12.png" width="669"/></figure>
<p class="packt_figref">Figure 5.12: Configuring our application environment for the AWS ECS task definition used for our ML microservice.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Next, IAM roles<a id="_idIndexMarker627"/> need to be configured. We will not be calling other AWS services from our application, so at this point, we do not need an IAM task role, but you can create one if you need this at a later point, for example, if you wish to call another data or ML service. For executing the tasks we need an execution role, which by default is created for you, so let’s use that. The IAM configuration section is shown in <em class="italic">Figure 5.13</em>.
    <figure class="mediaobject"><img alt="" height="512" role="presentation" src="../Images/B19525_05_13.png" width="759"/></figure>
<p class="packt_figref">Figure 5.13: The IAM roles defined for use by the AWS ECS task definition.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">The rest of this section<a id="_idIndexMarker628"/> contains optional sections for storage, monitoring, and tagging. The storage subsection refers to ephemeral storage used to decompress and host your Docker container. Again, for larger containers, you may need to consider increasing this size from the default 21 GiB. Monitoring can be enabled using <strong class="keyWord">Amazon CloudWatch</strong>, which is useful when you need infrastructure monitoring as part of your solution, but we will not cover that here and focus more on the core deployment. Keep these sections as is for now and click <strong class="screenText">Next</strong> at the bottom of the page.</li>
<li class="numberedList">We are almost there. Now we’ll review and create the task definition. If you are happy with the selections upon reviewing, then create the task definition and you will be taken to a summary page like that shown in <em class="italic">Figure 5.14</em>.
    <figure class="mediaobject"><img alt="" height="331" role="presentation" src="../Images/B19525_05_14.png" width="764"/></figure>
<p class="packt_figref">Figure 5.14: Successful creation of the ML microservice task definition.</p></li>
</ol>
<p class="normal">Now, the final step of setting<a id="_idIndexMarker629"/> up our ECS-hosted solution is the creation of a service. We will now walk through how to do this:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, navigate to the task definition we have just created in the previous steps and select the <strong class="screenText">Deploy</strong> button. This will provide a dropdown where you can select <strong class="screenText">Create service</strong>. <em class="italic">Figure 5.15</em> shows you what this looks like as it may be easy to miss.
    <figure class="mediaobject"><img alt="" height="278" role="presentation" src="../Images/B19525_05_15.png" width="759"/></figure>
<p class="packt_figref">Figure 5.15: Selecting the Create service option for the task definition we have just created in the previous steps.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2">You will then be taken to another page where we need to fill in the details of the service we wish to create. For <strong class="screenText">Existing cluster</strong>, select the ECS cluster we defined before, which for this example was called <strong class="screenText">mlewp2-ecs-cluster</strong>. For <strong class="screenText">Compute configuration</strong>, we will just use the <strong class="screenText">Launch type</strong> option, which means we can just allow Fargate to manage the infrastructure requirements. If you have multiple infrastructure options that you want to blend together, then you can use the <strong class="screenText">Capacity provider strategy</strong> option. Note that this is more advanced and so I encourage you to read more in the AWS documentation about your options here if you need to use this route. For reference, my selections are shown in <em class="italic">Figure 5.16</em>.
    <figure class="mediaobject"><img alt="" height="535" role="presentation" src="../Images/B19525_05_16.png" width="759"/></figure>
<p class="packt_figref">Figure 5.16: AWS ECS selections for the environment that we will run our ECS service on. This service will enable the task definition we defined before, and therefore our application, to run continuously.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Next is the deployment<a id="_idIndexMarker630"/> configuration, which refers to how the service runs in terms of the number of replicas and what actions to take upon failures of the solution. I have defined the service name simply as <strong class="screenText">basic-ml-microservice-service</strong>, and have used the <strong class="screenText">Replica</strong> service type, which specifies how many tasks should be maintained across the cluster. We can leave this as <strong class="screenText">1</strong> for now as we only have one task in our task definition. This is shown in <em class="italic">Figure 5.17</em>.
    <figure class="mediaobject"><img alt="" height="634" role="presentation" src="../Images/B19525_05_17.png" width="759"/></figure>
<p class="packt_figref">Figure 5.17: Configuring the AWS ECS service name and type.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">The <strong class="screenText">Deployment options</strong> and <strong class="screenText">Deployment failure detection</strong> subsections will be auto-populated <a id="_idIndexMarker631"/>with some defaults. A rolling deployment type refers to the replacement<a id="_idIndexMarker632"/> of the container with the latest version when that is available. The failure detection options ensure that deployments that run into errors fail to proceed and that rollbacks to previous versions are enabled. We do not need to enable <strong class="screenText">CloudWatch alarms</strong> at this stage as we have not configured CloudWatch, but this could be added in future iterations of your project. See <em class="italic">Figure 5.18</em> for reference.
    <figure class="mediaobject"><img alt="" height="634" role="presentation" src="../Images/B19525_05_18.png" width="756"/></figure>
<p class="packt_figref">Figure 5.18: Deployment and failure detection options for the AWS ECS service we are about to deploy.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5">As in the other<a id="_idIndexMarker633"/> examples, there is a <strong class="screenText">Networking</strong> section that should be prepopulated with the VPC and subnet information appropriate for your account. As before, you can switch these out for specific VPCs and subnets according to your requirements. <em class="italic">Figure 5.19</em> shows what this looks like for reference.
    <figure class="mediaobject"><img alt="" height="807" role="presentation" src="../Images/B19525_05_19.png" width="759"/></figure>
<p class="packt_figref">Figure 5.19: The networking section for the AWS ECS service that we are defining for hosting the ML microservice.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="6">The remaining sections<a id="_idIndexMarker634"/> are optional and contain configuration elements for load balancing, auto-scaling, and tagging. Although we do not necessarily need it for such a simple application, we will use this section to create an application load balancer, which is one of the options available. An application load balancer routes HTTP and HTTPS requests and supports useful capabilities like path-based routing and dynamic host port mapping, which allows for multiple tasks from a single service to run on the same container. We can name the load balancer <code class="inlineCode">basic-ml-microservice-lb</code> and configure the <strong class="screenText">listener</strong> for this load balancer to listen on <code class="inlineCode">port 80</code> with the HTTP protocol, as shown in <em class="italic">Figure 5.20</em>. This listener checks for connection requests at the given port and uses the specified protocol so that requests can then be routed by the load balancer to the downstream system.
    <figure class="mediaobject"><img alt="" height="714" role="presentation" src="../Images/B19525_05_20.png" width="687"/></figure>
<p class="packt_figref">Figure 5.20: Defining the load balancer name and listener details for the AWS ECS service.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7">Finally, we must specify<a id="_idIndexMarker635"/> a target group for the load balancer, which as the name suggests is basically the collection of target endpoints for the tasks in your service. AWS ECS ensures that this updates as task definitions are updated through the lifetime of your service. <em class="italic">Figure 5.21</em> shows the configurations for the target group, which just specifies the HTTP protocol and home path for health checks.
    <figure class="mediaobject"><img alt="" height="474" role="presentation" src="../Images/B19525_05_21.png" width="764"/></figure>
<p class="packt_figref">Figure 5.21: Target group definition for the application load balancer.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="8">After filling<a id="_idIndexMarker636"/> in these details, hit the <strong class="screenText">Create</strong> button. This will then deploy your service. If all has gone well, then you should be able to see the service in your cluster details on the AWS ECS console page. You can navigate<a id="_idIndexMarker637"/> to this service and find the load balancer. This will have a <strong class="keyWord">Domain Name System</strong> (<strong class="keyWord">DNS</strong>) address that will be the root of the target URL for sending requests. <em class="italic">Figure 5.22</em> shows what this page with the DNS looks like. Copy or save this DNS value.
    <figure class="mediaobject"><img alt="" height="441" role="presentation" src="../Images/B19525_05_22.png" width="759"/></figure>
<p class="packt_figref">Figure 5.22: The deployed load balancer for our service with the DNS name in the bottom right-hand corner.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="9">Finally, to test the service, we can<a id="_idIndexMarker638"/> run the same request we had for local testing in Postman, but now update the URL to contain the load balancer DNS name and the port we have specified that the load balancer will receive oncoming traffic with . For us, this is port 80. This is shown with the application response in <em class="italic">Figure 5.23</em>.
    <figure class="mediaobject"><img alt="" height="768" role="presentation" src="../Images/B19525_05_23.png" width="758"/></figure>
<p class="packt_figref">Figure 5.23: A valid result is returned by our simple forecasting service from the hosted application AWS ECS.</p></li>
</ol>
<p class="normal">And that’s it! We have now successfully built and deployed a simplified forecasting service using Flask, Docker, AWS Elastic Container Registry, AWS Elastic Container Service, and an application load balancer. These components can all be adapted for deploying your future ML microservices.</p>
<p class="normal">The first half of this chapter<a id="_idIndexMarker639"/> has been about architectural and design principles that apply at the system and code level, as well as showing you how some of this comes together in one mode of deployment that is very common for ML systems, that of the ML microservice. Now that we have done this, we will move on to discuss some tools and techniques that allow us to build, deploy, and host complex ML workflows as pipelines, a concept we briefly introduced earlier in the book. The tools and concepts we will cover in the second half of this chapter are crucial for any modern ML engineer to have a strong grasp of, as they are starting to form the backbone of so many deployed ML systems.</p>
<p class="normal">The next section will start this discussion<a id="_idIndexMarker640"/> with an exploration of how we can use Airflow to create and orchestrate flexible, general-purpose, production-ready pipelines, before we move on to some tools aimed specifically at advanced ML pipelining and orchestration.</p>
<h1 class="heading-1" id="_idParaDest-131">Building general pipelines with Airflow</h1>
<p class="normal">In <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>, we discussed<a id="_idIndexMarker641"/> the benefits of writing our ML code<a id="_idIndexMarker642"/> as pipelines. We discussed how to implement<a id="_idIndexMarker643"/> some basic ML pipelines using tools such as <code class="inlineCode">sklearn</code> and <strong class="keyWord">Spark ML</strong>. The pipelines we were concerned<a id="_idIndexMarker644"/> with there were very nice ways of streamlining your code and making several processes available to use within a single object to simplify an application. However, everything we discussed then was very much focused on one Python file and not necessarily something we could extend very flexibly outside the confines of the package we were using. With the techniques we discussed, for example, it would be very difficult to create pipelines where each step was using a different package or even where they were entirely different programs. They did not allow us to build much sophistication into our data flows or application logic either, as if one of the steps failed, the pipeline failed, and that was that.</p>
<p class="normal">The tools we are about to discuss take these concepts to the next level. They allow you to manage the workflows of your ML solutions so that you can organize, coordinate, and orchestrate elements with the appropriate level of complexity to get the job done.</p>
<h2 class="heading-2" id="_idParaDest-132">Airflow</h2>
<p class="normal"><strong class="keyWord">Apache Airflow</strong> is the workflow management<a id="_idIndexMarker645"/> tool that was initially developed by <strong class="keyWord">Airbnb</strong> in the 2010s and has been open-source since its inception. It gives data scientists, data engineers, and ML engineers the capability of programmatically creating complex pipelines through Python scripts. Airflow’s task management<a id="_idIndexMarker646"/> is based on the definition and then execution of a <strong class="keyWord">Directed Acyclic Graph</strong> (<strong class="keyWord">DAG</strong>) with nodes as the tasks to be run. DAGs are also used in <strong class="keyWord">TensorFlow</strong> and <strong class="keyWord">Spark</strong>, so you may have heard<a id="_idIndexMarker647"/> of them<a id="_idIndexMarker648"/> before.</p>
<p class="normal">Airflow contains a variety of default operators to allow you to define DAGs that can call and use multiple components as tasks, without caring about the specific details of a task. It also provides functionality for scheduling your pipelines. As an example, let’s build an Apache Airflow pipeline that will get data, perform some feature engineering, train a model, and then persist the model. We won’t cover the detailed implementation of each command, but simply show you how your ML processes hang together in an Airflow DAG. In <em class="chapterRef">Chapter 9</em>, <em class="italic">Building an Extract, Transform, Machine Learning Use Case</em>, we will build out a detailed end-to-end example discussing these lower-level details. This first example is more concerned with understanding the high level of how to write, deploy, and manage your DAGs in the cloud:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, in a file called <code class="inlineCode">classification_pipeline_dag.py</code>, we can import the relevant Airflow packages and any utility packages we need:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> datetime
<span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> timedelta
<span class="hljs-keyword">from</span> airflow <span class="hljs-keyword">import</span> DAG
<span class="hljs-keyword">from</span> airflow.operators.python <span class="hljs-keyword">import</span> PythonOperator
<span class="hljs-keyword">from</span> airflow.utils.dates <span class="hljs-keyword">import</span> days_ago
</code></pre>
</li>
<li class="numberedList">Next, Airflow<a id="_idIndexMarker649"/> allows you to define default arguments that can be referenced by all of the following tasks, with the option to overwrite at the same level:
        <pre class="programlisting code"><code class="hljs-code">default_args = {
    <span class="hljs-string">'owner'</span>: <span class="hljs-string">'Andrew McMahon'</span>,
    <span class="hljs-string">'depends_on_past'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'start_date'</span>: days_ago(<span class="hljs-number">31</span>),
    <span class="hljs-string">'email'</span>: [<span class="hljs-string">'example@example.com'</span>],
    <span class="hljs-string">'email_on_failure'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'email_on_retry'</span>: <span class="hljs-literal">False</span>,
    <span class="hljs-string">'retries'</span>: <span class="hljs-number">1</span>,
    <span class="hljs-string">'retry_delay'</span>: timedelta(minutes=<span class="hljs-number">2</span>)
}
</code></pre>
</li>
<li class="numberedList">We then have to instantiate our DAG and provide the relevant metadata, including our scheduling interval:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> DAG(
    dag_id=<span class="hljs-string">"classification_pipeline"</span>,
    start_date=datetime.datetime(<span class="hljs-number">2021</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>),
    schedule_interval=<span class="hljs-string">"@daily"</span>,
    catchup=<span class="hljs-literal">False</span>,
) <span class="hljs-keyword">as</span> dag:
</code></pre>
</li>
<li class="numberedList">Then, all that is required is to define your tasks within the <code class="inlineCode">DAG</code> definition. First, we define an initial task that gets our dataset. This next piece of code assumes there is a Python executable, for example, a function or class method, called <code class="inlineCode">get_data</code> that we can pass to the task. This could have been imported from any submodule or package we want. Note that <em class="italic">steps</em> <em class="italic">3</em>-<em class="italic">5</em> assume we are inside the code block of the DAG instantiation, so we assume another indent that we don’t show here to save space:
        <pre class="programlisting code"><code class="hljs-code">    get_data_task = PythonOperator(
        task_id=<span class="hljs-string">"get_data"</span>,
        python_callable=get_data
    )
</code></pre>
</li>
<li class="numberedList">We then perform a task<a id="_idIndexMarker650"/> that takes this data and performs our model training steps. This task could, for example, encapsulate one of the pipeline types we covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>; for example, a Spark ML pipeline, <strong class="keyWord">Scikit-Learn</strong> pipeline, or any other ML training pipeline we looked at. Again, we assume there is a Python executable called <code class="inlineCode">train_model</code> that can be used in this step:
        <pre class="programlisting code"><code class="hljs-code">    train_model_task = PythonOperator(
        task_id=<span class="hljs-string">"train_model"</span>,
        python_callable=train_model
    )
</code></pre>
</li>
<li class="numberedList">The final step of this process is a placeholder for taking the resultant trained model and persisting it to our storage layer. This means that other services or pipelines can use this model for prediction:
        <pre class="programlisting code"><code class="hljs-code">    persist_model_task = PythonOperator(
        task_id=<span class="hljs-string">"persist_model"</span>,
        python_callable=persist_model
    )
</code></pre>
</li>
<li class="numberedList">Finally, we define the running order of the task nodes that we have defined in the DAG using the <code class="inlineCode">&gt;&gt;</code> operator. The tasks above could have been defined in any order, but the following syntax stipulates how they must run:
        <pre class="programlisting code"><code class="hljs-code">get_data_task &gt;&gt; train_model_task &gt;&gt; persist_model_task
</code></pre>
</li>
</ol>
<p class="normal">In the next sections, we will briefly<a id="_idIndexMarker651"/> cover how to set up an Airflow pipeline on AWS using the <strong class="keyWord">Managed Workflows for Apache Airflow</strong> (<strong class="keyWord">MWAA</strong>) service. The section after will then show how you can use <strong class="keyWord">CI/CD</strong> principles to continuously <a id="_idIndexMarker652"/>develop and update your Airflow solutions. This will bring together some of the setup and work we have been doing in previous<a id="_idIndexMarker653"/> chapters of the book.</p>
<h3 class="heading-3" id="_idParaDest-133">Airflow on AWS</h3>
<p class="normal">AWS provides<a id="_idIndexMarker654"/> a cloud-hosted service called <strong class="keyWord">Managed Workflows for Apache Airflow</strong> (<strong class="keyWord">MWAA</strong>) that allows you to deploy and host<a id="_idIndexMarker655"/> your Airflow pipelines easily and robustly. Here, we will briefly cover how to do this.</p>
<p class="normal">Complete the following steps:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Select <strong class="screenText">Create an environment</strong> on the MWAA landing page. You can find this by searching for MWAA in the AWS Management Console.</li>
<li class="numberedList">You will then be provided with a screen asking for the details of your new Airflow environment. <em class="italic">Figure 5.24</em> shows the high-level steps that the website takes you through:
    <figure class="mediaobject"><img alt="Figure 5.29 – The high-level steps for setting up an MWAA environment and associated managed Airflow runs " height="327" src="../Images/B19525_05_24.png" width="757"/></figure>
<p class="packt_figref">Figure 5.24: The high-level steps for setting up an MWAA environment and associated managed Airflow runs.</p>
<p class="normal"><strong class="screenText">Environment details</strong>, as shown in <em class="italic">Figure 5.25</em>, is where we specify our environment name. Here, we have called it <strong class="screenText">mlewp2-airflow-dev-env</strong>:</p>
<figure class="mediaobject"><img alt="" height="349" role="presentation" src="../Images/B19525_05_25.png" width="565"/></figure>
<p class="packt_figref">Figure 5.25: Naming your MWAA environment.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">For MWAA to run, it needs<a id="_idIndexMarker656"/> to be able to access code defining the DAG and any associated requirements or plugin files. The system then asks for an AWS S3 bucket where these pieces of code and configuration reside. In this example, we create a bucket called <code class="inlineCode">mlewp2-ch5-airflow-example</code> that will contain these pieces. <em class="italic">Figure 5.26</em> shows the creation of the bucket:
    <figure class="mediaobject"><img alt="" height="181" role="presentation" src="../Images/B19525_05_26.png" width="758"/></figure>
<p class="packt_figref">Figure 5.26: The successful creation of our AWS S3 bucket for storing our Airflow code and supporting configuration elements.</p>
<p class="normal"><em class="italic">Figure 5.27</em> shows how we point MWAA to the correct bucket, folders, and plugins or requirement files if we have them too:</p>
<figure class="mediaobject"><img alt="" height="838" role="presentation" src="../Images/B19525_05_27.png" width="759"/></figure>
<p class="packt_figref">Figure 5.27: We reference the bucket we created in the previous step in the configuration of the MWAA instance.</p>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="4">We then have to define <a id="_idIndexMarker657"/>the configuration of the network that the managed instance of Airflow will use, similar to the other AWS examples in this chapter. This can get a bit confusing if you are new to networking, so it might be good to read around the topics of subnets, IP addresses, and VPCs. Creating a new MWAA VPC is the easiest approach for getting started in terms of networking here, but your organization will have networking specialists who can help you use the appropriate settings for your situation. We will go with this simplest route and click <strong class="screenText">Create MWAA VPC</strong>, which opens a new window where we can quickly spin up a new VPC and network setup based on a standard stack definition provided by AWS. You will be asked for a stack name. I have called mine <code class="inlineCode">MLEWP-2-MWAA-VPC</code>. The networking information will be populated with something like that shown in <em class="italic">Figure 5.28</em>:
    <figure class="mediaobject"><img alt="" height="717" role="presentation" src="../Images/B19525_05_28.png" width="759"/></figure>
<p class="packt_figref">Figure 5.28: An example stack template for creating your new VPC.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5">We are then taken<a id="_idIndexMarker658"/> to a page where we are asked for more details on networking. We can select <strong class="screenText">Public network (No additional setup)</strong> for this example as we will not be too concerned with creating an organizationally aligned security model. For deployments in an organization, work with your security team to understand what additional security you need to put in place. We can also select <strong class="screenText">Create new security group</strong>. This is shown in <em class="italic">Figure 5.29</em>.
    <figure class="mediaobject"><img alt="" height="547" role="presentation" src="../Images/B19525_05_29.png" width="759"/></figure>
<p class="packt_figref">Figure 5.29: Finalizing the networking for our MWAA setup.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="6">Next, we have to define the <strong class="screenText">Environment class</strong> that we want to spin up. Currently, there are three<a id="_idIndexMarker659"/> options. Here, we’ll use the smallest, but you can choose the environment that best suits your needs (always ask the billpayer’s permission!). <em class="italic">Figure 5.30</em> shows that we can select the <strong class="screenText">mw1.small</strong> environment class with a min to max worker count of 1-10. MWAA does allow you to change the environment class after instantiating if you need to, so it can often be better to start small and scale up as needed from a cost point of view. You will also be asked about the number of schedulers you want for the environment. Let’s leave this as the default, <strong class="screenText">2</strong>, for now, but you can go up to 5.
    <figure class="mediaobject"><img alt="" height="601" role="presentation" src="../Images/B19525_05_30.png" width="759"/></figure>
<p class="packt_figref">Figure 5.30: Selecting an environment class and worker sizes.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7">Now, if desired, we confirm<a id="_idIndexMarker660"/> some optional configuration parameters (or leave these blank, as done here) and confirm that we are happy for AWS to create and use a new execution role. We can also just proceed with the default monitoring settings. <em class="italic">Figure 5.31</em> shows an example of this (and don’t worry, the security group will have long been deleted by the time you are reading this page!):
    <figure class="mediaobject"><img alt="" height="374" role="presentation" src="../Images/B19525_05_31.png" width="759"/></figure>
<p class="packt_figref">Figure 5.31: The creation of the execution role used by AWS for the MWAA environment.</p></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="8">The next page will supply you with a final summary before allowing you to create your MWAA environment. Once you do this, you will be able to see your newly created environment in the MWAA service, as in <em class="italic">Figure 5.32</em>. This process can take some time, and for this example it took around 30 minutes:
    <figure class="mediaobject"><img alt="" height="212" role="presentation" src="../Images/B19525_05_32.png" width="759"/></figure>
<p class="packt_figref">Figure 5.32: Our newly minted MWAA environment.</p></li>
</ol>
<p class="normal">Now that you have this MWAA environment and you have supplied your DAG to the S3 bucket that it points to, you can open the Airflow UI and see the scheduled jobs defined by your DAG. You have now deployed a basic running service that we can build upon in later work.</p>
<p class="normal">Now we will want to see the DAGs<a id="_idIndexMarker661"/> in the Airflow UI so that we can orchestrate and monitor the jobs. To do this, you may need to configure access for your own account to the MWAA UI using the details outlined on the AWS documentation pages. As a quick summary, you need to go to the IAM service on AWS. You will need to be logged in as a root user, and then create a new policy title, <strong class="keyWord">AmazonMWAAWebServerAccess</strong>. Give this policy the following JSON body:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"Version"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"2012-10-17"</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">"Statement"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
        <span class="hljs-punctuation">{</span>
            <span class="hljs-attr">"Effect"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"</span><span class="hljs-string">Allow"</span><span class="hljs-punctuation">,</span>
            <span class="hljs-attr">"Action"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"airflow:CreateWebLoginToken"</span><span class="hljs-punctuation">,</span>
            <span class="hljs-attr">"Resource"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
                <span class="hljs-string">"arn:aws:airflow:{your-region}:YOUR_ACCOUNT_ID:role/{your-</span>                 <span class="hljs-string">environment-name}/{airflow-role}"</span>
            <span class="hljs-punctuation">]</span>
        <span class="hljs-punctuation">}</span>
    <span class="hljs-punctuation">]</span>
<span class="hljs-punctuation">}</span>
</code></pre>
<p class="normal">For this definition, the Airflow role<a id="_idIndexMarker662"/> refers to one of the five roles of <strong class="keyWord">Admin</strong>, <strong class="keyWord">Op</strong>, <strong class="keyWord">Viewer</strong>, <strong class="keyWord">User</strong>, or <strong class="keyWord">Public</strong>, as defined<a id="_idIndexMarker663"/> in the Airflow documentation at <a href="https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml"><span class="url">https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml</span></a>. I have used the Admin role for this example. If you add this policy to the permissions of your account, you should be able to access the Airflow UI by clicking the <strong class="screenText">Open Airflow UI</strong> button in the MWAA service. You will then be directed to the Airflow UI, as shown in <em class="italic">Figure 5.33</em>.</p>
<figure class="mediaobject"><img alt="" height="235" role="presentation" src="../Images/B19525_05_33.png" width="825"/></figure>
<p class="packt_figref">Figure 5.33: The Airflow UI accessed via the AWS MWAA service. This view shows the classification DAG that we wrote earlier in the example.</p>
<p class="normal">The Airflow UI<a id="_idIndexMarker664"/> allows you to trigger DAG runs, manage the jobs that you have scheduled, and monitor and troubleshoot your pipelines. As an example, upon a successful run, you can see summary information for the runs, as shown in <em class="italic">Figure 5.34</em>, and can use the different views to understand the time taken for each of the pipeline steps and diagnose where any issues have arisen if there are errors raised.</p>
<figure class="mediaobject"><img alt="" height="442" role="presentation" src="../Images/B19525_05_34.png" width="825"/></figure>
<p class="packt_figref">Figure 5.34: Example run summary for our simple classification DAG in the Airflow UI.</p>
<p class="normal">The pipeline we have built and run<a id="_idIndexMarker665"/> in this example is obviously very simple, with only core Python functionality being used. If you want to leverage other AWS services, for example, by submitting a Spark job to an EMR cluster, then you will need to configure further access policies like the one we did above for the UI access. This is covered in the MWAA documentation.</p>
<div class="note">
<p class="normal">IMPORTANT NOTE</p>
<p class="normal">Once you have created this MWAA environment, you cannot pause it, as it costs a small amount to run per hour (around 0.5 USD per hour for the environment configuration above). MWAA does not currently contain a feature for pausing and resuming an environment, so you will have to delete the environment and re-instantiate<a id="_idIndexMarker666"/> a new one with the same configuration<a id="_idIndexMarker667"/> when required. This can be automated using tools such as <strong class="keyWord">Terraform</strong> or <strong class="keyWord">AWS</strong> <strong class="keyWord">CloudFormation</strong>, which we will not cover here. So, a word of warning – <em class="italic">DO NOT ACCIDENTALLY LEAVE YOUR ENVIRONMENT RUNNING</em>. For example, definitely do not leave it running for a week, like I may or may not have done.</p>
</div>
<h3 class="heading-3" id="_idParaDest-134">Revisiting CI/CD for Airflow</h3>
<p class="normal">We introduced the basics of CI/CD in <em class="chapterRef">Chapter 2</em>, <em class="italic">The Machine Learning Development Process</em>, and discussed how<a id="_idIndexMarker668"/> this can be achieved by using <strong class="keyWord">GitHub Actions</strong>. We will now take this a step further<a id="_idIndexMarker669"/> and start to set up CI/CD pipelines that deploy code to the cloud.</p>
<p class="normal">First, we will start with an important example where we will push some code to an AWS S3 bucket. This can be done by creating a <code class="inlineCode">.yml</code> file in your GitHub repo under your <code class="inlineCode">.github./workflows</code> directory called <code class="inlineCode">aws-s3-deploy.yml</code>. This will be the nucleus around which we will form our CI/CD pipeline.</p>
<p class="normal">The .<code class="inlineCode">yml</code> file, in our case, will upload the Airflow DAG and contain the following pieces:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We name the process using the syntax for <code class="inlineCode">name</code> and express that we want the deployment process to be triggered on a push to the main branch or a pull request to the main branch:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">name:</span> <span class="hljs-string">Upload</span> <span class="hljs-string">DAGS</span> <span class="hljs-string">to</span> <span class="hljs-string">S3</span>
<span class="hljs-attr">on:</span>
  <span class="hljs-attr">push:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">main</span> ]
  <span class="hljs-attr">pull_request:</span>
    <span class="hljs-attr">branches:</span> [ <span class="hljs-string">main</span> ]
</code></pre>
</li>
<li class="numberedList">We then define the jobs we want to occur during the deployment process. In this case, we want to upload our DAG files to an S3 bucket we have already created, and we want to use the appropriate AWS credentials we have configured in our GitHub secrets store:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">jobs:</span>
  <span class="hljs-attr">deploy:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">Upload</span> <span class="hljs-string">DAGS</span> <span class="hljs-string">to</span> <span class="hljs-string">Amazon</span> <span class="hljs-string">S3</span>
    <span class="hljs-attr">runs-on:</span> <span class="hljs-string">ubuntu-latest</span>
    <span class="hljs-attr">steps:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Checkout</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">actions/checkout@v2</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Configure</span> <span class="hljs-string">AWS</span> <span class="hljs-string">credentials</span> <span class="hljs-string">from</span> <span class="hljs-string">account</span>
        <span class="hljs-attr">uses:</span> <span class="hljs-string">aws-actions/configure-aws-credentials@v1</span>
        <span class="hljs-attr">with:</span>
          <span class="hljs-attr">aws-access-key-id:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.AWS_ACCESS_KEY_ID</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">aws-secret-access-key:</span> <span class="hljs-string">${{</span> <span class="hljs-string">secrets.AWS_SECRET_ACCESS_KEY</span> <span class="hljs-string">}}</span>
          <span class="hljs-attr">aws-region:</span> <span class="hljs-string">us-east-1</span>
</code></pre>
<p class="normal">Then, as part of the job, we run the step that copies the relevant files to our specified AWS S3 bucket. In this case, we are also specifying some details about how to make <a id="_idIndexMarker670"/>the copy using the AWS CLI. Specifically, here we want to copy over all the Python files to the <code class="inlineCode">dags</code> folder of the repo:</p>
<pre class="programlisting code"><code class="hljs-code">      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">Copy</span> <span class="hljs-string">files</span> <span class="hljs-string">to</span> <span class="hljs-string">bucket</span> <span class="hljs-string">with</span> <span class="hljs-string">the</span> <span class="hljs-string">AWS</span> <span class="hljs-string">CLI</span>
        <span class="hljs-attr">run:</span> <span class="hljs-string">|</span>
          <span class="hljs-string">aws</span> <span class="hljs-string">s3</span> <span class="hljs-string">cp</span> <span class="hljs-string">./dags</span> <span class="hljs-string">s3://github-actions-ci-cd-tests</span>
          <span class="hljs-string">--recursive--include</span> <span class="hljs-string">"*.py"</span>
</code></pre> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Once we perform a <code class="inlineCode">git push</code> command with updated code, this will then execute the action and push the <code class="inlineCode">dag</code> Python code to the specified S3 bucket. In the GitHub UI, you will be able to see something like <em class="italic">Figure 5.35</em> on a successful run:
    <figure class="mediaobject"><img alt="Figure 5.38 – A successful CI/CD process run via GitHub Actions and using the AWS CLI " height="596" src="../Images/B19525_05_35.png" width="741"/></figure>
<p class="packt_figref">Figure 5.35: A successful CI/CD process run via GitHub Actions and using the AWS CLI.</p></li>
</ol>
<p class="normal">This process then allows<a id="_idIndexMarker671"/> you to successfully push new updates to your Airflow service into AWS to be run by your MWAA instance. This is real CI/CD and allows you to continually update the service you are providing without downtime.</p>
<h1 class="heading-1" id="_idParaDest-135">Building advanced ML pipelines</h1>
<p class="normal">We have already discussed<a id="_idIndexMarker672"/> in this chapter how <strong class="keyWord">SciKit-learn</strong> and <strong class="keyWord">Spark ML</strong> provide mechanisms for creating ML pipelines. You can think of these as the basic way to do this and to get started. There are a series of tools now available, both open-source and enterprise, that take this concept to the next level.</p>
<p class="normal">For awareness, the three main public cloud providers<a id="_idIndexMarker673"/> have tools in this area you may want to be aware of and try out. <strong class="keyWord">Amazon SageMaker</strong> is one of the giants of this space and contains within it a large ecosystem of tools and capabilities to help take your ML models into production. This book could have been entirely about Amazon SageMaker, but since that was done elsewhere, in <em class="italic">Learn Amazon SageMaker</em>, <a href="https://tinyurl.com/mr48rsxp"><span class="url">https://tinyurl.com/mr48rsxp</span></a>, we will leave the details for the reader to discover. The key thing you need to know is that this is AWS’s managed service for building up ML pipelines, as well as monitoring, model registry, and a series of other capabilities in a way that lets you develop <a id="_idIndexMarker674"/>and promote your models all the way through the ML lifecycle. <strong class="keyWord">Google Vertex AI</strong> is the Google Cloud Platform ML pipelining, development, and deployment tool. It brings tons of functionality under one UI and API, like Sagemaker, but seems to have less flexibility on the types<a id="_idIndexMarker675"/> of models you can train. <strong class="keyWord">Azure ML</strong> is the Microsoft cloud provider’s offering.</p>
<p class="normal">These are all enterprise-grade solutions that you can try for free, but you should be prepared to have your credit card ready when things scale up. The solutions above are also naturally tied into specific cloud providers and therefore can create “vendor lock-in,” where it becomes difficult to switch later. Thankfully, there are solutions that help with this and allow ML engineers to work with a less complex setup and then migrate to more complex infrastructure and environments later. The first one of these that we will discuss is <strong class="keyWord">ZenML</strong>.</p>
<h2 class="heading-2" id="_idParaDest-136">Finding your ZenML</h2>
<p class="normal"><strong class="keyWord">ZenML</strong> is a completely open-source<a id="_idIndexMarker676"/> framework that helps<a id="_idIndexMarker677"/> you write ML pipelines in a way that is totally abstracted from the underlying infrastructure. This means that your local development environment and your eventual production environment can be very different, and can be changed through changes in configuration without altering the core of your pipelines. This is a very powerful idea and is one of ZenML’s key strengths.</p>
<p class="normal">ZenML has some core concepts that you need to understand in order to get the best out of the tool:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Pipelines</strong>: As you might expect given<a id="_idIndexMarker678"/> the discussion in the rest of this chapter, these are the definitions of the steps in the ML workflow. Pipelines consist of “steps” chained together in a specified order.</li>
<li class="bulletList"><strong class="keyWord">Stacks</strong>: Configurations specifying<a id="_idIndexMarker679"/> the environment and infrastructure that the pipeline is to run on.</li>
<li class="bulletList"><strong class="keyWord">Orchestrator</strong>: Within the stack definition, there<a id="_idIndexMarker680"/> are two key components, the first of which is an orchestrator. Its job is to coordinate the steps in the pipeline that are executed on the infrastructure. This could be the default orchestrator that comes with the distribution or it could be something like Airflow or the Kubeflow orchestrator. Airflow is described in the <em class="italic">Building general pipelines with Airflow</em> section in this chapter and Kubeflow is covered in the <em class="italic">Going with the Kubeflow</em> section.</li>
<li class="bulletList"><strong class="keyWord">Artifact store</strong>: This is the stack<a id="_idIndexMarker681"/> component responsible for data and metadata storage. ZenML has a series of different compatible artifact stores out of the box, specifically AWS S3, Azure Blob Storage, and Google Cloud Storage. The assumption here is that the artifact store is really just a storage layer, and nothing too complex on top of that.</li>
</ul>
<p class="normal">So far, so straightforward. Let’s get on and start setting ZenML up. You can install it with:</p>
<pre class="programlisting con"><code class="hljs-con">pip install zenml
</code></pre>
<p class="normal">We will also want to use the React dashboard that comes with ZenML, but to run this locally you also need to install a different repository:</p>
<pre class="programlisting con"><code class="hljs-con">pip install zenml[server]
</code></pre>
<p class="normal">ZenML also comes with a series of existing templates you can leverage, which you can install with:</p>
<pre class="programlisting con"><code class="hljs-con">pip install zenml[templates]
</code></pre>
<p class="normal">We can then start working with a template by running:</p>
<pre class="programlisting con"><code class="hljs-con">zenml init —-template
</code></pre>
<p class="normal">This will then start a terminal-based wizard<a id="_idIndexMarker682"/> to help you generate the ZenML template. See <em class="italic">Figure 5.36</em>.</p>
<p class="packt_figref"><img alt="" height="134" role="presentation" src="../Images/B19525_05_36.png" width="820"/></p>
<p class="packt_figref">Figure 5.36: The ZenML template wizard.</p>
<p class="normal">Hit <em class="keystroke">Enter</em>; then you<a id="_idIndexMarker683"/> will be asked a series of questions to help configure the template. Some are shown with their answers in <em class="italic">Figure 5.37</em>.</p>
<p class="packt_figref"><img alt="" height="310" role="presentation" src="../Images/B19525_05_37.png" width="795"/></p>
<p class="packt_figref">Figure 5.37: Providing responses for the ZenML template definition.</p>
<p class="normal">The next series of questions start to get very interesting as we are asked about the details of the information we wish to be logged and made visible in the CLI, as well as selecting the dataset and model type. Here we will work with the <code class="inlineCode">Wine</code> dataset, again using a <code class="inlineCode">RandomForestClassifier</code>, as can be seen in <em class="italic">Figure 5.38</em>.</p>
<p class="packt_figref"><img alt="" height="289" role="presentation" src="../Images/B19525_05_38.png" width="828"/></p>
<p class="packt_figref">Figure 5.38: Selecting a model for the ZenML template instantiation.</p>
<p class="normal">ZenML will then start<a id="_idIndexMarker684"/> initializing the template for you. You can<a id="_idIndexMarker685"/> see that this process generates a lot of new files to use, as shown in <em class="italic">Figure 5.39</em>.</p>
<figure class="mediaobject"><img alt="" height="452" role="presentation" src="../Images/B19525_05_39.png" width="435"/></figure>
<p class="packt_figref">Figure 5.39: File and folder structure generated after using the ZenML template generation wizard.</p>
<p class="normal">Let’s start to explore some of these elements <a id="_idIndexMarker686"/>for the ZenML solution. First, let’s look at <code class="inlineCode">pipelines/model_training.py</code>. This is a short script that is there to give<a id="_idIndexMarker687"/> you a starting point. Omitting the comments in the file, we have the following code present:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.pipelines <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">@pipeline()</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">model_training_pipeline</span>(
<span class="hljs-params">    data_loader,</span>
<span class="hljs-params">    data_processor,</span>
<span class="hljs-params">    data_splitter,</span>
<span class="hljs-params">    model_trainer,</span>
<span class="hljs-params">    model_evaluator,</span>
):
    dataset = data_loader()
    processed_dataset = data_processor(dataset=dataset)
    train_set, test_set = data_splitter(dataset=processed_dataset)
    model = model_trainer(train_set=train_set)
    model_evaluator(
        model=model,
        train_set=train_set,
        test_set=test_set,
    )
</code></pre>
<p class="normal">We can already start to appreciate some of the features that are available in ZenML and how it works. First, we see that the use of the <code class="inlineCode">@pipeline</code> decorator signals that the function following will contain the main pipeline logic. We can also see that the pipeline is actually written in pure Python syntax; all you need is the decorator to make it “Zen.” This is a very powerful feature of ZenML as it provides you the flexibility to work as you want but still leverage the downstream abstraction we will see soon for deployment targets. The steps inside the pipeline are just dummy function calls created when the template was initialized to help guide you in what you should develop.</p>
<p class="normal">Now, we will look at the pipeline<a id="_idIndexMarker688"/> steps, which have been defined in the <code class="inlineCode">steps/data_loaders.py</code> and <code class="inlineCode">steps/model_trainers.py</code> files. In our discussions of these<a id="_idIndexMarker689"/> modules, we will not discuss the helper classes and utility functions used; these are left for the reader to play around with. Instead, we will focus on the pieces that show the most important ZenML functionality. Before we do that, let us briefly discuss some important ZenML modules that are imports at the top of the module:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.enums <span class="hljs-keyword">import</span> StrEnum
<span class="hljs-keyword">from</span> zenml.logger <span class="hljs-keyword">import</span> get_logger
<span class="hljs-keyword">from</span> zenml.steps <span class="hljs-keyword">import</span> (
   BaseParameters,
   Output,
   step,
)
</code></pre>
<p class="normal">The first import brings in <code class="inlineCode">StrEnum</code> from the <code class="inlineCode">enums</code> module of ZenML. This is a collection of Python enumerations that have been defined to help with specific elements of building ZenML workflows.</p>
<div class="note">
<p class="normal">IMPORTANT NOTE</p>
<p class="normal">Recall that a Python enumeration (or <code class="inlineCode">enum</code>) is a collection of members with unique values that can be iterated over to return the values in their order of definition. You can think of these as somewhere between a class and a dictionary. First, in the <code class="inlineCode">data_loaders.py</code> module, we can see that the first step wraps simple logic for pulling in different datasets from <code class="inlineCode">scikit-learn</code>, depending on the parameters supplied. This is a very basic example but can be updated to incorporate much more sophisticated behavior like calling out to databases or pulling from cloud-hosted object storage. The method looks like the following:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">data_loader</span>(
<span class="hljs-params">    params: DataLoaderStepParameters,</span>
    ) -&gt; pd.DataFrame:
    <span class="hljs-comment"># Load the dataset indicated in the step parameters and format it as a</span>
    <span class="hljs-comment"># pandas DataFrame</span>
    <span class="hljs-keyword">if</span> params.dataset == SklearnDataset.wine:
        dataset = load_wine(as_frame=<span class="hljs-literal">True</span>).frame
    <span class="hljs-keyword">elif</span> params.dataset == SklearnDataset.iris:
        dataset = load_iris(as_frame=<span class="hljs-literal">True</span>).frame
    <span class="hljs-keyword">elif</span> params.dataset == SklearnDataset.breast_cancer:
        dataset = load_breast_cancer(as_frame=<span class="hljs-literal">True</span>).frame
    <span class="hljs-keyword">elif</span> params.dataset == SklearnDataset.diabetes:
        dataset = load_diabetes(as_frame=<span class="hljs-literal">True</span>).frame
    logger.info(<span class="hljs-string">f"Loaded dataset </span><span class="hljs-subst">{params.dataset.value}</span><span class="hljs-string">:</span>
                                 <span class="hljs-string">%s"</span>, dataset.info())
    logger.info(dataset.head())
    <span class="hljs-keyword">return</span> dataset
</code></pre>
</div>
<p class="normal">Note that the output of this function<a id="_idIndexMarker690"/> is a pandas DataFrame, and in the language of ZenML this is an artifact. The next important step given is data processing. The example<a id="_idIndexMarker691"/> given in the template looks like the following:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">data_processor</span>(
<span class="hljs-params">    params: DataProcessorStepParameters,</span>
<span class="hljs-params">    dataset: pd.DataFrame,</span>
    ) -&gt; pd.DataFrame:
    <span class="hljs-keyword">if</span> params.drop_na:
        <span class="hljs-comment"># Drop rows with missing values</span>
        dataset = dataset.dropna()
    <span class="hljs-keyword">if</span> params.drop_columns:
        <span class="hljs-comment"># Drop columns</span>
        dataset = dataset.drop(columns=params.drop_columns)
    <span class="hljs-keyword">if</span> params.normalize:
        <span class="hljs-comment"># Normalize the data</span>
        target = dataset.pop(<span class="hljs-string">"target"</span>)
        dataset = (dataset - dataset.mean()) / dataset.std()
        dataset[<span class="hljs-string">"target"</span>] = target
    <span class="hljs-keyword">return</span> dataset
</code></pre>
<p class="normal">We can see that, here, the processing is relatively standard and will drop <code class="inlineCode">NULL</code> values in the dataset, remove columns we have labeled in the <code class="inlineCode">DataProcessingStepParameters</code> classes (not shown here), and apply some normalization using standard scaling – the steps given are in fact identical to applying the <code class="inlineCode">sklearn.preprocessing.StandardScaler</code> method.</p>
<p class="normal">The final method in the data loaders<a id="_idIndexMarker692"/> module performs train/test splitting<a id="_idIndexMarker693"/> of the data, using methods we have already seen in this book:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@step</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">data_splitter</span>(
<span class="hljs-params">    params: DataSplitterStepParameters,</span>
<span class="hljs-params">    dataset: pd.DataFrame,</span>
    ) -&gt; Output(train_set=pd.DataFrame, test_set=pd.DataFrame,):
    <span class="hljs-comment"># Split the dataset into training and dev subsets</span>
    train_set, test_set = train_test_split(
        dataset,
        test_size=params.test_size,
        shuffle=params.shuffle,
        stratify=dataset[<span class="hljs-string">"target"</span>] <span class="hljs-keyword">if</span> params.stratify <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,
        random_state=params.random_state,
    )
    <span class="hljs-keyword">return</span> train_set, test_set
</code></pre>
<p class="normal">Now, moving back into the <code class="inlineCode">steps</code> folder, we can see that there is also a module entitled <code class="inlineCode">model_trainers.py</code>. At the top of this folder are some more important imports we should understand before we proceed:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.enums <span class="hljs-keyword">import</span> StrEnum
<span class="hljs-keyword">from</span> zenml.logger <span class="hljs-keyword">import</span> get_logger
<span class="hljs-keyword">from</span> zenml.steps <span class="hljs-keyword">import</span> (
    BaseParameters,
    Output,
    step,
)
<span class="hljs-keyword">from</span> artifacts <span class="hljs-keyword">import</span> ModelMetadata
<span class="hljs-keyword">from</span> materializers <span class="hljs-keyword">import</span> ModelMetadataMaterializer
logger = get_logger(__name__)
</code></pre>
<p class="normal">In particular, we can see that ZenML provides a wrapper to the Python logging library and that there are two modules being used here, called <code class="inlineCode">artifacts</code> and <code class="inlineCode">materializers</code>. These are defined within the template repo and show how you can create custom code to work with the artifact store. Specifically, in the <code class="inlineCode">artifacts/model_metadata.py</code> module, there is a class that allows you to store model metadata in a format of your choosing for later serialization and deserialization. Once again, all docstrings and most imports are omitted for brevity:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>
<span class="hljs-keyword">from</span> sklearn.base <span class="hljs-keyword">import</span> ClassifierMixin

<span class="hljs-keyword">class</span> <span class="hljs-title">ModelMetadata</span>:
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        self.metadata: <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Any</span>] = {}
    <span class="hljs-keyword">def</span> <span class="hljs-title">collect_metadata</span>(
<span class="hljs-params">        self,</span>
<span class="hljs-params">        model: ClassifierMixin,</span>
<span class="hljs-params">        train_accuracy: </span><span class="hljs-built_in">float</span><span class="hljs-params">,</span>
<span class="hljs-params">        test_accuracy: </span><span class="hljs-built_in">float</span><span class="hljs-params">,</span>
<span class="hljs-params">        </span>) -&gt; <span class="hljs-literal">None</span>:
        self.metadata = <span class="hljs-built_in">dict</span>(
          model_type = model.__class__.__name__,
          train_accuracy = train_accuracy,
          test_accuracy = test_accuracy,
        )

    <span class="hljs-keyword">def</span> <span class="hljs-title">print_report</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-string">"""Print a user-friendly report from the model metadata."""</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"""</span>
        <span class="hljs-string">Model type: </span><span class="hljs-subst">{self.metadata.get(</span><span class="hljs-string">'model_type'</span><span class="hljs-subst">)}</span>
        <span class="hljs-string">Accuracy on train set: </span><span class="hljs-subst">{self.metadata.get(</span><span class="hljs-string">'train_accuracy'</span><span class="hljs-subst">)}</span>
        <span class="hljs-string">Accuracy on test set: </span><span class="hljs-subst">{self.metadata.get(</span><span class="hljs-string">'test_accuracy'</span><span class="hljs-subst">)}</span>
        <span class="hljs-string">"""</span>)
</code></pre>
<p class="normal">In ZenML, materializers are the objects<a id="_idIndexMarker694"/> that contain the logic for the serialization<a id="_idIndexMarker695"/> and deserialization of the artifacts. They define how your pipelines interact with the artifact store. When defining materializers, you can create custom code but you have to inherit from the <code class="inlineCode">BaseMaterializer</code> class in order to ensure that ZenML knows how to persist and read in data between steps and at the beginning and end of pipelines. This is shown below in important code from <code class="inlineCode">materializers/model_metadata.py</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> zenml.materializers.base_materializer <span class="hljs-keyword">import</span> BaseMaterializer

<span class="hljs-keyword">class</span> <span class="hljs-title">ModelMetadataMaterializer</span>(<span class="hljs-title">BaseMaterializer</span>):
    <span class="hljs-comment"># This needs to point to the artifact data type(s) associated with the</span>
    <span class="hljs-comment"># materializer</span>
    ASSOCIATED_TYPES = (ModelMetadata,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    <span class="hljs-keyword">def</span> <span class="hljs-title">save</span>(<span class="hljs-params">self, model_metadata: ModelMetadata</span>) -&gt; <span class="hljs-literal">None</span>:
        <span class="hljs-built_in">super</span>().save(model_metadata)
        <span class="hljs-comment"># Dump the model metadata directly into the artifact store as a</span>
        <span class="hljs-comment">YAML file</span>
        <span class="hljs-keyword">with</span> fileio.<span class="hljs-built_in">open</span>(os.path.join(self.uri, <span class="hljs-string">'model_metadata.yaml'</span>),
                         <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
            f.write(yaml.dump(model_metadata.metadata))
    <span class="hljs-keyword">def</span> <span class="hljs-title">load</span>(<span class="hljs-params">self, data_type: </span><span class="hljs-type">Type</span><span class="hljs-params">[ModelMetadata]</span>) -&gt; ModelMetadata:
        <span class="hljs-built_in">super</span>().load(data_type)
        <span class="hljs-keyword">with</span> fileio.<span class="hljs-built_in">open</span>(os.path.join(self.uri, <span class="hljs-string">'data.txt'</span>), <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
            model_metadata = ModelMetadata()
            model_metadata.metadata = yaml.safe_load(f.read())
        <span class="hljs-keyword">return</span> model_metadata
</code></pre>
<p class="normal">Note that the naming convention of the path to this module shows that the materializer is paired with the <code class="inlineCode">model_metadata</code> artifact we walked through in the previous code snippet. This is good practice and is similar to the organization of tests that we discussed in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>.</p>
<p class="normal">Now that we have discussed all the key<a id="_idIndexMarker696"/> pieces of the ZenML template, we want<a id="_idIndexMarker697"/> to run the pipeline. This is done via runner <code class="inlineCode">run.py</code> at the utmost level of the repository. You can then run the pipeline with:</p>
<pre class="programlisting con"><code class="hljs-con">python run.py
</code></pre>
<p class="normal">After the pipeline successfully runs (you will see a series of outputs in the terminal), you can run the following command to spin up a locally hosted ZenML dashboard:</p>
<pre class="programlisting con"><code class="hljs-con">zenml up
</code></pre>
<p class="normal">Now, if you navigate to the URL that is returned as output, usually something like <code class="inlineCode">http://127.0.0.1:8237/login</code>, you will see a home screen like that shown in <em class="italic">Figure 5.40</em>.</p>
<figure class="mediaobject"><img alt="" height="469" role="presentation" src="../Images/B19525_05_40.png" width="471"/></figure>
<p class="packt_figref">Figure 5.40: The ZenML UI login page.</p>
<p class="normal">In the output that gave you the URL<a id="_idIndexMarker698"/> is also a default username<a id="_idIndexMarker699"/> and password, conveniently <strong class="screenText">default</strong> and a blank. Fill these in and you will see the home page shown in <em class="italic">Figure 5.41</em>.</p>
<figure class="mediaobject"><img alt="" height="476" role="presentation" src="../Images/B19525_05_41.png" width="826"/></figure>
<p class="packt_figref">Figure 5.41: The ZenML UI home page.</p>
<p class="normal">If you then click through<a id="_idIndexMarker700"/> into the <strong class="screenText">Pipelines</strong> section on the left and then click the pipeline<a id="_idIndexMarker701"/> created by your first run, you will be able to see all of the times that it has been run since then. This view is shown in <em class="italic">Figure 5.42</em>.</p>
<p class="packt_figref"><img alt="" height="197" role="presentation" src="../Images/B19525_05_42.png" width="824"/></p>
<p class="packt_figref">Figure 5.42: The pipelines view in the ZenML UI.</p>
<p class="normal">You can then also get really detailed information about the specifics of each run by clicking through. This gives you information like a graphical representation of the pipeline as a DAG at the time of the run. See <em class="italic">Figure 5.43</em>.</p>
<figure class="mediaobject"><img alt="" height="963" role="presentation" src="../Images/B19525_05_43.png" width="531"/></figure>
<p class="packt_figref">Figure 5.43: An example DAG for a ZenML pipeline shown in the UI.</p>
<p class="normal">If you click through on the pipeline<a id="_idIndexMarker702"/> name in any of these views, you can also retrieve<a id="_idIndexMarker703"/> the configuration of the run at the time of its execution in YAML format, which you can download and then use in subsequent pipeline runs:</p>
<figure class="mediaobject"><img alt="" height="368" role="presentation" src="../Images/B19525_05_44.png" width="824"/></figure>
<p class="packt_figref">Figure 5.44: An example YAML configuration for a ZenML pipeline run, shown in the ZenML UI.</p>
<p class="normal">This has only begun to scratch the surface of what is possible with ZenML, but hopefully, you can already see how it is a very flexible way to define and execute your ML pipelines. This becomes even more powerful when you leverage its ability to deploy the same pipeline across different stacks and different artifact stores.</p>
<p class="normal">In the next section, we will discuss<a id="_idIndexMarker704"/> another pipelining tool that focuses<a id="_idIndexMarker705"/> on creating cross-platform compatibility and standardization for your ML pipelines, <strong class="keyWord">Kubeflow</strong>.</p>
<h2 class="heading-2" id="_idParaDest-137">Going with the Kubeflow</h2>
<p class="normal"><strong class="keyWord">Kubeflow</strong> is an open-source solution<a id="_idIndexMarker706"/> aimed at providing portable methods for building end-to-end ML<a id="_idIndexMarker707"/> systems. This tool has a particular focus on giving developers the ability to quickly create pipelines for data processing, ML model training, prediction, and monitoring that are platform agnostic. It does all this by leveraging Kubernetes, allowing you to develop your solution on very different environments from where you eventually deploy. Kubeflow is agnostic about the particular programming and ML frameworks you use, so you can leverage everything you like out there in the open-source community but still stitch it together in a way you can trust.</p>
<p class="normal">The Kubeflow documentation provides a great wealth of detail on the architecture and design principles<a id="_idIndexMarker708"/> behind the tool at <a href="https://www.kubeflow.org/docs/"><span class="url">https://www.kubeflow.org/docs/</span></a>. We will focus instead on understanding the most salient points and getting started with some practical examples. This will allow you to compare to the other tools we have discussed in this chapter and make your own decisions around which to take forward in future projects.</p>
<p class="normal">Kubeflow is a platform that consists of multiple modular components, each one playing a role in the ML development lifecycle. Specifically, there are:</p>
<ul>
<li class="bulletList">The Jupyter Notebook web app and controller for exploratory data analysis and initial modeling.</li>
<li class="bulletList">Training operators like PyTorch, TFJob, and XGBoost operators, among others, to build a variety of models.</li>
<li class="bulletList">Hyperparameter tuning and neural network architecture search capabilities using Katib.</li>
<li class="bulletList">Spark operators for data transformation, including an option for AWS EMR clusters.</li>
<li class="bulletList">Dashboard for interfacing with your Kubernetes cluster and for managing your Kubeflow workloads.</li>
<li class="bulletList">Kubeflow Pipelines: its own platform for building, running, and managing end-to-end ML workflows. This includes an orchestration engine for workflows with multiple steps and an SDK for working with your pipelines. You can install Kubeflow Pipelines as a standalone platform.</li>
</ul>
<p class="normal">The installation steps for getting<a id="_idIndexMarker709"/> Kubeflow up and running can be quite involved <a id="_idIndexMarker710"/>and so it is best to look at the official documentation and run the appropriate steps for your platform and needs. We will proceed via the following steps:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Install Kind, a tool that facilitates easy building and running of local Kubernetes clusters. On Linux, this is done with:
        <pre class="programlisting con"><code class="hljs-con">curl -Lo ./kind https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-linux-amd64 &amp;&amp; \
chmod +x ./kind &amp;&amp; \
mv ./kind /{YOUR_KIND_DIRECTORY}/kind
</code></pre>
<p class="normal">And on MacOS this is done by:</p>
<pre class="programlisting con"><code class="hljs-con">brew install kind
</code></pre></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2">Install the Kubernetes command-line tool <code class="inlineCode">kubectl</code>, which allows you to interact with your cluster. On Linux, this is done with:
        <pre class="programlisting con"><code class="hljs-con">sudo apt-get install kubectl
</code></pre>
<p class="normal">Or on MacOS:</p>
<pre class="programlisting con"><code class="hljs-con">brew install kubernetes-cli
</code></pre> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">To check this has worked, you can run the following command in the terminal:
        <pre class="programlisting con"><code class="hljs-con">kubectl version --client --output=yaml
</code></pre>
</li>
<li class="numberedList">And you should receive an output like this:
        <pre class="programlisting con"><code class="hljs-con">clientVersion:
  buildDate: "2023-01-18T15:51:24Z"
  compiler: gc
  gitCommit: 8f94681cd294aa8cfd3407b8191f6c70214973a4
  gitTreeState: clean
  gitVersion: v1.26.1
  goVersion: go1.19.5
  major: "1"
  minor: "26"
  platform: darwin/arm64
kustomizeVersion: v4.5.7
</code></pre>
</li>
<li class="numberedList">Use Kind to create<a id="_idIndexMarker711"/> your local cluster. As the default, the name<a id="_idIndexMarker712"/> of the cluster will be <code class="inlineCode">kind</code>, but you can provide your own name as a flag:
        <pre class="programlisting con"><code class="hljs-con">kind create cluster –name=mlewp
</code></pre>
</li>
<li class="numberedList">You will then see output that is something like this:
        <pre class="programlisting con"><code class="hljs-con">Creating cluster "mlewp" ...
  Ensuring node image (kindest/node:v1.25.3) <img alt="" height="25" role="presentation" src="../Images/B19525_05_001.png" width="26"/> 
  Preparing nodes <img alt="" height="25" role="presentation" src="../Images/B19525_05_002.png" width="29"/>
  Writing configuration <img alt="" height="30" role="presentation" src="../Images/B19525_05_003.png" width="31"/>
  Starting control-plane <img alt="" height="29" role="presentation" src="../Images/B19525_05_004.png" width="33"/>
  Installing CNI <img alt="" height="25" role="presentation" src="../Images/B19525_05_005.png" width="27"/>
  Installing StorageClass <img alt="" height="25" role="presentation" src="../Images/B19525_05_006.png" width="28"/>
Set kubectl context to "kind-mlewp"
You can now use your cluster with:
kubectl cluster-info --context kind-mlewp
Thanks for using kind! <img alt="" height="23" role="presentation" src="../Images/B19525_05_007.png" width="25"/>
</code></pre>
</li>
<li class="numberedList">You then have to deploy Kubeflow<a id="_idIndexMarker713"/> pipelines to the cluster. The commands for doing<a id="_idIndexMarker714"/> this have been brought into a script called <code class="inlineCode">deploy_kubeflow_pipelines.zsh</code> in the book’s GitHub repository and it contains the following code (the <code class="inlineCode">PIPELINE_VERSION</code> number can be updated as needed to match your installation):
        <pre class="programlisting con"><code class="hljs-con">export PIPELINE_VERSION=1.8.5
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION"
</code></pre>
</li>
</ol>
<p class="normal">After running these commands, you can verify that the installation was a success through port forwarding and opening the Kubeflow Pipelines UI at <code class="inlineCode">http://localhost:8080/:</code></p>
<pre class="programlisting con"><code class="hljs-con">kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80
</code></pre>
<p class="normal">This should then give you a landing page like the one shown in <em class="italic">Figure 5.45</em>.</p>
<p class="packt_figref"><img alt="" height="221" role="presentation" src="../Images/B19525_05_45.png" width="820"/></p>
<p class="packt_figref">Figure 5.45: The Kubeflow UI landing page.</p>
<p class="normal">Now that you have initiated port-forwarding<a id="_idIndexMarker715"/> with the previous command, you will use this to allow<a id="_idIndexMarker716"/> the Kubeflow Pipelines SDK to talk to the cluster via the following Python code (note that you cannot do this until you have installed the Kubeflow Pipelines SDK, which is covered in the next step):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> kfp

client = kfp.Client(host=<span class="hljs-string">"http://localhost:8080"</span>)
</code></pre>
<p class="normal">To install the Kubeflow Pipelines SDK, run:</p>
<pre class="programlisting con"><code class="hljs-con">pip install kfp –upgrade
</code></pre>
<p class="normal">To check that everything is in order, you can run this command:</p>
<pre class="programlisting con"><code class="hljs-con">pip list | grep kfp
</code></pre>
<p class="normal">Which gives output that should be similar to:</p>
<pre class="programlisting con"><code class="hljs-con">kfp                      1.8.19
kfp-pipeline-spec        0.1.16
kfp-server-api           1.8.5
</code></pre>
<p class="normal">And that’s it! We are now ready to start building some Kubeflow pipelines. Let’s get started with a basic example.</p>
<p class="normal">We can now start building out some basic pipelines using the SDK and then we can deploy them to our cluster. Let’s assume for the next few steps we are working in a file called <code class="inlineCode">pipeline_basic.py</code>:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, we import what is known as the KFP <strong class="keyWord">Domain-Specific Language</strong> (<strong class="keyWord">DSL</strong>), which is a set of Python<a id="_idIndexMarker717"/> packages with various utilities for defining KFP steps. We also import the client package for interacting with the cluster. We’ll also import several DSL sub-modules that we will use later. An important point to note here is that some functionality we will leverage is in fact contained in the <code class="inlineCode">V2</code> of the Kubeflow pipelines SDK and so we will need to import some of those specific modules as well:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> kfp <span class="hljs-keyword">import</span> Client
<span class="hljs-keyword">import</span> kfp.dsl
<span class="hljs-keyword">from</span> kfp.v2 <span class="hljs-keyword">import</span> dsl
<span class="hljs-keyword">from</span> kfp.v2.dsl <span class="hljs-keyword">import</span> Dataset
<span class="hljs-keyword">from</span> kfp.v2.dsl <span class="hljs-keyword">import</span> Input
<span class="hljs-keyword">from</span> kfp.v2.dsl <span class="hljs-keyword">import</span> Model
<span class="hljs-keyword">from</span> kfp.v2.dsl <span class="hljs-keyword">import</span> Output
</code></pre>
</li>
<li class="numberedList">The next step is to define<a id="_idIndexMarker718"/> the steps in the pipeline. These<a id="_idIndexMarker719"/> are called “components” and are functions wrapped with <code class="inlineCode">dsl</code> decorators. In this first step, we retrieve the Iris dataset and write it to CSV. In the first line, we will use the <code class="inlineCode">dsl</code> decorator and also define what packages need to be installed in the container that will run that step:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@dsl.component(</span><span class="hljs-params">packages_to_install=[</span><span class="hljs-string">'pandas==1.3.5'</span><span class="hljs-params">]</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">create_dataset</span>(<span class="hljs-params">iris_dataset: Output[Dataset]</span>):
<span class="hljs-keyword">    import</span> pandas <span class="hljs-keyword">as</span> pd
    csv_url = <span class="hljs-string">"https://archive.ics.uci.edu/ml/machine-learning-</span>
<span class="hljs-string">               databases/iris/iris.data"</span>
    col_names = [<span class="hljs-string">"Sepal_Length"</span>, <span class="hljs-string">"Sepal_Width"</span>, <span class="hljs-string">"Petal_Length"</span>,
                 <span class="hljs-string">"Petal_Width"</span>, <span class="hljs-string">"Labels"</span>]
    df = pd.read_csv(csv_url)
    df.columns = col_names
<span class="hljs-keyword">    with</span> <span class="hljs-built_in">open</span>(iris_dataset.path, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f: 
        df.to_csv(f)
</code></pre>
</li>
<li class="numberedList">Now that we have retrieved a dataset, and remembering what we learned in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, we want to feature engineer this data. So, we will normalize the data in another component. Most of the code should be self-explanatory, but note that we have had to add the <code class="inlineCode">scikit-learn</code> dependency in the <code class="inlineCode">packages_to_install</code> keyword argument and that we have again had to write the result of the component out to a CSV file:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@dsl.component(</span><span class="hljs-params">packages_to_install=[</span><span class="hljs-string">'pandas==1.3.5'</span><span class="hljs-params">, </span><span class="hljs-string">'scikit-learn==1.0.2'</span><span class="hljs-params">]</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">normalize_dataset</span>(
<span class="hljs-params">    input_iris_dataset: Input[Dataset],</span>
<span class="hljs-params">    normalized_iris_dataset: Output[Dataset],</span>
<span class="hljs-params">    standard_scaler: </span><span class="hljs-built_in">bool</span><span class="hljs-params">,</span>
<span class="hljs-params">    min_max_scaler: </span><span class="hljs-built_in">bool</span><span class="hljs-params">,</span>
):
    <span class="hljs-keyword">if</span> standard_scaler <span class="hljs-keyword">is</span> min_max_scaler:
        <span class="hljs-keyword">raise</span> ValueError(
            <span class="hljs-string">'Exactly one of standard_scaler or min_max_scaler must</span>
             <span class="hljs-string">be True.'</span>)
    <span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
    <span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> MinMaxScaler
    <span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(input_iris_dataset.path) <span class="hljs-keyword">as</span> f:
        df = pd.read_csv(f)
    labels = df.pop(<span class="hljs-string">'Labels'</span>)
    <span class="hljs-keyword">if</span> standard_scaler:
        scaler = StandardScaler()
    <span class="hljs-keyword">if</span> min_max_scaler:
        scaler = MinMaxScaler()
    df = pd.DataFrame(scaler.fit_transform(df))
    df[<span class="hljs-string">'</span><span class="hljs-string">Labels'</span>] = labels
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(normalized_iris_dataset.path, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:
        df.to_csv(f)
</code></pre>
</li>
<li class="numberedList">We will now train a K-nearest<a id="_idIndexMarker720"/> neighbors classifier on the data. Instead of outputting<a id="_idIndexMarker721"/> a dataset in this component, we will output the trained model artifact, a <code class="inlineCode">.pkl</code> file:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@dsl.component(</span><span class="hljs-params">packages_to_install=[</span><span class="hljs-string">'pandas==1.3.5'</span><span class="hljs-params">, </span><span class="hljs-string">'scikit-</span>
<span class="hljs-string">               learn==1.0.2'</span><span class="hljs-params">]</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">train_model</span>(
<span class="hljs-params">    normalized_iris_dataset: Input[Dataset],</span>
<span class="hljs-params">    model: Output[Model],</span>
<span class="hljs-params">    n_neighbors: </span><span class="hljs-built_in">int</span><span class="hljs-params">,</span>
):
    <span class="hljs-keyword">import</span> pickle
    <span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
    <span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
    <span class="hljs-keyword">from</span> sklearn.neighbors <span class="hljs-keyword">import</span> KNeighborsClassifier
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(normalized_iris_dataset.path) <span class="hljs-keyword">as</span> f:
        df = pd.read_csv(f)
    y = df.pop(<span class="hljs-string">'Labels'</span>)
    X = df
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                       random_state=<span class="hljs-number">0</span>)
    clf = KNeighborsClassifier(n_neighbors=n_neighbors)
    clf.fit(X_train, y_train)
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(model.path, <span class="hljs-string">'wb'</span>) <span class="hljs-keyword">as</span> f:
        pickle.dump(clf, f)
</code></pre>
</li>
<li class="numberedList">We now have all the components<a id="_idIndexMarker722"/> for the work we want to do, so now we can finally bring it together<a id="_idIndexMarker723"/> into a Kubeflow pipeline. To do this, we use the <code class="inlineCode">@dsl.pipeline</code> decorator and as an argument to that decorator, we provide the name of the pipeline:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">@dsl.pipeline(</span><span class="hljs-params">name=</span><span class="hljs-string">'iris-training-pipeline'</span><span class="hljs-meta">)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">my_pipeline</span>(
<span class="hljs-params">    standard_scaler: </span><span class="hljs-built_in">bool</span><span class="hljs-params">,</span>
<span class="hljs-params">    min_max_scaler: </span><span class="hljs-built_in">bool</span><span class="hljs-params">,</span>
<span class="hljs-params">    neighbors: </span><span class="hljs-type">List</span><span class="hljs-params">[</span><span class="hljs-built_in">int</span><span class="hljs-params">],</span>
):
    create_dataset_task = create_dataset()
    normalize_dataset_task = normalize_dataset(
        input_iris_dataset=create_dataset_task
        .outputs[<span class="hljs-string">'iris_dataset'</span>],
        standard_scaler=<span class="hljs-literal">True</span>,
        min_max_scaler=<span class="hljs-literal">False</span>)
    <span class="hljs-keyword">with</span> dsl.ParallelFor(neighbors) <span class="hljs-keyword">as</span> n_neighbors:
        train_model(
            normalized_iris_dataset=normalize_dataset_task
            .outputs[<span class="hljs-string">'</span><span class="hljs-string">normalized_iris_dataset'</span>],
            n_neighbors=n_neighbors)
</code></pre>
</li>
<li class="numberedList">The final stage is to submit the pipeline to run. This is done by instantiating a Kubeflow Pipelines client class and feeding in the appropriate arguments. <code class="inlineCode">&lt;KFP_UI_URL</code>&gt; is the URL for the host of your instance of Kubeflow Pipelines – in this case, the one that we got from performing port-forwarding earlier. It is also important to note that since we are using several features from the <code class="inlineCode">V2</code> Kubeflow Pipelines API, we should pass in the <code class="inlineCode">kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE</code> flag for the mode argument:
        <pre class="programlisting code"><code class="hljs-code">endpoint = <span class="hljs-string">'&lt;KFP_UI_URL&gt;'</span>
kfp_client = Client(host=endpoint)
run = kfp_client.create_run_from_pipeline_func(my_pipeline,
    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,
    arguments={
<span class="hljs-string">        'min_max_scaler'</span>: <span class="hljs-literal">True</span>,
<span class="hljs-string">        'standard_scaler'</span>: <span class="hljs-literal">False</span>,
<span class="hljs-string">        'neighbors'</span>: [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>]
<span class="hljs-string">    </span>},
)
url = <span class="hljs-string">f'</span><span class="hljs-subst">{endpoint}</span><span class="hljs-string">/#/runs/details/</span><span class="hljs-subst">{run.run_id}</span><span class="hljs-string">'</span>
<span class="hljs-built_in">print</span>(url)
</code></pre>
</li>
<li class="numberedList">To build and deploy this pipeline and run it, you can then execute:
        <pre class="programlisting con"><code class="hljs-con">python basic_pipeline.py
</code></pre>
</li>
</ol>
<p class="normal">After running this last step, you will see the URL of the run is printed to the terminal, and should look something like this:</p>
<pre class="programlisting con"><code class="hljs-con">http://localhost:8080/#/runs/details/&lt;UID&gt;
</code></pre>
<p class="normal">If you navigate<a id="_idIndexMarker724"/> to that link and the pipeline has successfully<a id="_idIndexMarker725"/> run, you should see a view in the Kubeflow dashboard showing the steps of the pipeline, with a sidebar that allows you to navigate through a series of metadata about your pipeline and its run. An example from running the above code is shown in <em class="italic">Figure 5.46</em>.</p>
<p class="packt_figref"><img alt="" height="409" role="presentation" src="../Images/B19525_05_46.png" width="820"/></p>
<p class="packt_figref">Figure 5.46: The Kubeflow UI showing the successful run of the training pipeline defined in the main text.</p>
<p class="normal">And that’s it, you have now built and run your first Kubeflow pipeline!</p>
<div class="note">
<p class="normal">IMPORTANT NOTE</p>
<p class="normal">You can also compile your Kubeflow pipelines to serialized YAML, which can then be read by the Kubeflow backend. You would do this by running a command like the following, where <code class="inlineCode">pipeline</code> is the same pipeline object used in the previous example:</p>
<pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">cmplr = compiler.Compiler()</code>
<code class="inlineCode">cmplr.compile(my_pipeline, package_path='my_pipeline.yaml')</code>
</code></pre>
<p class="normal">One reason to do this is it is then super easy to run the pipeline. You can just upload it to the Kubeflow Pipelines UI, or you can send the YAML to the cluster programmatically.</p>
</div>
<p class="normal">As in the <em class="italic">Finding your ZenML</em> section, we have only <a id="_idIndexMarker726"/>begun to scratch the surface of this tool and have focused<a id="_idIndexMarker727"/> initially on getting to know the basics in a local environment. The beauty of Kubeflow being based on Kubernetes is that platform agnosticism is very much at its core and so these pipelines can be effectively run anywhere that supports containers.</p>
<div class="note">
<p class="normal">IMPORTANT NOTE</p>
<p class="normal">That although I have presented ZenML and Kubeflow as two different pipelining tools, they can actually be viewed as complementary, so much so that ZenML provides the ability to deploy Kubeflow pipelines through the use of the ZenML Kubeflow orchestrator. This means you can leverage the higher-level abstractions provided by ZenML but still get the scaling behavior and robustness of Kubeflow as a deployment target. We will not cover the details here but the ZenML documentation<a id="_idIndexMarker728"/> provides an excellent guide: <a href="https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow"><span class="url">https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow</span></a>.</p>
</div>
<p class="normal">The next section will finish<a id="_idIndexMarker729"/> the chapter with a brief note on some different deployment<a id="_idIndexMarker730"/> strategies that you should be aware of when you aim to put all of these tools and techniques into practice with real solutions.</p>
<h1 class="heading-1" id="_idParaDest-138">Selecting your deployment strategy</h1>
<p class="normal">We have discussed<a id="_idIndexMarker731"/> many of the technical details of ways to take ML solutions into production in this chapter. The missing piece, however, is that we have not defined how you deal with existing infrastructure and how you introduce your solution to real traffic and requests. This is what is defined by your deployment strategy, and selecting an appropriate one is an important part of being an ML engineer.</p>
<p class="normal">Most deployment strategies are, like many of the concepts in this book, inherited from the world of <strong class="keyWord">software engineering</strong> and <strong class="keyWord">DevOps</strong>. Two of the most important to know about are listed below with some discussion about when they can be particularly useful in an ML context.</p>
<p class="normal"><strong class="keyWord">Blue/green deployments</strong> are deployments where the new version<a id="_idIndexMarker732"/> of your software runs alongside your existing solution until some predefined criteria are met. After this point, you then switch all incoming traffic/requests to the new system before decommissioning the old one or leave it there for use as a potential rollback solution. The method was originally developed by two developers, Daniel North and Jez Humble, who were working on an e-commerce site in 2005. </p>
<p class="normal">The origin of the name is described in this GitHub Gist, <a href="https://gitlab.com/-/snippets/1846041"><span class="url">https://gitlab.com/-/snippets/1846041</span></a>, but essentially boils down to the fact that any other naming convention they could come up with always implied one of the candidate solutions or environments was “better” or “worse” than the other, for example with “A and B” or “Green and Red.” The strategy has since become a classic. </p>
<p class="normal">In an ML engineering context, this is particularly useful in scenarios where you want to gather model and solution performance data over a known period of time before trusting full deployment. It also helps with giving stakeholders evidence that the ML solution will perform as expected “in the wild.” It also plays particularly well with batch jobs, as you are just effectively running another batch at the same time. This may have some cost implications for you to consider if the job is big<a id="_idIndexMarker733"/> or complex, or if your production environment is costly to maintain.</p>
<p class="normal">The next strategy is known as <strong class="keyWord">canary deployments</strong> and involves a similar setup to the blue/green method<a id="_idIndexMarker734"/> but involves a more gradual switching of traffic between the two solutions. Here the idea is that the new system is deployed and receives some percentage of the traffic initially, say 5% or 10%, before stability and performance are confirmed, and then the next increment of traffic is added. The total always remains at 100% so as the new system gains more traffic, the old system receives less. The name originates from the old coal mining technique of using canaries as a test of toxicity in the atmosphere in mines. Release the canaries and if they survive, all is well. Thankfully, no birds are harmed in the usage of this deployment technique. This strategy makes a lot of sense when you are able to divide the data you need to score and still get the information you need for progression to the next stage. As an example, an ML microservice that is called in the backend of a website would fit the bill nicely, as you can just gradually change the routing to the new service on your load balancer. This may make less sense for large batch jobs, as there may be no natural way to split your data into different increments, whereas with web traffic there definitely is.</p>
<p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">Building an Example ML Microservice</em>, will show you how to use these strategies when building a custom ML endpoint using Kubernetes.</p>
<p class="normal">No matter what deployment strategy you use, always remember that the key is to strike the balance<a id="_idIndexMarker735"/> between cost-effectiveness, the uptime of your solution, and trust in the outputs it produces. If you can do all of these, then you will have deployed a winning combination.</p>
<h1 class="heading-1" id="_idParaDest-139">Summary</h1>
<p class="normal">In this chapter, we have discussed some of the most important concepts when it comes to deploying your ML solutions. In particular, we focused on the concepts of architecture and what tools we could potentially use when deploying solutions to the cloud. We covered some of the most important patterns used in modern ML engineering and how these can be implemented with tools such as containers and AWS Elastic Container Registry and Elastic Container Service, as well as how to create scheduled pipelines in AWS using Managed Workflows for Apache Airflow. We also explored how to hook up the MWAA example with GitHub Actions, so that changes to your code can directly trigger updates of running services, providing a template to use in future CI/CD processes.</p>
<p class="normal">We then moved on to a discussion of more advanced pipelining tools to build on the discussion in <em class="chapterRef">Chapter 4</em>, <em class="italic">Packaging Up</em>. This focused on how to use Apache Airflow to build and orchestrate your generic pipelines for running your data engineering, ML, and MLOps pipelines. We then moved on to a detailed introduction to ZenML and Kubeflow, two powerful tools for developing and deploying ML and MLOps pipelines at scale.</p>
<p class="normal">In the next chapter, we will look at the question of other ways to scale up our solutions so that we can deal with large volumes of data and high-throughput calculations.</p>
<h1 class="heading-1" id="_idParaDest-140">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussion with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/mle"><span class="url">https://packt.link/mle</span></a></p>
<p class="normal"><img alt="" height="177" role="presentation" src="../Images/QR_Code102810325355484.png" width="177"/></p>
</div>
</div></body></html>