<html><head></head><body><div class="book" title="Chapter&#xA0;12.&#xA0;What's Next?"><div class="book" id="2BASE2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12" class="calibre1"/>Chapter 12. What's Next?</h1></div></div></div><p class="calibre7">Throughout this book, we have learned about ensemble learning and explored its applications in many scenarios. In the introductory chapter, we looked at different examples, datasets, and models, and found that there is no single model or technique that performs better than the others. This means that our guard should always be up when dealing with this matter, and hence the analyst has to proceed with extreme caution. The approach of selecting the best model from among the various models means that we reject all of the models whose performance is slightly less than that of the others, and hence a lot of resources are wasted in pursuit of the <span class="strong"><em class="calibre9">best</em></span> model.</p><p class="calibre7">In <a class="calibre1" title="Chapter 7. The General Ensemble Technique" href="part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee">Chapter 7</a>, <span class="strong"><em class="calibre9">The General Ensemble Technique</em></span>, we saw that if we have multiple classifiers with each classifier being better than a random guess, majority voting of the classifiers gives improved performance. We also saw that with a fairly good number of classifiers, the overall accuracy of the majority voting is higher than the most accurate classifier. Though majority voting works on the oversimplified assumption that the classifiers are independent classifiers, the basis and importance of <span class="strong"><em class="calibre9">ensemble learning</em></span> is realized and the prospect looks brighter as we are assured of an ensemble classifier with the highest accuracy. The classifiers or models of the ensemble are called the <span class="strong"><em class="calibre9">base classifiers/learners/models</em></span>. If all the base models belong to the same family of models, or if the family is a logistic model, a neural network, a decision tree, or an SVM, then we will classify these as a homogeneous ensemble. If the base models belong to two or more families of models, then the ensemble is referred to as a heterogeneous ensemble.</p><p class="calibre7">A fundamental aspect of ensemble learning lies in <span class="strong"><em class="calibre9">resampling techniques</em></span>. The jackknife and bootstrap methods were introduced in <a class="calibre1" title="Chapter 2. Bootstrapping" href="part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee">Chapter 2</a>, <span class="strong"><em class="calibre9">Bootstrapping</em></span>, and these two methods were illustrated for different classes of models. The jackknife method uses pseudovalues, and we witnessed its application in a few complex scenarios. The pseudovalues are underutilized in the machine learning domain, and they might be useful in scenarios in which the relationship or a parameter of interest is quite complex and a more flexible approach may be possible. The concept of pseudovalues can also be used in ensemble diagnostics, and a <span class="strong"><em class="calibre9">pseudoaccuracy</em></span> measure can be created that will give the impact of the classifier in the ensemble additional accuracy. Efron and Tibshirani (1990) discuss the different methods available for obtaining bootstrap samples. In the resamples that we obtained in the steps for bagging and random forest, we follow the simple bootstrap samples of drawing observations with replacement. Accessing the different sampling methods that are driven from existing bootstrap literature is a potential area of work for the future.</p><p class="calibre7">Bagging, random forest, and boosting are the homogeneous ensembles that have the decision tree as their base learner. The decision tree discussed in this book can be referred to as the frequentist method or the classical method. A Bayesian implementation of the decision tree is available in a technique known as the <span class="strong"><strong class="calibre8">Bayesian additive regression tree</strong></span>. The R implementation of this method is available in the<a id="id547" class="calibre1"/> BART package, located at <a class="calibre1" href="https://cran.r-project.org/web/packages/BART/index.html">https://cran.r-project.org/web/packages/BART/index.html</a>. The theoretical foundations of BART<a id="id548" class="calibre1"/> are available at <a class="calibre1" href="https://arxiv.org/pdf/0806.3286.pdf">https://arxiv.org/pdf/0806.3286.pdf</a>. The extension of homogeneous ensembles based on BART needs to be undertaken in a comprehensive way. In particular, bagging and random forests need to be extended with BART as a base learner.</p><p class="calibre7">Ensemble models for heterogeneous base models are set up using the stack ensembling method. For classification problems, we can expect weighted voting to be more useful than simple majority voting. Similarly, the weighted prediction of the models is expected to perform better than simple averaging. The statistical tests proposed to test the similarity of base models are all <span class="strong"><em class="calibre9">classical</em></span> tests. Bayesian measures of agreement are expected to give us further guidelines, and the author of this book is now aware of such assessments being performed in the context of ensemble diversity. You can read Broemeling (2009) for more information on this at <a class="calibre1" href="https://www.crcpress.com/Bayesian-Methods-for-Measures-of-Agreement/Broemeling/p/book/9781420083415">https://www.crcpress.com/Bayesian-Methods-for-Measures-of-Agreement/Broemeling/p/book/9781420083415</a>. Furthermore, when it comes to large datasets, the option of generating an independent set of models needs to be systematically developed.</p><p class="calibre7">Time series data is of a different structure, and the dependency of the observations means that we can't directly apply the ensemble methods without the appropriate tweaking. <a class="calibre1" title="Chapter 11. Ensembling Time Series Models" href="part0076_split_000.html#28FAO1-2006c10fab20488594398dc4871637ee">Chapter 11</a>, <span class="strong"><em class="calibre9">Ensembling Time Series Models</em></span>, looks at this topic in more detail. Random forests have been developed for time series data very recently. An implementation of the random forests<a id="id549" class="calibre1"/> can be seen at <a class="calibre1" href="https://petolau.github.io/Ensemble-of-trees-for-forecasting-time-series/">https://petolau.github.io/Ensemble-of-trees-for-forecasting-time-series/</a>, and if you are interested, you can refer to this link for more information.</p><p class="calibre7">High-dimensional data analysis is another more recent topic that is not covered in this book. Chapter 12 of Buhlmann and van de Geer (2011) (see <a class="calibre1" href="https://www.springer.com/in/book/9783642201912">https://www.springer.com/in/book/9783642201912</a>) gives useful pointers in this area. For boosting data, the treatise of Schapire and Freund (2012) (at <a class="calibre1" href="https://mitpress.mit.edu/books/boosting">https://mitpress.mit.edu/books/boosting</a>) is a real treasure. Zhou (2012) (found at <a class="calibre1" href="https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031">https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031</a>) is also a very important book on ensemble methods, and this book benefits from its insights. Kuncheva (2004-14) (found at <a class="calibre1" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118914564">https://onlinelibrary.wiley.com/doi/book/10.1002/9781118914564</a>) is probably the first book to detail ensemble methods, and it contains a lot of other details regarding ensemble diagnostics. Dixit (2017) (found at <a class="calibre1" href="https://www.packtpub.com/big-data-and-business-intelligence/ensemble-machine-learning">https://www.packtpub.com/big-data-and-business-intelligence/ensemble-machine-learning</a>) is another recent title on ensemble methods, and the methods are illustrated in the book using Python software.</p><p class="calibre7">Finally, the reader should always keep continuous track of recent developments. For R implementation, the best place for resources is <a class="calibre1" href="https://cran.r-project.org/web/views/MachineLearning.html">https://cran.r-project.org/web/views/MachineLearning.html</a>.</p><p class="calibre7">Next, we will look at a list of important journals. A lot of developments and discussions are taking place in these journals on the subject of machine learning, and therefore ensemble learning too. These journals are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Journal of Machine<a id="id550" class="calibre1"/> Learning Research (<a class="calibre1" href="http://www.jmlr.org/">http://www.jmlr.org/</a>)</li><li class="listitem">IEEE Transactions on <a id="id551" class="calibre1"/>Pattern Analysis and Machine Intelligence (<a class="calibre1" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34</a>)</li><li class="listitem">Pattern <a id="id552" class="calibre1"/>Recognition Letters (<a class="calibre1" href="https://www.journals.elsevier.com/pattern-recognition-letters">https://www.journals.elsevier.com/pattern-recognition-letters</a>)</li><li class="listitem">Machine <a id="id553" class="calibre1"/>Learning(<a class="calibre1" href="https://www.springer.com/computer/ai/journal/10994">https://www.springer.com/computer/ai/journal/10994</a>)</li><li class="listitem">Neurocomputing (<a class="calibre1" href="https://www.journals.elsevier.com/neurocomputing">https://www.journals.elsevier.com/neurocomputing</a>)</li><li class="listitem">And last, but <a id="id554" class="calibre1"/>not least (though this is not a journal), <a class="calibre1" href="https://www.packtpub.com/tech/Machine-Learning">https://www.packtpub.com/tech/Machine-Learning</a></li></ul></div><p class="calibre7">If you found this book useful, we should meet again in the next edition!</p></div>

<div class="book" title="Appendix&#xA0;A.&#xA0;Bibliography"><div class="book" id="2C9D02-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="appA" class="calibre1"/>Appendix A. Bibliography</h1></div></div></div></div>

<div class="book" title="Appendix&#xA0;A.&#xA0;Bibliography">
<div class="book" title="References"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch12lvl1sec89" class="calibre1"/>References</h1></div></div></div><p class="calibre7">Abraham, B. and Ledolter, J. (<a class="calibre1" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316610">https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316610</a>), 1983. <span class="strong"><em class="calibre9">Statistical methods for forecasting</em></span>. J. Wiley</p><p class="calibre7">Andersen, P.K., Klein, J.P. and Rosthøj, S., (<a class="calibre1" href="https://doi.org/10.1093/biomet/90.1.15">https://doi.org/10.1093/biomet/90.1.15</a>) 2003. Generalised linear models for correlated pseudo-observations, with applications to multi-state models. <span class="strong"><em class="calibre9">Biometrika</em></span>,  <span class="strong"><em class="calibre9">90</em></span>(1), pp.15-27.</p><p class="calibre7">Berk, R.A., (<a class="calibre1" href="https://www.springer.com/in/book/9783319440477">https://www.springer.com/in/book/9783319440477</a>) 2016. <span class="strong"><em class="calibre9">Statistical learning from a regression perspective, Second Edition</em></span>. New York: Springer.</p><p class="calibre7">Bou-Hamad, I., Larocque, D. and Ben-Ameur, H., (<a class="calibre1" href="https://projecteuclid.org/euclid.ssu/1315833185">https://projecteuclid.org/euclid.ssu/1315833185</a>) 2011. A review of survival trees. <span class="strong"><em class="calibre9">Statistics Surveys</em></span>, <span class="strong"><em class="calibre9">5</em></span>, pp.44-71.</p><p class="calibre7">Box, G.E., Jenkins, G.M., Reinsel, G.C. and Ljung, G.M., (<a class="calibre1" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118619193">https://onlinelibrary.wiley.com/doi/book/10.1002/9781118619193</a>)2015. <span class="strong"><em class="calibre9">Time series analysis: forecasting and control</em></span>. John Wiley &amp; Sons.</p><p class="calibre7">Breiman, L., (<a class="calibre1" href="https://link.springer.com/article/10.1007/BF00058655">https://link.springer.com/article/10.1007/BF00058655</a>) 1996. Bagging predictors. <span class="strong"><em class="calibre9">Machine learning</em></span>, <span class="strong"><em class="calibre9">24</em></span>(2), pp.123-140.</p><p class="calibre7">Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J., (<a class="calibre1" href="https://www.taylorfrancis.com/books/9781351460491">https://www.taylorfrancis.com/books/9781351460491</a>) 1984. <span class="strong"><em class="calibre9">Classification and regression trees</em></span>. Routledge.</p><p class="calibre7">Broemeling, L.D., (<a class="calibre1" href="https://www.crcpress.com/Bayesian-Methods-for-Measures-of-Agreement/Broemeling/p/book/9781420083415">https://www.crcpress.com/Bayesian-Methods-for-Measures-of-Agreement/Broemeling/p/book/9781420083415</a>)2009. <span class="strong"><em class="calibre9">Bayesian methods for measures of agreement</em></span>. Chapman and Hall/CRC.</p><p class="calibre7">Bühlmann, P. and Van De Geer, S., (<a class="calibre1" href="https://www.springer.com/in/book/9783642201912">https://www.springer.com/in/book/9783642201912</a>) 2011. <span class="strong"><em class="calibre9">Statistics for high-dimensional data: methods, theory and applications</em></span>. Springer Science &amp; Business Media.</p><p class="calibre7">Chatterjee, S. and Hadi, A.S., (<a class="calibre1" href="https://www.wiley.com/en-us/Regression+Analysis+by+Example%2C+5th+Edition-p-9780470905845">https://www.wiley.com/en-us/Regression+Analysis+by+Example%2C+5th+Edition-p-9780470905845</a>) 2012. Regression Analysis by Example, Fifth edition. John Wiley &amp; Sons.</p><p class="calibre7">Ciaburro, G., (<a class="calibre1" href="https://www.packtpub.com/big-data-and-business-intelligence/regression-analysis-r">https://www.packtpub.com/big-data-and-business-intelligence/regression-analysis-r</a>) 2018. Regression Analysis with R, Packt Publishing Ltd.</p><p class="calibre7">Cleveland, R.B., Cleveland, W.S., McRae, J.E. and Terpenning, I., (<a class="calibre1" href="http://www.nniiem.ru/file/news/2016/stl-statistical-model.pdf">http://www.nniiem.ru/file/news/2016/stl-statistical-model.pdf</a>) 1990. STL: A Seasonal-Trend Decomposition. <span class="strong"><em class="calibre9">Journal of Official Statistics</em></span>, <span class="strong"><em class="calibre9">6(</em></span>1), pp.3-73.</p><p class="calibre7">Cox, D.R., (<a class="calibre1" href="https://eclass.uoa.gr/modules/document/file.php/MATH394/Papers/%5BCox(1972)%5D%20Regression%20Models%20and%20Life%20Tables.pdf">https://eclass.uoa.gr/modules/document/file.php/MATH394/Papers/%5BCox(1972)%5D Regression Models and Life Tables.pdf</a>) 1972. Regression models and life-tables. <span class="strong"><em class="calibre9">Journal of the Royal Statistical Society</em></span>. Series B (Methodological), <span class="strong"><strong class="calibre8">34</strong></span>, pp. 187-220</p><p class="calibre7">Cox, D.R., (<a class="calibre1" href="https://academic.oup.com/biomet/article-abstract/62/2/269/337051">https://academic.oup.com/biomet/article-abstract/62/2/269/337051</a>) 1975. Partial likelihood. <span class="strong"><em class="calibre9">Biometrika</em></span>, <span class="strong"><em class="calibre9">62</em></span>(2), pp.269-276.</p><p class="calibre7">Dixit, A., (<a class="calibre1" href="https://www.packtpub.com/big-data-and-business-intelligence/ensemble-machine-learning">https://www.packtpub.com/big-data-and-business-intelligence/ensemble-machine-learning</a>)2017. <span class="strong"><em class="calibre9">Ensemble Machine Learning: A beginner's guide that combines powerful machine learning algorithms to build optimized models</em></span>. Packt Publishing Ltd.</p><p class="calibre7">Draper, N.R. and Smith, H., (<a class="calibre1" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118625590">https://onlinelibrary.wiley.com/doi/book/10.1002/9781118625590</a>)1999/2014. <span class="strong"><em class="calibre9">Applied regression analysis</em></span>, (Vol. 326). John Wiley &amp; Sons.</p><p class="calibre7">Efron, B. (<a class="calibre1" href="https://projecteuclid.org/download/pdf_1/euclid.aos/1176344552">https://projecteuclid.org/download/pdf_1/euclid.aos/1176344552</a>) 1979. Bootstrap methods (<a class="calibre1" href="https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41">https://link.springer.com/chapter/10.1007/978-1-4612-4380-9_41</a>): another look at the jackknife, <span class="strong"><em class="calibre9">The Annals of Statistics</em></span>, 7, 1-26. </p><p class="calibre7">Efron, B. and Hastie, T., 2016. (<a class="calibre1" href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf">https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf</a>) <span class="strong"><em class="calibre9">Computer age statistical inference</em></span> (Vol. 5). Cambridge University Press.</p><p class="calibre7">Efron, B. and Tibshirani, R.J., (<a class="calibre1" href="https://www.crcpress.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317">https://www.crcpress.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317</a>) 1994. <span class="strong"><em class="calibre9">An introduction to the bootstrap</em></span>. CRC press.</p><p class="calibre7">Friedman, J.H., Hastie, T. and Tibshirani, R. (<a class="calibre1" href="https://projecteuclid.org/download/pdf_1/euclid.aos/1016218223">https://projecteuclid.org/download/pdf_1/euclid.aos/1016218223</a>)2001. Greedy function approximation: A gradient boosting machine. <span class="strong"><em class="calibre9">Annals of Statistics</em></span>, 29(5):1189–1232.</p><p class="calibre7">Gordon, L. and Olshen, R.A., (<a class="calibre1" href="https://europepmc.org/abstract/med/4042086">https://europepmc.org/abstract/med/4042086</a>)1985. Tree-structured survival analysis. Cancer treatment reports, 69(10), pp.1065-1069.</p><p class="calibre7">Hastie, T., Tibshirani, R., and Friedman, J. (<a class="calibre1" href="https://www.springer.com/in/book/9780387848570">https://www.springer.com/in/book/9780387848570</a>) ,2009., <span class="strong"><em class="calibre9">The Elements of Statistical Learning, Second Edition</em></span>, Springer.</p><p class="calibre7">Haykin, S.S, 2009. (<a class="calibre1" href="https://www.pearson.com/us/higher-education/program/Haykin-Neural-Networks-and-Learning-Machines-3rd-Edition/PGM320370.html">https://www.pearson.com/us/higher-education/program/Haykin-Neural-Networks-and-Learning-Machines-3rd-Edition/PGM320370.html</a>) <span class="strong"><em class="calibre9">Neural networks and learning machines</em></span> (Vol. 3). Upper Saddle River, NJ, USA:: Pearson.</p><p class="calibre7">Kalbfleisch, J.D. and Prentice, R.L. (<a class="calibre1" href="https://onlinelibrary.wiley.com/doi/abs/10.2307/3315078">https://onlinelibrary.wiley.com/doi/abs/10.2307/3315078</a>), 2002. <span class="strong"><em class="calibre9">The statistical analysis of failure time data</em></span>. John Wiley &amp; Sons.</p><p class="calibre7">Kuncheva, L.I., (<a class="calibre1" href="https://www.wiley.com/en-us/Combining+Pattern+Classifiers%3A+Methods+and+Algorithms%2C+2nd+Edition-p-9781118315231">https://www.wiley.com/en-us/Combining+Pattern+Classifiers%3A+Methods+and+Algorithms%2C+2nd+Edition-p-9781118315231</a>) 2014. <span class="strong"><em class="calibre9">Combining pattern classifiers: methods and algorithms</em></span>. Second Edition. John Wiley &amp; Sons.</p><p class="calibre7">LeBlanc, M. and Crowley, J., (<a class="calibre1" href="https://www.jstor.org/stable/2532300">https://www.jstor.org/stable/2532300</a>)1992. Relative risk trees for censored survival data. <span class="strong"><em class="calibre9">Biometrics</em></span>, pp.411-425.</p><p class="calibre7">Lee, S.S. and Elder, J.F., (<a class="calibre1" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=6B151AAB29C69A4D4C35C8C4BBFC67F5?doi=10.1.1.34.1753&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=6B151AAB29C69A4D4C35C8C4BBFC67F5?doi=10.1.1.34.1753&amp;rep=rep1&amp;type=pdf</a>) 1997. Bundling heterogeneous classifiers with advisor perceptrons. <span class="strong"><em class="calibre9">White Paper</em></span>.</p><p class="calibre7">Mardia, K. , Kent, J., and Bibby, J.M.., (<a class="calibre1" href="https://www.elsevier.com/books/multivariate-analysis/mardia/978-0-08-057047-1">https://www.elsevier.com/books/multivariate-analysis/mardia/978-0-08-057047-1</a>) 1979. <span class="strong"><em class="calibre9">Multivariate analysis</em></span>. Academic Press.</p><p class="calibre7">Montgomery, D.C., Peck, E.A. and Vining, G.G., (<a class="calibre1" href="https://www.wiley.com/en-us/Introduction+to+Linear+Regression+Analysis%2C+5th+Edition-p-9781118627365">https://www.wiley.com/en-us/Introduction+to+Linear+Regression+Analysis%2C+5th+Edition-p-9781118627365</a>) 2012. <span class="strong"><em class="calibre9">Introduction to linear regression analysis</em></span> (Vol. 821). John Wiley &amp; Sons.</p><p class="calibre7">Perrone, M.P., and Cooper, L.N., (<a class="calibre1" href="https://www.worldscientific.com/doi/abs/10.1142/9789812795885_0025">https://www.worldscientific.com/doi/abs/10.1142/9789812795885_0025</a>)1993. When Networks Disagree: Ensemble Methods for Hybrid Neural Networks. In Mammone, R.J. (Ed.), <span class="strong"><em class="calibre9">Neural Networks for Speech and Image Processing</em></span>. Chapman Hall.</p><p class="calibre7">Ripley, B.D., (<a class="calibre1" href="http://admin.cambridge.org/fk/academic/subjects/statistics-probability/computational-statistics-machine-learning-and-information-sc/pattern-recognition-and-neural-networks">http://admin.cambridge.org/fk/academic/subjects/statistics-probability/computational-statistics-machine-learning-and-information-sc/pattern-recognition-and-neural-networks</a>)2007. <span class="strong"><em class="calibre9">Pattern recognition and neural networks</em></span>. Cambridge university press.</p><p class="calibre7">Quenouille, M.H., (<a class="calibre1" href="https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/approximate-tests-of-correlation-in-timeseries-3/F6D24B2A8574F1716E44BE788696F9C7">https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/approximate-tests-of-correlation-in-timeseries-3/F6D24B2A8574F1716E44BE788696F9C7</a>) 1949, July. Approximate tests of correlation in time-series 3. In <span class="strong"><em class="calibre9">Mathematical Proceedings of the Cambridge Philosophical Society</em></span> (Vol. 45, No. 3, pp. 483-484). Cambridge University Press.</p><p class="calibre7">Quinlan, J. R. (1993), (<a class="calibre1" href="https://www.elsevier.com/books/c45/quinlan/978-0-08-050058-4">https://www.elsevier.com/books/c45/quinlan/978-0-08-050058-4</a>) <span class="strong"><em class="calibre9">C4.5: Programs for Machine Learning</em></span>, Morgan Kaufmann.</p><p class="calibre7">Ridgeway, G., Madigan, D. and Richardson, T., (<a class="calibre1" href="http://dimacs.rutgers.edu/archive/Research/MMS/PAPERS/BNBR.pdf">http://dimacs.rutgers.edu/archive/Research/MMS/PAPERS/BNBR.pdf</a>) 1999, January. Boosting methodology for regression problems. In <span class="strong"><em class="calibre9">AISTATS</em></span>.</p><p class="calibre7">Schapire, R.E. and Freund, Y., (<a class="calibre1" href="http://dimacs.rutgers.edu/archive/Research/MMS/PAPERS/BNBR.pdf">http://dimacs.rutgers.edu/archive/Research/MMS/PAPERS/BNBR.pdf</a>) 2012. <span class="strong"><em class="calibre9">Boosting: Foundations and algorithms</em></span>. MIT press.</p><p class="calibre7">Seni, G. and Elder, J.F., (<a class="calibre1" href="https://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002">https://www.morganclaypool.com/doi/abs/10.2200/S00240ED1V01Y200912DMK002</a>)2010. Ensemble methods in data mining: improving accuracy through combining predictions. <span class="strong"><em class="calibre9">Synthesis Lectures on Data Mining and Knowledge Discovery</em></span>, <span class="strong"><em class="calibre9">2</em></span>(1), pp.1-126.</p><p class="calibre7">Tattar, P.N., Ramaiah, S. and Manjunath, B.G., (<a class="calibre1" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781119152743">https://onlinelibrary.wiley.com/doi/book/10.1002/9781119152743</a>) 2016. <span class="strong"><em class="calibre9">A Course in Statistics with R</em></span>. John Wiley &amp; Sons.</p><p class="calibre7">Tattar, P.N., 2017. (<a class="calibre1" href="https://www.packtpub.com/big-data-and-business-intelligence/statistical-application-development-r-and-python-second-edition">https://www.packtpub.com/big-data-and-business-intelligence/statistical-application-development-r-and-python-second-edition</a>) <span class="strong"><em class="calibre9">Statistical Application Development with R and Python</em></span>. Packt Publishing Ltd.</p><p class="calibre7">Tattar, P., Ojeda, T., Murphy, S.P., Bengfort, B. and Dasgupta, A., (<a class="calibre1" href="https://www.packtpub.com/big-data-and-business-intelligence/practical-data-science-cookbook-second-edition">https://www.packtpub.com/big-data-and-business-intelligence/practical-data-science-cookbook-second-edition</a>) 2017. <span class="strong"><em class="calibre9">Practical Data Science Cookbook</em></span>. Packt Publishing Ltd.</p><p class="calibre7">Zhang, H. and Singer, B.H., (<a class="calibre1" href="https://www.springer.com/in/book/9781441968234">https://www.springer.com/in/book/9781441968234</a>)2010. <span class="strong"><em class="calibre9">Recursive partitioning and applications</em></span>. Springer Science &amp; Business Media.</p><p class="calibre7">Zemel, R.S. and Pitassi, T., (<a class="calibre1" href="http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf">http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf</a> )2001. A gradient-based boosting algorithm for regression problems. In <span class="strong"><em class="calibre9">Advances in neural information processing systems</em></span> (pp. 696-702).</p><p class="calibre7">Zhou, Z.H., (<a class="calibre1" href="https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031">https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/p/book/9781439830031</a>)2012. <span class="strong"><em class="calibre9">Ensemble methods: foundations and algorithms</em></span>. Chapman and Hall/CRC.</p></div></div>
<div class="book" title="R package references"><div class="book" id="2D7TI2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch12lvl1sec90" class="calibre1"/>R package references</h1></div></div></div><p class="calibre7">Prabhanjan Tattar (2015). <code class="literal">ACSWR</code>: A Companion Package for the Book "A</p><p class="calibre7">  Course in Statistics with R". R package version 1.0.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=ACSWR">https://CRAN.R-project.org/package=ACSWR</a>
</p><p class="calibre7">Alfaro, E., Gamez, M. Garcia, N.(2013). <code class="literal">adabag</code>: An R Package for</p><p class="calibre7">  Classification with Boosting and Bagging. Journal of Statistical</p><p class="calibre7">  Software, 54(2), 1-35. URL <a class="calibre1" href="http://www.jstatsoft.org/v54/i02/">http://www.jstatsoft.org/v54/i02/</a>.</p><p class="calibre7">Angelo Canty and Brian Ripley (2017). <code class="literal">boot</code>: Bootstrap R (S-Plus)</p><p class="calibre7">  Functions. R package version 1.3-19.</p><p class="calibre7">John Fox and Sanford Weisberg (2011). An {R} Companion to Applied</p><p class="calibre7">  Regression, Second Edition. Thousand Oaks CA: Sage. URL:</p><p class="calibre7">  <a class="calibre1" href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a> <code class="literal">car</code>
</p><p class="calibre7">Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams,</p><p class="calibre7">  Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton</p><p class="calibre7">  Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew</p><p class="calibre7">  Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2017).</p><p class="calibre7">  <code class="literal">caret</code>: Classification and Regression Training. R package version</p><p class="calibre7">  6.0-77. <a class="calibre1" href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>
</p><p class="calibre7">Zachary A. Deane-Mayer and Jared E. Knowles (2016). <code class="literal">caretEnsemble</code>:</p><p class="calibre7">  Ensembles of Caret Models. R package version 2.0.0.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=caretEnsemble">https://CRAN.R-project.org/package=caretEnsemble</a> Venables, W. N. &amp; Ripley, B. D. (2002) Modern Applied Statistics</p><p class="calibre7">  with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0 <code class="literal">class</code>
</p><p class="calibre7">Marie Chavent, Vanessa Kuentz, Benoit Liquet and Jerome Saracco</p><p class="calibre7">  (2017). <code class="literal">ClustOfVar</code>: Clustering of Variables. R package version 1.1.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=ClustOfVar">https://CRAN.R-project.org/package=ClustOfVar</a> David Meyer, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel</p><p class="calibre7">  and Friedrich Leisch (2017). <code class="literal">e1071</code>: Misc Functions of the Department</p><p class="calibre7">  of Statistics, Probability Theory Group (Formerly: E1071), TU Wien.</p><p class="calibre7">  R package version 1.6-8. <a class="calibre1" href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a> Alboukadel Kassambara and Fabian Mundt (2017). <code class="literal">factoextra</code>: Extract</p><p class="calibre7">  and Visualize the Results of Multivariate Data Analyses. R package</p><p class="calibre7">  version 1.0.5. <a class="calibre1" href="https://CRAN.R-project.org/package=factoextra">https://CRAN.R-project.org/package=factoextra</a> </p><p class="calibre7">Sebastien Le, Julie Josse, Francois Husson (2008). <code class="literal">FactoMineR</code>: An R</p><p class="calibre7">  Package for Multivariate Analysis. Journal of Statistical Software,</p><p class="calibre7">  25(1), 1-18. 10.18637/jss.v025.i01  </p><p class="calibre7"> Alina Beygelzimer, Sham Kakadet, John Langford, Sunil Arya, David</p><p class="calibre7">  Mount and Shengqiao Li (2013). <code class="literal">FNN</code>: Fast Nearest Neighbor Search</p><p class="calibre7">  Algorithms and Applications. R package version 1.1.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=FNN">https://CRAN.R-project.org/package=FNN</a> </p><p class="calibre7">Hyndman RJ (2017). <code class="literal">_forecast</code>: Forecasting functions for time series</p><p class="calibre7">and linear models_. R package version 8.2, &lt;URL:</p><p class="calibre7">
<a class="calibre1" href="http://pkg.robjhyndman.com/forecast">http://pkg.robjhyndman.com/forecast</a>&gt;.</p><p class="calibre7">David Shaub and Peter Ellis (2018). forecastHybrid: Convenient</p><p class="calibre7">  Functions for Ensemble Time Series Forecasts. R package version</p><p class="calibre7">  2.0.10. <a class="calibre1" href="https://CRAN.R-project.org/package=forecastHybrid">https://CRAN.R-project.org/package=forecastHybrid</a> </p><p class="calibre7">Greg Ridgeway with contributions from others (2017). <code class="literal">gbm</code>:</p><p class="calibre7">  Generalized Boosted Regression Models. R package version 2.1.3.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=gbm">https://CRAN.R-project.org/package=gbm</a> </p><p class="calibre7">Vincent J Carey. Ported to R by Thomas Lumley and Brian Ripley. Note</p><p class="calibre7">  that maintainers are not available to give advice on using a package</p><p class="calibre7">  they did not author. (2015). <code class="literal">gee</code>: Generalized Estimation Equation</p><p class="calibre7">  Solver. R package version 4.13-19.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=gee">https://CRAN.R-project.org/package=gee</a> </p><p class="calibre7">The H2O.ai team (2017). <code class="literal">h2o</code>: R Interface for H2O. R package version</p><p class="calibre7">  3.16.0.2. <a class="calibre1" href="https://CRAN.R-project.org/package=h2o">https://CRAN.R-project.org/package=h2o</a> </p><p class="calibre7">Andrea Peters and Torsten Hothorn (2017). <code class="literal">ipred</code>: Improved</p><p class="calibre7">  Predictors. R package version 0.9-6.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=ipred">https://CRAN.R-project.org/package=ipred</a> </p><p class="calibre7">Alexandros Karatzoglou, Alex Smola, Kurt Hornik, Achim Zeileis</p><p class="calibre7">  (2004). <code class="literal">kernlab</code> - An S4 Package for Kernel Methods in R. Journal of</p><p class="calibre7">  Statistical Software 11(9), 1-20. URL</p><p class="calibre7">  <a class="calibre1" href="http://www.jstatsoft.org/v11/i09/">http://www.jstatsoft.org/v11/i09/</a> </p><p class="calibre7">Friedrich Leisch &amp; Evgenia Dimitriadou (2010). <code class="literal">mlbench</code>: Machine</p><p class="calibre7">  Learning Benchmark Problems. R package version 2.1-1.</p><p class="calibre7">Daniel J. Stekhoven (2013). <code class="literal">missForest</code>: Nonparametric Missing Value</p><p class="calibre7">  Imputation using Random Forest. R package version 1.4.</p><p class="calibre7">  Alan Genz, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, Friedrich Leisch,</p><p class="calibre7">Fabian Scheipl, Torsten Hothorn (2017). <code class="literal">mvtnorm</code>: Multivariate Normal</p><p class="calibre7">  and t Distributions. R package version 1.0-6. URL</p><p class="calibre7">  <a class="calibre1" href="http://CRAN.R-project.org/package=mvtnorm">http://CRAN.R-project.org/package=mvtnorm</a> </p><p class="calibre7">Beck M (2016). <code class="literal">_NeuralNetTools</code>: Visualization and Analysis Tools for</p><p class="calibre7">Neural Networks_. R package version 1.5.0, &lt;URL:</p><p class="calibre7">
<a class="calibre1" href="https://CRAN.R-project.org/package=NeuralNetTools">https://CRAN.R-project.org/package=NeuralNetTools</a>&gt;. </p><p class="calibre7">Venables, W. N. &amp; Ripley, B. D. (2002) Modern Applied Statistics</p><p class="calibre7">  with S. Fourth Edition. Springer, New York. ISBN 0-387-95457-0 <code class="literal">nnet</code>
</p><p class="calibre7">Michael P. Fay, Pamela A. Shaw (2010). Exact and Asymptotic Weighted</p><p class="calibre7">  Logrank Tests for Interval Censored Data: The interval R Package.</p><p class="calibre7">  Journal of Statistical Software, 36(2), 1-34. URL</p><p class="calibre7">  <a class="calibre1" href="http://www.jstatsoft.org/v36/i02/">http://www.jstatsoft.org/v36/i02/</a>. <code class="literal">perm</code>
</p><p class="calibre7">Hadley Wickham (2011). The Split-Apply-Combine Strategy for Data</p><p class="calibre7">  Analysis. Journal of Statistical Software, 40(1), 1-29. URL</p><p class="calibre7">  <a class="calibre1" href="http://www.jstatsoft.org/v40/i01/">http://www.jstatsoft.org/v40/i01/</a>. <code class="literal">plyr</code>
</p><p class="calibre7">Xavier Robin, Natacha Turck, Alexandre Hainard, Natalia Tiberti,</p><p class="calibre7">  Frédérique Lisacek, Jean-Charles Sanchez and Markus Müller (2011).</p><p class="calibre7">  <code class="literal">pROC</code>: an open-source package for R and S+ to analyze and compare ROC</p><p class="calibre7">  curves. BMC Bioinformatics, 12, p. 77.  DOI: 10.1186/1471-2105-12-77</p><p class="calibre7">  <a class="calibre1" href="http://www.biomedcentral.com/1471-2105/12/77/">http://www.biomedcentral.com/1471-2105/12/77/</a> </p><p class="calibre7">Maja Pohar Perme and Mette Gerster (2017). <code class="literal">pseudo</code>: Computes</p><p class="calibre7">  Pseudo-Observations for Modeling. R package version 1.4.3.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=pseudo">https://CRAN.R-project.org/package=pseudo</a> </p><p class="calibre7">A. Liaw and M. Wiener (2002). Classification and Regression by</p><p class="calibre7">  <code class="literal">randomForest</code>. R News 2(3), 18--22.</p><p class="calibre7">Aleksandra Paluszynska and Przemyslaw Biecek (2017).</p><p class="calibre7">  <code class="literal">randomForestExplainer</code>: Explaining and Visualizing Random Forests in</p><p class="calibre7">  Terms of Variable Importance. R package version 0.9.</p><p class="calibre7">  <a class="calibre1" href="https://CRAN.R-project.org/package=randomForestExplainer">https://CRAN.R-project.org/package=randomForestExplainer</a> </p><p class="calibre7">Terry Therneau, Beth Atkinson and Brian Ripley (2017). <code class="literal">rpart</code>:</p><p class="calibre7">  Recursive Partitioning and Regression Trees. R package version</p><p class="calibre7">  4.1-11. <a class="calibre1" href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a> </p><p class="calibre7">Prabhanjan Tattar (2013). <code class="literal">RSADBE</code>: Data related to the book "R</p><p class="calibre7">  Statistical Application Development by Example". R package version</p><p class="calibre7">  1.0. <a class="calibre1" href="https://CRAN.R-project.org/package=RSADBE">https://CRAN.R-project.org/package=RSADBE</a> </p><p class="calibre7">Therneau T (2015). _A Package for Survival Analysis in S_. version</p><p class="calibre7">2.38, &lt;URL: <a class="calibre1" href="https://CRAN.R-project.org/package=survival">https://CRAN.R-project.org/package=survival</a>&gt;. <code class="literal">survival</code>
</p><p class="calibre7">Terry M. Therneau and Patricia M. Grambsch (2000). _Modeling Survival</p><p class="calibre7">Data: Extending the Cox Model_. Springer, New York. ISBN</p><p class="calibre7">0-387-98784-3.</p><p class="calibre7">Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich and Yuan</p><p class="calibre7">  Tang (2018). <code class="literal">xgboost</code>: Extreme Gradient Boosting. R package version</p><p class="calibre7">  0.6.4.1. <a class="calibre1" href="https://CRAN.R-project.org/package=xgboost">https://CRAN.R-project.org/package=xgboost</a>
</p></div></body></html>