- en: High-Performance Computing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能计算
- en: In this chapter, we will learn about the specific aspects of virtualization
    that can enhance the productivity of a **High Performance Computing** (**HPC**)
    environment. We will focus on the capabilities provided by VMware vSphere, and
    how virtualization improves scientific productivity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解可以增强**高性能计算**（**HPC**）环境生产力的虚拟化特定方面。我们将关注VMware vSphere提供的能力，以及虚拟化如何提高科学生产力。
- en: We will explore vSphere features, such as **Single Root I/O Virtualization**
    (**SR-IOV**), **remote direct memory access** (**RDMA**), and vGPU, to architect
    and meet the requirements for research, computing, academic, scientific, and engineering
    workloads.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨vSphere功能，例如**单根I/O虚拟化**（**SR-IOV**）、**远程直接内存访问**（**RDMA**）和vGPU，以构建和满足研究、计算、学术、科学和工程工作负载的需求。
- en: 'This chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Virtualizing HPC applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟化HPC应用
- en: Multi-tenancy with guaranteed resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证资源的多租户
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can download vSphere Scale-Out from [https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7](https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7](https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7)下载vSphere
    Scale-Out。
- en: Virtualizing HPC applications
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟化HPC应用
- en: This chapter describes our work at VMware to support HPC applications. The first
    section describes in detail many of the values identified by customers of using
    virtualization in HPC environments. The second section shows a few examples of
    how virtualization is deployed in an HPC environment, and the third discusses
    various aspects of performance, starting with an examination of some core aspects
    of performance, and then turning to throughput applications and performance for
    parallel-distributed **M****essage Passing Interface** (**MPI**) applications.
    It also includes pointers to several technical publications that will be of interest
    to those considering virtualizing their HPC workloads.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了VMware在支持HPC应用方面的工作。第一部分详细描述了客户在使用HPC环境中的虚拟化所识别出的许多价值。第二部分展示了在HPC环境中部署虚拟化的几个示例，第三部分讨论了性能的各个方面，从检查性能的一些核心方面开始，然后转向吞吐量应用和并行分布式**消息传递接口**（**MPI**）应用的性能。还包括指向一些技术出版物，这些出版物将对考虑虚拟化其HPC工作负载的人感兴趣。
- en: The majority of HPC systems are clusters, which are aggregations of compute
    nodes connected via some interconnect, such as Ethernet or **InfiniBand** (**IB**).
    Clusters can range in size from a small handful of nodes to tens of thousands
    of nodes. HPC clusters exist to run HPC jobs, and the placement of those jobs
    in the cluster is handled by a **distributed resource manager** (**DRM**). DRM
    is the middleware that provides the ability for HPC users to launch their HPC
    jobs onto an HPC cluster in a load-balanced fashion.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数HPC系统都是集群，它们是通过某些互连（如以太网或**InfiniBand**（**IB**））连接的计算节点集合。集群的大小可以从几个节点到数万个节点不等。HPC集群的存在是为了运行HPC作业，而这些作业在集群中的放置由**分布式资源管理器**（**DRM**）处理。DRM是中间件，它为HPC用户提供了一种在负载均衡的方式下将他们的HPC作业启动到HPC集群上的能力。
- en: Users typically use command-line interfaces to specify the characteristics of
    the job or jobs they want to run, the DRM then enqueues those requests and schedules
    the jobs to run on the least loaded, appropriately configured nodes in the cluster.
    There are many DRMs available, both open source and commercial. Examples include
    Grid Engine (Univa), LSF (IBM), Torque, **Portable Batch System** (**PBS**), and
    Slurm. DRMs are also called batch schedulers. IB is a high-bandwidth, low-latency
    interconnect, commonly used in HPC environments to boost the performance of code/applications/jobs
    and to increase filesystem performance. IB is not Ethernet, it does not use TCP
    or any of the standard networking stack, and it is currently usable in a virtual
    environment only via VM direct path I/O (passthrough mode).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通常使用命令行界面来指定他们想要运行的工作或作业的特征，然后DRM将这些请求排队并调度作业在集群中负载最低、配置适当的节点上运行。有许多DRM可供选择，包括开源和商业的。例如包括Grid
    Engine（Univa）、LSF（IBM）、Torque、**可移植批处理系统**（**PBS**）和Slurm。DRM也被称为批处理调度器。IB是一种高带宽、低延迟的互连，常用于HPC环境中以提升代码/应用程序/作业的性能以及提高文件系统性能。IB不是以太网，它不使用TCP或任何标准网络堆栈，并且目前只能通过VM直接路径I/O（透传模式）在虚拟环境中使用。
- en: The purpose of this chapter is not to explain how virtualization works. x86
    virtualization has evolved from its invention at Stanford in the late 1990s using
    a purely software-based approach to the current situation in which both Intel
    and AMD have added successively more hardware support for virtualization, such
    as CPU, memory, and I/O. These hardware enhancements, along with increasingly
    sophisticated virtualization software, have greatly improved the performance for
    an ever-growing number of workloads. This is an important point to make because
    HPC people have often heard that the performance of HPC applications is very poor
    when they get virtualized with some cases but run extremely well, even very close
    to native performance, in most of the cases. Massive consolidation isn't appropriate
    for HPC environments, so the values of virtualization for HPC can be utilized
    elsewhere.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是不解释虚拟化是如何工作的。x86虚拟化从20世纪90年代末在斯坦福大学发明时使用纯软件方法发展到目前的情况，即英特尔和AMD都相继增加了对虚拟化的更多硬件支持，如CPU、内存和I/O。这些硬件增强功能以及日益复杂的虚拟化软件，大大提高了对不断增长的工作负载的性能。这是一个重要观点，因为HPC人员经常听说，当HPC应用程序虚拟化时，其性能非常差，有些情况下甚至运行得非常好，甚至在大多数情况下非常接近原生性能。大规模整合不适合HPC环境，因此虚拟化对HPC的价值可以在其他地方得到利用。
- en: We will now learn the use cases for virtualization in HPC that have been identified
    by customers and through our own research. Because HPC includes such a wide variety
    of workloads and environments, some of these will resonate more with specific
    customers than others. HPC clusters host a single, standard OS and application
    stack across all hosts as uniformity enables us to schedule jobs easily by limiting
    the options in these environments for different use cases, such as multiple user
    groups needing to be served from a single shared resource. Because these traditional
    clusters can't satisfy the needs of multiple groups, they encourage the creation
    of specialized *islands of compute* scattered across an organization, which is
    inefficient and expensive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习客户和我们自己研究确定的HPC虚拟化用例。由于HPC包括如此广泛的工作负载和环境，其中一些可能比其他一些更符合特定客户的需求。HPC集群在所有主机上托管单个、标准的操作系统和应用程序堆栈，因为一致性使我们能够通过限制这些环境中的不同用例选项来轻松调度作业，例如需要从单个共享资源中服务的多个用户组。由于这些传统集群无法满足多个组的需求，它们鼓励在组织内创建分散的专用*计算岛*，这既低效又昂贵。
- en: Multi-tenancy with guaranteed resources
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有保证资源的多租户
- en: Customers want to utilize existing investments in hardware and software such
    as in hypervisors or physical hardware along with the feasibility to provision
    directly into the public cloud. We can address this landscape, along with related
    services, through a service-automation solution that can manage across many platforms
    and many clouds. This solution can automate all manual processes of the provisioning
    services by abstracting the core tasks with its automation tool and then managing
    the access and control of these automations. Automation is very useful only if
    we link it with policy. Policy-based control and governance provides us with the
    ability to control the application of the automations that drive the cloud solution.
    The cloud service portal and catalog give end users self-service, on-demand access
    to authorized services.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 客户希望利用现有的硬件和软件投资，例如在虚拟机管理程序或物理硬件上，以及直接部署到公共云的可行性。我们可以通过一个能够跨多个平台和多个云管理的服务自动化解决方案来解决这个问题以及相关的服务。该解决方案可以通过其自动化工具抽象核心任务来自动化所有手动过程的服务配置，然后管理这些自动化的访问和控制。自动化只有在将其与策略链接时才非常有用。基于策略的控制和治理为我们提供了控制驱动云解决方案的自动化应用的能力。云服务门户和目录为最终用户提供自助、按需访问授权服务。
- en: All modern x86 systems are **non-uniform memory access** (**NUMA**) systems,
    which means that memory is attached directly to individual CPU sockets in the
    system. This means that access to memory from the local socket can be very fast,
    but access to memory that is attached to another socket will be slower because
    the request and data have to transit the communication pathway between the sockets.
    That's why it is called non-uniform memory access. Sometimes workloads can run
    a little faster in virtual environments than on bare metal. This is true especially
    of throughput workloads and is often the result of NUMA effects. The point isn't
    so much that virtualized can run faster than bare metal, but that virtualized's
    performance can be nearly identical to the bare-metal performance for some HPC
    workloads. Live migration can be used to increase the efficiency and flexibility
    of HPC environments. It can also be used to increase resiliency. In traditional
    bare-metal HPC environments, jobs are placed statically. Consider the scenario
    in which Application C must be scheduled, but there is currently no node with
    enough resources available to run it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有现代x86系统都是**非一致性内存访问**（**NUMA**）系统，这意味着内存直接连接到系统中的单个CPU插槽。这意味着从本地插槽访问内存可以非常快，但访问连接到另一个插槽的内存将会较慢，因为请求和数据必须通过插槽之间的通信路径进行传输。这就是为什么它被称为非一致性内存访问。有时，工作负载在虚拟环境中比在裸机环境中运行得更快。这尤其适用于吞吐量工作负载，通常是由于NUMA效应。重点不在于虚拟化可以比裸机运行得更快，而在于虚拟化的性能对于某些高性能计算（HPC）工作负载来说可以接近裸机性能。实时迁移可以用来提高HPC环境的效率和灵活性。它还可以用来提高弹性。在传统的裸机HPC环境中，作业是静态放置的。考虑以下场景：应用程序C必须被调度，但目前没有足够的资源节点可供其运行。
- en: 'There are a two choices in a bare-metal environment:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸机环境中有两个选择：
- en: Application C can wait in the queue until Application A or B finishes
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序C可以在队列中等待，直到应用程序A或B完成
- en: Application A or B could be killed to make room for Application C
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以终止应用程序A或B来为应用程序C腾出空间
- en: Either of these options reduces the cluster's job throughput. And in the case
    of killing jobs, the loss of work can be very expensive if the running applications
    are costly Independent Software Vendors' application. The solution in the virtual
    environment is to use live migration to move the workload to make room for Application
    C. This approach is relevant primarily in environments where these jobs are relatively
    long-running.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种选项都会降低集群的工作吞吐量。在终止作业的情况下，如果运行的应用程序是昂贵的独立软件供应商的应用程序，那么工作量的损失可能会非常昂贵。在虚拟环境中，解决方案是使用实时迁移来移动工作负载，为应用程序C腾出空间。这种方法主要适用于这些作业相对长时间运行的环境。
- en: 'Let''s look at another use of live migration: to increase overall throughput
    on a cluster. Consider the bare-metal environment with two jobs running. As the
    third job is started, it may consume more memory than expected by the user. This
    will bound other jobs on the system to swap, which will affect the overall performance
    of these jobs in negative way. **Dynamic Resource Scheduler** (**DRS**) can address
    these kind of situations in a virtual environment: as the third job starts to
    consume all available memory, DRS can shift the overloaded VM to another machine
    with few workloads and help the jobs to continue running with the desired performance.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看实时迁移的另一种用途：提高集群的整体吞吐量。考虑裸机环境中运行的两个作业。当第三个作业启动时，它可能比用户预期的消耗更多内存。这将导致系统上的其他作业绑定到交换空间，从而以负面方式影响这些作业的整体性能。**动态资源调度器（DRS**）可以在虚拟环境中解决这类情况：当第三个作业开始消耗所有可用内存时，DRS可以将过载的虚拟机转移到负载较少的其他机器上，并帮助作业以期望的性能继续运行。
- en: This lack of guaranteed resource use for specific groups or departments is another
    barrier to the centralization of bare-metal environments. For example, an owner
    of a large island of compute will often be unwilling to donate their hardware
    resources to a shared pool if they cannot be guaranteed access to at least those
    resources when required. DRS provides the ability to address this need. Snapshots
    can be used to save the state of a running VM to protect against hardware failures;
    when a machine fails, the VM is restored and the application continues the execution
    from the point at which the snapshot was taken.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对特定组或部门资源使用保证的缺乏是裸机环境集中化的另一个障碍。例如，一个大型计算岛的所有者通常不愿意将他们的硬件资源捐赠给共享池，如果他们不能保证在需要时至少可以访问那些资源。DRS提供了满足这一需求的能力。快照可以用来保存正在运行的虚拟机的状态，以防止硬件故障；当机器失败时，虚拟机被恢复，应用程序从快照被捕获的点继续执行。
- en: This is similar in concept to checkpoint mechanisms used in HPC, except that,
    rather than extracting the state of a process from a running OS, which is often
    subject to various limitations, we take advantage of the clean abstract that exists
    between the VM and the underlying virtual platform. A more advanced resiliency
    scheme (Proactive HA Policy) would use telemetry from the underlying system to
    predict upcoming hardware failures and then proactively migrate the workload from
    the questionable host to avoid application interruption. Examples include a fan
    failure on a system running an important job or an increase in the rate of detected
    soft memory errors, which could indicate increased probability of an upcoming
    hard memory error.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，这与HPC中使用的检查点机制相似，但不同的是，我们不是从正在运行的操作系统提取进程的状态，这通常受到各种限制，而是利用虚拟机和底层虚拟平台之间存在的干净抽象。更高级的弹性方案（主动高可用策略）将使用底层系统的遥测数据来预测即将发生的硬件故障，然后主动将工作负载从可疑的主机迁移到其他主机，以避免应用程序中断。例如，一个运行重要作业的系统上的风扇故障或检测到的软内存错误率的增加，这可能会表明即将发生的硬内存错误概率增加。
- en: While such an approach would not likely remove the need for checkpointing, it
    may reduce the need. Less frequent checkpoint operations and less frequent restorations
    from checkpoints can increase overall job throughput on the cluster.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法可能不会消除检查点的需要，但它可能会减少这种需要。更频繁的检查点操作和从检查点更频繁的恢复可以增加集群上的整体作业吞吐量。
- en: Critical use case – unification
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键用例 - 统一
- en: The desire of some administrators is to treat their HPC jobs as *just another
    workload* and move away from a partially virtual and partially physical infrastructure
    to an all-virtual infrastructure with more flexibility and manageability. This
    is a simple example of a deployment done by one of our financial services customers.
    In this case, the central IT department created a shared compute resource that
    could rent out virtual clusters to the different lines of business within the
    organization. Groups that need access to a cluster for some period of time would
    receive a set of VMs rather than a physical machine to be used for some specified
    time period.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些管理人员的愿望是将他们的HPC工作视为*仅仅是另一种工作负载*，并从部分虚拟和部分物理基础设施转向全部虚拟基础设施，以获得更大的灵活性和可管理性。这是一个我们金融服务客户所做的部署的简单例子。在这种情况下，中央IT部门创建了一个共享的计算资源，可以向组织内的不同业务线出租虚拟集群。需要一段时间访问集群的组将收到一组虚拟机，而不是用于指定时间段的物理机器。
- en: 'The benefits to the organization are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 组织从中获得的益处如下：
- en: '**Lines-of-business** (**LOBs**) get the resources they need when they need
    them'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务线**（**LOBs**）在需要时获得所需的资源'
- en: Clusters and the cluster nodes can be sized to the LOBs' application requirements
    (for example, most process runs consume only a single CPU per job; they are serial
    (not parallel) jobs)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群和集群节点可以根据LOB的应用需求进行规模调整（例如，大多数进程运行仅消耗每个作业的单个CPU；它们是串行（不是并行）作业）
- en: A central IT team can extract maximum from the available hardware
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中央IT团队能够从可用的硬件中提取最大价值
- en: Hardware is flexible enough to be shared among various jobs with security compliance
    between users/workloads
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件足够灵活，可以在各种作业之间共享，同时用户/工作负载之间保持安全合规
- en: 'Relative priorities between LOBs can be enforced by policy to ensure (for example)
    that groups with hard deadlines receive a higher *fairshare* of the underlying
    hardware resources:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过策略强制执行LOB之间的相对优先级，以确保（例如）具有硬截止日期的组获得底层硬件资源的更高*公平份额*：
- en: '![](img/4fc33cfb-9cf0-415e-8f8a-550857f6fe7c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4fc33cfb-9cf0-415e-8f8a-550857f6fe7c.png)'
- en: There are certain benefits of configuring platform-level (and sometimes guest-level)
    tuning in order to achieve best performance for HPC applications on vSphere. Tuning
    is needed because HPC applications (unlike most Enterprise applications) are latency-sensitive.
    The sensitivity might be in storage, in networking, or in the communication interconnect.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在vSphere上为HPC应用程序实现最佳性能，配置平台级（有时是客户机级）调整以获得某些好处。由于HPC应用程序（与大多数企业应用程序不同）对延迟敏感，因此需要进行调整。这种敏感性可能存在于存储、网络或通信互连中。
- en: While VMware's goal is excellent out-of-the-box performance for any application,
    the HPC workloads are relatively new to us, so some tuning is required. We have
    begun setting some of these tunables automatically by configuring vCenter advanced
    VM parameters where we intend to run latency-sensitive workloads in a VM. This
    auto-tuning will become more comprehensive over time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然VMware的目标是为任何应用程序提供出色的即插即用性能，但HPC工作负载对我们来说相对较新，因此需要进行一些调整。我们已经开始通过配置vCenter高级虚拟机参数来自动设置一些可调整参数，我们打算在这些参数中运行对延迟敏感的工作负载。随着时间的推移，这种自动调整将变得更加全面。
- en: The customer's initial **Network Filesystem Storage** (**NFS**) benchmark experience
    pointed directly to storage latencies in some cases. By tuning the networking
    stack that underlies any NFS data transfers, we were able to bring application
    performance directly in line, as seen in bare-metal environments. The network
    is by default tuned for throughput by moving large amounts of data through the
    network efficiently in an enterprise environment. This means that, as data packets
    arrive, they may not be processed immediately.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 客户最初的**网络文件系统存储**（**NFS**）基准测试体验直接指向了一些存储延迟问题。通过调整任何NFS数据传输背后的网络堆栈，我们能够直接提升应用性能，正如在裸机环境中所见。默认情况下，网络通过在企业环境中高效地通过大量数据来优化吞吐量。这意味着，当数据包到达时，它们可能不会立即被处理。
- en: Typically, a small number of messages are allowed to accumulate before the system
    wakes up and processes the entire batch. This reduces the load on the CPU, but
    it slows down message delivery. It makes much more sense to spend more CPU cycles
    to process the packets promptly as each packet arrives in situations where the
    arrival of the data is a gating factor on performance. This is often the case
    with HPC workloads. Both the virtual and physical network devices should have
    coalescing turned off to make this change.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在系统醒来并处理整个批次之前，允许少量消息积累。这减少了CPU的负载，但会减慢消息传递。在数据到达是性能瓶颈的情况下，更合理的是花费更多的CPU周期来及时处理每个到达的数据包。这在HPC工作负载中通常是这种情况。虚拟和物理网络设备都应该关闭合并，以实现这一变化。
- en: There is another level of coalescing, which happens at a higher level in the
    **t****ransmission control protocol** (**TCP**) stack (disable **Large Receive
    Offload** (**LRO**) within the guest)—it should be turned off. We evaluate results
    of some experiments to see whether the additional level of memory abstraction
    which is introduced by virtualization, has any effect on HPC application performance
    or not.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在**传输控制协议**（**TCP**）堆栈的更高级别存在另一个合并级别（在客户机内禁用**大接收卸载**（**LRO**））——它应该被关闭。我们评估了一些实验的结果，以查看虚拟化引入的额外内存抽象级别是否对HPC应用程序性能有任何影响。
- en: The special circumstance is applications that have little or no spatial data
    locality. This includes applications that can't effectively use their caches because
    they don't access memory in regular ways, such as the random-access benchmark.
    It doesn't affect except in a special circumstance where the overhead can be greatly
    reduced by using large pages within the guest operating system. Turning off EPT
    (RVI on AMD) and reverting to a software-based page-table approach can also help
    in this special circumstance. Created by the **National Security Agency** (**NSA**),
    it has no locality even updating random memory locations one after another.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 特殊情况是那些几乎没有空间数据局部性的应用程序。这包括那些无法有效使用其缓存的应用程序，因为它们不以常规方式访问内存，例如随机访问基准测试。除非在特殊情况下，通过在客户操作系统中使用大页面可以大大减少开销，否则它不会受到影响。关闭EPT（AMD上的RVI）并恢复基于软件的页表方法也可以在这种特殊情况下有所帮助。由**国家安全局（NSA**）创建，它甚至更新随机内存位置时也没有局部性。
- en: These workloads incur a very large number of **translation look-aside buffer** (**TLB**)
    misses. Because misses in this page-table cache are frequent, the application
    can be slowed down if that operation is slow. Indeed, it turns out that the EPT
    and RVI technologies created by Intel and RVI, although implemented in hardware,
    are slower at handling TLB misses than the older *shadow page table* approach
    developed by VMware. TLB misses can be reduced by using larger pages, so turning
    off EPT or RVI can help in these situations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工作负载会引发大量的**翻译查找缓冲区（TLB**）缺失。由于页表缓存中的缺失频繁发生，如果该操作缓慢，应用程序可能会变慢。实际上，英特尔和VMware创建的EPT和RVI技术，尽管在硬件中实现，但在处理TLB缺失方面比VMware开发的较老**影子页表**方法要慢。通过使用更大的页面可以减少TLB缺失，因此在这些情况下关闭EPT或RVI可能会有所帮助。
- en: The fact is that EPT and RVI perform very well in the vast majority of cases,
    but it pays to keep the issue in mind. **High-performance LINPACK** (**HPL**)
    uses caches very nicely like other HPC applications and get better performance.
    We can see that performance is uniformly excellent for this application type.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，EPT和RVI在绝大多数情况下表现良好，但保持这个问题在心中是有益的。**高性能LINPACK（HPL**）像其他高性能计算（HPC）应用程序一样很好地使用缓存，并获得了更好的性能。我们可以看到，对于这种应用程序类型，性能是一致的优秀。
- en: This is an important aspect of our virtualization platform for HPC customers
    who run applications that require large numbers of threads, and, therefore, VMs
    that would span multiple CPU sockets within a host.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是针对运行需要大量线程的应用程序且虚拟机将跨越主机内多个CPU插槽的高性能计算（HPC）客户的虚拟化平台的一个重要方面。
- en: ESXi has been *NUMA-aware* for many releases, which means that when it runs
    a VM, it is careful to place the executing thread on the same socket that is hosting
    the VM's memory so all memory accesses are local. When a VM spans the two sockets,
    we distribute similar threads across both sockets and then allocate memory on
    the local socket to deliver the best performance. So, even if the VM were big
    enough to span multiple sockets, it would not see the NUMA-ness of the underlying
    hardware. We introduced vNUMA to make the NUMA topology visible to the guest OS,
    which can then make its own optimizations based on this information. This can
    matter a lot from a performance perspective.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ESXi已经对**NUMA（非一致性内存访问**）有感知，这意味着当它运行虚拟机时，它会小心地将执行线程放置在托管虚拟机内存的相同插槽上，以便所有内存访问都是本地的。当一个虚拟机跨越两个插槽时，我们将类似的线程分布在两个插槽上，然后在本地插槽上分配内存以提供最佳性能。因此，即使虚拟机足够大，可以跨越多个插槽，它也不会看到底层硬件的NUMA特性。我们引入了vNUMA，使NUMA拓扑对客户操作系统可见，然后可以根据这些信息进行自己的优化。这在性能方面可能非常重要。
- en: High-performance computing cluster performances
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能计算集群性能
- en: '**Standard Performance Evaluation Corporation OpenMP** (**SPECOMP**) is a well
    known HPC benchmark suite for multi-threaded applications running on multiple
    nodes. Each benchmark (for example, Swim) is listed along with the *x* axis.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准性能评估公司OpenMP（SPECOMP**）是针对在多个节点上运行的**多线程应用程序**的知名高性能计算（HPC）基准测试套件。每个基准测试（例如，Swim）都列在x轴上。'
- en: 'For each benchmark, there are three pairs of comparisons: one for a 16-vCPU
    VM, one for a 32-vCPU VM, and one for a 64-way VM. Default-16 means performance
    for a 16-way VM with no vNUMA support and vNUMA-16 means the same 16-way VM, but
    with vNUMA enabled.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个基准测试，有三个比较对：一个用于16个vCPU虚拟机，一个用于32个vCPU虚拟机，一个用于64路虚拟机。默认-16表示没有vNUMA支持的16路虚拟机的性能，而vNUMA-16表示相同的16路虚拟机，但启用了vNUMA。
- en: '**Ratio to Native**, lower is better:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**与原生相比的比率**，越低越好：'
- en: '![](img/edbb2495-532b-495b-bd7c-2e023749442e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/edbb2495-532b-495b-bd7c-2e023749442e.png)'
- en: The chart shows run-times, so lower is better. We can see significant run-time
    drops in virtually all cases when moving from default to vNUMA. This is a hugely
    important feature for HPC users with a need for wide VMs. These charts show published
    performance results for a variety of life sciences workloads running on ESXi.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了运行时间，所以越低越好。我们可以看到，在从默认值切换到vNUMA时，几乎所有情况下都会出现显著的运行时间下降。这对于需要广泛虚拟机的HPC用户来说是一个非常重要的功能。这些图表显示了在ESXi上运行的多种生命科学工作负载的已发布性能结果。
- en: They show that these throughput-oriented applications run with generally under
    a 5% penalty when virtualized. More recent reports from customers indicate that
    this entire class of applications (throughput), which includes not just life sciences,
    but financial services, **electronic design automation** (**EDA**) chip designers,
    and digital content creation (movie rendering, and so on), run with well under
    a 5% performance degradation. Platform tuning, not application tuning, is required
    to achieve these results.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们表明，这些以吞吐量为导向的应用程序在虚拟化时通常低于5%的惩罚。来自客户的最新报告表明，这一类应用程序（吞吐量），不仅包括生命科学，还包括金融服务、**电子设计自动化**（**EDA**）芯片设计者和数字内容创作（电影渲染等），其性能下降都远远低于5%。要实现这些结果，需要平台调整而不是应用程序调整。
- en: We have results reported by one of EDA (chip designer) customers who ran first
    a single instance of one of their EDA jobs on a bare-metal Linux node. They then
    ran the same Linux and the same job on ESXi and compared the results. They saw
    a 6% performance degradation. We believe this would be lower with additional platform
    tuning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有来自EDA（芯片设计）客户的结果报告，他们在裸金属Linux节点上首先运行了他们EDA作业的一个实例。然后，他们在ESXi上运行了相同的Linux和相同的作业，并比较了结果。他们看到了6%的性能下降。我们相信，通过额外的平台调整，这个数字会更低。
- en: They then ran the second test with four instances of the application running
    in a single Linux instance versus four VMs running the same jobs. So, we have
    the same workload running in both cases. In this configuration, they discovered
    that the virtual jobs completed 2% sooner than the bare-metal jobs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们进行了第二次测试，将四个应用程序实例在一个Linux实例中运行与四个运行相同作业的虚拟机进行了比较。因此，在这两种情况下，我们都有相同的工作负载。在这个配置中，他们发现虚拟作业比裸金属作业提前2%完成。
- en: 'HPCC performance ratios (lower is better):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: HPCC性能比率（越低越好）：
- en: '![](img/b31757da-ef1a-4feb-bab3-aaea8c483f5f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b31757da-ef1a-4feb-bab3-aaea8c483f5f.png)'
- en: This speedup usually results from a NUMA effect and an OS scheduling effect.
    The Linux instance must do resource balancing between the four job instances and
    it must also handle the NUMA issues related to this being a multi-socket system.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加速通常源于NUMA效应和操作系统调度效应。Linux实例必须在四个作业实例之间进行资源平衡，并且还必须处理与这是一个多插槽系统相关的NUMA问题。
- en: 'Virtualization will help us with the following advantages:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟化将帮助我们获得以下优势：
- en: Each Linux instance must handle only one job
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Linux实例只需处理一个作业
- en: Because of the ESXi scheduler's NUMA awareness, each VM would be scheduled onto
    a socket so none of the Linux instances needs to suffer the potential inefficiencies
    of dealing with NUMA issues
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于ESXi调度器的NUMA意识，每个虚拟机将被调度到插槽上，这样Linux实例就不需要承受处理NUMA问题的潜在低效。
- en: We don't have to worry about multiple Linux instances and VMs consuming more
    memory: **transparent page sharing** (**TPS**) can mitigate this as the hypervisor
    will find common pages between VMs and share them where possible.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必担心多个Linux实例和虚拟机消耗更多内存：**透明页面共享**（**TPS**）可以缓解这一点，因为虚拟机管理程序将在虚拟机之间找到公共页面并在可能的情况下共享它们。
- en: A standard Hadoop architecture
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准的Hadoop架构
- en: 'Let''s understand a standard Hadoop architecture:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一个标准的Hadoop架构：
- en: '**Hadoop File System** (**HDFS**): A distributed filesystem instantiated across
    a set of local disks attached to the compute nodes in the Hadoop cluster'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop文件系统**（**HDFS**）：一个分布式文件系统，在Hadoop集群中连接到计算节点的本地磁盘上实例化'
- en: '**Map**: The embarrassingly parallel computation that is applied to every chunk
    of data read from HDFS (in parallel)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地图**：将应用于从HDFS（并行）读取的每一块数据的令人尴尬的并行计算'
- en: '**Reduce**: The phase that takes map results and combines them to perform the
    final computation'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少**：将映射结果组合起来执行最终计算的阶段'
- en: 'The final results are typically stored back into HDFS. The benefits of Serengeti
    (open source project) provide ease of provisioning, multi-tenancy, and flexibility
    to scale up or out. BDE allows Serengeti to be triggered from a vRealize blueprint,
    making it easy to self-provision a Hadoop cluster of a given size:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果通常存储回 HDFS。Serengeti（开源项目）的好处提供了部署便利性、多租户和可扩展性，可以向上或向外扩展。BDE 允许从 vRealize
    蓝图触发 Serengeti，使得自提供指定大小的 Hadoop 集群变得容易：
- en: '![](img/44c841bb-5f66-47f8-890c-c43d41ed41fb.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/44c841bb-5f66-47f8-890c-c43d41ed41fb.png)'
- en: The preceding diagram shows a virtualized Hadoop environment. Local disks are
    made available as VMDKs to the guest, Map, and Reduce tasks running in VMs on
    each Hadoop cluster node.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了虚拟化的 Hadoop 环境。本地磁盘作为 VMDK 提供给虚拟机、Map 和 Reduce 任务，这些任务在每个 Hadoop 集群节点上运行。
- en: 'The next generation of our approach is one in which there are two types of
    VMs: Compute nodes and Data nodes. The Data node is responsible for managing the
    physical disks attached to the host and for HDFS. The Compute nodes run the Map
    and Reduce tasks. Communication between the Compute node and Data node happens
    through fast VM-VM communication.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的新一代是其中有两种类型的 VM：计算节点和数据节点。数据节点负责管理连接到主机的物理磁盘和 HDFS。计算节点运行 Map 和 Reduce
    任务。计算节点和数据节点之间的通信通过快速的 VM-VM 通信进行。
- en: Standard tests
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准测试
- en: 'These tests were run on a 32-node (host) cluster with local disks and 10 Gigabit
    Ethernet interconnect**. **The most important point is that four configurations
    were run, each solving the same problem:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些测试是在一个 32 节点（主机）集群上运行的，该集群具有本地磁盘和 10 Gbps 以太网互连**。**最重要的点是运行了四种配置，每种配置都解决了相同的问题：
- en: '**Configuration 1**: A 32-host physical cluster'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置 1**：一个 32 个主机的物理集群'
- en: '**Configuration 2**: A 32-VM virtual cluster'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置 2**：一个 32 个 VM 的虚拟集群'
- en: '**Configuration 3**: A 64-VM virtual cluster (two VMs per host, each using
    half the hardware)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置 3**：一个 64 个 VM 的虚拟集群（每个主机有两个 VM，每个 VM 使用一半的硬件）'
- en: '**Configuration 4**: A 128-VM virtual cluster (four VMs per host, with two
    on each socket, each with one quarter of the hardware resources)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置 4**：一个 128 个 VM 的虚拟集群（每个主机有四个 VM，每个插槽上有两个，每个 VM 拥有四分之一的硬件资源）'
- en: 'We used common benchmarks to evaluate various aspects of Hadoop performance.
    Here is the ratio of virtual run-time to native/physical/bare-metal run-time:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用常见的基准测试来评估 Hadoop 性能的各个方面。以下是虚拟运行时间与本地/物理/裸机运行时间的比率：
- en: '**TeraGen**: 6%, 4%, 3%'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TeraGen**：6%，4%，3%'
- en: '**TeraSort**: 13%, 1%, -1%'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TeraSort**：13%，1%，-1%'
- en: '**TeraValidate**: 5%, 5%, 7%'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TeraValidate**：5%，5%，7%'
- en: Generally, breaking the problem into smaller pieces and running more VMs results
    in performance closer to native.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将问题分解成更小的部分并运行更多的 VM 可以使性能更接近本地。
- en: There is a wide range of latency sensitivities found in MPI applications. The
    application on the **Particle Mesh Ewald Molecular Dynamics** (**PMEMD**) is a
    molecular dynamics code with more than 40% MPI data transfers involving messages
    that are a single byte long. Contrast this with **Lattice Boltzmann Magneto-Hydrodynamics** (**LBMHD**) code,
    all of whose messages are greater than 1 MB in size. The first application is
    critically sensitive to interconnect latencies, while the second is insensitive
    and is instead bandwidth-bound.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MPI 应用程序中发现了广泛的延迟敏感性。**粒子网格 Ewald 分子动力学**（**PMEMD**）的应用程序是一个分子动力学代码，其中超过 40%
    的 MPI 数据传输涉及单字节长度的消息。与此相比，**格子玻尔兹曼磁流体动力学**（**LBMHD**）代码的所有消息大小都大于 1 MB。第一个应用程序对互连延迟非常敏感，而第二个则不敏感，而是带宽限制。
- en: Each application has its own communication patterns about its process flow with
    each other. The process/processor number is shown on both the *x* and *y* axis.
    The darker the data point at *(x, y)*, the more data is transferred between the *x*
    and *y* processes. The PMEMD pattern shows that each process communicates across
    the range of available processes, but significantly more communication happens
    with nearby/neighbor processes. There are algorithms where the displayed pattern
    would be a dark expanse across the entire pattern, indicating intense communication
    between all processes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用程序都有其自己的与其他应用程序的进程流程的通信模式。进程/处理器编号显示在 *x* 和 *y* 轴上。在 *(x, y)* 处的数据点越暗，*x*
    和 *y* 进程之间的数据传输就越多。PMEMD 模式显示每个进程与可用进程范围内的所有进程进行通信，但与附近/相邻进程的通信更多。有些算法中，显示的模式将是整个模式上的深色区域，表示所有进程之间有强烈的通信。
- en: 'It is common in HPC bare-metal environments to bypass the kernel to get optimum
    bandwidth with lowest latency, as this is important for many MPI applications.
    Rather than using a standard TCP networking stack (which adds overhead), **remote
    direct memory access** (**RDMA**) devices, such as IB, allow transfers to be initiated
    directly to and from the application without using the host CPU to transfer the
    data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在HPC裸机环境中，绕过内核以获得最低延迟下的最佳带宽是很常见的，这对于许多MPI应用程序来说很重要。而不是使用标准的TCP网络堆栈（这会增加开销），**远程直接内存访问**（**RDMA**）设备，例如IB，允许直接从应用程序启动和终止传输，而不使用主机CPU来传输数据：
- en: '![](img/dbe48a47-e3fc-4819-9a4c-7a82fce445bf.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dbe48a47-e3fc-4819-9a4c-7a82fce445bf.png)'
- en: We can opt  for the analog way for our virtual environment. We can make the
    hardware device, such as IB, directly visible to the guest (rightmost **rdma** box
    in the preceding diagram) using the VM direct path I/O. It will allow the application
    to get direct access to the hardware, as in the bare-metal case, by using ESXi.
    The **rdma** box to the left of the VM direct path I/O case represents ongoing
    work within VMware to develop a vRDMA device, which is a paravirtualized RDMA
    device.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为我们的虚拟环境选择模拟方式。我们可以通过VM直接路径I/O使硬件设备，如IB，直接对虚拟机可见（前图中最右侧的**rdma**框），这将允许应用程序通过使用ESXi直接访问硬件，就像裸机案例一样。VM直接路径I/O左侧的**rdma**框代表了VMware内部开发vRDMA设备的工作，这是一个虚拟化RDMA设备。
- en: This device will continue to allow direct application hardware access for data
    transfers, while also continuing to support the ability to perform snapshots and
    to use vMotion. This represents ongoing work that will make the RDMA approach
    available for use by ESXi itself and by ESXi services such as vMotion, vSAN, and
    FT. We've shown very large performance gains for vMotion using an RDMA transport.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此设备将继续允许直接应用硬件访问进行数据传输，同时继续支持执行快照和使用vMotion的能力。这代表了一项持续的工作，将使RDMA方法可用于ESXi本身以及ESXi服务，如vMotion、vSAN和FT。我们已经展示了使用RDMA传输的vMotion在性能上的显著提升。
- en: Intel tested a variety of HPC benchmarks
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 英特尔测试了各种HPC基准测试
- en: '**High-performance computing cluster** (**HPCC**):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**高性能计算集群**（**HPCC**）：'
- en: 'HPC challenge benchmarks are combination of multiple benchmarks to test various
    attributes along with their performance of HPC systems. STAR-CD is a **computational
    fluid dynamics** (**CFD**) code used for in-cylinder analysis. These are **Message
    Passing Interface** (**MPI**) codes responsible for multi-process running on multiple
    nodes. The results are for a small cluster in several configurations: 2-node,
    16 processes; 4-node, 32 processes; 8-node, 64 processes (*node* means *host* in
    HPC circles). We show excellent parity with native, even for this old configuration.
    Intel should turn off EPT or use large page sizes for better MPIRandomAccess.
    We have not explored naturally-ordered ring bandwidth to understand the issue
    there.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: HPC挑战基准测试是多个基准测试的组合，用于测试各种属性以及HPC系统的性能。STAR-CD是一种用于缸内分析的**计算流体动力学**（**CFD**）代码。这些是负责在多个节点上多进程运行的**消息传递接口**（**MPI**）代码。结果是在几个配置的小集群中：2节点，16个进程；4节点，32个进程；8节点，64个进程（在HPC领域，“节点”意味着“主机”）。即使对于这个旧配置，我们也展示了与原生环境的优秀一致性。英特尔应该关闭EPT或使用大页面大小以获得更好的MPIRandomAccess。我们尚未探索自然排序的环形带宽以了解那里的问题。
- en: HPCC represents a set application, such as benchmarks over a range of HPC needs.
    It is very positive that we do so well, even if this is admittedly at very low
    scale. Before we look at the STAR-CD results, it is useful to look at the messaging
    characteristics of the application.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: HPCC代表一套应用，例如覆盖HPC需求范围的基准测试。我们做得如此之好是非常积极的，即使这确实是在非常低的规模上。在我们查看STAR-CD的结果之前，查看应用程序的消息特性是有用的。
- en: 'MPI applications perform two kinds of communication between processes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: MPI应用程序在进程之间执行两种类型的通信：
- en: '**P2P**: Individual processes send or receive data from another process'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P2P**：单个进程从另一个进程发送或接收数据'
- en: '**Collective operations**: Many processes participate together to transfer
    data in one of a set of patterns'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集体操作**：许多进程共同参与，以某种模式之一传输数据'
- en: This majority of P2P messages exchanged by STAR-CD are in the 1 KB - 8 KB range.
    We would characterize these as medium-sized messages. As the node count is increased,
    more of these messages move into the 128 B-1 KB range. This means that interconnect
    latency becomes more of a factor as the number of nodes increases. For collective
    operations, there is a very clear latency-sensitivity in Star-CD since virtually
    all messages fall into the 0 - 128 B range.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: STAR-CD交换的大多数P2P消息在1 KB - 8 KB范围内。我们将这些定义为中等大小的消息。随着节点数量的增加，更多的这些消息移动到128 B-1
    KB范围内。这意味着随着节点数量的增加，互连延迟成为一个更重要的因素。对于集体操作，Star-CD有一个非常明显的延迟敏感性，因为几乎所有消息都落在0 -
    128 B范围内。
- en: The *y* axes on these two charts are different. Star-CD uses P2P messages much
    more often than collectives. Thus, while the collective operations are very latency-sensitive,
    the overall effect of latency overheads may be reduced since the collectives don't
    represent the majority of messages transferred by Star-CD. The results show a
    15% slowdown running an A-class model (a Mercedes car body) with STAR-CD using
    8 nodes and 32 processes on 2-socket nodes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个图表的y轴是不同的。Star-CD比集体操作更频繁地使用P2P消息。因此，尽管集体操作非常延迟敏感，但由于集体操作并不代表STAR-CD传输消息的大多数，延迟开销的整体影响可能有所减少。结果显示，使用STAR-CD运行A类模型（梅赛德斯汽车车身）时，速度慢了15%，使用了8个节点和32个进程在双插槽节点上。
- en: This is a significant slowdown for many HPC users, which is why we continue
    to work on characterizing and reducing latency overheads in the platform. The
    next set of our results is using the VM direct path I/O with QDR (40 Gigabits
    per second) IB and ESXi.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这对许多HPC用户来说是一个显著的减速，这就是我们继续在平台上表征和减少延迟开销的原因。我们下一组的结果是使用VM直接路径I/O与QDR（每秒40吉比特）IB和ESXi。
- en: The bare-metal and VM over a wide range of message sizes use two different transfer
    mechanisms send and RDMA read respectively. We deliver equivalent bandwidth in
    the virtual case.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛的报文大小范围内，裸金属和虚拟化使用两种不同的传输机制：发送和RDMA读取。我们在虚拟情况下提供等效的带宽。
- en: The **hybrid run time** (HRT, runtimes which run as kernel) latencies over a
    wide range of message sizes for bare-metal and virtual environment are different
    and depend on many factors. These results are generated using RDMA read operations.
    Using **quad data rate** (**QDR**) (40 gigabits per second) IB and ESXi, virtualization
    introduces about 0.7 microseconds of additional latency, which is significant
    at the smallest message sizes and less so for larger messages.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 裸金属和虚拟环境在广泛的报文大小范围内**混合运行时间**（HRT，作为内核运行的运行时间）的延迟不同，并取决于许多因素。这些结果是通过RDMA读取操作生成的。使用**四倍数据速率**（**QDR**）（每秒40吉比特）IB和ESXi，虚拟化引入了大约0.7微秒的额外延迟，在最小报文大小上这是显著的，而对于较大的报文则不那么显著。
- en: When using send/receive operations on the same hardware, ESXi latency overheads
    drop to about 0.4 microseconds and this disparity disappears for message sizes
    larger than 256 bytes. Again, the impact of these overheads on performance will
    depend entirely on the messaging characteristics of specific applications of interest.
    GPUs for computation, called **general-purpose GPUs** (**GPGPU**), have promising
    results. Customers may be interested in Intel's offering in this area with their
    accelerator product, Intel Xeon Phi.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当在相同硬件上使用发送/接收操作时，ESXi的延迟开销降至大约0.4微秒，并且对于大于256字节的报文大小，这种差异消失了。再次强调，这些开销对性能的影响将完全取决于感兴趣的具体应用的消息特性。用于计算的GPU，称为**通用GPU**（**GPGPU**），有令人鼓舞的结果。客户可能对英特尔在这一领域的加速产品，英特尔至强Phi感兴趣。
- en: It is essential that the platform be tuned appropriately to support the required
    latency of these applications. We are being very transparent about what works
    well and what doesn't on our platform so that customers have an accurate understanding
    of the values and challenges of virtualizing HPC workloads and environments.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 平台必须适当调整以支持这些应用所需的延迟，这是至关重要的。我们在平台上什么工作得好，什么工作得不好方面非常透明，以便客户能够准确了解虚拟化HPC工作负载和环境的值和挑战。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the specific features of virtualization that enhance
    the outcome of an HPC environment. We focused on unique features above and beyond
    a generic virtualization platform and looked at how virtualization addresses an
    improved scientific productivity. The impact of virtualization on the runtime
    of a particular simulation or calculation may have different results, but the
    overall performance of a compute deployment has optimum throughput.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了虚拟化的特定功能，这些功能增强了HPC环境的成果。我们关注了超越通用虚拟化平台独特功能的特点，并探讨了虚拟化如何提高科学生产力。虚拟化对特定模拟或计算运行时间的影响可能不同，但计算部署的整体性能具有最佳吞吐量。
- en: This book will help you to align the operational goals of your customers with
    their business objectives. Readers will learn how IT organizations can achieve
    both operational and business goals by providing a secure and flexible digital
    foundation to their businesses by addressing the key business issues. They will
    learn how VMware products help customers to deliver consistent and stable IT performance
    to the business. VMware recommend leveraging hyper-converged, virtualized resources
    in a solution that works out of the box to speed the deployment of a fully-virtualized
    infrastructure.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将帮助您将客户的运营目标与他们的商业目标对齐。读者将学习到IT组织如何通过解决关键商业问题，为他们的企业提供安全且灵活的数字基础，从而实现运营和商业目标。他们还将学习到VMware产品如何帮助客户向企业提供一致且稳定的IT性能。VMware建议利用即插即用的超融合、虚拟化资源，以加速完全虚拟化基础设施的部署。
- en: Readers should now know how a fully-virtualized infrastructure allows them to
    accelerate deployments and unify and streamline operations, monitoring, and IT
    management, while also improving the ability to scale. A software-defined infrastructure
    enables us to unify and ease operations, monitoring, and IT management, while
    improving scalability. All the solutions mentioned in this book have taken a software-defined
    approach to building a private cloud, one that extends virtualization across the
    entire digital infrastructure (compute, storage, and networking) through common
    hardware, which is managed with common existing tools and skill sets.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 读者现在应该了解一个完全虚拟化的基础设施如何帮助他们加速部署，统一和简化操作、监控和IT管理，同时提高扩展能力。软件定义的基础设施使我们能够统一并简化操作、监控和IT管理，同时提高可扩展性。本书中提到的所有解决方案都采用了软件定义的方法来构建私有云，这种方法通过通用硬件将虚拟化扩展到整个数字基础设施（计算、存储和网络），这些硬件可以通过通用的现有工具和技能集进行管理。
