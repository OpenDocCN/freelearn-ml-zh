<html><head></head><body>
		<div id="_idContainer080">
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Chapter 4: Adding Feature Store to ML Models</h1>
			<p>In the last chapter, we discussed <strong class="bold">Feast</strong> installation in your local system, common terminology in Feast, what the project structure looks like, API usage with an example, and a brief overview of the Feast architecture. </p>
			<p>So far in the book, we have been talking about issues with feature management and how a feature store can benefit data scientists and data engineers. It is time for us to get our hands dirty with an ML model and add Feast to the ML pipeline.</p>
			<p>In this chapter, we will revisit the <strong class="bold">Customer Lifetime Value</strong> (<strong class="bold">LTV/CLTV</strong>) ML model built in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. We will use AWS cloud services instead of the local system to run the examples in this chapter. As mentioned in <a href="B18024_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Feature Store Fundamentals, Terminology, and Usage</em>, installation for AWS is different from that of a local system, so we will have to create a few resources. I will be using some Free Tier services and some that are featured services (free for the first 2 months of use with limits). Also, the terms and API usage examples we looked at in <a href="B18024_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Feature Store Fundamentals, Terminology, and Usage</em>, will be very useful as we try to include Feast in the ML pipeline. </p>
			<p>The goal of this chapter is to learn what it takes to include a feature store in a project and how it differs from the traditional ML model building that we did in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. We will learn about Feast installation, how to build a feature engineering pipeline for the LTV model, how to define feature definitions, and we will also look at feature ingestion in Feast. </p>
			<p>We will discuss the following topics in order:</p>
			<ul>
				<li>Creating Feast resources in AWS</li>
				<li>Feast initialization for AWS</li>
				<li>Exploring the ML life cycle with Feast</li>
			</ul>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Technical requirements</h1>
			<p>To follow the code examples in the chapter, all you need is familiarity with Python and any notebook environment, which could be a local setup such as Jupyter or an online notebook environment such as Google Collab, Kaggle, or SageMaker. You will also need an AWS account with full access to resources such as Redshift, S3, Glue, DynamoDB, the IAM console, and more. You can create a new account and use all the services for free during the trial period. You can find the code examples for the book at the following GitHub link:</p>
			<p>https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter04</p>
			<p>The following GitHub link points to the feature repository:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation</a></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Creating Feast resources in AWS </h1>
			<p>As discussed in<a id="_idIndexMarker183"/> the previous chapter, Feast aims to provide a quick setup for beginners to try it out; however, for<a id="_idIndexMarker184"/> team collaboration and to run a model in production, it requires a better setup. In this section, we will set up a Feast environment in the AWS cloud and use it in model development. In the previous chapter, we also discussed that Feast provides multiple choices when picking an online and offline store. For this exercise, Amazon S3 with Redshift will be used as an offline/historical store and DynamoDB will be used as an online store. So, we need a few resources on AWS before we can start using the feature store in our project. Let's create the resources one after another. </p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Amazon S3 for storing data</h2>
			<p>As <a id="_idIndexMarker185"/>mentioned in the AWS documentation, <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>) <em class="italic">is an object storage service offering industry-leading scalability, data availability, security, and performance</em>. Feast provides<a id="_idIndexMarker186"/> the capability to use S3 to store and retrieve all data and metadata. You could also use version control systems such as GitHub or GitLab to collaborate on the metadata and sync to S3 during deployment. To create <a id="_idIndexMarker187"/>an S3 bucket in AWS, log in to your AWS account, navigate to the S3 service using the search box, or visit <a href="https://s3.console.aws.amazon.com/s3/home?region=us-east-1">https://s3.console.aws.amazon.com/s3/home?region=us-east-1</a>. A web page will be displayed, <a id="_idIndexMarker188"/>as shown in <em class="italic">Figure 4.1</em>. </p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B18024_04_01.jpg" alt="Figure 4.1 – AWS S3 home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – AWS S3 home page</p>
			<p>If you already have the buckets, you will see them on the page. I am using a new account, hence I don't see any buckets yet. To create a new bucket, click on <strong class="bold">Create bucket</strong> in the top right. You will see a page similar to the one in <em class="italic">Figure 4.2</em>. Choose a bucket name, leave everything else as the default, and scroll all the way down to click on <strong class="bold">Create bucket</strong>. I am going to name it <strong class="source-inline">feast-demo-mar-2022</strong>. One thing to keep in mind is that S3 bucket names are unique across accounts. If bucket creation fails with an error, <strong class="bold">Bucket with the same name already exists</strong>, try adding a few random characters to the end. </p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B18024_04_02.jpg" alt="Figure 4.2 – S3 Create bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – S3 Create bucket</p>
			<p>After <a id="_idIndexMarker189"/>successful bucket creation, you will see a <a id="_idIndexMarker190"/>screen similar to <em class="italic">Figure 4.3</em>. </p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B18024_04_03.jpg" alt="Figure 4.3 – After S3 bucket creation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – After S3 bucket creation</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>AWS Redshift for an offline store</h2>
			<p>As <a id="_idIndexMarker191"/>mentioned in the AWS documentation, <em class="italic">Amazon Redshift uses SQL to analyze structured and semi-structured data across data warehouses, operational databases, and data lakes, using AWS-designed hardware and machine learning to deliver the best price performance at any scale</em>. As mentioned earlier, we will use a Redshift cluster for querying <a id="_idIndexMarker192"/>historical data. We need to create a cluster since we don't have one already. Before we create a <a id="_idIndexMarker193"/>cluster, let's create an <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) role. This is a role that Redshift will assume on our behalf to query the historical data in S3.</p>
			<p>Let's start by creating an IAM role:</p>
			<ol>
				<li>To create an IAM role, navigate to the AWS IAM console using the search or visit the URL https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles. A web page similar to the one in <em class="italic">Figure 4.4</em> will be displayed.</li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B18024_04_04.jpg" alt="Figure 4.4 – IAM home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – IAM home page</p>
			<ol>
				<li value="2">To create a new role, click on the <strong class="bold">Create role</strong> button in the top-right corner. The following page will be displayed.</li>
			</ol>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B18024_04_05.jpg" alt="Figure 4.5 – IAM Create role&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – IAM Create role</p>
			<ol>
				<li value="3">From<a id="_idIndexMarker194"/> the options available on <a id="_idIndexMarker195"/>the page, select <strong class="bold">Custom trust policy</strong>, copy the following code block, and replace the policy in the JSON in the textbox:<p class="source-code">{</p><p class="source-code">    "Version": "2012-10-17",</p><p class="source-code">    "Statement": [</p><p class="source-code">        {</p><p class="source-code">            "Effect": "Allow",</p><p class="source-code">            "Principal": {</p><p class="source-code">                "Service": "redshift.amazonaws.com"</p><p class="source-code">            },</p><p class="source-code">            "Action": "sts:AssumeRole"</p><p class="source-code">        }</p><p class="source-code">    ]</p><p class="source-code">}</p></li>
				<li>Scroll all the way to the bottom and click on <strong class="bold">Next</strong>. On the next page, you will see a list of IAM <a id="_idIndexMarker196"/>policies that <a id="_idIndexMarker197"/>can be attached to the role, as shown in <em class="italic">Figure 4.6</em>.</li>
			</ol>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B18024_04_06.jpg" alt="Figure 4.6 – IAM permissions for the role&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – IAM permissions for the role</p>
			<ol>
				<li value="5">We need <strong class="bold">S3</strong> access, since the data will be stored in S3 as Parquet files, and <strong class="bold">AWS Glue</strong> access. The <a id="_idIndexMarker198"/>data stored in S3 will be loaded as an external schema into Redshift using AWS Glue Data Catalog/Lake Formation. Follow along here and you will understand what it means to load data as an external schema. For S3 access, search <a id="_idIndexMarker199"/>for <strong class="bold">AmazonS3FullAccess</strong> and select the corresponding checkbox, then search for <strong class="bold">AWSGlueConsoleFullAccess</strong> and<a id="_idIndexMarker200"/> select the corresponding checkbox. Scroll all the way down and click on <strong class="bold">Next</strong>. <p class="callout-heading">Important Note </p><p class="callout">We are providing full access to S3 and Glue on all the resources here, but it is recommended to restrict access to specific resources. I will leave that as an exercise since it is out of scope for this chapter. </p></li>
			</ol>
			<p>The following <a id="_idIndexMarker201"/>page will be displayed after you click on <strong class="bold">Next</strong>.</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18024_04_07.jpg" alt="Figure 4.7 – IAM review&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 4.7 – IAM review</p>
			<ol>
				<li value="6">On this<a id="_idIndexMarker202"/> page, provide a name for the role. I have named the role <strong class="source-inline">feast-demo-mar-2022-spectrum-role</strong>. Review the details of the role and click on <strong class="bold">Create role</strong>. On successful creation, you will find the role on the IAM console page. </li>
				<li>Now that we have the IAM role ready, the next step is to create a <strong class="bold">Redshift</strong> cluster and assign the created IAM role to it. To create the Redshift cluster, navigate to the Redshift home page using the search bar or visit the link https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#clusters. The following page will be displayed. </li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18024_04_08.jpg" alt="Figure 4.8 – Redshift home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Redshift home page</p>
			<ol>
				<li value="8">On the <a id="_idIndexMarker203"/>page in <em class="italic">Figure 4.8</em>, click <a id="_idIndexMarker204"/>on <strong class="bold">Create cluster</strong>. The following page will be displayed.</li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18024_04_09.jpg" alt="Figure 4.9 – Create a Redshift cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Create a Redshift cluster</p>
			<ol>
				<li value="9">From the web page displayed in <em class="italic">Figure 4.9</em>, I am picking <strong class="bold">Free trial</strong> for the demo, but this can be configured based on the dataset size and load. After picking <strong class="bold">Free trial</strong>, scroll <a id="_idIndexMarker205"/>all the way down and pick a password. The following figure shows<a id="_idIndexMarker206"/> the lower half of the window when you scroll down.</li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B18024_04_10.jpg" alt="Figure 4.10 – Create cluster lower half&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Create cluster lower half</p>
			<ol>
				<li value="10">Once you've picked a password, click on <strong class="bold">Create cluster</strong> at the bottom. Cluster creation takes a few minutes. Once the cluster creation is complete, you should see the newly created cluster in the AWS Redshift console. One last thing that is pending is associating the IAM role that we created earlier with the Redshift cluster. Let's do that now. Navigate to the newly created cluster. You will see a web page similar to the one in <em class="italic">Figure 4.11</em>.</li>
			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B18024_04_11.jpg" alt="Figure 4.11 – Redshift cluster page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Redshift cluster page</p>
			<ol>
				<li value="11">On the <a id="_idIndexMarker207"/>cluster home page, select the <strong class="bold">Properties</strong> tab and scroll down to <strong class="bold">Associated IAM roles</strong>. You will <a id="_idIndexMarker208"/>see the options displayed in <em class="italic">Figure 4.12</em>.</li>
			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B18024_04_12.jpg" alt="Figure 4.12 – Redshift Properties tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – Redshift Properties tab</p>
			<ol>
				<li value="12">From the web page, click on the <strong class="bold">Associate IAM role</strong> button. The IAM role that was created earlier will be displayed as shown in <em class="italic">Figure 4.13</em>. From the options, pick the<a id="_idIndexMarker209"/> IAM role you created earlier and click on the <strong class="bold">Associate IAM roles</strong> button. Please note, I named the IAM role <strong class="source-inline">feast-demo-mar-2022-spectrum-role</strong>, hence I am associating that role. Once you click on the button, the cluster will be updated with the new role. It may take a few minutes again. Once<a id="_idIndexMarker210"/> the cluster is ready, we are done with the required infrastructure for now. We will add the external data catalog when the features are ready to be ingested. </li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B18024_04_13.jpg" alt="Figure 4.13 – Redshift Associate IAM roles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – Redshift Associate IAM roles</p>
			<p>We need an IAM user to access these resources and perform operations on them. Let's create that next.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Creating an IAM user to access the resources</h2>
			<p>There are<a id="_idIndexMarker211"/> different ways to provide access to the users for the resources. If you are part of the organization, then the IAM roles can be integrated with Auth0 and active directories. Since that is out of scope here, I will be creating an IAM user and will give the required permissions for the user to access the resources created earlier: </p>
			<ol>
				<li value="1">Let's create the IAM user from the AWS console. The IAM console can be accessed using the search or visiting https://console.aws.amazon.com/iamv2/home#/users. The IAM console looks as shown in <em class="italic">Figure 4.14</em>.</li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18024_04_14.jpg" alt="Figure 4.14 – IAM user page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – IAM user page</p>
			<ol>
				<li value="2">On the IAM <a id="_idIndexMarker212"/>user page, click on the <strong class="bold">Add users</strong> button in the top right. The following web page will be displayed. </li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18024_04_15.jpg" alt="Figure 4.15 – IAM Add user&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – IAM Add user</p>
			<ol>
				<li value="3">On the web <a id="_idIndexMarker213"/>page, provide a user name and select <strong class="bold">Access key - Programmatic access</strong>, then click on <strong class="bold">Next: Permissions</strong> at the bottom. The following web page will be displayed. </li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18024_04_16.jpg" alt="Figure 4.16 – IAM permissions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – IAM permissions</p>
			<ol>
				<li value="4">On the displayed web page, click on <strong class="bold">Attach existing policies directly</strong> and from the list <a id="_idIndexMarker214"/>of available policies, search for and attach <a id="_idIndexMarker215"/>the following <a id="_idIndexMarker216"/>policies: <strong class="bold">AmazonRedshiftFullAccess</strong>, <strong class="bold">AmazonS3FullAccess</strong>, and <strong class="bold">AmazonDynamoDBFullAccess</strong>.<p class="callout-heading">Important Note </p><p class="callout">We are attaching full access here without restricting the user to specific resources. It is always a good practice to restrict access based on the resources and only provide required permissions. </p></li>
				<li>Click <strong class="bold">Next: Tags</strong> and <a id="_idIndexMarker217"/>feel free to add tags and again click on <strong class="bold">Next: Review</strong>. The review page looks like the following:</li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18024_04_17.jpg" alt="Figure 4.17 – IAM user review&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17 – IAM user review</p>
			<ol>
				<li value="6">From the review page, click on the <strong class="bold">Create user</strong> button. The web page in <em class="italic">Figure 4.18</em> will be displayed.</li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B18024_04_18.jpg" alt="Figure 4.18 – IAM user credentials&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.18 – IAM user credentials</p>
			<ol>
				<li value="7">On the <a id="_idIndexMarker218"/>web page, click on the <strong class="bold">Download.csv</strong> button and save the file in a secure<a id="_idIndexMarker219"/> location. It contains the <strong class="bold">Access key ID</strong> and <strong class="bold">Secret access key</strong> for the user we just created. The <a id="_idIndexMarker220"/>secret will be lost if you don't download and save it from this page. However, you can go into the user from the IAM user page and manage the secret (delete the existing credentials and create new credentials). </li>
			</ol>
			<p>Now that the infrastructure is ready, let's initialize the Feast project. </p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Feast initialization for AWS</h1>
			<p>We have <a id="_idIndexMarker221"/>the infrastructure required for running Feast now. However, we need to initialize a Feast project before we can start using it. To <a id="_idIndexMarker222"/>initialize a Feast project, we need to install the Feast library as we did in <a href="B18024_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Feature Store Fundamentals, Terminology, and Usage</em>. However, this time, we also need to install the AWS dependencies. Here is the link to the notebook: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb ">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_Feast_aws_initialization.ipynb.</a></p>
			<p>The following command installs Feast with the required AWS dependencies:</p>
			<p class="source-code">!pip install feast[aws]</p>
			<p>Once the <a id="_idIndexMarker223"/>dependencies are installed, we need to initialize the Feast project. Unlike the initialization we did in the last chapter, here, Feast initialization <a id="_idIndexMarker224"/>needs additional inputs such as Redshift ARN, database name, S3 path, and so on. Let's look at how the initialization<a id="_idIndexMarker225"/> differs here. Before we initialize the project, we need the following details:</p>
			<ul>
				<li><strong class="bold">AWS Region</strong>: The<a id="_idIndexMarker226"/> Region where your infrastructure is running. I have created all the resources in <strong class="bold">us-east-1</strong>. If you have created them in a different Region, use that.</li>
				<li><strong class="bold">Redshift Cluster ID</strong>: The <a id="_idIndexMarker227"/>cluster identifier of the Redshift cluster that was created earlier. It can be found on the home page.</li>
				<li><strong class="bold">Redshift Database Name</strong>: The <a id="_idIndexMarker228"/>database name in Redshift. I am calling the database <strong class="source-inline">dev</strong>.</li>
				<li><strong class="bold">Redshift User Name</strong>: The<a id="_idIndexMarker229"/> default user is <strong class="source-inline">awsuser</strong>. If you gave a different user name during cluster creation, use that here.</li>
				<li><strong class="bold">Redshift S3 Staging Location</strong>: The<a id="_idIndexMarker230"/> location that can be used for staging temporary files. I will be using the same S3 bucket that I created earlier with a different prefix: <strong class="source-inline">s3://feast-demo-mar-2022/staging</strong>. Also create the staging folder in the bucket. </li>
				<li><strong class="bold">Redshift IAM Role for S3</strong>: The <a id="_idIndexMarker231"/>ARN of the IAM role that we created earlier. It can be found on the IAM role details page. It will be in the following format: <strong class="source-inline">arn:aws:iam::&lt;account_number&gt;:role/feast-demo-mar-2022-spectrum-role</strong>.</li>
			</ul>
			<p>Once you have the values for the mentioned parameters, the new project can be initialized in two ways. One is using the following command:</p>
			<p class="source-code"> feast init -t aws customer_segmentation</p>
			<p>The preceding command initializes the Feast project. During initialization, the command will ask you for<a id="_idIndexMarker232"/> the mentioned arguments.</p>
			<p>The second way is to edit the <strong class="source-inline">feature_store.yaml</strong> file:</p>
			<pre class="source-code">project: customer_segmentation</pre>
			<pre class="source-code">registry: data/registry.db</pre>
			<pre class="source-code">provider: aws</pre>
			<pre class="source-code">online_store:</pre>
			<pre class="source-code">  type: dynamodb</pre>
			<pre class="source-code">  region: <strong class="bold">us-east-1</strong></pre>
			<pre class="source-code">offline_store:</pre>
			<pre class="source-code">  type: redshift</pre>
			<pre class="source-code">  cluster_id: <strong class="bold">feast-demo-mar-2022</strong></pre>
			<pre class="source-code">  region: <strong class="bold">us-east-1</strong></pre>
			<pre class="source-code">  database: dev</pre>
			<pre class="source-code">  user: <strong class="bold">awsuser</strong></pre>
			<pre class="source-code">  s3_staging_location: <strong class="bold">s3://feast-demo-mar-2022/staging</strong></pre>
			<pre class="source-code">  iam_role: <strong class="bold">arn:aws:iam::&lt;account_number&gt;:role/feast-demo-mar-2022-spectrum-role</strong></pre>
			<p>Whichever<a id="_idIndexMarker233"/> method you choose for initializing the project, make sure that you provide the appropriate values for the parameters. I have highlighted the parameter that may need to be replaced for the Feast functionalities to work without issues. If you are using the first method, the <strong class="source-inline">init</strong> command will give the option to choose whether to load example data or not. Choose <strong class="source-inline">no</strong> to upload the example data.</p>
			<p>Now that we have our feature repository initialized for the project, let's apply our initial feature set, which is basically empty. The following code block removes the unwanted files that get created if you use <strong class="source-inline">feast init</strong> for the initialization of the project:</p>
			<p class="source-code">%cd customer_segmentation</p>
			<p class="source-code">!rm -rf driver_repo.py test.py</p>
			<p>If you don't run the preceding commands, it will create the feature definitions for the entity and feature views in the <strong class="source-inline">driver_repo.py</strong> file.</p>
			<p>The following code block creates feature and entity definitions defined in the project. In this project, there are none so far: </p>
			<p class="source-code">!feast apply</p>
			<p>When the <a id="_idIndexMarker234"/>preceding command is run, it displays the message <strong class="bold">No changes to registry</strong>, which is correct since we don't have any feature definitions yet.</p>
			<p>The folder structure of <strong class="source-inline">customer_segmentation</strong> should look like <em class="italic">Figure 4.19</em>.</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18024_04_19.jpg" alt="Figure 4.19 – Project folder structure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.19 – Project folder structure</p>
			<p>The <a id="_idIndexMarker235"/>feature repository is ready for use now. This can be checked into <em class="italic">GitHub</em> or <em class="italic">GitLab</em> for versioning and collaboration.</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">Also note that all the preceding steps can be automated using infrastructure as code frameworks such as Terraform, the AWS CDK, Cloud Formation, or others. Depending on the team structure followed in the organization, it will be the responsibility of the data engineer or platform/infrastructure team to create the required resources and share the repository details that can be used by data scientists or engineers.</p>
			<p>In the next section, let's look at how the ML life cycle changes with the feature store.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Exploring the ML life cycle with Feast</h1>
			<p>In this<a id="_idIndexMarker236"/> section, let's discuss what ML model development looks like when you are using a feature store. We went through the ML life cycle in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. This makes it easy to understand how it changes with a feature store and enables us to skip through a few steps that will be redundant. </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B18024_04_20.jpg" alt="Figure 4.20 – ML life cycle &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.20 – ML life cycle </p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>Problem statement (plan and create)</h2>
			<p>The problem statement <a id="_idIndexMarker237"/>remains the same as it was in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. Let's assume that you own a retail business and would like to improve the customer experience. First and foremost, you want to find your customer segments and customer lifetime value.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>Data (preparation and cleaning)</h2>
			<p>Unlike in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, before exploring the data and figuring out<a id="_idIndexMarker238"/> the access and more, here the starting point for model building is the feature store. Here is the link to the notebook:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb ">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb</a></p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_browse_feast_for_features.ipynb ">Let's start with the Feature Store:</a></p>
			<ol>
				<li value="1">So, let's open up a notebook and install Feast with AWS dependencies:<p class="source-code">!pip install feast[aws]</p></li>
				<li>If the feature repository created in the last section was pushed into source control such as GitHub or GitLab, let's clone the repository. The following code clones the repository:  <p class="source-code">!git clone &lt;repo_url&gt;</p></li>
				<li>Now that we have the feature repository, let's connect to Feast/the feature store and check what's available: <p class="source-code"># change directory</p><p class="source-code">%cd customer_segmentation</p><p class="source-code">"""import feast and load feature store object with the path to the directory which contains <strong class="bold">feature_story.yaml</strong>."""</p><p class="source-code">from feast import FeatureStore</p><p class="source-code">store = FeatureStore(repo_path=".")</p></li>
			</ol>
			<p>The preceding code block connects to the Feast feature repository. The <strong class="source-inline">repo_path="."</strong> parameter indicates that <strong class="source-inline">feature_store.yaml</strong> is in the current working directory.</p>
			<ol>
				<li value="4">Let's check whether the feature store contains any <strong class="bold">entities</strong> or <strong class="bold">feature views</strong> that can be used in the model instead of exploring the data and regenerating the features that already exist: <p class="source-code">#Get list of entities and feature views</p><p class="source-code">print(f"List of entities: {store.list_entities()}")</p><p class="source-code">print(f"List of FeatureViews: {store.list_feature_views()}")</p></li>
			</ol>
			<p>The preceding<a id="_idIndexMarker239"/> code block lists the <strong class="bold">entities</strong> and <strong class="bold">feature views</strong> that exist in the current feature repository we are connected to. The code block outputs two empty lists as follows:</p>
			<p class="source-code">List of entities: []</p>
			<p class="source-code">List of FeatureViews: []</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You may be wondering <em class="italic">What about the features created by other teams? How can I get access to them and check what's available?</em> There are ways to manage that. We will get to that a little later. </p>
			<p>Since the entities and feature views are empty, there is nothing that can be used. The next step is to perform data exploration and feature engineering. </p>
			<p>We will be skipping over the data exploration stage as we have already done it in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. Also, the steps for generating the features would be the same. Hence I will not be expanding on feature engineering. Instead, I will use the same code and briefly mention what the code does. Refer to <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, for a detailed description of how features are generated. </p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor075"/>Model (feature engineering)</h2>
			<p>In this <a id="_idIndexMarker240"/>section, we will generate the features required for the model. Just the way we did in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, we will <a id="_idIndexMarker241"/>use 3 months of data to generate RFM features and 6 months of data to generate the labels for the dataset. We will go through the steps in the same order as we did in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. Here is the link to the feature engineering notebook:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb</a>.</p>
			<p>Let's start with feature engineering:</p>
			<ol>
				<li value="1">The following code block reads the dataset and filters out the data that doesn't belong to <strong class="source-inline">United Kingdom</strong>:<p class="source-code">%pip install feast[aws]==0.19.3 s3fs</p><p class="source-code">import pandas as pd</p><p class="source-code">from datetime import datetime, timedelta, date</p><p class="source-code">from sklearn.cluster import Kmeans</p><p class="source-code">##Read the data and filter out data that belongs to country other than UK</p><p class="source-code">retail_data = pd.read_csv('/content/OnlineRetail.csv', encoding= 'unicode_escape')</p><p class="source-code">retail_data['InvoiceDate'] = pd.to_datetime(retail_data['InvoiceDate'], errors = 'coerce')</p><p class="source-code">uk_data = retail_data.query("Country=='United Kingdom'").reset_index(drop=True)</p></li>
				<li>Once we have the filtered data, the next step is to create two DataFrames, one for 3 months and one for 6 months.</li>
			</ol>
			<p>The following code block creates two different DataFrames, one for the data between <strong class="source-inline">2011-03-01 00:00:00.054000</strong> and <strong class="source-inline">2011-06-01 00:00:00.054000</strong>, the second one for the data between <strong class="source-inline">2011-06-01 00:00:00.054000</strong> and <strong class="source-inline">2011-12-01 00:00:00.054000</strong>:</p>
			<p class="source-code">## Create 3months and 6 months DataFrames</p>
			<p class="source-code">t1 = pd.Timestamp("2011-06-01 00:00:00.054000")</p>
			<p class="source-code">t2 = pd.Timestamp("2011-03-01 00:00:00.054000")</p>
			<p class="source-code">t3 = pd.Timestamp("2011-12-01 00:00:00.054000")</p>
			<p class="source-code">uk_data_3m = uk_data[(uk_data.InvoiceDate &lt; t1) &amp; (uk_data.InvoiceDate &gt;= t2)].reset_index(drop=True)</p>
			<p class="source-code">uk_data_6m = uk_data[(uk_data.InvoiceDate &gt;= t1) &amp; (uk_data.InvoiceDate &lt; t3)].reset_index(drop=True)</p>
			<ol>
				<li value="3">The next <a id="_idIndexMarker242"/>step is to generate RFM features<a id="_idIndexMarker243"/> from the 3 months DataFrame. The following code block generates RFM values for all customers:<p class="source-code">## Calculate RFM values.</p><p class="source-code">Uk_data_3m['revenue'] = uk_data_3m['UnitPrice'] * uk_data_3m['Quantity']</p><p class="source-code">max_date = uk_data_3m['InvoiceDate'].max() + timedelta(days=1)</p><p class="source-code">rfm_data = uk_data_3m.groupby(['CustomerID']).agg({</p><p class="source-code">  'InvoiceDate': lambda x: (max_date – x.max()).days,</p><p class="source-code">  'InvoiceNo': 'count',</p><p class="source-code">  'revenue': 'sum'})</p><p class="source-code">rfm_data.rename(columns={'InvoiceDate': 'Recency',</p><p class="source-code">                         'InvoiceNo': 'Frequency',</p><p class="source-code">                         'revenue': 'MonetaryValue'},</p><p class="source-code">                inplace=True)</p></li>
			</ol>
			<p>Now that we have generated RFM values for all the customers, the next step is to generate an R group, an F group, and an M group for each of the customers ranging from 0 to 3. Once we have the RFM groups for the customers, they will be used to <a id="_idIndexMarker244"/>calculate the RFM score by <a id="_idIndexMarker245"/>summing the individual group values for the customer. </p>
			<ol>
				<li value="4">The following code block generates RFM groups for the customers and calculates the RFM score:<p class="source-code">## Calculate RFM groups of customers </p><p class="source-code">r_grp = pd.qcut(rfm_data['Recency'],</p><p class="source-code">                q=4, labels=range(3,-1,-1))</p><p class="source-code">f_grp = pd.qcut(rfm_data['Frequency'],</p><p class="source-code">                q=4, labels=range(0,4))</p><p class="source-code">m_grp = pd.qcut(rfm_data['MonetaryValue'], </p><p class="source-code">                q=4, labels=range(0,4))</p><p class="source-code">rfm_data = rfm_data.assign(R=r_grp.values).assign(F=f_grp.values).assign(M=m_grp.values)</p><p class="source-code">rfm_data['R'] = rfm_data['R'].astype(int)</p><p class="source-code">rfm_data['F'] = rfm_data['F'].astype(int)</p><p class="source-code">rfm_data['M'] = rfm_data['M'].astype(int)</p><p class="source-code">rfm_data['RFMScore'] = rfm_data['R'] + rfm_data['F'] + rfm_data['M']</p></li>
				<li>With the RFM score calculated, it is time to group customers into low-, mid-, and high-value customers.</li>
			</ol>
			<p>The following code block groups customers into these groups:</p>
			<p class="source-code"># segment customers.</p>
			<p class="source-code">Rfm_data['Segment'] = 'Low-Value'</p>
			<p class="source-code">rfm_data.loc[rfm_data['RFMScore']&gt;4,'Segment'] = 'Mid-Value' </p>
			<p class="source-code">rfm_data.loc[rfm_data['RFMScore']&gt;6,'Segment'] = 'High-Value' </p>
			<p class="source-code">rfm_data = rfm_data.reset_index()</p>
			<ol>
				<li value="6">Now we <a id="_idIndexMarker246"/>have the RFM features ready. Let's keep those aside and calculate the revenue using the 6-month DataFrame that <a id="_idIndexMarker247"/>was created in an earlier step.</li>
			</ol>
			<p>The following code block calculates the revenue from every customer in the 6-months dataset:</p>
			<p class="source-code"># Calculate revenue using the six month dataframe.</p>
			<p class="source-code">Uk_data_6m['revenue'] = uk_data_6m['UnitPrice'] * uk_data_6m['Quantity']</p>
			<p class="source-code">revenue_6m = uk_data_6m.groupby(['CustomerID']).agg({</p>
			<p class="source-code">        'revenue': 'sum'})</p>
			<p class="source-code">revenue_6m.rename(columns={'revenue': 'Revenue_6m'}, </p>
			<p class="source-code">                  inplace=True)</p>
			<p class="source-code">revenue_6m = revenue_6m.reset_index()</p>
			<ol>
				<li value="7">The next step is to merge the 6-months dataset with revenue into the RFM features DataFrame. The following code block merges both the DataFrames in the <strong class="source-inline">CustomerId</strong> column:<p class="source-code"># Merge the 6m revenue DataFrame with RFM data.</p><p class="source-code">Merged_data = pd.merge(rfm_data, revenue_6m, how="left")</p><p class="source-code">merged_data.fillna(0)</p></li>
				<li>Since we are treating the problem as a classification problem, let's generate the customer LTV labels to <a id="_idIndexMarker248"/>use the <strong class="bold">k-means</strong> clustering algorithm. Here, we will be using the 6-months revenue to generate the labels. Customers will be <a id="_idIndexMarker249"/>grouped<a id="_idIndexMarker250"/> into three<a id="_idIndexMarker251"/> groups, namely <strong class="bold">LowLTV</strong>, <strong class="bold">MidLTV</strong>, and <strong class="bold">HighLTV</strong>.</li>
			</ol>
			<p>The<a id="_idIndexMarker252"/> following code block generates the LTV groups for the customers: </p>
			<p class="source-code"># Create LTV cluster groups</p>
			<p class="source-code">merged_data = merged_data[merged_data['Revenue_6m']&lt;merged_data['Revenue_6m'].quantile(0.99)]</p>
			<p class="source-code">kmeans = Kmeans(n_clusters=3)</p>
			<p class="source-code">kmeans.fit(merged_data[['Revenue_6m']])</p>
			<p class="source-code">merged_data['LTVCluster'] = kmeans.predict(merged_data[['Revenue_6m']])</p>
			<ol>
				<li value="9">Now we have <a id="_idIndexMarker253"/>the final dataset, let's look at what the feature set that we have generated looks like. The following code block converts categorical values into integer values: <p class="source-code">Feature_data = pd.get_dummies(merged_data)</p><p class="source-code">feature_data['CustomerID'] = feature_data['CustomerID'].astype(str)</p><p class="source-code">feature_data.columns = ['customerid', 'recency', 'frequency', 'monetaryvalue', 'r', 'f', 'm', 'rfmscore', 'revenue6m', 'ltvcluster', 'segmenthighvalue', 'segmentlowvalue', 'segmentmidvalue']</p><p class="source-code">feature_data.head(5)</p></li>
			</ol>
			<p>The preceding code block produces the following feature set:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18024_04_21.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.21 – Final feature set for the model</p>
			<p>In <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, the next step that was performed was model training and scoring. This is where we'll diverge from that. I am assuming this will be our <a id="_idIndexMarker254"/>final feature set. However, during the model development, the <a id="_idIndexMarker255"/>feature set evolves over time. We will discuss how to handle these changes in later chapters. </p>
			<p>Now that we have a feature set, the next thing is to create entities and feature views in Feast.</p>
			<h3>Creating entities and feature views</h3>
			<p>In the <a id="_idIndexMarker256"/>previous chapter, <a href="B18024_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a>, <em class="italic">Feature Store Fundamentals, Terminology, and Usage</em>, we defined <strong class="bold">entities</strong> and <strong class="bold">feature views</strong>. An entity is defined as a collection of semantically related features. Entities are <a id="_idIndexMarker257"/>domain objects to which features can be mapped. A feature view is defined as feature view is like a database table. It represents the structure of the feature data at its source. A feature view consists of entities, one or more features, and a data source. A feature view is generally modeled around a domain object similar to database objects. Since creating and applying a feature definition is a one-time activity, it is better to keep it in a separate notebook or Python file. Here is a link to the notebook:</p>
			<p><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb ">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb</a></p>
			<p>Let's open a notebook, install the libraries, and clone the feature repository as mentioned before:</p>
			<pre class="source-code">!pip install feast[aws]==0.19.3</pre>
			<pre class="source-code">!git clone &lt;feature_repo&gt;</pre>
			<p>Now that we have cloned the feature repository, let's create the entity and feature views. Going by the definition of entity and feature views, the job is to identify the entities, features, and feature views in the feature set in <em class="italic">Figure 4.21</em>. Let's start with the entities. The only domain object that can be found in <em class="italic">Figure 4.21</em> is <strong class="source-inline">customerid</strong>: </p>
			<ol>
				<li value="1">Let's start by defining the customer entity. The following code block defines the customer entity for Feast:<p class="source-code"># Customer ID entity definition.</p><p class="source-code">from feast import Entity, ValueType</p><p class="source-code">customer = Entity(</p><p class="source-code">    name='customer',</p><p class="source-code">    value_type=ValueType.STRING,</p><p class="source-code">    join_key='customeriD',</p><p class="source-code">    description="Id of the customer"</p><p class="source-code">)</p></li>
			</ol>
			<p>The <a id="_idIndexMarker258"/>preceding entity definition <a id="_idIndexMarker259"/>has a few required attributes, such as <strong class="source-inline">name</strong>, <strong class="source-inline">value_type</strong>, and <strong class="source-inline">join_key</strong>, and others are optional. There are additional attributes that can be added if the users want to provide more information. The most important attribute is <strong class="source-inline">join_key</strong>. The value of this attribute should match the column name in the feature DataFrame. </p>
			<p>We have figured out the entity in the feature set. The next job is to define the feature views. Before we define feature views, a thing to keep in mind is to define the feature views as if you are a consumer who didn't generate the feature set. What I mean by that is do not name the feature views <strong class="source-inline">customer_segmentation_features</strong> or <strong class="source-inline">LTV_features</strong> and push all of them to a single table. Always try to break them into logical groups that are meaningful when other data scientists browse through them. </p>
			<ol>
				<li value="2">With that in mind, let's look at the feature set and decide how many logical groups can be formed here and what features go into what groups. From <em class="italic">Figure 4.21</em>, it can be grouped into either one or two groups. The two groups I see are RFM features for the customers and revenue features. Since RFM also has revenue details, I <a id="_idIndexMarker260"/>would rather group them into one group instead of two as there are no <a id="_idIndexMarker261"/>clear subgroups here. I will call it <strong class="source-inline">customer_rfm_features</strong>.</li>
			</ol>
			<p>The following code block defines the feature view:</p>
			<p class="source-code">from feast import ValueType, FeatureView, Feature, RedshiftSource</p>
			<p class="source-code">from datetime import timedelta </p>
			<p class="source-code"># Redshift batch source</p>
			<p class="source-code">rfm_features_source = RedshiftSource(</p>
			<p class="source-code">    query="SELECT * FROM spectrum.customer_rfm_features",</p>
			<p class="source-code">    event_timestamp_column="event_timestamp",</p>
			<p class="source-code">    created_timestamp_column="created_timestamp",</p>
			<p class="source-code">)</p>
			<p class="source-code"># FeatureView definition for RFM features.</p>
			<p class="source-code">rfm_features_features = FeatureView(</p>
			<p class="source-code">    name="customer_rfm_features",</p>
			<p class="source-code">    entities=["customer"],</p>
			<p class="source-code">    ttl=timedelta(days=3650),</p>
			<p class="source-code">    features=[</p>
			<p class="source-code">        Feature(name="recency", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="frequency", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="monetaryvalue", </p>
			<p class="source-code">        dtype=ValueType.DOUBLE),</p>
			<p class="source-code">        Feature(name="r", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="f", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="m", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="rfmscore", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="revenue6m", dtype=ValueType.DOUBLE),</p>
			<p class="source-code">        Feature(name="ltvcluster", dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="segmenthighvalue", </p>
			<p class="source-code">        dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="segmentlowvalue", </p>
			<p class="source-code">        dtype=ValueType.INT32),</p>
			<p class="source-code">        Feature(name="segmentmidvalue", </p>
			<p class="source-code">        dtype=ValueType.INT32),</p>
			<p class="source-code">    ],</p>
			<p class="source-code">    batch_source=rfm_features_source,</p>
			<p class="source-code">)</p>
			<p>The <a id="_idIndexMarker262"/>preceding code block has two definitions. The<a id="_idIndexMarker263"/> first one is the batch source definition. Depending on the offline store that is being used, the definition of the batch source differs. In the previous chapter, we used <strong class="source-inline">FileSource</strong> in the example. Since we are using Redshift to query the offline store, <strong class="source-inline">RedshiftSource</strong> has been defined. The input to the object is query, which is a simple <strong class="source-inline">SELECT</strong> statement. The source can be configured to have complex SQL queries with joins, aggregation, and more. However, the output should match the column names defined in <strong class="source-inline">FeatureView</strong>. The other input to the source is <strong class="source-inline">created_timestamp_column</strong> and <strong class="source-inline">event_timestamp_column</strong>. These columns are missing in <em class="italic">Figure 4.21</em>. The columns represent what their headings state, the time when the event occurred, and when the event was created. These columns need to be added to the data before we ingest it. </p>
			<p><strong class="source-inline">FeatureView</strong> represents the table structure of the data at the source. As we looked at it in the last chapter, it has <strong class="source-inline">entities</strong>, <strong class="source-inline">features</strong>, and the <strong class="source-inline">batch_source</strong>. In <em class="italic">Figure 4.21</em>, the entity is <strong class="source-inline">customer</strong>, that was defined earlier. The rest of the columns are the features and the batch source, which is the <strong class="source-inline">RedshiftSource</strong> object. The feature name should match the column name and <strong class="source-inline">dtype</strong> should match the value type of the columns. </p>
			<ol>
				<li value="3">Now that <a id="_idIndexMarker264"/>we <a id="_idIndexMarker265"/>have the feature definition for our feature set, we must register the new definitions to be able to use them. To register the definitions, let's copy the entity and feature definitions into a Python file and add this file to our feature repository folder. I will be naming the file <strong class="source-inline">rfm_features.py</strong>. After adding the file to the repository, the folder structure looks like the following figure.</li>
			</ol>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18024_04_22.jpg" alt="Figure 4.22 – Project with feature definitions file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.22 – Project with feature definitions file</p>
			<p>Before registering<a id="_idIndexMarker266"/> the <a id="_idIndexMarker267"/>definition using the <strong class="source-inline">apply</strong> command, let's map the external schema on Redshift.</p>
			<h3>Creating an external catalog</h3>
			<p>If you<a id="_idIndexMarker268"/> recall correctly, during the Redshift resource creation, I mentioned that the data in Amazon S3 will be added as an external mapping using Glue/Lake Formation. What that means is data will not be ingested into Redshift directly; instead, the dataset will be in S3. The structure of the dataset will be defined in the Lake Formation catalog, which you will see in a moment. Then, the database will be mapped as an external schema on Redshift. Hence, the ingestion will push data into S3 directly and the query will be executed using the Redshift cluster. </p>
			<p>Now that we understand the workings of ingestion and querying, let's create the database and catalog for our feature set in Lake Formation: </p>
			<ol>
				<li value="1">To create a database, visit the AWS Lake Formation page via a search or using this URL: https://console.aws.amazon.com/lakeformation/home?region=us-east-1#databases.</li>
			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18024_04_23.jpg" alt="Figure 4.23 – Databases in AWS Lake Formation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23 – Databases in AWS Lake Formation</p>
			<p><em class="italic">Figure 4.23</em> displays the list of databases in AWS Lake Formation. </p>
			<ol>
				<li value="2">On the <a id="_idIndexMarker269"/>web page, click on <strong class="bold">Create database</strong>. The following web page will appear. If you see any popups in the transition, asking you to get started with Lake Formation, it can either be canceled or accepted. </li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B18024_04_24.jpg" alt="Figure 4.24 – Lake formation Create database&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.24 – Lake formation Create database</p>
			<ol>
				<li value="3">From the web page displayed above, give the database a name. I am calling it <strong class="source-inline">dev</strong>. Leave <a id="_idIndexMarker270"/>everything else as the default and click on <strong class="bold">Create database</strong>. The database will be created, and it will redirect to the database details page. As databases are groupings of tables together, you can think of this database as a grouping for all the feature views in the project. Once you have the database, the next step is to create the table. As you might have already realized, the table we create here corresponds to the feature view. In the current exercise, there is just one feature view. Hence, a corresponding table needs to be created. <p class="callout-heading">Note</p><p class="callout">As and when you add a new feature view, a corresponding table needs to be added to the database in Lake Formation.</p></li>
				<li>To create a table in the database, click on <strong class="bold">Tables</strong> from the page in <em class="italic">Figure 4.23</em> or visit this URL: <a href="https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables ">https://console.aws.amazon.com/lakeformation/home?region=us-east-1#tables.</a></li>
			</ol>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18024_04_25.jpg" alt="Figure 4.25 – Lake Formation tables&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.25 – Lake Formation tables</p>
			<ol>
				<li value="5">From the web <a id="_idIndexMarker271"/>page in <em class="italic">Figure 4.25</em>, click on the <strong class="bold">Create table</strong> button at the top right. The following web page will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B18024_04_26.jpg" alt="Figure 4.26 – Lake Formation Create table 1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.26 – Lake Formation Create table 1</p>
			<ol>
				<li value="6">For the <strong class="bold">Name</strong> parameter, I have set <strong class="source-inline">customer_rfm_features</strong> and I have selected <a id="_idIndexMarker272"/>the database that was created earlier (<strong class="source-inline">dev</strong>). A description is optional. Once these details are filled in, scroll down. The following options will be seen in the next part of the <strong class="bold">Create table</strong> page.</li>
			</ol>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18024_04_27.jpg" alt="Figure 4.27 – Lake Formation Create table 2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.27 – Lake Formation Create table 2</p>
			<ol>
				<li value="7">The <a id="_idIndexMarker273"/>data store is one of the important properties here. It stands for the location of data in S3. So far, we haven't pushed any data to S3 yet. We will be doing that soon. Let's define where data for this table will be pushed to. I am going to use the S3 bucket we created earlier, hence the location will be <strong class="source-inline">s3://feast-demo-mar-2022/customer-rfm-features/</strong>.<p class="callout-heading">Important Note</p><p class="callout"> Create the <strong class="source-inline">customer-rfm-features</strong> folder in the S3 path.</p></li>
				<li>After selecting the S3 path, scroll down to the last part of the page – the following options will be displayed.</li>
			</ol>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B18024_04_28.jpg" alt="Figure 4.28 – Lake Formation Create table 3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.28 – Lake Formation Create table 3</p>
			<p><em class="italic">Figure 4.28</em> shows the last part of the table creation. The <strong class="bold">Data format</strong> section is asking for <a id="_idIndexMarker274"/>the file format of the data. We will be selecting <strong class="bold">PARQUET</strong> for this exercise. Feel free to experiment with others. Whatever format is selected here, all the ingested data files should be of the same format, else it might not work as expected.</p>
			<ol>
				<li value="9">The last section is the <strong class="bold">Schema</strong> definition of the dataset. You can either click on the <strong class="bold">Add column</strong> button and add the columns individually or can click on the <strong class="bold">Upload Schema</strong> button to upload a JSON defining all the columns at once. Let's use the <strong class="bold">Add column</strong> button and add all the columns in order. Once all the columns are added along with the data types, the columns should look like the following:</li>
			</ol>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B18024_04_29.jpg" alt="Figure 4.29 – Column list in Create table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.29 – Column list in Create table</p>
			<p>As can <a id="_idIndexMarker275"/>be seen from <em class="italic">Figure 4.29</em>, all the columns have been added, along with the entity <strong class="source-inline">customerid</strong> and the two timestamp columns: <strong class="source-inline">event_timestamp</strong> and <strong class="source-inline">created_timestamp</strong>. Once the columns are added, click on the <strong class="bold">Submit</strong> button at the bottom.</p>
			<ol>
				<li value="10">Now, the only thing that is pending is to map this table in the Redshift cluster that has been created. Let's do that next. To create the mapping of the external schema, visit the Redshift cluster page and select the cluster that was created earlier. A web page similar to the one in <em class="italic">Figure 4.30</em> will be displayed. </li>
			</ol>
			<p class="figure-caption">  </p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B18024_04_30.jpg" alt="Figure 4.30 – Redshift cluster details page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.30 – Redshift cluster details page</p>
			<ol>
				<li value="11">From the <a id="_idIndexMarker276"/>web page displayed in <em class="italic">Figure 4.30</em>, click on <strong class="bold">Query data</strong> in the top right of the page. Among the options in the dropdown, pick <strong class="bold">Query in query editor v2</strong>. It will open up a query editor as shown in the following figure:</li>
			</ol>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B18024_04_31.jpg" alt="Figure 4.31 – Redshift query editor v2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.31 – Redshift query editor v2</p>
			<ol>
				<li value="12">Select the <a id="_idIndexMarker277"/>cluster from the left panel and also the database if not selected by default. In the query editor shown in <em class="italic">Figure 4.31</em>, run the following query to map the external database into a schema called <strong class="source-inline">spectrum</strong>: <p class="source-code">create external schema spectrum </p><p class="source-code">from data catalog database dev </p><p class="source-code">iam_role '&lt;redshift_role_arn&gt;' </p><p class="source-code">create external database if not exists;</p></li>
				<li>In the preceding code block, replace <strong class="source-inline">&lt;redshift_role_arn&gt;</strong> with<a id="_idIndexMarker278"/> the <strong class="bold">ARN</strong> of the role that was created and associated with Redshift. The ARN can be found in the IAM console on the role details page, similar to the one in <em class="italic">Figure 4.32</em>.</li>
			</ol>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B18024_04_32.jpg" alt="Figure 4.32 – IAM role details page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.32 – IAM role details page</p>
			<p>On successful <a id="_idIndexMarker279"/>execution of the query, you should be able to see the output <strong class="source-inline">spectrum</strong> schema under the database after refreshing the page as shown in <em class="italic">Figure 4.33</em>.</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B18024_04_33.jpg" alt="Figure 4.33 – Redshift spectrum schema&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.33 – Redshift spectrum schema</p>
			<ol>
				<li value="14">You can <a id="_idIndexMarker280"/>also verify the mapping by executing the following SQL <strong class="source-inline">SELECT</strong> query:<p class="source-code"><strong class="bold">SELECT * from spectrum.customer_rfm_features limit 5</strong></p></li>
			</ol>
			<p>The preceding SQL query will return an empty table in the result as the data is not ingested yet.</p>
			<p>We have completed the mapping of the external table now. All we are left with is to apply the feature set and ingest the data. Let's do that next. </p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">It might seem like a lot of work to add a feature store in an ML pipeline, however, that is not true. Since we are doing it for the first time, it just seems like that. Also, all the steps from resource creation to mapping the external table can be automated using infrastructure as code. Here is a link to an example that automates infrastructure creation (<a href="https://github.com/feast-dev/feast-aws-credit-scoring-tutorial">https://github.com/feast-dev/feast-aws-credit-scoring-tutorial</a>). Apart from that, if you use managed feature stores such as Tecton, SageMaker, or Databricks, the infrastructure is managed and all you will have to do is to create features, ingest them, and use them without worrying about the infrastructure. We will do a comparison of Feast with other feature stores in <a href="B18024_07_ePub.xhtml#_idTextAnchor113"><em class="italic">Chapter 7</em></a>, <em class="italic">Feast Alternatives and ML Best Practices</em>.</p>
			<h3>Applying definitions and ingesting data</h3>
			<p>So far, we<a id="_idIndexMarker281"/> have performed data cleaning, feature engineering, defined the entities and feature definitions, and also<a id="_idIndexMarker282"/> created and mapped the external table to Redshift. Now, let's apply the feature definitions and ingest the data. Continue in the same notebook that we created in the <em class="italic">Creating entities and feature views</em> section (<a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb</a>). </p>
			<p>To apply a feature set, we need the IAM user credentials that was created earlier. Recall that, during the creation of the IAM user, the credential files were available for download. The file contains <strong class="source-inline">AWS_ACCESS_KEY_ID</strong> and <strong class="source-inline">AWS_SECRET_ACCESS_KEY</strong>. Once you have it handy, replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret&gt;</strong> in the following code block:</p>
			<pre class="source-code">import os</pre>
			<pre class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</pre>
			<pre class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</pre>
			<pre class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</pre>
			<p class="callout-heading">Important Note</p>
			<p class="callout">It is never a good idea to set the credentials in the notebook as a raw string. Depending on the tools that are available to the user, it is a good practice to use a secret manager to store secrets. </p>
			<p>After setting the environment variable, all you have to do is to run the following code block to apply the defined feature set: </p>
			<pre class="source-code">%cd customer_segmentation/</pre>
			<pre class="source-code">!feast apply</pre>
			<p>The preceding <a id="_idIndexMarker283"/>code block registers the new <a id="_idIndexMarker284"/>feature definitions and also creates the AWS DynamoDB tables for all the feature views in the definition. The output of the preceding code block is displayed in <em class="italic">Figure 4.34</em>.</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B18024_04_34.jpg" alt="Figure 4.34 – Feast apply output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.34 – Feast apply output</p>
			<p>To verify that DynamoDB tables are created for the feature views, navigate to the DynamoDB console, using the search or visit https://console.aws.amazon.com/dynamodbv2/home?region=us-east-1#tables. You should see the <strong class="source-inline">customer_rfm_features</strong> table as shown in <em class="italic">Figure 4.35</em>.</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B18024_04_35.jpg" alt="Figure 4.35 – DynamoDB tables&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.35 – DynamoDB tables</p>
			<p>Now <a id="_idIndexMarker285"/>that feature definitions <a id="_idIndexMarker286"/>have been applied, to ingest the feature data, let's pick up the feature engineering notebook created in the <em class="italic">Model (feature engineering)</em> section (<a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_feature_engineering.ipynb</a>) and continue in that (the last command of feature engineering produced <em class="italic">Figure 4.21</em>). To ingest the data, the only thing we have to do is write the features DataFrame to the S3 location that is mapped in <em class="italic">Figure 4.28</em>. I mapped the data store location as <strong class="source-inline">s3://feast-demo-mar-2022/customer-rfm-features/</strong>. Let's write the DataFrame to the location as Parquet.</p>
			<p>The following code block ingests the data in the S3 location:</p>
			<pre class="source-code">import os</pre>
			<pre class="source-code">from datetime import datetime</pre>
			<pre class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</pre>
			<pre class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</pre>
			<pre class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</pre>
			<pre class="source-code">file_name = f"rfm_features-{datetime.now()}.parquet" </pre>
			<pre class="source-code">feature_data["event_timestamp"] = datetime.now()</pre>
			<pre class="source-code">feature_data["created_timestamp"] = datetime.now()</pre>
			<pre class="source-code">s3_url = f's3://feast-demo-mar-2022/customer-rfm-features/{file_name}'</pre>
			<pre class="source-code">feature_data.to_parquet(s3_url)</pre>
			<p>The preceding code block sets the AWS credentials of the IAM user, adds the missing columns, <strong class="source-inline">event_timestamp</strong> and <strong class="source-inline">created_timestamp</strong>, and finally writes the Parquet file to the <a id="_idIndexMarker287"/>S3 location. To verify that the file is written successfully, navigate to the S3 location <a id="_idIndexMarker288"/>and verify that the file exists. To make sure that the file is in the correct format, let's navigate to the Redshift query editor in <em class="italic">Figure 4.32</em> and run the following query:</p>
			<pre class="source-code"><strong class="bold">SELECT * from spectrum.customer_rfm_features limit 5</strong></pre>
			<p>The preceding command should result in success, with the output as shown in <em class="italic">Figure 4.36</em>.</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B18024_04_36.jpg" alt="Figure 4.36 – Redshift query after ingesting the data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.36 – Redshift query after ingesting the data</p>
			<p>Before we move on to the next stage of ML, let's just run a couple of APIs, look at what our feature repository looks like, and verify that the query to the historical store works okay. For the <a id="_idIndexMarker289"/>following code, let's use the notebook we used to create and apply feature definitions (<a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter04/ch4_create_apply_feature_definitions.ipynb</a>). </p>
			<p>The following code <a id="_idIndexMarker290"/>connects to the feature store and lists the available entities and feature views:</p>
			<pre class="source-code">"""import feast and load feature store object with the path to the directory which contains <strong class="bold">feature_story.yaml</strong>."""</pre>
			<pre class="source-code">from feast import FeatureStore</pre>
			<pre class="source-code">store = FeatureStore(repo_path=".")</pre>
			<pre class="source-code">#Get list of entities and feature views</pre>
			<pre class="source-code">print("-----------------------Entity---------------------")</pre>
			<pre class="source-code">for entity in store.list_entities():</pre>
			<pre class="source-code">  print(f"entity: {entity}")</pre>
			<pre class="source-code">print("--------------------Feature Views-----------------")</pre>
			<pre class="source-code">for feature_view in store.list_feature_views():</pre>
			<pre class="source-code">  print(f"List of FeatureViews: {feature_view}")</pre>
			<p>The preceding code block prints the <strong class="source-inline">customer</strong> entity and <strong class="source-inline">customer_rfm_features</strong> feature view. Let's query the offline store for a few entities and see if it works as expected. </p>
			<p>To query offline data, we need entity IDs and timestamp columns. The entity ID column is a list of customer IDs and the timestamp column is used for performing point-in-time join queries on the dataset. The following code creates an entity DataFrame for the query:</p>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">from datetime import datetime, timedelta</pre>
			<pre class="source-code">entity_df = pd.DataFrame.from_dict(</pre>
			<pre class="source-code">    {</pre>
			<pre class="source-code">        "customerid": ["12747.0", "12748.0", "12749.0"],</pre>
			<pre class="source-code">        "event_timestamp": [datetime.now()]*3</pre>
			<pre class="source-code">    }</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">entity_df.head()</pre>
			<p>The <a id="_idIndexMarker291"/>preceding code block produces an<a id="_idIndexMarker292"/> entity DataFrame like the one in <em class="italic">Figure 4.37</em>.</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B18024_04_37.jpg" alt="Figure 4.37 – Entity DataFrame&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.37 – Entity DataFrame</p>
			<p>With the sample entity DataFrame, let's query the historical data. The following code fetches a subset of features from the historical store:</p>
			<pre class="source-code">job = store.get_historical_features(</pre>
			<pre class="source-code">    entity_df=entity_df,</pre>
			<pre class="source-code">    features=[</pre>
			<pre class="source-code">              "customer_rfm_features:recency", </pre>
			<pre class="source-code">              "customer_rfm_features:frequency", </pre>
			<pre class="source-code">              "customer_rfm_features:monetaryvalue", </pre>
			<pre class="source-code">              "customer_rfm_features:r", </pre>
			<pre class="source-code">              "customer_rfm_features:f", </pre>
			<pre class="source-code">              "customer_rfm_features:m"]</pre>
			<pre class="source-code">    )</pre>
			<pre class="source-code">df = job.to_df()</pre>
			<pre class="source-code">df.head()</pre>
			<p>The following <a id="_idIndexMarker293"/>code block may take a couple of minutes to run but finally outputs the following results:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B18024_04_38.jpg" alt="Figure 4.38 – Historical retrieval job output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.38 – Historical retrieval job output</p>
			<p>Now we <a id="_idIndexMarker294"/>can say that our feature engineering pipeline is ready. The next steps that are required are to train the model, perform validation, and, if happy with the performance of the model, deploy the pipeline into production. We will look at training, validation, deployment, and model scoring in the next chapter. Let's briefly summarize what we have learned next.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor076"/>Summary</h1>
			<p>In this chapter, we started with the goal of adding the Feast feature store to our ML model development. We accomplished that by creating the required resources on AWS, adding an IAM user to access those resources. After creating the resources, we went through the steps of the ML life cycle again from the problem statement to feature engineering and feature ingestion. We also verified that created feature definitions and ingested data could be queried through the API. </p>
			<p>Now that we have set the stage for the next steps of the ML life cycle – model training, validation, deployment, and scoring, in the next chapter, we will learn how the addition of the feature store right from the beginning makes the model production-ready when the development is complete.  </p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor077"/>References</h1>
			<ul>
				<li>Feast documentation: <a href="https://docs.feast.dev/">https://docs.feast.dev/</a></li>
				<li>Credit scoring with Feast on AWS: <a href="https://github.com/feast-dev/feast-aws-credit-scoring-tutorial">https://github.com/feast-dev/feast-aws-credit-scoring-tutorial</a></li>
			</ul>
		</div>
	</body></html>