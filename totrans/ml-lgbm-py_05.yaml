- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: LightGBM Parameter Optimization with Optuna
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Optuna 进行 LightGBM 参数优化
- en: Previous chapters have discussed the LightGBM hyperparameters and their effect
    on building models. A fundamental problem when building a new model is finding
    the optimal hyperparameters to achieve the best performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章讨论了 LightGBM 的超参数及其对构建模型的影响。构建新模型时的一个基本问题是找到最佳超参数以实现最佳性能。
- en: This chapter focuses on the parameter optimization process using a framework
    called Optuna. Different optimization algorithms are discussed alongside the pruning
    of the hyperparameter space. A practical example shows how to apply Optuna to
    find optimal parameters for LightGBM. Advanced use cases for Optuna are also shown.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍使用名为 Optuna 的框架进行参数优化过程。讨论了不同的优化算法以及超参数空间的剪枝。一个实际示例展示了如何将 Optuna 应用于寻找
    LightGBM 的最佳参数。还展示了 Optuna 的高级用例。
- en: 'The chapter’s main topics are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容包括以下内容：
- en: Optuna and optimization algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Optuna 和优化算法
- en: Optimizing LightGBM with Optuna
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Optuna 优化 LightGBM
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The chapter includes examples and code excerpts illustrating how to perform
    parameter optimization studies for LightGBM using Optuna. Complete examples and
    instructions for setting up a suitable environment for this chapter are available
    at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-5](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-5).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括示例和代码片段，说明如何使用 Optuna 对 LightGBM 进行参数优化研究。完整的示例和设置本章所需环境的说明可在 [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-5](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-5)
    获取。
- en: Optuna and optimization algorithms
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Optuna 和优化算法
- en: Examples from previous chapters have shown that choosing the best hyperparameters
    for a problem is critical in solving a machine learning problem. The hyperparameters
    significantly impact the algorithm’s performance and generalization capability.
    The optimal parameters are also specific to the model used and the learning problem
    being solved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章的示例表明，为问题选择最佳超参数对于解决机器学习问题至关重要。超参数对算法的性能和泛化能力有显著影响。最佳参数也特定于所使用的模型和要解决的问题。
- en: 'Other issues complicating hyperparameter optimization are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 其他复杂超参数优化的因素如下：
- en: '**Cost**: For each unique set of hyperparameters (of which there can be many),
    an entire training run, often with cross-validation, must be performed. This is
    highly time-consuming and computationally expensive.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：对于每一组独特的超参数集合（可能有很多），必须执行整个训练过程，通常包括交叉验证。这非常耗时且计算成本高昂。'
- en: '**High-dimensional search spaces**: Each parameter can have a vast range of
    potential values, making testing each value impossible.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高维搜索空间**：每个参数可以有一个广泛的潜在值范围，使得测试每个值变得不可能。'
- en: '**Parameter interaction**: Optimizing each parameter in isolation is often
    impossible, as some parameters’ values interact with others’ values. A good example
    is the learning rate and the number of estimators in LightGBM: fewer estimators
    necessitate a larger learning rate, and vice versa. This phenomenon is shown in
    *Figure 5**.1*.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数交互**：单独优化每个参数通常是不可能的，因为某些参数的值会与其他参数的值相互作用。一个很好的例子是 LightGBM 中的学习率和估计器的数量：更少的估计器需要更大的学习率，反之亦然。这种现象在
    *图 5.1* 中显示。'
- en: '![Figure 5.1 – A parallel coordinate plot showing a parameter interaction between
    the learning rate and the number of estimators: having more estimators requires
    a lower learning rate, and vice versa](img/B16690_05_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 一个并行坐标图，显示了学习率和估计器数量之间的参数交互：拥有更多估计器需要更低的 learning rate，反之亦然](img/B16690_05_01.jpg)'
- en: 'Figure 5.1 – A parallel coordinate plot showing a parameter interaction between
    the learning rate and the number of estimators: having more estimators requires
    a lower learning rate, and vice versa'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 一个并行坐标图，显示了学习率和估计器数量之间的参数交互：拥有更多估计器需要更低的 learning rate，反之亦然
- en: '*Figure 5**.1* visualizes parameter interactions using a technique called a
    **parallel coordinate plot**. Parallel coordinate plots are a visualization tool
    designed to represent high-dimensional data, making them especially useful for
    visualizing the results of hyperparameter optimization. Each dimension (in this
    context, a hyperparameter) is portrayed as a vertical axis arranged in parallel.
    The range of each axis mirrors the range of values that the hyperparameter can
    assume. Every individual configuration of hyperparameters is depicted as a line
    crossing all these axes, with the intersection point on each axis indicating the
    value of that hyperparameter for the given configuration. Lines can also be color-coded
    based on performance metrics, such as validation accuracy, to discern which hyperparameter
    combinations yield superior outcomes.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.1*使用一种称为**平行坐标图**的技术来可视化参数交互。平行坐标图是一种用于表示高维数据的可视化工具，因此对于可视化超参数优化的结果特别有用。每个维度（在此上下文中，指超参数）被描绘为一条垂直轴，平行排列。每个轴的范围反映了超参数可以假设的值的范围。每个超参数的配置都被描绘为一条穿过所有这些轴的线，每个轴上的交点表示给定配置中该超参数的值。线条还可以根据性能指标，如验证准确率，进行着色编码，以区分哪些超参数组合产生更好的结果。'
- en: The beauty of parallel coordinate plots lies in their ability to illustrate
    relationships between multiple hyperparameters and their cumulative impact on
    performance, such as the parameter interaction shown in *Figure 5**.1*. Observing
    the lines’ clustering or similarities in their color allows us to glean trends
    and intricate interdependencies between hyperparameters. This ability to visualize
    multidimensional patterns helps data scientists pinpoint which hyperparameter
    values or combinations are most conducive to optimal model performance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 平行坐标图之美在于它们能够展示多个超参数之间的关系及其对性能的累积影响，例如*图5.1*中所示的超参数交互。观察线条的聚类或颜色相似性，我们可以了解超参数之间的趋势和复杂的相互依赖关系。这种可视化多维模式的能力有助于数据科学家确定哪些超参数值或组合最有利于模型性能的最优化。
- en: Given the challenges and complications of hyperparameter optimization, a naive
    approach to finding the optimal parameters is manual optimization. With manual
    optimization, a human practitioner selects parameters based on intuitive understanding
    and experience. A model is trained with these parameters, and the process is repeated
    until satisfactory parameters are found. Manual optimization is simple to implement
    but is very time-consuming due to the human-in-the-loop nature of the process.
    Human intuition is also fallible, and good parameter combinations can easily be
    missed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数优化的挑战和复杂性，寻找最佳参数的直观方法是手动优化。在手动优化中，人类从业者根据直观理解和经验选择参数。使用这些参数训练模型，然后重复此过程，直到找到令人满意的参数。手动优化易于实现，但由于过程中涉及人类，因此非常耗时。人类的直觉也可能出错，并且很容易错过好的参数组合。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The process of finding optimal parameters is often called a parameter **study**.
    Each configuration (combination of parameters) tested in the study is referred
    to as a **trial**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳参数的过程通常被称为**参数研究**。研究中测试的每个配置（参数组合）被称为**试验**。
- en: In the previous chapters’ examples, the approach we used thus far was **grid
    search**. With grid search, we set up a parameter grid consisting of each parameter
    and a range of potential values and exhaustively tested each possible combination
    to find the optimal values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章的示例中，我们迄今为止使用的方法是**网格搜索**。使用网格搜索，我们设置一个参数网格，包括每个参数和潜在值的范围，并彻底测试每个可能的组合以找到最佳值。
- en: 'Grid search solves the parameter interaction problem well: since each possible
    combination is tested, each interaction is accounted for.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索很好地解决了参数交互问题：由于每个可能的组合都进行了测试，每个交互都被考虑在内。
- en: 'However, the downside of using grid search is the cost. Since we exhaustively
    test each parameter combination, the number of trials quickly becomes prohibitive,
    especially if more parameters are added. For example, consider the following grid:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用网格搜索的缺点是成本。由于我们彻底测试了每个参数组合，试验次数迅速变得难以承受，尤其是在添加更多参数的情况下。例如，考虑以下网格：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: An optimization study for this grid would require 36 trials. Adding just one
    additional parameter with two possible values doubles the cost of the study.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对此网格进行的优化研究需要36次试验。仅添加一个具有两个可能值的额外参数就会将研究成本翻倍。
- en: What’s needed is an algorithm and framework that can intelligently optimize
    the parameters within a limited number of trials that we control. Several frameworks
    exist for this purpose, including SHERPA, a Python library for tuning machine
    learning models; Hyperopt, another Python library for parameter optimization over
    complex search spaces; and Talos, a tool specifically tailored for Keras. However,
    in the next section, and for the rest of the chapter, we look at **Optuna**, a
    framework designed to automate tuning machine learning models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 需要的是一个算法和框架，能够在有限的试验次数内智能地优化我们控制的参数。为此目的，存在几个框架，包括用于调整机器学习模型的 Python 库 SHERPA；另一个用于在复杂搜索空间中进行参数优化的
    Python 库 Hyperopt；以及专门针对 Keras 的工具 Talos。然而，在下一节以及本章的其余部分，我们将探讨**Optuna**，这是一个旨在自动化调整机器学习模型的框架。
- en: Introducing Optuna
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Optuna
- en: Optuna is an open source **hyperparameter optimization** (**HPO**) framework
    designed to automate finding the best hyperparameters for machine learning models
    ([https://optuna.org/](https://optuna.org/)). It is written in Python and can
    be easily integrated with various machine learning libraries, including LightGBM.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 是一个开源的**超参数优化**（**HPO**）框架，旨在自动化寻找机器学习模型的最佳超参数 ([https://optuna.org/](https://optuna.org/))。它用
    Python 编写，可以轻松集成到各种机器学习库中，包括 LightGBM。
- en: Optuna provides efficient optimization algorithms to search hyperparameter spaces
    more effectively. In addition to the optimization algorithms, Optuna also provides
    pruning strategies to save computational resources and time by pruning poorly
    performing trials.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 提供了高效的优化算法，以更有效地搜索超参数空间。除了优化算法之外，Optuna 还提供了剪枝策略，通过剪枝表现不佳的试验来节省计算资源和时间。
- en: Besides optimization and pruning algorithms, Optuna also provides an easy-to-use
    API for defining parameter types (integer, float, or categorical), creating and
    automating resumable optimization studies, and visualizing the results of optimization
    runs. Later in the chapter, we see how to use the API practically.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了优化和剪枝算法之外，Optuna 还提供了一个易于使用的 API，用于定义参数类型（整数、浮点或分类），创建和自动化可恢复的优化研究，以及可视化优化运行的结果。在本章的后面部分，我们将看到如何实际使用该
    API。
- en: Optimization algorithms
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化算法
- en: 'Optuna provides several efficient optimization algorithms. In this section,
    we focus on two of the available algorithms: a **Tree-Structured Parzen Estimator**
    (**TPE**) and a **Covariance Matrix Adaptation Evolution Strategy** (**CMA-ES**)
    algorithm.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 提供了几个高效的优化算法。在本节中，我们重点关注两种可用的算法：**树结构帕累托估计器**（**TPE**）和**协方差矩阵自适应进化策略**（**CMA-ES**）算法。
- en: TPE
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TPE
- en: To understand TPE, we must first know what a Parzen estimator is.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 TPE，我们首先必须知道什么是帕累托估计器。
- en: A Parzen estimator, or **Kernel Density Estimator** (**KDE**), is a technique
    used to estimate the probability distribution of a set of data points. It’s a
    non-parametric method, meaning it doesn’t assume any specific underlying distribution
    for the data. Instead, it tries to “learn” the distribution based on the observed
    data points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托估计器，或**核密度估计器**（**KDE**），是一种用于估计一组数据点概率分布的技术。它是一种非参数方法，这意味着它不假设数据有任何特定的潜在分布。相反，它试图根据观察到的数据点“学习”分布。
- en: Imagine you have data points and want to know how the data is distributed. One
    way to do this is by placing small “hills” (kernel functions) over each data point.
    These “hills” can have different shapes, such as Gaussian (bell-shaped) or uniform
    (box-shaped). The height of the “hill” at any point represents the likelihood
    that a new data point would fall at that location. The Parzen estimator works
    by adding up all these “hills” to create a smooth landscape representing the estimated
    probability distribution of the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一组数据点，并想知道数据是如何分布的。一种方法是在每个数据点上放置小的“山丘”（核函数）。这些“山丘”可以有不同的形状，例如高斯（钟形）或均匀（箱形）。任何点的“山丘”高度代表新数据点落在该位置的可能性。帕累托估计器通过将这些“山丘”相加来创建一个平滑的地形，代表数据的估计概率分布。
- en: In the case of TPE, the data points we care about are the parameter combinations,
    and the probability distribution is the likelihood of a set of parameters being
    `good` or `bad` [1], [2].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TPE 的情况下，我们关心的数据点是参数组合，概率分布是一组参数被认为是“好”或“坏”的可能性 [1]，[2]。
- en: 'TPE starts by sampling a few random combinations of hyperparameters and evaluating
    the model’s performance for each. Based on these initial results, TPE divides
    the hyperparameter combinations into two groups: `good` (those that lead to better
    performance) and `bad` (those that lead to worse performance):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: TPE首先采样一些随机组合的超参数，并评估每个组合的模型性能。基于这些初步结果，TPE将超参数组合分为两组：`良好`（那些导致更好性能的组合）和`不良`（那些导致更差性能的组合）：
- en: 'l(x): The probability density function of `good` configurations'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: l(x)：`良好`配置的概率密度函数
- en: 'g(x) : The probability density function of `bad` configurations'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'g(x) : `不良`配置的概率密度函数'
- en: TPE then estimates the probability distributions of hyperparameter combinations
    for both `good` and `bad` groups using the Parzen estimator technique.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TPE随后使用Parzen估计技术估计`良好`和`不良`两组超参数组合的概率分布。
- en: 'With estimations of the probability distributions available, TPE calculates
    the **Expected Improvement** (**EI**) of hyperparameter configurations. EI can
    be calculated as the ratio between the two densities:  l(x) _ g(x) .     With each trail, the algorithm samples new hyperparameter configurations that
    maximize the EI.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率分布估计可用的情况下，TPE计算超参数配置的**期望改进**（**EI**）。EI可以计算为两个密度之间的比率：l(x) _ g(x)。每次试验，算法都会采样新的超参数配置，以最大化EI。
- en: The tree structure in TPE comes from the algorithm’s ability to handle parameter
    interaction within the hyperparameter search space, where specific hyperparameters’
    relevance depends on others’ values. To handle this, TPE builds a hierarchical
    structure that captures the relationships between different hyperparameters and
    adapts the sampling process accordingly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: TPE中的树结构来源于算法在超参数搜索空间内处理参数交互的能力，其中特定超参数的相关性取决于其他超参数的值。为了处理这种情况，TPE构建了一个层次结构，捕捉不同超参数之间的关系，并相应地调整采样过程。
- en: In summary, TPE estimates the distributions of `good` and `bad` parameters and
    uses them to find optimal parameters by maximizing new trials’ expected improvement.
    TPE is cost-effective since it approximates the distributions and can search for
    better parameters optimally (in a non-exhaustive way). TPE also handles parameter
    interactions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TPE估计`良好`和`不良`参数的分布，并利用它们通过最大化新试验的期望改进来寻找最佳参数。由于TPE可以近似分布并以最优方式（非穷举方式）搜索更好的参数，因此它具有成本效益。TPE还可以处理参数交互。
- en: An alternative algorithm provided by Optuna is the CMA-ES algorithm, which we
    discuss next.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna提供的另一种算法是CMA-ES算法，我们将在下文中讨论。
- en: CMA-ES
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CMA-ES
- en: CMA-ES is another optimization algorithm that can be used to find optimal hyperparameters
    [3]. Compared to TPE, CMA-ES is well suited to cases that involve continuous variables
    and when the search space is non-linear and non-convex.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES是另一种可以用来寻找最佳超参数的优化算法[3]。与TPE相比，CMA-ES非常适合涉及连续变量以及搜索空间非线性且非凸的情况。
- en: CMA-ES is an example of an **evolutionary algorithm** (**EA**). An EA is a type
    of optimization algorithm inspired by the process of natural evolution. It aims
    to find the best solution to a problem by mimicking how nature evolves species
    through selection, reproduction, mutation, and inheritance. Evolutionary algorithms
    start with a population of candidate solutions and modify the candidates with
    each subsequent *generation* to adapt more closely to the best solution. This
    generational process is illustrated in *Figure 5**.2*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES是**进化算法**（**EA**）的一个例子。EA是一种受自然进化过程启发的优化算法。它通过模拟自然界通过选择、繁殖、突变和遗传进化物种的方式，旨在找到问题的最佳解决方案。进化算法从候选解决方案的种群开始，并在每一代中修改候选方案以更接近最佳解决方案。这种代际过程在*图5.2*中展示。
- en: '![Figure 5.2 – A two-dimensional illustration of candidate solutions (red x
    marks) evolving with each subsequent generation to approximate the global optimum
    (located at the top and center of each landscape). In the context of CMA-ES, each
    candidate solution represents a combination of hyperparameter values, and the
    algorithm’s performance determines the optimum](img/B16690_05_02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 候选解决方案（红色x标记）随每一代进化以近似全局最优（位于每个景观的顶部和中心）。在CMA-ES的上下文中，每个候选解决方案代表一组超参数值，算法的性能决定了最优解](img/B16690_05_02.png)'
- en: Figure 5.2 – A two-dimensional illustration of candidate solutions (red x marks)
    evolving with each subsequent generation to approximate the global optimum (located
    at the top and center of each landscape). In the context of CMA-ES, each candidate
    solution represents a combination of hyperparameter values, and the algorithm’s
    performance determines the optimum
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 一个二维图示，展示了候选解（红色x标记）随着每一代后续演变，以逼近全局最优解（位于每个景观的顶部和中心）。在CMA-ES的上下文中，每个候选解代表一组超参数值的组合，算法的性能决定了最优解。
- en: Central to the evolutionary process of CMA-ES is the covariance matrix. A covariance
    matrix is a square, symmetric matrix representing the covariance between pairs
    of variables (in the case of CMA-ES, the hyperparameters), providing insight into
    their relationships. The diagonal elements of the matrix represent the variances
    of individual variables, while the off-diagonal elements represent the covariances
    between pairs of variables. When there’s a positive covariance, it signals that
    the variables usually move in the same direction, either increasing or decreasing.
    Conversely, a negative covariance points to a relationship where, as one variable
    rises, the other tends to fall, and vice versa. A covariance of zero suggests
    no linear relationship between the variables.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES的进化过程的核心是协方差矩阵。协方差矩阵是一个表示变量对（在CMA-ES的情况下，是超参数）之间协方差的正方形、对称矩阵，提供了它们之间关系的洞察。矩阵的对角元素代表单个变量的方差，而矩阵的非对角元素代表变量对的协方差。当存在正协方差时，它表示变量通常在同一方向上移动，要么增加要么减少。相反，负协方差指向一种关系，其中一个变量上升时，另一个变量倾向于下降，反之亦然。协方差为零表示变量之间没有线性关系。
- en: 'CMA-ES applies the evolutionary principles as follows when optimizing hyperparameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当优化超参数时，CMA-ES应用以下进化原则：
- en: Within the hyperparameter search space, initialize the mean and the covariance
    matrix.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在超参数搜索空间内，初始化平均值和协方差矩阵。
- en: 'Repeat the evolutionary process:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复进化过程：
- en: Generate a population of candidates from the search space using the mean and
    the covariance matrix. Each candidate represents a combination of hyperparameter
    values.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平均值和协方差矩阵从搜索空间生成候选解。每个候选解代表一组超参数值的组合。
- en: Evaluate the fitness of the candidates. **Fitness** refers to the quality of
    a candidate or how well it solves the optimization problem. With CMA-ES, this
    means training the model on the dataset using the candidate hyperparameters and
    evaluating the performance on the validation set.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估候选解的适应度。**适应度**指的是候选解的质量或它解决优化问题的程度。在CMA-ES中，这意味着使用候选超参数在数据集上训练模型，并在验证集上评估性能。
- en: Select the best candidates from the population.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从种群中选择最佳候选解。
- en: Update the mean and the covariance matrix from the best candidates.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从最佳候选解更新平均值和协方差矩阵。
- en: Repeat for a maximum number of trials or until no improvement is seen in the
    population’s fitness.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复进行试验，直到达到最大试验次数或种群适应度不再提高。
- en: CMA-ES performs well in complex search spaces and intelligently samples the
    search space, guided by the covariance matrix. It is beneficial when the hyperparameter
    search space is complex and non-linear or when the evaluation of the validation
    data is noisy (for instance, when a metric is an inconsistent performance indicator).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CMA-ES在复杂搜索空间中表现良好，并智能地采样搜索空间，由协方差矩阵引导。当超参数搜索空间复杂且非线性，或者验证数据的评估有噪声（例如，当指标是一个不一致的性能指标）时，这很有益。
- en: 'Both TPE and CMA-ES address the issues associated with hyperparameter optimization:
    both algorithms effectively search a high-dimensional search space. Both algorithms
    capture parameter interaction. Both algorithms give us control of the cost: we
    can decide our optimization budget and limit our search to that.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TPE和CMA-ES都解决了超参数优化相关的问题：两种算法都有效地搜索高维搜索空间。两种算法都捕捉参数交互。两种算法都让我们对成本有了控制：我们可以决定我们的优化预算，并将搜索限制在那个范围内。
- en: The main differences between TPE and CMA-ES lie in their overall approach. TPE
    is a probabilistic model with a sequential search strategy, compared to CMA-ES,
    which is population-based and evaluates solutions in parallel. This often means
    TPE is more exploitative in its search, while CMA-ES balances exploration and
    exploitation using population control mechanisms. However, TPE is typically more
    efficient than CMA-ES, especially for a small number of parameters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: TPE 和 CMA-ES 之间的主要区别在于它们整体的方法。TPE 是一个具有顺序搜索策略的概率模型，与基于群体的 CMA-ES 相比，CMA-ES 会并行评估解决方案。这通常意味着
    TPE 在搜索中更具探索性，而 CMA-ES 通过群体控制机制平衡探索和利用。然而，TPE 通常比 CMA-ES 更有效率，尤其是在参数数量较少的情况下。
- en: Optuna provides further optimization to the search process in pruning ineffective
    trials. We’ll discuss some pruning strategies next.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 为剪枝无效试验的搜索过程提供了进一步的优化。接下来，我们将讨论一些剪枝策略。
- en: Pruning strategies
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪枝策略
- en: 'Pruning strategies refer to methods that avoid spending optimization time on
    unpromising trials by pruning these trials from the study. Pruning occurs synchronously
    with the model training process: the validation error is checked during training,
    and the training is stopped if the algorithm is underperforming. In this way,
    pruning is similar to *early stopping*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝策略是指通过剪枝这些试验来避免在无望的试验上浪费优化时间的方法。剪枝与模型训练过程同步发生：在训练过程中检查验证误差，如果算法表现不佳，则停止训练。这样，剪枝类似于*早期停止*。
- en: Median pruning
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中值剪枝
- en: Optuna provides several pruning strategies, one of the simplest being **median
    pruning**. With median pruning, each trial reports an intermediate result after
    *n* steps. The median of the intermediate results is then taken, and any trials
    below the median of previous trials at the same step are stopped.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 提供了多种剪枝策略，其中最简单的一种是**中值剪枝**。在中值剪枝中，每个试验在 *n* 步之后报告一个中间结果。然后取中间结果的平均值，并停止任何在相同步骤中低于先前试验中值的结果。
- en: Successive halving and Hyperband
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续减半和 Hyperband
- en: 'A more sophisticated strategy is called **successive halving** [4]. This takes
    a more global approach and assigns a small, equal budget of training steps to
    all trials. Successive halving then proceeds iteratively: at each iteration, the
    performance of each trial is evaluated, and the top half of the candidates are
    selected for the next round, with the bottom half pruned away. The training budget
    is doubled for the next iteration, and the process is repeated. This way, the
    optimization budget is spent on the most promising candidates. As a result, a
    small optimization budget is spent on eliminating the underperforming candidates,
    and more resources are spent on finding the best parameters.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更复杂的策略被称为**连续减半**[4]。它采取了一种更全局的方法，并将相同的小预算的训练步骤分配给所有试验。连续减半然后迭代进行：在每个迭代中，评估每个试验的性能，并选择候选者中的上半部分进入下一轮，而下半部分被剪枝。下一轮的训练预算加倍，然后重复此过程。这样，优化预算被花在最有希望的候选者上。因此，一小部分优化预算被用于消除表现不佳的候选者，而更多的资源被用于寻找最佳参数。
- en: '**Hyperband** is another pruning technique that extends successive halving
    by incorporating random search and a multi-bracket resource allocation strategy
    [5]. While successive halving efficiently narrows down a set of candidate configurations
    by iteratively pruning underperforming ones and allocating more resources to the
    remaining promising ones, it relies on a fixed initial set of configurations and
    a single resource allocation scheme.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hyperband** 是另一种剪枝技术，它通过结合随机搜索和多括号资源分配策略来扩展连续减半[5]。虽然连续减半通过迭代剪枝表现不佳的候选配置并分配更多资源给剩余的有希望的配置，从而有效地缩小候选配置集，但它依赖于一个固定的初始配置集和单一的资源分配方案。'
- en: Hyperband instead uses a multi-bracket resource allocation strategy, which divides
    the total computational budget into several brackets, each representing a different
    level of resource allocation. Within each bracket, successive halving is applied
    to iteratively eliminate underperforming configurations and allocate more resources
    to the remaining promising ones. At the beginning of each bracket, a new set of
    hyperparameter configurations is sampled using random search, which allows Hyperband
    to explore the hyperparameter space more broadly and reduce the risk of missing
    good configurations. This concurrent process enables Hyperband to adaptively balance
    exploration and exploitation in the search process, ultimately leading to more
    efficient and effective hyperparameter tuning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperband 而是使用多区间资源分配策略，将总计算预算分成几个区间，每个区间代表不同级别的资源分配。在每个区间内，应用连续减半来迭代消除表现不佳的配置，并将更多资源分配给剩余的有希望的配置。在每个区间的开始，使用随机搜索采样一个新的超参数配置集，这允许
    Hyperband 更广泛地探索超参数空间并降低错过良好配置的风险。这种并发过程使 Hyperband 能够在搜索过程中自适应地平衡探索和利用，最终导致更高效和有效的超参数调整。
- en: Optuna has performed empirical studies of optimization algor[ithms and corresponding
    pruning strategies (https://github.com](https://github.com/optuna/optuna/wiki/Benchmarks-with-Kurobako)/optuna/optuna/wiki/Benchmarks-with-Kurobako).
    *Empirically, they found that Hyperband is the best TPE or CMA-ES* *optimization
    strategy*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna 对优化算法及其相应的剪枝策略进行了实证研究[https://github.com](https://github.com/optuna/optuna/wiki/Benchmarks-with-Kurobako)/optuna/optuna/wiki/Benchmarks-with-Kurobako)。*实证研究表明，Hyperband
    是最佳的 TPE 或 CMA-ES 优化策略*。
- en: This section gave an overview of the theory and algorithms powering Optuna,
    focusing on TPE, CMA-ES, and advanced pruning strategies. In the next section,
    we’ll practically apply Optuna to a machine learning problem with LightGBM.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了 Optuna 所使用的理论和算法，重点关注 TPE、CMA-ES 和高级剪枝策略。在下一节中，我们将实际应用 Optuna 到一个与 LightGBM
    相关的机器学习问题上。
- en: Optimizing LightGBM with Optuna
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Optuna 优化 LightGBM
- en: 'We’ll walk through applying Optuna using a classification example. The problem
    we’ll be modeling is to predict customer churn (*Yes*/*No*) for a telecommunications
    provider. The dataset is available from [https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data](https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data).     The data describes each customer using data available to the provider – for example,
    gender, whether the customer is paying for internet service, has paperless billing,
    pays for tech support, and their monthly charges. The data consists of both numeric
    and categorical features. The data has already been cleaned and is balanced, allowing
    us to focus on the parameter optimization study.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用分类示例来演示如何应用 Optuna。我们将要建模的问题是为电信提供商预测客户流失（*是*/*否*）。数据集可以从[https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data](https://github.com/IBM/telco-customer-churn-on-icp4d/tree/master/data)获取。数据描述了每个客户使用提供商可用的数据——例如，性别、客户是否支付互联网服务费、是否有无纸化账单、是否支付技术支持费以及他们的月度费用。数据包括数值和分类特征。数据已经过清洗且平衡，这使我们能够专注于参数优化研究。
- en: 'We start by defining the objective of our parameter study. The `objective`
    function is called once for each trial. In this case, we want to train a LightGBM
    model on the data and calculate the F1 score. Optuna passes a `trial` object to
    the `objective` function, which we can use to set up the parameters for the specific
    trial. The following is an example code snippet that shows how to define an `objective`
    function with parameters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义参数研究的目标。`objective` 函数为每个试验调用一次。在这种情况下，我们希望在数据上训练一个 LightGBM 模型并计算 F1
    分数。Optuna 将一个 `trial` 对象传递给 `objective` 函数，我们可以使用它来设置特定试验的参数。以下是一个示例代码片段，展示了如何定义带有参数的
    `objective` 函数：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we can see how we use the methods provided by `trial` to set up the hyperparameters.
    For each parameter, a value is suggested by the optimization algorithm within
    the range specified. We can suggest categorical variables using `trial.suggest_categorical`
    (as can be seen for the `boosting` type), and `int` and `float` parameters using
    `suggest_int` and `suggest_float`, respectively. When suggesting floats or integers,
    a range and, optionally, a step size are specified:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们如何使用 `trial` 提供的方法来设置超参数。对于每个参数，优化算法在指定的范围内建议一个值。我们可以使用 `trial.suggest_categorical`
    建议分类变量（如 `boosting` 类型所示），并分别使用 `suggest_int` 和 `suggest_float` 建议整数和浮点参数。在建议浮点数或整数时，指定一个范围，可选地还可以指定步长：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Setting a step size means the optimization algorithm does not suggest any arbitrary
    value in the range but limits suggestions to the steps between the lower and upper
    bound (40, 60, 80, 100, …, 400).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 设置步长意味着优化算法不会在范围内建议任何任意值，而是将建议限制在上下限之间的步长（40, 60, 80, 100, …, 400）。
- en: We also have the option to log scale the range of possible values by passing
    `log=True` for numeric parameters. Log scaling the parameter range has the effect
    that more values are tested close to the range’s lower bound and (logarithmically)
    fewer values towards the upper bound. Log scaling is particularly well suited
    to the learning rate where we want to focus on smaller values and exponentially
    increase tested values until the upper bound.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择通过为数值参数传递 `log=True` 来对可能值的范围进行对数缩放。对数缩放参数范围的效果是，在范围的下限附近测试更多的值，而在上限附近（对数地）测试较少的值。对数缩放特别适合学习率，因为我们希望关注较小的值，并通过指数增加测试值直到上限。
- en: 'To apply pruning when training LightGBM models, Optuna provides a purpose-built
    callback that integrates with the optimization process:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要在训练LightGBM模型时应用剪枝，Optuna提供了一个专门定制的回调函数，该回调函数与优化过程集成：
- en: '[PRE3]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We must specify an error metric when creating the callback, and, in our case,
    we specify `"binary"` for the binary error.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建回调函数时，我们必须指定一个错误度量标准，在我们的情况下，我们为二进制错误指定 `"binary"`。
- en: 'With the hyperparameters set up, we can fit as we usually do, passing the parameters
    and the callback as we would normally:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好超参数后，我们可以像平常一样拟合，传递参数和回调函数：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We train the model using five-fold cross-validation with the F1 macro score
    for scoring. Finally, the `objective` function returns the mean of the F1 scores
    as the trial evaluation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用五折交叉验证和F1宏分数作为评分标准来训练模型。最后，`objective` 函数返回F1分数的平均值作为试验评估。
- en: 'We are ready to start an optimization study with the defined `objective` function.
    We create a sampler, pruner, and the study itself and then call `optimize` with
    our `objective` function:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好使用定义的 `objective` 函数开始一个优化研究。我们创建一个采样器、剪枝器和研究本身，然后调用 `optimize` 并传递我们的
    `objective` 函数：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We use the TPE optimization algorithm as a sampler alongside Hyperband pruning.
    The minimum and maximum resources specified for the Hyperband pruner control the
    minimum and the maximum number of iterations (or estimators) trained per trial.
    When applying pruning, the reduction factor controls how many trials are promoted
    in each halving round.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TPE优化算法作为采样器，与Hyperband剪枝一起使用。Hyperband剪枝器指定的最小和最大资源控制了每个试验训练的最小和最大迭代次数（或估计器）。在应用剪枝时，缩减因子控制每个减半回合中提升的试验数量。
- en: The study is created by specifying the optimization direction (`maximize` or
    `minimize`). Here, we are optimizing the F1 score, so we want to maximize the
    value.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定优化方向（`maximize` 或 `minimize`）来创建研究。在这里，我们正在优化F1分数，因此我们希望最大化这个值。
- en: 'We then call `study.optimize` and set our optimization budget: `n_trials=100`.
    We also perform a memory optimization setting, `gc_after_trial=True`. Performing
    `n_jobs=-1` runs as many trials as there are CPU cores in parallel.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后调用 `study.optimize` 并设置我们的优化预算：`n_trials=100`。我们还执行了一个内存优化设置，`gc_after_trial=True`。执行
    `n_jobs=-1` 将并行运行与CPU核心数量相同的试验。
- en: 'After running the optimization, we can get the best trial and parameters by
    calling the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行优化后，我们可以通过调用以下代码来获取最佳试验和参数：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding example shows how to apply Optuna to find LightGBM hyperparameters
    effectively. Next, we look at some advanced features of the Optuna framework.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了如何有效地将Optuna应用于寻找LightGBM超参数。接下来，我们将探讨Optuna框架的一些高级特性。
- en: Advanced Optuna features
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级Optuna特性
- en: When optimizing hyperparameters for large machine learning problems, the optimization
    process may run for days or weeks. In these cases, saving an optimization study
    and resuming it later is helpful to guard against data loss or migrating the study
    between different machines.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当优化大型机器学习问题的超参数时，优化过程可能需要持续数天或数周。在这些情况下，保存优化研究并在以后恢复它有助于防止数据丢失或将研究迁移到不同的机器之间。
- en: Saving and resuming an optimization study
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存和恢复优化研究
- en: 'Optuna supports saving and resuming an optimization study in two ways: **in
    memory** and using a **remote** **database** (**RDB**).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna支持两种方式来保存和恢复优化研究：**内存中**和使用**远程**数据库（**RDB**）。
- en: 'When a study is run in memory, the standard Python methods for serializing
    an object can be applied. For example, either `joblib` or `pickle` may be used.
    We use `joblib` to save a study:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当在内存中运行研究时，可以应用标准的Python序列化对象的方法。例如，可以使用`joblib`或`pickle`。我们使用`joblib`来保存研究：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To restore and resume the study, we deserialize the `study` object and continue
    with optimization:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了恢复和继续研究，我们需要反序列化`study`对象并继续优化：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The alternative to running the study in memory is to use an RDB. When using
    an RDB, the study’s intermediate (trial) and final results are persisted in a
    SQL database backend. The RDB can be hosted on a separate machine. Any of the
    SQL databases supported by SQL Alchemy may be used (https://docs.sqlalchemy.org/en/20/core/engines.xhtml#database-urls).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 运行研究在内存中的替代方法是使用关系数据库。当使用关系数据库时，研究的中间（试验）和最终结果将持久化在SQL数据库后端。RDB可以托管在单独的机器上。可以使用SQLAlchemy支持的任何SQL数据库（https://docs.sqlalchemy.org/en/20/core/engines.xhtml#database-urls）。
- en: 'In our example, we use a SQLite database as an RDB:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们使用SQLite数据库作为关系数据库（RDB）：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Optuna manages the connection to the RDB and the persistence of the results.
    After setting up the connection, optimization can proceed as usual.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Optuna管理着与关系数据库（RDB）的连接和结果的持久化。在设置连接后，优化可以像往常一样进行。
- en: 'Restoring the study from an RDB backend is straightforward; we specify the
    same `storage` and set `load_if_exists` to `True`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从RDB后端恢复研究很简单；我们指定相同的`storage`并将`load_if_exists`设置为`True`：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Understanding parameter effects
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解参数影响
- en: In many cases, it’s also valuable to better understand the effects of hyperparameters
    when solving a specific problem. For example, the `n_estimators` parameter directly
    affects the computational complexity of a model. If we know the parameter to be
    less important, we can choose smaller values to improve our model’s runtime performance.
    Optuna provides several visualizations to dive deeper into the results of a study
    and gain insight into hyperparameters.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，在解决特定问题时更好地理解超参数的影响也是很有价值的。例如，`n_estimators`参数直接影响到模型的计算复杂度。如果我们知道该参数不太重要，我们可以选择较小的值来提高我们模型的运行时性能。Optuna提供了几种可视化方法，以深入了解研究的结果并洞察超参数。
- en: 'A straightforward visualization plots the *importance of each parameter*: how
    much each affected the training outcome. We can create an importance plot as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直接的可视化方法可以绘制每个参数的*重要性*：每个参数对训练结果的影响程度。我们可以创建一个重要性图如下：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The importance plot for our study is shown here:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的参数重要性图如下所示：
- en: '![Figure 5.3 – A parameter importance plot showing the importance of each hyperparameter
    to the object values (F1 score)](img/B16690_05_03.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 一个参数重要性图，显示了每个超参数对目标值（F1分数）的重要性](img/B16690_05_03.jpg)'
- en: Figure 5.3 – A parameter importance plot showing the importance of each hyperparameter
    to the object values (F1 score)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 一个参数重要性图，显示了每个超参数对目标值（F1分数）的重要性
- en: In *Figure 5**.3*, we can see that the learning rate is by far the most critical
    parameter affecting the success of a trial. The number of leaves and estimators
    follows this. Using this information, we may decide to focus more heavily on finding
    an optimal learning rate in future studies.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.3*中，我们可以看到学习率是影响试验成功最关键的参数。叶子和估计器的数量紧随其后。利用这些信息，我们可能会决定在未来的研究中更加重视寻找最优的学习率。
- en: 'We create a parallel coordinate plot as follows, specifying the parameters
    it should contain. The plot helps us visualize the interaction between hyperparameters:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个并行坐标图如下，指定它应包含的参数。该图帮助我们可视化超参数之间的交互：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the resulting plot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的图表：
- en: '![Figure 5.4 – A parallel coordinate plot for our study. Each horizontal line
    is the configuration for a single trial. Darker lines indicate more successful
    trials (higher F1 scores)](img/B16690_05_04.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – A parallel coordinate plot for our study. Each horizontal line
    is the configuration for a single trial. Darker lines indicate more successful
    trials (higher F1 scores)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'The parallel coordinate plot shows that the best trials all used DART as the
    boosting type and have a learning rate of just below 0.1 and more than 200 estimators.
    We can also visually see some parameter interactions: GBDT models correlate with
    slightly higher learning rates. Far fewer leaf nodes are required when there is
    a large number of estimators because having many estimators and large numbers
    of leaf nodes leads to overfitting.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Multi-objective optimization
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the optimization studies shown previously, we focused on a single optimization
    objective: maximizing our F1 score. However, in some instances, we would like
    to optimize two potentially competing objectives. For example, say we want to
    create the smallest GBDTs possible (fewest leaves) while obtaining a good F1 score.
    Reducing the number of leaves can potentially negatively impact our performance,
    so a trade-off exists.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Optuna supports solving this type of problem by using `objective` function and
    specify the optimization directions.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the trade-off between the learning rate and performance.
    We want to train our model as fast as possible, which requires a high learning
    rate. However, we know the best performance is achieved using a small learning
    rate and many iterations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Optuna to optimize this trade-off. We define a new `objective` function,
    fixing all other parameters to the optimal values found earlier. We return two
    evaluations: the learning and the cross-validated F1-score. We want to maximize
    both values:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'When calling `optimize`, we then set the direction for the optimization of
    both evaluations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When performing MOO, there isn’t always a single best result: a trade-off often
    exists between the objectives. Therefore, we want to visualize the study results
    to explore the trade-off and select parameter values that perform well with both
    objectives. This type of visualization is called a **Pareto front** and can be
    created as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – A scatter plot showing the Pareto front for a MOO study](img/B16690_05_05.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – A scatter plot showing the Pareto front for a MOO study
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 5**.5*, the F1 score is poor if the learning rate is too
    low and picks up quickly as the learning rate gets to 0.01\. The F1 score peaks
    at 0.12 and slowly trails off as the learning rate increases. We now have the
    necessary information to decide on our trade-off: we can choose a higher learning
    rate for faster training, sacrificing the minimum amount of classification performance.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced Optuna as a framework for HPO. We discussed the problems
    of finding optimal hyperparameters and how HPO algorithms may be used to find
    suitable parameters efficiently.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 Optuna 作为 HPO 的框架。我们讨论了寻找最佳超参数的问题以及如何使用 HPO 算法高效地找到合适的参数。
- en: 'We discussed two optimization algorithms available in Optuna: TPE and CMA-ES.
    Both algorithms allow a user to set a specific budget for optimization (the number
    of trials to perform) and proceed to find suitable parameters within the constraints.
    Further, we discussed the pruning of unpromising optimization trials to save additional
    resources and time. Median pruning and the more complex but effective pruning
    techniques of successive halving and Hyperband were discussed.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 Optuna 中可用的两种优化算法：TPE 和 CMA-ES。这两种算法都允许用户为优化设置一个特定的预算（要执行的试验次数）并在约束条件下寻找合适的参数。此外，我们还讨论了剪枝无望的优化试验以节省额外资源和时间。讨论了中值剪枝以及更复杂但有效的连续减半和
    Hyperband 剪枝技术。
- en: We then proceeded to show how to perform HPO studies for LightGBM in a practical
    example. We also showed advanced features of Optuna that can be used to save and
    resume studies, understand the effects of parameters, and perform MOO.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们继续展示如何在实际示例中执行 LightGBM 的 HPO 研究。我们还展示了 Optuna 的高级功能，这些功能可用于保存和恢复研究，了解参数的影响，并执行
    MOO。
- en: The next chapter focuses on two case studies using LightGBM, where the data
    science process is discussed and applied in detail.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章重点介绍使用 LightGBM 的两个案例研究，其中详细讨论并应用了数据科学流程。
- en: References
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, “Algorithms
    for Hyper-Parameter Optimization,” in Advances in Neural Information Processing*
    *Systems, 2011.* |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| *[**1]* | *J. Bergstra, R. Bardenet, Y. Bengio 和 B. Kégl, “超参数优化的算法，”载于神经信息处理系统进展，2011年。*
    |'
- en: '| *[**2]* | *J. Bergstra, D. Yamins, and D. Cox, “Making a Science of Model
    Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,”
    in Proceedings of the 30th International Conference on Machine Learning,* *Atlanta,
    2013.* |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| *[**2]* | *J. Bergstra, D. Yamins 和 D. Cox, “使模型搜索成为一门科学：视觉架构在数百维度的超参数优化，”载于第30届国际机器学习会议论文集，亚特兰大，2013年。*
    |'
- en: '| *[**3]* | *N. Hansen and A. Ostermeier, “Adapting arbitrary normal mutation
    distributions in evolution strategies: the covariance matrix adaptation,” in Proceedings
    of IEEE International Conference on Evolutionary* *Computation, 1996.* |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| *[**3]* | *N. Hansen 和 A. Ostermeier, “在进化策略中调整任意正态变异分布：协方差矩阵调整，”载于 IEEE
    国际进化计算会议论文集，1996年。* |'
- en: '| *[**4]* | *K. Jamieson and A. Talwalkar, Non-stochastic Best Arm Identification
    and Hyperparameter* *Optimization, 2015.* |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| *[**4]* | *K. Jamieson 和 A. Talwalkar, 非随机最佳臂识别和超参数优化，2015年。* |'
- en: '| *[**5]* | *L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar,
    Hyperband: A Novel Bandit-Based Approach to Hyperparameter* *Optimization, 2018.*
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| *[**5]* | *L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh 和 A. Talwalkar,
    Hyperband：一种基于Bandit的新的超参数优化方法，2018年。* |'
