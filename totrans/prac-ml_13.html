<html><head></head><body><div class="chapter" title="Chapter&#xA0;13.&#xA0;Ensemble learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch13"/>Chapter 13. Ensemble learning</h1></div></div></div><p>This chapter is the concluding chapter of all the learning methods we have learned from <a class="link" href="ch05.html" title="Chapter 5. Decision Tree based learning">Chapter 5</a>, <span class="emphasis"><em>Decision Tree based learning</em></span>. It is only apt to have this chapter as a closing chapter for the learning methods, as this learning method explains how effectively these methods can be used in a combination to maximize the outcome from the learners. Ensemble methods have an effective, powerful technique to achieve high accuracy across supervised and unsupervised solutions. Different models are efficient and perform very well in the selected business cases. It is important to find a way to combine the competing models into a committee, and there has been much research in this area with a fair degree of success. Also, as different views generate a large amount of data, the key aspect is to consolidate different concepts for intelligent decision making. Recommendation systems and stream-based text mining applications use ensemble methods extensively.</p><p>There have been many independent studies done in supervised and unsupervised learning groups. The common theme observed was that many mixed models, when brought together, strengthened the weak models and brought in an overall better performance. One of the important goals of this chapter details a systematic comparison of different ensemble techniques that combine both supervised and unsupervised techniques and a mechanism to merge the results.</p><div class="mediaobject"><img src="graphics/B03980_13_01.jpg" alt="Ensemble learning"/></div><p>This chapter covers the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An overview of ensemble methods based learning—the concept of <span class="emphasis"><em>the wisdom of the crowd</em></span> and the key attributes.</li><li class="listitem" style="list-style-type: disc">Core ensemble method taxonomy, real-world examples, and applications of ensemble learning</li><li class="listitem" style="list-style-type: disc">Ensemble method categories and various representative methods:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Supervised ensemble methods</strong></span> <a id="id1387" class="indexterm"/>provide an overview and a detailed coverage of concepts such as bagging, boosting, gradient boosting methods, and Random decision trees and Random forests</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Unsupervised ensemble methods</strong></span><a id="id1388" class="indexterm"/> provide an overview of generative, direct and indirect methods that include clustering ensembles</li></ul></div></li><li class="listitem" style="list-style-type: disc">Hands-on implementation exercises using Apache Mahout, R, Julia, Python (scikit-learn), and Apache Spark</li></ul></div><div class="section" title="Ensemble learning methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec59"/>Ensemble learning methods</h1></div></div></div><p>Ensemble, in general, means<a id="id1389" class="indexterm"/> a group of things that are usually seen as a whole rather than in terms of the value as against the individual value. Ensembles follow a divide-and-conquer approach used to improve performance.</p><div class="mediaobject"><img src="graphics/B03980_13_02.jpg" alt="Ensemble learning methods"/></div><p>We will start understanding the specific algorithm with an introduction to the famous concept of the wisdom of the crowd.</p><div class="section" title="The wisdom of the crowd"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec149"/>The wisdom of the crowd</h2></div></div></div><p>Imperfect<a id="id1390" class="indexterm"/> judgments when aggregated in a right way result in a <a id="id1391" class="indexterm"/>collective intelligence, thus resulting in a superior outcome. The wisdom of the crowd is all about this collective intelligence.</p><p>In general, the term crowd is usually associated with irrationality and the common perception that there is some influence, which sways the behavior of the crowd in the context of mobs and cults. However, the fact is that this need not always be negative and works well when working with collating intellect. The key concept of Wisdom of Crowds is that the decisions made by a group of people are always robust and accurate than those made by individuals. The ensemble learning methods of Machine learning have exploited this idea effectively to produce efficiency and accuracy in their results.</p><p>The term the wisdom of the crowd was coined by Galton in 1906. Once he attended a farmer's fair where there was a contest to guess the weight of an ox that is butchered and dressed. The closest guess won the prize from a total of 800 contestants. He chose to collect all the responses and analyze them. When he took the average of the guesses, he was shocked to notice that they were very close to the actual value. This collective guess was both better than the contestant who won the prize and also proved to be the best in comparison to the guesses by cattle experts. The democracy of thoughts was a clear winner. For such a useful output, it is important that each contestant had his/her strong source of information. The independent guess provided by the contestant should not be influenced by his/her neighbor's guess, and also, there is an error-free mechanism to consolidate the guesses across the group. So in short, this is not an easy process. Another important aspect is also to the fact these guesses were superior to any individual expert's guess.</p><p>Some basic everyday<a id="id1392" class="indexterm"/> examples include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Google search results that usually have the most popular pages listed at the top</li><li class="listitem" style="list-style-type: disc">In a game like "Who wants to be a billionaire", the audience poll is used for the answering questions that the contestant has no knowledge about. Usually, the answer that is most voted by the crowd is the right answer.</li></ul></div><p>The results of the <a id="id1393" class="indexterm"/>wisdom of the crowd approach is not guaranteed. Following is the basic criteria for an optimal result using this approach:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Aggregation</strong></span>: There<a id="id1394" class="indexterm"/> needs to be a foolproof way of consolidating individual responses into a collective response or judgment. Without this, the core purpose of collective views or responses across the board goes in vain.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Independence</strong></span>: Within the <a id="id1395" class="indexterm"/>crowd, there needs to be discipline around controlling the response from one entity over the rest in the crowd. Any influence would skew the response, thus impacting the accuracy.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Decentralization</strong></span>: <a id="id1396" class="indexterm"/>Individual responses have their source and thrive on the limited knowledge.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The diversity of opinion</strong></span>: It is<a id="id1397" class="indexterm"/> important that each person has a response that is isolated; the response's unusualness is still acceptable.</li></ul></div><p>The word ensemble means grouping. To build ensemble classifiers, we first need to build a set of classifiers from the training data, aggregate the predictions made by these classifiers, and predict a class label of a new record using this data.</p><p>The following diagram depicts this process:</p><div class="mediaobject"><img src="graphics/B03980_13_03.jpg" alt="The wisdom of the crowd"/></div><p>Technically, the<a id="id1398" class="indexterm"/> core building blocks include a training set, an inducer, and an ensemble generator. Inducer handles defining classifiers for each of the sample training datasets. The ensemble generator creates classifiers and a combiner or aggregator that consolidates the responses across the combiners. With these building blocks and the relationships between them, we have the following properties that we will be using to categorize the ensemble methods. The next section covers these methods:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Usage of a combiner</strong></span>: This<a id="id1399" class="indexterm"/> property defines the relationship between the ensemble generator and the combiner</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dependency between classifiers</strong></span>: This <a id="id1400" class="indexterm"/>property defines the degree to which the classifiers are dependent on each other</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Generating diversity</strong></span>: This <a id="id1401" class="indexterm"/>property defines the procedure used to ensure diversity across combiners</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The size of the ensemble</strong></span>: This<a id="id1402" class="indexterm"/> property denotes the number of classifiers used in the ensembles</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cross inducers</strong></span>: This <a id="id1403" class="indexterm"/>property defines how the classifiers leverage the inducers. There are cases where the classifiers are built to work with a certain set of inducers</li></ul></div><p>In summary, the <a id="id1404" class="indexterm"/>building model ensembles first involves building experts and letting them provide a response/vote. The expected benefit is an improvement in prediction performance and produces a single global structure. Although, any interim results produced might end up being difficult to analyze.</p><p>Let's look at how the performance of an aggregated/combined classifier works better in a comprehensive manner.</p><p>Let's consider three classifiers that have an error rate of 0.35(ԑ) or an accuracy of 0.65. For each of the classifiers, the probability that the classifier goes wrong with its prediction is 35%.</p><div class="mediaobject"><img src="graphics/B03980_13_04.jpg" alt="The wisdom of the crowd"/></div><p>Given here is the truth table denoting the error rate of 0.35(35%) and the accuracy rate of 0.65(65%):</p><div class="mediaobject"><img src="graphics/B03980_13_05.jpg" alt="The wisdom of the crowd"/></div><p>After the three classifiers are combined, the class label of a test instance is predicted by using the majority vote process across the combiners to compute the probability that the ensemble classifier makes an error. This is depicted in the formula given below.</p><div class="mediaobject"><img src="graphics/B03980_13_06.jpg" alt="The wisdom of the crowd"/></div><p>Moreover, the<a id="id1405" class="indexterm"/> accuracy is 71.83%. Very clearly, the error rate is lowered when aggregated across the classifiers. Now, if we extend this to 25 classifiers, the accuracy goes up to 94% as per the computation of the error rate (6%).</p><div class="mediaobject"><img src="graphics/B03980_13_07.jpg" alt="The wisdom of the crowd"/></div><p>Thus, the ensembles work as they give the bigger picture.</p><p>We have covered the criteria for the the wisdom of the crowd to work in the previous section. Let's now take the preceding case where we have 25 base classifiers, and see how the accuracy of the ensemble classifier changes for different error rates of the base classifier.</p><div class="mediaobject"><img src="graphics/B03980_13_08.jpg" alt="The wisdom of the crowd"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip08"/>Tip</h3><p>The ensemble classifier's performance deteriorates and performs much worse than the base classifier in cases where the base classifier error rate is more than 0.5.</p></div></div><p>In the next section, we <a id="id1406" class="indexterm"/>will cover some real-world use cases that apply ensemble methods.</p></div><div class="section" title="Key use cases"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec150"/>Key use cases</h2></div></div></div><p>Some of the key <a id="id1407" class="indexterm"/>real-world applications of ensemble learning methods <a id="id1408" class="indexterm"/>are detailed and discussed in this section.</p><div class="section" title="Recommendation systems"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec122"/>Recommendation systems</h3></div></div></div><p>The purpose of <a id="id1409" class="indexterm"/>recommendation <a id="id1410" class="indexterm"/>systems is to produce significant or meaningful recommendations to a community of users around the products that would possibly interest them. Some examples include suggestions related to decision-making process such as what books to read on Amazon, what movies to watch on Netflix, or what news to read on a news website. The business domain, or the context and the characteristics of the business attributes are the primary inputs to the design of recommendation systems. The rating (on a scale of 1-5) that users provide for each of the films is a significant input as it records the degree to which users interact with the system. In addition to this, the details of the user (such as demographics and other personal or profile attributes) are also used by the recommender systems to identify a potential match between items and potential users.</p><p>The following <a id="id1411" class="indexterm"/>screenshot is an <a id="id1412" class="indexterm"/>example of a recommendation system result on Netflix:</p><div class="mediaobject"><img src="graphics/B03980_13_09.jpg" alt="Recommendation systems"/></div></div><div class="section" title="Anomaly detection"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec123"/>Anomaly detection</h3></div></div></div><p>Anomaly detection <a id="id1413" class="indexterm"/>or outlier detection<a id="id1414" class="indexterm"/> is one of the most popular use cases or applications of ensemble learning methods. It is all about finding patterns in the data that look abnormal or unusual. Identifying anomalies is important as it may result in taking any decisive action. Some famous examples include (among many others):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fraud detection with credit cards</li><li class="listitem" style="list-style-type: disc">Unusual disease detection in healthcare</li><li class="listitem" style="list-style-type: disc">Anomaly detection in aircraft engines</li></ul></div><p>Let's now expand on the example for anomaly detection in aircraft engines. Features of a plane engine which are considered to verify if it is anomalous or not are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Heat generated(<span class="emphasis"><em>x</em></span><sub>1</sub>)</li><li class="listitem" style="list-style-type: disc">The intensity of vibration (<span class="emphasis"><em>x</em></span><sub>2</sub>)</li><li class="listitem" style="list-style-type: disc">With the <a id="id1415" class="indexterm"/>dataset = <span class="emphasis"><em>x</em></span><sub>(1)</sub><span class="emphasis"><em>, x</em></span><sub>(2)</sub><span class="emphasis"><em> … x</em></span><sub>(m)</sub> marked, following are the anomalous and non-anomalous<a id="id1416" class="indexterm"/> cases:<div class="mediaobject"><img src="graphics/B03980_13_10.jpg" alt="Anomaly detection"/></div></li></ul></div></div><div class="section" title="Transfer learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec124"/>Transfer learning</h3></div></div></div><p>Traditionally, all<a id="id1417" class="indexterm"/> the Machine learning algorithms assume <a id="id1418" class="indexterm"/>learning to happen from scratch for every new learning problem. The assumption is that no previous learning will be leveraged. In cases where the domains for the learning problems relate, there will be some learnings from the past that can be acquired and used. Some common examples include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The knowledge of French could help students learn Spanish</li><li class="listitem" style="list-style-type: disc">The knowledge of mathematics could help students learn Physics</li><li class="listitem" style="list-style-type: disc">The knowledge of driving a car could help drivers learn to drive a truck</li></ul></div><p>In the<a id="id1419" class="indexterm"/> Machine learning context, this refers to identifying<a id="id1420" class="indexterm"/> and applying the knowledge accumulated from previous tasks to new tasks from a related domain. The key here is the ability to identify the commonality between the domains. Reinforcement learning and classification and regression problems apply transfer learning. The transfer learning process flow is as shown here:</p><div class="mediaobject"><img src="graphics/B03980_13_11.jpg" alt="Transfer learning"/></div></div><div class="section" title="Stream mining or classification"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec125"/>Stream mining or classification</h3></div></div></div><p>Mining <a id="id1421" class="indexterm"/>data that comes in as streams<a id="id1422" class="indexterm"/> has now become a key requirement <a id="id1423" class="indexterm"/>for a wide range of applications with the growing technological advancements and social media.</p><p>The primary difference between the traditional learning is that the training and test data sets evolved in a distributed way as they come in streams. The goal of prediction now becomes a bit complex as the probabilities keep changing for changing timestamps, thus making this an ideal context for applying ensemble learning methods. The next diagram shows how <span class="emphasis"><em>P(y)</em></span> changes with timestamp and changes to <span class="emphasis"><em>P(x)</em></span> and <span class="emphasis"><em>P (y|x)</em></span>:</p><div class="mediaobject"><img src="graphics/B03980_13_12.jpg" alt="Stream mining or classification"/></div><p>With ensemble<a id="id1424" class="indexterm"/> learning methods, the <a id="id1425" class="indexterm"/>variance produced by single<a id="id1426" class="indexterm"/> models is reduced, and the prediction or result is more accurate or robust as the distribution is evolving.</p></div></div><div class="section" title="Ensemble methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec151"/>Ensemble methods</h2></div></div></div><p>As discussed in the<a id="id1427" class="indexterm"/> previous sections, ensemble methods are now proven to be powerful methods for improving the accuracy and robustness of the supervised, semi-supervised, and unsupervised solutions. Also, we have seen how the dynamics of decision-making are becoming complex as different sources have started generating enormous amounts of data continuously. Effective consolidation is now critical for successful and intelligent decision making.</p><p>The supervised and unsupervised ensemble methods share the same principles that involve combining the diverse base models that strengthen weak models. In the sections to follow, let's look at supervised, semi-supervised, and unsupervised techniques independently and in detail.</p><p>The following model depicts various learning categories and different algorithms that cover both, combining the learning and consensus approaches to the ensemble learning:</p><div class="mediaobject"><img src="graphics/B03980_13_13.jpg" alt="Ensemble methods"/><div class="caption"><p>Source: On the Power of Ensemble: Supervised and Unsupervised Methods Reconciled (<a class="ulink" href="http://it.engineering.illinois.edu/ews/">http://it.engineering.illinois.edu/ews/</a>)</p></div></div><p>Before we go deeper <a id="id1428" class="indexterm"/>into each of the ensemble techniques, let's understand the difference between combining by learning versus combining by consensus:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom"> </th><th style="text-align: left" valign="bottom">
<p>Benefits</p>
</th><th style="text-align: left" valign="bottom">
<p>Downside</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Combining by learning</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Uses labeled data as a feedback mechanism</li><li class="listitem" style="list-style-type: disc">Has the potential to improve accuracy</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Works only with labeled data</li><li class="listitem" style="list-style-type: disc">There are chances of over-fitting</li></ul></div>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Combining by consensus</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Does not require labeled data</li><li class="listitem" style="list-style-type: disc">Has the potential to improve performance</li></ul></div>
</td><td style="text-align: left" valign="top">
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The valued feedback from the labeled data is missing</li><li class="listitem" style="list-style-type: disc">Works on the assumption that consensus is a good thing</li></ul></div>
</td></tr></tbody></table></div><div class="section" title="Supervised ensemble methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec126"/>Supervised ensemble methods</h3></div></div></div><p>In the case of<a id="id1429" class="indexterm"/> supervised learning methods, the <a id="id1430" class="indexterm"/>input is always a labeled data. The <span class="emphasis"><em>combining by learning</em></span> method includes boosting stack generalization and the rule ensembles techniques. The <span class="emphasis"><em>combining by consensus</em></span> methods includes bagging, Random forests, and Random decision trees techniques. The following shows the process flow for combining by learning followed by another model for combining by consensus:</p><div class="mediaobject"><img src="graphics/B03980_13_14.jpg" alt="Supervised ensemble methods"/></div><div class="mediaobject"><img src="graphics/B03980_13_15.jpg" alt="Supervised ensemble methods"/></div><p>The supervised<a id="id1431" class="indexterm"/> ensemble method problem statement is defined as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The input data set is <span class="emphasis"><em>D={x</em></span><sub>1</sub><span class="emphasis"><em>, x</em></span><sub>2</sub><span class="emphasis"><em>, …, x</em></span><sub>n</sub><span class="emphasis"><em>}</em></span> the respective labels are <span class="emphasis"><em>L={l</em></span><sub>1</sub><span class="emphasis"><em>,l</em></span><sub>2</sub><span class="emphasis"><em>,…,l</em></span><sub>n</sub><span class="emphasis"><em>}</em></span></li><li class="listitem" style="list-style-type: disc">The ensemble method now generated a set of classifiers <span class="emphasis"><em>C = {f</em></span><sub>1</sub><span class="emphasis"><em>,f</em></span><sub>2</sub><span class="emphasis"><em>,…,f</em></span><sub>k</sub><span class="emphasis"><em>}</em></span></li><li class="listitem" style="list-style-type: disc">Finally, the combination of classifiers <span class="emphasis"><em>f*</em></span> minimizes generalization error as per the <span class="emphasis"><em>f*(x)= ω</em></span><sub>1</sub><span class="emphasis"><em>f</em></span><sub>1</sub><span class="emphasis"><em>(x)+ ω</em></span><sub>2</sub><span class="emphasis"><em>f</em></span><sub>2</sub><span class="emphasis"><em>(x)+ ….+ ω</em></span><sub>k</sub><span class="emphasis"><em>f</em></span><sub>k</sub><span class="emphasis"><em>(x)</em></span> formula</li></ul></div><div class="section" title="Boosting"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl4sec52"/>Boosting</h4></div></div></div><p>Boosting<a id="id1432" class="indexterm"/> is a <a id="id1433" class="indexterm"/>pretty straightforward approach to calculating the output by applying a weighted average of all the outputs generated by multiple models. It is a framework for weak learners. The weights applied can be varied by using a powerful weighting formula to come up with a strong predictive model that addresses the pitfalls of these approaches and also works for a wider range of input data, using different narrowly tuned models.</p><p>Boosting has been successful in solving binary classification problems. This technique was introduced by Freund and Scaphire in the 1990s via the famous AdaBoost algorithm. Here listed are some key <a id="id1434" class="indexterm"/>characteristics of this framework:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It combines several base <a id="id1435" class="indexterm"/>classifiers that demonstrate improved performance in comparison to the base classifier</li><li class="listitem" style="list-style-type: disc">The weak learners are sequentially trained</li><li class="listitem" style="list-style-type: disc">The data used for training each of the base classifiers is based on the performance of the previous classifier</li><li class="listitem" style="list-style-type: disc">Every classifier votes and contributes to the outcome</li><li class="listitem" style="list-style-type: disc">This framework works and uses the online algorithm strategy</li><li class="listitem" style="list-style-type: disc">For every iteration, the weights are recomputed or redistributed where the incorrect classifiers will start to have their weights reduced</li><li class="listitem" style="list-style-type: disc">Correct classifiers receive more weight while incorrect classifiers have reduced weight</li><li class="listitem" style="list-style-type: disc">Boosting methods, though were originally designed for solving classification problems, are extended to handle regression as well</li></ul></div><p>Boosting algorithm is stated next:</p><div class="mediaobject"><img src="graphics/B03980_13_16.jpg" alt="Boosting"/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Train a set of weak hypotheses: <span class="emphasis"><em>h</em></span><sub>1</sub><span class="emphasis"><em>, …, h</em></span><sub>T</sub><span class="emphasis"><em>.</em></span></li><li class="listitem">Combine the hypothesis <span class="emphasis"><em>H</em></span> as a weighted and majority vote of <span class="emphasis"><em>T</em></span> weaker hypotheses.<div class="mediaobject"><img src="graphics/B03980_13_17.jpg" alt="Boosting"/></div></li><li class="listitem">Every iteration focuses <a id="id1436" class="indexterm"/>on the misclassifications and recomputes the weights <span class="emphasis"><em>D</em></span><sub>t</sub><span class="emphasis"><em>(i)</em></span>.</li></ol></div><div class="section" title="AdaBoost"><div class="titlepage"><div><div><h5 class="title"><a id="ch13lvl5sec14"/>AdaBoost</h5></div></div></div><p>AdaBoost <a id="id1437" class="indexterm"/>is a linear classifier and constructs a stronger classifier <span class="emphasis"><em>H(x)</em></span> as a linear combination of weaker functions <span class="emphasis"><em>h</em></span><sub>t</sub><span class="emphasis"><em>(x)</em></span>.</p><div class="mediaobject"><img src="graphics/B03980_13_18.jpg" alt="AdaBoost"/></div><p>The pictorial representation next demonstrates how the boosting framework works:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">All the data points are labeled into two classes <span class="emphasis"><em>+1</em></span> and <span class="emphasis"><em>-1</em></span> with equal weights—<span class="emphasis"><em>1</em></span>.<div class="mediaobject"><img src="graphics/B03980_13_19.jpg" alt="AdaBoost"/></div></li><li class="listitem">Apply a <span class="emphasis"><em>p (error)</em></span> and classify the data points as shown here:<div class="mediaobject"><img src="graphics/B03980_13_20.jpg" alt="AdaBoost"/></div></li><li class="listitem">Recompute the weights.<div class="mediaobject"><img src="graphics/B03980_13_21.jpg" alt="AdaBoost"/></div></li><li class="listitem">Have the weak <a id="id1438" class="indexterm"/>classifiers participate again with a new problem set.<div class="mediaobject"><img src="graphics/B03980_13_22.jpg" alt="AdaBoost"/></div><div class="mediaobject"><img src="graphics/B03980_13_23.jpg" alt="AdaBoost"/></div></li><li class="listitem">A strong <a id="id1439" class="indexterm"/>non-linear classifier is built using the weak classifiers iteratively.<div class="mediaobject"><img src="graphics/B03980_13_24.jpg" alt="AdaBoost"/></div></li></ol></div></div></div><div class="section" title="Bagging"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl4sec53"/>Bagging</h4></div></div></div><p>Bagging<a id="id1440" class="indexterm"/> is also<a id="id1441" class="indexterm"/> called <a id="id1442" class="indexterm"/>
<span class="strong"><strong>Bootstrap Aggregation</strong></span>. This technique of ensemble learning combines the <span class="emphasis"><em>consensus</em></span> approach. There are three important steps in this technique:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Build the bootstrap sample that contains approximately 63.2% of the original records.</li><li class="listitem">Classify training using each bootstrap sample.</li><li class="listitem">Use the majority voting and identify the class label of the ensemble classifier.</li></ol></div><p>This process decreases the prediction variance by generating additional data generation, based on the original dataset by combining the datasets of the same size repetitively. The accuracy of the model increases with a decrease in variance and not by increasing dataset size. Following is the bagging algorithm:</p><div class="mediaobject"><img src="graphics/B03980_13_25.jpg" alt="Bagging"/></div><p>As per the previous<a id="id1443" class="indexterm"/> algorithm steps, an example flow of the bagging algorithm and the process is depicted here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Training step</strong></span>: For<a id="id1444" class="indexterm"/> each iteration <span class="emphasis"><em>t, t=1,…T</em></span>, create <span class="emphasis"><em>N</em></span> samples from the training sets (this process is called bootstrapping), select a base model (for example, decision tree, neural networks, and so on), and train it using the samples built.</li><li class="listitem"><span class="strong"><strong>Testing step</strong></span>: For<a id="id1445" class="indexterm"/> every test cycle, predict by combining the results of all the <span class="emphasis"><em>T</em></span> trained models. In the case of a classification problem, the majority of the voting approach is applied and for regression, it is the averaging approach.</li></ol></div><p>Some of the error computations are done as follows:</p><div class="mediaobject"><img src="graphics/B03980_13_26.jpg" alt="Bagging"/></div><p>Bagging works <a id="id1446" class="indexterm"/>both in the over-fitting and under-fitting cases under the following conditions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>For under-fitting</strong></span>: High<a id="id1447" class="indexterm"/> bias and low variance case</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>For over-fitting</strong></span>: Small <a id="id1448" class="indexterm"/>bias and large variance case</li></ul></div><p>Here is an <a id="id1449" class="indexterm"/>example of Bagging:</p><div class="mediaobject"><img src="graphics/B03980_13_27.jpg" alt="Bagging"/></div></div><div class="section" title="Wagging"><div class="titlepage"><div><div><h4 class="title"><a id="ch13lvl4sec54"/>Wagging</h4></div></div></div><p>
<span class="strong"><strong>Wagging</strong></span>
<a id="id1450" class="indexterm"/> is another <a id="id1451" class="indexterm"/>variation of bagging. The entire dataset is used to train each model. Also, weights are assigned stochastically. So in short, wagging is bagging with additional weights that are assignment-based on the Poisson or exponential distribution. Following is the Wagging algorithm:</p><div class="mediaobject"><img src="graphics/B03980_13_28.jpg" alt="Wagging"/></div><div class="section" title="Random forests"><div class="titlepage"><div><div><h5 class="title"><a id="ch13lvl5sec15"/>Random forests</h5></div></div></div><p>Radom forests is another<a id="id1452" class="indexterm"/> ensemble learning method that combines multiple Decision trees. The following diagram represents the Random forest ensemble:</p><div class="mediaobject"><img src="graphics/B03980_13_29.jpg" alt="Random forests"/><div class="caption"><p>Source: <a class="ulink" href="https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/">https://citizennet.com/blog/2012/11/10/random-forests-ensembles-and-performance-metrics/</a>
</p></div></div><p>For a Random forest<a id="id1453" class="indexterm"/> with T-trees, training of the decision tree classifiers is done as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Similar to the standard Bagging technique, a sample of <span class="emphasis"><em>N</em></span> cases is defined with a random replacement to create a subset of the data that is about 62-66% of the comprehensive set.</li><li class="listitem" style="list-style-type: disc">For each node, do as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Select the <span class="emphasis"><em>m</em></span> predictor variables in such a way that the identified variable gives the best split (binary split)</li><li class="listitem" style="list-style-type: disc">At the next node, choose other <span class="emphasis"><em>m</em></span> variables that do the same</li></ul></div></li><li class="listitem" style="list-style-type: disc">The value of the <span class="emphasis"><em>m</em></span> can vary<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For Random splitter selection—<span class="emphasis"><em>m=1</em></span></li><li class="listitem" style="list-style-type: disc">For Breiman's bagger: <span class="emphasis"><em>m= total number of predictor variables</em></span></li><li class="listitem" style="list-style-type: disc">For Random forest, <span class="emphasis"><em>m</em></span> is less than the number of predictor variables, and it can take three values: <span class="emphasis"><em>½√m</em></span>, <span class="emphasis"><em>√m</em></span>, and <span class="emphasis"><em>2√m</em></span></li></ul></div></li></ul></div><p>Now, for every<a id="id1454" class="indexterm"/> new input to the Random forest for prediction, the new value is run down through all the trees and an average, weighted average, or a voting majority is used to get the predicted value.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip09"/>Tip</h3><p>In <a class="link" href="ch05.html" title="Chapter 5. Decision Tree based learning">Chapter 5</a>, <span class="emphasis"><em>Decision Tree based learning</em></span>, we have covered Random forests in detail.</p></div></div></div><div class="section" title="Gradient boosting machines (GBM)"><div class="titlepage"><div><div><h5 class="title"><a id="ch13lvl5sec16"/>Gradient boosting machines (GBM)</h5></div></div></div><p>GBMs are one of the highly <a id="id1455" class="indexterm"/>adopted Machine learning algorithms. They are used to address classification and regression problems. The basis for GBMs is Decision trees, and they apply boosting technique where multiple weak algorithms are combined algorithmically to produce a strong learner. They are stochastic and gradient boosted, which means that they iteratively solve residuals.</p><p>They are known to be highly customizable as they can use a variety of loss functions. We have seen that the Random forests ensemble technique uses simple averaging against GBMs, which uses a practical strategy of the ensemble formation. In this strategy, new models are iteratively added to the ensemble where every iteration trains the weak modeler to identify the next steps.</p><p>GBMs are flexible and are relatively more efficient than any other ensemble learning methods. The following table details the GBM algorithm:</p><div class="mediaobject"><img src="graphics/B03980_13_30.jpg" alt="Gradient boosting machines (GBM)"/></div><p>
<span class="strong"><strong>Gradient boosted regression trees</strong></span> (<span class="strong"><strong>GBRT</strong></span>) <a id="id1456" class="indexterm"/>are similar to GBMs that follow regression techniques.</p></div></div></div><div class="section" title="Unsupervised ensemble methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch13lvl3sec127"/>Unsupervised ensemble methods</h3></div></div></div><p>As a part of unsupervised ensemble <a id="id1457" class="indexterm"/>learning methods, one of the consensus-based ensembles is the clustering ensemble. The next diagram depicts the working of the clustering-based ensemble:</p><div class="mediaobject"><img src="graphics/B03980_13_31.jpg" alt="Unsupervised ensemble methods"/></div><p>For a given <a id="id1458" class="indexterm"/>unlabeled dataset <span class="emphasis"><em>D={x</em></span><sub>1</sub><span class="emphasis"><em>,x</em></span><sub>2</sub><span class="emphasis"><em>,…,x</em></span><sub>n</sub><span class="emphasis"><em>}</em></span>, a clustering ensemble computes a set of clusters <span class="emphasis"><em>C = { C</em></span><sub>1</sub><span class="emphasis"><em>,C</em></span><sub>2</sub><span class="emphasis"><em>,…,C</em></span><sub>k</sub><span class="emphasis"><em>}</em></span>, each of which maps the data to a cluster. A consensus-based unified cluster is formed. The following diagram depicts this flow:</p><div class="mediaobject"><img src="graphics/B03980_13_32.jpg" alt="Unsupervised ensemble methods"/></div></div></div></div></div>
<div class="section" title="Implementing ensemble methods"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec60"/>Implementing ensemble methods</h1></div></div></div><p>Refer to the source code <a id="id1459" class="indexterm"/>provided for this chapter to implement the ensemble learning methods (only supervised learning techniques). (source code path <code class="literal">.../chapter13/...</code> under each of the folders for the technology).</p><div class="section" title="Using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec152"/>Using Mahout</h2></div></div></div><p>Refer <a id="id1460" class="indexterm"/>to the<a id="id1461" class="indexterm"/> folder <code class="literal">.../mahout/chapter13/ensembleexample/</code>.</p></div><div class="section" title="Using R"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec153"/>Using R</h2></div></div></div><p>Refer<a id="id1462" class="indexterm"/> to<a id="id1463" class="indexterm"/> the folder <code class="literal">.../r/chapter13/ensembleexample/</code>.</p></div><div class="section" title="Using Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec154"/>Using Spark</h2></div></div></div><p>Refer <a id="id1464" class="indexterm"/>to<a id="id1465" class="indexterm"/> the folder <code class="literal">.../spark/chapter13/ensembleexample/</code>.</p></div><div class="section" title="Using Python (Scikit-learn)"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec155"/>Using Python (Scikit-learn)</h2></div></div></div><p>Refer to <a id="id1466" class="indexterm"/>the <a id="id1467" class="indexterm"/>folder <code class="literal">.../python (scikit-learn)/chapter13/ensembleexample/</code>.</p></div><div class="section" title="Using Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch13lvl2sec156"/>Using Julia</h2></div></div></div><p>Refer<a id="id1468" class="indexterm"/> to the <a id="id1469" class="indexterm"/>folder <code class="literal">.../julia/chapter13/ensembleexample/</code>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch13lvl1sec61"/>Summary</h1></div></div></div><p>In this chapter, we have covered the ensemble learning methods of Machine learning. We covered the concept of <span class="emphasis"><em>the wisdom of the crowd</em></span>, how and when it is applied in the context of Machine learning, and how the accuracy and performance of the learners are improved. Specifically, we looked at some supervised ensemble learning techniques with some real-world examples. Finally, this chapter has source code examples for the gradient boosting algorithm using R, Python (scikit-learn), Julia, and Spark Machine learning tools and recommendation engines using the Mahout libraries.</p><p>This chapter covers all the Machine learning methods and in the last chapter that follows, we will cover some advanced and upcoming architecture and technology strategies for Machine learning.</p></div></body></html>