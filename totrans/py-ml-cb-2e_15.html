<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Automated Machine Learning and Transfer Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this chapter, we will cover the following</span> recipes:</p>
<ul>
<li>Working with Auto-WEKA</li>
<li>Using AutoML to generate machine learning pipelines with TPOT</li>
<li>Working with Auto-Keras</li>
<li>Working with auto-sklearn</li>
<li>Using MLBox for selection and leak detection</li>
<li>C<span>onvolutional neural networks with </span>transfer learning</li>
<li><span>Transfer learning – p</span>retrained image classifiers with ResNet-50</li>
<li>Transfer learning <span>–</span> feature extraction with the VGG16 model</li>
<li>Transfer learning with retrained GloVe embedding</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>To address the recipes in this chapter, you will need the following files (available on GitHub):</p>
<ul>
<li><kbd><span>TPOTIrisClassifier.py</span></kbd></li>
<li><kbd><span>AKClassifier</span><span>.py</span></kbd></li>
<li><kbd><span>MLBoxRegressor.py</span></kbd></li>
<li><kbd>ASKLClassifier.py</kbd></li>
<li><kbd><span>ImageTransferLearning.py</span></kbd></li>
<li><kbd><span>PretrainedImageClassifier.py</span></kbd></li>
<li><kbd><span>ExtractFeatures.py</span></kbd></li>
<li><kbd><span>PTGloveEMB.py</span></kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><strong>Automated machine learning</strong> (<strong>AutoML</strong>) refers to those applications that are able to automate the end-to-end process of applying machine learning to real-world problems. Generally, scientific analysts must process data through a series of preliminary procedures before submitting it to machine learning algorithms. In the previous chapters, you saw the necessary steps for performing a proper analysis of data through these algorithms. You saw how simple it is to build a model based on deep neural networks by using several libraries. In some cases, these skills are beyond those possessed by analysts, who must seek support from industry experts to solve the problem.</p>
<p>AutoML was born from a need to create an application that automated the whole machine learning process so that the user could take advantage of these services. Generally, machine learning experts must perform the following tasks:</p>
<ul>
<li>Data preparation</li>
<li>Selecting features</li>
<li>Selecting an appropriate model class</li>
<li>Choosing and optimizing model hyperparameters</li>
<li>Post-processing machine learning models</li>
<li>Analyzing the results obtained</li>
</ul>
<p>AutoML automates all of these operations. It offers the advantages of producing simpler and faster-to-create solutions that often outperform hand-designed models. There are a number of AutoML frameworks; in the following sections, we will look at some of them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Auto-WEKA</h1>
                </header>
            
            <article>
                
<p><span class="tlid-translation translation"><span>Weka is a software environment that's entirely written in Java.</span> <span><strong>Weka</strong>, an acronym for <strong>Waikato Environment for Knowledge Analysis</strong>, is a machine learning software that was developed at the University of Waikato in New Zealand.</span> <span>It is open source and is distributed under the GNU General Public License.</span> <span class="">It is possible to build many models based on machine learning by using it.</span></span></p>
<p><span class="tlid-translation translation"><span>However, each of the algorithms has its own hyperparameters, which can drastically change their performance.</span> <span>The task of the researcher is to find the right combination of these parameters that will maximize the performance of the model.</span> <span>Auto-WEKA automatically solves the problem of the selection of a learning algorithm and the setting of its hyperparameters.</span> </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to use Auto-WEKA in only three main steps. To use this library, it is necessary to install it beforehand. For information on the system requirements and the installation procedure, refer to <a href="https://www.cs.ubc.ca/labs/beta/Projects/autoweka/manual.pdf">https://www.cs.ubc.ca/labs/beta/Projects/autoweka/manual.pdf</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to work with Auto-WEKA, as follows:</p>
<ol>
<li><strong>Building the experiment definition and instantiating it</strong>: In this step, you specify which dataset to use and which type of hyperparameter search will be performed. Then, the experiment is completely instantiated so that Auto-WEKA can identify the classifier to be used. In this phase, Auto-WEKA transforms all of the paths into absolute paths.</li>
<li><strong>Experiment execution</strong><span>: Auto-WEKA uses multiple cores by running the same experiment with several random seeds; the only requirement is that all of the experiments have a similar filesystem.</span></li>
<li><strong>Analysis phase</strong><span>: When Auto-WEKA uses a model-based optimization method, it produces a trajectory of hyperparameters that have identified by the optimization method as the best at a given time. The simplest form of analysis examines the best hyperparameters that have been found in all seeds and uses the trained model to make predictions about a new dataset.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span><span class="tlid-translation translation"><span class="">To select a learning algorithm and set its hyperparameters, Auto-WEKA uses a completely automated approach, taking advantage of recent innovations in Bayesian optimization.</span></span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span class="tlid-translation translation"><span>Auto-WEKA was the first library to use Bayesian optimization to automatically instantiate a highly parametric machine learning framework. Later, </span><span class="">AutoML was also applied by other libraries.</span></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the official Auto-WEKA website: <a href="https://www.cs.ubc.ca/labs/beta/Projects/autoweka/">https://www.cs.ubc.ca/labs/beta/Projects/autoweka/</a></span></li>
<li><span>Refer to </span><em><span>Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA</span></em>: <a href="https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/16-599.pdf">https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/16-599.pdf</a></li>
<li><span>Refer to</span><span> </span><em><span>Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</span></em>: <a href="https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf">https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using AutoML to generate machine learning pipelines with TPOT</h1>
                </header>
            
            <article>
                
<p><span><strong>TPOT</strong> is a Python automated machine learning tool that optimizes machine learning pipelines by using genetic programming. In artificial intelligence, genetic algorithms are part of the class of evolutionary algorithms. A characteristic of the latter is finding solutions to problems by using techniques that are borrowed from natural evolution. The search for a solution to a problem is entrusted to an iterative process that selects and recombines more and more refined solutions until a criterion of optimality is reached. In a genetic algorithm, the population of solutions is pushed toward a given objective by evolutionary pressure. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to build the best performing model to classify the iris species (setosa, virginica, and versicolor) from the <kbd>iris</kbd> dataset, using TPOT. To use this library, it is necessary to install it. For information on the system requirements and for the installation procedure, refer to <a href="https://epistasislab.github.io/tpot/installing/">https://epistasislab.github.io/tpot/installing/</a>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to use AutoML to generate machine learning pipelines with TPOT:</p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>TPOTIrisClassifier.py</kbd></span><span> </span><span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from tpot import TPOTClassifier<br/>from sklearn.datasets import load_iris<br/>from sklearn.model_selection import train_test_split<br/>import numpy as np</pre>
<ol start="2">
<li>Let's import the iris dataset, as follows:</li>
</ol>
<pre style="padding-left: 60px">IrisData = load_iris()</pre>
<ol start="3">
<li>Let's split the dataset, as follows:</li>
</ol>
<pre style="padding-left: 60px">XTrain, XTest, YTrain, YTest = train_test_split(IrisData.data.astype(np.float64),<br/>    IrisData.target.astype(np.float64), train_size=0.70, test_size=0.30)</pre>
<ol start="4">
<li>Now, we can build the classifier:</li>
</ol>
<pre style="padding-left: 60px">TpotCL = TPOTClassifier(generations=5, population_size=50, verbosity=2)</pre>
<ol start="5">
<li>Then, we can train the model:</li>
</ol>
<pre style="padding-left: 60px">TpotCL.fit(XTrain, YTrain)</pre>
<ol start="6">
<li>Then, we will use the model with unseen data (<kbd>XTest</kbd>) to evaluate the performance:</li>
</ol>
<pre style="padding-left: 60px">print(TpotCL.score(XTest, YTest))</pre>
<ol start="7">
<li>Finally, we will export the model pipeline:</li>
</ol>
<pre style="padding-left: 60px">TpotCL.export('TPOTIrisPipeline.py')</pre>
<p style="padding-left: 60px">If you run this code, a pipeline that achieves about 97% test accuracy will be returned.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>TPOT automates machine learning pipeline construction by combining a flexible representation of the pipeline expression tree with stochastic search algorithms, such as genetic programming. In this recipe, you learned how to use <span>TPOT to search for the best pipeline to classify the iris species from the</span> iris dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>TPOT is built on the basis of <kbd>scikit-learn</kbd>, so all of the code that is generated will seem very familiar to us, given the extensive use of the <kbd>scikit-learn</kbd> libraries in the previous chapters. TPOT is a platform that's under active development, and it is therefore subject to continuous updates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of<span> the TPOT tool: <a href="https://epistasislab.github.io/tpot/">https://epistasislab.github.io/tpot/</a></span></li>
<li><span><em>Automating biomedical data science through tree-based pipeline optimization</em>, by </span>Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Auto-Keras</h1>
                </header>
            
            <article>
                
<p>Auto-Keras is an open source software library for AutoML that aims at providing easy access to deep learning models. Auto-Keras has a number of features that allow you to automatically set up the architecture and parameters of deep learning models. Its ease of use, simple installation, and numerous examples make it a very popular framework. Auto-Keras was developed by the DATA Lab at Texas A and M University and community contributors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to use the Auto-Keras library to classify handwritten digits. To install the Auto-Keras package, we can use the <kbd>pip</kbd> command, as follows:</p>
<pre><strong>$ pip install autokeras</strong> </pre>
<p class="mce-root"/>
<p>At the time of writing this book, Auto-Keras was only compatible with Python 3.6. For the installation procedure, refer to the official website at <a href="https://autokeras.com/">https://autokeras.com/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to work with Auto-Keras:</p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>AKClassifier.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from keras.datasets import mnist<br/>import autokeras as ak</pre>
<ol start="2">
<li>Let's import the <kbd>mnist</kbd> dataset, as follows:</li>
</ol>
<pre style="padding-left: 60px">(XTrain, YTrain), (XTest, YTest) = mnist.load_data()</pre>
<ol start="3">
<li><span>Before defining a classifier, we must give a new form to the arrays containing the input data without changing its contents</span>:</li>
</ol>
<pre style="padding-left: 60px">XTrain = XTrain.reshape(XTrain.shape + (1,))<br/>XTest = XTest.reshape(XTest.shape + (1,))</pre>
<ol start="4">
<li>Now, we can build the classifier:</li>
</ol>
<pre style="padding-left: 60px">AKClf = ak.ImageClassifier()</pre>
<ol start="5">
<li>Then, we can train the model:</li>
</ol>
<pre style="padding-left: 60px">AKClf.fit(XTrain, YTrain)</pre>
<ol start="6">
<li>Finally, we will use the model with unseen data (<kbd>XTest</kbd>):</li>
</ol>
<pre style="padding-left: 60px">Results = AKClf.predict(XTest)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>In this recipe, with a few lines of code, we have managed to construct a classifier which, by providing a series of images of handwritten digits, can correctly classify the digits.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>This is a package that allows us to automatically create an algorithm based on machine learning without worrying about the setting of the training parameters that, as you saw in previous chapters, are fundamental to the success of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to the official documentation of<span> the Auto-Keras library: <a href="https://autokeras.com/">https://autokeras.com/</a></span></li>
<li>Refer to <em>Auto-Keras: Efficient Neural Architecture Search with Network Morphism</em>, by Haifeng Jin, Qingquan Song, and Xia Hu (arXiv:1806.10282).</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with auto-sklearn</h1>
                </header>
            
            <article>
                
<p>Auto-sklearn works on the <kbd>scikit-learn</kbd> machine learning library. It represents a platform based on supervised machine learning that's ready for use. It automatically searches for the correct machine learning algorithm for a new dataset and optimizes its hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to use auto-sklearn to build a classifier. To import the data, the <kbd>sklearn.datasets.load_digits</kbd> function will be used. This function loads and returns the digits dataset for classification problems. Each datapoint is an 8x8 image of a digit. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to work with auto-sklearn:</p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>ASKLClassifier.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">import autosklearn.classification<br/>import sklearn.model_selection<br/>import sklearn.datasets<br/>import sklearn.metrics</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>Let's import the <kbd>digits</kbd> dataset, as follows:</li>
</ol>
<pre style="padding-left: 60px">Input, Target = sklearn.datasets.load_digits()</pre>
<ol start="3">
<li><span>Let's split the dataset, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">XTrain, XTest, YTrain, YTest = sklearn.model_selection.train_test_split(Input, Target, random_state=3)</pre>
<ol start="4">
<li>Now, we can build the classifier:</li>
</ol>
<pre style="padding-left: 60px">ASKModel = autosklearn.classification.AutoSklearnClassifier()</pre>
<ol start="5">
<li>Then, we can train the model:</li>
</ol>
<pre style="padding-left: 60px">ASKModel.fit(XTrain, YTrain)</pre>
<ol start="6">
<li>Finally, we will use the model with unseen data (<kbd>XTest</kbd>):</li>
</ol>
<pre style="padding-left: 60px">YPred = ASKModel.predict(XTest)<br/>print("Accuracy score", sklearn.metrics.accuracy_score(YTest, YPred))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p><span>Auto-sklearn uses Bayesian optimization for hyperparameter tuning for traditional machine learning algorithms that are implemented within <kbd>scikit-learn</kbd>. The best machine learning algorithm and the parameters that are optimized are searched automatically.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span class="tlid-translation translation"><span class="">Auto-sklearn is a good choice to automate the process of selecting and optimizing an automatic learning model because it creates extremely precise machine learning models, avoiding the tedious tasks of selecting, training, and testing different models.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the <span><kbd>auto-sklearn</kbd> package: <a href="https://automl.github.io/auto-sklearn/stable/">https://automl.github.io/auto-sklearn/stable/</a></span></li>
<li><em>Efficient and Robust Automated Machine Learning</em>, by Feurer, et al., in Advances in Neural Information Processing Systems</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using MLBox for selection and leak detection</h1>
                </header>
            
            <article>
                
<p>MLBox is an automated library for machine learning. It supports distributed data processing, cleaning, formatting, and numerous algorithms for classification and regression. It allows for the extremely robust selection of functions and leak detection. It also provides stacking models, which means combining a set of model information to generate a new model that aims to perform better than the individual models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>To use this library, it is necessary to install it beforehand. For information on the system requirements and the installation procedure, refer to <a href="https://mlbox.readthedocs.io/en/latest/installation.html">https://mlbox.readthedocs.io/en/latest/installation.html</a>.<a href="https://mlbox.readthedocs.io/en/latest/installation.html"> </a></span></p>
<p class="mce-root">In this recipe, you will learn what's strictly necessary to set up a pipeline using MLBox. A regression problem will be addressed via the use of the Boston dataset that was already used in <a href="f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml">Chapter 1</a>, <em>The Realm of Supervised Learning</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's look at how to use MLBox for selection and leak detection:</p>
<ol>
<li><span>Import the following packages (the full code is in the <kbd>MLBoxRegressor.py</kbd> file that's already been provided for you)</span><span>:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="kn">from</span> <span class="nn">mlbox.preprocessing</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mlbox.optimisation</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">mlbox.prediction</span> <span class="kn">import</span> <span class="o">*</span></pre>
<ol start="2">
<li>Let's import the data, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">paths</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"train.csv"</span><span class="p">,</span><span class="s2">"test.csv"</span><span class="p">]</span> <br/><span class="n">target_name</span> <span class="o">=</span> <span class="s2">"</span><span class="s2">SalePrice</span><span class="s2">"</span></pre>
<p style="padding-left: 60px">With this code, we have set up the list of paths to our datasets and the name of the target that we are trying to predict.</p>
<ol start="3">
<li>Now, we will <span>read and preprocess these files:</span></li>
</ol>
<pre style="padding-left: 60px">data = Reader(sep=",").train_test_split(paths, target_name)<br/>data = Drift_thresholder().fit_transform(data)</pre>
<ol start="4">
<li>To evaluate the model, the following code will be used:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">Optimiser</span><span class="p">()</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></pre>
<p style="padding-left: 60px">In this case, the <span>default configuration was used.</span></p>
<ol start="5">
<li>Finally, to predict <span>on the test set, use the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="n">Predictor</span><span class="p">()</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">None</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></pre>
<p style="padding-left: 60px">If you want configure the pipeline (steps, parameters, and values), the following optional step must be used.</p>
<ol start="6">
<li>T<span>o test and optimize the whole pipeline, we will use the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="n">space</span> <span class="o">=</span> <span class="p">{</span>

       <span class="s1">'ne__numerical_strategy'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">'mean'</span><span class="p">]},</span>

        <span class="s1">'ce__strategy'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="s2">"label_encoding"</span><span class="p">,</span> <span class="s2">"random_projection"</span><span class="p">,</span> <span class="s2">"entity_embedding"</span><span class="p">]},</span>

        <span class="s1">'fs__strategy'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="s2">"variance"</span><span class="p">,</span> <span class="s2">"rf_feature_importance"</span><span class="p">]},</span>
        <span class="s1">'fs__threshold'</span><span class="p">:</span> <span class="p">{</span><span class="s2">"search"</span> <span class="p">:</span> <span class="s2">"choice"</span><span class="p">,</span> <span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]},</span>

        <span class="s1">'est__strategy'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="s2">"XGBoost"</span><span class="p">]},</span>
        <span class="s1">'est__max_depth'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"search"</span> <span class="p">:</span> <span class="s2">"choice"</span><span class="p">,</span> <span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]},</span>
        <span class="s1">'est__subsample'</span> <span class="p">:</span> <span class="p">{</span><span class="s2">"search"</span> <span class="p">:</span> <span class="s2">"uniform"</span><span class="p">,</span> <span class="s2">"space"</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.9</span><span class="p">]}</span>

        <span class="p">}</span>

<span class="n">best</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">optimise</span><span class="p">(</span><span class="n">space</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">max_evals</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span></pre>
<p class="mce-root"/>
<ol start="7">
<li>Finally, to predict <span>on the test set, we will use the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="n">Predictor</span><span class="p">()</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">best</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>MLBox builds the whole pipeline with the following three steps:</p>
<ol>
<li><strong>Preprocessing</strong>: All of the operations related to this phase make use of the <kbd>mlbox.preprocessing</kbd> sub-package. In this phase, we proceed to the reading and cleaning of the input file and then to the removal of the drift variables.</li>
<li><strong>Optimization</strong>: All of the operations related to this phase make use of the sub-package <kbd>mlbox.mlbox.optimisation</kbd>. In this phase, the whole pipeline is optimized. The hyperparametric optimization method that's adopted uses the <kbd>hyperopt</kbd> library. This library creates a highly-dimensional space for the parameters to be optimized and chooses the best combination of parameters that lowers the validation score.</li>
<li><strong>Prediction</strong>: All of the operations related to this phase make use of the <kbd>mlbox.prediction</kbd> sub-package. In this phase, we proceed to prediction by using the test dataset and the best hyperparameters that were identified in the previous phase.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>MLBox provides advanced algorithms and techniques, such as hyperparameter optimization, stacking, deep learning, leak detection, entity embedding, parallel processing, and more. The use of MLBox is currently limited to Linux only. MLBox was first developed using Python 2, and then it was extended to Python 3. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>MLBox's official documentation: <a href="https://mlbox.readthedocs.io/en/latest/">https://mlbox.readthedocs.io/en/latest/</a></li>
<li>Installation guide: <a href="https://mlbox.readthedocs.io/en/latest/installation.html">https://mlbox.readthedocs.io/en/latest/installation.html</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks with transfer learning</h1>
                </header>
            
            <article>
                
<p><strong>Transfer learning</strong> is a methodology based on machine learning that exploits the memorization of the knowledge that's acquired during the resolution of a problem and the application of the same to different (but related) problems. The need to use transfer learning takes place when there is a limited supply of training data. This could be due to the fact that data is rare or expensive to collect or label, or inaccessible. With the growing presence of large amounts of data, the transfer learning option has become more frequently used.</p>
<p><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) are essentially <strong>artificial neural networks</strong> (<strong>ANNs</strong>). In fact, just like the latter, CNNs are made up of neurons that are connected to one another by weighted branches (weight); the training parameters of the networks are once again the weight and the bias. In CNNs, the connection pattern between neurons is inspired by the structure of the visual cortex in the animal world. The individual neurons that are present in this part of the brain (the visual cortex) respond to certain stimuli in a narrow region of the observation, called the <strong>receptive field</strong>. The receptive fields of different neurons are partially overlapped to cover the entire field of vision. The response of a single neuron to stimuli taking place in its receptive field can be mathematically approximated by a convolution operation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to build an image recognition model by using transfer learning in Keras. To do this, the MobileNet model and Keras <span>high-level neural networks API will be used to train the model images extracted from the <kbd>Caltech256</kbd> dataset that we already used in</span> <a href="8c346bea-36ab-4087-918f-b5d8712977cc.xhtml">Chapter 10</a>, <em>Image Content Analysis</em><span>. <kbd>Caltech256</kbd> is very popular in this field! It contains 256 classes of images, where each class contains thousands of samples. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's build an image recognition model by using transfer learning in Keras; in this section, we will explain the code step by step:</p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>ImageTransferLearning.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from keras.layers import Dense,GlobalAveragePooling2D<br/>from keras.applications import MobileNet<br/>from keras.applications.mobilenet import preprocess_input<br/>from keras.preprocessing.image import ImageDataGenerator<br/>from keras.models import Model</pre>
<ol start="2">
<li>Let's import the <kbd>MobileNet</kbd> model and discard the last 1,000 neuron layers:</li>
</ol>
<pre style="padding-left: 60px">BasicModel=MobileNet(input_shape=(224, 224, 3), weights='imagenet',include_top=False)</pre>
<ol start="3">
<li>Let's define the Keras model architecture:</li>
</ol>
<pre style="padding-left: 60px">ModelLayers=BasicModel.output<br/>ModelLayers=GlobalAveragePooling2D()(ModelLayers)<br/>ModelLayers=Dense(1024,activation='relu')(ModelLayers) <br/>ModelLayers=Dense(1024,activation='relu')(ModelLayers) <br/>ModelLayers=Dense(512,activation='relu')(ModelLayers) <br/>OutpModel=Dense(3,activation='softmax')(ModelLayers) </pre>
<ol start="4">
<li>Now, we can build a model based on the architecture that was previously defined:</li>
</ol>
<pre style="padding-left: 60px">ConvModel=Model(inputs=BasicModel.input,outputs=OutpModel)</pre>
<ol start="5">
<li>Now, we can move on to the training phase. Having adopted an approach based on transfer learning, it is not necessary to proceed with the training of the whole model. This is because MobileNet is already trained. Let's define the last dense levels as the trainable layer:</li>
</ol>
<pre style="padding-left: 60px">for layer in ConvModel.layers[:20]:<br/>    layer.trainable=False<br/>for layer in ConvModel.layers[20:]:<br/>    layer.trainable=True</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>Let's load the training data into <span><kbd>ImageDataGenerator</kbd>:</span></li>
</ol>
<pre style="padding-left: 60px">TrainDataGen=ImageDataGenerator(preprocessing_function=preprocess_input)</pre>
<p style="padding-left: 60px"><kbd>ImageDataGenerator</kbd> is a built-in Keras class that creates groups of tensor image data with real-time data augmentation. The data will be wound over in groups.</p>
<ol start="7">
<li>Let's define some dependencies and a path for the training data:</li>
</ol>
<pre style="padding-left: 60px">TrainGenerator=TrainDataGen.flow_from_directory('training_images/', #'train/'<br/>                                                 target_size=(224,224),<br/>                                                 color_mode='rgb',<br/>                                                 batch_size=32,<br/>                                                 class_mode='categorical',<br/>                                                 shuffle=True)</pre>
<ol start="8">
<li>Let's compile the Keras model:</li>
</ol>
<pre style="padding-left: 60px">ConvModel.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])</pre>
<p style="padding-left: 60px">The following three arguments are passed:</p>
<ul>
<li><kbd>optimizer='adam'</kbd>: An algorithm for first-order, gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.</li>
<li><kbd>loss='categorical_crossentropy'</kbd>: We have used the <kbd>categorical_crossentropy</kbd> argument here. When using<br/>
<kbd>categorical_crossentropy</kbd>, your targets should be in a categorical format (we have 10 classes; the target for each sample must be a 10-dimensional vector that is all-zeros, except for a one at the index corresponding to the class of the sample).</li>
<li><kbd>metrics=['accuracy']</kbd>: A metric is a function that is used to evaluate the performance of your model during training and testing.</li>
</ul>
<ol start="9">
<li>Finally, we will define the step size for training and fit the model, as follows:</li>
</ol>
<pre style="padding-left: 60px">StepSizeTrain=TrainGenerator.n//TrainGenerator.batch_size<br/>ConvModel.fit_generator(generator=TrainGenerator,<br/> steps_per_epoch=StepSizeTrain,<br/> epochs=10)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>Found 60 images belonging to 3 classes.</strong><br/><strong>Epoch 1/10</strong><br/><strong>1/1 [==============================] - 31s 31s/step - loss: 1.1935 - acc: 0.3125</strong><br/><strong>Epoch 2/10</strong><br/><strong>1/1 [==============================] - 21s 21s/step - loss: 2.7700 - acc: 0.5714</strong><br/><strong>Epoch 3/10</strong><br/><strong>1/1 [==============================] - 24s 24s/step - loss: 0.0639 - acc: 1.0000</strong><br/><strong>Epoch 4/10</strong><br/><strong>1/1 [==============================] - 21s 21s/step - loss: 0.2819 - acc: 0.7500</strong><br/><strong>Epoch 5/10</strong><br/><strong>1/1 [==============================] - 26s 26s/step - loss: 0.0012 - acc: 1.0000</strong><br/><strong>Epoch 6/10</strong><br/><strong>1/1 [==============================] - 21s 21s/step - loss: 0.0024 - acc: 1.0000</strong><br/><strong>Epoch 7/10</strong><br/><strong>1/1 [==============================] - 22s 22s/step - loss: 8.7767e-04 - acc: 1.0000</strong><br/><strong>Epoch 8/10</strong><br/><strong>1/1 [==============================] - 24s 24s/step - loss: 1.3191e-04 - acc: 1.0000</strong><br/><strong>Epoch 9/10</strong><br/><strong>1/1 [==============================] - 25s 25s/step - loss: 9.6636e-04 - acc: 1.0000</strong><br/><strong>Epoch 10/10</strong><br/><strong>1/1 [==============================] - 21s 21s/step - loss: 3.2019e-04 - acc: 1.0000</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this recipe, you learned how to <span>use transfer learning in an image recognition problem. Through transfer learning, a pretrained model can be used on a large and accessible dataset to find layers whose output have reusable features, which is done by using this output as input to train a smaller network that requires fewer parameters. This network will only need to know the relationships between the patterns that are obtained from the pretrained models and the specific problem to be solved. As a pretrained model, the MobileNet model was used. </span></p>
<p class="mce-root"/>
<p><kbd>MobileNet</kbd> is an architecture that was proposed by Google and that is particularly suitable for vision-based applications. MobileNet uses deep separable convolutions that significantly reduce the number of parameters, compared to a network with normal convolutions with the same depth in the networks. Neural networks based on the MobileNet model are thus lighter. The normal convolution is replaced by a <span>in-depth </span>convolution, followed by a punctual convolution that is called <strong>convolution separable in depth</strong>.</p>
<p>The transfer learning procedure was then performed in two phases:</p>
<ul>
<li>First, almost all levels of the neural network were trained on a very large and generic dataset to allow for the acquisition of global notions</li>
<li>Later, we used the specific dataset for the training of the remaining layers, deciding whether to propagate the errors through fine-tuning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used fine-tuning; in fact, we didn't simply replace the final level, but we also trained some of the previous levels. In the network that we used, the initial levels were used to acquire generic functionalities (exploiting the potential of the MobileNet trained network), while the subsequent ones were used to finalize the experience that was acquired on the specific activity in question. Using this procedure, we froze the first 20 layers while we traced the following layers to meet our needs. This methodology helps to achieve better performance with less training time.</p>
<p>Fine-tuning can be achieved through the following steps:</p>
<ol>
<li class="mce-root">We start with a pretrained network trained on a similar problem and replace the output level with a new level of output by adjusting the number of classes.</li>
<li class="mce-root">The initial values ​​of the weights are those of the pretrained net, except for the connections between successive layers whose weights are initialized randomly.</li>
<li class="mce-root">We perform new training iterations (SGD) for optimized weights with respect to the peculiarities of the new dataset (it does not need to be large).</li>
</ol>
<p><span>In the fine-tuning process, the model parameters will be adjusted precisely to fit with certain observations.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to Keras application models: <a href="https://keras.io/applications/">https://keras.io/applications/</a></li>
<li><span>Refer to </span><em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</em>: <a href="https://arxiv.org/pdf/1704.04861.pdf">https://arxiv.org/pdf/1704.04861.pdf</a></li>
<li><span>Refer to </span><em>Transfer Learning and Computer Vision</em> (from Yale University): <a href="http://euler.stat.yale.edu/~tba3/stat665/lectures/lec18/lecture18.pdf">http://euler.stat.yale.edu/~tba3/stat665/lectures/lec18/lecture18.pdf</a></li>
<li><span>Refer to</span> <em>A Survey on Transfer Learning</em>, <span>S. J. Pan and Q. Yang, </span>in IEEE Transactions on Knowledge and Data Engineering: <a href="https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf">https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning with pretrained image classifiers using ResNet-50</h1>
                </header>
            
            <article>
                
<p>The <strong>residual network</strong> (<strong>ResNet</strong>) represents an architecture that, through the use of new and innovative types of blocks (known as <strong>residual blocks</strong>) and the concept of residual learning, has allowed researchers to reach depths that were unthinkable with the classic feedforward model, due to the problem of the degradation of the gradient. </p>
<p>Pretrained models are trained on a large set of data, and so they allow us to obtain excellent performance. We can therefore adopt pretrained models for a problem similar to the one that we want to solve, to avoid the problem of a lack of data. Because of the computational costs of the formation of such models, they are available in ready-to-use formats. For example, the Keras library offers several models such as Xception, VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2, MobileNet, MobileNetV2, DenseNet, and NASNet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to use a pretrained model to predict the class of a single image. To do this, a ResNet-50 model will be used. This model is available from the <kbd>keras.applications</kbd> library. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span>Now, we will use a pretrained model to classify a single image; in this section, we will explain the code step by step:</span></p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>PretrainedImageClassifier.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from keras.applications.resnet50 import ResNet50<br/>from keras.preprocessing import image<br/>from keras.applications.resnet50 import preprocess_input, decode_predictions<br/>import numpy as np</pre>
<ol start="2">
<li>Let's define the <span>pretrained model</span>:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">PTModel = ResNet50(weights='imagenet')</pre>
<ol start="3">
<li>Let's define the image to classify:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">ImgPath = 'airplane.jpg'<br/>Img = image.load_img(ImgPath, target_size=(224, 224))</pre>
<ol start="4">
<li>Here, we will take an image instance and turn it into a <kbd>numpy</kbd> array with <kbd>dtype</kbd> <kbd>float32</kbd>:</li>
</ol>
<pre style="padding-left: 60px">InputIMG = image.img_to_array(Img)</pre>
<ol start="5">
<li>Now, we will expand the <kbd>numpy</kbd> array that's obtained in the shape that's required by the pretrained model:</li>
</ol>
<pre style="padding-left: 60px">InputIMG = np.expand_dims(InputIMG, axis=0)</pre>
<ol start="6">
<li>Then, we will preprocess the data:</li>
</ol>
<pre style="padding-left: 60px">InputIMG = preprocess_input(InputIMG)</pre>
<ol start="7">
<li>Finally, we will use the pretrained model to classify the input image:</li>
</ol>
<pre style="padding-left: 60px">PredData = PTModel.predict(InputIMG)</pre>
<ol start="8">
<li>To evaluate the model's performance, we will use the <kbd>decode_predictions</kbd> function, as follows: </li>
</ol>
<pre style="padding-left: 60px">print('Predicted:', decode_predictions(PredData, top=3)[0])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The <kbd>keras.applications.resnet50.decode_predictions</kbd> function decodes the results into a list of tuples (class, description, and probability). The following results are printed:</p>
<pre style="padding-left: 60px"><strong>Predicted: [('n02690373', 'airliner', 0.80847234), ('n04592741', 'wing', 0.17411195), ('n04552348', 'warplane', 0.008112171)]</strong></pre>
<p style="padding-left: 60px">The higher probability (<kbd>0.80847234</kbd>) tells us that it is an airliner; in fact, the following is the image that was provided as input:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1061 image-border" src="assets/999001d5-8a86-43b0-8bc7-fc359f12e3b6.png" style="width:43.75em;height:26.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Instead of trying to estimate a function <kbd>G</kbd> that, given an <kbd>x</kbd>, returns <kbd>G (x)</kbd>, ResNet learns the difference between the two values—a value called the <strong>residual</strong>. In the residual layer of the network, a classical convolution takes place and the input is added to the result. If the input and output are of different sizes, the input is transformed with another 1×1 filter convolution before being added to the output so that it has the same feature map number. The size of a feature map is preserved by padding. A benefit of this technique is that the L2 regularization, which tends the weights toward zero, does not make us forget what was learned previously, but simply preserves it.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>There are ResNet implementations with different depths; the deepest counts as many as 152 levels. There is also a prototype with 1,202 levels, but it achieved worse results due to overfitting. This architecture won ILSVRC 2015, with an error of 3.6%. To understand the value of this result, just consider that the error that's generally achieved by a human being is around 5-10%, based on their skills and knowledge. Thanks to these results, the ResNet model is currently state of the art in the field of computer vision.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The official documentation of the<span> </span><kbd>keras.applications</kbd> <span>models: <a href="https://keras.io/applications/">https://keras.io/applications/ </a></span></li>
<li><em>Deep Residual Learning for Image Recognition</em> (by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun): <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a></li>
<li><em>Pretrained Models</em> (from Toronto University): <a href="https://www.cs.toronto.edu/~frossard/tags/pre-trained-models/">https://www.cs.toronto.edu/~frossard/tags/pre-trained-models/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning using feature extraction with the VGG16 model</h1>
                </header>
            
            <article>
                
<p>As we stated in <span>the <em>Visualizing the MNIST dataset using PCA and t-SNE</em></span><span> recipe of</span> <a href="bfa120ec-7b32-4af2-85c3-f16bbaf84998.xhtml">Chapter 14</a><em>, Unsupervised Representation Learning</em>, in the case of datasets of important dimensions, the data was transformed into a reduced series of representation functions. This process of transforming the input data into a set of functionalities is named <strong>feature extraction</strong>. This is because the extraction of the characteristics proceeds from an initial series of measured data and produces derived values that can keep the information contained in the original dataset, but excluded from the redundant data. In the case of images, feature extraction is aimed at obtaining information that can be identified by a computer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, you will learn how to extract features from a series of images. Then, we will use these features to classify the images by using the k-means algorithm. In this recipe, we will use the VGG16 pretrained model and the <kbd>klearn.cluster.KMeans</kbd> function. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's perform a feature extraction procedure by using the VGG16 model:</p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>ExtractFeatures.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from keras.applications.vgg16 import VGG16<br/>from keras.preprocessing import image<br/>from keras.applications.vgg16 import preprocess_input<br/>import numpy as np<br/>from sklearn.cluster import KMeans</pre>
<ol start="2">
<li>Let's define the pretrained model:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">model = VGG16(weights='imagenet', include_top=False)</pre>
<ol start="3">
<li>Let's initialize the list of features that will be extracted:</li>
</ol>
<pre style="padding-left: 90px">VGG16FeatureList = []</pre>
<ol start="4">
<li>For each image in the dataset, we have to proceed with the extraction of features:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>for path, subdirs, files in os.walk('training_images'):<br/>    for name in files:<br/>        img_path = os.path.join(path, name)<br/>        print(img_path)</pre>
<p style="padding-left: 60px">In this way, we have recovered the path of each image contained in the folder. The images that are used are contained in the <kbd>training_images</kbd> folder, which we already used in the <em>Convolutional neural networks with transfer learning </em>recipe. It is a series of images that was extracted from the <kbd>Caltech256</kbd> dataset.</p>
<ol start="5">
<li><span>Let's import the image, as follows:</span></li>
</ol>
<pre style="padding-left: 60px" class="mce-root">img = image.load_img(img_path, target_size=(224, 224))</pre>
<ol start="6">
<li><span>We will take an image instance and turn it into a NumPy array, with datatype as <kbd>float32</kbd>:</span></li>
</ol>
<pre>        img_data = image.img_to_array(img)</pre>
<ol start="7">
<li>Now, we will expand the NumPy array that's obtained in the shape that's required by the pretrained model:</li>
</ol>
<pre>        img_data = np.expand_dims(img_data, axis=0)</pre>
<p class="mce-root"/>
<ol start="8">
<li>Then, we will preprocess the data:</li>
</ol>
<pre>        img_data = preprocess_input(img_data)</pre>
<ol start="9">
<li><span>We will use the pretrained model to extract features from the input image:</span></li>
</ol>
<pre>        VGG16Feature = model.predict(img_data)</pre>
<ol start="10">
<li>At this point, we will create an array with the obtained features:</li>
</ol>
<pre>        VGG16FeatureNp = np.array(VGG16Feature)</pre>
<ol start="11">
<li>Now, we will add the array that was obtained, to the list of features that we are building (one element for each image):</li>
</ol>
<pre>        VGG16FeatureList.append(VGG16FeatureNp.flatten())</pre>
<ol start="12">
<li>We will transform the final list into an array:</li>
</ol>
<pre style="padding-left: 60px">VGG16FeatureListNp = np.array(VGG16FeatureList)</pre>
<ol start="13">
<li>Now, we can use the features that was obtained from the images to group them by type. Remember that these are images from three categories: airplanes, cars, and motorbikes. So, we expect the images to be labeled with three different labels. To do this, we use the <kbd>KMeans</kbd> algorithm, as follows:</li>
</ol>
<pre style="padding-left: 60px">KmeansModel = KMeans(n_clusters=3, random_state=0)</pre>
<ol start="14">
<li>After defining the model, we move on to training it:</li>
</ol>
<pre style="padding-left: 60px">KmeansModel.fit(VGG16FeatureListNp)</pre>
<ol start="15">
<li>Finally, we print the labels of the images that are used:</li>
</ol>
<pre style="padding-left: 60px">print(KmeansModel.labels_)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 </strong><br/><strong> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</strong><br/><strong> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]</strong></pre>
<p style="padding-left: 60px" class="mce-root">As you can see, the 60 images have been correctly labeled in the three available categories.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, you learned how to extract features from a series of images. As we have a limited number of images available, we used a pretrained model (VGG16) to correctly extract the information that was needed for subsequent identification. This procedure is useful to understand how to proceed to perform automatic recognition of the images through an unsupervised model. After extracting the features, we used them to classify the images, using the KMeans algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>VGG16 is a convolutional neural network model that was presented by K. Simonyan and A. Zisserman, from the University of Oxford, in the paper <em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em>. This model has achieved excellent results in image recognition (with 92.7% accuracy). The test was performed on the ImageNet dataset, with over 14 million images belonging to 1,000 classes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><span>Refer to the </span><em>Visualizing Mnist dataset using PCA and t-SNE</em><span> recipe in </span><a href="bfa120ec-7b32-4af2-85c3-f16bbaf84998.xhtml">Chapter 14</a><em>, Unsupervised Representation Learning</em></li>
<li>Refer to <em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em>: <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning with pretrained GloVe embedding</h1>
                </header>
            
            <article>
                
<p><strong>GloVe</strong> is an unsupervised learning algorithm for obtaining vector representations of words. The training is performed on the aggregate global statistics on the co-occurrence of words that has been extracted from a body of text present in the code files. The resulting representations show interesting linear substructures in the vector space of words. In this recipe, you will learn how to use a pretrained GloVe embedding model to classify adjectives to describe a person in a positive or negative fashion.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>To follow this recipe, you will need to download the <kbd>glove.6B.100d.txt</kbd> file. This file is available at <a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>. There are several versions of the pretrained word vectors:</p>
<ul>
<li><strong>glove.6B</strong>: 6B tokens, 400K vocab, uncased, 50d, 100d, 200d, and 300d vectors—822 MB</li>
<li><strong>glove.42B.300d</strong>: 42B tokens, 1.9M vocab, uncased, 300d vectors—1.75 GB</li>
<li><strong>glove.840B.300d</strong>: 840B tokens, 2.2M vocab, cased, 300d vectors—2.03 GB</li>
<li><strong>Twitter</strong>: 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, and 200d vectors—1.42 GB</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's <span>classify the adjectives that are used to describe a person in a positive and negative fashion:</span></p>
<ol>
<li><span>Create a new Python file and import the following packages (the full code is in the <kbd>PTGloveEMB.py</kbd></span> <span>file that's already been provided for you):</span></li>
</ol>
<pre style="padding-left: 60px">from numpy import array<br/>from numpy import zeros<br/>from numpy import asarray<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Flatten<br/>from keras.layers import Embedding</pre>
<ol start="2">
<li>Let's define the 10 <span>positive and 10 </span>negative adjectives that are used to describe a person:</li>
</ol>
<pre style="padding-left: 60px">Adjectives = ['Wonderful',<br/>        'Heroic',<br/>        'Glamorous',<br/> 'Valuable',<br/>        'Excellent',<br/>        'Optimistic',<br/>        'Peaceful',<br/>        'Romantic',<br/>        'Loving',<br/>        'Faithful',<br/>        'Aggressive',<br/>        'Arrogant',<br/>        'Bossy',<br/>        'Boring',<br/>        'Careless',<br/>        'Selfish',<br/>        'Deceitful',<br/>        'Dishonest',<br/>        'Greedy',<br/>        'Impatient']</pre>
<ol start="3">
<li>Let's define the labels of the <span>adjectives that were defined previously (<kbd>1</kbd> = positive, <kbd>0</kbd> = negative):</span></li>
</ol>
<pre style="padding-left: 60px">AdjLabels = array([1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0])</pre>
<ol start="4">
<li>Let's tokenize the <span>adjectives and prepare the vocabulary:</span></li>
</ol>
<pre style="padding-left: 60px">TKN = Tokenizer()<br/>TKN.fit_on_texts(Adjectives)<br/>VocabSize = len(TKN.word_index) + 1</pre>
<ol start="5">
<li>Let's encode the <span>adjectives into an integer sequence and transform a list of sequences into a two-dimensional NumPy array:</span></li>
</ol>
<pre style="padding-left: 60px">EncodedAdjectives = TKN.texts_to_sequences(Adjectives)<br/>PaddedAdjectives = pad_sequences(EncodedAdjectives, maxlen=4, padding='post')</pre>
<ol start="6">
<li>Let's load the pretrained model: </li>
</ol>
<pre style="padding-left: 60px">EmbeddingsIndex = dict()<br/>f = open('glove.6B.100d.txt',encoding="utf8")<br/>for line in f:<br/>  Values = line.split()<br/>  Word = Values[0]<br/>  Coefs = asarray(Values[1:], dtype='float32')<br/>  EmbeddingsIndex[Word] = Coefs<br/>f.close()</pre>
<ol start="7">
<li>We will create a weight matrix for words in tokenized adjectives:</li>
</ol>
<pre style="padding-left: 60px">EmbeddingMatrix = zeros((VocabSize, 100))<br/>for word, i in TKN.word_index.items():<br/>  EmbeddingVector = EmbeddingsIndex.get(word)<br/>  if EmbeddingVector is not None:<br/>    EmbeddingMatrix[i] = EmbeddingVector</pre>
<p class="mce-root"/>
<ol start="8">
<li>Now, we are ready to define the <kbd>keras</kbd> sequential model:</li>
</ol>
<pre style="padding-left: 60px">AdjModel = Sequential()<br/>PTModel = Embedding(VocabSize, 100, weights=[EmbeddingMatrix], input_length=4, trainable=False)<br/>AdjModel.add(PTModel)<br/>AdjModel.add(Flatten())<br/>AdjModel.add(Dense(1, activation='sigmoid'))<br/>print(AdjModel.summary())</pre>
<p style="padding-left: 60px">The following summary is printed:</p>
<pre style="padding-left: 60px"><strong>_________________________________________________________________</strong><br/><strong>Layer (type) Output Shape Param # </strong><br/><strong>=================================================================</strong><br/><strong>embedding_13 (Embedding) (None, 4, 100) 2100 </strong><br/><strong>_________________________________________________________________</strong><br/><strong>flatten_10 (Flatten) (None, 400) 0 </strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_17 (Dense) (None, 1) 401 </strong><br/><strong>=================================================================</strong><br/><strong>Total params: 2,501</strong><br/><strong>Trainable params: 401</strong><br/><strong>Non-trainable params: 2,100</strong></pre>
<p style="padding-left: 60px" class="mce-root">As you can see, only part of the parameters have been trained.</p>
<ol start="9">
<li>Let's compile and fit the model:</li>
</ol>
<pre>AdjModel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])<br/>AdjModel.fit(PaddedAdjectives, AdjLabels, epochs=50, verbose=1)</pre>
<ol start="10">
<li>Finally, we will evaluate the model's performance:</li>
</ol>
<pre style="padding-left: 60px">loss, accuracy = AdjModel.evaluate(PaddedAdjectives, AdjLabels, verbose=1)<br/>print('Model Accuracy: %f' % (accuracy*100))</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Model Accuracy: 100.000000</strong></pre>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>To quantitatively capture the nuances that are necessary to distinguish a positive adjective from a negative adjective, a model has to associate more than a single number with word combinations. A simple method for a set of words is the vector difference between two vectors of words. GloVe is designed so that these vector differences capture the meanings specified by the juxtaposition of several words as closely as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>In transfer learning, the weights of the network are adapted and transferred so that we can use this knowledge to pursue multiple different objectives. To obtain good performance from transfer learning, certain conditions must be met: the initial and final datasets must not be too different from each other, and they must share the same preprocessing operations.</p>
<p>So far, you have seen several examples of how the concepts of transfer learning can be applied to real cases. Actually, in practice, transfer learning takes on different types: <kbd>Inductive Transfer learning</kbd>, <kbd>Unsupervised Transfer Learning</kbd>, <kbd>Transductive Transfer Learning</kbd>, and <kbd>Instance Transfer</kbd>. We are trying to deepen those concepts.</p>
<p>To understand the differences between these methodologies, we will look at the terms—domains and tasks. By the term <strong>domain</strong>, we mean the type of data that's used by the network, while by the term <strong>task</strong>, we mean what the network intends to do. We will also use the terms <strong>source</strong> and <strong>destination</strong> to distinguish the network that's already trained on a large amount of data from the network that we intend to build.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inductive transfer learning</h1>
                </header>
            
            <article>
                
<p>One of the simplest forms of supervised machine learning is <kbd>inductive learning</kbd>. It is based solely on observation. Given an initial set of input-output examples, the agent elaborates on hypotheses to reconstruct the transfer function. The agent is designed to observe interactions with the outside world. In particular, the agent analyzes the feedback of its decisions. The perceptions of the artificial agent can be used as follows:</p>
<ul>
<li>To make decisions (reactive agent)</li>
<li>To improve the agent's decision-making capacity (machine learning)</li>
</ul>
<p class="mce-root"/>
<p>In <kbd>Inductive Transfer Learning</kbd> methods, the information that's processed by the two networks (the source and destination) is of the same type (images, sounds, and so on), while the tasks performed by the networks are different. In this case, the purpose of transfer learning is to use the <kbd>inductive-bias</kbd> that was recovered in the training of the source network to improve the performance of the destination network. By the term <strong>inductive-bias</strong>, we mean a series of hypotheses concerning the distribution of the data that the algorithm recovers in the training phase.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised transfer learning</h1>
                </header>
            
            <article>
                
<p>In unsupervised transfer learning, the information that's processed by the two networks (the source and destination) is of the same type (images, sounds, and so on), while the tasks that are performed by the networks are different, like in inductive transfer learning. The substantial difference between the two methods lies in the fact that no labeled data is available in unsupervised transfer learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transductive transfer learning</h1>
                </header>
            
            <article>
                
<p>In transductive transfer learning, the information that's processed by the two networks (the source and destination) is different, while the tasks that are performed by the networks are similar. This methodology is based on the concept of transductive inference, which brings the reasoning from specific (training) cases to specific cases (tests). Unlike induction, which requires the solution to a more general problem before solving a more specific problem, in transduction, we try to get the answer that we really need, but not a more general one.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Instance transfer learning</h1>
                </header>
            
            <article>
                
<p>A scenario in which the domains of the origin and destination are perfectly similar is difficult to find. It is more possible to identify a part of data that is better approximating to those of destination<span> but lies in the domain of origin which is of a much larger size than the destination one</span>. In instance transfer learning, we look for the training samples in the origin domain that have a strong correlation with the destination domain. Once they are identified, they are reused in the learning phase of the target activity; in this way, the accuracy of the classification is improved.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to <em>Global Vectors for Word Representation</em> (by Jeffrey Pennington, Richard Socher, and Christopher D. Manning): <a href="https://www.aclweb.org/anthology/D14-1162">https://www.aclweb.org/anthology/D14-1162</a></li>
<li>Refer to <em>A Review of Transfer Learning Algorithms</em> (by Mohsen Kaboli): <a href="https://hal.archives-ouvertes.fr/hal-01575126/document">https://hal.archives-ouvertes.fr/hal-01575126/document</a></li>
</ul>


            </article>

            
        </section>
    </body></html>