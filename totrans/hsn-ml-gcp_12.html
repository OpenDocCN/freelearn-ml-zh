<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preventing Overfitting with Regularization</h1>
                </header>
            
            <article>
                
<p>So far, in the previous chapters, we understood about building neural network, evaluating the TensorBoard results, and varying the hyperparameters of the neural network model to improve the accuracy of the model.</p>
<p>While the hyperparameters in general help with improving the accuracy of model, certain configuration of hyperparameters results in the model overfitting to the training data, while not generalizing for testing data is the problem of overfitting to the training data.</p>
<p>A key parameter that can help us in avoiding overfitting while generalizing on an unseen dataset is the regularization technique. Some of the key regularization techniques are as follows:</p>
<ul>
<li>L2 regularization</li>
<li>L1 regularization</li>
<li>Dropout</li>
<li>Scaling</li>
<li>Batch normalization</li>
<li>Weight initialization</li>
</ul>
<p>In this chapter, we will go through the following:</p>
<ul>
<li>Intuition of over/under fitting</li>
<li>Reducing overfitting using regularization</li>
<li>Improving the underfitting scenario</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intuition of over/under fitting</h1>
                </header>
            
            <article>
                
<p>Before we understand about how the preceding techniques are useful, let's build a scenario, so that we understand the phenomenon of overfitting.</p>
<p><strong>Scenario 1: A case of not generalizing on an unseen dataset</strong></p>
<p>In this scenario, we will create a dataset, for which there is a clear linearly separable mapping between input and output. For example, whenever the independent variables are positive, the output is <kbd>[1,0]</kbd>, and when the input variables are negative, the output is <kbd>[0,1]</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f2ad6e3-05f0-47df-9eee-b87485c35826.png" style=""/></div>
<p>To that dataset, we will add a small amount of noise (10% of the preceding dataset created) by adding some data points that follow the opposite of the preceding pattern, that is, when the input variables are positive, the output is <kbd>[0,1]</kbd>, and the output is <kbd>[1,0]</kbd> when the input variables are negative:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/13da13ac-d612-4e7e-a42a-f04ca911688a.png" style=""/></div>
<p>Appending the datasets obtained by the preceding two steps gives us the training dataset, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/14edfbdf-5912-4c66-a5fc-4973132f5bbb.png" style=""/></div>
<p>In the next step, we create the test dataset, where it follows the criterion followed by the majority of the training dataset, that is, when the input is positive, the output is <kbd>[1,0]</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f9d36cea-4bc8-454e-be9f-8d0541614933.png" style=""/></div>
<p>Now that we have created the dataset, let's go ahead and build a model to predict the output with the given inputs.</p>
<p>The intuition here is that, if training accuracy improves by more than 90.91% it is a classic case of overfitting, as the model tries to fit for the minority of the observations which do not generalize for an unseen dataset.</p>
<p>To check that—let's first import all the relevant packages to build a model in <kbd>keras</kbd>:</p>
<p>We build a model with three layers where the layers have 1,000, 500 and 100 units in each respective hidden layer:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0848aeeb-7310-4057-a8c1-430d6cdc486a.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5afb4ede-cd47-4e44-856f-04e064ce4615.png"/></div>
<p>A TensorBoard visualization of loss and accuracy on train and test datasets is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bbe00504-88bf-46cf-8a26-fd16fd21e95e.png" style=""/></div>
<p>From the first two graphs, we can see that, as loss on train dataset decreased, its accuracy improved.</p>
<p>Also, note that the training loss was not reducing smoothly—potentially indicating to us that it is overfitting to the training data.</p>
<p>You should observe that, the validation accuracy (test accuracy) started to decrease as training dataset accuracy improved—again indicating to us that the model does not generalize well to unseen dataset.</p>
<p>This phenomenon typically happens when the model is too complex and tries to fit the last few misclassifications to reduce the training loss.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing overfitting</h1>
                </header>
            
            <article>
                
<p>Typically, overfitting results in some weights being very high relative to others. To understand that, let's look at the histogram of weights that are obtained by running the model on the artificially created dataset in <em>scenario 1</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b2634e8c-5d8d-4d48-8397-159249df98e9.png" style=""/></div>
<p>We see that there are some weights that have a high value (&gt; 0.1) and a majority that are centered around zero.</p>
<p>Let's now explore the impact of penalizing for having a high weight value through L1 and L2 regularizations.</p>
<p>The intuition of regularization is as follows:</p>
<ul>
<li>If the weight values are shrunk to as minimal as possible, it is less likely that some of those weights contribute more towards fine-tuning our model to the few outlier cases</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing L2 regularization</h1>
                </header>
            
            <article>
                
<p>Now that we have seen how overfitting occurs on our dataset, we will explore the impact of L2 regularization in reducing overfitting on the dataset.</p>
<p>L2 regularization on a dataset can be defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/320843d0-3683-4422-80b2-c2913f8d02d4.png" style=""/></div>
<p>Note that the loss function is the traditional loss function where <em>y</em> is the dependent variable, <em>x</em> is the independent variables, and <em>W</em> is the kernel (weight matrices).</p>
<p>The regularization term is added to the loss function. Note the regularization value is the sum of squared weight values across all the dimensions of a weight matrix. Given that we are minimizing the sum of squared value of weights along with loss function, the cost function ensures that there is no weight value that is large—thereby ensuring that less overfitting occurs.</p>
<p>The lambda parameter is a hyperparameter that adjusts the weightage that we give to regularization term.</p>
<p>Let's explore the impact of adding L2 regularization to the model defined in <em>scenario 1</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b2fc3fc5-0869-4653-b2b0-37f83c18f3e6.png"/></div>
<p>Note that we modified the code that we have seen in <em>scenario 1</em> by adding <kbd>kernel_regularizer</kbd>, which, in this case, is the L2 regularizer with a lambda value of <kbd>0.01</kbd>.</p>
<p>Note the output of TensorBoard, as we train the preceding model:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/91fc92ac-abda-4234-b963-1f3b085ec531.png" style=""/></div>
<p>The training loss kept on decreasing and the validation accuracy remained stable while the training accuracy is 90.9% without accounting for the overfitting scenario.</p>
<p>Let's explore the distribution of weights to understand the difference between weight distributions when L2 regularization was done and there was no regularization:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/49101e45-d418-4a7f-9a2c-73156b597df6.png"/></div>
<p>You should notice that the kernels (primarily the kernels at <kbd>dense_2</kbd> and <kbd>dense_3</kbd>) have a lot sharper peak at zero in the L2 regularization scenario when compared to the no regularization scenario.</p>
<p>To further understand the peak distributions, we will modify the lambda value and give regularization a higher weightage of 0.1 instead of 001 and see how the weights look:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f084eba1-7195-4b34-98bb-396d7b514043.png"/></div>
<p>Note that, with a higher weightage given to the regularization term, weights have a much sharper distribution around the center (a value of 0).</p>
<p>Also, you should notice that the kernel is <kbd>dense_4</kbd> and is not changed by a lot, as we did not apply regularization at this layer.</p>
<p>From the preceding points we conclude that, by implementing L2 regularization, we can reduce the over-fitting issue that we see when there is no regularization.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing L1 regularization</h1>
                </header>
            
            <article>
                
<p>L1 regularization works in a similar way as that of L2; however, the cost function for L1 regularization is different than L2 regularization, as follows:</p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/9a15e389-ccb9-4097-984b-4a9d79d2bcba.png" style="width:27.50em;height:5.08em;"/></div>
<div class="packt_infobox">Note that, in the preceding equation, all the terms remain the same; just the regularization term is the summation of absolute values of weights than squared values of weights.</div>
<p>Let's implement the L1 regularization in code; now we see the corresponding outputs as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/54e4fb02-51be-487d-a0d9-a4bbf6682839.png"/></div>
<p>Note that, as the regularization term does not involve squaring in L1 regularization, we may have to lower the lambda value in L1 when compared to L2 (given most of the weights are less than one, squaring them would make the weight values even smaller).</p>
<p>Post-defining the model (this time with regularization), we fit it, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b8c44bd0-f88f-47af-8d96-c715a0b001f8.png"/></div>
<p>The preceding code fit results in the accuracy on train and test datasets, as per our expectation, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/433b5c22-116a-4544-a484-2cd92929b8b8.png" style=""/></div>
<p>Let's also look into the weight distribution across the layers in the <span class="packt_screen">Histograms</span> tab:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cb992e90-c72a-4bbc-aff0-7327f8ef4523.png" style=""/></div>
<p>We should note that, the kernel distribution here is similar to the kernel distribution when the lambda value of the L2 regularization was high.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing dropout</h1>
                </header>
            
            <article>
                
<p>Another way to reduce overfitting is by implementing the dropout technique. While performing weight updates in a typical back propagation, we ensure that some random portion of the weights are left out from updating the weights in a given epoch—hence the name dropout.</p>
<p>Dropout as a technique can also help in reducing overfitting, as reduction in the number of weights that need to be updated in a single epoch results in less chance that the output depends on few input values.</p>
<p>Dropout can be implemented as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/df7d0745-e877-446d-aa79-c679ca7b1568.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/023c7d4b-fbe1-4b85-b5cc-af57587abf9a.png"/></div>
<p>The result of the model fit is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f711d1e4-d7bf-4fb6-8d10-dcaf33f095c8.png" style=""/></div>
<p>You should note that dropout in the given configuration has resulted in a slightly broad distribution of weights when compared to the no regularization scenario:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6b30b541-925b-4466-9d07-5e78387e8b96.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducing underfitting</h1>
                </header>
            
            <article>
                
<p>Underfitting typically happens when:</p>
<ul>
<li>The model is extremely complex and is run for fewer epochs</li>
<li>The data is not normalized</li>
</ul>
<p><strong>Scenario 2: Underfitting in action on the MNIST dataset</strong></p>
<p>In the following scenario, we see the case of underfitting in action on the MNIST dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f7260cbc-bab0-4c15-ae6b-7a48d40255d5.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1bbeec24-6d34-49c3-b32f-655d258e79e4.png" style=""/></div>
<p>Note that, in the preceding code, we have not scaled our data—the training and test dataset columns have values ranging from 0 to 255:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a685d65a-fbc3-4c17-a157-923532de8e01.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/735eefa4-1f2b-45e4-8236-c853ac466c30.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b4a0932f-f7f9-44c1-a306-0b5ca8e91011.png"/></div>
<p>The TensorBoard visualization of accuracy and loss on train and test datasets for the preceding model is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/522b8ce1-dfed-4ea7-bf30-6cdfa835e8df.png" style=""/></div>
<p>Note that, in the preceding charts, both the loss and the accuracy of the training dataset have hardly changed (note the <em>y</em> axis values in both).</p>
<p>This scenario (where the loss hardly changes), typically happens when the input has numbers that are very high (typically &gt;5).</p>
<p>The preceding can be rectified by performing any of the following:</p>
<ul>
<li>Scaling the data</li>
<li>Batch normalization</li>
</ul>
<p>Scaling the data is as simple as repeating the preceding architecture, but with the small modification of scaling the train and test datasets:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/41fe7b3c-541e-4d7e-99f7-d5c55518c983.png" style=""/></div>
<p>Batch normalization can be performed (even on an unscaled MNIST dataset) as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/85ead417-162b-400b-9e8c-b871b1c57a81.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d6668e63-cf79-4284-9b18-b301c8f9b51a.png"/></div>
<p>A visualization of training and test accuracies can be seen as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0dab88fe-6f7a-491d-92c7-f313c7962fc3.png" style=""/></div>
<p>In the preceding scenario, we see that, even on unscaled dataset, the test accuracy is quite high.</p>
<p><strong>Scenario 3: Incorrect weight initialization</strong></p>
<p>Just like the previous scenario, it is highly likely that we will encounter an underfitting scenario if the weights are not initialized properly (even if the dataset is a properly scaled dataset). For example, in the following code, we initialize all the weights (kernels) to zero and then notice the accuracy on the test dataset:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f8e06ae8-7d32-4846-b1f2-c39b9e784415.png" style=""/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ea349dcc-c086-4e33-8bb3-ee8ee01a54ff.png"/></div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/516e7457-73df-4d0e-a2cf-4c08d1efa8fa.png"/></div>
<p>The output of the preceding code results in the following TensorBoard visualization:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5e2789c5-d820-4ec6-bf07-0aae69762961.png" style=""/></div>
<p>Similar to <em>scenario 2</em>, the preceding charts indicate that there is no learning that is happening through the preceding defined architecture.</p>
<p>No learning is happening, as the weights are initialized to zero.</p>
<p>It is advisable to initialize the weights to normal initialization. The other initializations that can be tried out to test whether accuracy could improve are:</p>
<ul>
<li><kbd>glorot_normal</kbd></li>
<li><kbd>lecun_uniform</kbd></li>
<li><kbd>glorot_uniform</kbd></li>
<li><kbd>he_normal</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have seen the characteristics of over fitting and how they can be handled through L1 and L2 regularizations, and dropout. Similarly, we have seen the scenario where there was quite a lot of underfitting and how scaling or batch normalization helped us in improving the under-fitting scenario.</p>


            </article>

            
        </section>
    </body></html>