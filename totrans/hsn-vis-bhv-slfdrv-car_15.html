<html><head></head><body><div><div><h1 id="_idParaDest-248"><a id="_idTextAnchor267"/>Assessments</h1>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor268"/>Chapter 1</h1>
			<ol>
				<li value="1">Yes, though in some cases, you might need a custom build.</li>
				<li>Usually <code>bilateralFilter()</code>.</li>
				<li>The HOG detector.</li>
				<li>Using <code>VideoCapture()</code>.</li>
				<li>A bigger aperture increases the amount of light available to the sensor, but reduces the depth of field.</li>
				<li>You need a higher ISO when there is not enough light for your required shutter speed and aperture settings.</li>
				<li>Yes, sub-pixel precision improves the calibration in a noticeable way.</li>
			</ol>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor269"/>Chapter 2</h1>
			<ol>
				<li value="1">For <strong class="bold">UART</strong>: <strong class="bold">Single-ended</strong>: Two wires (data and ground). There is no clock line needed since it is asynchronous and devices keep their own time and agree ahead of time on the baud rate. <strong class="bold">Differential</strong>: Two wires (data high and data low). The differential voltage is measured instead of voltage versus ground.<p><strong class="bold">I2C</strong>: Two wires, <strong class="bold">serial clock</strong> (<strong class="bold">SCL</strong>) and <strong class="bold">serial data</strong> (<strong class="bold">SDA</strong>), using a bus architecture with master and slave devices.</p><p><strong class="bold">SPI</strong>: 3 + 1<em class="italic">n</em> wires, where <em class="italic">n</em> is the number of slave devices. Three primary wires: <strong class="bold">signal clock</strong> (<strong class="bold">SCLK</strong>), <strong class="bold">master out slave in</strong> (<strong class="bold">MOSI</strong>), and <strong class="bold">master in slave out </strong>(<strong class="bold">MISO</strong>); and one <strong class="bold">slave select</strong> (<strong class="bold">SS</strong>) wire for each slave device.</p><p><strong class="bold">CAN</strong>: Two wires, CAN-HI and CAN-LO, using a bus architecture with CAN-HI and CAN-LO acting as a differential pair.</p></li>
				<li>Noise can be reduced by using a differential pair where noise affects both signals similarly. The wires are twisted around each other to cancel any induced currents.</li>
				<li>Serial transmission sends all bits across a single wire one after the other, serially. Parallel transmission sends each bit on its own wire simultaneously. So, for an 8-bit word, parallel transmission will be 8x faster.</li>
				<li>I2C, SPI, CAN, and Ethernet.</li>
				<li>I2C and SPI.</li>
				<li>UART.</li>
			</ol>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor270"/>Chapter 3</h1>
			<ol>
				<li value="1">HLS, HSV, LAB, and YcbCr.</li>
				<li>To get a bird's-eye view of the lanes, so that they are parallel also in the image.</li>
				<li>Using a histogram.</li>
				<li>Sliding window.</li>
				<li>Using <code>polyfit()</code>, and then using the coefficients to draw the line.</li>
				<li><code>Scharr()</code> works well.</li>
				<li>The <strong class="bold">exponentially weighted moving average</strong> is simple and effective.</li>
			</ol>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor271"/>Chapter 4</h1>
			<ol>
				<li value="1">It is a neuron in a neural network.</li>
				<li>Adam.</li>
				<li>It is an operation that applies a kernel to some pixels, obtaining a new pixel as a result.</li>
				<li>It is a neural network with at least one convolutional layer.</li>
				<li>It is a layer connecting all the neurons of one layer to all the neurons of the previous layer.</li>
				<li>It linearizes the 2D output of a convolutional layer, to make it possible to use the output with a dense layer.</li>
				<li>TensorFlow.</li>
				<li>LeNet.</li>
			</ol>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor272"/>Chapter 5</h1>
			<ol>
				<li value="1">You do what you have to do... but ideally, you want to use it only once, in order to avoid bias in your choice of model.</li>
				<li>It is the process of generating more data out of the initial dataset, to increase its size and improve the generalization of the network.</li>
				<li>Not exactly: Keras replaces the original images with the ones coming from data augmentation.</li>
				<li>In general, the dense layers tend to be the ones with the most parameters; in particular, the first one after the last convolutional layer is usually by far the biggest.</li>
				<li>While the line of the training loss goes down with the number of epochs, the validation loss goes up.</li>
				<li>Not always: you can use a strategy to first overfit the network (to properly learn the training dataset), and then to improve generalization and remove the overfitting.</li>
			</ol>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor273"/>Chapter 6</h1>
			<ol>
				<li value="1">To increase the number of non-linear activations and let the network learn more complex functions.</li>
				<li>Not necessarily. In fact, a well-crafted deep network can be faster and more precise.</li>
				<li>When the training accuracy increases but the validation accuracy decreases.</li>
				<li>Early stopping.</li>
				<li>With batch normalization.</li>
				<li>Using data augmentation.</li>
				<li>Because it learns not to rely on only a few channels.</li>
				<li>The training will probably be slower.</li>
				<li>The training will probably be slower.</li>
			</ol>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor274"/>Chapter 7</h1>
			<ol>
				<li value="1">SSD is a neural network able to find multiple objects in an image, and its output includes the position of the object detected. It can work in real time.</li>
				<li>Inception is an influential and precise neural network created by Google.</li>
				<li>A layer that has been frozen cannot be trained.</li>
				<li>No, it cannot. It can only detect traffic lights, but not their color.</li>
				<li>Transfer learning is the process of taking a neural network trained on a task and adapting it to solve a new, related task.</li>
				<li>Adding dropout, increasing the size of the dataset, increasing the variety of data augmentation, and adding batch normalization.</li>
				<li>Given the variety of images in ImageNet, it's difficult to choose the kernel size of the convolutional layers, so they used kernels of multiple sizes in parallel.</li>
			</ol>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor275"/>Chapter 8</h1>
			<ol>
				<li value="1">DAVE-2, but it can also be called DriveNet.</li>
				<li>In a classification task, the image is classified according to some pre-defined categories, while in a regression task we generate a continuous prediction; in our case, for example, a steering angle between â€“1 and 1.</li>
				<li>You can use the <code>yield</code> keyword.</li>
				<li>It is a visualization tool that can help us understand where the neural network is focusing its attention.</li>
				<li>We need three video streams to help the neural network understand how to correct wrong positions, as the side cameras are effectively corrections from positions far from the center of the car.</li>
				<li>For performance reasons, and to be sure that all the code is running only on the client.</li>
			</ol>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor276"/>Chapter 9</h1>
			<ol>
				<li value="1">The dense blocks, where each layer is connected to all the output of the previous layers, including the input.</li>
				<li>ResNet.</li>
				<li>It is an adaptation of DenseNet for performing semantic segmentation.</li>
				<li>Because it can be visualized like a U, with the left side downsampling and the right side upsampling.</li>
				<li>No, you can just stack a series of convolutions. However, due to the high resolution, achieving a good result will be challenging, and most likely would use a lot of memory and be quite slow.</li>
				<li>You can use a median blur to remove the <em class="italic">salt and pepper</em> noise that can be present in the segmentation masks of poorly trained networks.</li>
				<li>They are used to propagate high-resolution channels and help the network achieve a good real resolution.</li>
			</ol>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor277"/>Chapter 10</h1>
			<ol>
				<li value="1">PID, since only simple algebraic equations are being solved. Recall that MPC needs to solve a multivariate optimization in real time, which requires very high levels of processing power to ensure a low enough latency for driving.</li>
				<li>The integral term in PID corrects for any steady-state bias in the system by applying a control input based on the accumulated errors of the system.</li>
				<li>The derivative term in PID corrects for overshooting the setpoint by adjusting the control input based on the time rate of change of the error.</li>
				<li>A cost is used to assign a value to a trajectory where that value is minimized. Example costs are the cost of collisions, the cost of sequential actuations, the cost of using actuators, and the cost of not being at the destination.<p>A constraint is a physical limit of the system, such as turn radius, maximum lateral and longitudinal acceleration, vehicle dynamics, and maximum steering angle.</p></li>
			</ol>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor278"/>Chapter 11</h1>
			<ol>
				<li value="1">Mapping seeks to store information about navigable space in the environment, whereas localization seeks to determine where a robot is within the environment.</li>
				<li><code>odom_frame</code>.</li>
				<li>SLAM is needed because a map will never have fully current information about the environment. Therefore, you need to constantly be creating a map of navigable space as you go. SLAM also provides a method for mapping environments without expensive high-accuracy IMU equipment.</li>
				<li><code>assets_writer_cart_3d.lua</code>.</li>
			</ol>
		</div>
	</div>



  </body></html>