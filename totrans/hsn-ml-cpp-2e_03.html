<html><head></head><body>
		<div id="_idContainer119">
			<h1 class="chapter-number" id="_idParaDest-63"><a id="_idTextAnchor152"/>3</h1>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor153"/>Measuring Performance and Selecting Models</h1>
			<p>This chapter describes bias and variance effects and their pathological cases, which usually appear when training <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) models.</span></p>
			<p>In this chapter, we will learn how to deal with overfitting by using regularization and discuss different techniques we can use. We will also consider different model performance estimation metrics and how they can be used to detect training problems. Toward the end of this chapter, we will look at how to find the best hyperparameters for a model by introducing the grid search technique and its implementation <span class="No-Break">in C++.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Performance metrics for <span class="No-Break">ML models</span></li>
				<li>Understanding bias and <span class="No-Break">variance characteristics</span></li>
				<li>Model selection with the grid <span class="No-Break">search technique</span><a id="_idTextAnchor154"/></li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor155"/>Technical requirements</h1>
			<p>For this chapter, you will need <span class="No-Break">the following:</span></p>
			<ul>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>CMake build system version &gt;= <span class="No-Break">3.10</span></li>
				<li><span class="No-Break"><strong class="source-inline">Dlib</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">mlpack</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">Flashlight</strong></span><span class="No-Break"> library</span></li>
				<li><span class="No-Break"><strong class="source-inline">Plotcpp</strong></span><span class="No-Break"> library</span></li>
			</ul>
			<p>The code files for this chapter can be found in the following GitHub <span class="No-Break">repo: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/tree/main/Chapter03</span><span id="_idTextAnchor156"/></a></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor157"/>Performance metrics for ML models</h1>
			<p>When we develop or implement a particular ML algorithm, we need to estimate how well it works. In other words, we<a id="_idIndexMarker300"/> need to estimate how well it solves our task. Usually, we use some numeric metrics for algorithm performance estimation. An example of such a metric could be a value of <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) that’s been <a id="_idIndexMarker301"/>calculated for target and predicted values. We can use this value to estimate how <a id="_idIndexMarker302"/>distant our predictions are from the target values we used for training. Another use case for performance metrics is their use as objective functions in optimization processes. Some performance metrics are used for manual observations, though others can be used for optimization <span class="No-Break">purposes too.</span></p>
			<p>Performance metrics are different for each of the ML algorithm types. In <a href="B19849_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introduction to Machine Learning with C++</em>, we discussed that two main categories of ML algorithms exist: <strong class="bold">regression algorithms</strong> and <strong class="bold">classification algorithms</strong>. There are <a id="_idIndexMarker303"/>other types of <a id="_idIndexMarker304"/>algorithms in the ML discipline, but these two are the most common ones. This section will go over the most popular performance metrics for regression and <span class="No-Break">classification algori<a id="_idTextAnchor158"/>t<a id="_idTextAnchor159"/>hms.</span></p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor160"/>Regression metrics</h2>
			<p>Regression task metrics are used to<a id="_idIndexMarker305"/> measure how close predicted values are to ground truth ones. Such measurements can help us estimate the prediction quality of the algorithm. Under regression metrics, there are four main metrics, which we will dive into in the <span class="No-Break">following subsec<a id="_idTextAnchor161"/>t<a id="_idTextAnchor162"/>ions.</span></p>
			<h3>MSE and RMSE</h3>
			<p>MSE is a widely used <a id="_idIndexMarker306"/>metric for regression algorithms to<a id="_idIndexMarker307"/> estimate their quality. It is an average squared difference between the predictions and ground truth values. This is given by the <span class="No-Break">following equation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer077">
					<img alt="" role="presentation" src="image/B19849_Formula_011.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_021.png"/> is the number of predictions and ground truth items, <img alt="" role="presentation" src="image/B19849_Formula_031.png"/> is the ground truth value for the <em class="italic">i</em><span class="superscript">th</span> item, and <img alt="" role="presentation" src="image/B19849_Formula_041.png"/> is the prediction value for the <span class="No-Break"><em class="italic">i</em></span><span class="No-Break"><span class="superscript">th</span></span><span class="No-Break"> item.</span></p>
			<p>MSE is often used as<a id="_idIndexMarker308"/> a target loss function for optimization algorithms because it is smoothly differentiable and is a <span class="No-Break">convex function.</span></p>
			<p>The <strong class="bold">root mean squared error</strong> (<strong class="bold">RMSE</strong>) metric is usually used to estimate performance, such as when we need<a id="_idIndexMarker309"/> to give bigger weights to higher <a id="_idIndexMarker310"/>errors (to penalize them). We can interpret this as the standard deviation of the differences between predictions and ground truth values. This is given by the <span class="No-Break">following equation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="" role="presentation" src="image/B19849_Formula_051.jpg"/>
				</div>
			</div>
			<p>To calculate MSE with the <strong class="source-inline">Dlib</strong> library, there exists the <strong class="source-inline">mean_squared_error</strong> function, which takes two floating-point vectors and returns <span class="No-Break">the MSE.</span></p>
			<p>The <strong class="source-inline">mlpack</strong> library provides the <strong class="source-inline">MeanSquaredError</strong> class with the static <strong class="source-inline">Evaluate</strong> function that runs an algorithm prediction and calculates <span class="No-Break">the MSE.</span></p>
			<p>The <strong class="source-inline">Flashlight</strong> library also has the <strong class="source-inline">MeanSquaredError</strong> class; objects to this class can be used as a loss function so that it has forward and backward functions. Also, this library has the <strong class="source-inline">MSEMeter</strong> class that measures the MSE between targets and predictions and can be used for <span class="No-Break">performance trac<a id="_idTextAnchor163"/>king.</span></p>
			<h3>Mean absolute error</h3>
			<p><strong class="bold">Mean absolute error</strong> (<strong class="bold">MAE</strong>) is another popular metric that’s used for quality estimation for regression<a id="_idIndexMarker311"/> algorithms. The MAE metric is a linear function with equally weighted prediction errors. It means that it does not take into <a id="_idIndexMarker312"/>account the direction of errors, which can be problematic in some cases. For example, if a model consistently underestimates or overestimates the true value, the MAE will still give a low score, even though the model may not be performing well. But this metric is more robust for outliers than RMSE. It is given by the <span class="No-Break">following equation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="" role="presentation" src="image/B19849_Formula_061.jpg"/>
				</div>
			</div>
			<p>We can use the <strong class="source-inline">MeanSquaredError</strong> class in the <strong class="source-inline">Flashlight</strong> library to calculate this type of error. The <strong class="source-inline">MeanSquaredError</strong> class implements the loss functionality so that it has the forward/backward functions. Unfortunately, there is no specific functionality for the<a id="_idIndexMarker313"/> MAE calculation in the <strong class="source-inline">Dlib</strong> and <strong class="source-inline">mlpack</strong> libraries, but it can be easily implemented with their linear <span class="No-Break">algebra</span><span class="No-Break"><a id="_idIndexMarker314"/></span><span class="No-Break"> b<a id="_idTextAnchor164"/>ackends.</span></p>
			<h3>R-squared</h3>
			<p>The R-squared metric is also known as a <strong class="bold">coefficient of determination</strong>. It is used to measure how well our independent <a id="_idIndexMarker315"/>variables (features from the training set) describe<a id="_idIndexMarker316"/> the problem and explain the variability of<a id="_idIndexMarker317"/> dependent variables (prediction values). Higher values tell us that the model explains our data well enough, while lower values tell us that the model makes many errors. This is given by the <span class="No-Break">following equations:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="" role="presentation" src="image/B19849_Formula_071.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer084">
					<img alt="" role="presentation" src="image/B19849_Formula_081.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer085">
					<img alt="" role="presentation" src="image/B19849_Formula_092.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_10.png"/> is the number of predictions and ground truth items, <img alt="" role="presentation" src="image/B19849_Formula_111.png"/> is the ground truth value for the <em class="italic">i</em><span class="superscript">th</span> item, and <img alt="" role="presentation" src="image/B19849_Formula_122.png"/> is the prediction value for the <span class="No-Break"><em class="italic">i</em></span><span class="No-Break"><span class="superscript">th</span></span><span class="No-Break"> item.</span></p>
			<p>The only problem with this metric is that adding new independent variables may increase R-squared in some cases, so it’s crucial to consider the quality and relevance of these variables and to avoid overfitting the model. It may seem that the model begins to explain data better, but this isn’t true— this value only increases if there are more <span class="No-Break">training items.</span></p>
			<p>There are no out-of-the-box functions for calculating this metric in the <strong class="source-inline">Flashlight</strong> library; however, it is simple to implement it with linear <span class="No-Break">algebra functions.</span></p>
			<p>There is the <strong class="source-inline">r_squared</strong> function in the <strong class="source-inline">Dlib</strong> library for computing the R-squared coefficient between matching elements of two <span class="No-Break"><strong class="source-inline">std::vector</strong></span><span class="No-Break"> instances.</span></p>
			<p>The <strong class="source-inline">m</strong><strong class="source-inline">lpack</strong> library has the <strong class="source-inline">R2Score</strong> class, which has the static <strong class="source-inline">Evaluate</strong> function that runs prediction for the <a id="_idIndexMarker318"/>specified algorithm and calculates the <span class="No-Break">R-squared<a id="_idTextAnchor165"/> <a id="_idTextAnchor166"/>error.</span></p>
			<h3>Adjusted R-squared</h3>
			<p>The adjusted R-squared metric was designed to solve the previously described problem of the R-squared metric. It is the <a id="_idIndexMarker319"/>same as the R-squared metric but<a id="_idIndexMarker320"/> with a penalty for a large number of independent variables. The main idea is that if new independent variables improve the model’s quality, the values of this metric increase; otherwise, they decrease. This can be given by the <span class="No-Break">following equation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer089">
					<img alt="" role="presentation" src="image/B19849_Formula_13.jpg"/>
				</div>
			</div>
			<p>Here, <em class="italic">k</em> is the number of parameters and <em class="italic">n</em> is the number <span class="No-Break">of <a id="_idTextAnchor167"/>s<a id="_idTextAnchor168"/>amples.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor169"/>Classification metrics</h2>
			<p>Before we start discussing classification metrics, we have to introduce an important concept called the <strong class="bold">confusion matrix</strong>. Let’s assume that we have two classes and an algorithm that <a id="_idIndexMarker321"/>assigns them to an object. Here, the<a id="_idIndexMarker322"/> confusion matrix will look <span class="No-Break">like this:</span></p>
			<table class="No-Table-Style" id="table001-2">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><img alt="" role="presentation" src="image/B19849_Formula_141.png"/></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><img alt="" role="presentation" src="image/B19849_Formula_15.png"/></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><img alt="" role="presentation" src="image/B19849_Formula_161.png"/></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">True </strong><span class="No-Break"><strong class="bold">positive</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">TP</strong></span><span class="No-Break">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">False</strong> <span class="No-Break"><strong class="bold">positive</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FP</strong></span><span class="No-Break">)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US"><img alt="" role="presentation" src="image/B19849_Formula_17.png"/></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">False </strong><span class="No-Break"><strong class="bold">negative</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FN</strong></span><span class="No-Break">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">True </strong><span class="No-Break"><strong class="bold">negative</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">TN</strong></span><span class="No-Break">)</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_18.png"/> is the predicted class of the object and <img alt="" role="presentation" src="image/B19849_Formula_19.png"/> is the ground truth label. The confusion matrix is an abstraction<a id="_idIndexMarker323"/> that we use to calculate different classification metrics. It gives us the number of items that were classified correctly and misclassified. It also provides us with information about the misclassification type. The false negatives are items that our algorithm incorrectly classified as negative ones, while the false positives are items that our algorithm incorrectly classified as positive ones. In this section, we’ll learn how to use this matrix and calculate different classification <span class="No-Break">performance metrics.</span></p>
			<p>A confusion matrix<a id="_idIndexMarker324"/> can be created in the <strong class="source-inline">mlpack</strong> library with the <strong class="source-inline">ConfusionMatrix</strong> function; this function works only for discrete <span class="No-Break">data/categorical data.</span></p>
			<p>The <strong class="source-inline">Dlib</strong> library also has instruments to get a confusion matrix. There is the <strong class="source-inline">test_multiclass_decision</strong> function that tests a multi-class decision function and returns a confusion matrix <a id="_idIndexMarker325"/>describing the results. It also has the <strong class="source-inline">test_sequence_labeler</strong> class that tests the <strong class="source-inline">labeler</strong> object against the given samples and labels and returns a confusion matrix summarizing <span class="No-Break">the results<a id="_idTextAnchor170"/>.</span><a id="_idTextAnchor171"/></p>
			<h3>Accuracy</h3>
			<p>One of the most obvious classification metrics <span class="No-Break">is accuracy:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer096">
					<img alt="" role="presentation" src="image/B19849_Formula_20.jpg"/>
				</div>
			</div>
			<p>This provides us with a<a id="_idIndexMarker326"/> ratio of all positive predictions to all others. In general, this metric is not very useful because it doesn’t show us the real picture in terms of cases with an odd number of classes. Let’s consider a spam classification task and assume we have 10 spam letters and 100 non-spam letters. Our algorithm predicted 90 of them correctly as non-spam and classified only 5 spam letters correctly. In this case, accuracy will have the <span class="No-Break">following value:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer097">
					<img alt="" role="presentation" src="image/B19849_Formula_21.jpg"/>
				</div>
			</div>
			<p>However, if the algorithm predicts all letters as non-spam, then its accuracy should be <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="" role="presentation" src="image/B19849_Formula_22.jpg"/>
				</div>
			</div>
			<p>This example shows that our model still doesn’t work because it is unable to predict all spam letters, but the accuracy value is <span class="No-Break">good enough.</span></p>
			<p>To calculate accuracy in the <strong class="source-inline">Flashlight</strong> library, we use the <strong class="source-inline">FrameErrorMeter</strong> class that can estimate the accuracy or the error rate depending on <span class="No-Break">user settings.</span></p>
			<p>The <strong class="source-inline">mlpack</strong> library has the <strong class="source-inline">Accuracy</strong> class with the static <strong class="source-inline">Evaluate</strong> function that runs classification for the specified algorithm and calculates the <span class="No-Break">accuracy value.</span></p>
			<p>Unfortunately, the <strong class="source-inline">Dlib</strong> library <a id="_idIndexMarker327"/>doesn’t have functions to calculate accuracy values, so if needed, the function should be implemented with the linear <span class="No-Break">algebra backend.</span><a id="_idTextAnchor172"/></p>
			<h3><a id="_idTextAnchor173"/>Precision and recall</h3>
			<p>To estimate algorithm quality for <a id="_idIndexMarker328"/>each classification class, we will introduce two metrics: <strong class="bold">precision</strong> and <strong class="bold">recall</strong>. The following diagram <a id="_idIndexMarker329"/>shows all objects that<a id="_idIndexMarker330"/> are used in <a id="_idIndexMarker331"/>classification and how they have been marked according to the <span class="No-Break">algorithm’s results:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="Figure 3.1 – Precision and recall" src="image/B19849_03_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Precision and recall</p>
			<p>The circle in the center contains <em class="italic">selected elements</em>—the elements our algorithm predicted as <span class="No-Break">positive ones.</span></p>
			<p>Precision is proportional to<a id="_idIndexMarker332"/> the number of correctly classified items within selected ones that are defined <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer100">
					<img alt="" role="presentation" src="image/B19849_Formula_231.jpg"/>
				</div>
			</div>
			<p>Recall is proportional to<a id="_idIndexMarker333"/> the number of correctly classified items within all ground truth positive items that are defined <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer101">
					<img alt="" role="presentation" src="image/B19849_Formula_241.jpg"/>
				</div>
			</div>
			<p>Another name for recall is <strong class="bold">sensitivity</strong>. Let’s assume that we are interested in the detection of positive<a id="_idIndexMarker334"/> items—let’s call them relevant ones. So, we use the recall value as a measure of an algorithm’s ability to detect relevant items and the precision value as a measure of an algorithm’s ability to see the differences between classes. These measures do not depend on the number of objects in each of the classes, and we can use them for imbalanced <span class="No-Break">dataset classification.</span></p>
			<p>There are several approaches to calculate these metrics in the <strong class="source-inline">Dlib</strong> library. There is the <strong class="source-inline">average_precision</strong> function that can be used to calculate the precision value directly. Also, there is the <strong class="source-inline">test_ranking_function</strong> function that tests the given ranking function on the provided data and can return the mean average precision. Another way is to use the <strong class="source-inline">test_sequence_segmenter</strong> class that tests a <strong class="source-inline">segmenter</strong> object against the given samples and returns the precision, recall, and F1-score, where <strong class="source-inline">sequence_segmenter</strong> is an object for segmenting a sequence of objects into a set of <span class="No-Break">non-overlapping chunks.</span></p>
			<p>The <strong class="source-inline">mlpack</strong> library has two classes—<strong class="source-inline">Precision</strong> and <strong class="source-inline">Recall</strong>—with static <strong class="source-inline">Evaluate</strong> functions that run classification for the specified algorithm and calculate precision and <span class="No-Break">recall correspondingly.</span></p>
			<p>The <strong class="source-inline">Flashlight</strong> library doesn’t have the functionality to calculate <span class="No-Break">these val<a id="_idTextAnchor174"/>u<a id="_idTextAnchor175"/>es.</span></p>
			<h3>F-score</h3>
			<p>In many cases, it is useful to have only one metric that shows the classification’s quality. For example, it makes sense to use some algorithms to search for the best hyperparameters, such as the grid search algorithm, which will be discussed later in this chapter. Such algorithms<a id="_idIndexMarker335"/> usually use one metric to compare different classification results after applying various parameter values during the search process. One of<a id="_idIndexMarker336"/> the most popular metrics for this case is the F-measure (or the F-score), which can be given <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer102">
					<img alt="" role="presentation" src="image/B19849_Formula_251.jpg"/>
				</div>
			</div>
			<p>Here, <img alt="" role="presentation" src="image/B19849_Formula_26.png"/> is the precision metric weight. Usually, the <img alt="" role="presentation" src="image/B19849_Formula_27.png"/> value is equal to <em class="italic">1</em>. In such a case, we have the multiplier value equal to <em class="italic">2</em>, which gives us <img alt="" role="presentation" src="image/B19849_Formula_281.png"/> if the <em class="italic">precision</em> = <em class="italic">1</em> and the <em class="italic">recall</em> = <em class="italic">1</em>. In other cases, when the precision value or the recall value tends to be zero, the F-measure value will <span class="No-Break">also decrease.</span></p>
			<p>The <strong class="source-inline">Dlib</strong> library provides the F1-score calculation only in the scope of the <strong class="source-inline">test_sequence_segmenter</strong> class functionality, which tests a <strong class="source-inline">segmenter</strong> object against the given samples and returns the precision, recall, <span class="No-Break">and F1-score.</span></p>
			<p>There is the <strong class="source-inline">F1</strong> class in the <strong class="source-inline">mlpack</strong> library that has the <strong class="source-inline">Evaluate</strong> function, which can be used to calculate the F1-score value by running a classification with the specified algorithm <span class="No-Break">and data<a id="_idTextAnchor176"/>.</span></p>
			<p>The <strong class="source-inline">Flashlight</strong> library doesn’t have the functionality to calculate <span class="No-Break">F-score values<a id="_idTextAnchor177"/>.</span></p>
			<h3>AUC-ROC</h3>
			<p>Usually, a classification algorithm will not return a concrete class identifier but a probability of an object belonging to<a id="_idIndexMarker337"/> some class. So, we usually use a threshold to decide whether an object belongs to a class or not. The most apparent threshold is 0.5, but it can work incorrectly in the case of imbalanced data (when we have a lot of values for one class and significantly fewer for <span class="No-Break">another class).</span></p>
			<p>One of the methods we can <a id="_idIndexMarker338"/>use to estimate a model without the actual threshold is the value of the <strong class="bold">Area Under the Receiver Operating Characteristic Curve</strong> (<strong class="bold">AUC-ROC)</strong>. This curve is a line from (0,0) to (1,1) in<a id="_idIndexMarker339"/> coordinates of the <strong class="bold">True Positive Rate</strong> (<strong class="bold">TPR</strong>) and <a id="_idIndexMarker340"/>the <strong class="bold">False Positive </strong><span class="No-Break"><strong class="bold">Rate</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FPR</strong></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer106">
					<img alt="" role="presentation" src="image/B19849_Formula_291.jpg"/>
				</div>
			</div>
			<div>
				<div class="IMG---Figure" id="_idContainer107">
					<img alt="" role="presentation" src="image/B19849_Formula_30.jpg"/>
				</div>
			</div>
			<p>The TPR value is equal to the recall, while the FPR value is proportional to the number of objects of the negative class that were classified incorrectly (they should be positive). In an ideal case, when there are no classification errors, we have <strong class="source-inline">FPR = 0</strong>, <strong class="source-inline">TPR = 1</strong>, and the area under the <strong class="bold">receiver operating characteristic</strong> (<strong class="bold">ROC</strong>) curve will be equal to <strong class="source-inline">1</strong>. In the case of random <a id="_idIndexMarker341"/>predictions, the area under the ROC curve will be equal to <strong class="source-inline">0.5</strong> because we will have an equal number of TP and <span class="No-Break">FP classifications:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer108">
					<img alt="Figure 3.2 – ROC" src="image/B19849_03_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – ROC</p>
			<p>Each point on the curve corresponds to some threshold value. Notice that the curve’s steepness is an essential characteristic because we want to minimize the FPR, so we usually want this curve to<a id="_idIndexMarker342"/> tend to point (0,1). We can also successfully use the AUC-ROC metric with <span class="No-Break">imbalanced datasets.</span></p>
			<p>There is the <strong class="source-inline">compute_roc_curve</strong> function in the <strong class="source-inline">Dlib</strong> library that computes the ROC curve of the <span class="No-Break">given data.</span></p>
			<p>Unfortunately, the <strong class="source-inline">Flashlight</strong> and <strong class="source-inline">mlpack</strong> libraries don’t have the functionality to compute the <span class="No-Break">AUC-ROC me<a id="_idTextAnchor178"/>t<a id="_idTextAnchor179"/>ric.</span></p>
			<h3>Log-Loss</h3>
			<p>The logistic loss function<a id="_idIndexMarker343"/> value (the Log-Loss) is used as a target loss function for <a id="_idIndexMarker344"/>optimization purposes. It is given by the <span class="No-Break">following equation:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer109">
					<img alt="" role="presentation" src="image/B19849_Formula_31.jpg"/>
				</div>
			</div>
			<p>We can understand the Log-Loss value as the accuracy being corrected but with penalties for incorrect predictions. This function gives significant penalties, even for single misclassified objects, so all outlier objects in the data should be processed separately or removed from <span class="No-Break">the dataset.</span></p>
			<p>There is the <strong class="source-inline">loss_binary_log</strong> class in the <strong class="source-inline">Dlib</strong> library that implements the Log-Loss, which is appropriate for binary classification<a id="_idIndexMarker345"/> problems. This class is designed to be used as a <strong class="bold">neural network</strong> (<span class="No-Break"><strong class="bold">NN</strong></span><span class="No-Break">) module.</span></p>
			<p>The <strong class="source-inline">Flashlight</strong> library has the <strong class="source-inline">BinaryCrossEntropy</strong> class that computes the binary cross-entropy loss between an input tensor <strong class="source-inline">x</strong> and a target tensor <strong class="source-inline">y</strong>. Also, the main aim of this class is the loss function implementation for <span class="No-Break">NN training.</span></p>
			<p>The <strong class="source-inline">CrossEntropyError</strong> class in the <strong class="source-inline">mlpack</strong> library also represents a loss function for NN construction; it has forward and backward functions. So, it is used to measure the network’s performance according to the cross-entropy between the input and <span class="No-Break">target distributions.</span></p>
			<p>In the current section, we learned about performance estimation metrics that can give you a clearer picture of your model accuracy, precision, and other performance characteristics. In the following section, we will learn about bias and variance and how to estimate and fix model <span class="No-Break">prediction characteri<a id="_idTextAnchor180"/>stics.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor181"/>Understanding bias and variance characteristics</h1>
			<p>Bias and variance characteristics are <a id="_idIndexMarker346"/>used to predict model behavior. For example, the high variance<a id="_idIndexMarker347"/> effect, also<a id="_idIndexMarker348"/> known as <strong class="bold">overfitting</strong>, is a phenomenon in ML where the constructed model explains the examples from the training set but works relatively poorly on the examples that did not participate in the training process. This occurs because while training a model, random patterns will start appearing that are normally absent from the general population. The opposite of overfitting is known as <strong class="bold">underfitting</strong>, which corresponds to the high bias effect. This happens when the trained model becomes unable to predict patterns in new data or even in the training data. Such<a id="_idIndexMarker349"/> an effect can be the result of a limited training dataset or weak <span class="No-Break">model design.</span></p>
			<p>Before we go any further and describe what they mean, we should<a id="_idIndexMarker350"/> consider <strong class="bold">validation</strong>. Validation is a technique that’s used to test model performance. It estimates how well the model makes predictions on new data. New data is data that we did not use for the training process. To perform validation, we usually divide our initial dataset into two or three parts. One part should contain most of the data and will be used for training, while the other ones will be used to validate and test the model. Usually, validation is performed for<a id="_idIndexMarker351"/> iterative algorithms after one training cycle (often called an <strong class="bold">epoch</strong>). Alternatively, we perform testing after the overall <span class="No-Break">training process.</span></p>
			<p>Validation and testing operations evaluate the model on the data we have excluded from the training process, which results in the values of the performance metrics that we chose for this particular model. For example, the original dataset can be divided into the following parts: 80% for training, 10% for validation, and 10% for testing. The values of these validation metrics can be used to estimate model and prediction error trends. The most crucial issue for validation and testing is that the data for them should always be from the same distribution as the <span class="No-Break">training data.</span></p>
			<p>Throughout the rest of this chapter, we will use the polynomial regression model to show different prediction behaviors. The polynomial degree will be used as <span class="No-Break">a hy<a id="_idTextAnchor182"/>p<a id="_idTextAnchor183"/>erparameter.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor184"/>Bias</h2>
			<p>Bias is a prediction characteristic that tells us<a id="_idIndexMarker352"/> about the distance between model predictions and ground<a id="_idIndexMarker353"/> truth values. Usually, we use the term <em class="italic">high bias</em> or <em class="italic">underfitting</em> to say that model <a id="_idIndexMarker354"/>prediction is too far from the ground truth values, which means that the model generalization ability is weak. Consider the <span class="No-Break">following graph:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer110">
					<img alt="Figure 3.3 – Regression model predictions with the polynomial degree equal to 1" src="image/B19849_03_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Regression model predictions with the polynomial degree equal to 1</p>
			<p>This graph shows the original values, the values used for validation, and a line that represents the polynomial regression model output. In this case, the polynomial degree is equal to <em class="italic">1</em>. We can see that the predicted values do not describe the original data at all, so we can say<a id="_idIndexMarker355"/> that this model has <a id="_idIndexMarker356"/>a high bias. Also, we can plot validation metrics for each training cycle to get more information about the training process and the <span class="No-Break">model’s behavior.</span></p>
			<p>The following graph shows the MAE metric values for the training process of the polynomial regression model, where the polynomial degree is equal <span class="No-Break">to </span><span class="No-Break"><em class="italic">1</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer111">
					<img alt="Figure 3.4 – Train and validation loss values" src="image/B19849_03_4.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Train and validation loss values</p>
			<p>We can see that the lines for<a id="_idIndexMarker357"/> the metric values for the train and validation data are parallel and distant enough. Moreover, these lines do not change their direction after numerous training iterations. These facts also tell us that the model has a high bias because, for a regular training process, validation metric values should be close to the <span class="No-Break">training values.</span></p>
			<p>To deal with high bias, we can add more features to the training samples. For example, increasing the polynomial degree for the polynomial regression model adds more features; these all-new features describe the original training sample because each additional polynomial term is based on the origi<a id="_idTextAnchor185"/>n<a id="_idTextAnchor186"/>al <span class="No-Break">sample value.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor187"/>Variance</h2>
			<p>Variance is a prediction characteristic<a id="_idIndexMarker358"/> that tells us about the variability of model predictions; in other words, how big the range of output values can be. Usually, we use the term <em class="italic">high variance</em> or <em class="italic">overfitting</em> in the case when a model tries to incorporate many training samples very precisely. In such a case, the model cannot provide a good approximation for new data but has excellent performance on the <span class="No-Break">training data.</span></p>
			<p>The following graph shows the <a id="_idIndexMarker359"/>behavior of the polynomial regression model, with the polynomial degree equal <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">15</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer112">
					<img alt="Figure 3.5 – Regression model predictions with the polynomial degree equal to 15" src="image/B19849_03_5.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Regression model predictions with the polynomial degree equal to 15</p>
			<p>We can see that the model incorporates almost all the training data. Notice that the training data is indicated as <strong class="source-inline">orig</strong> in the plot’s legend, while the data used for validation is indicated as <strong class="source-inline">val</strong> in the plot’s legend. We can see that these two sets of data—training data and validation data—are somehow distant from each other and that our model misses the validation data because of a lack of approximation. The following graph shows the MAE values for the <span class="No-Break">learning process:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer113">
					<img alt="Figure 3.6 – Validation error" src="image/B19849_03_6.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Validation error</p>
			<p>We can see that after approximately 75 learning iterations, the model began to predict training data much better, and the error value became lower. However, for the validation data, the MAE values began to increase. To deal with high variance, we can use special regularization <a id="_idIndexMarker360"/>techniques, which we will discuss in the following sections. We can also increase the number of training samples and decrease the number of features in one sample to reduce <span class="No-Break">high variance.</span></p>
			<p>The performance metrics plots we discussed in the preceding paragraphs can be drawn at the runtime of the training process. We can use them to monitor the training process to see high bias or high variance problems. Notice that for the polynomial regression model, MAE is a better performance characteristic than MSE or RMSE because squared functions average errors too much. Moreover, even a straight-line model can have low MSE values for such data because errors from both sides of the line compensate for each other. The choice between MAE and MSE depends on the specific task and goals of the project. If the overall accuracy of predictions is important, then MAE may be preferable. But MAE gives equal weight to all errors, regardless of their magnitude. This means that it is not sensitive to outliers, which can significantly affect the overall error. So, if it is necessary to <a id="_idIndexMarker361"/>minimize large errors, then MSE can give more accurate results. In some cases, you can use both metrics to analyze the performance of the m<a id="_idTextAnchor188"/>o<a id="_idTextAnchor189"/>del in <span class="No-Break">more depth.</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor190"/>Normal training</h2>
			<p>Consider the case of a training process <a id="_idIndexMarker362"/>where the model has balanced bias <span class="No-Break">and variance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer114">
					<img alt="Figure 3.7 – Predictions when a model was trained ideally" src="image/B19849_03_7.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Predictions when a model was trained ideally</p>
			<p>In this graph, we can see that the polynomial regression model’s output for the polynomial degree is equal to eight. The output values are close to both the training data and validation data. The following graph shows the MAE values during the <span class="No-Break">training process:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer115">
					<img alt="Figure 3.8 – Loss values when a model was trained ideally" src="image/B19849_03_8.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Loss values when a model was trained ideally</p>
			<p>We can see that the MAE value decreases consistently and that the predicted values for the training and validation <a id="_idIndexMarker363"/>data become close to the ground truth values. This means that the model’s hyperparameters were good enough to balance<a id="_idTextAnchor191"/> <a id="_idTextAnchor192"/>bias <span class="No-Break">and variance.</span></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor193"/>Regularization</h2>
			<p>Regularization is a technique that’s <a id="_idIndexMarker364"/>used to reduce model overfitting. There are two main approaches to regularization. The first one is known as training data preprocessing. The second one<a id="_idIndexMarker365"/> is loss function modification. The main idea of the loss function modification<a id="_idIndexMarker366"/> technique is to add terms to the loss function that penalize algorithm results, thereby leading to significant variance. The idea of training data preprocessing techniques is to add more distinct training samples. Usually, in such an approach, new training samples are generated by augmenting existing ones. In general, both approaches add some prior knowledge about the task domain to the model. This additional information helps us with variance regularization. Therefore, we can conclude that regularization<a id="_idIndexMarker367"/> is any technique that leads to minimizing<a id="_idTextAnchor194"/> <a id="_idTextAnchor195"/><span class="No-Break">generalization errors.</span></p>
			<h3>L1 regularization – Lasso</h3>
			<p>L1 regularization is an additional term to the <span class="No-Break">loss function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer116">
					<img alt="" role="presentation" src="image/B19849_Formula_32.jpg"/>
				</div>
			</div>
			<p>This additional term <a id="_idIndexMarker368"/>adds the absolute value of the magnitude of parameters as a penalty. Here, <em class="italic">λ</em> is a regularization coefficient. Higher values of this coefficient lead to stronger regularization and can lead to underfitting. Sometimes, this type of regularization is called <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong> (<strong class="bold">Lasso</strong>) regularization. The<a id="_idIndexMarker369"/> general idea behind L1 regularization is to penalize less important features. We can think of it as a feature selection process because as the optimization proceeds, some of the coefficients (for example, in linear regression) become zero, indicating that those features are not contributing to the model’s performance. We end up with a sparse mo<a id="_idTextAnchor196"/>d<a id="_idTextAnchor197"/>el with <span class="No-Break">fewer features.</span></p>
			<h3>L2 regularization – Ridge</h3>
			<p>L2 regularization is also an additional term to the <span class="No-Break">loss function:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer117">
					<img alt="" role="presentation" src="image/B19849_Formula_331.jpg"/>
				</div>
			</div>
			<p>This additional term adds a squared<a id="_idIndexMarker370"/> value of the magnitude of parameters as a penalty. This penalty shrinks the magnitude of the parameters toward zero. <em class="italic">λ</em> is also a coefficient of <a id="_idIndexMarker371"/>regularization. Its higher values lead to stronger regularization and can lead to underfitting because the model becomes too constrained and unable to learn complex relationships in the data. Another name for this regularization type is <strong class="bold">ridge regularization</strong>. Unlike L1 regularization, this type does not have a feature selection characteristic. Instead, we<a id="_idIndexMarker372"/> can interpret it as a model smoothness configurator. In addition, L2 regularization is computationally more efficient for gradient descent-based optimizers because its differentiation has an <span class="No-Break">analytical solution.</span></p>
			<p>In the <strong class="source-inline">Dlib</strong> library, regularization mechanisms are usually integrated into algorithm implementations —for example, the <strong class="source-inline">rr_trainer</strong> class that represents a tool for performing linear ridge regression, which is<a id="_idIndexMarker373"/> the regularized <strong class="bold">least squares support vector </strong><span class="No-Break"><strong class="bold">machine</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LSSVM</strong></span><span class="No-Break">).</span></p>
			<p>There is the <strong class="source-inline">LRegularizer</strong> class in the <strong class="source-inline">mlpack</strong> library that implements a generalized L-regularizer, allowing both L1 and L2 regularization methods for NNs. Some algorithm implementations, such as the <strong class="bold">least angle regression</strong>, known as <strong class="bold">LARS</strong>, and <strong class="bold">linear regression</strong>, also<a id="_idIndexMarker374"/> have integrated regularization. An object of the <strong class="source-inline">LARS</strong> class can train a LARS/LASSO/Elastic Net model and has L1 and L2 regularization <a id="_idIndexMarker375"/>parameters. The <strong class="source-inline">LinearRegression</strong> class has a regularization parameter for <span class="No-Break">ridge regression.</span></p>
			<p>There is no standalone functionality in the <strong class="source-inline">Flashlight</strong> library for regularization. All regularizations are integrated into the NN optimizatio<a id="_idTextAnchor198"/>n <span class="No-Break">algorithm implementations.</span></p>
			<h3>Data augmentation</h3>
			<p>The data augmentation process can be<a id="_idIndexMarker376"/> treated as regularization because it adds some prior knowledge about the problem to the model. This approach is common in <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) tasks such<a id="_idIndexMarker377"/> as image classification or object detection. In such cases, when we can see that the model begins to overfit and<a id="_idIndexMarker378"/> does not have enough training data, we can augment the images we already have to increase the size of our dataset and provide more distinct training samples. Image augmentations are random image rotations, cropping and translations, mirroring flips, s<a id="_idTextAnchor199"/>caling, and proportion changes. But data augmentation should be carefully designed for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li>If the generated data is too similar to the original data, it can lead <span class="No-Break">to overfitting.</span></li>
				<li>It can introduce noise or artifacts into the dataset, which can degrade the quality of the <span class="No-Break">resulting models.</span></li>
				<li>The augmented data may not accurately reflect the real-world distribution of data, leading to a domain shift between the training and test sets. This can result in p<a id="_idTextAnchor200"/>oor <span class="No-Break">generalization performance.</span></li>
			</ul>
			<h3>Early stopping</h3>
			<p>Stopping the training process early can also be interpreted as a form of regularization. This means that if we detected that <a id="_idIndexMarker379"/>the model started to overfit, we can stop the training process. In this case, the model will have paramete<a id="_idTextAnchor201"/>r<a id="_idTextAnchor202"/>s once the training <span class="No-Break">has stopped.</span></p>
			<h3>Regularization for NNs</h3>
			<p>L1 and L2 regularizations are widely used to train NNs and are usually called <strong class="bold">weight decay</strong>. Data augmentation also plays<a id="_idIndexMarker380"/> an essential role in the training processes for NNs. Other regularization methods can be used in NNs. For example, Dropout is a particular type of<a id="_idIndexMarker381"/> regularization that was developed especially for NNs. This algorithm randomly drops some NN nodes; it makes other nodes more insensitive to the weights of other nodes, which means the model becomes more robust and <span class="No-Break">stops overfitting.</span></p>
			<p>In the following sections, we will see how to select model hyperparameters with automat<a id="_idTextAnchor203"/>e<a id="_idTextAnchor204"/>d algorithms versus <span class="No-Break">manual tuning.</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor205"/>Model selection with the grid search technique</h1>
			<p>It is necessary to have a set of proper hyperparameter values to create a good ML model. The reason for this is that<a id="_idIndexMarker382"/> having random values leads to controversial results and behaviors that are not expected by the practitioner. There are several approaches we can follow to choose the best set of hyperparameter values. We can try to use hyperparameters from algorithms we have already trained that are similar to our task. We can also try to find some heuristics and tune them manually. However, this task can be automated. The grid search technique is an automated approach for searching for the best hyperparameter values. It uses the cross-validation techniq<a id="_idTextAnchor206"/>u<a id="_idTextAnchor207"/>e for model <span class="No-Break">performance estimation.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor208"/>Cross-validation</h2>
			<p>We have already discussed what<a id="_idIndexMarker383"/> the validation process is. It is used to estimate the model’s performance data that we haven’t used for training. If we have a<a id="_idIndexMarker384"/> limited or small training dataset, randomly sampling the validation data from the original dataset leads to the <span class="No-Break">following problems:</span></p>
			<ul>
				<li>The size of the original dataset <span class="No-Break">is reduced</span></li>
				<li>There is the probability of leaving data that’s important for validation in the <span class="No-Break">training part</span></li>
			</ul>
			<p>To solve these problems, we can use the cross-validation approach. The main idea behind it is to split the <a id="_idIndexMarker385"/>original dataset in such a way that all the data will be used for training and validation. Then, the training and validation processes are performed for all partitions,<a id="_idTextAnchor209"/> and the resulting values <span class="No-Break">are averaged.</span></p>
			<p>The most well-known method of <a id="_idIndexMarker386"/>cross-validation is <em class="italic">K</em>-fold cross-validation, where K refers to the number of <a id="_idIndexMarker387"/>folds or partitions used to split the dataset The idea is to divide the dataset into <em class="italic">K</em> blocks of the same size. Then, we use one of the<a id="_idIndexMarker388"/> blocks for validation and the others for training. We repeat this process <em class="italic">K</em> times, each time choosing a different block for <a id="_idIndexMarker389"/>validation, and in the end, we average all the results. The data splitting scheme during the whole cross-validation cycle looks <span class="No-Break">like this:</span></p>
			<ol>
				<li>Divide the dataset into <em class="italic">K</em> blocks of the <span class="No-Break">same size.</span></li>
				<li>Select one of the blocks for validation and the remaining <em class="italic">K</em>-1 blocks <span class="No-Break">for training.</span></li>
				<li>Repeat this process, making sure that each block is used for validation and the rest are used <span class="No-Break">for training.</span></li>
				<li>Average the results of the performance metrics that were calculated for the validation sets on <span class="No-Break">each iteration.</span></li>
			</ol>
			<p>The following diagram shows the <span class="No-Break">cross-validation cycl<a id="_idTextAnchor210"/>e<a id="_idTextAnchor211"/>:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer118">
					<img alt="Figure 3.9 – K-fold validation scheme" src="image/B19849_03_9.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – K-fold validation scheme</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor212"/>Grid search</h2>
			<p>The main idea behind the grid search approach is to create a grid of the most reasonable hyperparameter <a id="_idIndexMarker390"/>values. The grid is used to generate a reasonable number of distinct parameter sets quickly. We should have some prior knowledge about the task domain to initialize the minimum and maximum values for grid generation, or we can initialize the grid with some reasonable broad ranges. However, if the chosen ranges are too broad, the process of searching for parameters can take a long time and will require a significant amount<a id="_idIndexMarker391"/> of <span class="No-Break">computational resources.</span></p>
			<p>At each step, the grid search algorithm chooses a set of hyperparameter values and trains a model. After that, the training step algorithm uses the K-fold cross-validation technique to estimate model performance. We should also define a single model performance estimation metric for model comparison that the algorithm will calculate at each training step for every model. After completing the model training process with each set of parameters from every grid cell, the algorithm chooses the best set of hyperparameter values by comparing the metric’s values and selecting the best one. Usually, the set with the smallest value is the <span class="No-Break">best one.</span></p>
			<p>Consider an implementation <a id="_idIndexMarker392"/>of this algorithm in different libraries. Our task is to select the best set of hyperparameters for the polynomial regression model, which gives us the best curve that fits the <a id="_idIndexMarker393"/>given data. The data in this example is some<a id="_idTextAnchor213"/> c<a id="_idTextAnchor214"/>osine function values with some <span class="No-Break">random noise.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor215"/>mlpack example</h2>
			<p>The <strong class="source-inline">mlpack</strong> library contains a special <strong class="source-inline">HyperParameterTuner</strong> class to do hyperparameter searches with different algorithms in both discrete and continuous spaces. The default search algorithm is grid search. This class is a template and should be<a id="_idIndexMarker394"/> specialized for a concrete task. The general definition is <span class="No-Break">the following:</span></p>
			<pre class="source-code">
template&lt;typename MLAlgorithm, typename Metric,
    typename CV,...&gt;
class HyperParameterTuner{…};</pre>			<p>We can see that the main template parameters are the algorithm that we want to find, hyperparameters for the performance metric, and the cross-validation algorithm. Let’s define a <strong class="source-inline">HyperParameterTuner</strong> object to search for the best regularization value for the linear ridge regression algorithm. The definition will be <span class="No-Break">the following:</span></p>
			<pre class="source-code">
double validation_size = 0.2;
HyperParameterTuner&lt;LinearRegression, 
  MSE, 
  SimpleCV&gt; parameters_tuner(
    validation_size, samples, labels);</pre>			<p>Here, <strong class="source-inline">LinearRegression</strong> is the target algorithm class, <strong class="source-inline">MSE</strong> is the performance metric class that calculates the MSE, and <strong class="source-inline">SimpleCV</strong> is the class that implements cross-validation. It splits data into two sets, training and validation, and then runs training on the training set and evaluates performance on the validation set. Also, we see that we pass the <strong class="source-inline">validation_size</strong> parameter into the constructor. It has a value of 0.2, which means usage of 80% of data for training and the remaining 20% for evaluation with MSE. The two following constructor parameters are our training dataset; <strong class="source-inline">samples</strong> are just samples, and <strong class="source-inline">labels</strong> are <span class="No-Break">corresponding labels.</span></p>
			<p>Let’s see how we can generate a training dataset for these examples. It will take the two <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Generating data that follows some predefined pattern—for example, 2D normally distributed points, plus<a id="_idIndexMarker395"/> <span class="No-Break">some noise</span></li>
				<li><span class="No-Break">Data normalization</span></li>
			</ol>
			<p>The following sample shows how to generate data using the Armadillo library, which is the <strong class="source-inline">mlpack</strong> <span class="No-Break">mathematical </span><span class="No-Break"><a id="_idIndexMarker396"/></span><span class="No-Break">backend:</span></p>
			<pre class="source-code">
std::pair&lt;arma::mat, arma::rowvec&gt; GenerateData(
  size_t num_samples) {
  arma::mat samples = arma::randn&lt;arma::mat&gt;(1, num_samples);
  arma::rowvec labels = samples + arma::randn&lt;arma::rowvec(
    num_samples, arma::distr_param(1.0, 1.5));
  return {samples, labels};
}
...
size_t num_samples = 1000;
auto [raw_samples, raw_labels] = GenerateData(num_samples);</pre>			<p>Notice that for samples, we used the <strong class="source-inline">arma::mat</strong> type, and for labels, the <strong class="source-inline">arma::rowvec</strong> type. So, samples are placed into the matrix entity and labels into a one-dimensional vector correspondingly. Also, we used the <strong class="source-inline">arma::randn</strong> function to generate normally distributed data <span class="No-Break">and noise.</span></p>
			<p>Now, we can normalize data in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
data::StandardScaler sample_scaler;
sample_scaler.Fit(raw_samples);
arma::mat samples(1, num_samples);
sample_scaler.Transform(raw_samples, samples);
data::StandardScaler label_scaler;
label_scaler.Fit(raw_labels);
arma::rowvec labels(num_samples);
label_scaler.Transform(raw_labels, labels);</pre>			<p>We used the object of the <strong class="source-inline">StandardScaler</strong> class from the <strong class="source-inline">mlpack</strong> library to perform normalization. This object <a id="_idIndexMarker397"/>should be first trained on some data with the <strong class="source-inline">Fit</strong> method to learn mean and variance, and then it can be applied to other data with the <span class="No-Break"><strong class="source-inline">Transform</strong></span><span class="No-Break"> method.</span></p>
			<p>Now, let’s discuss how to prepare the data and how to define the hyperparameter tuner object. So, we are ready <a id="_idIndexMarker398"/>to launch the grid search for the best regularization values; the following sample shows how to <span class="No-Break">do it:</span></p>
			<pre class="source-code">
arma::vec lambdas{0.0, 0.001, 0.01, 0.1, 1.0};
double best_lambda = 0;
std::tie(best_lambda) = parameters_tuner.Optimize(lambdas);</pre>			<p>We defined a <strong class="source-inline">lambdas</strong> vector with our search space and then called the <strong class="source-inline">Optimize</strong> method of the hyperparameter tuner object. You can see that the return value is a tuple, and we use the <strong class="source-inline">std::tie</strong> function to extract the specific value. The <strong class="source-inline">Optimize</strong> method takes a variable number of arguments depending on the ML algorithm we use in the search, and each argument will define a search space for each hyperparameter used in the algorithm. The constructor of the <strong class="source-inline">LinearRegression</strong> class has only one <strong class="source-inline">lambda</strong> parameter. After the search is finished, we use the best-searched parameter directly, or we can get the best-optimized model object, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
LinearRegression&amp; linear_regression =  parameters_tuner.BestModel();</pre>			<p>We can now try the new model on new data to see how it works. At first, we will generate <a id="_idIndexMarker399"/>data and normalize it with a pre-trained scaler object, as shown in the <span class="No-Break">following sample:</span></p>
			<pre class="source-code">
size_t num_new_samples = 50;
arma::dvec new_samples_values = arma::linspace&lt;
  arma::dvec&gt;(x_minmax.first, x_minmax.second, num_new_samples);
arma::mat new_samples(1, num_new_samples);
new_samples.row(0) = arma::trans(new_samples_values);
arma::mat norm_new_samples(1, num_new_samples);
sample_scaler.Transform(new_samples, norm_new_samples);</pre>			<p>Here, we used the <strong class="source-inline">arma::linspace</strong> function to get a linearly distributed range of data. This function produces a vector; we wrote additional code that transforms this vector into a matrix object. Then, we used the already trained <strong class="source-inline">sample_scaler</strong> object to normalize <span class="No-Break">the data.</span></p>
			<p>The following sample<a id="_idIndexMarker400"/> shows how to use the model with the best parameter we found with the <span class="No-Break">grid search:</span></p>
			<pre class="source-code">
arma::rowvec predictions(num_new_samples);
linear_regression.Predict(norm_new_samples, predictions);</pre>			<p>The one important thing you have to notice is that the ML algorithm used for a grid search should be supported by <strong class="source-inline">SimpleCV</strong> or other validation classes. If it doesn’t have a default <a id="_idTextAnchor216"/>implementation, you wil<a id="_idTextAnchor217"/>l need to provide <span class="No-Break">it yourself.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor218"/>Optuna with Flashlight example</h2>
			<p>There is no support for any hyperparameter<a id="_idIndexMarker401"/> tuning algorithms in the<a id="_idIndexMarker402"/> Flashlight library, but we can use an external tool named Optuna to deal with ML programs that we want to search the <a id="_idIndexMarker403"/>best hyperparameters for. The main idea is to use some <strong class="bold">inter-process communication</strong> (<strong class="bold">IPC</strong>) approach to run training with different parameters and get some performance metric values <span class="No-Break">after training.</span></p>
			<p>Optuna is a hyperparameter tuning framework designed to be used with different ML libraries and programs. It is implemented with the Python programming language, so the main area of its application is tools that have some Python APIs. But Optuna also has a <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) that can be<a id="_idIndexMarker404"/> used with tools that don’t support Python. Another way to use such tools is to call them from Python, passing their command-line parameters and reading their standard output. This book will show an example of such type of Optuna usage, because<a id="_idIndexMarker405"/> writing a Python program is more useful than creating Bash scripts for the<a id="_idIndexMarker406"/> same automatization tasks, from the author’s point <span class="No-Break">of view.</span></p>
			<p>To use Optuna for hyperparameter tuning, we need to complete the three <span class="No-Break">following stages:</span></p>
			<ol>
				<li>Define an objective function <span class="No-Break">for optimization.</span></li>
				<li>Create a <span class="No-Break">study object.</span></li>
				<li>Run <span class="No-Break">optimization process.</span></li>
			</ol>
			<p>Let’s see how we can write a simple Optuna program in Python to search for the best parameter for a polynomial regression algorithm written in C++ with the <span class="No-Break">Flashlight library.</span></p>
			<p>First, we need to import the required <span class="No-Break">Python libraries:</span></p>
			<pre class="source-code">
import optuna
import subprocess</pre>			<p>Then, we need to <a id="_idIndexMarker407"/>define an objective function; it’s a function that is called by the Optuna tuning algorithm. It takes the <strong class="source-inline">Trial</strong> class object that contains a set of hyperparameter distributions and should return a performance metric value. The exact hyperparameter values should be sampled from the passed distributions. The following sample shows how we implement such <span class="No-Break">a function:</span></p>
			<pre class="source-code">
def objective(trial: optuna.trial.Trial):
  lr = trial.suggest_float("learning_rate", low=0.01, high=0.05)
  d = trial.suggest_int("polynomial_degree", low=8, high=16)
  bs = trial.suggest_int("batch_size", low=16, high=64)
  result = subprocess.run(
    [binary_path, str(d), str(lr), str(bs)],
    stdout=subprocess.PIPE
  )
  mse = float(result.stdout)
  return mse</pre>			<p>In this code, we used a family of functions in the <strong class="source-inline">trial</strong> object to sample concrete hyperparameter values. We sampled the <strong class="source-inline">lr</strong> learning rate with the call of the <strong class="source-inline">suggest_float</strong> method, and the <strong class="source-inline">d</strong> polynomial degree and the <strong class="source-inline">bs</strong> batch size with the <strong class="source-inline">suggest_int</strong> method. You can see that the signatures of these methods are pretty much the same. They take the name of hyperparameter, the low and the high bounds of a value range, and they can take a <a id="_idIndexMarker408"/>step value, which we didn’t use. These methods can sample values from<a id="_idIndexMarker409"/> discrete space and from continuous space too. The <strong class="source-inline">suggest_float</strong> method samples from a continuous space, and the <strong class="source-inline">suggest_int</strong> method samples from a <span class="No-Break">discrete space.</span></p>
			<p>Then, we called the <strong class="source-inline">run</strong> method from the <strong class="source-inline">subprocess</strong> module; it launches another process in the system. This method takes the array of strings of command-line parameters and some other parameters —in our case, the <strong class="source-inline">stdout</strong> redirection. This redirection is needed because we want to get the process’s output in a return value of the <strong class="source-inline">run</strong> method call; you can see this in the last lines as <strong class="source-inline">result.stdout</strong>, which is converted from string to <a id="_idIndexMarker410"/>floating-point values and is interpreted as <span class="No-Break">the MSE.</span></p>
			<p>Having the objective function, we can define a <strong class="source-inline">study</strong> object. This object tells Optuna how to tune hyperparameters. The two main characteristics of this object are the optimization direction and a search <a id="_idIndexMarker411"/>space for the hyperparameter sampler algorithm. The<a id="_idIndexMarker412"/> following sample shows how to define a discrete search space for <span class="No-Break">our task:</span></p>
			<pre class="source-code">
search_space = {
  "learning_rate": [0.01, 0.025, 0.045],],],
  "polynomial_degree": [8, 14, 16],
  "batch_size": [16, 32, 64],
}</pre>			<p>In this Python code, we defined a <strong class="source-inline">search_space</strong> dictionary with three items. Each item has the key string and the array value. The keys are <strong class="source-inline">learning_rate</strong>, <strong class="source-inline">polynomial_degree</strong>, and <strong class="source-inline">batch_size</strong>. After we define the search space, we can create a <strong class="source-inline">study</strong> object; the following sample <span class="No-Break">shows this:</span></p>
			<pre class="source-code">
study = optuna.create_study(
  study_name="PolyFit",
  direction="minimize",
  sampler=optuna.samplers.GridSampler(search_space),
)</pre>			<p>We used the <strong class="source-inline">create_study</strong> function from the <strong class="source-inline">optuna</strong> module and passed three parameters: <strong class="source-inline">study_name</strong>, <strong class="source-inline">direction</strong>, and <strong class="source-inline">sampler</strong>. The optimization direction we specified will be minimization, as we want to minimize MSE. For the <strong class="source-inline">sampler</strong> object, we used <strong class="source-inline">GridSampler</strong>, because we want to implement the grid search approach, and we initialized it with ou<a id="_idIndexMarker413"/>r <span class="No-Break">search space.</span></p>
			<p>The last step is to apply an optimization process and get the best hyperparameters. We can do this in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
study.optimize(objective)
print(f"Best value: {study.best_value}
     (params: {study.best_params})\n")</pre>			<p>We used the <strong class="source-inline">optimize</strong> method of our <strong class="source-inline">study</strong> object. You can see that it took a single parameter —our <strong class="source-inline">objective</strong> function that calls the external process to try sampled hyperparameters. The result of optimization was stored in the <strong class="source-inline">study</strong> object in the <strong class="source-inline">best_value</strong> and the <strong class="source-inline">best_params</strong> fields. The <strong class="source-inline">best_value</strong> field contains the best MSE value, and the <strong class="source-inline">best_params</strong> field contains the dictionary with the <span class="No-Break">best hyperparameters.</span></p>
			<p>This is the minimal sample <a id="_idIndexMarker414"/>of how to use Optuna. This framework has a wide variety <a id="_idIndexMarker415"/>of tuning and sampling algorithms, and a real application can be much more complicated. Also, the use of Python saves us from writing a lot of boilerplate code for the <span class="No-Break">CLI approach.</span></p>
			<p>Let’s take a short look at a polynomial regression implementation with the Flashlight library. I will show only the most important parts; a full example can be found <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/Chapter03/flashlight/grid_fl.cc</span></a><span class="No-Break">.</span></p>
			<p>The first important part is that our program should take all hyperparameters from the command-line argument. It can be implemented in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
int main(int argc, char** argv) {
  if (argc &lt; 3) {
    std::cout &lt;&lt; "Usage: " &lt;&lt; argv[0] &lt;&lt;
    " polynomial_degree learning_rate batch_size" &lt;&lt; std::endl;
    return 0;
  } else {
    // Hyper parameters
    int polynomial_degree = std::atoi(argv[1]);
    double learning_rate = std::atof(argv[2]);
    int batch_size = std::atoi(argv[3]);
    // Other code...
  }
}</pre>			<p>First, we checked that we had enough command-line arguments by comparing <strong class="source-inline">argc</strong> parameter with the<a id="_idIndexMarker416"/> required number. In the fail case, we print a help message. But in the successful case, we read all hyperparameters from the <strong class="source-inline">argv</strong> parameter and convert them from strings to the <span class="No-Break">appropriate types.</span></p>
			<p>Then, our program generates 2D <a id="_idIndexMarker417"/>cosine function points and mixes them with noise. To <a id="_idIndexMarker418"/>approximate a nonlinear function with linear regression, we can convert a simple Ax+b approach to a more complex polynomial like the <span class="No-Break">following one:</span></p>
			<p><strong class="source-inline">a1*x+a2*x^2+a3*x^3+...+an*x^n + b</strong></p>
			<p>It means that we have to choose some polynomial degree and convert our single-dimensional <strong class="source-inline">x</strong> value to a multidimensional one by raising <strong class="source-inline">x</strong> elements to the corresponding powers. So, the polynomial degree is the most important hyperparameter in <span class="No-Break">this algorithm.</span></p>
			<p>The Flashlight library doesn’t have any special implementation for regression algorithms because this library is oriented toward NN algorithms. But regression can be easily implemented with the gradient descent approach; the following code sample shows how it can <span class="No-Break">be done:</span></p>
			<pre class="source-code">
// define learnable variables
auto weight = fl::Variable(fl::rand({polynomial_degree, 1}),
                           /*calcGrad*/ true);
auto bias = fl::Variable(fl::full({1}, 0.0),
                         /*calcGrad*/ true);
float mse = 0;
fl::MeanSquaredError mse_func;
for (int e = 1; e &lt;= num_epochs; ++e) {
  fl::Tensor error = fl::fromScalar(0);
  for (auto&amp; batch : *batch_dataset) {
    auto input = fl::Variable(batch[0],
                              /*calcGrad*/ false);
    auto local_batch_size = batch[0].shape().dim(1);
    auto predictions = fl::matmul(fl::transpose(
                             weight), input) + fl::tile(
                             bias, {1, local_batch_size});
    auto targets = fl::Variable(
        fl::reshape(batch[1], {1, local_batch_size}),
        /*calcGrad*/ false);
    // Mean Squared Error Loss
    auto loss = mse_func.forward(predictions, targets);
    // Compute gradients using backprop
    loss.backward();
    // Update the weight and bias
    weight.tensor() -= learning_rate * weight.grad().tensor();
    bias.tensor() -= learning_rate * bias.grad().tensor();
    // Clear the gradients for next iteration
    weight.zeroGrad();
    bias.zeroGrad();
    mse_func.zeroGrad();
    error += loss.tensor();
  }
// Mean Squared Error for the epoch
error /= batch_dataset-&gt;size();
mse = error.scalar&lt;float&gt;();</pre>			<p>First, we defined learnable variables for the Flashlight <strong class="source-inline">autograd</strong> system; they are weights for each power of <strong class="source-inline">X</strong> and bias term. Then, we ran a loop for a specified number of epochs and the second loop over data batches to make the calculation vectorized; it makes computations more effective and makes the learning process less noise-dependent. For each batch of training data, we calculated predictions by getting a polynomial value; see the line with a call <a id="_idIndexMarker419"/>of the <strong class="source-inline">matmul</strong> function. The objective of the <strong class="source-inline">MeanSquareError</strong> class was<a id="_idIndexMarker420"/> used to get the loss function value. To calculate the corresponding gradients, see the <strong class="source-inline">mse_func.forward</strong> and <strong class="source-inline">mse_func.backward</strong> calls. Then, we updated our polynomial weights and biases with the learning rate and <span class="No-Break">corresponding gradients.</span></p>
			<p>All these concepts will be described in detail in the following chapters. The next important part is the <strong class="source-inline">error</strong> and <strong class="source-inline">mse</strong> value calculations. The <strong class="source-inline">error</strong> value is the Flashlight tensor object <a id="_idIndexMarker421"/>that contains the average MSE for the whole epoch, and the <strong class="source-inline">mse</strong> value is the floating-point value of this single-value tensor. This <strong class="source-inline">mse</strong> variable is printed to the standard output stream of the program at the end of the training, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
std::cout &lt;&lt; mse;</pre>			<p>We read this value in our Python program and return the resu<a id="_idTextAnchor219"/>lt of our Optuna objective function for the given set <span class="No-Break">of hyperparameters.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor220"/>Dlib example</h2>
			<p>The <strong class="source-inline">Dlib</strong> library also contains <a id="_idIndexMarker422"/>all the necessary functionality for the grid search algorithm. However, we should use functions instead of classes. The following code snippet shows the <strong class="source-inline">CrossValidationScore</strong> function’s definition. This function performs cross-validation and returns the value of the <span class="No-Break">performance metric:</span></p>
			<pre class="source-code">
auto CrossValidationScore = [&amp;](const double gamma,
  const double c,
  const double degree_in) {
  auto degree = std::floor(degree_in);
  using KernelType = Dlib::polynomial_kernel&lt;SampleType&gt;;
  Dlib::svr_trainer&lt;KernelType&gt; trainer;
  trainer.set_kernel(KernelType(gamma, c, degree));
  Dlib::matrix&lt;double&gt; result = Dlib::
    cross_validate_regression_trainer(
      trainer, samples, raw_labels, 10);
  return result(0, 0);
};</pre>			<p>The <strong class="source-inline">CrossValidationScore</strong> function takes the hyperparameters that were set as arguments. Inside this function, we defined a trainer for a model with the <strong class="source-inline">svr_trainer</strong> class, which implements kernel ridge<a id="_idIndexMarker423"/> regression based on the <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>) algorithm. We used the polynomial kernel, just like we did for the <strong class="source-inline">Shogun</strong> <span class="No-Break">library example.</span></p>
			<p>After we defined the model, we used the <strong class="source-inline">cross_validate_regression_trainer()</strong> function to train the model with the cross-validation approach. This function automatically splits our data into folds, with its last argument being the number of folds. The <strong class="source-inline">cross_validate_regression_trainer()</strong> function returns the matrix, along with the values of different performance metrics. Notice that we do not need to define them because they are<a id="_idIndexMarker424"/> predefined in the <span class="No-Break">library’s implementation.</span></p>
			<p>The first value in this matrix is the average MSE value. We used this value as a function result. However, there is no strong requirement for what value this function should return; the requirement is that the return value should be numeric and comparable. Also, notice that we defined the <strong class="source-inline">CrossValidationScore</strong> function as a lambda to simplify access to the<a id="_idIndexMarker425"/> training data container defined in the <span class="No-Break">outer scope.</span></p>
			<p>Next, we can search for the best parameters that were set with the <span class="No-Break"><strong class="source-inline">find_min_global</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
auto result = find_min_global(
    CrossValidationScore,
    {0.01, 1e-8, 5}, // minimum values for gamma, c, and degree
    {0.1, 1, 15}, // maximum values for gamma, c, and degree
    max_function_calls(50));</pre>			<p>This function takes the cross-validation function, the container with minimum values for parameter ranges, the container with maximum values for parameter ranges, and the number of cross-validation repeats. Notice that the initialization values for parameter ranges should go in the same order as the arguments that were defined in the <strong class="source-inline">CrossValidationScore</strong> function. Then, we can extract the best hyperparameters and train our model <span class="No-Break">with them:</span></p>
			<pre class="source-code">
double gamma = result.x(0);
double c = result.x(1);
double degree = result.x(2);
using KernelType = Dlib::polynomial_kernel&lt;SampleType&gt;;
Dlib::svr_trainer&lt;KernelType&gt; trainer;
trainer.set_kernel(KernelType(gamma, c, degree));
auto descision_func = trainer.train(samples, raw_labels)</pre>			<p>We used the same model definition as in the <strong class="source-inline">CrossValidationScore</strong> function. For the training process, we <a id="_idIndexMarker426"/>used all of our training data. The <strong class="source-inline">train</strong> method of the <strong class="source-inline">trainer</strong> object was used to complete the training <a id="_idIndexMarker427"/>process. The training result is a fu<a id="_idTextAnchor221"/>n<a id="_idTextAnchor222"/>ction that takes a single sample as an argument and returns a <span class="No-Break">prediction value.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor223"/>Summary</h1>
			<p>In this chapter, we discussed how to estimate an ML model’s performance and what metrics can be used for such estimation. We considered different metrics for regression and classification tasks and what characteristics they have. We also saw how performance metrics can be used to determine the model’s behavior and looked at bias and variance characteristics. We looked at some high bias (underfitting) and high variance (overfitting) problems and considered how to solve them. We also learned about regularization approaches, which are often used to deal with overfitting. We then studied what validation is and how it is used in the cross-validation technique. We saw that the cross-validation technique allows us to estimate model performance while training limited data. In the last section, we combined an evaluation metric and cross-validation in the grid search algorithm, which we can use to select the best set of hyperparameters for <span class="No-Break">our model.</span></p>
			<p>In the next chapter, we’ll learn about ML algorithms we can use to solve concrete problems. The next topic we will discuss in depth is clustering—the procedure of splitting the original set of objects into groups classified by proper<a id="_idTextAnchor224"/>t<a id="_idTextAnchor225"/>ies. We will look at different clustering approaches and <span class="No-Break">their characteristics.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor226"/>Further reading</h1>
			<ul>
				<li><em class="italic">Choosing the Right Metric for Evaluating Machine Learning Models—Part </em><span class="No-Break"><em class="italic">1</em></span><span class="No-Break">: </span><a href="https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4"><span class="No-Break">https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4</span></a></li>
				<li><em class="italic">Understand Regression Performance </em><span class="No-Break"><em class="italic">Metrics</em></span><span class="No-Break">: </span><a href="https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3"><span class="No-Break">https://becominghuman.ai/understand-regression-performance-metrics-bdb0e7fcc1b3</span></a></li>
				<li><em class="italic">Classification Performance </em><span class="No-Break"><em class="italic">Metrics</em></span><span class="No-Break">: </span><a href="https://nlpforhackers.io/classification-performance-metrics/"><span class="No-Break">https://nlpforhackers.io/classification-performance-metrics/</span></a></li>
				<li><em class="italic">REGULARIZATION: An important concept in Machine </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61"><span class="No-Break">https://towardsdatascience.com/regularization-for-machine-learning-67c37b132d61</span></a></li>
				<li>An overview of regularization techniques in <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) (with Python <span class="No-Break">code): </span><a href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques"><span class="No-Break">https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques</span></a></li>
				<li><em class="italic">Understanding the Bias-Variance </em><span class="No-Break"><em class="italic">Tradeoff</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229"><span class="No-Break">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</span></a></li>
				<li>DL – <span class="No-Break">Overfitting: </span><a href="https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc"><span class="No-Break">https://towardsdatascience.com/combating-overfitting-in-deep-learning-efb0fdabfccc</span></a></li>
				<li><em class="italic">A Gentle Introduction to k-fold </em><span class="No-Break"><em class="italic">Cross-Validation</em></span><span class="No-Break">: </span><a href="https://machinelearningmastery.com/k-fold-cross-validation/"><span class="No-Break">https://machinelearningmastery.com/k-fold-cross-validation/</span></a></li>
			</ul>
		</div>
	

		<div class="Content" id="_idContainer120">
			<h1 id="_idParaDest-82" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor227"/>Part 2: Machine Learning Algorithms</h1>
			<p>In this part, we’ll show you how to implement different well-known machine learning models (algorithms) using a variety of <span class="No-Break">C++ frameworks.</span></p>
			<p>This part comprises the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19849_04.xhtml#_idTextAnchor228"><em class="italic">Chapter 4</em></a>, <em class="italic">Clustering</em></li>
				<li><a href="B19849_05.xhtml#_idTextAnchor258"><em class="italic">Chapter 5</em></a>, <em class="italic">Anomaly Detection</em></li>
				<li><a href="B19849_06.xhtml#_idTextAnchor301"><em class="italic">Chapter 6</em></a>, <em class="italic">Dimensionality Reduction</em></li>
				<li><a href="B19849_07.xhtml#_idTextAnchor383"><em class="italic">Chapter 7</em></a>, <em class="italic">Classification</em></li>
				<li><a href="B19849_08.xhtml#_idTextAnchor438"><em class="italic">Chapter 8</em></a>, <em class="italic">Recommender Systems</em></li>
				<li><a href="B19849_09.xhtml#_idTextAnchor496"><em class="italic">Chapter 9</em></a>, <em class="italic">Ensemble Learning</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer121">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer122">
			</div>
		</div>
	</body></html>