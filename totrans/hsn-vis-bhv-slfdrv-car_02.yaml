- en: '*Chapter 1*: OpenCV Basics and Camera Calibration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter is an introduction to OpenCV and how to use it in the initial phases
    of a self-driving car pipeline, to ingest a video stream, and prepare it for the
    next phases. We will discuss the characteristics of a camera from the point of
    view of a self-driving car and how to improve the quality of what we get out of
    it. We will also study how to manipulate the videos and we will try one of the
    most famous features of OpenCV, object detection, which we will use to detect
    pedestrians.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: With this chapter, you will build a solid foundation on how to use OpenCV and
    NumPy, which will be very useful later.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV and NumPy basics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading, manipulating, and saving images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading, manipulating, and saving videos
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to detect pedestrians with HOG
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characteristics of a camera
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform the camera calibration
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the instructions and code in this chapter, you need the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Python 3.7
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The opencv-Python module
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The NumPy module
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for the chapter can be found here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter1](https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter1)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'The Code in Action videos for this chapter can be found here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/2TdfsL7](https://bit.ly/2TdfsL7)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to OpenCV and NumPy
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV is a computer vision and machine learning library that has been developed
    for more than 20 years and provides an impressive number of functionalities. Despite
    some inconsistencies in the API, its simplicity and the remarkable number of algorithms
    implemented make it an extremely popular library and an excellent choice for many
    situations.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV is written in C++, but there are bindings for Python, Java, and Android.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we will focus on OpenCV for Python, with all the code tested using
    OpenCV 4.2.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV in Python is provided by `opencv-python`, which can be installed using
    the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: OpenCV can take advantage of hardware acceleration, but to get the best performance,
    you might need to build it from the source code, with different flags than the
    default, to optimize it for your target hardware.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV and NumPy
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python bindings use NumPy, which increases the flexibility and makes it
    compatible with many other libraries. As an OpenCV image is a NumPy array, you
    can use normal NumPy operations to get information about the image. A good understanding
    of NumPy can improve the performance and reduce the length of your code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Let's dive right in with some quick examples of what you can do with NumPy in
    OpenCV.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Image size
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The size of the image can be retrieved using the `shape` attribute:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For a grayscale image of 50x50, `image.shape()` would return the tuple (50,
    50), while for an RGB image, the result would be (50, 50, 3).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: False friends
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, the attribute size is the size in bytes of the array; for a 50x50
    gray image, it would be 2,500, while for the same image in RGB, it would be 7,500\.
    It's the `shape` attribute that contains the size of the image – (50, 50) and
    (50, 50, 3), respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，`size`属性是数组的字节数；对于一个50x50的灰度图像，它将是2,500，而对于相同的RGB图像，它将是7,500。`shape`属性包含图像的大小——分别是(50,
    50)和(50, 50, 3)。
- en: Grayscale images
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灰度图像
- en: Grayscale images are represented by a two-dimensional NumPy array. The first
    index affects the rows (*y* coordinate) and the second index the columns (*x*
    coordinate). The *y* coordinates have their origin in the top corner of the image
    and *x* coordinates have their origin in the left corner of the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 灰度图像由一个二维NumPy数组表示。第一个索引影响行(*y*坐标)和第二个索引影响列(*x*坐标)。*y*坐标的起点在图像的顶部角落，而*x*坐标的起点在图像的左上角。
- en: 'It is possible to create a black image using `np.zeros()`, which initializes
    all the pixels to 0:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`np.zeros()`可以创建一个黑色图像，它将所有像素初始化为0：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The previous code creates a grayscale image with size (100, 100), composed of
    10,000 unsigned bytes (`dtype=np.uint8`).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码创建了一个大小为(100, 100)的灰度图像，由10,000个无符号字节组成(`dtype=np.uint8`)。
- en: 'To create an image with pixels with a different value than 0, you can use the
    `full()` method:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个像素值不为0的图像，你可以使用`full()`方法：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To change the color of all the pixels at once, it''s possible to use the `[:]`
    notation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要一次性改变所有像素的颜色，可以使用`[:]`表示法：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To affect only some rows, it is enough to provide a range of rows in the first
    index:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要只影响某些行，只需要在第一个索引中提供一个行范围：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The previous code changes the color of rows 10-20, including row 10, but excluding
    row 20.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码改变了第10-20行的颜色，包括第10行，但不包括第20行。
- en: 'The same mechanism works for columns; you just need to specify the range in
    the second index. To instruct NumPy to include a full index, we use the `[:]`
    notation that we already encountered:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的机制也适用于列；你只需要在第二个索引中指定范围。要指示NumPy包含一个完整的索引，我们使用之前遇到的`[:]`表示法：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also combine operations on rows and columns, selecting a rectangular
    area:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以组合行和列的操作，选择一个矩形区域：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It is, of course, possible to operate on a single pixel, as you would do on
    a normal array:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以操作单个像素，就像在普通数组中做的那样：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It is possible to use NumPy to select a part of an image, also called the **Region
    Of Interest** (**ROI**). For example, the following code copies a 10x10 **ROI**
    from the position (90, 90) to the position (80, 80):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy选择图像的一部分，也称为**感兴趣区域**(**ROI**)是可能的。例如，以下代码从位置(90, 90)复制一个10x10的**ROI**到位置(80,
    80)：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is the result of the previous operations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为之前操作的结果：
- en: '![Figure 1.1 – Some manipulation of images using NumPy slicing](img/Figure_1.1_B16322.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 使用NumPy切片对图像进行的一些操作](img/Figure_1.1_B16322.jpg)'
- en: Figure 1.1 – Some manipulation of images using NumPy slicing
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 使用NumPy切片对图像进行的一些操作
- en: 'To make a copy of an image, you can simply use the `copy()` method:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要复制一张图片，你可以简单地使用`copy()`方法：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: RGB images
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RGB图像
- en: RGB images differ from grayscale because they are three-dimensional, with the
    third index representing the three channels. Please note that OpenCV stores the
    images in BGR format, not RGB, so channel 0 is blue, channel 1 is green, and channel
    2 is red.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RGB图像与灰度图像不同，因为它们是三维的，第三个索引代表三个通道。请注意，OpenCV以BGR格式存储图像，而不是RGB，所以通道0是蓝色，通道1是绿色，通道2是红色。
- en: Important note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: OpenCV stores the images as BGR, not RGB. In the rest of the book, when talking
    about RGB images, it will only mean that it is a 24-bit color image, but the internal
    representation will usually be BGR.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV将图像存储为BGR，而不是RGB。在本书的其余部分，当谈到RGB图像时，它仅意味着它是一个24位彩色图像，但内部表示通常是BGR。
- en: 'To create an RGB image, we need to provide three sizes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个RGB图像，我们需要提供三个尺寸：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you were going to run the same code previously used on the grayscale image
    with the new RGB image (skipping the third index), you would get the same result.
    This is because NumPy would apply the same color to all the three channels, which
    results in a shade of gray.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算用之前在灰度图像上运行的相同代码来运行新的RGB图像（跳过第三个索引），你会得到相同的结果。这是因为NumPy会将相同的颜色应用到所有三个通道上，这会导致灰色。
- en: 'To select a color, it is enough to provide the third index:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择一个颜色，只需要提供第三个索引：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In NumPy, it is also possible to select rows, columns, or channels that are
    not contiguous. You can do this by simply providing a tuple with the required
    indexes. To make the image magenta, you need to set the blue and red channels
    to `255`, which can be achieved with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can convert an RGB image into grayscale using `cvtColor()`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Working with image files
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV provides a very simple way to load images, using `imread()`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To show the image, you can use `imshow()`, which accepts two parameters:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The name to write on the caption of the window that will show the image
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image to be shown
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, its behavior is counterintuitive, as it will not show an image
    unless it is followed by a call to `waitKey()`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The call to `waitKey()` after `imshow()` will have two effects:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: It will actually allow OpenCV to show the image provided to `imshow()`.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will wait for the specified amount of milliseconds, or until a key is pressed
    if the amount of milliseconds passed is `<=0`. It will wait indefinitely.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An image can be saved on disk using the `imwrite()` method, which accepts three
    parameters:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: The name of the file
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An optional format-dependent parameter:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Sometimes, it can be very useful to combine multiple pictures by putting them
    next to each other. Some examples in this book will use this feature extensively
    to compare images.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV provides two methods for this purpose: `hconcat()` to concatenate the
    pictures horizontally and `vconcat()` to concatenate them vertically, both accepting
    as a parameter a list of images. Take the following example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here''s the result:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Horizontal concatenation with hconcat() and vertical concatenation
    with vconcat()](img/Figure_1.2_B16322.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Horizontal concatenation with hconcat() and vertical concatenation
    with vconcat()
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'We could use these two methods to create a chequerboard pattern:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You will see the following chequerboard:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – A chequerboard pattern created using hconcat() in combination
    with vconcat()](img/Figure_1.3_B16322.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – A chequerboard pattern created using hconcat() in combination with
    vconcat()
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: After having worked with images, it's time we work with videos.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Working with video files
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using videos in OpenCV is very simple; in fact, every frame is an image and
    can be manipulated with the methods that we have already analyzed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'To open a video in OpenCV, you need to call the `VideoCapture()` method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After that, you can call `read()`, typically in a loop, to retrieve a single
    frame. The method returns a tuple with two values:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: A Boolean value that is false when the video is finished
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next frame:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To save a video, there is the `VideoWriter` object; its constructor accepts
    four parameters:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The filename
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A FOURCC (four-character code) of the video code
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of frames per second
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resolution
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Take the following example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once `VideoWriter` has been created, the `write()` method can be used to add
    a frame to the video file:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When you have finished using the `VideoCapture` and `VideoWriter` objects,
    you should call their release method:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Working with webcams
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Webcams are handled similarly to a video in OpenCV; you just need to provide
    a different parameter to `VideoCapture`, which is the 0-based index identifying
    the webcam:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The previous code opens the first webcam; if you need to use a different one,
    you can specify a different index.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try manipulating some images.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating images
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of a computer vision pipeline for a self-driving car, with or without
    deep learning, you might need to process the video stream to make other algorithms
    work better as part of a preprocessing step.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: This section will provide you with a solid foundation to preprocess any video
    stream.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Flipping an image
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenCV provides the `flip()` method to flip an image, and it accepts two parameters:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number that can be 1 (horizontal flip), 0 (vertical flip), or -1 (both horizontal
    and vertical flip)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see a sample code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will produce the following result:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Original image, horizontally flipped, vertically flipped, and
    both](img/Figure_1.4_B16322.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Original image, horizontally flipped, vertically flipped, and both
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the first image is our original image, which was flipped horizontally
    and vertically, and then both, horizontally and vertically together.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Blurring an image
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, an image can be too noisy, possibly because of some processing steps
    that you have done. OpenCV provides several methods to blur an image, which can
    help in these situations. Most likely, you will have to take into consideration
    not only the quality of the blur but also the speed of execution.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest method is `blur()`, which applies a low-pass filter to the image
    and requires at least two parameters:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The kernel size (a bigger kernel means more blur):'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Another option is to use `GaussianBlur()`, which offers more control and requires
    at least three parameters:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel size
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sigmaX`, which is the standard deviation on X'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is recommended to specify both `sigmaX` and `sigmaY` (standard deviation
    on Y, the forth parameter):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'An interesting blurring method is `medianBlur()`, which computes the median
    and therefore has the characteristic of emitting only pixels with colors present
    in the image (which does not necessarily happen with the previous method). It
    is effective at reducing "salt and pepper" noise and has two mandatory parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The kernel size (an odd integer greater than 1):'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There is also a more complex filter, `bilateralFilter()`, which is effective
    at removing noise while keeping the edge sharp. It is the slowest of the filters,
    and it requires at least four parameters:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diameter of each pixel neighborhood
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sigmaColor`: Filters sigma in the color space, affecting how much the different
    colors are mixed together, inside the pixel neighborhood'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sigmaSpace`: Filters sigma in the coordinate space, affecting how distant
    pixels affect each other, if their colors are closer than `sigmaColor`:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Choosing the best filter will probably require some experiments. You might
    also need to consider the speed. To give you some ballpark estimations based on
    my tests, and considering that the performance is dependent on the parameters
    supplied, note the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '`blur()` is the fastest.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GaussianBlur()` is similar, but it can be 2x slower than blur().'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`medianBlur()` can easily be 20x slower than `blur()`.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BilateralFilter()` is the slowest and can be 45x slower than `blur()`.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the resultant images:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Original, blur(), GaussianBlur(), medianBlur(), and BilateralFilter(),
    with the parameters used in the code samples](img/Figure_1.5_B16322.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Original, blur(), GaussianBlur(), medianBlur(), and BilateralFilter(),
    with the parameters used in the code samples
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Changing contrast, brightness, and gamma
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A very useful function is `convertScaleAbs()`, which executes several operations
    on all the values of the array:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: It multiplies them by the scaling parameter, `alpha`.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It adds to them the delta parameter, `beta`.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the result is above 255, it is set to 255.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is converted into an unsigned 8-bit int.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function accepts four parameters:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The source image
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The destination (optional)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `alpha` parameter used for the scaling
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `beta` delta parameter
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`convertScaleAbs()` can be used to affect the contrast, as an `alpha` scaling
    factor above 1 increases the contrast (amplifying the color difference between
    pixels), while a scaling factor below one reduces it (decreasing the color difference
    between pixels):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It can also be used to affect the brightness, as the `beta` delta factor can
    be used to increase the value of all the pixels (increasing the brightness) or
    to reduce them (decreasing the brightness):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s see the resulting images:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Original, more contrast (2x), less contrast (0.5x), more brightness
    (+64), and less brightness (-64)](img/Figure_1.6_B16322.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Original, more contrast (2x), less contrast (0.5x), more brightness
    (+64), and less brightness (-64)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'A more sophisticated method to change the brightness is to apply gamma correction.
    This can be done with a simple calculation using NumPy. A gamma value above 1
    will increase the brightness, and a gamma value below 1 will reduce it:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following images will be produced:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Original, higher gamma (1.5), and lower gamma (0.7)](img/Figure_1.7_B16322.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Original, higher gamma (1.5), and lower gamma (0.7)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: You can see the effect of different gamma values in the middle and right images.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Drawing rectangles and text
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working on object detection tasks, it is a common need to highlight an
    area to see what has been detected. OpenCV provides the `rectangle()` function,
    accepting at least the following parameters:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upper-left corner of the rectangle
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lower-right corner of the rectangle
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The color to use
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(Optional) The thickness:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To write some text in the image, you can use the `putText()` method, accepting
    at least six parameters:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text to print
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coordinates of the bottom-left corner
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The font face
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scale factor, to change the size
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The color:'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Pedestrian detection using HOG
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Histogram of Oriented Gradients** (**HOG**) is an object detection technique
    implemented by OpenCV. In simple cases, it can be used to see whether there is
    a certain object present in the image, where it is, and how big it is.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV includes a detector trained for pedestrians, and you are going to use
    it. It might not be enough for a real-life situation, but it is useful to learn
    how to use it. You could also train another one with more images to see whether
    it performs better. Later in the book, you will see how to use deep learning to
    detect not only pedestrians but also cars and traffic lights.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Sliding window
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The HOG pedestrian detector in OpenCV is trained with a model that is 48x96
    pixels, and therefore it is not able to detect objects smaller than that (or,
    better, it could, but the box will be 48x96).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: At the core of the HOG detector, there is a mechanism able to tell whether a
    given 48x96 image is a pedestrian. As this is not terribly useful, OpenCV implements
    a sliding window mechanism, where the detector is applied many times, on slightly
    different positions; the "image window" under consideration slides a bit. Once
    it has analyzed the whole image, the image window is increased in size (scaled)
    and the detector is applied again, to be able to detect bigger objects. Therefore,
    the detector is applied hundreds or even thousands of times for each image, which
    can be slow.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Using HOG with OpenCV
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, you need to initialize the detector and specify that you want to use
    the detector for pedestrians:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, it is just a matter of calling `detectMultiScale()`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The parameters that we used require some explanation, and they are as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The image
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`winStride`, the window stride, which specifies how much the sliding window
    moves every time'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding, which can add some padding pixels at the border of the image (useful
    to detect pedestrians close to the border)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale, which specifies how much to increase the window image every time
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should consider that `winSize` can improve the accuracy (as more positions
    are considered), but it has a big impact on performance. For example, a stride
    of (4, 4) can be up to 16 times faster than a stride of (1, 1), though in practice,
    the performance difference is a bit less, maybe 10 times.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: In general, **decreasing the scale** also improves the precision and decreases
    the performance, though the impact is not dramatic.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Improving the precision means detecting more pedestrians, but this can also
    increase the false positives. `detectMultiScale()` has a couple of advanced parameters
    that can be used for this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`hitThreshold`, which changes the distance required from the **Support Vector
    Machine** (**SVM**) plane. A higher threshold means than the detector is more
    confident with the result.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`finalThreshold`, which is related to the number of detections in the same
    area.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning these parameters requires some experiments, but in general, a higher
    `hitThreshold` value (typically in the range 0–1.0) should reduce the false positives.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: A higher `finalThreshold` value (such as 10) will also reduce the false positives.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `detectMultiScale()` on an image with pedestrians generated by
    Carla:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – HOG detection, winStride=(1, 2), scale=1.05, padding=(0, 0)
    Left: hitThreshold = 0, finalThreshold = 1; Center: hitThreshold = 0, inalThreshold
    = 3; Right: hitThreshold = 0.2, finalThreshold = 1](img/Figure_1.8_B16322.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8 – HOG detection, winStride=(1, 2), scale=1.05, padding=(0, 0)Left:
    hitThreshold = 0, finalThreshold = 1; Center: hitThreshold = 0, finalThreshold
    = 3;Right: hitThreshold = 0.2, finalThreshold = 1'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have pedestrians being detected in the image. Using a low
    hit threshold and a low final threshold can result in false positives, as in the
    left image. Your goal is to find the right balance, detecting the pedestrians
    but without having too many false positives.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the camera
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The camera is probably one of the most ubiquitous sensors in our modern world.
    They are used in everyday life in our mobile phones, laptops, surveillance systems,
    and of course, photography. They provide rich, high-resolution imagery containing
    extensive information about the environment, including spatial, color, and temporal
    information.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: It is no surprise that they are heavily used in self-driving technologies. One
    reason why the camera is so popular is that it mirrors the functionality of the
    human eye. For this reason, we are very comfortable using them as we connect on
    a deep level with their functionality, limitations, and strengths.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will learn about the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Camera terminology
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The components of a camera
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strengths and weaknesses
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right camera for self-driving
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each in detail.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Camera terminology
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you learn about the components of a camera and its strengths and weaknesses,
    you need to know some basic terminology. These terms will be important when evaluating
    and ultimately choosing your camera for your self-driving application.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Field of View (FoV)
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the vertical and horizontal angular portion of the environment (scene)
    that is visible to the sensor. In self-driving cars, you typically want to balance
    the FoV with the resolution of the sensor to ensure we see as much of the environment
    as possible with the least number of cameras. There is a trade space related to
    FoV. Larger FoV usually means more lens distortion, which you will need to compensate
    for in your camera calibration (see the section on camera calibration):'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Field of View, credit: https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596](img/Figure_1.9_B16322.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9 – Field of View, credit: [https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596](https://www.researchgate.net/figure/Illustration-of-camera-lenss-field-of-view-FOV_fig4_335011596)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Resolution
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the total number of pixels in the horizontal and vertical directions
    on the sensor. This parameter is often discussed using the term **megapixels**
    (**MP**). For example, a 5 MP camera, such as the FLIR Blackfly, has a sensor
    with 2448 × 2048 pixels, which equates to 5,013,504 pixels.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Higher resolutions allow you to use a lens with a wider FoV but still provide
    the detail needed for running your computer vision algorithms. This means you
    can use fewer cameras to cover the environment and thereby lower the cost.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The Blackfly, in all its different flavors, is a common camera used in self-driving
    vehicles thanks to its cost, small form, reliability, robustness, and ease of
    integration:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Pixel resolution](img/Figure_1.10_New.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Pixel resolution
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Focal length
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the length from the lens optical center to the sensor. The focal length
    is best thought of as the zoom of the camera. A longer focal length means you
    will be zoomed in closer to objects in the environment. In your self-driving car,
    you may choose different focal lengths based on what you need to see in the environment.
    For example, you might choose a relatively long focal length of 100 mm to ensure
    enough resolution for your classifier algorithm to detect a traffic signal at
    a distance far enough to allow the car to react with smooth and safe stopping:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Focal length, credit: https://photographylife.com/what-is-focal-length-in-photography](img/Figure_1.11_B16322.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11 – Focal length, credit: [https://photographylife.com/what-is-focal-length-in-photography](https://photographylife.com/what-is-focal-length-in-photography)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Aperture and f-stop
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the opening through which light passes to shine on the sensor. The
    unit that is commonly used to describe the size of the opening is the f-stop,
    which refers to the ratio of the focal length over the aperture size. For example,
    a lens with a 50 mm focal length and an aperture diameter of 35 mm will equate
    to an f-stop of f/1.4\. The following figure illustrates different aperture diameters
    and their f-stop values on a 50 mm focal length lens. Aperture size is very important
    in your self-driving car as it is directly correlated with the **Depth of Field**
    (**DoF**). Large apertures also allow the camera to be tolerant of obscurants
    (for example, bugs) that may be on the lens. Larger apertures allow light to pass
    around the bug and still make it to the sensor:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Aperture, credit: https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg](img/Figure_1.12_B16322.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12 – Aperture, credit: [https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg](https://en.wikipedia.org/wiki/Aperture#/media/File:Lenses_with_different_apertures.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Depth of field (DoF)
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the distance range in the environment that will be in focus. This is
    directly correlated to the size of the aperture. Generally, in self-driving cars,
    you will want a deep DoF so that everything in the FoV is in focus for your computer
    vision algorithms. The problem is that deep DoF is achieved with a small aperture,
    which means less light impacting the sensor. So, you will need to balance DoF
    with dynamic range and ISO to ensure you see everything you need to in your environment.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the relationship between DoF and aperture:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – DoF versus aperture, credit: https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg](img/Figure_1.13_B16322.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13 – DoF versus aperture, credit: [https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg](https://thumbs.dreamstime.com/z/aperture-infographic-explaining-depth-field-corresponding-values-their-effect-blur-light-75823732.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic range
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is a property of the sensor that indicates its contrast ratio or the ratio
    of the brightest over the darkest subjects that it can resolve. This may be referred
    to using the unit dB (for example, 78 dB) or contrast ratio (for example, 2,000,000/1).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-driving cars need to operate both during the day and at night. This means
    that the sensor needs to be sensitive enough to provide useful detail in dark
    conditions while not oversaturating when driving in bright sunlight. Another reason
    for **High Dynamic Range** (**HDR**) is the example of driving when the sun is
    low on the horizon. I am sure you have experienced this while driving yourself
    to work in the morning and the sun is right in your face and you can barely see
    the environment in front of you because it is saturating your eyes. HDR means
    that the sensor will be able to see the environment even in the face of direct
    sunlight. The following figure illustrates these conditions:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Example HDR, credit: https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/](img/Figure_1.14_New.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14 – Example HDR, credit: [https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/](https://petapixel.com/2011/05/02/use-iso-numbers-that-are-multiples-of-160-when-shooting-dslr-video/)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Your dream dynamic range
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: If you could make a wish and have whatever dynamic range you wanted in your
    sensor, what would it be?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: International Organization for Standardization (ISO) sensitivity
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 国际标准化组织（ISO）灵敏度
- en: This is the sensitivity of the pixels to incoming photons.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这是像素对入射光子的敏感度。
- en: '*Wait a* *minute*, you say, *do you have your acronym mixed up*? It looks like
    it, but the International Organization for Standardization decided to standardize
    even their acronym since it would be different in every language otherwise. Thanks,
    ISO!'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '*等一下*，你说，*你的缩写搞混了吗*？看起来是这样，但国际标准化组织决定连它们的缩写也要标准化，因为否则每种语言都会不同。谢谢，ISO！'
- en: 'The standardized ISO values can range from 100 to upward of 10,000\. Lower
    ISO values correspond to a lower sensitivity of the sensor. Now you may ask, "why
    wouldn''t I want the highest sensitivity?" Well, sensitivity comes at a cost...NOISE.
    The higher the ISO, the more noise you will see in your images. This added noise
    may cause trouble for your computer vision algorithms when trying to classify
    objects. In the following figure, you can see the effect of higher ISO values
    on noise in an image. These images are all taken with the lens cap on (fully dark).
    As you increase the ISO value, random noise starts to creep in:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的ISO值可以从100到超过10,000。较低的ISO值对应于传感器较低的灵敏度。现在你可能要问，“为什么我不想要最高的灵敏度？”好吧，灵敏度是有代价的……噪声。ISO值越高，你会在图像中看到更多的噪声。这种额外的噪声可能会在尝试对物体进行分类时给你的计算机视觉算法带来麻烦。在下面的图中，你可以看到较高ISO值对图像噪声的影响。这些图像都是在镜头盖盖上（完全黑暗）的情况下拍摄的。随着ISO值的增加，随机噪声开始渗入：
- en: '![Figure 1.15 – Example ISO values and noise in a dark room](img/Figure_1.15_New.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15 – 暗室中的ISO值示例和噪声](img/Figure_1.15_New.jpg)'
- en: Figure 1.15 – Example ISO values and noise in a dark room
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15 – 暗室中的ISO值示例和噪声
- en: Frame rate (FPS)
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 帧率（FPS）
- en: 'This is the rate at which the sensor can obtain consecutive images, usually
    expressed in Hz or **Frames Per Second** (**FPS**). Generally speaking, you want
    to have the fastest frame rate so that fast-moving objects are not blurry in your
    scene. The main trade-off here is latency: the time from a real event happening
    until your computer vision algorithm detects it. The higher the frame rate that
    must be processed, the higher the latency. In the following figure, you can see
    the effect of frame rate on motion blur.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这是传感器获取连续图像的速率，通常以Hz或**每秒帧数**（FPS）表示。一般来说，你希望有最快的帧率，这样快速移动的物体在你的场景中就不会模糊。这里的主要权衡是延迟：从真实事件发生到你的计算机视觉算法检测到它的时间。必须处理的帧率越高，延迟就越高。在下面的图中，你可以看到帧率对运动模糊的影响。
- en: 'Blur is not the only reason for choosing a higher frame rate. Depending on
    the speed of your vehicle, you will need a frame rate that will allow the vehicle
    to react if an object suddenly appears in its FoV. If your frame rate is too slow,
    by the time the vehicle sees something, it may be too late to react:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊并不是选择更高帧率的唯一原因。根据你的车辆速度，你需要一个帧率，以便车辆能够在物体突然出现在其视场（FoV）中时做出反应。如果你的帧率太慢，当车辆看到某物时，可能已经太晚做出反应了：
- en: '![Figure 1.16 – 120 Hz versus 60 Hz frame rate, credit: https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/](img/Figure_1.16_B16322.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图1.16 – 120 Hz与60 Hz帧率对比，来源：https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/](img/Figure_1.16_B16322.jpg)'
- en: 'Figure 1.16 – 120 Hz versus 60 Hz frame rate, credit: [https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/](https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16 – 120 Hz与60 Hz帧率对比，来源：[https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/](https://gadgetstouse.com/blog/2020/03/18/difference-between-60hz-90hz-120hz-displays/)
- en: Lens flare
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 镜头眩光
- en: These are the artifacts of light from an object that impact pixels on the sensor
    that do not correlate with the position of the object in the environment. You
    have likely experienced this driving at night when you see oncoming headlights.
    That starry effect is due to light scattered in the lens of your eye (or camera),
    due to imperfections, leading some of the photons to impact "pixels" that do not
    correlate with where the photons came from – that is, the headlights. The following
    figure shows what that effect looks like. You can see that the starburst makes
    it very difficult to see the actual object, the car!
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是来自物体的光线在传感器上产生的伪影，这些伪影与物体在环境中的位置不相关。你可能在夜间驾驶时遇到过这种情况，当时你会看到迎面而来的车灯。那种星光效果是由于你的眼睛（或相机）的镜头中散射的光线，由于不完美，导致一些光子撞击了“像素”，而这些像素与光子来自的地方不相关——也就是说，是车灯。以下图示显示了这种效果。你可以看到星光效果使得实际物体，即汽车，非常难以看清！
- en: '![Figure 1.17 – Lens flare from oncoming headlights, credit: https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg](img/Figure_1.17_B16322.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图1.17 – 迎面车灯产生的镜头眩光，来源：https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg](img/Figure_1.17_B16322.jpg)'
- en: 'Figure 1.17 – Lens flare from oncoming headlights, credit: [https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg](https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17 – 迎面车灯产生的镜头眩光，来源：[https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg](https://s.blogcdn.com/cars.aol.co.uk/media/2011/02/headlights-450-a-g.jpg)
- en: Lens distortion
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 镜头畸变
- en: 'This is the difference between the rectilinear or real scene to what your camera
    image sees. If you have ever seen action camera footage, you probably recognized
    the "fish-eye" lens effect. The following figure shows an extreme example of the
    distortion from a wide-angle lens. You will learn to correct this distortion with
    OpenCV:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这是直线或真实场景与你的相机图像所看到场景之间的区别。如果你曾经看过动作相机的视频，你可能已经注意到了“鱼眼”镜头效果。以下图示显示了广角镜头的极端畸变示例。你将学会如何使用OpenCV来纠正这种畸变：
- en: '![Figure 1.18 – Lens distortion, credit: https://www.slacker.xyz/post/what-lens-should-i-get](img/Figure_1.18_B16322.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图1.18 – 镜头畸变，来源：https://www.slacker.xyz/post/what-lens-should-i-get](img/Figure_1.18_B16322.jpg)'
- en: 'Figure 1.18 – Lens distortion, credit: [https://www.slacker.xyz/post/what-lens-should-i-get](https://www.slacker.xyz/post/what-lens-should-i-get)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.18 – 镜头畸变，来源：[https://www.slacker.xyz/post/what-lens-should-i-get](https://www.slacker.xyz/post/what-lens-should-i-get)
- en: The components of a camera
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相机的组成部分
- en: Like the eye, a camera is made up of a light-sensitive array, an aperture, and
    a lens.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 就像眼睛一样，相机由光敏感阵列、光圈和镜头组成。
- en: Light sensitive array – CMOS sensor (the camera's retina)
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 光敏感阵列 – CMOS传感器（相机的视网膜）
- en: The light-sensitive array, in most consumer cameras, is called a CMOS active-pixel
    sensor (or just a sensor). Its basic function is to convert incident photons into
    an electrical current that can be digitized based on the color wavelength of the
    photon.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数消费级相机中，光敏感阵列被称为CMOS有源像素传感器（或简称传感器）。其基本功能是将入射光子转换为电信号，该信号可以根据光子的颜色波长进行数字化。
- en: The aperture (the camera's iris)
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 光圈（相机的光圈）
- en: The aperture or iris of a camera is the opening through which light can pass
    on its way to the sensor. This can be variable or fixed depending on the type
    of camera you are using. The aperture is used to control parameters such as depth
    of field and the amount of light hitting the sensor.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 相机的光圈或瞳孔是光线通往传感器的通道。这可以是可变的或固定的，具体取决于你使用的相机类型。光圈用于控制诸如景深和到达传感器的光线量等参数。
- en: The lens (the camera's lens)
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 镜头（相机的镜头）
- en: The lens or optics are the components of the camera that focus the light from
    the environment onto the sensor. The lens primarily determines the **FoV** of
    the camera through its focal length. In self-driving applications, the FoV is
    very important since it determines how much of the environment the car can see
    with a single camera. The optics of a camera are often some of the most expensive
    parts and have a large impact on image quality and lens flare.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 镜头或光学元件是相机将环境中的光线聚焦到传感器上的组成部分。镜头主要通过其焦距决定相机的**视场角（FoV）**。在自动驾驶应用中，视场角非常重要，因为它决定了汽车单次使用一个摄像头可以看到多少环境。相机的光学元件通常是成本最高的部分之一，并对图像质量和镜头眩光有重大影响。
- en: Considerations for choosing a camera
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择相机的考虑因素
- en: 'Now that you have learned all the basics of what a camera is and the relevant
    terminology, it is time to learn how to choose a camera for your self-driving
    application. The following is a list of the primary factors that you will need
    to balance when choosing a camera:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Resolution
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FoV
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic range
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Size
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress protection (IP rating)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perfect camera
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you could design the ideal camera, what would it be?
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: My perfect self-driving camera would be able to see in all directions (spherical
    FoV, 360º HFoV x 360º VFoV). It would have infinite resolution and dynamic range,
    so you could digitally resolve objects at any distance in any lighting condition.
    It would be the size of a grain of rice, completely water- and dustproof, and
    would cost $5! Obviously, this is not possible. So, we must make some careful
    trade-offs for what we need.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: The best place to start is with your budget for cameras. This will give you
    an idea of what models and specifications to look for.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, consider what you need to see for your application:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Do you need to be able to see a child from 200 m away while traveling at 100
    km/h?
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What coverage around the vehicle do you need, and can you tolerate any blind
    spots on the side of the vehicle?
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you need to see at night and during the day?
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, consider how much room you have to integrate these cameras. You probably
    don''t want your vehicle to look like this:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Camera art, credit: https://www.flickr.com/photos/laughingsquid/1645856255/](img/Figure_1.19_B16322.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.19 – Camera art, credit: [https://www.flickr.com/photos/laughingsquid/1645856255/](https://www.flickr.com/photos/laughingsquid/1645856255/)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: This may be very overwhelming, but it is important when thinking about how to
    design your computer vision system. A good camera to start with that is very popular
    is the FLIR Blackfly S series. They strike an excellent balance of resolution,
    FPS, and cost. Next, pair it with a lens that meets your FoV needs. There are
    some helpful FoV calculators available on the internet, such as the one from [http://www.bobatkins.com/photography/technical/field_of_view.html](http://www.bobatkins.com/photography/technical/field_of_view.html).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Strengths and weaknesses of cameras
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, no sensor is perfect, and even your beloved camera will have its pros and
    cons. Let's go over some of them now.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the strengths first:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '**High-resolution**: Relative to other sensor types, such as radar, lidar,
    and sonar, cameras have an excellent resolution for picking out objects in your
    scene. You can easily find cameras with 5 MP resolution quite cheaply.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Texture, color, and contrast information**: Cameras provide very rich information
    about the environment that other sensor types just can''t. This is because of
    a variety of wavelengths that cameras sense.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: Cameras are one of the cheapest sensors you can find, especially
    for the quality of data they provide.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size**: CMOS technology and modern ASICs have made cameras incredibly small,
    many less than 30 mm cubed.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺寸**：CMOS技术和现代ASIC技术使得相机变得非常小，许多小于30立方毫米。'
- en: '**Range**: This is really thanks to the high resolution and passive nature
    of the sensor.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围**：这主要归功于高分辨率和传感器的被动性质。'
- en: 'Next, here are the weaknesses:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这里有一些弱点：
- en: '**A large amount of data to process for object detection**: With high resolution
    comes a lot of data. Such is the price we pay for such accurate and detailed imagery.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大量数据处理用于物体检测**：随着分辨率的提高，数据量也随之增加。这就是我们为了如此精确和详细的图像所付出的代价。'
- en: '**Passive**: A camera requires an external illumination source, such as the
    sun, headlights, and so on.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**被动**：相机需要一个外部照明源，如太阳、车头灯等。'
- en: '**Obscurants (such as bugs, raindrops, heavy fog, dust, or snow)**: A camera
    is not particularly good at seeing through heavy rain, fog, dust, or snow. Radars
    are typically better suited for this.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遮蔽物（如昆虫、雨滴、浓雾、灰尘或雪）**：相机在穿透大雨、雾、灰尘或雪方面并不特别擅长。雷达通常更适合这项任务。'
- en: '**Lack native depth/velocity information**: A camera image alone doesn''t give
    you any information on an object''s speed or distance.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺乏原生深度/速度信息**：仅凭相机图像本身无法提供关于物体速度或距离的任何信息。'
- en: Photogrammetry is helping to bolster this weakness but costs valuable processing
    resources (GPU, CPU, latency, and so on.) It is also less accurate than a radar
    or lidar sensor, which produce this information natively.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测绘学正在帮助弥补这一弱点，但代价是宝贵的处理资源（GPU、CPU、延迟等）。它也比雷达或激光雷达传感器产生的信息准确性低。
- en: Now that you have a good understanding of how a camera works, as well as its
    basic parts and terminology, it's time to get your hands dirty and start calibrating
    a camera with OpenCV.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经很好地理解了相机的工作原理，以及其基本部分和术语，是时候动手使用OpenCV校准相机了。
- en: Camera calibration with OpenCV
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV进行相机校准
- en: In this section, you will learn how to take objects with a known pattern and
    use them to correct lens distortion using OpenCV.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何使用已知模式的物体，并使用OpenCV来纠正镜头扭曲。
- en: Remember the lens distortion we talked about in the previous section? You need
    to correct this to ensure you accurately locate where objects are relative to
    your vehicle. It does you no good to see an object if you don't know whether it
    is in front of you or next to you. Even good lenses can distort the image, and
    this is particularly true for wide-angle lenses. Luckily, OpenCV provides a mechanism
    to detect this distortion and correct it!
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在上一节中提到的镜头扭曲吗？您需要纠正这一点，以确保您能够准确地定位物体相对于车辆的位置。如果您不知道物体是在您前面还是旁边，那么看到物体对您没有任何好处。即使是好的镜头也可能扭曲图像，这尤其适用于广角镜头。幸运的是，OpenCV提供了一个检测这种扭曲并纠正它的机制！
- en: The idea is to take pictures of a chessboard, so OpenCV can use this high-contrast
    pattern to detect the position of the points and compute the distortion based
    on the difference between the expected image and the recorded one.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是拍摄棋盘的图片，这样OpenCV就可以使用这种高对比度图案来检测点的位置，并根据预期图像与记录图像之间的差异来计算扭曲。
- en: 'You need to provide several pictures at different orientations. It might take
    some experiments to find a good set of pictures, but 10 to 20 images should be
    enough. If you use a printed chessboard, take care to have the paper as flat as
    possible so as to not compromise the measurements:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要提供几张不同方向的图片。可能需要一些实验来找到一组好的图片，但10到20张图片应该足够了。如果您使用的是打印的棋盘，请确保纸张尽可能平整，以免影响测量：
- en: '![Figure 1.20 – Some examples of pictures that can be used for calibration](img/Figure_1.20_B16322.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图1.20 – 一些可用于校准的图片示例](img/Figure_1.20_B16322.jpg)'
- en: Figure 1.20 – Some examples of pictures that can be used for calibration
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20 – 一些可用于校准的图片示例
- en: As you can see, the central image clearly shows some barrel distortion.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，中心图像清楚地显示了某些桶形扭曲。
- en: Distortion detection
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扭曲检测
- en: OpenCV tries to map a series of three-dimensional points to the two-dimensional
    coordinates of the camera. OpenCV will then use this information to correct the
    distortion.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV试图将一系列三维点映射到相机的二维坐标。然后，OpenCV将使用这些信息来纠正扭曲。
- en: 'The first thing to do is to initialize some structures:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 首件事是初始化一些结构：
- en: '[PRE38]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Please note `nX` and `nY`, which are the number of points to find in the chessboard
    on the `x` and `y` axes,, respectively. In practice, this is the number of squares
    minus 1.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to call `findChessboardCorners()`:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`found` is true if OpenCV found the points, and `corners` will contain the
    points found.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: In our code, we will assume that the image has been converted into grayscale,
    but you can calibrate using an RGB picture as well.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCV provides a nice image depicting the corners found, ensuring that the
    algorithm is working properly:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s see the resulting image:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Corners of the calibration image found by OpenCV](img/Figure_1.21_B16322.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – Corners of the calibration image found by OpenCV
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Calibration
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After finding the corners in several images, we are finally ready to generate
    the calibration data using `calibrateCamera()`:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, we are ready to correct our images, using `undistort()`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s see the result:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.22 – Original image and calibrated image](img/Figure_1.22_B16322.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: Figure 1.22 – Original image and calibrated image
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the second image has less barrel distortion, but it is not great.
    We probably need more and better calibration samples.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'But we can also try to get more precision from the same calibration images
    by looking for `cornerSubPix()` after `findChessboardCorners()`:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following is the resulting image:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.23 – Image calibrated with sub-pixel precision](img/Figure_1.23_B16322.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
- en: Figure 1.23 – Image calibrated with sub-pixel precision
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: As the complete code is a bit long, I recommend checking out the full source
    code on GitHub.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, you have had a great start to your computer vision journey toward making
    a real self-driving car.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: You learned about a very useful toolset called OpenCV with bindings for Python
    and NumPy. With these tools, you are now able to create and import images using
    methods such as `imread()`, `imshow()`, `hconcat()`, and `vconcat()`. You learned
    how to import and create video files, as well as capturing video from a webcam
    with methods such as `VideoCapture()` and `VideoWriter()`. Watch out Spielberg,
    there is a new movie-maker in town!
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: It was wonderful to be able to import images, but how do you start manipulating
    them to help your computer vision algorithms learn what features matter? You learned
    how to do this through methods such as `flip()`, `blur()`, `GaussianBlur()`, `medianBlur()`,
    `bilateralFilter()`, and `convertScaleAbs()`. Then, you learned how to annotate
    images for human consumption with methods such as `rectangle()` and `putText()`.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Then came the real magic, where you learned how to take the images and do your
    first piece of real computer vision using HOG to detect pedestrians. You learned
    how to apply a sliding window to scan the detector over an image in various sized
    windows using the `detectMultiScale()` method, with parameters such as `winStride`,
    `padding`, `scale`, `hitThreshold`, and `finalThreshold`.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: You had a lot of fun with all the new tools you learned for working with images.
    But there was something missing. How do I get these images on my self-driving
    car? To answer this, you learned about the camera and its basic terminology, such
    as **resolution**, **FoV**, **focal length**, **aperture**, **DoF**, **dynamic
    range**, **ISO**, **frame rate**, **lens flare**, and finally, **lens distortion**.
    Then, you learned the basic components that comprise a camera, namely the lens,
    aperture, and light-sensitive arrays. With these basics, you moved on to some
    considerations for choosing a camera for your application by learning about the
    strengths and weaknesses of a camera.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 你在使用图像处理的新工具时玩得很开心。但似乎还缺少了什么。我该如何将这些图像应用到我的自动驾驶汽车上？为了回答这个问题，你学习了关于相机及其基本术语，例如**分辨率**、**视场角**、**焦距**、**光圈**、**景深**、**动态范围**、**ISO**、**帧率**、**镜头眩光**，以及最后的**镜头畸变**。然后，你学习了组成相机的基本组件，即镜头、光圈和光敏阵列。有了这些基础知识，你继续学习如何根据相机的优缺点来选择适合你应用的相机。
- en: Armed with this knowledge, you boldly began to remove one of these weaknesses,
    lens distortion, with the tools you learned in OpenCV for distortion correction.
    You used methods such as `findChessboardCorners()`, `calibrateCamera()`, `undistort()`,
    and `cornerSubPix()` for this.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些知识，你勇敢地开始使用在OpenCV中学习的畸变校正工具来消除这些弱点之一，即镜头畸变。你使用了`findChessboardCorners()`、`calibrateCamera()`、`undistort()`和`cornerSubPix()`等方法来完成这项工作。
- en: Wow, you are really on your way to being able to perceive the world in your
    self-driving application. You should take a moment and be proud of what you have
    learned so far. Maybe you can celebrate with a selfie and apply some of what you
    learned!
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，你真的在朝着能够在自动驾驶应用中感知世界的方向前进。你应该花点时间为自己到目前为止所学到的感到自豪。也许你可以通过自拍来庆祝，并应用一些你所学的知识！
- en: In the next chapter, you are going to learn about some of the basic signal types
    and protocols you are likely to encounter when trying to integrate sensors in
    your self-driving application.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习一些基本信号类型和协议，这些类型和协议在你尝试将传感器集成到自动驾驶应用中时可能会遇到。
- en: Questions
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can OpenCV take advantage of hardware acceleration?
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenCV能否利用硬件加速？
- en: What's the best blurring method if CPU power is not a problem?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果CPU性能不是问题，最好的模糊方法是什么？
- en: Which detector can be used to find pedestrians in an image?
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个检测器可以用来在图像中找到行人？
- en: How can you read the video stream from a webcam?
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你该如何从网络摄像头中读取视频流？
- en: What is the trade-off between aperture and depth of field?
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 光圈和景深之间的权衡是什么？
- en: When do you need a high ISO?
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在什么情况下你需要高ISO？
- en: Is it worth computing sub-pixel precision for camera calibration?
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为相机校准计算亚像素精度值得吗？
