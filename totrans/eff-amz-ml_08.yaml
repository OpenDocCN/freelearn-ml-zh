- en: Creating Datasources from Redshift
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Redshift创建数据源
- en: In this chapter, we will use the power of SQL queries to address non-linear
    datasets. Creating datasources in Redshift or RDS gives us the potential for upstream
    SQL-based feature engineering prior to the datasource creation. We implemented
    a similar approach in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml),
    *Loading and Preparing the Dataset*, by leveraging the new AWS Athena service
    to apply preliminary transformations on the data before creating the datasource.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将利用SQL查询的力量来处理非线性数据集。在创建数据源之前，在Redshift或RDS中创建数据源为我们提供了上游基于SQL的特征工程潜力。我们在[第4章](08d9b49a-a25c-4706-8846-36be9538b087.xhtml)，*加载数据集和准备数据*中实施了一种类似的方法，通过利用新的AWS
    Athena服务在创建数据源之前对数据进行初步转换。
- en: This enabled us to expand the `Titanic` dataset by creating new features, such
    as the `Deck` number, replacing the `Fare` with its log or replacing missing values
    for the `Age` variable. The SQL transformations were simple, but allowed us to
    expand the original dataset in a very flexible way. The **AWS Athena** service
    is S3 based. It allows us to run SQL queries on datasets hosted on S3 and dump
    the results in S3 buckets. We were still creating Amazon ML datasources from S3,
    but simply adding an extra data preprocessing layer to massage the dataset.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够通过创建新的特征，如“甲板”号，用其对数替换“票价”，或替换“年龄”变量的缺失值来扩展“泰坦尼克”数据集。SQL转换很简单，但使我们能够非常灵活地扩展原始数据集。**AWS
    Athena**服务是基于S3的。它允许我们在S3上托管的数据集中运行SQL查询，并将结果存储在S3桶中。我们仍然从S3创建亚马逊ML数据源，但只是添加了一个额外的数据预处理层来整理数据集。
- en: 'AWS offers two other SQL services from which it is possible to create datasources:
    RDS and Redshift. Datasource creation is very similar for both RDS and Redshift,
    and we will focus on creating datasources in Redshift via the Python SDK `Boto3`.
    The key point for us is that RDS/Redshift-based datasources are created directly
    via SQL queries, thus giving us the opportunity for further feature engineering.
    Redshift also integrates seamlessly with AWS Kinesis, a service we''ll explore
    for streaming data in [Chapter 9](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=1053),
    *Building a Streaming Data Analysis Pipeline*.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: AWS还提供了另外两种SQL服务，可以从这些服务中创建数据源：RDS和Redshift。RDS和Redshift的数据源创建非常相似，我们将重点介绍通过Python
    SDK `Boto3`在Redshift中创建数据源。对我们来说，关键点是基于RDS/Redshift的数据源是通过SQL查询直接创建的，这为我们提供了进一步的特征工程的机会。Redshift还与AWS
    Kinesis无缝集成，我们将在[第9章](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=1053)，*构建流数据分析管道*中探讨该服务。
- en: Amazon Machine Learning is built on intrinsically linear models leveraging with
    good results quantile binning data transformation as a method to handle non-linearities
    in the dataset. Polynomial regression is another important machine learning method
    used to deal with non-linear datasets. We will use our new SQL powers to implement
    polynomial regression using Amazon ML.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习建立在本质上线性的模型之上，利用良好的结果量级分箱数据转换作为处理数据集中非线性的一种方法。多项式回归是另一种重要的机器学习方法，用于处理非线性数据集。我们将利用我们新的SQL功能，通过亚马逊ML实现多项式回归。
- en: 'In this chapter, you will learn the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下内容：
- en: Choosing between RDS and Redshift
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在RDS和Redshift之间进行选择
- en: How to create a Redshift database with PostgreSQL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用PostgreSQL创建Redshift数据库
- en: How to load your S3 data into Redshift
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将您的S3数据加载到Redshift
- en: How to create a datasource from Redshift
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从Redshift创建数据源
- en: What is polynomial regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是多项式回归
- en: How to use polynomial regression with Amazon ML
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用亚马逊ML进行多项式回归
- en: Choosing between RDS and Redshift
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在RDS和Redshift之间进行选择
- en: 'AWS offers no less than six different cloud database and SQL/NoSQL services:
    RDS, Aurora, DynamoDB, Redshift, Athena, and AWS Database Migration Service! Out
    of all these services, only two are compatible with Amazon Machine Learning: RDS
    and Redshift. You can store data in either service and create datasources from
    these sources. The datasource creation methods for the two services have similar
    parameters, but differ quite significantly when it comes to the underlying AWS
    service communication.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了不少于六种不同的云数据库和SQL/NoSQL服务：RDS、Aurora、DynamoDB、Redshift、Athena和AWS数据库迁移服务！在所有这些服务中，只有两个与亚马逊机器学习兼容：RDS和Redshift。您可以在任一服务中存储数据，并从这些来源创建数据源。这两个服务的源数据创建方法具有类似的参数，但在底层AWS服务通信方面差异很大。
- en: RDS and Redshift are very different services. Redshift is a data warehouse used
    to answer a few complex and long running queries on large datasets, while RDS
    is made for frequent, small, and fast queries. Redshift is more suited for massive
    parallel processing to perform operations on millions of rows of data with minimal
    latency, while RDS offers a server instance that runs a given database. RDS offers
    several different database types – MySQL, PostgreSQL, MariaDB, Oracle, SQL Server,
    and Amazon Aurora, while Redshift is Amazon's own analytics database offering
    based on the **ParAccel** technology and running a fork of PostgreSQL. You connect
    to Redshift using standard ODBC and JDBC connections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RDS和Redshift是两种非常不同的服务。Redshift是一个数据仓库，用于在大数据集上回答一些复杂且运行时间较长的查询，而RDS是为频繁、小规模和快速查询而设计的。Redshift更适合进行大规模并行处理，以最小延迟执行数百万行数据的操作，而RDS提供了一个运行特定数据库的服务器实例。RDS提供多种不同的数据库类型——MySQL、PostgreSQL、MariaDB、Oracle、SQL
    Server和Amazon Aurora，而Redshift是基于**ParAccel**技术的亚马逊自己的分析数据库，运行的是PostgreSQL的一个分支。您可以使用标准的ODBC和JDBC连接连接到Redshift。
- en: Amazon Redshift and PostgreSQL have a number of very important differences that
    you must be aware of as you build your database in Redshift. Many functions, data
    types, and PostgreSQL features are not supported in Amazon Redshift. More info
    is available at [http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html](http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在您在Redshift中构建数据库时，亚马逊Redshift和PostgreSQL之间存在许多非常重要的差异，您必须注意。许多函数、数据类型和PostgreSQL功能在亚马逊Redshift中不受支持。更多信息可在[http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html](http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html)找到。
- en: A more in-depth explanation of the difference between RDS and Redshift can be
    found on this thread: [https://www.quora.com/What-is-the-difference-between-redshift-and-RDS](https://www.quora.com/What-is-the-difference-between-redshift-and-RDS)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RDS和Redshift之间差异的更深入解释，可以在本线程中找到：[https://www.quora.com/What-is-the-difference-between-redshift-and-RDS](https://www.quora.com/What-is-the-difference-between-redshift-and-RDS)
- en: 'In the context of Amazon ML, an important difference between the two services
    is that the Amazon ML web console only allows for datasource creation from S3
    and Redshift, but not from RDS, as shown in this screenshot:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在亚马逊机器学习（Amazon ML）的背景下，这两种服务之间的重要区别在于，亚马逊机器学习Web控制台仅允许从S3和Redshift创建数据源，但不能从RDS创建，如本截图所示：
- en: '![](img/B05028_09_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_01.png)'
- en: The Python SDK and the AWS CLI, however, both allow datasource creation from
    RDS and Redshift.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Python SDK和AWS CLI都允许从RDS和Redshift创建数据源。
- en: '**SDK:**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**SDK**：'
- en: '`create_data_source_from_rds()`:  [http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_data_source_from_rds()`：[http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds)'
- en: '`create_data_source_from_redshift()`: [http://boto3.readthedocs.io/en/latest/reference/service/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_data_source_from_redshift()`：[http://boto3.readthedocs.io/en/latest/reference/service/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift)'
- en: '**CLI**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLI**：'
- en: '`create-data-source-from-rds`: [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-data-source-from-rds`：[http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html)'
- en: '`create-data-source-from-redshift`: [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create-data-source-from-redshift`：[http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html)'
- en: 'We now compare the parameters required by the Python SDK to connect to either
    services:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们比较Python SDK连接到任一服务所需的参数：
- en: 'Redshift parameters:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redshift参数：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'RDS parameters:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDS参数：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The difference between the two sets of parameters lies in the way we allow
    access to the data store. Both sets include `DatabaseInformation`, `DatabaseCredentials`,
    `SelectSqlQuery`, `DataSchema`, and `DataRearrangement`. RDS also requires manually
    setting up two roles with the right policies: `ResourceRole: DataPipelineDefaultRole`
    and `ServiceRole:DataPipelineDefaultResourceRole`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '这两组参数之间的区别在于我们允许访问数据存储的方式。两组都包括 `DatabaseInformation`、`DatabaseCredentials`、`SelectSqlQuery`、`DataSchema`
    和 `DataRearrangement`。RDS 还需要手动设置两个具有正确策略的角色：`ResourceRole: DataPipelineDefaultRole`
    和 `ServiceRole:DataPipelineDefaultResourceRole`。'
- en: RDS is more adapted to our volume of data than Redshift, and we should use RDS
    instead of Redshift for our machine learning project. However, we previously found
    that manually creating the roles and policies for RDS required an in-depth knowledge
    of AWS inner permissions workings, which was too complex for this book. Although
    the parameters and concepts are very similar for creating datasources from RDS
    and Redshift, in the background, they differ quite a lot. RDS datasource creation
    involves the creation of AWS data pipelines, another AWS service that allows you
    to process and move data between different AWS computeing and storage services.
    Having to set up data pipelines adds a non-trivial layer of complexity to the
    whole project.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RDS 更适合我们的数据量，我们应该在机器学习项目中使用 RDS 而不是 Redshift。然而，我们之前发现手动创建 RDS 的角色和策略需要深入了解
    AWS 内部权限的工作方式，这对于本书来说过于复杂。尽管从 RDS 和 Redshift 创建数据源时的参数和概念非常相似，但在后台，它们有很大的不同。RDS
    数据源创建涉及创建 AWS 数据管道，这是另一个 AWS 服务，允许您在不同 AWS 计算和存储服务之间处理和移动数据。必须设置数据管道为整个项目增加了非平凡的复杂性层。
- en: Redshift, on the other hand, does not require building a data pipeline and setting
    permissions, roles, and policies to create datasources. In the end, this extra
    simplicity made Redshift more adapted to this book, as we wanted to keep the focus
    on the machine learning side of things and not delve into the intricacies of AWS
    access roles and policies, although RDS would have been more suited for our low
    volume of data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Redshift 不需要构建数据管道和设置权限、角色和策略来创建数据源。最终，这种额外的简单性使 Redshift 更适合本书，因为我们希望保持对机器学习方面的关注，而不是深入研究
    AWS 访问角色和策略的复杂性，尽管 RDS 对于我们低数据量来说可能更合适。
- en: '**Redshift**: Presenting Redshift in depth far exceeds the scope of this book.
    We recommend the *Getting Started with Amazon Redshift* book by *Stefan Bauer,
    Packt* ([https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift)](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift), the
    AWS documentation ([https://aws.amazon.com/redshift/getting-started/](https://aws.amazon.com/redshift/getting-started/)) and
    this blog post for a good introduction to Cluster Configuration ([https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration](https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**Redshift**：深入介绍 Redshift 超出了本书的范围。我们推荐阅读由 *Stefan Bauer, Packt* 编写的 *《Amazon
    Redshift 入门》* 书籍 ([https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift)](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift))，AWS
    文档 ([https://aws.amazon.com/redshift/getting-started/](https://aws.amazon.com/redshift/getting-started/))
    以及这篇关于集群配置的博客文章 ([https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration](https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration))，以获得对集群配置的良好介绍。'
- en: Let's start with Redshift, using the AWS Redshift console to create a PostgreSQL-based
    instance and load the `Titanic` dataset we already have available in our S3 bucket.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Redshift 开始，使用 AWS Redshift 控制台创建一个基于 PostgreSQL 的实例，并加载我们已经在 S3 存储桶中可用的
    `Titanic` 数据集。
- en: Creating a Redshift instance
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 Redshift 实例
- en: Log in to your AWS account, and go to the Redshift dashboard at [https://console.aws.amazon.com/redshift/](https://console.aws.amazon.com/redshift/).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 登录您的 AWS 账户，并转到 Redshift 仪表板，链接为 [https://console.aws.amazon.com/redshift/](https://console.aws.amazon.com/redshift/)。
- en: 'Creating a database in Redshift is quite simple and well handled by the AWS
    Redshift wizard. First, click on the Launch Cluster button. In the first screen,
    we define the Cluster identifier* as `amlpackt`, the Database name as `amlpacktdb`,
    and the Master user name as shown in the following screenshot:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Redshift 中创建数据库非常简单，并且由 AWS Redshift 向导很好地处理。首先，点击“启动集群”按钮。在第一个屏幕中，我们定义集群标识符*为
    `amlpackt`，数据库名称为 `amlpacktdb`，以及主用户名，如以下截图所示：
- en: '![](img/B05028_09_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_09_02.png)'
- en: 'In the next screen, we choose the default parameters to configure the node,
    as shown in the following screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一屏幕中，我们选择默认参数来配置节点，如下面的截图所示：
- en: '![](img/B05028_09_03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_03.png)'
- en: 'Choose the default settings for the next configuration screen, but make sure
    that the cluster is publicly accessible. You do not need a public IP:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配置屏幕中选择默认设置，但请确保集群是公开可访问的。您不需要公共 IP：
- en: '![](img/B05028_09_04.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_04.png)'
- en: 'Choose the Machine Learning/RDS VPC security group:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 选择机器学习/RDS VPC 安全组：
- en: '![](img/B05028_09_05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_05.png)'
- en: 'The final validation screen prior to launching the cluster will inform you
    about the associated costs as shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动集群之前的最终验证屏幕将显示相关的成本，如下所示：
- en: '![](img/B05028_09_06.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_06.png)'
- en: 'It will take a few minutes once you click on the final Launch Cluster button
    for the cluster to be ready. We will connect to the newly created database using
    Psql. Other external connection types are available through JDBC and ODBC. The
    cluster information page shows the Endpoint URL that you need to connect to the
    newly created database:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 点击最终的启动集群按钮后，集群准备就绪可能需要几分钟。我们将使用 Psql 连接到新创建的数据库。其他外部连接类型可通过 JDBC 和 ODBC 获得。集群信息页面显示了您需要连接到新创建数据库的端点
    URL：
- en: '![](img/B05028_09_07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_07.png)'
- en: Connecting through the command line
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过命令行连接
- en: Psql is a command line program that acts as the primary front-end to a Postgresql
    database. It provides a number of shell commands (`pg_dump` to dump the content
    of a database, `createdb` to create a database, and many others). It also has
    a number of meta commands to list elements and display information (see the **Psql
    cheatsheet** information box).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Psql 是一个命令行程序，充当 PostgreSQL 数据库的主要前端。它提供了一系列的 shell 命令（例如，`pg_dump` 用于转储数据库内容，`createdb`
    用于创建数据库，以及其他许多命令）。它还具有许多元命令来列出元素和显示信息（请参阅 **Psql 快速参考** 信息框）。
- en: '**Psql cheatsheet:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Psql 快速参考**：'
- en: '`q`: Quit/Exit'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`q`: 退出/退出'
- en: '`d __table__`: Show table definition, including triggers'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`d __table__`: 显示表定义，包括触发器'
- en: '`dt` : List tables'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`dt` : 列出表'
- en: '`df`: List functions'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`df`: 列出函数'
- en: '`dv`: List views'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`dv`: 列出视图'
- en: '`x`: Pretty-format query results instead of the not-so-useful ASCII tables'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`: 以更美观的格式显示查询结果，而不是不那么有用的 ASCII 表'
- en: '`connect __database__`: Connect to a database'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`connect __database__`: 连接到数据库'
- en: '`l`: List databases'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`l`: 列出数据库'
- en: 'Adding a `+` on a `d` command will show more results: compare, for instance,
    d titanic with `d+ titanic`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `d` 命令上添加一个 `+` 将显示更多结果：例如，比较 `d titanic` 与 `d+ titanic`
- en: see [http://postgresguide.com/utilities/psql.html](http://postgresguide.com/utilities/psql.html) for
    more info on Psql
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Psql 的更多信息，请参阅[http://postgresguide.com/utilities/psql.html](http://postgresguide.com/utilities/psql.html)
- en: 'You can now connect from the terminal to your database using **Psql** with
    the following command, using the endpoint URL (`amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com`)
    for the host:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用以下命令从终端连接到您的数据库使用 **Psql**，使用端点 URL (`amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com`)
    作为主机：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `amlpacktdb` database is, of course, empty:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，`amlpacktdb` 数据库是空的：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There are many ways we can import data into a Redshift database. To keep things
    simple, we will upload a CSV file available on S3 to a Redshift table with the
    copy command, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方式可以将数据导入 Redshift 数据库。为了简化，我们将使用复制命令将 S3 上可用的 CSV 文件上传到 Redshift 表，如下所示：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When using the copy command, Redshift needs to authenticate with the S3 service.
    Authentication between AWS services can be implemented in two ways:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用复制命令时，Redshift 需要与 S3 服务进行认证。AWS 服务之间的认证可以通过两种方式实现：
- en: '**User-based**: authentication is granted by  passing the user access keys.
    It is a simpler mean of granting access between AWS services but it is not always
    available.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于用户的**：通过传递用户访问密钥进行认证。这是一种在 AWS 服务之间授予访问权限的更简单方式，但它并不总是可用。'
- en: '**Role-based**: authentication requires creating roles with the right policies
    and permissions. It is a preferred andmore secure mean of authentication than
    user-based authentication. However, it requires extra roles and policies creation
    steps and is less straightforward to setup.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于角色的**：认证需要创建具有正确策略和权限的角色。与基于用户的认证相比，这是一种更受欢迎且更安全的认证方式。然而，它需要额外的角色和策略创建步骤，并且设置起来不太直接。'
- en: 'More info on user versus role based authentication for AWS services is available
    at [http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html](http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html).
    In our copy example, we plan to use the aws access keys of our main AWS user.
    But before we can copy data into the table, we first need to create it with an
    SQL query. For the `Titanic` CSV file we have been working on, the table creation
    query is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 关于AWS服务的用户与基于角色的身份验证的更多信息，请参阅[http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html](http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html)。在我们的复制示例中，我们计划使用我们主要AWS用户的aws访问密钥。但在我们可以将数据复制到表中之前，我们首先需要使用SQL查询创建它。对于我们一直在工作的`Titanic`
    CSV文件，创建表的查询如下：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The table now exists in our Redshift database, as shown by the `dt` command:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如`dt`命令所示，该表现在存在于我们的Redshift数据库中：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The table structure is as expected, as shown by the  `d+` command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表结构如预期所示，如`d+`命令所示：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now upload the CSV file to S3 and fill in our table by running the following
    commands from your terminal:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将CSV文件上传到S3，并从终端运行以下命令来填充我们的表：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the CSV file should not include the CSV headers. To verify that the
    copy command worked, we can count the number of records in the Titanic table by
    running the following query:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，CSV文件不应包含CSV标题。为了验证复制命令是否成功，我们可以通过运行以下查询来计算`titanic`表中的记录数：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The result shows we now have 1309 records in the titanic table.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，我们现在在`titanic`表中有了1309条记录。
- en: Executing Redshift queries using Psql
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Psql执行Redshift查询
- en: 'We''ve seen that we can connect to our database with the following `Psql` command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到我们可以使用以下`Psql`命令连接到我们的数据库：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then need to type in our password. To shorten the line and avoid having
    to type the password each time, we can set both the connection string and the
    password as shell environment variables. In your terminal, execute the following
    command to create a global `REDSHIFT_CONNECT` shell variable:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要输入我们的密码。为了缩短行并避免每次都输入密码，我们可以将连接字符串和密码都设置为shell环境变量。在你的终端中，执行以下命令以创建全局`REDSHIFT_CONNECT`
    shell变量：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Similarly for the password, execute the command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于密码，执行以下命令：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'From now on, you can connect to the database with the simple command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，你可以使用以下简单命令连接到数据库：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note that `REDSHIFT_CONNECT` is a variable name we chose, while `PGPASSWORD`
    is a predefined shell variable name that is recognized by Psql.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`REDSHIFT_CONNECT`是我们选择的变量名，而`PGPASSWORD`是一个由Psql识别的预定义shell变量名。
- en: 'We now have a choice in the way we can run queries on our Redshift database.
    We can perform either of the following steps:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以选择在Redshift数据库上运行查询的方式。我们可以执行以下步骤中的任何一个：
- en: '`Psql` into the database shell and type some SQL queries:'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Psql`进入数据库shell并输入一些SQL查询：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Write the SQL query into a file (for instance, `my_file.sql`) and, from the
    terminal, run the command:'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将SQL查询写入文件（例如，`my_file.sql`），然后从终端运行以下命令：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the query directly with the `Psql` command:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Psql`命令直接运行查询：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are now ready to work on our dataset. Since we have already worked extensively
    on the Titanic dataset, we will use another dataset for the rest of the chapter.
    Let's create an artificial dataset that exhibits strong non-linear patterns.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始处理我们的数据集了。由于我们已经对泰坦尼克号数据集进行了广泛的研究，我们将使用另一个数据集来完成本章的剩余部分。让我们创建一个表现出强烈非线性模式的合成数据集。
- en: Creating our own non-linear dataset
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们自己的非线性数据集
- en: 'A good way to create a non-linear dataset is to mix sines with different phases.
    The dataset we will work with in this chapter is created with the following Python
    script and exported to a CSV file:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 创建非线性数据集的一个好方法是混合不同相位的正弦波。我们将在本章中使用以下Python脚本创建数据集，并将其导出为CSV文件：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As usual, *X* is the predictor, and *y* the outcome. You can use variations
    on that script to easily generate other non-linear datasets. Note that we have
    used a `lambda` function, which is a Pythonic way of declaring a function on the
    spot when needed. Then we shuffle the dataset by sorting randomly (`np.random.rand(n_samples)`). We
    then save the data to a CSV file (`nonlinear.csv`) using the **Pandas** dataframe:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，*X*是预测变量，*y*是结果。你可以通过修改该脚本轻松生成其他非线性数据集。注意，我们使用了`lambda`函数，这是一种在需要时即时声明函数的Python方式。然后我们通过随机排序（`np.random.rand(n_samples)`）对数据集进行洗牌。然后我们使用**Pandas**数据框将数据保存到CSV文件（`nonlinear.csv`）中：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Plotting the data gives the following graph:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制数据给出以下图表：
- en: '![](img/B05028_09_11.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_11.png)'
- en: It is obviously not linear. A line has no hope of approximating, let alone predicting
    the outcome *y* from the predictor *X*. Now that we have a highly non-linear dataset
    available, we need to upload it to Redshift.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 显然不是线性的。一条线根本无法近似，更不用说从预测变量*X*预测结果*y*了。现在我们有一个高度非线性的数据集可用，我们需要将其上传到Redshift。
- en: Uploading the nonlinear data to Redshift
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将非线性数据上传到Redshift
- en: 'We first need to create the table that will host the data. We will call that
    table `nonlinear`. It only has three columns: an index, the predictor `X`, and
    the outcome `y`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个将托管数据的表。我们将把这个表称为`nonlinear`。它只有三个列：一个索引，预测变量`X`和结果`y`：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the table is created, we can upload the CSV file to S3, connect to the
    database, and import the data in the non-linear table with the command:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了表，我们就可以将CSV文件上传到S3，连接到数据库，并使用以下命令将数据导入非线性表：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can verify that the `nonlinear` table now has a thousand rows with the query:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查询来验证`nonlinear`表现在有一千行：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our data has been uploaded to Redshift. We are ready to create datasources and
    train and evaluate models! But before we dive into model building on this dataset,
    let's introduce the polynomial regression method, which will allow us to deal
    with this highly non-linear dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据已经上传到Redshift。我们准备创建数据源并训练和评估模型！但在我们深入到这个数据集上的模型构建之前，让我们介绍多项式回归方法，这将使我们能够处理这个高度非线性的数据集。
- en: Introducing polynomial regression
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍多项式回归
- en: In two dimensions, where we have a predictor and an outcome, linear modeling
    is all about finding the best line that approximates your data. In three dimensions
    (two predictors and one outcome), the idea is then to find the best plane, or
    the best flat surface, that approximates your data. In the `N` dimension, the
    surface becomes an hyperplane, but the goal is always the same – to find the hyperplane
    of dimension *N-1* that gives the best approximation for regression or that separates
    the classes the best for classification. That hyperplane is always flat.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，我们有一个预测变量和一个结果，线性建模就是找到最佳直线来近似你的数据。在三维空间（两个预测变量和一个结果）中，想法是找到最佳平面，或者最佳平坦表面，来近似你的数据。在`N`维空间中，表面变成了超平面，但目标始终相同——找到维度为*N-1*的超平面，以给出回归的最佳近似或最好地分离类。那个超平面总是平坦的。
- en: Coming back to the very non-linear two-dimensional dataset we created, it is
    obvious that no line can properly approximate the relation between the predictor
    and the outcome. There are many different methods to model non-linear data, including
    polynomial regression, step functions, splines, and **Generalized additive models**
    (**GAM**). See *Chapter 7* of *An Introduction to Statistical Learning* by *James,
    Witten*, *Hastie* and *Tibshirani Springer, 2013* for a great introduction to
    these methods. The book is available in PDF at [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/).
    We will apply the polynomial regression method.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们创建的非常非线性二维数据集，很明显，没有任何一条线能够恰当地近似预测变量和结果之间的关系。有许多不同的方法可以用来建模非线性数据，包括多项式回归、阶梯函数、样条曲线和**广义加性模型**（**GAM**）。参见由James,
    Witten, Hastie和Tibshirani编写的《统计学习引论》的第7章，那里对这些方法有很好的介绍。这本书的PDF版本可在[http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)找到。我们将应用多项式回归方法。
- en: 'Polynomial regression consists in replacing the standard linear model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归包括用标准线性模型替换：
- en: '![](img/B05028_09_12.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_12.png)'
- en: Here, *ŷ* is the predicted outcome, *x* the predictor, (*w[o], w[1]*) the linear
    model coefficients. By a polynomial function of order *k:*
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*ŷ*是预测结果，*x*是预测变量，(*w[0], w[1]*)是线性模型的系数。通过一个阶数为*k*的多项式函数：
- en: '![](img/B05028_09_13.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_09_13.png)'
- en: The power of the polynomial regression approach is that we can use the same
    linear modeling method as with the linear model, and we can therefore still use
    Amazon ML SGD to find the coefficients {*w[k]*} of the polynomial regression equation.
    In the next section, we will train successive models by increasing the degree
    of the polynomial.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归方法的优势在于我们可以使用与线性模型相同的线性建模方法，因此我们仍然可以使用Amazon ML SGD来找到多项式回归方程的系数{*w[k]*}。在下一节中，我们将通过增加多项式的次数来训练连续的模型。
- en: Establishing a baseline
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建立基线
- en: 'We first need to establish a baseline. Amazon ML quantile binning transformation
    is Amazon Machine Learning preferred method of dealing with non-linearities in
    a dataset. Let''s establish how a simple linear model performs with Amazon''s
    default recipe. We will create a baseline score using the usual AWS console tools.
    This time, we choose to create a data source from Redshift and not from S3\. Fill
    the information as shown in the next screenshot, and click on Test access to check
    your access, and at the same time, make Amazon ML create the necessary IAM role.
    Finally, click on Verify once you''re done:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要建立一个基准。Amazon ML的量级分箱转换是处理数据集中非线性问题的Amazon机器学习首选方法。让我们看看一个简单的线性模型使用Amazon的默认配方表现如何。我们将使用常规的AWS控制台工具创建一个基准分数。这次，我们选择从Redshift而不是S3创建数据源。填写下一张截图所示的信息，然后点击“测试访问”以检查您的访问权限，同时让Amazon
    ML创建必要的IAM角色。完成后，点击“验证”：
- en: '![](img/B05028_09_08.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_09_08.png)'
- en: 'Amazon ML handles all the role and policy creation in the background by creating
    the following resources:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML通过创建以下资源在后台处理所有角色和策略的创建：
- en: 'A new role: AmazonMLRedshift_us-east-1_amlpackt. We will use the arn related
    to this role when creating datsources using the Python SDK.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新角色：AmazonMLRedshift_us-east-1_amlpackt。我们将使用与此角色相关的arn在Python SDK创建数据源时使用。
- en: 'Two new policies attached to this role:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 附属于此角色的两个新策略：
- en: '`AmazonMLS3LocationAccess_aml.packt`'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AmazonMLS3LocationAccess_aml.packt`'
- en: '`AmazonMLRedshiftAccess_us-east-1_amlpackt`'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AmazonMLRedshiftAccess_us-east-1_amlpackt`'
- en: AWS also sets up the `Trust Relationship` so that `roleAmazonMLRedshift_us-east-1_amlpackt` is
    able assume a `machinelearning` service role.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS还设置了`信任关系`，使得`roleAmazonMLRedshift_us-east-1_amlpackt`能够承担`machinelearning`服务角色。
- en: 'Creating these roles and policies manually requires a solid understanding of
    access permissions between services in AWS. Creating them using the console is
    a significant time-saver. The next steps are standard schema and target definition,
    and data source creation. The default schema generated by Amazon ML is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 手动创建这些角色和策略需要深入了解AWS中服务之间的访问权限。使用控制台创建它们可以节省大量时间。接下来的步骤是标准模式和目标定义，以及数据源创建。Amazon
    ML生成的默认模式如下：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We reuse that schema later on by saving the JSON string into the `data/nonlinear.schema`
    file and uploading it to S3 with `aws s3 cp data/nonlinear.schema s3://aml.packt/data/ch9/` .
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后通过将JSON字符串保存到`data/nonlinear.schema`文件并将它上传到S3（使用`aws s3 cp data/nonlinear.schema
    s3://aml.packt/data/ch9/`）来重用该模式。
- en: 'Once the datasource is available, we can create and evaluate a model via the
    console. The recipe generated by Amazon ML during the model creation uses the
    quantile binning transformation on the predictor with 500 bins, which may seem
    like a large value since we only have 700 samples in the training dataset. The
    auto-generated Amazon ML recipe is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据源可用，我们就可以通过控制台创建和评估一个模型。Amazon ML在模型创建期间生成的配方对预测变量使用500个分箱的量级分箱转换，这可能看起来像是一个很大的值，因为我们只有700个训练数据集样本。自动生成的Amazon
    ML配方如下：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We train a model with L2 mild regularization and 100 passes, and evaluate that
    model on 30% of our dataset. We obtain the following results:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用L2轻微正则化和100次迭代训练一个模型，并在我们数据集的30%上评估该模型。我们得到了以下结果：
- en: With quantile binning
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量级分箱
- en: 'RMSE: 0.1540'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE：0.1540
- en: Baseline RMSE: 1.034
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准RMSE：1.034
- en: Without quantile binning
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用量级分箱
- en: 'RMSE: 1.0207'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSE：1.0207
- en: Baseline RMSE: 1.025
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准RMSE：1.025
- en: Quantile binning correctly handles the non-linearities and results in a pretty
    good score, while a raw linear model does not fare much better than the baseline.
    In the case of linear regression, Amazon ML baseline is simply the mean of the
    outcome in the training dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 量级分箱正确处理了非线性，并得到了相当不错的分数，而原始线性模型的表现并没有比基准好多少。在线性回归的情况下，Amazon ML的基准仅仅是训练数据集中结果的平均值。
- en: Let's see if we can beat these results with polynomial regression.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否能通过多项式回归击败这些结果。
- en: Polynomial regression in Amazon ML
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon ML中的多项式回归
- en: 'We will use `Boto3` and Python SDK and follow the same method of generating
    the parameters for datasources that we used in *[Chapter 7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK*, to do the **Monte Carlo** validation: we will generate
    features corresponding to power 2 of *x* to power `P` of `x` and run `N` Monte
    Carlo cross-validation. The pseudo-code is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Boto3`和Python SDK，并遵循我们在*[第7章](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)，命令行和SDK*中使用的方法来生成数据源的参数，进行**蒙特卡洛**验证：我们将生成对应于*x*的2次幂到*x*的`P`次幂的特征，并运行`N`次蒙特卡洛交叉验证。伪代码如下：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this exercise, we will go from *2 to 5* powers of *x* and do 5 trials for
    each model. The Python code to create a datasource from Redshift using `create_data_source_from_rds()`
    is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从*x*的2次幂到5次幂进行，并为每个模型进行5次试验。使用`create_data_source_from_rds()`从Redshift创建数据源的Python代码如下：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Beyond the obvious parameters (`Database Information`, `DataSchemaURI`, `DataSourceId`,
    and `DataSourceName`), you need to find the value for the Role ARN identifier.
    Go to the IAM console, click on Roles, and then on the AmazonMLRedshift_us-east-1_amlpackt
    role in order to find the Role ARN string:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了明显的参数（`数据库信息`、`数据模式URI`、`数据源ID`和`数据源名称`）之外，你还需要找到Role ARN标识符的值。转到IAM控制台，点击角色，然后点击AmazonMLRedshift_us-east-1_amlpackt角色，以找到Role
    ARN字符串：
- en: '![](img/B05028_09_10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_09_10.png)'
- en: The `DataRearrangement` string will depend on the nature of the datasource with
    a 0% to 70% split for the training datasource and a 70% to 100% for the evaluation
    datasource. The `SelectSqlQuery` is where we are going to do the feature engineering
    and create new variables as powers of *x*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataRearrangement`字符串将取决于数据源的性质，训练数据源为0%到70%的分割，评估数据源为70%到100%。`SelectSqlQuery`是我们将要进行特征工程并创建新的变量作为*x*的幂的地方。'
- en: 'For instance, the following query generates an x to the power of 2 variable:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下查询生成一个*x*的2次幂变量：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This query also generates an x to the power of 3 variable:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个查询还生成一个*x*的三次幂变量：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Besides having to generate new queries for each new set or variables, we also
    need to generate a new schema. The original schema for the non-linear dataset
    is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为每个新的集合或变量生成新的查询之外，我们还需要生成一个新的模式。原始的非线性数据集模式如下：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We modify this original schema by adding the following element to the schema
    attributes list for each new power of x variable:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过向每个新的*x*变量幂的模式属性列表中添加以下元素来修改这个原始模式：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In order to run our trials, compare different feature sets, and do cross-validation
    to select the best model, we need to write a set of Python functions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行我们的试验，比较不同的特征集，并进行交叉验证以选择最佳模型，我们需要编写一组Python函数。
- en: Driving the trials in Python
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中驱动试验
- en: So far, we have written sequential code in Python. Writing simple object-oriented
    code instead is always a time saver in the end. The code is more organized, maintainable,
    and less prone to becoming unusable after a while. Taking the time to write simple
    classes with clear initialization, instances and class methods will make your
    code much simpler and robust in the end. With that in mind, we will now write
    a `NonLinear` class for our experiment.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在Python中编写了顺序代码。最终编写简单的面向对象代码总是节省时间。代码更组织化，易于维护，并且不太可能在一段时间后变得不可用。花时间编写具有清晰初始化、实例和类方法的简单类，最终会使你的代码更加简单和健壮。考虑到这一点，我们现在将为我们的实验编写一个`NonLinear`类。
- en: 'Let''s first write down the different functions of that class that generate
    some of the fields that depend on the power of the polynomial regression:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先写下那个类中不同的函数，这些函数生成一些依赖于多项式回归幂的域：
- en: 'This function takes in a power `p` and returns the SQL query:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个函数接受一个幂`p`并返回一个SQL查询：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This function takes in the name of the data split (training versus evaluation)
    and returns the string, formatted as JSON, which will be required during the datasource
    creation:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个函数接受数据分割的名称（训练与评估），并返回一个格式为JSON的字符串，这在数据源创建过程中是必需的：
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, the following function takes in the power `p` and returns the schema
    JSON string:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，以下函数接受幂`p`并返回模式JSON字符串：
- en: '[PRE32]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next three functions use the machine learning client to create the datasources,
    the model, and the evaluation. They are very similar to the scripts we wrote in
    [C](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)[hapter](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)
    *[7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的三个函数使用机器学习客户端来创建数据源、模型和评估。它们与我们写的[C](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)[hapter](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)
    *[7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    命令行和SDK*中的脚本非常相似。
- en: 'The data source creation takes in a power `p`, and an index `k` for the cross-validation,
    and splits the nature of the datasource created. The script calls the `generate_sql`
    and `generate_data_rearrangement` methods:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源创建接受一个幂`p`和一个交叉验证的索引`k`，并分割创建的数据源的性质。脚本调用`generate_sql`和`generate_data_rearrangement`方法：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The create model method also takes in the power `p` and index `k`:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型方法也接受幂`p`和索引`k`：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, the create evaluation method is as follows:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，创建评估方法如下：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We use `create_sql(p)` and `create_schema(p)` to render the `SelectSqlQuery`
    and `Data.Schema` fields when creating datasources. The model creation function
    uses two class items not yet initialized: `sgd_parameters` and `recipe`. The datasource
    creation function returns the response from the Amazon ML `create_data_source_from_redshift`
    function. We memorize the response in `ds_training` and `ds_evaluation` and use
    these items to pass on the appropriate `DataSourceId` in the model and evaluation
    creation functions.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`create_sql(p)`和`create_schema(p)`在创建数据源时渲染`SelectSqlQuery`和`Data.Schema`字段。模型创建函数使用两个尚未初始化的类项：`sgd_parameters`和`recipe`。数据源创建函数返回Amazon
    ML的`create_data_source_from_redshift`函数的响应。我们将响应保存在`ds_training`和`ds_evaluation`中，并使用这些项在模型和评估创建函数中传递适当的`DataSourceId`。
- en: 'The global code for running all the different evaluations is this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 运行所有不同评估的全局代码如下：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These functions are defined by the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数由以下定义：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The integral code is available on GitHub at [https://github.com/alexperrier/packt-aml](https://github.com/alexperrier/packt-aml).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码在GitHub上可用，地址为[https://github.com/alexperrier/packt-aml](https://github.com/alexperrier/packt-aml)。
- en: Interpreting the results
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释结果
- en: 'The following graph shows the different RMSE obtained for the five cross-validations
    and the different polynomial degrees (1 to 5):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了五次交叉验证和不同多项式度（1到5）获得的不同RMSE：
- en: '![](img/B05028_09_15.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_09_15.png)'
- en: We see that the best fit is obtained for the polynomes of degrees *3* and *4*.
    In the end, the overall RMSE of our models based on polynomial regression models is
    not good compared to the RMSE obtained with quantile binning. Polynomial regression
    gives RMSE values at best around 0.85, while the RMSE with quantile binning was
    found to be around 0.15\. Quantile binning, as it is done by Amazon ML, beats
    polynomial regression by a large factor.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，对于三次和四次多项式，最佳拟合效果最好。最后，我们基于多项式回归模型的模型整体RMSE与使用分位数分箱获得的RMSE相比并不好。多项式回归的最佳RMSE值约为0.85，而分位数分箱的RMSE发现约为0.15。分位数分箱，如Amazon
    ML所做的那样，比多项式回归好得多。
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how to use Redshift as a datasource for Amazon ML. Although
    RDS could also have been used to create datasources, Redshift is much easier to
    use with Amazon ML as all the access configuration is taken care of by the AWS
    wizard.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何将Redshift用作Amazon ML的数据源。尽管RDS也可以用来创建数据源，但与Amazon ML相比，Redshift更容易使用，因为所有访问配置都由AWS向导处理。
- en: We have shown how to use simple SQL queries on Redshift to carry out feature
    engineering and implement a polynomial regression approach on a highly non-linear
    dataset. We have also shown how to generate the required SQL queries, schemas,
    and recipes to carry out the Monte Carlo cross-validation.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何使用Redshift上的简单SQL查询进行特征工程，并在高度非线性数据集上实现多项式回归方法。我们还展示了如何生成所需的SQL查询、模式和配方以执行蒙特卡洛交叉验证。
- en: In the next chapter, we will build on our Redshift integration and start streaming
    data using the AWS Kinesis service.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将基于我们的Redshift集成，并开始使用AWS Kinesis服务进行数据流。
