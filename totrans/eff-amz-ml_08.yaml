- en: Creating Datasources from Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use the power of SQL queries to address non-linear
    datasets. Creating datasources in Redshift or RDS gives us the potential for upstream
    SQL-based feature engineering prior to the datasource creation. We implemented
    a similar approach in [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml),
    *Loading and Preparing the Dataset*, by leveraging the new AWS Athena service
    to apply preliminary transformations on the data before creating the datasource.
  prefs: []
  type: TYPE_NORMAL
- en: This enabled us to expand the `Titanic` dataset by creating new features, such
    as the `Deck` number, replacing the `Fare` with its log or replacing missing values
    for the `Age` variable. The SQL transformations were simple, but allowed us to
    expand the original dataset in a very flexible way. The **AWS Athena** service
    is S3 based. It allows us to run SQL queries on datasets hosted on S3 and dump
    the results in S3 buckets. We were still creating Amazon ML datasources from S3,
    but simply adding an extra data preprocessing layer to massage the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS offers two other SQL services from which it is possible to create datasources:
    RDS and Redshift. Datasource creation is very similar for both RDS and Redshift,
    and we will focus on creating datasources in Redshift via the Python SDK `Boto3`.
    The key point for us is that RDS/Redshift-based datasources are created directly
    via SQL queries, thus giving us the opportunity for further feature engineering.
    Redshift also integrates seamlessly with AWS Kinesis, a service we''ll explore
    for streaming data in [Chapter 9](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=1053),
    *Building a Streaming Data Analysis Pipeline*.'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Machine Learning is built on intrinsically linear models leveraging with
    good results quantile binning data transformation as a method to handle non-linearities
    in the dataset. Polynomial regression is another important machine learning method
    used to deal with non-linear datasets. We will use our new SQL powers to implement
    polynomial regression using Amazon ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between RDS and Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a Redshift database with PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to load your S3 data into Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a datasource from Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use polynomial regression with Amazon ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing between RDS and Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AWS offers no less than six different cloud database and SQL/NoSQL services:
    RDS, Aurora, DynamoDB, Redshift, Athena, and AWS Database Migration Service! Out
    of all these services, only two are compatible with Amazon Machine Learning: RDS
    and Redshift. You can store data in either service and create datasources from
    these sources. The datasource creation methods for the two services have similar
    parameters, but differ quite significantly when it comes to the underlying AWS
    service communication.'
  prefs: []
  type: TYPE_NORMAL
- en: RDS and Redshift are very different services. Redshift is a data warehouse used
    to answer a few complex and long running queries on large datasets, while RDS
    is made for frequent, small, and fast queries. Redshift is more suited for massive
    parallel processing to perform operations on millions of rows of data with minimal
    latency, while RDS offers a server instance that runs a given database. RDS offers
    several different database types – MySQL, PostgreSQL, MariaDB, Oracle, SQL Server,
    and Amazon Aurora, while Redshift is Amazon's own analytics database offering
    based on the **ParAccel** technology and running a fork of PostgreSQL. You connect
    to Redshift using standard ODBC and JDBC connections.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift and PostgreSQL have a number of very important differences that
    you must be aware of as you build your database in Redshift. Many functions, data
    types, and PostgreSQL features are not supported in Amazon Redshift. More info
    is available at [http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html](http://docs.aws.amazon.com/redshift/latest/dg/c_redshift-and-postgres-sql.html).
  prefs: []
  type: TYPE_NORMAL
- en: A more in-depth explanation of the difference between RDS and Redshift can be
    found on this thread: [https://www.quora.com/What-is-the-difference-between-redshift-and-RDS](https://www.quora.com/What-is-the-difference-between-redshift-and-RDS)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of Amazon ML, an important difference between the two services
    is that the Amazon ML web console only allows for datasource creation from S3
    and Redshift, but not from RDS, as shown in this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: The Python SDK and the AWS CLI, however, both allow datasource creation from
    RDS and Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: '**SDK:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`create_data_source_from_rds()`:  [http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_rds)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_data_source_from_redshift()`: [http://boto3.readthedocs.io/en/latest/reference/service/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift](http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html#MachineLearning.Client.create_data_source_from_redshift)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CLI**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create-data-source-from-rds`: [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-rds.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create-data-source-from-redshift`: [http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html](http://docs.aws.amazon.com/cli/latest/reference/machinelearning/create-data-source-from-redshift.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We now compare the parameters required by the Python SDK to connect to either
    services:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Redshift parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'RDS parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between the two sets of parameters lies in the way we allow
    access to the data store. Both sets include `DatabaseInformation`, `DatabaseCredentials`,
    `SelectSqlQuery`, `DataSchema`, and `DataRearrangement`. RDS also requires manually
    setting up two roles with the right policies: `ResourceRole: DataPipelineDefaultRole`
    and `ServiceRole:DataPipelineDefaultResourceRole`.'
  prefs: []
  type: TYPE_NORMAL
- en: RDS is more adapted to our volume of data than Redshift, and we should use RDS
    instead of Redshift for our machine learning project. However, we previously found
    that manually creating the roles and policies for RDS required an in-depth knowledge
    of AWS inner permissions workings, which was too complex for this book. Although
    the parameters and concepts are very similar for creating datasources from RDS
    and Redshift, in the background, they differ quite a lot. RDS datasource creation
    involves the creation of AWS data pipelines, another AWS service that allows you
    to process and move data between different AWS computeing and storage services.
    Having to set up data pipelines adds a non-trivial layer of complexity to the
    whole project.
  prefs: []
  type: TYPE_NORMAL
- en: Redshift, on the other hand, does not require building a data pipeline and setting
    permissions, roles, and policies to create datasources. In the end, this extra
    simplicity made Redshift more adapted to this book, as we wanted to keep the focus
    on the machine learning side of things and not delve into the intricacies of AWS
    access roles and policies, although RDS would have been more suited for our low
    volume of data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Redshift**: Presenting Redshift in depth far exceeds the scope of this book.
    We recommend the *Getting Started with Amazon Redshift* book by *Stefan Bauer,
    Packt* ([https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift)](https://www.packtpub.com/big-data-and-business-intelligence/getting-started-amazon-redshift), the
    AWS documentation ([https://aws.amazon.com/redshift/getting-started/](https://aws.amazon.com/redshift/getting-started/)) and
    this blog post for a good introduction to Cluster Configuration ([https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration](https://www.periscopedata.com/amazon-redshift-guide/cluster-configuration)).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with Redshift, using the AWS Redshift console to create a PostgreSQL-based
    instance and load the `Titanic` dataset we already have available in our S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Redshift instance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log in to your AWS account, and go to the Redshift dashboard at [https://console.aws.amazon.com/redshift/](https://console.aws.amazon.com/redshift/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a database in Redshift is quite simple and well handled by the AWS
    Redshift wizard. First, click on the Launch Cluster button. In the first screen,
    we define the Cluster identifier* as `amlpackt`, the Database name as `amlpacktdb`,
    and the Master user name as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next screen, we choose the default parameters to configure the node,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Choose the default settings for the next configuration screen, but make sure
    that the cluster is publicly accessible. You do not need a public IP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Choose the Machine Learning/RDS VPC security group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The final validation screen prior to launching the cluster will inform you
    about the associated costs as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It will take a few minutes once you click on the final Launch Cluster button
    for the cluster to be ready. We will connect to the newly created database using
    Psql. Other external connection types are available through JDBC and ODBC. The
    cluster information page shows the Endpoint URL that you need to connect to the
    newly created database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: Connecting through the command line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Psql is a command line program that acts as the primary front-end to a Postgresql
    database. It provides a number of shell commands (`pg_dump` to dump the content
    of a database, `createdb` to create a database, and many others). It also has
    a number of meta commands to list elements and display information (see the **Psql
    cheatsheet** information box).
  prefs: []
  type: TYPE_NORMAL
- en: '**Psql cheatsheet:**'
  prefs: []
  type: TYPE_NORMAL
- en: '`q`: Quit/Exit'
  prefs: []
  type: TYPE_NORMAL
- en: '`d __table__`: Show table definition, including triggers'
  prefs: []
  type: TYPE_NORMAL
- en: '`dt` : List tables'
  prefs: []
  type: TYPE_NORMAL
- en: '`df`: List functions'
  prefs: []
  type: TYPE_NORMAL
- en: '`dv`: List views'
  prefs: []
  type: TYPE_NORMAL
- en: '`x`: Pretty-format query results instead of the not-so-useful ASCII tables'
  prefs: []
  type: TYPE_NORMAL
- en: '`connect __database__`: Connect to a database'
  prefs: []
  type: TYPE_NORMAL
- en: '`l`: List databases'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding a `+` on a `d` command will show more results: compare, for instance,
    d titanic with `d+ titanic`'
  prefs: []
  type: TYPE_NORMAL
- en: see [http://postgresguide.com/utilities/psql.html](http://postgresguide.com/utilities/psql.html) for
    more info on Psql
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now connect from the terminal to your database using **Psql** with
    the following command, using the endpoint URL (`amlpackt.cenllwot8v9r.us-east-1.redshift.amazonaws.com`)
    for the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `amlpacktdb` database is, of course, empty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many ways we can import data into a Redshift database. To keep things
    simple, we will upload a CSV file available on S3 to a Redshift table with the
    copy command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'When using the copy command, Redshift needs to authenticate with the S3 service.
    Authentication between AWS services can be implemented in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-based**: authentication is granted by  passing the user access keys.
    It is a simpler mean of granting access between AWS services but it is not always
    available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role-based**: authentication requires creating roles with the right policies
    and permissions. It is a preferred andmore secure mean of authentication than
    user-based authentication. However, it requires extra roles and policies creation
    steps and is less straightforward to setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More info on user versus role based authentication for AWS services is available
    at [http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html](http://docs.aws.amazon.com/redshift/latest/mgmt/redshift-iam-authentication-access-control.html).
    In our copy example, we plan to use the aws access keys of our main AWS user.
    But before we can copy data into the table, we first need to create it with an
    SQL query. For the `Titanic` CSV file we have been working on, the table creation
    query is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The table now exists in our Redshift database, as shown by the `dt` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The table structure is as expected, as shown by the  `d+` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now upload the CSV file to S3 and fill in our table by running the following
    commands from your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the CSV file should not include the CSV headers. To verify that the
    copy command worked, we can count the number of records in the Titanic table by
    running the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The result shows we now have 1309 records in the titanic table.
  prefs: []
  type: TYPE_NORMAL
- en: Executing Redshift queries using Psql
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve seen that we can connect to our database with the following `Psql` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to type in our password. To shorten the line and avoid having
    to type the password each time, we can set both the connection string and the
    password as shell environment variables. In your terminal, execute the following
    command to create a global `REDSHIFT_CONNECT` shell variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly for the password, execute the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'From now on, you can connect to the database with the simple command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that `REDSHIFT_CONNECT` is a variable name we chose, while `PGPASSWORD`
    is a predefined shell variable name that is recognized by Psql.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a choice in the way we can run queries on our Redshift database.
    We can perform either of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Psql` into the database shell and type some SQL queries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the SQL query into a file (for instance, `my_file.sql`) and, from the
    terminal, run the command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the query directly with the `Psql` command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to work on our dataset. Since we have already worked extensively
    on the Titanic dataset, we will use another dataset for the rest of the chapter.
    Let's create an artificial dataset that exhibits strong non-linear patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our own non-linear dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good way to create a non-linear dataset is to mix sines with different phases.
    The dataset we will work with in this chapter is created with the following Python
    script and exported to a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, *X* is the predictor, and *y* the outcome. You can use variations
    on that script to easily generate other non-linear datasets. Note that we have
    used a `lambda` function, which is a Pythonic way of declaring a function on the
    spot when needed. Then we shuffle the dataset by sorting randomly (`np.random.rand(n_samples)`). We
    then save the data to a CSV file (`nonlinear.csv`) using the **Pandas** dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Plotting the data gives the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: It is obviously not linear. A line has no hope of approximating, let alone predicting
    the outcome *y* from the predictor *X*. Now that we have a highly non-linear dataset
    available, we need to upload it to Redshift.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the nonlinear data to Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first need to create the table that will host the data. We will call that
    table `nonlinear`. It only has three columns: an index, the predictor `X`, and
    the outcome `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the table is created, we can upload the CSV file to S3, connect to the
    database, and import the data in the non-linear table with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that the `nonlinear` table now has a thousand rows with the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our data has been uploaded to Redshift. We are ready to create datasources and
    train and evaluate models! But before we dive into model building on this dataset,
    let's introduce the polynomial regression method, which will allow us to deal
    with this highly non-linear dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing polynomial regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In two dimensions, where we have a predictor and an outcome, linear modeling
    is all about finding the best line that approximates your data. In three dimensions
    (two predictors and one outcome), the idea is then to find the best plane, or
    the best flat surface, that approximates your data. In the `N` dimension, the
    surface becomes an hyperplane, but the goal is always the same – to find the hyperplane
    of dimension *N-1* that gives the best approximation for regression or that separates
    the classes the best for classification. That hyperplane is always flat.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the very non-linear two-dimensional dataset we created, it is
    obvious that no line can properly approximate the relation between the predictor
    and the outcome. There are many different methods to model non-linear data, including
    polynomial regression, step functions, splines, and **Generalized additive models**
    (**GAM**). See *Chapter 7* of *An Introduction to Statistical Learning* by *James,
    Witten*, *Hastie* and *Tibshirani Springer, 2013* for a great introduction to
    these methods. The book is available in PDF at [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/).
    We will apply the polynomial regression method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Polynomial regression consists in replacing the standard linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ŷ* is the predicted outcome, *x* the predictor, (*w[o], w[1]*) the linear
    model coefficients. By a polynomial function of order *k:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: The power of the polynomial regression approach is that we can use the same
    linear modeling method as with the linear model, and we can therefore still use
    Amazon ML SGD to find the coefficients {*w[k]*} of the polynomial regression equation.
    In the next section, we will train successive models by increasing the degree
    of the polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We first need to establish a baseline. Amazon ML quantile binning transformation
    is Amazon Machine Learning preferred method of dealing with non-linearities in
    a dataset. Let''s establish how a simple linear model performs with Amazon''s
    default recipe. We will create a baseline score using the usual AWS console tools.
    This time, we choose to create a data source from Redshift and not from S3\. Fill
    the information as shown in the next screenshot, and click on Test access to check
    your access, and at the same time, make Amazon ML create the necessary IAM role.
    Finally, click on Verify once you''re done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Amazon ML handles all the role and policy creation in the background by creating
    the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A new role: AmazonMLRedshift_us-east-1_amlpackt. We will use the arn related
    to this role when creating datsources using the Python SDK.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two new policies attached to this role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonMLS3LocationAccess_aml.packt`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AmazonMLRedshiftAccess_us-east-1_amlpackt`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS also sets up the `Trust Relationship` so that `roleAmazonMLRedshift_us-east-1_amlpackt` is
    able assume a `machinelearning` service role.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating these roles and policies manually requires a solid understanding of
    access permissions between services in AWS. Creating them using the console is
    a significant time-saver. The next steps are standard schema and target definition,
    and data source creation. The default schema generated by Amazon ML is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We reuse that schema later on by saving the JSON string into the `data/nonlinear.schema`
    file and uploading it to S3 with `aws s3 cp data/nonlinear.schema s3://aml.packt/data/ch9/` .
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the datasource is available, we can create and evaluate a model via the
    console. The recipe generated by Amazon ML during the model creation uses the
    quantile binning transformation on the predictor with 500 bins, which may seem
    like a large value since we only have 700 samples in the training dataset. The
    auto-generated Amazon ML recipe is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We train a model with L2 mild regularization and 100 passes, and evaluate that
    model on 30% of our dataset. We obtain the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: With quantile binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMSE: 0.1540'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline RMSE: 1.034
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Without quantile binning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RMSE: 1.0207'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline RMSE: 1.025
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantile binning correctly handles the non-linearities and results in a pretty
    good score, while a raw linear model does not fare much better than the baseline.
    In the case of linear regression, Amazon ML baseline is simply the mean of the
    outcome in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can beat these results with polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial regression in Amazon ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use `Boto3` and Python SDK and follow the same method of generating
    the parameters for datasources that we used in *[Chapter 7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK*, to do the **Monte Carlo** validation: we will generate
    features corresponding to power 2 of *x* to power `P` of `x` and run `N` Monte
    Carlo cross-validation. The pseudo-code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In this exercise, we will go from *2 to 5* powers of *x* and do 5 trials for
    each model. The Python code to create a datasource from Redshift using `create_data_source_from_rds()`
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Beyond the obvious parameters (`Database Information`, `DataSchemaURI`, `DataSourceId`,
    and `DataSourceName`), you need to find the value for the Role ARN identifier.
    Go to the IAM console, click on Roles, and then on the AmazonMLRedshift_us-east-1_amlpackt
    role in order to find the Role ARN string:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: The `DataRearrangement` string will depend on the nature of the datasource with
    a 0% to 70% split for the training datasource and a 70% to 100% for the evaluation
    datasource. The `SelectSqlQuery` is where we are going to do the feature engineering
    and create new variables as powers of *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the following query generates an x to the power of 2 variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This query also generates an x to the power of 3 variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides having to generate new queries for each new set or variables, we also
    need to generate a new schema. The original schema for the non-linear dataset
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We modify this original schema by adding the following element to the schema
    attributes list for each new power of x variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In order to run our trials, compare different feature sets, and do cross-validation
    to select the best model, we need to write a set of Python functions.
  prefs: []
  type: TYPE_NORMAL
- en: Driving the trials in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have written sequential code in Python. Writing simple object-oriented
    code instead is always a time saver in the end. The code is more organized, maintainable,
    and less prone to becoming unusable after a while. Taking the time to write simple
    classes with clear initialization, instances and class methods will make your
    code much simpler and robust in the end. With that in mind, we will now write
    a `NonLinear` class for our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first write down the different functions of that class that generate
    some of the fields that depend on the power of the polynomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function takes in a power `p` and returns the SQL query:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes in the name of the data split (training versus evaluation)
    and returns the string, formatted as JSON, which will be required during the datasource
    creation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the following function takes in the power `p` and returns the schema
    JSON string:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The next three functions use the machine learning client to create the datasources,
    the model, and the evaluation. They are very similar to the scripts we wrote in
    [C](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)[hapter](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609)
    *[7](https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609),
    Command Line and SDK*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data source creation takes in a power `p`, and an index `k` for the cross-validation,
    and splits the nature of the datasource created. The script calls the `generate_sql`
    and `generate_data_rearrangement` methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The create model method also takes in the power `p` and index `k`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the create evaluation method is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `create_sql(p)` and `create_schema(p)` to render the `SelectSqlQuery`
    and `Data.Schema` fields when creating datasources. The model creation function
    uses two class items not yet initialized: `sgd_parameters` and `recipe`. The datasource
    creation function returns the response from the Amazon ML `create_data_source_from_redshift`
    function. We memorize the response in `ds_training` and `ds_evaluation` and use
    these items to pass on the appropriate `DataSourceId` in the model and evaluation
    creation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The global code for running all the different evaluations is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'These functions are defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The integral code is available on GitHub at [https://github.com/alexperrier/packt-aml](https://github.com/alexperrier/packt-aml).
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following graph shows the different RMSE obtained for the five cross-validations
    and the different polynomial degrees (1 to 5):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B05028_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that the best fit is obtained for the polynomes of degrees *3* and *4*.
    In the end, the overall RMSE of our models based on polynomial regression models is
    not good compared to the RMSE obtained with quantile binning. Polynomial regression
    gives RMSE values at best around 0.85, while the RMSE with quantile binning was
    found to be around 0.15\. Quantile binning, as it is done by Amazon ML, beats
    polynomial regression by a large factor.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we saw how to use Redshift as a datasource for Amazon ML. Although
    RDS could also have been used to create datasources, Redshift is much easier to
    use with Amazon ML as all the access configuration is taken care of by the AWS
    wizard.
  prefs: []
  type: TYPE_NORMAL
- en: We have shown how to use simple SQL queries on Redshift to carry out feature
    engineering and implement a polynomial regression approach on a highly non-linear
    dataset. We have also shown how to generate the required SQL queries, schemas,
    and recipes to carry out the Monte Carlo cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on our Redshift integration and start streaming
    data using the AWS Kinesis service.
  prefs: []
  type: TYPE_NORMAL
