<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Practical Approach to Real-World Supervised Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Practical Approach to Real-World Supervised Learning</h1></div></div></div><p>The ability to learn from observations accompanied by marked targets or labels, usually in order <a id="id157" class="indexterm"/>to make predictions about unseen data, is known as <span class="strong"><strong>supervised machine learning</strong></span>. If the targets are categories, the problem is one <a id="id158" class="indexterm"/>of classification and if they are numeric values, it is called <span class="strong"><strong>regression</strong></span>. In effect, what is being attempted is to infer the function that maps the data to the target. Supervised machine learning is used extensively in a wide variety of machine learning applications, whenever labeled data is available or the labels can be added manually.</p><p>The core assumption of supervised machine learning is that the patterns that are learned from the data used in training will manifest themselves in yet unseen data.</p><p>In this chapter, we will discuss the steps used to explore, analyze, and pre-process the data before proceeding to training models. We will then introduce different modeling techniques ranging from simple linear models to complex ensemble models. We will present different evaluation metrics and validation criteria that allow us to compare model performance. Some of the discussions are accompanied by brief mathematical explanations that should help express the concepts more precisely and whet the appetite of the more mathematically inclined readers. In this chapter, we will focus on classification as a method of supervised learning, but the principles apply to both classification and regression, the two broad applications of supervised learning.</p><p>Beginning with this chapter, we will introduce tools to help illustrate how the concepts presented in each chapter are used to solve machine learning problems. Nothing reinforces the understanding of newly learned material better than the opportunity to apply that material to a real-world problem directly. In the process, we often gain a clearer and more relatable understanding of the subject than what is possible with passive absorption of the theory alone. If the opportunity to learn new tools is part of the learning, so much the better! To meet this goal, we will introduce a classification dataset familiar to most data science practitioners and use it to solve a classification problem while highlighting the process and methodologies that guide the solution.</p><p>In this chapter, we will <a id="id159" class="indexterm"/>use RapidMiner and Weka for building the process by which we learn from a single well-known dataset. The workflows and code are available on the website for readers to download, execute, and modify.</p><p>RapidMiner is a GUI-based Java <a id="id160" class="indexterm"/>framework that makes it very easy to conduct a data science project, end-to-end, from within the tool. It has a simple drag-and-drop interface to build process workflows to ingest and clean data, explore and transform features, perform training using a wide selection of machine learning algorithms, do validation and model evaluation, apply your best models to test data, and more. It is an excellent tool to learn how to make the various parts of the process work together and produce rapid results. Weka is another GUI-based framework and it has a Java API that we will use to illustrate more of the coding required for performing analysis.</p><p>The major topics that we will cover in this chapter are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data quality analysis</li><li class="listitem" style="list-style-type: disc">Descriptive data analysis</li><li class="listitem" style="list-style-type: disc">Visualization analysis</li><li class="listitem" style="list-style-type: disc">Data transformation and preprocessing</li><li class="listitem" style="list-style-type: disc">Data sampling</li><li class="listitem" style="list-style-type: disc">Feature relevance analysis and dimensionality reduction</li><li class="listitem" style="list-style-type: disc">Model building</li><li class="listitem" style="list-style-type: disc">Model assessment, evaluation, and comparison</li><li class="listitem" style="list-style-type: disc">Detailed case study—Horse Colic Classification</li></ul></div><div class="section" title="Formal description and notation"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Formal description and notation</h1></div></div></div><p>We would <a id="id161" class="indexterm"/>like to introduce some notation and formal definitions for the terms used in supervised learning. We will follow this notation through the rest of the book when not specified and extend it as appropriate when new concepts are encountered. The notation will provide a precise and consistent language to describe the terms of art and enable a more rapid and efficient comprehension of the subject.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Instance</strong></span>: Every <a id="id162" class="indexterm"/>observation is a data instance. Normally the variable <span class="emphasis"><em>X</em></span> is used to represent the input space. Each data instance has many variables (also called features) and is referred to as <span class="strong"><strong>x</strong></span> (vector representation with bold) of dimension <span class="emphasis"><em>d</em></span> where <span class="emphasis"><em>d</em></span> denotes the number of variables or features or attributes in each instance. The features are represented as <span class="strong"><strong>x</strong></span> = <span class="emphasis"><em>(x</em></span><sub>1</sub><span class="emphasis"><em>,x</em></span><sub>2</sub><span class="emphasis"><em>,…x</em></span><sub>d</sub><span class="emphasis"><em>)</em></span><sup>T</sup>, where each value is a scalar when it is numeric corresponding to the feature value.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Label</strong></span>: The label (also called target) is the dependent variable of interest, generally <a id="id163" class="indexterm"/>denoted by <span class="emphasis"><em>y</em></span>. In <span class="strong"><strong>classification</strong></span>, values of <a id="id164" class="indexterm"/>the label are well-defined categories in the problem domain; they need not <a id="id165" class="indexterm"/>be numeric or things that can be ordered. In <span class="strong"><strong>regression</strong></span>, the label is real-valued.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Binary classification</strong></span>, where the target takes only two values, it is mathematically <a id="id166" class="indexterm"/>represented as:<p>y ∈ {1,–1}</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Regression</strong></span>, where the <a id="id167" class="indexterm"/>target can take any value in the real number domain, is represented as:<div class="mediaobject"><img src="graphics/B05137_02_008.jpg" alt="Formal description and notation"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dataset</strong></span>: Generally, the <a id="id168" class="indexterm"/>dataset is denoted by <span class="emphasis"><em>D</em></span> and consists of individual data instances and their labels. The instances are normally represented as set {<span class="strong"><strong>x</strong></span><sub>1</sub>,<span class="strong"><strong>x</strong></span><sub>2</sub>…<span class="strong"><strong>x</strong></span><sub>n</sub>}. The labels for each instance are represented as the set <span class="strong"><strong>y</strong></span> = {<span class="emphasis"><em>y</em></span><sub>1</sub><span class="emphasis"><em>,y</em></span><sub>2</sub><span class="emphasis"><em>,…y</em></span><sub>n</sub>}. The entire labeled dataset is represented as paired elements in a set as given by <span class="emphasis"><em>D</em></span> = {(<span class="strong"><strong>x</strong></span><sub>1</sub>, <span class="emphasis"><em>y</em></span><sub>1</sub>),(<span class="strong"><strong>x</strong></span><sub>2</sub>, <span class="emphasis"><em>y</em></span><sub>2</sub>)…(<span class="strong"><strong>x</strong></span><sub>n</sub>, <span class="emphasis"><em>y</em></span><sub>n</sub>)} where <span class="inlinemediaobject"><img src="graphics/B05137_02_013.jpg" alt="Formal description and notation"/></span> for real-valued features.
 </li></ul></div><div class="section" title="Data quality analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec08"/>Data quality analysis</h2></div></div></div><p>There <a id="id169" class="indexterm"/>are limitations to what can be learned from data that suffers from poor quality. Problems with quality can include, among other factors, noisy data, missing values, and errors in labeling. Therefore, the first step is to understand the data before us in order that we may determine how to address any data quality issues. Are the outliers merely noise or indicative of interesting anomalies in the population? Should missing data be handled the same way for all features? How should sparse features be treated? These and similar questions present themselves at the very outset.</p><p>If we're fortunate, we receive a cleansed, accurately labeled dataset accompanied by documentation <a id="id170" class="indexterm"/>describing the data elements, the data's pedigree, and what if any transformations were already done to the data. Such a dataset would be ready to be split into train, validation, and test samples, using methods described in the section on Data Sampling. However, if data is not cleansed and suitable to be partitioned for our purposes, we must first prepare the data in a principled way before sampling can begin. (The significance of partitioning the data is explained later in this chapter in a section dedicated to train, validation, and test sets).</p><p>In the following sections, we will discuss the data quality analysis and transformation steps that are needed before we can analyze the features.</p></div><div class="section" title="Descriptive data analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec09"/>Descriptive data analysis</h2></div></div></div><p>The <a id="id171" class="indexterm"/>complete data sample (including train, validation, and test) should be analyzed and summarized for the following characteristics. In cases where the data is not already split into train, validate, and test, the task of data transformation needs to make sure that the samples have similar characteristics and statistics. This is of paramount importance to ensure that the trained model can generalize over unseen data, as we will learn in the section on data sampling.</p><div class="section" title="Basic label analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec01"/>Basic label analysis</h3></div></div></div><p>The <a id="id172" class="indexterm"/>first step of analysis is understanding the distribution of labels in different sets as well as in the data as a whole. This helps to determine whether, for example, there is imbalance in the distribution of the target variable, and if so, whether it is consistent across all the samples. Thus, the very first step is usually to find out how many examples in the training and test sets belong to each class.</p></div><div class="section" title="Basic feature analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec02"/>Basic feature analysis</h3></div></div></div><p>The <a id="id173" class="indexterm"/>next step is to calculate the statistics for each feature, such as</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Number of unique values</li><li class="listitem" style="list-style-type: disc">Number of missing values: May include counts grouped by different missing value surrogates (NA, null, ?, and so on).</li><li class="listitem" style="list-style-type: disc">For <a id="id174" class="indexterm"/>categorical: This counts across feature categories, counts across feature categories by label category, most frequently occurring category (mode), mode by label category, and so on.</li><li class="listitem" style="list-style-type: disc">For numeric:  Minimum, maximum, median, standard deviation, variance, and so on.</li></ul></div><p>Feature analysis gives basic insights that can be a useful indicator of missing values and noise that can affect the learning process or choice of the algorithms.</p></div></div><div class="section" title="Visualization analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>Visualization analysis</h2></div></div></div><p>Visualization <a id="id175" class="indexterm"/>of the data is a broad topic and it is a continuously evolving area in the field of machine learning and data mining. We will only cover some of the important aspects of visualization that help us analyze the data in practice.</p><div class="section" title="Univariate feature analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec03"/>Univariate feature analysis</h3></div></div></div><p>The <a id="id176" class="indexterm"/>goal here is to visualize one feature <a id="id177" class="indexterm"/>at a time, in relation to the label. The techniques used are as follows:</p><div class="section" title="Categorical features"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec01"/>Categorical features</h4></div></div></div><p>Stacked <a id="id178" class="indexterm"/>bar graphs are a simple way of showing the distribution of each feature category among the labels, when the problem is one of classification.</p></div><div class="section" title="Continuous features"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec02"/>Continuous features</h4></div></div></div><p>Histograms <a id="id179" class="indexterm"/>and box plots are two basic visualization techniques for continuous features.</p><p>Histograms have predefined bins whose widths are either fixed intervals or based on some calculation used to split the full range of values of the feature. The number of instances of data that falls within each bin is then counted and the height of the bin is adjusted based on this count. There are variations of histograms such as relative or frequency-based histograms, Pareto histograms, two-dimensional histograms, and so on; each is a slight variation of the concept and permits a different insight into the feature. For those interested in finding out more about these variants, the Wikipedia article on histograms is a great resource.</p><p>Box plots <a id="id180" class="indexterm"/>are a key visualization technique for numeric features as they show distributions in terms of percentiles and outliers.</p></div></div><div class="section" title="Multivariate feature analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec04"/>Multivariate feature analysis</h3></div></div></div><p>The idea <a id="id181" class="indexterm"/>of multivariate feature <a id="id182" class="indexterm"/>analysis is to visualize more than one feature to get insights into relationships between them. Some of the well-known plots are explained here.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scatter plots</strong></span>: An important <a id="id183" class="indexterm"/>technique for understanding the relationship between different features and between <a id="id184" class="indexterm"/>features and labels. Typically, two-dimensional scatter plots are used in practice where numeric features form the dimensions. Alignment of data points on some imaginary axis shows correlation while scattering of the data points shows no correlation. It can also be useful to identify clusters in lower dimensional space. A bubble chart is a variation of a scatter plot where two features form the dimensional axes and the third is proportional to the size of the data point, with the plot giving the appearance of a field of "bubbles". Density charts help visualize even more features together by introducing data point color, background color, and so on, to give additional insights.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ScatterPlot Matrix</strong></span>: ScatterPlot Matrix is an extension of scatter plots where pair-wise <a id="id185" class="indexterm"/>scatter plots <a id="id186" class="indexterm"/>for each feature (and label) is visualized. It gives a way to compare and perform multivariate analysis of high dimensional data in an effective way.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Parallel Plots</strong></span>: In this visualization, each feature is linearly arranged on the x-axis <a id="id187" class="indexterm"/>and the ranges of values for <a id="id188" class="indexterm"/>each feature form the <span class="emphasis"><em>y</em></span> axis. So each data element is represented as a line with values for each feature on the parallel axis. Class labels, if available, are used to color the lines. Parallel plots offer a great understanding of features that are effective in separating the data. Deviation charts are variations of parallel plots, where instead of showing actual data points, mean and standard deviations are plotted. Andrews plots are another variation <a id="id189" class="indexterm"/>of parallel plots where <a id="id190" class="indexterm"/>data is transformed using Fourier series and the function values corresponding to each is projected.</li></ul></div></div></div></div></div>
<div class="section" title="Data transformation and preprocessing"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Data transformation and preprocessing</h1></div></div></div><p>In <a id="id191" class="indexterm"/>this section, we will cover the broad topic of data transformation. The main idea of data transformation is to take the input data and transform <a id="id192" class="indexterm"/>it in careful ways so as to clean it, extract the most relevant information from it, and to turn it into a usable form for further analysis and learning. During these transformations, we must only use methods that are designed while keeping in mind not to add any bias or artifacts that would affect the integrity of the data.</p><div class="section" title="Feature construction"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Feature construction</h2></div></div></div><p>In <a id="id193" class="indexterm"/>the case of some datasets, we need to create more <a id="id194" class="indexterm"/>features from features we are already given. Typically, some form of aggregation is done using common aggregators such as average, sum, minimum, or maximum to create additional features. In financial fraud detection, for example, Card Fraud datasets usually contain transactional behaviors of accounts over various time periods during which the accounts were active. Performing behavioral synthesis such as by capturing the "Sum of Amounts whenever a Debit transaction occurred, for each Account, over One Day" is an example of feature construction that adds a new dimension to the dataset, built from existing features. In general, designing new features that enhance the predictive power of the data requires domain knowledge and experience with data, making it as much an art as a science.</p></div><div class="section" title="Handling missing values"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Handling missing values</h2></div></div></div><p>In <a id="id195" class="indexterm"/>real-world datasets, often, many features <a id="id196" class="indexterm"/>have missing values. In some cases, they are missing due to errors in measurement, lapses in recording, or because they are not available due to various circumstances; for example, individuals may choose not to disclose age or occupation. Why care about missing values at all? One extreme and not uncommon way to deal with it is to ignore the records that have any missing features, in other words, retain only examples that are "whole". This approach may severely reduce the size of the dataset when missing features are widespread in the data. As we shall see later, if the system we are dealing with is complex, dataset size can afford us precious advantage. Besides, there is often <a id="id197" class="indexterm"/>predictive value that can be exploited even in the "un-whole" records, despite the missing values, as long as we use appropriate measures to deal with the problem. On the other hand, one <a id="id198" class="indexterm"/>may unwittingly be throwing out key information when the omission of the data itself is significant, as in the case of deliberate misrepresentation or obfuscation on a loan application by withholding information that could be used to conclusively establish bone fides.</p><p>Suffice it to say, that an important step in the learning process is to adopt some systematic way to handle missing values and understand the consequences of the decision in each case. There are some algorithms such as Naïve Bayes that are less sensitive to missing values, but in general, it is good practice to handle these missing values as a pre-processing step before any form of analysis is done on the data. Here are some of the ways to handle missing values.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Replacing by means and modes</strong></span>: When we replace the missing value of a continuous value feature with the mean value of that feature, the new mean clearly remains the same. But if the mean is heavily influenced by outliers, a better approach may be to use the mean after dropping the outliers from the calculation, or use the median or mode, instead. Likewise, when a feature is sparsely represented in the dataset, the mean value may not be meaningful. In the case of features with categorical values, replacing the missing value with the one that occurs with the highest frequency in the sample makes for a reasonable choice.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Replacing by imputation</strong></span>: When we impute a missing value, we are in effect constructing a classification or regression model of the feature and making a prediction based on the other features in the record in order to classify or estimate the missing value.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Nearest Neighbor imputation</strong></span>: For missing values of a categorical feature, we consider the feature in question to be the target and train a KNN model with k taken to be the known number of distinct categories. This model is then used to predict the missing values. (A KNN model is non-parametric and assigns a value to the "incoming" data instance based on a function of its neighbors—the algorithm is described later in this chapter when we talk about non-linear models).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Regression-based imputation</strong></span>: In the case of continuous value variables, we use linear models like Linear Regression to estimate the missing data—the principle <a id="id199" class="indexterm"/>is the same as for categorical values.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>User-defined imputation</strong></span>: In many cases, the most suitable value for imputing missing values must come from the problem domain. For instance, a pH value <a id="id200" class="indexterm"/>of 7.0 is neutral, higher is basic, and lower is acidic. It may make most sense to impute a neutral value for pH than either mean or median, and this insight is an instance of a user-defined imputation. Likewise, in the case of substitution with normal body temperature or resting heart rate—all are examples from medicine.</li></ul></div></div><div class="section" title="Outliers"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Outliers</h2></div></div></div><p>Handling <a id="id201" class="indexterm"/>outliers requires a lot of care and analysis. Outliers <a id="id202" class="indexterm"/>can be noise or errors in the data, or they can be anomalous behavior of particular interest. The latter case is treated in depth in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>. Here we assume the former case, that the domain expert is satisfied that the values are indeed outliers in the first sense, that is, noise or erroneously acquired or recorded data that needs to be handled appropriately.</p><p>Following are <a id="id203" class="indexterm"/>different techniques in detecting outliers in the data</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Interquartile Ranges (IQR)</strong></span>: Interquartile ranges are a measure of variability in <a id="id204" class="indexterm"/>the data or, equivalently, the statistical dispersion. Each <a id="id205" class="indexterm"/>numeric feature is sorted based on its value in the dataset and the ordered set is then divided into quartiles. The median value is generally used to measure central tendency. IQR is measured as the difference between upper and lower quartiles, Q3-Q1. The outliers are generally considered to be data values above Q3 + 1.5 * IQR and below Q1 - 1.5 * IQR.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distance-based Methods</strong></span>: The most basic form of distance-based methods uses <span class="strong"><strong>k-Nearest Neighbors</strong></span> (<span class="strong"><strong>k-NN</strong></span>) and distance metrics to score the data points. The <a id="id206" class="indexterm"/>usual parameter is the value <span class="emphasis"><em>k</em></span> in k-NN and a <a id="id207" class="indexterm"/>distance metric such as Euclidean <a id="id208" class="indexterm"/>distance. The data points at the farthest distance are considered outliers. There are many variants of these that use local neighborhoods, probabilities, or other factors, which will all be covered in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>. Mixed datasets, which <a id="id209" class="indexterm"/>have both categorical and <a id="id210" class="indexterm"/>numeric features, can skew distance-based metrics.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Density-based methods</strong></span>: Density-based methods calculate the proportion of data points <a id="id211" class="indexterm"/>within a given distance <span class="emphasis"><em>D</em></span>, and if the <a id="id212" class="indexterm"/>proportion is less than the specified threshold p, it is considered an outlier. The parameter p and D are considered user-defined values; the challenge of selecting these values appropriately presents one of the main hurdles in using these methods in the preprocessing stage.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mathematical transformation of feature</strong></span>: With non-normal data, comparing <a id="id213" class="indexterm"/>the mean value is <a id="id214" class="indexterm"/>highly misleading, as in the case when outliers are present. Non-parametric statistics allow us to make meaningful observations about highly skewed data. Transformation of such values using the logarithm <a id="id215" class="indexterm"/>or square root function tends to normalize <a id="id216" class="indexterm"/>the data in many cases, or make them more amenable to statistical tests. These transformations alter the shape of the distribution of the feature drastically—the more extreme an outlier, the greater the effect of the log transformation, for example.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Handling outliers using robust statistical algorithms in machine learning models</strong></span>: Many classification algorithms which we discuss in the next section on <a id="id217" class="indexterm"/>modeling, implicitly or explicitly handle outliers. Bagging and Boosting variants, which work as meta-learning frameworks, are generally resilient to outliers or noisy data points and may not need a preprocessing step to handle them.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Normalization</strong></span>: Many algorithms—distance-based methods are a case in point—are very sensitive to <a id="id218" class="indexterm"/>the scale of the features. Preprocessing the numeric features makes sure that all of them are in a well-behaved range. The most well-known techniques of normalization of features are given here:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Min-Max Normalization</strong></span>: In this technique, given the range <span class="emphasis"><em>[L,U]</em></span>, which is typically <span class="emphasis"><em>[0,1]</em></span>, each <a id="id219" class="indexterm"/>feature with value <span class="emphasis"><em>x</em></span> is normalized <a id="id220" class="indexterm"/>in terms of the minimum and maximum values, <span class="emphasis"><em>x</em></span><sub>max</sub> and <span class="emphasis"><em>x</em></span><sub>min</sub>, respectively, using the formula:<div class="mediaobject"><img src="graphics/B05137_02_016.jpg" alt="Outliers"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Z-Score Normalization</strong></span>: In this <a id="id221" class="indexterm"/>technique, also known as standardization, the feature <a id="id222" class="indexterm"/>values get auto-transformed so that the mean is 0 and standard deviation is 1. The technique to transform <a id="id223" class="indexterm"/>is as follows: for each feature <span class="emphasis"><em>f</em></span>, the mean value µ(<span class="emphasis"><em>f</em></span>) and standard deviation σ(<span class="emphasis"><em>f</em></span>) are computed and then the feature with value <span class="emphasis"><em>x</em></span> is transformed as:</li></ul></div><div class="mediaobject"><img src="graphics/B05137_02_019.jpg" alt="Outliers"/></div></li></ul></div></div><div class="section" title="Discretization"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>Discretization</h2></div></div></div><p>Many <a id="id224" class="indexterm"/>algorithms can only handle categorical values or <a id="id225" class="indexterm"/>nominal values to be effective, for example Bayesian Networks. In such cases, it becomes imperative to discretize the numeric features into categories using either supervised or unsupervised methods. Some of the techniques discussed are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Discretization by binning</strong></span>: This technique <a id="id226" class="indexterm"/>is also referred to as equal width discretization. The entire scale of data for each feature <span class="emphasis"><em>f</em></span>, ranging from values <span class="emphasis"><em>x</em></span><sub>max</sub> and <span class="emphasis"><em>x</em></span><sub>min</sub> is divided into a predefined number, <span class="emphasis"><em>k</em></span>, of equal intervals, each having the width <span class="inlinemediaobject"><img src="graphics/B05137_02_020.jpg" alt="Discretization"/></span>.
The "cut points" or discretization intervals are:
<div class="mediaobject"><img src="graphics/B05137_02_021.jpg" alt="Discretization"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Discretization by frequency</strong></span>: This technique is also referred to as equal frequency <a id="id227" class="indexterm"/>discretization. The feature is sorted and then the entire data is discretized into predefined <span class="emphasis"><em>k</em></span> intervals, such that each interval contains the same proportion. Both the techniques, discretization by binning and discretization by frequency, suffer from loss of information due to the predefined value of <span class="emphasis"><em>k</em></span>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Discretization by entropy</strong></span>: Given the labels, the entropy is calculated over the split <a id="id228" class="indexterm"/>points where the value changes in an iterative way, so that the bins of intervals are as pure or discriminating as possible. Refer to the <span class="emphasis"><em>Feature evaluation techniques</em></span> section for entropy-based (information gain) theory and calculations.</li></ul></div></div><div class="section" title="Data sampling"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Data sampling</h2></div></div></div><p>The dataset <a id="id229" class="indexterm"/>one receives may often require judicious <a id="id230" class="indexterm"/>sampling in order to effectively learn from the data. The characteristics of the data as well as the goals of the modeling exercise determine whether sampling is needed, and if so, how to go about it. Before we begin to learn from this data it is crucially important to create train, validate, and test data samples, as explained in this section.</p><div class="section" title="Is sampling needed?"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec05"/>Is sampling needed?</h3></div></div></div><p>When the <a id="id231" class="indexterm"/>dataset is large or noisy, or skewed towards one type, the question as to whether to sample or not to sample becomes important. The answer depends on various aspects such as the dataset itself, the objective and the evaluation criteria used for selecting the models, and potentially other practical considerations. In some situations, algorithms have scalability issues in memory and space, but work effectively on samples, as measured by model performance with respect to the regression or classification goals they are expected to achieve. For example, SVM scales as <span class="emphasis"><em>O(n</em></span><sup>2</sup><span class="emphasis"><em>)</em></span> and <span class="emphasis"><em>O(n</em></span><sup>3</sup><span class="emphasis"><em>)</em></span> in memory and training times, respectively. In other situations, the data is so imbalanced that many algorithms are not robust enough to handle the skew. In the literature, the step intended to re-balance the distribution of classes in the original data extract by <a id="id232" class="indexterm"/>creating new training samples is also called <span class="strong"><strong>resampling</strong></span>.</p></div><div class="section" title="Undersampling and oversampling"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec06"/>Undersampling and oversampling</h3></div></div></div><p>Datasets <a id="id233" class="indexterm"/>exhibiting a marked imbalance in the distribution <a id="id234" class="indexterm"/>of classes can be said to contain a distinct <a id="id235" class="indexterm"/>minority class. Often, this minority class is the set of instances that <a id="id236" class="indexterm"/>we are especially interested in precisely because its members occur in such rare cases. For example, in credit card fraud, less than 0.1% of the data belongs to fraud. This skewness is not conducive to learning; after all, when we seek to minimize the total error in classification, we give equal weight to all classes regardless of whether one class is underrepresented compared to another. In binary classification problems, we call the minority class the positive class and the majority class as the negative class, a convention that we will follow in the following discussion.</p><p>Undersampling of the majority class is a technique that is commonly used to address skewness in <a id="id237" class="indexterm"/>data. Taking credit-card fraud as an example, we <a id="id238" class="indexterm"/>can create different training samples from the original <a id="id239" class="indexterm"/>dataset such that each sample has all the fraud cases from the <a id="id240" class="indexterm"/>original dataset, whereas the non-fraud instances are distributed across all the training samples in some fixed ratios. Thus, in a given training set created by this method, the majority class is now underrepresented compared to the original skewed dataset, effectively balancing out the distribution of classes. Training samples with labeled positive and labeled negative instances in ratios of, say, 1:20 to 1:50 can be created in this way, but care must be taken that the sample of negative instances used should have similar characteristics to the data statistics and distributions of the main datasets. The reason for using multiple training samples, and in different proportions of positive and negative instances, is so that any sampling bias that may be present becomes evident.</p><p>Alternatively, we may choose to oversample the minority class. As before, we create multiple samples wherein instances from the minority class have been selected by either sampling with replacement or without replacement from the original dataset. When sampling without replacement, there are no replicated instances across samples. With replacement, some instances may be found in more than one sample. After this initial seeding of the samples, we can produce more balanced distributions of classes by random sampling with replacement from within the minority class in each sample until we have the desired ratios of positive to negative instances. Oversampling can be prone to over-fitting as classification decision boundaries tend to become more specific due to replicated values. <span class="strong"><strong>SMOTE</strong></span> (<span class="strong"><strong>Synthetic Minority Oversampling Technique</strong></span>) is a technique <a id="id241" class="indexterm"/>that alleviates this problem by creating synthetic data points in the interstices of the feature space by interpolating between neighboring instances of the positive class (<span class="emphasis"><em>References</em></span> [20]).</p><div class="section" title="Stratified sampling"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec03"/>Stratified sampling</h4></div></div></div><p>Creating <a id="id242" class="indexterm"/>samples so that data with similar characteristics <a id="id243" class="indexterm"/>is drawn in the same proportion as they appear in the population is known as stratified sampling. In multi-class classification, if there are <span class="emphasis"><em>N</em></span> classes each in a certain proportion, then samples are created such that they represent each class in the same proportion as in the original dataset. Generally, it is good practice to create <a id="id244" class="indexterm"/>multiple samples to train and test the models <a id="id245" class="indexterm"/>to validate against biases of sampling.</p></div></div></div><div class="section" title="Training, validation, and test set"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>Training, validation, and test set</h2></div></div></div><p>The <a id="id246" class="indexterm"/>Holy Grail of creating good classification models <a id="id247" class="indexterm"/>is to train on a set of good quality, representative, (training data), tune the parameters and find effective models (validation data), and finally, estimate the model's performance by its behavior on unseen data (test data).</p><p>The <a id="id248" class="indexterm"/>central idea behind the logical grouping is to make sure models are validated or tested on data that has not been seen during training. Otherwise, a simple "rote learner" can outperform the algorithm. The generalization capability of the learning algorithm must be evaluated on a dataset which is different from the training dataset, but comes from the same population (<span class="emphasis"><em>References</em></span> [11]). The balance between removing too much data from training to increase the budget of validation and testing can result in models which suffer from "underfitting", that is, not having enough examples to build patterns that can help in generalization. On the other hand, the extreme choice of allocating all the labeled data for training and not performing any validation or testing can lead to "overfitting", that is, models that fit the examples too faithfully and do not generalize well enough.</p><p>Typically, in most machine learning challenges and real world customer problems, one is given a training set and testing set upfront for evaluating the performance of the models. In these engagements, the only question is how to validate and find the most effective parameters given the training set. In some engagements, only the labeled dataset is given and you need to consider the training, validation, and testing sets to make sure your models do not overfit or underfit the data.</p><p>Three logical processes are needed for modeling and hence three logical datasets are needed, namely, training, validation, and testing. The purpose of the training dataset is to give labeled <a id="id249" class="indexterm"/>data to the learning algorithm to build the models. The purpose of the validation set is to see the effect of the parameters of the training model being evaluated by training on the validation set. Finally, the best parameters or models are retrained on the combination of the training and validation sets to find an optimum model that is then tested on the blind test set.</p><div class="mediaobject"><img src="graphics/B05137_02_022.jpg" alt="Training, validation, and test set"/><div class="caption"><p>Figure 1: Training, Validation, and Test data and how to use them</p></div></div><p>Two <a id="id250" class="indexterm"/>things affect the learning or the generalization capability: the choice of the algorithm (and its parameters) and number of training data.  This <a id="id251" class="indexterm"/>ability to generalize can be estimated by various <a id="id252" class="indexterm"/>metrics including the prediction errors. The overall estimate of unseen error or risk of the model is given by:</p><div class="mediaobject"><img src="graphics/B05137_02_023.jpg" alt="Training, validation, and test set"/></div><p>Here, <span class="emphasis"><em>Noise</em></span> is the stochastic noise, <span class="emphasis"><em>Var (G,n)</em></span> is called the variance error  and is a measure of how susceptible our hypothesis or the algorithm <span class="emphasis"><em>(G)</em></span> is, if given different datasets. <span class="inlinemediaobject"><img src="graphics/B05137_02_023a.jpg" alt="Training, validation, and test set"/></span> is called the bias error and represents how far away the best algorithm in the model (average learner over all possible datasets) is from the optimal one.</p><p>Learning curves as shown in <span class="emphasis"><em>Figure 2</em></span> and <span class="emphasis"><em>Figure 3</em></span>—where training and testing errors are plotted keeping either the algorithm with its parameters constant or the training data size constant—give an indication of underfitting or overfitting.</p><p>When the training data size is fixed, different algorithms or the same algorithms with different <a id="id253" class="indexterm"/>parameter choices can exhibit different learning <a id="id254" class="indexterm"/>curves. The <span class="emphasis"><em>Figure 2</em></span> shows two cases of algorithms on the same data size giving two different learning curves based on bias and variance.</p><div class="mediaobject"><img src="graphics/B05137_02_028.jpg" alt="Training, validation, and test set"/><div class="caption"><p>Figure 2: The training data relationship with error rate when the model complexity is fixed indicates different choices of models.</p></div></div><p>The <a id="id255" class="indexterm"/>algorithm or model choice also impacts model performance. A complex algorithm, with more parameters to tune, can result in overfitting, while a simple algorithm with less parameters might be underfitting. The classic figure to illustrate the model performance and complexity when the training data size is fixed is as follows:</p><div class="mediaobject"><img src="graphics/B05137_02_030.jpg" alt="Training, validation, and test set"/><div class="caption"><p>Figure 3: The Model Complexity relationship with Error rate, over the training and the testing data when training data size is fixed.</p></div></div><p>Validation <a id="id256" class="indexterm"/>allows for exploring the parameter space to find the<a id="id257" class="indexterm"/> model that generalizes best. Regularization (will be discussed in linear models) and validation are two mechanisms that should <a id="id258" class="indexterm"/>be used for preventing overfitting. Sometimes the "k-fold cross-validation" process is used for validation, which involves creating <span class="emphasis"><em>k</em></span> samples of the data and using <span class="emphasis"><em>(k – 1)</em></span> to train on and the remaining one to test, repeated <span class="emphasis"><em>k</em></span> times to give an average estimate. The following figure shows 5-fold cross-validation as an example:</p><div class="mediaobject"><img src="graphics/B05137_02_034.jpg" alt="Training, validation, and test set"/><div class="caption"><p>Figure 4: 5-fold cross-validation.</p></div></div><p>The <a id="id259" class="indexterm"/>following <a id="id260" class="indexterm"/>are some commonly <a id="id261" class="indexterm"/>used techniques to perform data sampling, validation, and learning:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Random split of training, validation, and testing</strong></span>: 60, 20, 20. Train on 60%, use 20% for validation, and then combine the train and validation datasets <a id="id262" class="indexterm"/>to train a final model that is used to<a id="id263" class="indexterm"/> test on the remaining 20%. Split may be <a id="id264" class="indexterm"/>done randomly, based on time, based on region, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Training, cross-validation, and testing</strong></span>: Split into Train and Test two to one, do validation using cross-validation on the train set, train on whole two-thirds and test on one-third. Split may be done randomly, based on time, based on region, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Training and cross-validation</strong></span>: When <a id="id265" class="indexterm"/>the training <a id="id266" class="indexterm"/>set is <a id="id267" class="indexterm"/>small and only model selection can be done without much parameter tuning. Run cross-validation on the whole dataset and chose the best models with learning on the entire dataset.</li></ul></div></div></div>
<div class="section" title="Feature relevance analysis and dimensionality reduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Feature relevance analysis and dimensionality reduction</h1></div></div></div><p>The <a id="id268" class="indexterm"/>goal of feature relevance and selection is to find the <a id="id269" class="indexterm"/>features that are discriminating with respect to the target variable and help reduce the dimensions of the data [1,2,3]. This improves the model performance mainly by ameliorating the effects of the curse of dimensionality and by removing noise due to irrelevant features. By carefully evaluating models on the validation set with and without features removed, we can see the impact of feature relevance. Since the exhaustive search for <span class="emphasis"><em>k</em></span> features involves 2<sup>k</sup> – 1 sets (consider all combinations of <span class="emphasis"><em><sup>k</sup></em></span> features where each feature is either retained or removed, disregarding the degenerate case where none is present) the corresponding number of models that have to be evaluated can become prohibitive, so some form of heuristic search techniques are needed. The most common of these techniques are described next.</p><div class="section" title="Feature search techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Feature search techniques</h2></div></div></div><p>Some <a id="id270" class="indexterm"/>of the very common search <a id="id271" class="indexterm"/>techniques employed to find feature sets are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Forward or hill climbing</strong></span>: In this search, one feature is added at a time until the evaluation module outputs no further change in performance.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Backward search</strong></span>: Starting from the whole set, one feature at a time is removed until no performance improvement occurs. Some applications interleave both forward and backward techniques to search for features.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Evolutionary search</strong></span>: Various evolutionary techniques such as genetic algorithms can be used as a search mechanism and the evaluation metrics from either filter- or wrapper-based methods can be used as fitness criterion to guide the process.</li></ul></div></div><div class="section" title="Feature evaluation techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Feature evaluation techniques</h2></div></div></div><p>At a <a id="id272" class="indexterm"/>high level, there are three <a id="id273" class="indexterm"/>basic methods to evaluate features.</p><div class="section" title="Filter approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec07"/>Filter approach</h3></div></div></div><p>This <a id="id274" class="indexterm"/>approach refers to the use of techniques <a id="id275" class="indexterm"/>without using machine learning algorithms for evaluation. The basic idea of the filter approach is to use a search technique to select a feature (or subset of features) and measure its importance using some statistical measure until a stopping criterion is reached.</p><div class="section" title="Univariate feature selection"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec04"/>Univariate feature selection</h4></div></div></div><p>This <a id="id276" class="indexterm"/>search is as simple as ranking each feature based on the statistical measure employed.</p><div class="section" title="Information theoretic approach"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec01"/>Information theoretic approach</h5></div></div></div><p>All information theoretic approaches <a id="id277" class="indexterm"/>use the concept of entropy mechanism at their core. The idea is that if the feature is randomly present in the dataset, there is maximum entropy, or, equivalently, the ability to compress or encode is low, and the feature may be irrelevant. On the other hand, if the distribution of the feature value is such some range of values are more prevalent in one class relative to the others, then the entropy is minimized and the feature is discriminating. Casting the problem in terms of entropy in this way requires some form of discretization to convert the numeric features into categories in order to compute the probabilities.</p><p>Consider a binary classification problem with training data <span class="emphasis"><em>D</em></span><sub>X</sub>. If <span class="emphasis"><em>X</em></span><sub>i</sub> is the <span class="emphasis"><em>i</em></span><sup>th</sup> feature with <span class="emphasis"><em>v</em></span> distinct categorical values such that <span class="emphasis"><em>D</em></span><sub>Xi</sub><span class="emphasis"><em> = {D</em></span><sub>1</sub><span class="emphasis"><em>, D</em></span><sub>2</sub><span class="emphasis"><em>… D</em></span><sub>v</sub><span class="emphasis"><em>}</em></span>, then information or the entropy in feature <span class="emphasis"><em>X</em></span><sub>i</sub> is:</p><div class="mediaobject"><img src="graphics/B05137_02_041.jpg" alt="Information theoretic approach"/></div><p>Here, <span class="emphasis"><em>Info(D</em></span><sub>j</sub><span class="emphasis"><em>)</em></span> is the entropy of the partition and is calculated as:</p><div class="mediaobject"><img src="graphics/B05137_02_043.jpg" alt="Information theoretic approach"/></div><p>Here, <span class="emphasis"><em>p</em></span><sub>+</sub><span class="emphasis"><em>(D)</em></span> is the probability that the data in set <span class="emphasis"><em>D</em></span> is in the positive class and <span class="emphasis"><em>p_(D)</em></span> is the probability that it is in the negative class, in that sample. Information gain for the feature is calculated in terms of the overall information and information of the feature as</p><p>
<span class="emphasis"><em>InfoGain(X</em></span><sub>i</sub><span class="emphasis"><em>) = Info(D) – Info(D</em></span><sub>Xi</sub><span class="emphasis"><em>)</em></span>
</p><p>For <a id="id278" class="indexterm"/>numeric features, the values are sorted in ascending order and split points between neighboring values are considered as distinct values.</p><p>The greater the decrease in entropy, the higher the relevance of the feature. Information gain has problems when the feature has a large number of values; that is when Gain Ratio comes in handy. Gain Ratio corrects the information gain over large splits by introducing Split Information. Split Information for feature <span class="emphasis"><em>X</em></span><sub>i</sub> and <span class="emphasis"><em>GainRatio</em></span> is given by:</p><div class="mediaobject"><img src="graphics/B05137_02_048.jpg" alt="Information theoretic approach"/></div><div class="mediaobject"><img src="graphics/B05137_02_049.jpg" alt="Information theoretic approach"/></div><p>There are other impurity measures such as Gini Impurity Index (as described in the section on the <span class="emphasis"><em>Decision Tree</em></span> algorithm) and Uncertainty-based measures to compute feature relevance.</p></div><div class="section" title="Statistical approach"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec02"/>Statistical approach</h5></div></div></div><p>Chi-Squared <a id="id279" class="indexterm"/>feature selection is one of the most <a id="id280" class="indexterm"/>common feature selection methods that has statistical hypothesis testing as its base. The null hypothesis is that the feature and the class variable are independent of each other. The numeric features are discretized so that all features have categorical values. The contingency table is calculated as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature Values</p>
</th><th style="text-align: left" valign="bottom">
<p>Class=P</p>
</th><th style="text-align: left" valign="bottom">
<p>Class=N</p>
</th><th style="text-align: left" valign="bottom">
<p>Sum over classes <span class="emphasis"><em>niP</em></span> + <span class="emphasis"><em>niN</em></span>
</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>X</em></span><sub>1</sub></p>
</td><td style="text-align: left" valign="top">
<p>(<span class="emphasis"><em>n</em></span><sub>1P</sub>|<span class="emphasis"><em>µ</em></span><sub>1P</sub>)</p>
</td><td style="text-align: left" valign="top">
<p>(<span class="emphasis"><em>n</em></span><sub>1N</sub>|<span class="emphasis"><em>µ</em></span><sub>1N</sub>)</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>n</em></span><sub>1</sub></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>….</p>
</td><td style="text-align: left" valign="top">
<p>…</p>
</td><td style="text-align: left" valign="top">
<p>….</p>
</td><td style="text-align: left" valign="top">
<p>…</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>X</em></span><sub>m</sub></p>
</td><td style="text-align: left" valign="top">
<p>(<span class="emphasis"><em>n</em></span><sub>mP</sub>|<span class="emphasis"><em>µ</em></span><sub>mP</sub>)</p>
</td><td style="text-align: left" valign="top">
<p>(<span class="emphasis"><em>n</em></span><sub>mN</sub>|<span class="emphasis"><em>µ</em></span><sub>mN</sub>)</p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>n</em></span><sub>m</sub></p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>n</em></span><sub>*P</sub></p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>n</em></span><sub>*P</sub></p>
</td><td style="text-align: left" valign="top">
<p>
<span class="emphasis"><em>n</em></span>
</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Contingency Table 1: Showing feature values and class distribution for binary class. </em></span></p></blockquote></div><p>In the <a id="id281" class="indexterm"/>preceding table, <span class="emphasis"><em>n</em></span><sub>ij</sub> is a count of the number of features with value—after discretization—equal to <span class="emphasis"><em>x</em></span><sub>i</sub> and class value of <span class="emphasis"><em>j</em></span>.</p><p>The value summations are:</p><div class="mediaobject"><img src="graphics/B05137_02_065.jpg" alt="Statistical approach"/></div><div class="mediaobject"><img src="graphics/B05137_02_066.jpg" alt="Statistical approach"/></div><div class="mediaobject"><img src="graphics/B05137_02_067.jpg" alt="Statistical approach"/></div><div class="mediaobject"><img src="graphics/B05137_02_068.jpg" alt="Statistical approach"/></div><p>Here <span class="emphasis"><em>n</em></span> is number of data instances, <span class="emphasis"><em>j = P, N</em></span> is the class value and <span class="emphasis"><em>i =1,2, … m</em></span> indexes the different discretized values of the feature and the table has <span class="emphasis"><em>m – 1</em></span> degrees of freedom.</p><p>The Chi-Square Statistic is given by:</p><div class="mediaobject"><img src="graphics/B05137_02_072.jpg" alt="Statistical approach"/></div><p>The Chi-Square value is compared to confidence level thresholds for testing significance. For example, for <span class="emphasis"><em>i = 2</em></span>, the Chi-Squared value at threshold of 5% is 3.84; if our value is smaller than the table value of 3.83, then we know that the feature is interesting and the null hypothesis is rejected.</p></div></div><div class="section" title="Multivariate feature selection"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec05"/>Multivariate feature selection</h4></div></div></div><p>Most <a id="id282" class="indexterm"/>multivariate methods of feature <a id="id283" class="indexterm"/>selection have two goals:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reduce the redundancy between the feature and other selected features</li><li class="listitem" style="list-style-type: disc">Maximize the relevance or correlation of the feature with the class label</li></ul></div><p>The task of finding such subsets of features cannot be exhaustive as the process can have a large <a id="id284" class="indexterm"/>search space. Heuristic search methods <a id="id285" class="indexterm"/>such as backward search, forward search, hill-climbing, and genetic algorithms are typically used to find a subset of features. Two very well-known evaluation techniques for meeting the preceding goals are presented next.</p><div class="section" title="Minimal redundancy maximal relevance (mRMR)"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec03"/>Minimal redundancy maximal relevance (mRMR)</h5></div></div></div><p>In <a id="id286" class="indexterm"/>this technique, numeric features are often discretized—as done in univariate pre-processing—to get distinct categories of values.</p><p>For <a id="id287" class="indexterm"/>each subset <span class="emphasis"><em>S</em></span>, the redundancy between two features <span class="emphasis"><em>X</em></span><sub>i</sub> and <span class="emphasis"><em>X</em></span><sub>j</sub> can be measured as:</p><div class="mediaobject"><img src="graphics/B05137_02_077.jpg" alt="Minimal redundancy maximal relevance (mRMR)"/></div><p>Here, <span class="emphasis"><em>MI (X</em></span><sub>i</sub><span class="emphasis"><em>, X</em></span><sub>j</sub><span class="emphasis"><em>)</em></span> = measure of mutual information between two features <span class="emphasis"><em>X</em></span><sub>i</sub> and <span class="emphasis"><em>X</em></span><sub>j</sub>. Relevance between feature <span class="emphasis"><em>X</em></span><sub>i</sub> and class <span class="emphasis"><em>C</em></span> can be measured as:</p><div class="mediaobject"><img src="graphics/B05137_02_081.jpg" alt="Minimal redundancy maximal relevance (mRMR)"/></div><p>Also, the two goals can be combined to find the best feature subset using:</p><div class="mediaobject"><img src="graphics/B05137_02_082.jpg" alt="Minimal redundancy maximal relevance (mRMR)"/></div></div><div class="section" title="Correlation-based feature selection (CFS)"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec04"/>Correlation-based feature selection (CFS)</h5></div></div></div><p>The <a id="id288" class="indexterm"/>basic idea is <a id="id289" class="indexterm"/>similar to the previous example; the overall merit of subset <span class="emphasis"><em>S</em></span> is measured as:</p><div class="mediaobject"><img src="graphics/B05137_02_083.jpg" alt="Correlation-based feature selection (CFS)"/></div><p>Here, <span class="emphasis"><em>k</em></span> is the total number of features, <span class="inlinemediaobject"><img src="graphics/B05137_02_084.jpg" alt="Correlation-based feature selection (CFS)"/></span> is the average feature class correlation and <span class="inlinemediaobject"><img src="graphics/B05137_02_085.jpg" alt="Correlation-based feature selection (CFS)"/></span> is the average feature-feature inter correlation. The numerator gives the relevance factor while <a id="id290" class="indexterm"/>the denominator gives the redundancy factor and hence the goal of the search is to maximize the overall ratio or the <span class="emphasis"><em>Merit (S)</em></span>.</p><p>There are other techniques such as Fast-Correlation-based feature selection that is based on the same <a id="id291" class="indexterm"/>principles, but with variations in computing the metrics. Readers can experiment with this and other techniques in Weka.</p><p>The advantage of the Filter approach is that its methods are independent of learning algorithms and hence one is freed from choosing the algorithms and parameters. They are also faster than wrapper-based approaches.</p></div></div></div><div class="section" title="Wrapper approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec08"/>Wrapper approach</h3></div></div></div><p>The <a id="id292" class="indexterm"/>search technique remains the same <a id="id293" class="indexterm"/>as discussed in the feature search approach; only the evaluation method changes. In the wrapper approach, a machine learning algorithm is used to evaluate the subset of features that are found to be discriminating based on various metrics. The machine learning algorithm used as the wrapper approach may be the same or different from the one used for modeling.</p><p>Most commonly, cross-validation is used in the learning algorithm. Performance metrics such as area under curve or F-score, obtained as an average on cross-validation, guide the search process. Since the cost of training and evaluating models is very high, we choose algorithms that have fast training speed, such as Linear Regression, linear SVM, or ones that are Decision Tree-based.</p><p>Some wrapper approaches have been very successful using specific algorithms such as Random Forest to measure feature relevance.</p></div><div class="section" title="Embedded approach"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec09"/>Embedded approach</h3></div></div></div><p>This <a id="id294" class="indexterm"/>approach does not require feature <a id="id295" class="indexterm"/>search techniques. Instead of performing feature selection as preprocessing, it is done in the machine learning algorithm itself. Rule Induction, Decision Trees, Random Forest, and so on, perform feature selection as part of the training algorithm. Some <a id="id296" class="indexterm"/>algorithms such as regression or SVM-based methods, known as <span class="strong"><strong>shrinking methods</strong></span>, can add a regularization <a id="id297" class="indexterm"/>term in the model <a id="id298" class="indexterm"/>to overcome the impact of noisy features in the dataset. Ridge and lasso-based regularization are well-known techniques available in regressions to provide feature selection implicitly.</p><p>There are other techniques using unsupervised algorithms that will be discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, that can be used effectively in a supervised <a id="id299" class="indexterm"/>setting too, for example, <span class="strong"><strong>Principal Component Analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>).</p></div></div></div>
<div class="section" title="Model building"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Model building</h1></div></div></div><p>In real-world <a id="id300" class="indexterm"/>problems, there are many constraints on learning and many ways to assess model performance on unseen data. Each modeling algorithm has its strengths and weaknesses when applied to a given problem or to a class of problems in a particular domain. This is articulated <a id="id301" class="indexterm"/>in the famous <span class="strong"><strong>No Free Lunch Theorem</strong></span> (<span class="strong"><strong>NFLT</strong></span>), which says—for the case of supervised learning—that averaged over all distributions of data, every classification algorithm performs about as well as any other, including one that always picks the same class! Application of NFLT to supervised learning and search and optimization can be found at <a class="ulink" href="http://www.no-free-lunch.org/">http://www.no-free-lunch.org/</a>.</p><p>In this section, we will discuss the most commonly used practical algorithms, giving the necessary details to answer questions such as what are the algorithm's inputs and outputs? How does it work? What are the advantages and limitations to consider while choosing the algorithm? For each model, we will include sample code and outputs obtained from testing the model on the chosen dataset. This should provide the reader with insights into the process. Some algorithms such as neural networks and deep learning, Bayesian networks, stream-based earning, and so on, will be covered separately in their own chapters.</p><div class="section" title="Linear models"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Linear models</h2></div></div></div><p>Linear <a id="id302" class="indexterm"/>models work well when the data is linearly separable. This <a id="id303" class="indexterm"/>should always be the first thing to establish.</p><div class="section" title="Linear Regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec10"/>Linear Regression</h3></div></div></div><p>Linear <a id="id304" class="indexterm"/>Regression can be used for both classification <a id="id305" class="indexterm"/>and estimation problems. It is one of the most widely used methods in practice. It consists of finding the best-fitting hyperplane through the data points.</p><div class="section" title="Algorithm input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec06"/>Algorithm input and output</h4></div></div></div><p>Features <a id="id306" class="indexterm"/>must be numeric. Categorical features are transformed <a id="id307" class="indexterm"/>using various pre-processing techniques, as when a categorical value becomes a feature with 1 and 0 values. Linear Regression models output a categorical class in classification or numeric values in regression. Many implementations also give confidence values.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec07"/>How does it work?</h4></div></div></div><p>The <a id="id308" class="indexterm"/>model tries to learn a "hyperplane" in the input space that minimizes the error between the data points of each class (<span class="emphasis"><em>References</em></span> [4]).</p><p>A <a id="id309" class="indexterm"/>hyperplane in d-dimensional inputs that linear model learns is given by:</p><div class="mediaobject"><img src="graphics/B05137_02_087.jpg" alt="How does it work?"/></div><p>The two regions (binary classification) the model divides the input space into are <span class="inlinemediaobject"><img src="graphics/B05137_02_088.jpg" alt="How does it work?"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_02_089.jpg" alt="How does it work?"/></span>. Associating a value of 1 to the coordinate of feature 0, that is, <span class="emphasis"><em>x</em></span>0=1, the vector representation of hypothesis space or the model is:</p><div class="mediaobject"><img src="graphics/B05137_02_091.jpg" alt="How does it work?"/></div><p>The weight matrix can be derived using various methods such as ordinary least squares or iterative methods using matrix notation as follows:</p><div class="mediaobject"><img src="graphics/B05137_02_092.jpg" alt="How does it work?"/></div><p>Here <span class="strong"><strong>X</strong></span> is the input matrix and <span class="strong"><strong>y</strong></span> is the label. If the matrix <span class="strong"><strong>X</strong></span><sup>T</sup><span class="strong"><strong>X</strong></span> in the least squares problem is not of full rank or if encountering various numerical stability issues, the solution is modified as:</p><div class="mediaobject"><img src="graphics/B05137_02_096.jpg" alt="How does it work?"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B05137_02_097.jpg" alt="How does it work?"/></span> is added to the diagonal of an identity matrix <span class="strong"><strong>I</strong></span><sub>n</sub> of size (<span class="emphasis"><em>n</em></span> + 1, <span class="emphasis"><em>n</em></span> + 1) with the rest <a id="id310" class="indexterm"/>of the values being set to 0. This solution is called <span class="strong"><strong>ridge regression</strong></span> and parameter λ theoretically controls the trade-off between the square loss and low norm of the solution. The constant λ is also known as regularization constant and helps in preventing "overfitting".</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec08"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is <a id="id311" class="indexterm"/>an appropriate method to try and get insights when <a id="id312" class="indexterm"/>there are less than 100 features and a few thousand data points.</li><li class="listitem" style="list-style-type: disc">Interpretable to some level as the weights give insights on the impact of each feature.</li><li class="listitem" style="list-style-type: disc">Assumes linear relationship, additive and uncorrelated features, hence it doesn't model complex non-linear real-world data. Some implementations of Linear Regression allow removing collinear features to overcome this issue.</li><li class="listitem" style="list-style-type: disc">Very susceptible to outliers in the data, if there are huge outliers, they have to be treated prior to performing Linear Regression.</li><li class="listitem" style="list-style-type: disc">Heteroskedasticity, that is, unequal training point variances, can affect the simple least square regression models. Techniques such as weighted least squares are employed to overcome this situation.</li></ul></div></div></div><div class="section" title="Naïve Bayes"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec11"/>Naïve Bayes</h3></div></div></div><p>Based <a id="id313" class="indexterm"/>on the Bayes rule, the Naïve Bayes classifier assumes the <a id="id314" class="indexterm"/>features of the data are independent of each other (<span class="emphasis"><em>References</em></span> [9]). It is especially suited for large datasets and frequently performs better than other, more elaborate techniques, despite its naïve assumption of feature independence.</p><div class="section" title="Algorithm input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec09"/>Algorithm input and output</h4></div></div></div><p>The <a id="id315" class="indexterm"/>Naïve Bayes model can take features that are both <a id="id316" class="indexterm"/>categorical and continuous. Generally, the performance of Naïve Bayes models improves if the continuous features are discretized in the right format. Naïve Bayes outputs the class and the probability score for all class values, making it a good classifier for scoring models.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec10"/>How does it work?</h4></div></div></div><p>It <a id="id317" class="indexterm"/>is a probability-based modeling algorithm. The basic idea is using Bayes' rule and measuring the probabilities of different terms, as given here. Measuring probabilities can be done either using pre-processing such as discretization, assuming a certain distribution, or, given enough data, mapping the distribution for numeric features.</p><p>Bayes' rule is applied to get the posterior probability as predictions and <span class="emphasis"><em>k</em></span> represents <span class="emphasis"><em>k</em></span><sup>th</sup> class.:</p><div class="mediaobject"><img src="graphics/B05137_02_101.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_02_102.jpg" alt="How does it work?"/></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec11"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is <a id="id318" class="indexterm"/>robust against isolated noisy data points because such points <a id="id319" class="indexterm"/>are averaged when estimating the probabilities of input data.</li><li class="listitem" style="list-style-type: disc">Probabilistic scores as confidence values from Bayes classification can be used as scoring models.</li><li class="listitem" style="list-style-type: disc">Can handle missing values very well as they are not used in estimating probabilities.</li><li class="listitem" style="list-style-type: disc">Also, it is robust against irrelevant attributes. If the features are not useful the probability distribution for the classes will be uniform and will cancel itself out.</li><li class="listitem" style="list-style-type: disc">Very good in training speed and memory, it can be parallelized as each computation of probability in the equation is independent of the other.</li><li class="listitem" style="list-style-type: disc">Correlated features can be a big issue when using Naïve Bayes because the conditional independence assumption is no longer valid.</li><li class="listitem" style="list-style-type: disc">Normal distribution of errors is an assumption in most optimization algorithms.</li></ul></div></div></div><div class="section" title="Logistic Regression"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec12"/>Logistic Regression</h3></div></div></div><p>If <a id="id320" class="indexterm"/>we employ Linear Regression model using, say, the <a id="id321" class="indexterm"/>least squares regression method, the outputs have to be converted to classes, say 0 and 1. Many Linear Regression algorithms output class and confidence as probability. As a rule of thumb, if we see that the probabilities of Linear Regression are mostly beyond the ranges of 0.2 to 0.8, then logistic regression algorithm may be a better choice.</p><div class="section" title="Algorithm input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec12"/>Algorithm input and output</h4></div></div></div><p>Similar <a id="id322" class="indexterm"/>to Linear Regression, all features must be <a id="id323" class="indexterm"/>numeric. Categorical features have to be transformed to numeric. Like in Naïve Bayes, this algorithm outputs class and probability for each class and can be used as a scoring model.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec13"/>How does it work?</h4></div></div></div><p>Logistic <a id="id324" class="indexterm"/>regression models the posterior probabilities of classes using linear functions in the input features.</p><p>The logistic regression model for a binary classification is given as:</p><div class="mediaobject"><img src="graphics/B05137_02_104.jpg" alt="How does it work?"/></div><p>The <a id="id325" class="indexterm"/>model is a log-odds or logit transformation of linear models (<span class="emphasis"><em>References</em></span> [6]). The weight vector is generally computed using various <a id="id326" class="indexterm"/>optimization methods such as <span class="strong"><strong>iterative reweighted least squares</strong></span> (<span class="strong"><strong>IRLS</strong></span>) or the <span class="strong"><strong>Broyden–Fletcher–Goldfarb–Shanno</strong></span> (<span class="strong"><strong>BFGS</strong></span>) method, or variants of these methods.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec14"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Overcomes <a id="id327" class="indexterm"/>the issue of heteroskedasticity and some <a id="id328" class="indexterm"/>non-linearity between inputs and outputs.</li><li class="listitem" style="list-style-type: disc">No need of normal distribution assumptions in the error estimates.</li><li class="listitem" style="list-style-type: disc">It is interpretable, but less so than Linear Regression models as some understanding of statistics <a id="id329" class="indexterm"/>is required. It gives information such as odds ratio, <span class="emphasis"><em>p</em></span> values, and so on, which are useful in understanding the effects of features on the <a id="id330" class="indexterm"/>classes as well as doing implicit feature relevance based on significance of <span class="emphasis"><em>p</em></span> values.</li><li class="listitem" style="list-style-type: disc">L1 or L2 regularization has to be employed in practice to overcome overfitting in the logistic regression models.</li><li class="listitem" style="list-style-type: disc">Many optimization algorithms are available for improving speed of training and robustness.</li></ul></div></div></div></div><div class="section" title="Non-linear models"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Non-linear models</h2></div></div></div><p>Next, we <a id="id331" class="indexterm"/>will discuss some of the well-known, practical, and <a id="id332" class="indexterm"/>most commonly used non-linear models.</p><div class="section" title="Decision Trees"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec13"/>Decision Trees</h3></div></div></div><p>Decision <a id="id333" class="indexterm"/>Trees are also known as <span class="strong"><strong>Classification and Regression Trees</strong></span> (<span class="strong"><strong>CART</strong></span>) (<span class="emphasis"><em>References</em></span> [5]). Their representation is a binary <a id="id334" class="indexterm"/>tree constructed by evaluating <a id="id335" class="indexterm"/>an inequality in terms of a single attribute at each internal node, with each leaf-node corresponding to the output value or class resulting from the decisions in the path leading to it. When a new input is provided, the output is predicted by traversing the tree starting at the root.</p><div class="section" title="Algorithm inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec15"/>Algorithm inputs and outputs</h4></div></div></div><p>Features <a id="id336" class="indexterm"/>can be both categorical and numeric. It generates <a id="id337" class="indexterm"/>class as an output and most implementations give a score or probability using frequency-based estimation. Decision Trees probabilities are not smooth functions like Naïve Bayes and Logistic Regression, though there are extensions that are.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec16"/>How does it work?</h4></div></div></div><p>Generally, a single <a id="id338" class="indexterm"/>tree is created, starting with single features at the root with decisions split into branches based on the values of the features while at the leaf there is either a class or more features. There are many choices to be made, such as how many trees, how to choose features at the root level or at subsequent leaf level, and how to split the feature values when not categorical. This has resulted in many different algorithms or modifications to the basic Decision Tree. Many techniques to split the feature <a id="id339" class="indexterm"/>values are similar to what was discussed in the section on discretization. Generally, some form of pruning is applied to reduce the size of the tree, which helps in addressing overfitting.</p><p>Gini index is another popular technique used to split the features. Gini index of data in set <span class="emphasis"><em>S</em></span> of all the data points is <span class="inlinemediaobject"><img src="graphics/B05137_02_106.jpg" alt="How does it work?"/></span> where <span class="emphasis"><em>p</em></span><sub>1</sub>, <span class="emphasis"><em>p</em></span><sub>2</sub> … <span class="emphasis"><em>p</em></span><sub>k</sub> are probability distribution for each class.</p><p>If <span class="emphasis"><em>p</em></span> is the fraction or probability of data in set <span class="emphasis"><em>S</em></span> of all the data points belonging to say class positive, then 1 – <span class="emphasis"><em>p</em></span> is the fraction for the other class or the error rate in binary classification. If the dataset <span class="emphasis"><em>S</em></span> is split in <span class="emphasis"><em>r</em></span> ways <span class="emphasis"><em>S</em></span><sub>1</sub><span class="emphasis"><em>, S</em></span><sub>2</sub><span class="emphasis"><em>, …S</em></span><sub>r</sub> then the error rate of each set can be quantified as |<span class="emphasis"><em>S</em></span><sub>i</sub>|. Gini index for an <span class="emphasis"><em>r</em></span> way split is as follows:</p><div class="mediaobject"><img src="graphics/B05137_02_113.jpg" alt="How does it work?"/></div><p>The split with <a id="id340" class="indexterm"/>the lowest Gini index is used for selection. The CART algorithm, a popular Decision Tree algorithm, uses Gini index for split criteria.</p><p>The entropy of the set of data points <span class="emphasis"><em>S</em></span> can similarly be computed as:</p><div class="mediaobject"><img src="graphics/B05137_02_114.jpg" alt="How does it work?"/></div><p>Similarly, entropy-based split is computed as:</p><div class="mediaobject"><img src="graphics/B05137_02_115.jpg" alt="How does it work?"/></div><p>The lower the value of the entropy split, the better the feature, and this is used in ID3 and C4.5 Decision Tree algorithms (<span class="emphasis"><em>References</em></span> [12]).</p><p>The stopping criteria and pruning criteria are related. The idea behind stopping the growth of the tree early or pruning is to reduce the "overfitting" and it works similar to regularization in linear and logistic models. Normally, the training set is divided into tree growing sets and pruning sets so that pruning uses different data to overcome any biases from the growing set. <span class="strong"><strong>Minimum Description Length</strong></span> (<span class="strong"><strong>MDL</strong></span>), which penalizes the complexity <a id="id341" class="indexterm"/>of the tree based on number of nodes is a <a id="id342" class="indexterm"/>popular methodology used in many Decision Tree algorithms.</p><div class="mediaobject"><img src="graphics/B05137_02_116.jpg" alt="How does it work?"/><div class="caption"><p>Figure 5: Shows a two-dimensional binary classification problem and a Decision Tree induced using splits at thresholds <span class="emphasis"><em>X</em></span><sub>1t</sub> and <span class="emphasis"><em>X</em></span><sub>1t</sub>, respectively</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec17"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id343" class="indexterm"/>main advantages of Decision Trees are they are quite <a id="id344" class="indexterm"/>easily interpretable. They can be understood in layman's terms and are especially suited for business domain experts to easily understand the exact model.</li><li class="listitem" style="list-style-type: disc">If there are a large number of features, then building Decision Tree can take lots of training time as the complexity of the algorithm increases.</li><li class="listitem" style="list-style-type: disc">Decision Trees have an inherent problem with overfitting. Many tree algorithms have pruning options to reduce the effect. Using pruning and validation techniques can alleviate the problem to a large extent.</li><li class="listitem" style="list-style-type: disc">Decision Trees work well when there is correlation between the features.</li><li class="listitem" style="list-style-type: disc">Decision Trees build axis-parallel boundaries across classes, the bias of which can introduce errors, especially in a complex, smooth, non-linear boundary.</li></ul></div></div></div><div class="section" title="K-Nearest Neighbors (KNN)"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec14"/>K-Nearest Neighbors (KNN)</h3></div></div></div><p>K-Nearest <a id="id345" class="indexterm"/>Neighbors falls under the branch <a id="id346" class="indexterm"/>of non-parametric and lazy algorithms. K-Nearest neighbors doesn't make any assumptions on the underlying data and doesn't build and generalize models from training data (<span class="emphasis"><em>References</em></span> [10]).</p><div class="section" title="Algorithm inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec18"/>Algorithm inputs and outputs</h4></div></div></div><p>Though <a id="id347" class="indexterm"/>KNN's can work with categorical <a id="id348" class="indexterm"/>and numeric features, the distance computation, which is the core of finding the neighbors, works better with numeric features. Normalization of numeric features to be in the same ranges is one of the mandatory steps required. KNN's outputs are generally the classes based on the neighbors' distance calculation.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec19"/>How does it work?</h4></div></div></div><p>KNN uses <a id="id349" class="indexterm"/>the entire training data to make predictions on unseen test data. When unseen test data is presented KNN finds the K "nearest neighbors" using some distance computation and based on the neighbors and the metric of deciding the category it classifies the new point. If we consider two vectors represented by <span class="strong"><strong>x</strong></span><sub>1</sub> and <span class="strong"><strong>x</strong></span><sub>2</sub> corresponding to two data points the distance is calculated as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Euclidean Distance:<div class="mediaobject"><img src="graphics/B05137_02_121.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc">Cosine Distance or similarity:<div class="mediaobject"><img src="graphics/B05137_02_122.jpg" alt="How does it work?"/></div></li></ul></div><p>The metric used to classify an unseen may simply be the majority class among the <span class="emphasis"><em>K</em></span> neighbors.</p><p>The training time is small as all it has to do is build data structures to hold the data in such a way that the computation of the nearest neighbor is minimized when unseen data is presented. The algorithm relies on choices of how the data is stored from training data points for efficiency of searching the neighbors, which distance computation is used to find the nearest neighbor, and which metrics are used to categorize based on classes of all neighbors. Choosing the value of "<span class="emphasis"><em>K</em></span>" in KNN by <a id="id350" class="indexterm"/>using validation techniques is critical.</p><div class="mediaobject"><img src="graphics/B05137_02_123.jpg" alt="How does it work?"/><div class="caption"><p>Figure 6: K-Nearest Neighbor illustrated using two-dimensional data with different choices of k.</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec20"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">No assumption <a id="id351" class="indexterm"/>on underlying data distribution <a id="id352" class="indexterm"/>and minimal training time makes it a very attractive method for learning.</li><li class="listitem" style="list-style-type: disc">KNN uses local information for computing the distances and in certain domains can yield highly adaptive behaviors.</li><li class="listitem" style="list-style-type: disc">It is robust to noisy training data when <span class="emphasis"><em>K</em></span> is effectively chosen.</li><li class="listitem" style="list-style-type: disc">Holding the entire training data for classification can be problematic depending on the number of data points and hardware constraints</li><li class="listitem" style="list-style-type: disc">Number of features and the curse of dimensionality affects this algorithm more hence some form of dimensionality reduction or feature selection has to be done prior to modeling in KNN.</li></ul></div></div></div><div class="section" title="Support vector machines (SVM)"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec15"/>Support vector machines (SVM)</h3></div></div></div><p>SVMs, in <a id="id353" class="indexterm"/>simple terms, can be viewed <a id="id354" class="indexterm"/>as linear classifiers that maximize the margin between the separating hyperplane and the data by solving a constrained <a id="id355" class="indexterm"/>optimization problem. SVMs can even <a id="id356" class="indexterm"/>deal with data that is not linearly separable by invoking transformation to a higher dimensional space using kernels described later.</p><div class="section" title="Algorithm inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec21"/>Algorithm inputs and outputs</h4></div></div></div><p>SVM is <a id="id357" class="indexterm"/>effective with numeric features <a id="id358" class="indexterm"/>only, though most implementations can handle categorical features with transformation to numeric or binary. Normalization is often a choice as it helps the optimization part of the training. Outputs of SVM are class predictions. There are implementations that give probability estimates as confidences, but this requires considerable training time as they use k-fold cross-validation to build the estimates.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec22"/>How does it work?</h4></div></div></div><p> In its <a id="id359" class="indexterm"/>linear form, SVM works similar to Linear Regression classifier, where a linear decision boundary is drawn between the two classes. The difference between the two is that with SVM, the boundary is drawn in such a way that the "margin" or the distance between the points near the boundary is maximized. The points on the boundaries are known as "support vectors" (<span class="emphasis"><em>References</em></span> [13 and 8]).</p><p>Thus, SVM tries to find the weight vector in linear models similar to Linear Regression model as given by the following:</p><div class="mediaobject"><img src="graphics/B05137_02_124.jpg" alt="How does it work?"/></div><p>The weight <span class="emphasis"><em>w</em></span><sub>0</sub> is represented by <span class="emphasis"><em>b</em></span> here. SVM for a binary class y ∈{1,-1} tries to find a hyperplane:</p><div class="mediaobject"><img src="graphics/B05137_02_127.jpg" alt="How does it work?"/></div><p>The hyperplane tries to separate the data points such that all points with the class lie on the side of the hyperplane as:</p><div class="mediaobject"><img src="graphics/B05137_02_128.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_02_129.jpg" alt="How does it work?"/></div><p>The <a id="id360" class="indexterm"/>models are subjected to maximize the margin using constraint-based optimization with a penalty function denoted by <span class="emphasis"><em>C</em></span> for overcoming the errors denoted by <span class="inlinemediaobject"><img src="graphics/B05137_02_131.jpg" alt="How does it work?"/></span>:</p><div class="mediaobject"><img src="graphics/B05137_02_132.jpg" alt="How does it work?"/></div><p>Such that <span class="inlinemediaobject"><img src="graphics/B05137_02_133.jpg" alt="How does it work?"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_02_134.jpg" alt="How does it work?"/></span>.</p><p>They are also known as large margin classifiers for the preceding reason. The kernel-based SVM transforms the input data into a hypothetical feature space where SV machinery works in a linear way and the boundaries are drawn in the feature spaces.</p><p>A kernel function on the transformed representation is given by:</p><div class="mediaobject"><img src="graphics/B05137_02_135.jpg" alt="How does it work?"/></div><p>Here Φ is a transformation on the input space. It can be seen that the entire optimization and solution of SVM remains the same with the only exception that the dot-product <span class="strong"><strong>x</strong></span><sub>i</sub> · <span class="strong"><strong>x</strong></span><sub>j</sub> is replaced by the kernel function <span class="emphasis"><em>k</em></span>(<span class="strong"><strong>x</strong></span><sub>i</sub>, <span class="strong"><strong>x</strong></span><sub>j</sub>), which is a function involving the two vectors in a different space <a id="id361" class="indexterm"/>without actually transforming to that space. This is known as the <span class="strong"><strong>kernel trick</strong></span>.</p><p>The <a id="id362" class="indexterm"/>most well-known kernels that are normally used are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Gaussian Radial Basis Kernel</strong></span>:<div class="mediaobject"><img src="graphics/B05137_02_139.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Polynomial Kernel</strong></span>:<div class="mediaobject"><img src="graphics/B05137_02_140.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sigmoid Kernel</strong></span>:<div class="mediaobject"><img src="graphics/B05137_02_141.jpg" alt="How does it work?"/></div></li></ul></div><p>SVM's performance <a id="id363" class="indexterm"/>is very sensitive to some of the parameters of optimization and the kernel <a id="id364" class="indexterm"/>parameters and <a id="id365" class="indexterm"/>the core SV parameter such as the cost function <span class="emphasis"><em>C</em></span>. Search techniques such as grid search or evolutionary search combined with validation techniques such as cross-validation are generally used to find the best parameter values.</p><div class="mediaobject"><img src="graphics/B05137_02_142.jpg" alt="How does it work?"/><div class="caption"><p>Figure 7: SVM Linear Hyperplane learned from training data that creates a maximum margin separation between two classes.</p></div></div><div class="mediaobject"><img src="graphics/B05137_02_144.jpg" alt="How does it work?"/><div class="caption"><p>Figure 8: Kernel transformation illustrating how two-dimensional input space can be transformed using a polynomial transformation into a three-dimensional feature space where data is linearly separable.</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec23"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">SVMs are <a id="id366" class="indexterm"/>among the best in generalization, low overfitting, and have a good theoretical foundation for complex non-linear <a id="id367" class="indexterm"/>data if the parameters are chosen judiciously.</li><li class="listitem" style="list-style-type: disc">SVMs work well even with a large number of features and less training data.</li><li class="listitem" style="list-style-type: disc">SVMs are less sensitive to noisy training data.</li><li class="listitem" style="list-style-type: disc">The biggest disadvantage of SVMs is that they are not interpretable.</li><li class="listitem" style="list-style-type: disc">Another big issue with SVM is its training time and memory requirements. They are <span class="emphasis"><em>O(n</em></span><sup>2</sup><span class="emphasis"><em>)</em></span> and <span class="emphasis"><em>O(n</em></span><sup>3</sup><span class="emphasis"><em>)</em></span> and can result in major scalability issues when the data is large or there are hardware constraints. There are some modifications that help in reducing both.</li><li class="listitem" style="list-style-type: disc">SVM generally works well for binary classification problems, but for multiclass classification problems, though there are techniques such as one versus many and one versus all, it is not as robust as some other classifiers such as Decision Trees.</li></ul></div></div></div></div><div class="section" title="Ensemble learning and meta learners"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Ensemble learning and meta learners</h2></div></div></div><p>Combining <a id="id368" class="indexterm"/>multiple algorithms or models to classify instead of <a id="id369" class="indexterm"/>relying on just one is known as ensemble learning. It <a id="id370" class="indexterm"/>helps to combine various models as each model can <a id="id371" class="indexterm"/>be considered—at a high level—as an expert in detecting specific patterns in the whole dataset. Each base learner can be made to learn on slightly different datasets too. Finally, the results from all models are combined to perform prediction. Based on how similar the algorithms used in combination are, how the training dataset is presented to each algorithm, and how the algorithms combine the results to finally classify the unseen dataset, there are many branches of ensemble learning:</p><div class="mediaobject"><img src="graphics/B05137_02_145.jpg" alt="Ensemble learning and meta learners"/><div class="caption"><p>Figure 9: Illustration of ensemble learning strategies</p></div></div><p>Some common <a id="id372" class="indexterm"/>types of ensemble learning are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Different learning algorithms</li><li class="listitem" style="list-style-type: disc">Same learning algorithms, but with different parameter choices</li><li class="listitem" style="list-style-type: disc">Different learning algorithms on different feature sets</li><li class="listitem" style="list-style-type: disc">Different learning algorithms with different training data</li></ul></div><div class="section" title="Bootstrap aggregating or bagging"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec16"/>Bootstrap aggregating or bagging</h3></div></div></div><p>It is one <a id="id373" class="indexterm"/>of the most commonly <a id="id374" class="indexterm"/>used ensemble methods for dividing the data in different samples and building classifiers on each sample.</p><div class="section" title="Algorithm inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec24"/>Algorithm inputs and outputs</h4></div></div></div><p>The <a id="id375" class="indexterm"/>input is constrained by the choice <a id="id376" class="indexterm"/>of the base learner used—if using Decision Trees there are basically no restrictions. The method outputs class membership along with the probability distribution for classes.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec25"/>How does it work?</h4></div></div></div><p>The <a id="id377" class="indexterm"/>core idea of bagging is to apply the bootstrapping estimation to different learners that have high variance, such as Decision Trees. Bootstrapping is any statistical measure that depends on random sampling with replacement. The entire data is split into different samples using bootstrapping and for each sample, a model is built using the base learner. Finally, while predicting, the average prediction is arrived at using a majority vote—this is one technique to combine over all the learners.</p><div class="section" title="Random Forest"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec05"/>Random Forest</h5></div></div></div><p>Random Forest is <a id="id378" class="indexterm"/>an improvement over basic bagged Decision Trees. Even with bagging, the basic Decision Tree has a <a id="id379" class="indexterm"/>choice of all the features at every split point in creating a tree. Because of this, even with different samples, many trees can form highly correlated submodels, which causes the performance of bagging to deteriorate. By giving random features to different models in addition to a random dataset, the correlation between the submodels reduces and Random Forest shows much better performance compared to basic bagged trees. Each tree in Random Forest grows its structure on the random features, thereby minimizing the bias; combining many such trees on decision reduces the variance (<span class="emphasis"><em>References</em></span> [15]). Random Forest is also used to measure feature relevance by averaging the impurity decrease in the trees and ranking them across all the features to give the relative importance of each.</p></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec26"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Better <a id="id380" class="indexterm"/>generalization than the single base learner. Overcomes the issue of overfitting of base learners.</li><li class="listitem" style="list-style-type: disc">Interpretability of bagging <a id="id381" class="indexterm"/>is very low as it works as meta learner combining even the interpretable learners.</li><li class="listitem" style="list-style-type: disc">Like most other ensemble learners, Bagging is resilient to noise and outliers.</li><li class="listitem" style="list-style-type: disc">Random Forest generally does not tend to overfit given the training data is iid.</li></ul></div></div></div><div class="section" title="Boosting"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec17"/>Boosting</h3></div></div></div><p>Boosting <a id="id382" class="indexterm"/>is another popular form of ensemble learning, which <a id="id383" class="indexterm"/>is based on using a weak learner and iteratively learning the points that are "misclassified" or difficult to learn. Thus, the idea is to "boost" the difficult to learn instances and making the base learners learn the decision boundaries more effectively. There are various flavors of boosting such as AdaBoost, LogitBoost, ConfidenceBoost, Gradient Boosting, and so on. We present a very basic form of AdaBoost here (<span class="emphasis"><em>References</em></span> [14]).</p><div class="section" title="Algorithm inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec27"/>Algorithm inputs and outputs</h4></div></div></div><p>The <a id="id384" class="indexterm"/>input is constrained by the choice of the base learner used—if using Decision Trees there are basically no restrictions. Outputs class membership <a id="id385" class="indexterm"/>along with probability distribution for classes.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec28"/>How does it work?</h4></div></div></div><p>The basic <a id="id386" class="indexterm"/>idea behind boosting is iterative reweighting of input samples to create new distribution of the data for learning a model from a simple base learner in every iteration.</p><p>Initially, all the instances are uniformly weighted with weights <span class="inlinemediaobject"><img src="graphics/B05137_02_146.jpg" alt="How does it work?"/></span> and at every iteration <span class="emphasis"><em>t</em></span>, the population is resampled or reweighted as <span class="inlinemediaobject"><img src="graphics/B05137_02_148.jpg" alt="How does it work?"/></span> where <span class="inlinemediaobject"><img src="graphics/B05137_02_149.jpg" alt="How does it work?"/></span> and <span class="emphasis"><em>Z</em></span>t is the normalization constant.</p><p>The final model works as a linear combination of all the models learned in the iteration:</p><div class="mediaobject"><img src="graphics/B05137_02_151.jpg" alt="How does it work?"/></div><p>The reweighting or resampling of the data in each iteration is based on "errors"; the data points that <a id="id387" class="indexterm"/>result in errors are sampled more or have larger weights.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec29"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Better <a id="id388" class="indexterm"/>generalization than the base learner and overcomes the issue <a id="id389" class="indexterm"/>of overfitting very effectively.</li><li class="listitem" style="list-style-type: disc">Some boosting algorithms such as AdaBoost can be susceptible to uniform noise. There are variants of boosting such as "GentleBoost" and "BrownBoost" that decrease the effect of outliers.</li><li class="listitem" style="list-style-type: disc">Boosting has a theoretical bounds and guarantee on the error estimation making it a statistically robust algorithm.</li></ul></div></div></div></div></div>
<div class="section" title="Model assessment, evaluation, and comparisons"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Model assessment, evaluation, and comparisons</h1></div></div></div><p>The key <a id="id390" class="indexterm"/>ideas discussed here are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to <a id="id391" class="indexterm"/>assess or estimate the performance of the classifier on unseen datasets that it will be predicting on future unseen datasets.</li><li class="listitem" style="list-style-type: disc">What are the <a id="id392" class="indexterm"/>metrics that we should use to assess the performance of the model?</li><li class="listitem" style="list-style-type: disc">How do we compare algorithms if we have to choose between them?</li></ul></div><div class="section" title="Model assessment"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Model assessment</h2></div></div></div><p>In order <a id="id393" class="indexterm"/>to train the model(s), tune the model parameters, select the models, and finally estimate the predictive behavior of models on unseen data, we need many datasets. We cannot train the model on one set of data and estimate its behavior on the same set of data, as it will have a clear optimistic bias and estimations will be unlikely to match the behavior in the unseen data. So at a minimum, there is a need to partition data available into training sets and testing sets. Also, we need to tune the parameters of the model and test the effect of the tuning on a separate dataset before we perform testing on the test set. The same argument of optimistic bias and wrong estimation applies if we use the same dataset for training, parameter tuning, and testing. Thus there is a theoretical and practical need to have three datasets, that is, training, validation, and testing.</p><p>The models are <a id="id394" class="indexterm"/>trained on the training set, the effect of different parameters on the training set are validated on the validation set, and the finalized model with the selected parameters is run on the test set to gauge the performance of the model on future unseen data. When the dataset is not large enough, or is large but the imbalance between classes is wide, that is, one class is present only in a small fraction of the total population, we cannot create too many samples. Recall that one of the steps described in our methodology is to create different data samples and datasets. If the total training data is large and has a good proportion of data and class ratios, then creating these three sets using random stratified partitioning is the most common option employed. In certain datasets that show seasonality and time-dependent behaviors, creating datasets based on time bounds is a common practice. In many cases, when the dataset is not large enough, only two physical partitions, that is, training and testing may be created. The training dataset ranges roughly from 66% to 80% while the rest is used for testing. The validation set is then created from the training dataset using the k-fold cross-validation technique. The training dataset is split <span class="emphasis"><em>k</em></span> times, each time producing <span class="emphasis"><em>k-1/k</em></span> random training <span class="emphasis"><em>1/k</em></span> testing data samples, and the average metrics of performance needed is generated. This way the limited training data is partitioned <span class="emphasis"><em>k</em></span> times and average performance across different split of training/testing is used for gauging the effect of the parameters. Using 10-fold cross-validation is the most common practice employed in cross-validation.</p></div><div class="section" title="Model evaluation metrics"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Model evaluation metrics</h2></div></div></div><p>The next <a id="id395" class="indexterm"/>important decision when tuning parameters or selecting models is to base your decision on certain performance metrics. In classification learning, there are different metrics available on which you can base your decision, depending on the business requirement. For example, in certain domains, not missing a single true positive is the most important concern, while in other domains where humans are involved in adjudicating results of models, having too many false positives is the greater concern. In certain cases, having overall good accuracy is considered more vital. In highly imbalanced datasets such as fraud or cyber attacks, there are just a handful of instances of one class and millions of the other classes. In such cases accuracy gives a wrong indication of model performance and some other metrics such as precision, true positive ratio, or area under the <a id="id396" class="indexterm"/>curve are used as metrics.</p><p>We will now discuss the most commonly employed metrics in classification algorithms evaluation (<span class="emphasis"><em>References</em></span> [16, 17, and 19]).</p><div class="mediaobject"><img src="graphics/B05137_02_152.jpg" alt="Model evaluation metrics"/><div class="caption"><p>Figure 10: Model evaluation metrics for classification models</p></div></div><div class="section" title="Confusion matrix and related metrics"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec18"/>Confusion matrix and related metrics</h3></div></div></div><div class="mediaobject"><img src="graphics/B05137_02_153.jpg" alt="Confusion matrix and related metrics"/><div class="caption"><p>Figure 11: Confusion Matrix</p></div></div><p>The <a id="id397" class="indexterm"/>confusion matrix is central to the definition of a <a id="id398" class="indexterm"/>number of model performance metrics. The proliferation of metrics and synonymous terms is a result of the utility of different quantities derived from the elements of the matrix in various disciplines, each emphasizing a different aspect of the model's behavior.</p><p>The <a id="id399" class="indexterm"/>four elements of the matrix are raw counts of the number of False Positives, False Negatives, True Positives, and True Negatives. Often more interesting are the different ratios of these quantities, the True Positive Rate (or Sensitivity, or Recall), and the False Positive Rate (FPR, or 1—Specificity, or Fallout). Accuracy reflects the percentage of correct predictions, whether Class 1 or Class 0. For skewed datasets, accuracy is not particularly useful, as even a constant prediction can appear to perform well.</p></div><div class="section" title="ROC and PRC curves"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec19"/>ROC and PRC curves</h3></div></div></div><p>The <a id="id400" class="indexterm"/>previously mentioned metrics such as accuracy, precision, recall, sensitivity, and specificity are aggregates, that is, they describe <a id="id401" class="indexterm"/>the behavior of the entire dataset. In many <a id="id402" class="indexterm"/>complex problems it is often valuable to see the trade-off <a id="id403" class="indexterm"/>between metrics such as TPs and say FPs.</p><p>Many classifiers, mostly probability-based classifiers, give confidence or probability of the prediction, in addition to giving classification. The process to obtain the ROC or PRC curves is to run the unseen validation or test set on the learned models, and then obtain the prediction and the probability of prediction. Sort the predictions based on the confidences in decreasing order. For every probability or confidence calculate two metrics, the fraction of FP (FP rate) and the fraction of TP (TP rate).</p><p>Plotting the TP rate on the <span class="emphasis"><em>y</em></span> axis and FP rate on the <span class="emphasis"><em>x</em></span> axis gives the ROC curves. ROC curves of random classifiers lie close to the diagonal while the ROC curves of good classifiers tend towards the upper left of the plot. The <span class="strong"><strong>area under the curve</strong></span> (<span class="strong"><strong>AUC</strong></span>) is the area measured under the ROC curve by using the trapezoidal area from 0 to 1 of ROC curves. While running cross-validation for instance there can be many ROC curves. There are two ways to get "average" ROC curves: first, using vertical averaging, that is, TPR average is plotted at different FP rate or second, using horizontal averaging, that is, FPR average is plotted at different TP rate. The classifiers that have area under curves greater than 0.8, as a rule-of-thumb are considered good for prediction for unseen data.</p><p>Precision Recall curves or PRC curves are similar to ROC curves, but instead of TPR versus FPR, metrics Precision and Recall are plotted on the <span class="emphasis"><em>y</em></span> and <span class="emphasis"><em>x</em></span> axis, respectively. When the data is highly imbalanced, that is, ROC curves don't really show the impact while PRC curves are more reliable in judging performance.</p></div><div class="section" title="Gain charts and lift curves"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec20"/>Gain charts and lift curves</h3></div></div></div><p>Lift and Gain charts <a id="id404" class="indexterm"/>are more biased towards sensitivity <a id="id405" class="indexterm"/>or true positives. The whole purpose <a id="id406" class="indexterm"/>of these two charts is to show how instead of random selection, the <a id="id407" class="indexterm"/>models prediction and confidence can detect better quality or true positives in the sample of unseen data.</p><p>This is usually very appealing for detection engines that are used in detecting fraud in financial crime or threats in cyber security. The gain charts and lift curves give exact estimates of real true positives that will be detected at different quartiles or intervals of total data. This will give insight to the business decision makers on how many investigators would <a id="id408" class="indexterm"/>be needed or how many hours would be spent towards detecting <a id="id409" class="indexterm"/>fraudulent actions or cyber attacks and thus can give real ROI of the models.</p><p>The process for <a id="id410" class="indexterm"/>generating gain charts or lift curves <a id="id411" class="indexterm"/>has a similar process of running unseen validation or test data through the models and getting the predictions along with the confidences or probabilities. It involves ranking the probabilities in decreasing order and keeping count of TPs per quartile of the dataset. Finally, the histogram of counts per quartile give the lift curve, while the cumulative count of TPs added over quartile gives the gains chart. In many tools such as RapidMiner, instead of coarse intervals such as quartiles, fixed larger intervals using binning is employed for obtaining the counts and cumulative counts.</p></div></div><div class="section" title="Model comparisons"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Model comparisons</h2></div></div></div><p>When it <a id="id412" class="indexterm"/>comes to choosing between algorithms, or the right parameters for a given algorithm, we make the comparison either on different datasets, or, as in the case of cross-validation, on different splits of the same dataset. Measures of statistical testing are employed in decisions involved in these comparisons. The basic idea of using hypothesis testing from classical statistics is to compare the two metrics from the algorithms. The null hypothesis is that there is no difference between the algorithms based on the measured metrics and so the test is done to validate or reject the null hypothesis based on the measured metrics (<span class="emphasis"><em>References</em></span> [16]). The main question answered by statistical tests is- are the results or metrics obtained by the algorithm its real characteristics, or is it by chance?</p><p>In this section, we will discuss the most common methods for comparing classification algorithms used in practical scenarios.</p><div class="section" title="Comparing two algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec21"/>Comparing two algorithms</h3></div></div></div><p>The general <a id="id413" class="indexterm"/>process is to train the algorithms on the same training set and run the models on either multiple validation sets, different test sets, or cross-validation, gauge the metrics of interest discussed previously, such as error rate or area under curve, and then get the statistics of the metrics for each of the algorithms to decide which worked better. Each method has its advantages and disadvantages.</p><div class="section" title="McNemar's Test"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec30"/>McNemar's Test</h4></div></div></div><p>This <a id="id414" class="indexterm"/>is a non-parametric test and thus it makes no <a id="id415" class="indexterm"/>assumptions on data and distribution. McNemar's test builds a contingency table of a performance metric such as "misclassification or errors" with:</p><p>Count of misclassification by both algorithms (<span class="emphasis"><em>c</em></span><sub>00</sub>)</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Count of misclassification by algorithm <span class="emphasis"><em>G1</em></span>, but correctly classified by algorithm <span class="emphasis"><em>G2</em></span>(<span class="emphasis"><em>c</em></span><sub>01</sub>)</li><li class="listitem" style="list-style-type: disc">Count of misclassification by algorithm <span class="emphasis"><em>G2</em></span>, but correctly classified by algorithm <span class="emphasis"><em>G1</em></span> (<span class="emphasis"><em>c</em></span><sub>10</sub>)</li><li class="listitem" style="list-style-type: disc">Count of correctly classified by both <span class="emphasis"><em>G1</em></span> and <span class="emphasis"><em>G2</em></span>(<span class="emphasis"><em>c</em></span><sub>11</sub>)<div class="mediaobject"><img src="graphics/B05137_02_162.jpg" alt="McNemar's Test"/></div></li></ul></div><p>If χ<sup>2</sup> exceeds <span class="inlinemediaobject"><img src="graphics/B05137_02_164.jpg" alt="McNemar's Test"/></span> statistic then we can reject the null hypothesis that the two performance metrics on algorithms <span class="emphasis"><em>G1</em></span> and <span class="emphasis"><em>G2</em></span> were equal under the confidence value of  1 – α.</p><div class="section" title="Paired-t test"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec06"/>Paired-t test</h5></div></div></div><p>This is a <a id="id416" class="indexterm"/>parametric test and an assumption of normally distributed computed metrics becomes valid. Normally it is coupled with cross-validation processes and results of metrics such as area under curve or precision or error rate is computed for each and then the mean and standard deviations are measured. Apart from normal distribution assumption, the additional assumption that two metrics come from a population of equal variance can be a big disadvantage for this method.</p><div class="mediaobject"><img src="graphics/B05137_02_166.jpg" alt="Paired-t test"/></div><p><span class="inlinemediaobject"><img src="graphics/B05137_02_167.jpg" alt="Paired-t test"/></span>
 is difference of means in performance metrics of two algorithms <span class="emphasis"><em>G1</em></span> and <span class="emphasis"><em>G2</em></span>.</p><div class="mediaobject"><img src="graphics/B05137_02_168.jpg" alt="Paired-t test"/></div><p>Here, <span class="emphasis"><em>d</em></span>i is the difference between the performance metrics of two algorithms <span class="emphasis"><em>G1</em></span> and <span class="emphasis"><em>G2</em></span> in the trial and there are <span class="emphasis"><em>n</em></span> trials.</p><p>The <span class="emphasis"><em>t</em></span>-statistic <a id="id417" class="indexterm"/>is computed using the mean differences and the standard errors from the standard deviation as follows and is compared to the table for the right alpha value to check for significance:</p><div class="mediaobject"><img src="graphics/B05137_02_170.jpg" alt="Paired-t test"/></div></div></div><div class="section" title="Wilcoxon signed-rank test"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec31"/>Wilcoxon signed-rank test</h4></div></div></div><p>The most <a id="id418" class="indexterm"/>popular non-parametric method <a id="id419" class="indexterm"/>of testing two metrics over datasets is to use the Wilcoxon signed-rank test. The algorithms are trained on the same training data and metrics such as error rate or area under accuracy are calculated over different validation or test sets. Let <span class="emphasis"><em>d</em></span><sub>i</sub> be the difference between the performance metrics of two classifiers in the <span class="emphasis"><em>i</em></span><sup>th</sup> trial for <span class="emphasis"><em>N</em></span> datasets. Differences are then ranked according to their absolute values, and mean ranks associated for ties. Let <span class="emphasis"><em>R</em></span><sup>+</sup> be the sum of ranks where the second algorithm outperformed the first and R<sup>–</sup> be the sum of ranks where the first outperformed the second:</p><div class="mediaobject"><img src="graphics/B05137_02_174.jpg" alt="Wilcoxon signed-rank test"/></div><div class="mediaobject"><img src="graphics/B05137_02_175.jpg" alt="Wilcoxon signed-rank test"/></div><p>The statistic <span class="inlinemediaobject"><img src="graphics/B05137_02_176.jpg" alt="Wilcoxon signed-rank test"/></span> is then compared to threshold value at an alpha, <span class="inlinemediaobject"><img src="graphics/B05137_02_177.jpg" alt="Wilcoxon signed-rank test"/></span> to reject the hypothesis.</p></div></div><div class="section" title="Comparing multiple algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec22"/>Comparing multiple algorithms</h3></div></div></div><p>We will <a id="id420" class="indexterm"/>now discuss the two most common techniques used when there are more than two algorithms involved and we need to perform comparison across many algorithms for evaluation metrics.</p><div class="section" title="ANOVA test"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec32"/>ANOVA test</h4></div></div></div><p>These <a id="id421" class="indexterm"/>are parametric tests that assume normal <a id="id422" class="indexterm"/>distribution of the samples, that is, metrics we are calculating for evaluations. ANOVA test follows the same process as others, that is, train the models/algorithms on similar training sets and run it on different validation or test sets. The main quantities computed in ANOVA are the metric means for each algorithm performance and then compute the overall metric means across all algorithms.</p><p>Let <span class="emphasis"><em>p</em></span><sub>ij</sub> be the performance metric for <span class="emphasis"><em>i = 1,2… k</em></span> and <span class="emphasis"><em>j = 1,2 …l</em></span> for <span class="emphasis"><em>k</em></span> trials and <span class="emphasis"><em>l</em></span> classifiers. The mean performance of classifier <span class="emphasis"><em>j</em></span> on all trials and overall mean performance is:</p><div class="mediaobject"><img src="graphics/B05137_02_182.jpg" alt="ANOVA test"/></div><p>         </p><div class="mediaobject"><img src="graphics/B05137_02_183.jpg" alt="ANOVA test"/></div><p>
</p><p>Two types of variation are evaluated. The first is within-group variation, that is, total deviation of each algorithm from the overall metric mean, and the second is between-group variation, that is, deviation of each algorithm metric mean. Within-group variation and between-group variation are used to compute the respective within- and between- sum of squares as:</p><div class="mediaobject"><img src="graphics/B05137_02_184.jpg" alt="ANOVA test"/></div><p>Using the two sum of squares and a computation such as F-statistic, which is the ratio of the two, the significance test can be done at alpha values to accept or reject the null hypothesis:</p><div class="mediaobject"><img src="graphics/B05137_02_185.jpg" alt="ANOVA test"/></div><p>ANOVA tests have the same limitations as paired-t tests on the lines of assumptions of normal distribution of metrics and assuming the variances being equal.</p></div><div class="section" title="Friedman's test"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec33"/>Friedman's test</h4></div></div></div><p>Friedman's <a id="id423" class="indexterm"/>test is a non-parametric test <a id="id424" class="indexterm"/>for multiple algorithm comparisons and it has no assumption on the data distribution or variances of metrics that ANOVA does. It uses ranks instead <a id="id425" class="indexterm"/>of the performance metrics directly <a id="id426" class="indexterm"/>for its computation. On each dataset or trials, the algorithms are sorted and the best one is ranked 1 and so on for all classifiers. The average rank of an algorithm over <span class="emphasis"><em>n</em></span> datasets is computed, say <span class="emphasis"><em>R</em></span><sub>j</sub>. The Friedman's statistic over <span class="emphasis"><em>l</em></span> classifiers is computed as follows and compared to alpha values to accept or reject the null hypothesis:</p><div class="mediaobject"><img src="graphics/B05137_02_187.jpg" alt="Friedman's test"/></div></div></div></div></div>
<div class="section" title="Case Study &#x2013; Horse Colic Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Case Study – Horse Colic Classification</h1></div></div></div><p>To illustrate <a id="id427" class="indexterm"/>the different steps and methodologies described in <a class="link" href="ch01.html" title="Chapter 1. Machine Learning Review">Chapter 1</a>, <span class="emphasis"><em>Machine Learning Review</em></span>, from data analysis to model evaluation, a representative dataset that has real-world characteristics is essential.</p><p>We have chosen "Horse Colic Dataset" from the UCI Repository available at the following link: <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Horse+Colic">https://archive.ics.uci.edu/ml/datasets/Horse+Colic</a>
</p><p>The <a id="id428" class="indexterm"/>dataset has 23 features and has a good mix of categorical and continuous features. It has a large number of features and instances with missing values, hence understanding how to replace these missing values and using it in modeling is made more practical in this treatment. The large number of missing data (30%) is in fact a notable feature of this dataset. The data consists of attributes that are continuous, as well as nominal in type. Also, the presence of self-predictors makes working with this dataset instructive from a practical standpoint.</p><p>The goal of the exercise is to apply the techniques of supervised learning that we have assimilated so far. We will do this using a real dataset and by working with two open source toolkits—WEKA and RapidMiner. With the help of these tools, we will construct the pipeline that will allow us to start with the ingestion of the data file through data cleansing, the learning process, and model evaluation.</p><p>Weka is a Java framework <a id="id429" class="indexterm"/>for machine learning—we will see how to use this framework to solve a classification problem from beginning to end in a few lines of code. In addition to a Java API, Weka also has a GUI.</p><p>RapidMiner is a <a id="id430" class="indexterm"/>graphical environment with drag and drop capability and a large suite of algorithms and visualization tools that makes it extremely easy to quickly run experiments <a id="id431" class="indexterm"/>with data and different modeling techniques.</p><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Business problem</h2></div></div></div><p>The business <a id="id432" class="indexterm"/>problem is to determine given values <a id="id433" class="indexterm"/>for the well-known variables of the dataset—if the lesion of the horse was surgical. We will use the test set as the unseen data that must be classified.</p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Machine learning mapping</h2></div></div></div><p> Based <a id="id434" class="indexterm"/>on the data and <a id="id435" class="indexterm"/>labels, this is a binary classification problem. The data is already split into training and testing data. This makes the evaluation technique simpler as all methodologies from feature selection to models can be evaluated on the same test data.</p><p>The dataset contains 300 training and 68 test examples. There are 28 attributes and the target corresponds to whether or not a lesion is surgical.</p></div><div class="section" title="Data analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Data analysis</h2></div></div></div><p>After <a id="id436" class="indexterm"/>looking at the distribution <a id="id437" class="indexterm"/>of the label categories over the training and test samples, we combine the 300 training samples and the 68 test samples prior to feature analyzes.</p><div class="section" title="Label analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec23"/>Label analysis</h3></div></div></div><p>The ratio <a id="id438" class="indexterm"/>of the No Class to Yes Class is 109/191 = 0.57 in the Training set and 0.66 in the Test set:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th colspan="3" style="text-align: center" valign="bottom">
<p>Training dataset</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Surgical Lesion?</p>
</td><td style="text-align: left" valign="top">
<p>1 (Yes)</p>
</td><td style="text-align: left" valign="top">
<p>2 (No)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of examples</p>
</td><td style="text-align: left" valign="top">
<p>191</p>
</td><td style="text-align: left" valign="top">
<p>109</p>
</td></tr><tr><td colspan="3" style="text-align: center" valign="top">
<p>Testing dataset</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Surgical Lesion?</p>
</td><td style="text-align: left" valign="top">
<p>1 (Yes)</p>
</td><td style="text-align: left" valign="top">
<p>2 (No)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Number of examples</p>
</td><td style="text-align: left" valign="top">
<p>41</p>
</td><td style="text-align: left" valign="top">
<p>27</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 2: Label analysis </em></span></p></blockquote></div><div class="section" title="Features analysis"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl3sec24"/>Features analysis</h4></div></div></div><p>The <a id="id439" class="indexterm"/>following is a screenshot of top features with characteristics of types, missing values, basic statistics of minimum, maximum, modes, and standard deviations sorted by missing values. Observations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There are no categorical or continuous features with non-missing values; the least is the feature "pulse" with 74 out of 368 missing, that is, 20% values missing, which is higher than general noise threshold!</li><li class="listitem" style="list-style-type: disc">Most numeric features have missing values, for example, "nasogastric reflux PH" has 247 out of 368 values missing, that is, 67% values are missing!</li><li class="listitem" style="list-style-type: disc">Many categorical features have missing values, for example, "abidominocentesis appearance" have 165 out of 368 missing, that is, 45% values are missing!</li><li class="listitem" style="list-style-type: disc">Missing values have to be handled in some way to overcome the noise created by such large numbers!<div class="mediaobject"><img src="graphics/B05137_02_188.jpg" alt="Features analysis"/><div class="caption"><p>Figure 12: Basic statistics of features from datasets.</p></div></div></li></ul></div></div></div></div><div class="section" title="Supervised learning experiments"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec28"/>Supervised learning experiments</h2></div></div></div><p>In <a id="id440" class="indexterm"/>this section, we <a id="id441" class="indexterm"/>will cover supervised learning experiments using two different tools—highlighting coding and analysis in one tool and the GUI framework in the other. This gives the developers the opportunity to explore whichever route they are most comfortable with.</p><div class="section" title="Weka experiments"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec25"/>Weka experiments</h3></div></div></div><p>In <a id="id442" class="indexterm"/>this section, we have given <a id="id443" class="indexterm"/>the entire code and will walk through the process from loading data, transforming the data, selecting features, building sample models, evaluating them on test data, and even comparing the algorithms for statistical significance.</p><div class="section" title="Sample end-to-end process in Java"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec34"/>Sample end-to-end process in Java</h4></div></div></div><p>In <a id="id444" class="indexterm"/>each algorithm, the same training/testing data is used and evaluation is performed for all the metrics as follows. The training and testing file is loaded in memory as follows:</p><div class="informalexample"><pre class="programlisting">DataSource source = new DataSource(trainingFile);
Instances data = source.getDataSet();
if (data.classIndex() == -1)
  data.setClassIndex(data.numAttributes() - 1);</pre></div><p>The generic code, using WEKA, is shown here, where each classifier is wrapped by a filtered classifier for replacing missing values:</p><div class="informalexample"><pre class="programlisting">//replacing the nominal and numeric with modes and means
Filter missingValuesFilter= new ReplaceMissingValues();
//create a filtered classifier to use filter and classifier
FilteredClassifier filteredClassifier = new FilteredClassifier();
filteredClassifier.setFilter(f);
// create a bayesian classifier
NaiveBayes naiveBayes = new NaiveBayes();
// use supervised discretization
naiveBayes.setUseSupervisedDiscretization(true);
//set the base classifier e.g naïvebayes, linear //regression etc.
fc.setClassifier(filteredClassifier)</pre></div><p>When the classifier needs to perform Feature Selection, in Weka, <code class="literal">AttributeSelectedClassifier</code> further wraps the <code class="literal">FilteredClassifier</code> as shown in the following listing:</p><div class="informalexample"><pre class="programlisting">AttributeSelectedClassifier attributeSelectionClassifier = new AttributeSelectedClassifier();
//wrap the classifier
attributeSelectionClassifier.setClassifier(filteredClassifier);
//univariate information gain based feature evaluation
    InfoGainAttributeEval evaluator = new InfoGainAttributeEval();
//rank the features
Ranker ranker = new Ranker();
//set the threshold to be 0, less than that is rejected
ranker.setThreshold(0.0);
attributeSelectionClassifier.setEvaluator(evaluator);
attributeSelectionClassifier.setSearch(ranker);
//build on training data
attributeSelectionClassifier.buildClassifier(trainingData);
// evaluate classifier giving same training data
Evaluation eval = new Evaluation(trainingData);
//evaluate the model on test data
eval.evaluateModel(attributeSelectionClassifier,testingData);</pre></div><p>The <a id="id445" class="indexterm"/>sample output of evaluation is given here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>=== Summary ===</strong></span>

<span class="strong"><strong>Correctly Classified Instances     53       77.9412 %</strong></span>
<span class="strong"><strong>Incorrectly Classified Instances    15       22.0588 %</strong></span>
<span class="strong"><strong>Kappa statistic             0.5115</strong></span>
<span class="strong"><strong>Mean absolute error           0.3422</strong></span>
<span class="strong"><strong>Root mean squared error         0.413</strong></span>
<span class="strong"><strong>Relative absolute error        72.4875 %</strong></span>
<span class="strong"><strong>Root relative squared error      84.2167 %</strong></span>
<span class="strong"><strong>Total Number of Instances       68 </strong></span>

<span class="strong"><strong>=== Detailed Accuracy By Class ===</strong></span>

<span class="strong"><strong>        TP Rate FP Rate Precision Recall F-Measure MCC   ROC Area PRC Area Class</strong></span>
<span class="strong"><strong>        0.927  0.444  0.760   0.927  0.835   0.535  0.823  0.875  1</strong></span>
<span class="strong"><strong>        0.556  0.073  0.833   0.556  0.667   0.535  0.823  0.714  2</strong></span>
<span class="strong"><strong>Weighted Avg.  0.779  0.297  0.789   0.779  0.768   0.535  0.823  0.812 </strong></span>

<span class="strong"><strong>=== Confusion Matrix ===</strong></span>

<span class="strong"><strong> a b &lt;-- classified as</strong></span>
<span class="strong"><strong> 38 3 | a = 1</strong></span>
<span class="strong"><strong> 12 15 | b = 2</strong></span>
</pre></div></div><div class="section" title="Weka experimenter and model selection"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec35"/>Weka experimenter and model selection</h4></div></div></div><p>As explained in the <span class="emphasis"><em>Model evaluation metrics</em></span> section, to select models, we need to validate <a id="id446" class="indexterm"/>which one will work well on unseen datasets. Cross-validation <a id="id447" class="indexterm"/>must be done on the training set and the performance metric of choice needs to be analyzed using standard statistical testing metrics. Here we show an example using the same training data, 10-fold cross validation, performing 30 experiments on two models, and comparison of results using paired-t tests.</p><p>One uses Naïve Bayes with preprocessing that includes replacing missing values and performing feature selection by removing any features with a score below 0.0.</p><p>Another uses the same preprocessing and AdaBoostM1 with Naïve Bayes.</p><div class="mediaobject"><img src="graphics/B05137_02_189.jpg" alt="Weka experimenter and model selection"/><div class="caption"><p>Figure 13: WEKA experimenter showing the process of using cross-validation runs with 30 repetitions with two algorithms.</p></div></div><div class="mediaobject"><img src="graphics/B05137_02_190.jpg" alt="Weka experimenter and model selection"/><div class="caption"><p>Figure 14: WEKA Experimenter results showing two algorithms compared on metric of percent correct or accuracy using paired-t test.</p></div></div></div></div><div class="section" title="RapidMiner experiments"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec26"/>RapidMiner experiments</h3></div></div></div><p>Let's now <a id="id448" class="indexterm"/>run some experiments using <a id="id449" class="indexterm"/>the Horse-colic dataset in RapidMiner. We will again follow the methodology presented in the first part of the chapter.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>This section is not intended as a tutorial on the RapidMiner tool. The experimenter is expected to read the excellent documentation and user guide to familiarize themselves with the use of the tool. There is a tutorial dedicated to every operator in the software—we recommend you make use of these tutorials whenever you want to learn how a particular operator is to be used.</p></div></div><p>Once we have imported the test and training data files using the data access tools, we will want to visually explore the dataset to familiarize ourselves with the lay of the land. Of particular importance is to recognize whether each of the 28 attributes are continuous (numeric, integer, or real in RapidMiner) or categorical (nominal, binominal, or polynominal <a id="id450" class="indexterm"/>in RapidMiner).</p><div class="section" title="Visualization analysis"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec36"/>Visualization analysis</h4></div></div></div><p>From the <span class="strong"><strong>Results</strong></span> panel of the tool, we perform univariate, bivariate, and multivariate analyses <a id="id451" class="indexterm"/>of the data. The Statistics tool gives a short summary for each feature—min, max, mean, and standard deviation for continuous types and least, most, and frequency by category for nominal types.</p><p>Interesting characteristics of the data begin to show themselves as we get into bivariate analysis. In the Quartile Color Matrix, the color represents the two possible target values. As seen in the box plots, we immediately notice some attributes discriminate between the two target values more clearly than others. Let's examine a few:</p><div class="mediaobject"><img src="graphics/B05137_02_191.jpg" alt="Visualization analysis"/><div class="caption"><p>Figure 15: Quartile Color Matrix</p></div></div><p>Peristalsis: This feature shows a <a id="id452" class="indexterm"/>marked difference in distribution when separated by target value. There is almost no overlap in the inter-quartile regions between the two. This points to the discriminating power of this feature with respect to the target.</p><p>The plot for <a id="id453" class="indexterm"/>Rectal Temperature, on the other hand, shows no perceptible difference in the distributions. This suggests that this feature has low correlation with the target. A similar inference may be drawn from the feature Pulse. We expect these features to rank fairly low when we evaluate the features for their discriminating power with respect to the target.</p><p>Lastly, the plot for Pain has a very different characteristic. It is also discriminating of the target, but in a very different way than Peristalsis. In the case of Pain, the variance in data for Class 2 is much larger than Class 1. Abdominal Distension also has markedly dissimilar variance across the classes, except with the larger variance in Class 2 compared to Class 1.</p><div class="mediaobject"><img src="graphics/B05137_02_192.jpg" alt="Visualization analysis"/><div class="caption"><p>Figure 16: Scatter plot matrix</p></div></div><p>An important part of exploring the data is understanding how different attributes correlate with each other and with the target. Here we consider pairs of features and see if the occurrence of values <span class="emphasis"><em>in combination</em></span> tells us something about the target. In these plots, the color of the data points is the target.</p><div class="mediaobject"><img src="graphics/B05137_02_193.jpg" alt="Visualization analysis"/><div class="caption"><p>Figure 17: Bubble chart</p></div></div><p>In the bubble chart we can visualize four features at once by using the graphing tools to specify the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes as well as a third dimension expressed as the size of bubble representing the feature. The target class is denoted by the color.</p><p>At the low end of total protein, we see higher pH values in the mid-range of rectal temperature values. In this cluster, high pH values appear to show a stronger correlation to lesions that were surgical. Another cluster with wider variance in total protein is also found for values of total protein greater than 50. The variance in pH is also low in this cluster.</p></div><div class="section" title="Feature selection"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec37"/>Feature selection</h4></div></div></div><p>Having <a id="id454" class="indexterm"/>gained some insight into the data, we are ready to use some of the techniques presented in the theory that evaluate feature relevance.</p><p>Here we use two techniques: one that calculates the weights for features based on Chi-squared statistics with respect to the target attribute and the other based on the Gini Impurity Index. The results are shown in the table. Note that as we inferred while doing analysis of the features via visualization, both Pulse and Rectal Temperature prove to have low relevance as shown by both techniques.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th colspan="2" style="text-align: center" valign="bottom">
<p>Chi-squared</p>
</th><th colspan="2" style="text-align: center" valign="bottom">
<p>Gini index</p>
</th></tr><tr><th style="text-align: left" valign="bottom">
<p>Attribute</p>
</th><th style="text-align: left" valign="bottom">
<p>Weight</p>
</th><th style="text-align: left" valign="bottom">
<p>Attribute</p>
</th><th style="text-align: left" valign="bottom">
<p>Weight</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Pain</p>
</td><td style="text-align: left" valign="top">
<p>54.20626</p>
</td><td style="text-align: left" valign="top">
<p>Pain</p>
</td><td style="text-align: left" valign="top">
<p>0.083594</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Abdomen</p>
</td><td style="text-align: left" valign="top">
<p>53.93882</p>
</td><td style="text-align: left" valign="top">
<p>Abdomen</p>
</td><td style="text-align: left" valign="top">
<p>0.083182</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Peristalsis</p>
</td><td style="text-align: left" valign="top">
<p>38.73474</p>
</td><td style="text-align: left" valign="top">
<p>Peristalsis</p>
</td><td style="text-align: left" valign="top">
<p>0.059735</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AbdominalDistension</p>
</td><td style="text-align: left" valign="top">
<p>35.11441</p>
</td><td style="text-align: left" valign="top">
<p>AbdominalDistension</p>
</td><td style="text-align: left" valign="top">
<p>0.054152</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PeripheralPulse</p>
</td><td style="text-align: left" valign="top">
<p>23.65301</p>
</td><td style="text-align: left" valign="top">
<p>PeripheralPulse</p>
</td><td style="text-align: left" valign="top">
<p>0.036476</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AbdominocentesisAppearance</p>
</td><td style="text-align: left" valign="top">
<p>20.00392</p>
</td><td style="text-align: left" valign="top">
<p>AbdominocentesisAppearance</p>
</td><td style="text-align: left" valign="top">
<p>0.030849</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>TemperatureOfExtremeties</p>
</td><td style="text-align: left" valign="top">
<p>17.07852</p>
</td><td style="text-align: left" valign="top">
<p>TemperatureOfExtremeties</p>
</td><td style="text-align: left" valign="top">
<p>0.026338</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>MucousMembranes</p>
</td><td style="text-align: left" valign="top">
<p>15.0938</p>
</td><td style="text-align: left" valign="top">
<p>MucousMembranes</p>
</td><td style="text-align: left" valign="top">
<p>0.023277</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NasogastricReflux</p>
</td><td style="text-align: left" valign="top">
<p>14.95926</p>
</td><td style="text-align: left" valign="top">
<p>NasogastricReflux</p>
</td><td style="text-align: left" valign="top">
<p>0.023069</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>PackedCellVolume</p>
</td><td style="text-align: left" valign="top">
<p>13.5733</p>
</td><td style="text-align: left" valign="top">
<p>PackedCellVolume</p>
</td><td style="text-align: left" valign="top">
<p>0.020932</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>RectalExamination-Feces</p>
</td><td style="text-align: left" valign="top">
<p>11.88078</p>
</td><td style="text-align: left" valign="top">
<p>RectalExamination-Feces</p>
</td><td style="text-align: left" valign="top">
<p>0.018322</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>CapillaryRefillTime</p>
</td><td style="text-align: left" valign="top">
<p>8.078319</p>
</td><td style="text-align: left" valign="top">
<p>CapillaryRefillTime</p>
</td><td style="text-align: left" valign="top">
<p>0.012458</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>RespiratoryRate</p>
</td><td style="text-align: left" valign="top">
<p>7.616813</p>
</td><td style="text-align: left" valign="top">
<p>RespiratoryRate</p>
</td><td style="text-align: left" valign="top">
<p>0.011746</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>TotalProtein</p>
</td><td style="text-align: left" valign="top">
<p>5.616841</p>
</td><td style="text-align: left" valign="top">
<p>TotalProtein</p>
</td><td style="text-align: left" valign="top">
<p>0.008662</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NasogastricRefluxPH</p>
</td><td style="text-align: left" valign="top">
<p>2.047565</p>
</td><td style="text-align: left" valign="top">
<p>NasogastricRefluxPH</p>
</td><td style="text-align: left" valign="top">
<p>0.003158</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Pulse</p>
</td><td style="text-align: left" valign="top">
<p>1.931511</p>
</td><td style="text-align: left" valign="top">
<p>Pulse</p>
</td><td style="text-align: left" valign="top">
<p>0.002979</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Age</p>
</td><td style="text-align: left" valign="top">
<p>0.579216</p>
</td><td style="text-align: left" valign="top">
<p>Age</p>
</td><td style="text-align: left" valign="top">
<p>8.93E-04</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NasogastricTube</p>
</td><td style="text-align: left" valign="top">
<p>0.237519</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>AbdomcentecisTotalProtein</p>
</td><td style="text-align: left" valign="top">
<p>0.181868</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>RectalTemperature</p>
</td><td style="text-align: left" valign="top">
<p>0.139387</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 3: Relevant features determined by two different techniques, Chi-squared and Gini index. </em></span></p></blockquote></div></div><div class="section" title="Model process flow"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec38"/>Model process flow</h4></div></div></div><p>In RapidMiner <a id="id455" class="indexterm"/>you can define a pipeline of computations using operators with inputs and outputs that can be chained together. The following process represents the flow used to perform the entire set of operations starting with loading the training and test data, handling missing values, weighting features by relevance, filtering out low scoring features, training an ensemble model that uses Bagging with Random Forest as the algorithm, and finally applying the learned model to the test data and outputting the performance metrics. Note that all the preprocessing steps that are applied to the training dataset must also be applied, in the same order, to the test set by means of the Group Models operator:</p><div class="mediaobject"><img src="graphics/B05137_02_194.jpg" alt="Model process flow"/><div class="caption"><p>Figure 18: RapidMiner process diagram</p></div></div><p>Following the top of the process, the training set is ingested in the left-most operator, followed by the exclusion of non-predictors (Hospital Number, CP data) and self-predictors (Lesion 1). This is followed by the operator that replaces missing values with the mean and mode for continuous and categorical attributes, respectively. Next, the Feature <a id="id456" class="indexterm"/>Weights operator evaluates weights for each feature based on the Chi-squared statistic, which is followed by a filter that ignores low-weighted features. This pre-processed dataset is then used to train a model using Bagging with a Random Forest classifier.</p><p>The preprocessing steps used on the training data are grouped together in the appropriate order via the Group Models operator and applied to the test data in the penultimate step. Finally, the predictions of the target variable on the test examples accompanied by the confusion matrix and other performance metrics are made evaluated and presented in the last step.</p></div><div class="section" title="Model evaluation metrics"><div class="titlepage"><div><div><h4 class="title"><a id="ch02lvl4sec39"/>Model evaluation metrics</h4></div></div></div><p>We are <a id="id457" class="indexterm"/>now ready to compare the results from the <a id="id458" class="indexterm"/>various models. If you have followed along you may find that your results vary from what's presented here—that may be due to the stochastic nature of some learning algorithms, or differences in the values of some hyper-parameters used in the models.</p><p>We have considered three different training datasets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Original training data with missing values</li><li class="listitem" style="list-style-type: disc">Training data transformed with missing values handled</li><li class="listitem" style="list-style-type: disc">Training data transformed with missing values handled and with feature selection (Chi-Square) applied to select features that are highly discriminatory.</li></ul></div><p>We have considered three different sets of algorithms on each of the datasets:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Linear algorithms (Naïve Bayes and Logistic Regression)</li><li class="listitem" style="list-style-type: disc">Non-linear algorithms (Decision Tree and KNN)</li><li class="listitem" style="list-style-type: disc">Ensemble algorithms (Bagging, Ada Boost, and Random Forest).</li></ul></div><div class="section" title="Evaluation on Confusion Metrics"><div class="titlepage"><div><div><h5 class="title"><a id="ch02lvl5sec07"/>Evaluation on Confusion Metrics</h5></div></div></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Models</p>
</th><th style="text-align: left" valign="bottom">
<p>TPR</p>
</th><th style="text-align: left" valign="bottom">
<p>FPR</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>AUC</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>68.29%</p>
</td><td style="text-align: left" valign="top">
<p>14.81%</p>
</td><td style="text-align: left" valign="top">
<p>87.50%</p>
</td><td style="text-align: left" valign="top">
<p>85.19%</p>
</td><td style="text-align: left" valign="top">
<p>75.00%</p>
</td><td style="text-align: left" valign="top">
<p>0.836</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Logistic <a id="id459" class="indexterm"/>Regression</p>
</td><td style="text-align: left" valign="top">
<p>78.05%</p>
</td><td style="text-align: left" valign="top">
<p>14.81%</p>
</td><td style="text-align: left" valign="top">
<p>88.89%</p>
</td><td style="text-align: left" valign="top">
<p>85.19%</p>
</td><td style="text-align: left" valign="top">
<p>80.88%</p>
</td><td style="text-align: left" valign="top">
<p>0.856</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree</p>
</td><td style="text-align: left" valign="top">
<p>68.29%</p>
</td><td style="text-align: left" valign="top">
<p>33.33%</p>
</td><td style="text-align: left" valign="top">
<p>75.68%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>67.65%</p>
</td><td style="text-align: left" valign="top">
<p>0.696</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>k-NN</p>
</td><td style="text-align: left" valign="top">
<p>90.24%</p>
</td><td style="text-align: left" valign="top">
<p>85.19%</p>
</td><td style="text-align: left" valign="top">
<p>61.67%</p>
</td><td style="text-align: left" valign="top">
<p>14.81%</p>
</td><td style="text-align: left" valign="top">
<p>60.29%</p>
</td><td style="text-align: left" valign="top">
<p>0.556</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bagging (GBT)</p>
</td><td style="text-align: left" valign="top">
<p>90.24%</p>
</td><td style="text-align: left" valign="top">
<p>74.07%</p>
</td><td style="text-align: left" valign="top">
<p>64.91%</p>
</td><td style="text-align: left" valign="top">
<p>25.93%</p>
</td><td style="text-align: left" valign="top">
<p>64.71%</p>
</td><td style="text-align: left" valign="top">
<p>0.737</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ada Boost (Naïve Bayes)</p>
</td><td style="text-align: left" valign="top">
<p>63.41%</p>
</td><td style="text-align: left" valign="top">
<p>48.15%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>51.85%</p>
</td><td style="text-align: left" valign="top">
<p>58.82%</p>
</td><td style="text-align: left" valign="top">
<p>0.613</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 4: Results on unseen (Test) data for models trained on Horse-colic data with missing values </em></span></p></blockquote></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Models</p>
</th><th style="text-align: left" valign="bottom">
<p>TPR</p>
</th><th style="text-align: left" valign="bottom">
<p>FPR</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>AUC</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>68.29%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>60.87%</p>
</td><td style="text-align: left" valign="top">
<p>33.33%</p>
</td><td style="text-align: left" valign="top">
<p>54.41%</p>
</td><td style="text-align: left" valign="top">
<p>0.559</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Logistic Regression</p>
</td><td style="text-align: left" valign="top">
<p>78.05%</p>
</td><td style="text-align: left" valign="top">
<p>62.96%</p>
</td><td style="text-align: left" valign="top">
<p>65.31%</p>
</td><td style="text-align: left" valign="top">
<p>37.04%</p>
</td><td style="text-align: left" valign="top">
<p>61.76%</p>
</td><td style="text-align: left" valign="top">
<p>0.689</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree</p>
</td><td style="text-align: left" valign="top">
<p>97.56%</p>
</td><td style="text-align: left" valign="top">
<p>96.30%</p>
</td><td style="text-align: left" valign="top">
<p>60.61%</p>
</td><td style="text-align: left" valign="top">
<p>3.70%</p>
</td><td style="text-align: left" valign="top">
<p>60.29%</p>
</td><td style="text-align: left" valign="top">
<p>0.812</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>k-NN</p>
</td><td style="text-align: left" valign="top">
<p>75.61%</p>
</td><td style="text-align: left" valign="top">
<p>48.15%</p>
</td><td style="text-align: left" valign="top">
<p>70.45%</p>
</td><td style="text-align: left" valign="top">
<p>51.85%</p>
</td><td style="text-align: left" valign="top">
<p>66.18%</p>
</td><td style="text-align: left" valign="top">
<p>0.648</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bagging (Random Forest)</p>
</td><td style="text-align: left" valign="top">
<p>97.56%</p>
</td><td style="text-align: left" valign="top">
<p>74.07%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>25.93%</p>
</td><td style="text-align: left" valign="top">
<p>69.12%</p>
</td><td style="text-align: left" valign="top">
<p>0.892</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bagging (GBT)</p>
</td><td style="text-align: left" valign="top">
<p>82.93%</p>
</td><td style="text-align: left" valign="top">
<p>18.52%</p>
</td><td style="text-align: left" valign="top">
<p>87.18%</p>
</td><td style="text-align: left" valign="top">
<p>81.48%</p>
</td><td style="text-align: left" valign="top">
<p>82.35%</p>
</td><td style="text-align: left" valign="top">
<p>0.870</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ada Boost (Naïve Bayes)</p>
</td><td style="text-align: left" valign="top">
<p>68.29%</p>
</td><td style="text-align: left" valign="top">
<p>7.41%</p>
</td><td style="text-align: left" valign="top">
<p>93.33%</p>
</td><td style="text-align: left" valign="top">
<p>92.59%</p>
</td><td style="text-align: left" valign="top">
<p>77.94%</p>
</td><td style="text-align: left" valign="top">
<p>0.895</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 5: Results on unseen (Test) data for models trained on Horse-colic data with missing values replaced </em></span></p></blockquote></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Models</p>
</th><th style="text-align: left" valign="bottom">
<p>TPR</p>
</th><th style="text-align: left" valign="bottom">
<p>FPR</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>AUC</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>75.61%</p>
</td><td style="text-align: left" valign="top">
<p>77.78%</p>
</td><td style="text-align: left" valign="top">
<p>59.62%</p>
</td><td style="text-align: left" valign="top">
<p>29.63%</p>
</td><td style="text-align: left" valign="top">
<p>54.41%</p>
</td><td style="text-align: left" valign="top">
<p>0.551</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Logistic Regression</p>
</td><td style="text-align: left" valign="top">
<p>82.93%</p>
</td><td style="text-align: left" valign="top">
<p>62.96%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>37.04%</p>
</td><td style="text-align: left" valign="top">
<p>64.71%</p>
</td><td style="text-align: left" valign="top">
<p>0.692</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree </p>
</td><td style="text-align: left" valign="top">
<p>95.12%</p>
</td><td style="text-align: left" valign="top">
<p>92.59%</p>
</td><td style="text-align: left" valign="top">
<p>60.94%</p>
</td><td style="text-align: left" valign="top">
<p>7.41%</p>
</td><td style="text-align: left" valign="top">
<p>60.29%</p>
</td><td style="text-align: left" valign="top">
<p>0.824</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>k-NN</p>
</td><td style="text-align: left" valign="top">
<p>75.61%</p>
</td><td style="text-align: left" valign="top">
<p>48.15%</p>
</td><td style="text-align: left" valign="top">
<p>70.45%</p>
</td><td style="text-align: left" valign="top">
<p>51.85%</p>
</td><td style="text-align: left" valign="top">
<p>66.18%</p>
</td><td style="text-align: left" valign="top">
<p>0.669</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bagging (Random Forest)</p>
</td><td style="text-align: left" valign="top">
<p>92.68%</p>
</td><td style="text-align: left" valign="top">
<p>33.33%</p>
</td><td style="text-align: left" valign="top">
<p>80.85%</p>
</td><td style="text-align: left" valign="top">
<p>66.67%</p>
</td><td style="text-align: left" valign="top">
<p>82.35%</p>
</td><td style="text-align: left" valign="top">
<p>0.915</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bagging (GBT)</p>
</td><td style="text-align: left" valign="top">
<p>78.05%</p>
</td><td style="text-align: left" valign="top">
<p>22.22%</p>
</td><td style="text-align: left" valign="top">
<p>84.21%</p>
</td><td style="text-align: left" valign="top">
<p>77.78%</p>
</td><td style="text-align: left" valign="top">
<p>77.94%</p>
</td><td style="text-align: left" valign="top">
<p>0.872</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ada Boost (Naïve Bayes)</p>
</td><td style="text-align: left" valign="top">
<p>68.29%</p>
</td><td style="text-align: left" valign="top">
<p>18.52%</p>
</td><td style="text-align: left" valign="top">
<p>84.85%</p>
</td><td style="text-align: left" valign="top">
<p>81.48%</p>
</td><td style="text-align: left" valign="top">
<p>73.53%</p>
</td><td style="text-align: left" valign="top">
<p>0.848</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 6: Results on unseen (Test) data for models trained on Horse-colic data using features selected by Chi-squared
statistic technique </em></span></p></blockquote></div><div class="section"><div class="titlepage"><div><div><h6 class="title"><a id="ch02lvl3sec27"/></h6></div></div></div><div class="section"><div class="titlepage"><div><div><h6 class="title"><a id="ch02lvl4sec40"/></h6></div></div></div><div class="section" title="ROC Curves, Lift Curves, and Gain Charts"><div class="titlepage"><div><div><h6 class="title"><a id="ch02lvl5sec08"/>ROC Curves, Lift Curves, and Gain Charts</h6></div></div></div><p>The <a id="id460" class="indexterm"/>performance plots enable us to visually assess the <a id="id461" class="indexterm"/>models used in two of the three experiments—without any replacement of missing data, and with using features from Chi-squared <a id="id462" class="indexterm"/>weighting after replacing missing data—and to compare them against each other. Pairs of plots display the performance curves of each Linear (Logistic Regression), Non-linear (Decision Tree), and Ensemble (Bagging, using Gradient Boosted Tree) technique we learned about earlier in the chapter, drawn from results of the two experiments.</p><div class="mediaobject"><img src="graphics/B05137_02_195.jpg" alt="ROC Curves, Lift Curves, and Gain Charts"/><div class="caption"><p>Figure 19: ROC Performance curves for experiment using Missing Data</p></div></div><div class="mediaobject"><img src="graphics/B05137_02_196.jpg" alt="ROC Curves, Lift Curves, and Gain Charts"/><div class="caption"><p>Figure 20: Cumulative Gains performance curves for experiment using Missing Data</p></div></div><div class="mediaobject"><img src="graphics/B05137_02_197.jpg" alt="ROC Curves, Lift Curves, and Gain Charts"/><div class="caption"><p>Figure 21: Lift performance curves for experiment using Missing Data</p></div></div></div></div></div></div></div></div></div><div class="section" title="Results, observations, and analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec29"/>Results, observations, and analysis</h2></div></div></div><p>The impact of handling missing values is significant. Of the seven classifiers, with the exception of  Naïve <a id="id463" class="indexterm"/>Bayes and Logistic Regression, all show remarkable improvement when missing values are handled as indicated <a id="id464" class="indexterm"/>by various metrics, including AUC, precision, accuracy, and specificity. This tells us that handling missing values that can be "noisy" is an important aspect of data transformation. Naive Bayes has its own internal way of managing missing values and the results from our experiments show that it does a better job of null-handling than our external transformations. But in general, the idea of transforming missing values seems beneficial when you consider all of the classifiers.</p><p>As discussed in the section on modeling, some of the algorithms require the right handling of missing values and feature selection to get optimum performance. From the results, we can see that the performance of Decision Trees, for example, improved incrementally from 0.696 with missing data, 0.812 with managed missing data, and for the best performance of 0.824 with missing data handled together with feature selection. Six out of seven classifiers improve the performance in AUC (and in others metrics) when both the steps are performed; comparing <span class="emphasis"><em>Table 5</em></span> and <span class="emphasis"><em>Table 6</em></span> for AUC gives us these quick insights. This demonstrates the importance of doing preprocessing such as missing value handling along with feature selection before performing modeling.</p><p>A major conclusion from the results is that the problem is highly non-linear and therefore most non-linear classifiers from the simplest Decision Trees to ensemble Random Forest perform very well. The best performance comes from the meta-learning algorithm Random Forest, with missing values properly handled and the most relevant features used in training. The best linear model performance measured by AUC was 0.856 for Logistic Regression with data as-is (that is, with missing values), whereas Random Forest achieved AUC performance of 0.915 with proper handling of missing data accompanied by feature selection. Generally, as evident from <span class="emphasis"><em>Table 3</em></span>, the non-linear classifiers or meta-learners performed better than linear classifiers by most performance measures.</p><p>Handling missing values, which can be thought as "noise", in the appropriate manner improves the performance of AdaBoost by a significant amount. The AUC improves from 0.613 to 0.895 and FPR reduces from 48.15 to 7.41%. This indeed conforms to the expected theoretical behavior for this technique.</p><p>Meta-learning techniques, which use concepts of boosting and bagging, are relatively more effective when dealing with real-world data, when compared to other common techniques. This seems to be justified by the results since AdaBoost with Naïve Bayes as base learner trained on data that has undergone proper handling of noise outperforms Naive Bayes in <a id="id465" class="indexterm"/>most of the metrics, as shown in <span class="emphasis"><em>Table 5</em></span> and <span class="emphasis"><em>Table 6</em></span>. Random Forest and GBTs also show the best performance along with AdaBoost as compared to base classifiers in <span class="emphasis"><em>Table 6</em></span>, again confirming that the right process and <a id="id466" class="indexterm"/>ensemble learning can produce the most optimum results in real-world noisy datasets.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>All data, models, and results for both WEKA and RapidMiner process files from this chapter are available at: <a class="ulink" href="https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2">https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter2</a>.</p></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Summary</h1></div></div></div><p>Supervised learning is the predominant technique used in machine learning applications. The methodology consists of a series of steps beginning with data exploration, data transformation, and data sampling, through feature reduction, model building, and ultimately, model assessment and comparison. Each step of the process involves some decision making which must answer key questions: How  should we impute missing values? What data sampling strategy should we use? What is the most appropriate algorithm given the amount of noise in the dataset and the prescribed goal of interpretability? This chapter demonstrated the application of these processes and techniques to a real-world problem—the classification problem using the UCI Horse Colic dataset.</p><p>Whether the problem is one of classification, when the target is a categorical value, or Regression, when it is a real-valued continuous variable, the methodology used for supervised learning is similar. In this chapter, we have used classification for illustration.</p><p>The first step is data quality analysis, which includes descriptive statistics of the features, visualization analysis using univariate, and multivariate feature analysis. With the help of various plot types, we can uncover different trends in the data and examine how certain features may or may not correlate with the label values and with each other. Data analysis is followed by data pre-processing, where the techniques include ways to address noise, as in the case of missing data, and outliers, as well as preparing the data for modeling techniques through normalization and discretization.</p><p>Following pre-processing, we must suitably split the data into train, validation, and test samples. Different sampling strategies may be used depending on the characteristics of the data and the problem at hand, for example, when the data is skewed or when we have a multi-class classification problem. Depending on data size, cross-validation is a common alternative to creating a separate validation set.</p><p>The next step is the culling of irrelevant features. In the filter approach, techniques that use univariate analysis are either entropy-based (Information Gain, Gains Ratio) or based on statistical hypothesis testing (Chi-Squared). With the main multivariate methods, the aim is reduction of redundant features when considered together, or using the ones that correlate most closely with the target label. In the wrapper approach, we use machine learning algorithms to tell us about the more discriminating features. Finally, some learning techniques have feature selection embedded in the algorithm in the form of a regularization term, typically using ridge or lasso techniques. These represent the embedded approach.</p><p>Modeling techniques are broadly classified into linear, non-linear, and ensemble methods. Among linear algorithms, the type of features can determine the algorithms to use—Linear Regression (numeric features only), Naïve Bayes (numeric or categorical), and logistic regression (numeric features only, or categorical transformed to numeric) are the work-horses. The outlined advantages and disadvantages of each method must be understood when choosing between them or interpreting the results of learning using these models.</p><p>Decision Tree, k-NN, and SVM are non-linear techniques, each with their own strengths and limitations. For example, interpretability is the main advantage of Decision Tree. k-NN is robust in the face of noisy data, but it does poorly with high-dimensional data. SVM suffers from poor interpretability, but shines even when the dataset is limited, and the number of features is large.</p><p>With a number of different models collaborating, ensemble methods can leverage the best of all. Bagging and boosting both are techniques that generalize better in the ensemble compared to the base learner they use and are popular in many applications.</p><p>Finally, what are the strategies and methods that can be used in evaluating model performance and comparing models to each other? The role of validation sets or cross-validation is essential to the ability to generalize over unseen data. Performance evaluation metrics derived from the confusion matrix are used universally to evaluate classifiers; some are used more commonly in certain domains and disciplines than others. ROC, Gain, and Lift curves are great visual representations of the range of model performance as the classification threshold is varied. When comparing models in pairs, several metrics based on statistical hypothesis testing are used. Wilcoxon and McNemar's are two non-parametric tests; Paired-t test is an example of a parametric method. Likewise, when comparing multiple algorithms, a common non-parametric test that does not make assumptions about the data distribution is Friedman's test. ANOVA, which are parametric tests, assume normal distribution of the metrics and equal variances.</p><p>The final sections of the chapter present the process undertaken using the RapidMiner tool to develop and evaluate models generated to classify test data from the UCI Horse-colic dataset. Three experiments are designed to compare and contrast the performance of models under different data pre-processing conditions, namely, without handling missing data, with replacement of missing data using standard techniques, and finally, with feature selection following null replacement. In each experiment we choose multiple linear, non-linear, and ensemble methods. As part of the overall process, we illustrate how the modeling environment is used. We can draw revealing conclusions from the results, which give us insights into the data as well as demonstrating the relative strengths and weakness of the various classes of techniques in different situations. We conclude that the data is highly non-linear and that ensemble learning demonstrates clear advantages over other techniques.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec25"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">D. Bell and H. Wang (2000). <span class="emphasis"><em>A Formalism for Relevance and its Application in Feature Subset Selection. Machine Learning</em></span>, 41(2):175–195.</li><li class="listitem">J. Doak (1992). <span class="emphasis"><em>An Evaluation of Feature Selection Methods and their Application to Computer Security</em></span>. Technical Report CSE–92–18, Davis, CA: University of California, Department of Computer Science.</li><li class="listitem">M. Ben-Bassat (1982). <span class="emphasis"><em>Use of Distance Measures, Information Measures and Error Bounds in Feature Evaluation</em></span>. In P. R. Krishnaiah and L. N. Kanal, editors, Handbook of Statistics, volume 2, pages 773–791, North Holland.</li><li class="listitem">Littlestone N, Warmuth M (1994) <span class="emphasis"><em>The weighted majority algorithm</em></span>. Information Computing 108(2):212–261</li><li class="listitem">Breiman L., Friedman J.H., Olshen R.A., Stone C.J. (1984) <span class="emphasis"><em>Classification and Regression Trees</em></span>, Wadsforth International Group.</li><li class="listitem">B. Ripley(1996), <span class="emphasis"><em>Pattern recognition and neural networks</em></span>. Cambridge University Press, Cambridge.</li><li class="listitem">Breiman, L., (1996). <span class="emphasis"><em>Bagging Predictors, Machine Learning</em></span>, 24 123-140.</li><li class="listitem">Burges, C. (1998). <span class="emphasis"><em>A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery</em></span>. 2(2):1-47.</li><li class="listitem">Bouckaert, R. (2004), <span class="emphasis"><em>Naive Bayes Classifiers That Perform Well with Continuous Variables, Lecture Notes in Computer Science</em></span>, Volume 3339, Pages 1089 – 1094.</li><li class="listitem">Aha D (1997). <span class="emphasis"><em>Lazy learning</em></span>, Kluwer Academic Publishers, Dordrecht</li><li class="listitem">Nadeau, C. and Bengio, Y. (2003), <span class="emphasis"><em>Inference for the generalization error</em></span>. In Machine Learning 52:239– 281.</li><li class="listitem">Quinlan, J.R. (1993). C4.5: <span class="emphasis"><em>Programs for machine learning</em></span>, Morgan Kaufmann, San Francisco.</li><li class="listitem">Vapnik, V. (1995), <span class="emphasis"><em>The Nature of Statistical Learning Theory</em></span>. Springer Verlag.</li><li class="listitem">Schapire RE, Singer Y, Singhal A (1998). <span class="emphasis"><em>Boosting and Rocchio applied to text filtering</em></span>. In SIGIR '98: Proceedings of the 21st Annual International Conference on Research and Development in Information Retrieval, pp 215–223</li><li class="listitem">Breiman L.(2001). <span class="emphasis"><em>Random Forests</em></span>. Machine Learning, 45 (1), pp 5-32.</li><li class="listitem">Nathalie Japkowicz and Mohak Shah (2011). <span class="emphasis"><em>Evaluating Learning Algorithms: A Classification Perspective</em></span>. Cambridge University Press.</li><li class="listitem">Hanley, J. &amp; McNeil, B. (1982). <span class="emphasis"><em>The meaning and use of the area under a receiver operating characteristic (ROC) curve</em></span>. Radiology 143, 29–36.</li><li class="listitem">Tjen-Sien, L., Wei-Yin, L., Yu-Shan, S. (2000). <span class="emphasis"><em>A Comparison of Prediction Accuracy, Complexity, and Training Time of Thirty-Three Old and New Classification Algorithms</em></span>. Machine Learning 40: 203–228.</li><li class="listitem">A. W. Moore and M. S. Lee (1994). <span class="emphasis"><em>Efficient Algorithms for Minimizing Cross Validation Error</em></span>. In Proc. of the 11th Int. Conf. on Machine Learning, pages 190–198, New Brunswick, NJ. Morgan Kaufmann.</li><li class="listitem">Nitesh V. Chawla et. al. (2002). <span class="emphasis"><em>Synthetic Minority Over-sampling Technique</em></span>. Journal of Artificial Intelligence Research. 16:321-357.</li></ol></div></div></body></html>