- en: Automated Machine Learning and Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with Auto-WEKA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AutoML to generate machine learning pipelines with TPOT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Auto-Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with auto-sklearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MLBox for selection and leak detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks with transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning – pretrained image classifiers with ResNet-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning – feature extraction with the VGG16 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning with retrained GloVe embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To address the recipes in this chapter, you will need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`TPOTIrisClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AKClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLBoxRegressor.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ASKLClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImageTransferLearning.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PretrainedImageClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ExtractFeatures.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PTGloveEMB.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Automated machine learning** (**AutoML**) refers to those applications that
    are able to automate the end-to-end process of applying machine learning to real-world
    problems. Generally, scientific analysts must process data through a series of
    preliminary procedures before submitting it to machine learning algorithms. In
    the previous chapters, you saw the necessary steps for performing a proper analysis
    of data through these algorithms. You saw how simple it is to build a model based
    on deep neural networks by using several libraries. In some cases, these skills
    are beyond those possessed by analysts, who must seek support from industry experts
    to solve the problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML was born from a need to create an application that automated the whole
    machine learning process so that the user could take advantage of these services.
    Generally, machine learning experts must perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting an appropriate model class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing and optimizing model hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-processing machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the results obtained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoML automates all of these operations. It offers the advantages of producing
    simpler and faster-to-create solutions that often outperform hand-designed models.
    There are a number of AutoML frameworks; in the following sections, we will look
    at some of them.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Auto-WEKA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weka is a software environment that's entirely written in Java. **Weka**, an
    acronym for **Waikato Environment for Knowledge Analysis**, is a machine learning
    software that was developed at the University of Waikato in New Zealand. It is
    open source and is distributed under the GNU General Public License. It is possible
    to build many models based on machine learning by using it.
  prefs: []
  type: TYPE_NORMAL
- en: However, each of the algorithms has its own hyperparameters, which can drastically
    change their performance. The task of the researcher is to find the right combination
    of these parameters that will maximize the performance of the model. Auto-WEKA
    automatically solves the problem of the selection of a learning algorithm and
    the setting of its hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to use Auto-WEKA in only three main steps. To
    use this library, it is necessary to install it beforehand. For information on
    the system requirements and the installation procedure, refer to [https://www.cs.ubc.ca/labs/beta/Projects/autoweka/manual.pdf](https://www.cs.ubc.ca/labs/beta/Projects/autoweka/manual.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to work with Auto-WEKA, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building the experiment definition and instantiating it**: In this step,
    you specify which dataset to use and which type of hyperparameter search will
    be performed. Then, the experiment is completely instantiated so that Auto-WEKA
    can identify the classifier to be used. In this phase, Auto-WEKA transforms all
    of the paths into absolute paths.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Experiment execution**: Auto-WEKA uses multiple cores by running the same
    experiment with several random seeds; the only requirement is that all of the
    experiments have a similar filesystem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analysis phase**: When Auto-WEKA uses a model-based optimization method,
    it produces a trajectory of hyperparameters that have identified by the optimization
    method as the best at a given time. The simplest form of analysis examines the
    best hyperparameters that have been found in all seeds and uses the trained model
    to make predictions about a new dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To select a learning algorithm and set its hyperparameters, Auto-WEKA uses a
    completely automated approach, taking advantage of recent innovations in Bayesian
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-WEKA was the first library to use Bayesian optimization to automatically
    instantiate a highly parametric machine learning framework. Later, AutoML was
    also applied by other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the official Auto-WEKA website: [https://www.cs.ubc.ca/labs/beta/Projects/autoweka/](https://www.cs.ubc.ca/labs/beta/Projects/autoweka/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization
    in WEKA*: [https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/16-599.pdf](https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/16-599.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Auto-WEKA: Combined Selection and Hyperparameter Optimization of
    Classification Algorithms*: [https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf](https://www.cs.ubc.ca/labs/beta/Projects/autoweka/papers/autoweka.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AutoML to generate machine learning pipelines with TPOT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TPOT** is a Python automated machine learning tool that optimizes machine
    learning pipelines by using genetic programming. In artificial intelligence, genetic
    algorithms are part of the class of evolutionary algorithms. A characteristic
    of the latter is finding solutions to problems by using techniques that are borrowed
    from natural evolution. The search for a solution to a problem is entrusted to
    an iterative process that selects and recombines more and more refined solutions
    until a criterion of optimality is reached. In a genetic algorithm, the population
    of solutions is pushed toward a given objective by evolutionary pressure.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to build the best performing model to classify
    the iris species (setosa, virginica, and versicolor) from the `iris` dataset,
    using TPOT. To use this library, it is necessary to install it. For information
    on the system requirements and for the installation procedure, refer to [https://epistasislab.github.io/tpot/installing/](https://epistasislab.github.io/tpot/installing/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to use AutoML to generate machine learning pipelines with
    TPOT:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `TPOTIrisClassifier.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the iris dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s split the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the model with unseen data (`XTest`) to evaluate the performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will export the model pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you run this code, a pipeline that achieves about 97% test accuracy will
    be returned.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TPOT automates machine learning pipeline construction by combining a flexible
    representation of the pipeline expression tree with stochastic search algorithms,
    such as genetic programming. In this recipe, you learned how to use TPOT to search
    for the best pipeline to classify the iris species from the iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TPOT is built on the basis of `scikit-learn`, so all of the code that is generated
    will seem very familiar to us, given the extensive use of the `scikit-learn` libraries
    in the previous chapters. TPOT is a platform that's under active development,
    and it is therefore subject to continuous updates.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the TPOT tool: [https://epistasislab.github.io/tpot/](https://epistasislab.github.io/tpot/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automating biomedical data science through tree-based pipeline optimization*,
    by Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender,
    La Creis Kidd, and Jason H. Moore (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Auto-Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-Keras is an open source software library for AutoML that aims at providing
    easy access to deep learning models. Auto-Keras has a number of features that
    allow you to automatically set up the architecture and parameters of deep learning
    models. Its ease of use, simple installation, and numerous examples make it a
    very popular framework. Auto-Keras was developed by the DATA Lab at Texas A and
    M University and community contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will learn how to use the Auto-Keras library to classify
    handwritten digits. To install the Auto-Keras package, we can use the `pip` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: At the time of writing this book, Auto-Keras was only compatible with Python
    3.6\. For the installation procedure, refer to the official website at [https://autokeras.com/](https://autokeras.com/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to work with Auto-Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `AKClassifier.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the `mnist` dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Before defining a classifier, we must give a new form to the arrays containing
    the input data without changing its contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use the model with unseen data (`XTest`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, with a few lines of code, we have managed to construct a classifier
    which, by providing a series of images of handwritten digits, can correctly classify
    the digits.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a package that allows us to automatically create an algorithm based
    on machine learning without worrying about the setting of the training parameters
    that, as you saw in previous chapters, are fundamental to the success of the model.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the Auto-Keras library: [https://autokeras.com/](https://autokeras.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Auto-Keras: Efficient Neural Architecture Search with Network Morphism*,
    by Haifeng Jin, Qingquan Song, and Xia Hu (arXiv:1806.10282).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with auto-sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-sklearn works on the `scikit-learn` machine learning library. It represents
    a platform based on supervised machine learning that's ready for use. It automatically
    searches for the correct machine learning algorithm for a new dataset and optimizes
    its hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to use auto-sklearn to build a classifier.
    To import the data, the `sklearn.datasets.load_digits` function will be used.
    This function loads and returns the digits dataset for classification problems.
    Each datapoint is an 8x8 image of a digit.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to work with auto-sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `ASKLClassifier.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the `digits` dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s split the dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use the model with unseen data (`XTest`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-sklearn uses Bayesian optimization for hyperparameter tuning for traditional
    machine learning algorithms that are implemented within `scikit-learn`. The best
    machine learning algorithm and the parameters that are optimized are searched
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Auto-sklearn is a good choice to automate the process of selecting and optimizing
    an automatic learning model because it creates extremely precise machine learning
    models, avoiding the tedious tasks of selecting, training, and testing different
    models.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `auto-sklearn` package: [https://automl.github.io/auto-sklearn/stable/](https://automl.github.io/auto-sklearn/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Efficient and Robust Automated Machine Learning*, by Feurer, et al., in Advances
    in Neural Information Processing Systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MLBox for selection and leak detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLBox is an automated library for machine learning. It supports distributed
    data processing, cleaning, formatting, and numerous algorithms for classification
    and regression. It allows for the extremely robust selection of functions and
    leak detection. It also provides stacking models, which means combining a set
    of model information to generate a new model that aims to perform better than
    the individual models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use this library, it is necessary to install it beforehand. For information
    on the system requirements and the installation procedure, refer to [https://mlbox.readthedocs.io/en/latest/installation.html](https://mlbox.readthedocs.io/en/latest/installation.html).[ ](https://mlbox.readthedocs.io/en/latest/installation.html)
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, you will learn what's strictly necessary to set up a pipeline
    using MLBox. A regression problem will be addressed via the use of the Boston
    dataset that was already used in [Chapter 1](f552bbc7-5e56-41b8-8e8d-915cc1bd53ab.xhtml), *The
    Realm of Supervised Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at how to use MLBox for selection and leak detection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the following packages (the full code is in the `MLBoxRegressor.py` file
    that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the data, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With this code, we have set up the list of paths to our datasets and the name
    of the target that we are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will read and preprocess these files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the model, the following code will be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the default configuration was used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to predict on the test set, use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: If you want configure the pipeline (steps, parameters, and values), the following
    optional step must be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'To test and optimize the whole pipeline, we will use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, to predict on the test set, we will use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLBox builds the whole pipeline with the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preprocessing**: All of the operations related to this phase make use of
    the `mlbox.preprocessing` sub-package. In this phase, we proceed to the reading
    and cleaning of the input file and then to the removal of the drift variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Optimization**: All of the operations related to this phase make use of the
    sub-package `mlbox.mlbox.optimisation`. In this phase, the whole pipeline is optimized.
    The hyperparametric optimization method that''s adopted uses the `hyperopt` library.
    This library creates a highly-dimensional space for the parameters to be optimized
    and chooses the best combination of parameters that lowers the validation score.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction**: All of the operations related to this phase make use of the
    `mlbox.prediction` sub-package. In this phase, we proceed to prediction by using
    the test dataset and the best hyperparameters that were identified in the previous
    phase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLBox provides advanced algorithms and techniques, such as hyperparameter optimization,
    stacking, deep learning, leak detection, entity embedding, parallel processing,
    and more. The use of MLBox is currently limited to Linux only. MLBox was first
    developed using Python 2, and then it was extended to Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLBox''s official documentation: [https://mlbox.readthedocs.io/en/latest/](https://mlbox.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installation guide: [https://mlbox.readthedocs.io/en/latest/installation.html](https://mlbox.readthedocs.io/en/latest/installation.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks with transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transfer learning** is a methodology based on machine learning that exploits
    the memorization of the knowledge that''s acquired during the resolution of a
    problem and the application of the same to different (but related) problems. The
    need to use transfer learning takes place when there is a limited supply of training
    data. This could be due to the fact that data is rare or expensive to collect
    or label, or inaccessible. With the growing presence of large amounts of data,
    the transfer learning option has become more frequently used.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) are essentially **artificial neural
    networks** (**ANNs**). In fact, just like the latter, CNNs are made up of neurons
    that are connected to one another by weighted branches (weight); the training
    parameters of the networks are once again the weight and the bias. In CNNs, the
    connection pattern between neurons is inspired by the structure of the visual
    cortex in the animal world. The individual neurons that are present in this part
    of the brain (the visual cortex) respond to certain stimuli in a narrow region
    of the observation, called the **receptive field**. The receptive fields of different
    neurons are partially overlapped to cover the entire field of vision. The response
    of a single neuron to stimuli taking place in its receptive field can be mathematically
    approximated by a convolution operation.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to build an image recognition model by using
    transfer learning in Keras. To do this, the MobileNet model and Keras high-level
    neural networks API will be used to train the model images extracted from the
    `Caltech256` dataset that we already used in [Chapter 10](8c346bea-36ab-4087-918f-b5d8712977cc.xhtml), *Image
    Content Analysis*. `Caltech256` is very popular in this field! It contains 256
    classes of images, where each class contains thousands of samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s build an image recognition model by using transfer learning in Keras;
    in this section, we will explain the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `ImageTransferLearning.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s import the `MobileNet` model and discard the last 1,000 neuron layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the Keras model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build a model based on the architecture that was previously defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can move on to the training phase. Having adopted an approach based
    on transfer learning, it is not necessary to proceed with the training of the
    whole model. This is because MobileNet is already trained. Let''s define the last
    dense levels as the trainable layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the training data into `ImageDataGenerator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`ImageDataGenerator` is a built-in Keras class that creates groups of tensor
    image data with real-time data augmentation. The data will be wound over in groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define some dependencies and a path for the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s compile the Keras model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following three arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`optimizer=''adam''`: An algorithm for first-order, gradient-based optimization
    of stochastic objective functions, based on adaptive estimates of lower-order
    moments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss=''categorical_crossentropy''`: We have used the `categorical_crossentropy`
    argument here. When using'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`categorical_crossentropy`, your targets should be in a categorical format
    (we have 10 classes; the target for each sample must be a 10-dimensional vector
    that is all-zeros, except for a one at the index corresponding to the class of
    the sample).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`metrics=[''accuracy'']`: A metric is a function that is used to evaluate the
    performance of your model during training and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we will define the step size for training and fit the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you learned how to use transfer learning in an image recognition
    problem. Through transfer learning, a pretrained model can be used on a large
    and accessible dataset to find layers whose output have reusable features, which
    is done by using this output as input to train a smaller network that requires
    fewer parameters. This network will only need to know the relationships between
    the patterns that are obtained from the pretrained models and the specific problem
    to be solved. As a pretrained model, the MobileNet model was used.
  prefs: []
  type: TYPE_NORMAL
- en: '`MobileNet` is an architecture that was proposed by Google and that is particularly
    suitable for vision-based applications. MobileNet uses deep separable convolutions
    that significantly reduce the number of parameters, compared to a network with
    normal convolutions with the same depth in the networks. Neural networks based
    on the MobileNet model are thus lighter. The normal convolution is replaced by
    a in-depth convolution, followed by a punctual convolution that is called **convolution
    separable in depth**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The transfer learning procedure was then performed in two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: First, almost all levels of the neural network were trained on a very large
    and generic dataset to allow for the acquisition of global notions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Later, we used the specific dataset for the training of the remaining layers,
    deciding whether to propagate the errors through fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used fine-tuning; in fact, we didn't simply replace the final
    level, but we also trained some of the previous levels. In the network that we
    used, the initial levels were used to acquire generic functionalities (exploiting
    the potential of the MobileNet trained network), while the subsequent ones were
    used to finalize the experience that was acquired on the specific activity in
    question. Using this procedure, we froze the first 20 layers while we traced the
    following layers to meet our needs. This methodology helps to achieve better performance
    with less training time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning can be achieved through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with a pretrained network trained on a similar problem and replace
    the output level with a new level of output by adjusting the number of classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The initial values ​​of the weights are those of the pretrained net, except
    for the connections between successive layers whose weights are initialized randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We perform new training iterations (SGD) for optimized weights with respect
    to the peculiarities of the new dataset (it does not need to be large).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the fine-tuning process, the model parameters will be adjusted precisely
    to fit with certain observations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to Keras application models: [https://keras.io/applications/](https://keras.io/applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
    Applications*: [https://arxiv.org/pdf/1704.04861.pdf](https://arxiv.org/pdf/1704.04861.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Transfer Learning and Computer Vision* (from Yale University): [http://euler.stat.yale.edu/~tba3/stat665/lectures/lec18/lecture18.pdf](http://euler.stat.yale.edu/~tba3/stat665/lectures/lec18/lecture18.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to *A Survey on Transfer Learning*, S. J. Pan and Q. Yang, in IEEE Transactions
    on Knowledge and Data Engineering: [https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning with pretrained image classifiers using ResNet-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **residual network** (**ResNet**) represents an architecture that, through
    the use of new and innovative types of blocks (known as **residual blocks**) and
    the concept of residual learning, has allowed researchers to reach depths that
    were unthinkable with the classic feedforward model, due to the problem of the
    degradation of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models are trained on a large set of data, and so they allow us to
    obtain excellent performance. We can therefore adopt pretrained models for a problem
    similar to the one that we want to solve, to avoid the problem of a lack of data.
    Because of the computational costs of the formation of such models, they are available
    in ready-to-use formats. For example, the Keras library offers several models
    such as Xception, VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2,
    MobileNet, MobileNetV2, DenseNet, and NASNet.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to use a pretrained model to predict the
    class of a single image. To do this, a ResNet-50 model will be used. This model
    is available from the `keras.applications` library.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will use a pretrained model to classify a single image; in this section,
    we will explain the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `PretrainedImageClassifier.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the image to classify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will take an image instance and turn it into a `numpy` array with
    `dtype` `float32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will expand the `numpy` array that''s obtained in the shape that''s
    required by the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will preprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will use the pretrained model to classify the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To evaluate the model''s performance, we will use the `decode_predictions`
    function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The `keras.applications.resnet50.decode_predictions` function decodes the results
    into a list of tuples (class, description, and probability). The following results
    are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The higher probability (`0.80847234`) tells us that it is an airliner; in fact,
    the following is the image that was provided as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/999001d5-8a86-43b0-8bc7-fc359f12e3b6.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of trying to estimate a function `G` that, given an `x`, returns `G
    (x)`, ResNet learns the difference between the two values—a value called the **residual**. In
    the residual layer of the network, a classical convolution takes place and the
    input is added to the result. If the input and output are of different sizes,
    the input is transformed with another 1×1 filter convolution before being added
    to the output so that it has the same feature map number. The size of a feature
    map is preserved by padding. A benefit of this technique is that the L2 regularization,
    which tends the weights toward zero, does not make us forget what was learned
    previously, but simply preserves it.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are ResNet implementations with different depths; the deepest counts as
    many as 152 levels. There is also a prototype with 1,202 levels, but it achieved
    worse results due to overfitting. This architecture won ILSVRC 2015, with an error
    of 3.6%. To understand the value of this result, just consider that the error
    that's generally achieved by a human being is around 5-10%, based on their skills
    and knowledge. Thanks to these results, the ResNet model is currently state of
    the art in the field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `keras.applications` models: [https://keras.io/applications/ ](https://keras.io/applications/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Residual Learning for Image Recognition* (by Kaiming He, Xiangyu Zhang,
    Shaoqing Ren, and Jian Sun): [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pretrained Models* (from Toronto University): [https://www.cs.toronto.edu/~frossard/tags/pre-trained-models/](https://www.cs.toronto.edu/~frossard/tags/pre-trained-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning using feature extraction with the VGG16 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we stated in the *Visualizing the MNIST dataset using PCA and t-SNE* recipe
    of [Chapter 14](bfa120ec-7b32-4af2-85c3-f16bbaf84998.xhtml)*, Unsupervised Representation
    Learning*, in the case of datasets of important dimensions, the data was transformed
    into a reduced series of representation functions. This process of transforming
    the input data into a set of functionalities is named **feature extraction**.
    This is because the extraction of the characteristics proceeds from an initial
    series of measured data and produces derived values that can keep the information
    contained in the original dataset, but excluded from the redundant data. In the
    case of images, feature extraction is aimed at obtaining information that can
    be identified by a computer.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to extract features from a series of images.
    Then, we will use these features to classify the images by using the k-means algorithm.
    In this recipe, we will use the VGG16 pretrained model and the `klearn.cluster.KMeans`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s perform a feature extraction procedure by using the VGG16 model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `ExtractFeatures.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s initialize the list of features that will be extracted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'For each image in the dataset, we have to proceed with the extraction of features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we have recovered the path of each image contained in the folder.
    The images that are used are contained in the `training_images` folder, which
    we already used in the *Convolutional neural networks with transfer learning *recipe.
    It is a series of images that was extracted from the `Caltech256` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We will take an image instance and turn it into a NumPy array, with datatype
    as `float32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will expand the NumPy array that''s obtained in the shape that''s required
    by the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will preprocess the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the pretrained model to extract features from the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we will create an array with the obtained features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will add the array that was obtained, to the list of features that
    we are building (one element for each image):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We will transform the final list into an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the features that was obtained from the images to group them
    by type. Remember that these are images from three categories: airplanes, cars,
    and motorbikes. So, we expect the images to be labeled with three different labels.
    To do this, we use the `KMeans` algorithm, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the model, we move on to training it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the labels of the images that are used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the 60 images have been correctly labeled in the three available
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you learned how to extract features from a series of images.
    As we have a limited number of images available, we used a pretrained model (VGG16)
    to correctly extract the information that was needed for subsequent identification.
    This procedure is useful to understand how to proceed to perform automatic recognition
    of the images through an unsupervised model. After extracting the features, we
    used them to classify the images, using the KMeans algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VGG16 is a convolutional neural network model that was presented by K. Simonyan
    and A. Zisserman, from the University of Oxford, in the paper *Very Deep Convolutional
    Networks for Large-Scale Image Recognition*. This model has achieved excellent
    results in image recognition (with 92.7% accuracy). The test was performed on
    the ImageNet dataset, with over 14 million images belonging to 1,000 classes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the *Visualizing Mnist dataset using PCA and t-SNE* recipe in [Chapter
    14](bfa120ec-7b32-4af2-85c3-f16bbaf84998.xhtml)*, Unsupervised Representation
    Learning*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *Very Deep Convolutional Networks for Large-Scale Image Recognition*:
    [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning with pretrained GloVe embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GloVe** is an unsupervised learning algorithm for obtaining vector representations
    of words. The training is performed on the aggregate global statistics on the
    co-occurrence of words that has been extracted from a body of text present in
    the code files. The resulting representations show interesting linear substructures
    in the vector space of words. In this recipe, you will learn how to use a pretrained
    GloVe embedding model to classify adjectives to describe a person in a positive
    or negative fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow this recipe, you will need to download the `glove.6B.100d.txt` file.
    This file is available at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/).
    There are several versions of the pretrained word vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**glove.6B**: 6B tokens, 400K vocab, uncased, 50d, 100d, 200d, and 300d vectors—822
    MB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**glove.42B.300d**: 42B tokens, 1.9M vocab, uncased, 300d vectors—1.75 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**glove.840B.300d**: 840B tokens, 2.2M vocab, cased, 300d vectors—2.03 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Twitter**: 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, and 200d vectors—1.42
    GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s classify the adjectives that are used to describe a person in a positive
    and negative fashion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    in the `PTGloveEMB.py` file that''s already been provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the 10 positive and 10 negative adjectives that are used to describe
    a person:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the labels of the adjectives that were defined previously (`1` =
    positive, `0` = negative):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s tokenize the adjectives and prepare the vocabulary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s encode the adjectives into an integer sequence and transform a list
    of sequences into a two-dimensional NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a weight matrix for words in tokenized adjectives:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to define the `keras` sequential model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The following summary is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, only part of the parameters have been trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compile and fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will evaluate the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To quantitatively capture the nuances that are necessary to distinguish a positive
    adjective from a negative adjective, a model has to associate more than a single
    number with word combinations. A simple method for a set of words is the vector
    difference between two vectors of words. GloVe is designed so that these vector
    differences capture the meanings specified by the juxtaposition of several words
    as closely as possible.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In transfer learning, the weights of the network are adapted and transferred
    so that we can use this knowledge to pursue multiple different objectives. To
    obtain good performance from transfer learning, certain conditions must be met:
    the initial and final datasets must not be too different from each other, and
    they must share the same preprocessing operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, you have seen several examples of how the concepts of transfer learning
    can be applied to real cases. Actually, in practice, transfer learning takes on
    different types: `Inductive Transfer learning`, `Unsupervised Transfer Learning`,
    `Transductive Transfer Learning`, and `Instance Transfer`. We are trying to deepen
    those concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand the differences between these methodologies, we will look at the
    terms—domains and tasks. By the term **domain**, we mean the type of data that's
    used by the network, while by the term **task**, we mean what the network intends
    to do. We will also use the terms **source** and **destination** to distinguish
    the network that's already trained on a large amount of data from the network
    that we intend to build.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the simplest forms of supervised machine learning is `inductive learning`.
    It is based solely on observation. Given an initial set of input-output examples,
    the agent elaborates on hypotheses to reconstruct the transfer function. The agent
    is designed to observe interactions with the outside world. In particular, the
    agent analyzes the feedback of its decisions. The perceptions of the artificial
    agent can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To make decisions (reactive agent)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To improve the agent's decision-making capacity (machine learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In `Inductive Transfer Learning` methods, the information that's processed by
    the two networks (the source and destination) is of the same type (images, sounds,
    and so on), while the tasks performed by the networks are different. In this case,
    the purpose of transfer learning is to use the `inductive-bias` that was recovered
    in the training of the source network to improve the performance of the destination
    network. By the term **inductive-bias**, we mean a series of hypotheses concerning
    the distribution of the data that the algorithm recovers in the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In unsupervised transfer learning, the information that's processed by the two
    networks (the source and destination) is of the same type (images, sounds, and
    so on), while the tasks that are performed by the networks are different, like
    in inductive transfer learning. The substantial difference between the two methods
    lies in the fact that no labeled data is available in unsupervised transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Transductive transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In transductive transfer learning, the information that's processed by the two
    networks (the source and destination) is different, while the tasks that are performed
    by the networks are similar. This methodology is based on the concept of transductive
    inference, which brings the reasoning from specific (training) cases to specific
    cases (tests). Unlike induction, which requires the solution to a more general
    problem before solving a more specific problem, in transduction, we try to get
    the answer that we really need, but not a more general one.
  prefs: []
  type: TYPE_NORMAL
- en: Instance transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A scenario in which the domains of the origin and destination are perfectly
    similar is difficult to find. It is more possible to identify a part of data that
    is better approximating to those of destination but lies in the domain of origin
    which is of a much larger size than the destination one. In instance transfer
    learning, we look for the training samples in the origin domain that have a strong
    correlation with the destination domain. Once they are identified, they are reused
    in the learning phase of the target activity; in this way, the accuracy of the
    classification is improved.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to *Global Vectors for Word Representation* (by Jeffrey Pennington, Richard
    Socher, and Christopher D. Manning): [https://www.aclweb.org/anthology/D14-1162](https://www.aclweb.org/anthology/D14-1162)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to *A Review of Transfer Learning Algorithms* (by Mohsen Kaboli): [https://hal.archives-ouvertes.fr/hal-01575126/document](https://hal.archives-ouvertes.fr/hal-01575126/document)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
