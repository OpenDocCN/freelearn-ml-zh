<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:pls="http://www.w3.org/2005/01/pronunciation-lexicon" xmlns:ssml="http://www.w3.org/2001/10/synthesis">
<head>
  <meta charset="UTF-8"/>
  <title>Features and scikit-learn Transformers</title>
  <link type="text/css" rel="stylesheet" media="all" href="style.css"/>
  <link type="text/css" rel="stylesheet" media="all" href="core.css"/>
</head>
<body>
  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Features and scikit-learn Transformers</h1>
            </header>

            <article>
                
<p><span><span><span>The datasets we have used so far have been described in terms of <em>features</em>. In the previous chapter, we used a transaction-centric dataset. However, ultimately this was just a different format for representing feature-based data.</span></span></span></p>
<p><span><span><span>There are many other types of datasets, including text, images, sounds, movies, or even real objects. Most data mining algorithms rely on having numerical or categorical features. This means we need a way to represent these types before we input them into the data mining algorithm. We call this representation a <strong>model</strong>.</span></span></span></p>
<p><span><span><span>In this chapter, we will discuss how to extract numerical and categorical features, and choose the best features when we do have them. We will discuss some common patterns and techniques for extracting features. Choosing your model appropriately is critically important to the outcome of the data mining exercise, more so than the choice of classification algorithm.</span></span></span></p>
<p><span><span><span>The key concepts introduced in this chapter include:</span></span></span></p>
<ul>
<li><span><span><span>Extracting features from datasets</span></span></span></li>
<li>Creating models for your data</li>
<li><span><span><span>Creating new features</span></span></span></li>
<li><span><span><span>Selecting good features</span></span></span></li>
<li><span><span><span>Creating your own transformer for custom datasets</span></span></span></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Feature extraction</h1>
            </header>

            <article>
                
<p>Extracting features is one of the most critical tasks in data mining, and it generally affects your end result more than the choice of data mining algorithm. Unfortunately, there are no hard and fast rules for choosing features that will result in high-performance data mining. The choice of features determines the model that you are using to represent your data.</p>
<div class="packt_infobox"><span><span><span>Model creation is where the science of data mining becomes more of an art and why automated methods of performing data mining (there are several methods of this type) focus on algorithm choice and not model creation. Creating good models relies on intuition, domain expertise, data mining experience, trial and error, and sometimes a little luck.</span></span></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Representing reality in models</h1>
            </header>

            <article>
                
<p><span><span><span>Given what we have done so far in the book, it is easy to forget that the reason we are performing data mining is to affect real world objects, not just manipulating a matrix of values. Not all datasets are presented in terms of features. Sometimes, a dataset consists of nothing more than all of the books that have been written by a given author. Sometimes, it is the film of each of the movies released in 1979. At other times, it is a library collection of interesting historical artifacts.&#160;</span></span></span></p>
<p><span><span><span>From these datasets, we may want to perform a data mining task. For the books, we may want to know the different categories that the author writes. In the films, we may wish to see how women are portrayed. In the historical artifacts, we may want to know whether they are from one country or another. It isn't possible to just pass these raw datasets into a decision tree and see what the result is.</span></span></span></p>
<p><span><span><span>For a data mining algorithm to assist us here, we need to represent these as <strong>features</strong>. Features are a way to create a model and the model provides an approximation of reality in a way that data mining algorithms can understand. Therefore, a model is just a simplified version of some aspect of the real world. As an example, the game of chess is a simplified model (in game form) for historical warfare.</span></span></span></p>
<div class="packt_infobox"><span><span><span>Selecting features has another advantage: they reduce the complexity of the real world into a more manageable model.</span></span></span></div>
<p><span><span><span>Imagine how much information it would take to properly, accurately, and fully describe a real-world object to someone that has no background knowledge of the item. You would need to describe the size, weight, texture, composition, age, flaws, purpose, origin, and so on.</span></span></span></p>
<p><span><span><span>As the complexity of real objects is too much for current algorithms, we use these simpler models instead.</span></span></span></p>
<p><span><span><span>This simplification also focuses our intent in the data mining application. In later chapters, we will look at <span class="packt_screen">clustering</span> and where it is critically important. If you put random features in, you will get random results out.</span></span></span></p>
<p><span><span><span>However, there is a downside as this simplification reduces the detail, or may remove good indicators of the things we wish to perform data mining on.</span></span></span></p>
<p><span><span><span>Thought should always be given to how to represent reality in the form of a model. Rather than just using what has been used in the past, you need to consider the goal of the data mining exercise. What are you trying to achieve? In <a href="lrn-dtmn-py-2e_ch03.html" target="_blank"><span><span>Chapter 3</span></span></a>, <em>Predicting Sports Winners with Decision Trees</em>, we created features by thinking about the goal (predicting winners) and used a little domain knowledge to come up with ideas for new features.</span></span></span></p>
<div class="packt_tip">Not all features need to be numeric or categorical. Algorithms have been developed that work directly on text, graphs, and other data structures. Unfortunately, those algorithms are outside the scope of this book. In this book, and normally in your data mining career, we mainly use numeric or categorical features.</div>
<p><span><span><span>The <em>Adult</em> dataset is a great example of taking a complex reality and attempting to model it using features. In this dataset, the aim is to estimate if someone earns more than $50,000 per year.</span></span></span></p>
<div class="packt_tip"><span><span><span>To download the dataset, navigate to <a href="http://archive.ics.uci.edu/ml/datasets/Adult"><span><span><span><span><span>http://archive.ics.uci.edu/ml/datasets/Adult</span></span></span></span></span></a>&#160;and click on the <span><span class="packt_screen">Data Folder</span></span> link. Download the <kbd><span><span><span><span><span>adult.data</span></span></span></span></span></kbd> and <kbd><span><span><span><span><span>adult.names</span></span></span></span></span></kbd> into a directory named <span><span><span><span><span>Adult</span></span></span></span></span> in your data folder.</span></span></span></div>
<p><span><span><span>This dataset takes a complex task and describes it in features. These features describe the person, their environment, their background, and their life status.</span></span></span></p>
<p><span>Open a new Jupyter Notebook<span>&#160;</span>for this chapter, set the data filename and load the data with pandas:</span></p>
<pre><span><span><span>import os<br/></span></span></span><span><span><span>import pandas as pd<br/></span></span></span><span><span><span>data_folder = os.path.join(os.path.expanduser("~"), "Data", "Adult")<br/></span></span></span><span><span><span>adult_filename = os.path.join(data_folder, "adult.data")</span></span></span>
</pre>
<pre>adult = pd.read_csv(adult_filename, header=None, <span><span><span>names=["Age", "Work-Class", "fnlwgt", <br/></span></span></span><span><span><span>                     "Education", "Education-Num", </span></span></span><span><span><span>"Marital-Status", "Occupation",<br/></span></span></span><span><span><span>                     "Relationship", "Race", "Sex", </span></span></span><span><span><span>"Capital-gain", "Capital-loss",<br/></span></span></span><span><span><span>                     "Hours-per-week", "Native-Country", </span></span></span><span>"Earnings-Raw"])</span>
</pre>
<p><span><span><span>Most of the code is the same as in the previous chapters.</span></span></span></p>
<div class="packt_tip">Don't want to type those heading names? Don't forget you can download the code from Packt Publishing, or alternatively from the author's GitHub repository for this book:<br/>
<a href="https://github.com/dataPipelineAU/LearningDataMiningWithPython2">https://github.com/dataPipelineAU/LearningDataMiningWithPython2</a></div>
<p><span><span><span>The <span><span><span><span><span>adult</span></span></span></span></span> file itself contains two blank lines at the end of the file. By default,</span></span></span> pandas w<span><span><span>ill interpret the penultimate new line to be an empty (but valid) row. To remove this, we remove any line with invalid numbers (the use of <kbd><span><span><span><span><span>inplace</span></span></span></span></span></kbd> just makes sure the same Dataframe is affected, rather than creating a new one):</span></span></span></p>
<pre>adult.dropna(how='all', inplace=True)
</pre>
<p><span><span><span>Having a look at the dataset, we can see a variety of features from <kbd><span><span><span><span><span>adult.columns</span></span></span></span></span></kbd>:</span></span></span></p>
<pre>adult.columns
</pre>
<p><span><span><span>The results show each of the feature names that are stored inside an <span><span><span><span><span>Index</span></span></span></span></span></span></span></span> object from pandas<span><span><span>:</span></span></span></p>
<pre><span><span><span>Index(['Age', 'Work-Class', 'fnlwgt', 'Education', <br/>'Education-Num', 'Marital-Status', 'Occupation', 'Relationship', <br/>'Race', 'Sex', 'Capital-gain', 'Capital-loss', 'Hours-per-week', <br/>'Native-Country', 'Earnings-Raw'], dtype='object')</span></span></span>
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Common feature patterns</h1>
            </header>

            <article>
                
<p>While there are millions of ways to create <span>models</span><span><span><span>, there are some common patterns that are employed across different disciplines. However, choosing appropriate</span></span></span> features <span><span><span>is tricky and it is worth considering how a feature might correlate to the end result. As a well known adage goes, <em>don't judge a book by its cover</em>—it is probably not worth considering the size of a book if you are interested in the message contained within.</span></span></span></p>
<p><span><span><span>Some commonly used features focus on the physical properties of the real world objects being studied, for example:</span></span></span></p>
<ul>
<li><span><span><span>Spatial properties such as the length, width, and height of an object</span></span></span></li>
<li><span><span><span>Weight and/or density of the object</span></span></span></li>
<li><span><span><span>Age of an object or its components</span></span></span></li>
<li><span><span><span>The type of the object</span></span></span></li>
<li><span><span><span>The quality of the object</span></span></span></li>
</ul>
<p><span><span><span>Other features might rely on the usage or history of the object:</span></span></span></p>
<ul>
<li><span><span><span>The producer, publisher, or creator of the object</span></span></span></li>
<li><span><span><span>The year of manufacturing</span></span></span></li>
</ul>
<p><span><span><span>Other features describe a dataset in terms of its components:</span></span></span></p>
<ul>
<li><span><span><span>Frequency of a given subcomponent, such as a word in a book</span></span></span></li>
<li><span><span><span>Number of subcomponents and/or the number of different subcomponents</span></span></span></li>
<li><span><span><span>Average size of the subcomponents, such as the average sentence length</span></span></span></li>
</ul>
<p><span><span><span>Ordinal</span></span></span> features <span><span><span>allow us to perform ranking, sorting, and grouping of similar values. As we have seen in previous chapters, <span class="packt_screen">features</span> can be numerical or categorical.</span></span></span></p>
<p><span><span><span>Numerical features are often described as being <span><span><span>ordinal</span></span></span>. For example, three people, Alice, Bob, and Charlie, may have heights of 1.5 m, 1.6 m, and 1.7 m. We would say that Alice and Bob are more similar in height than Alice and Charlie.</span></span></span></p>
<p><span><span><span>The Adult dataset that we loaded in the last section contains examples of continuous, ordinal features. For example, the <span><span><span><span><span><span class="packt_screen">Hours-per-week</span></span></span></span></span></span> feature tracks how many hours per week people work. Certain operations make sense on a feature like this. They include computing the mean, standard deviation, minimum, and maximum. There is a function in</span></span></span> pandas for giving some basic summary stats of this type:</p>
<pre>adult["Hours-per-week"].describe()
</pre>
<p><span><span><span>The result tells us a little about this feature:</span></span></span></p>
<pre><span><span><span>count 32561.000000<br/></span></span></span><span><span><span>mean 40.437456<br/></span></span></span><span><span><span>std 12.347429<br/></span></span></span><span><span><span>min 1.000000<br/></span></span></span><span><span><span>25% 40.000000<br/></span></span></span><span><span><span>50% 40.000000<br/></span></span></span><span><span><span>75% 45.000000<br/></span></span></span><span><span><span>max 99.000000<br/></span></span></span><span><span><span>dtype: float64</span></span></span>
</pre>
<p><span><span><span>Some of these operations do not make sense for other features. For example, it doesn't make sense to compute the sum of the education statuses&#160;of these people. In contrast, it would make sense to compute the sum of the number of orders by each customer on an online store.</span></span></span></p>
<p><span><span><span>There are also features that are not numerical, but still ordinal. The <span><span><span><span><span><span class="packt_screen">Education</span></span></span></span></span></span> feature in the Adult dataset is an example of this. For example, a Bachelor's degree is a higher education status than finishing high school, which is a higher status than not completing high school. It doesn't quite make sense to compute the mean of these values, but we can create an approximation by taking the median value. The dataset gives a helpful feature, <kbd><span><span><span><span><span>Education-Num</span></span></span></span></span></kbd>, which assigns a number that is basically equivalent to the number of years of education completed. This allows us to quickly compute the median:</span></span></span></p>
<pre>adult["Education-Num"].median()
</pre>
<p><span><span><span>The result is 10, or finishing one year past high school. If we didn't have this, we could compute the median by creating an ordering over the education values.</span></span></span></p>
<p><span><span><span>Features can also be categorical. For instance, a ball can be a <span><span>tennis ball</span></span>, <span><span>cricket ball</span></span>, <span><span>football</span></span>, or any other type of ball. Categorical features are also referred to as nominal features. For nominal features, the values are either the same or they are different. While we could rank balls by size or weight, just the category alone isn't enough to compare things. A tennis ball is not a cricket ball, and it is also not a football. We could argue that a tennis ball is more similar to a cricket ball (say, in size), but the category alone doesn't differentiate this—they are the same, or they are not.</span></span></span></p>
<p><span><span><span>We can convert categorical features to numerical features using the one-hot encoding, as we saw in <a href="lrn-dtmn-py-2e_ch03.html"><span><span>Chapter 3</span></span></a>, <em>Predicting Sports Winners with Decision Trees</em>. For the aforementioned categories of balls, we can create three new binary features: <span><span><span><span><span>is a tennis ball</span></span></span></span></span>, <span><span><span><span><span>is a cricket ball</span></span></span></span></span>, and <span><span><span><span><span>is a football</span></span></span></span></span>. This process is the one-hot encoding we used in <a href="lrn-dtmn-py-2e_ch03.html">Chapter 3</a>, <em>Predicting Sports Winners with Decision Trees</em>. For a tennis ball, the vector would be <kbd>[1, 0, 0]</kbd>. A cricket ball has the values <kbd>[0, 1, 0]</kbd>, while a football has the value <kbd>[0, 0, 1]</kbd>. These are binary features&#160;but can be used as continuous features by many algorithms. One key reason for doing this is that it easily allows for direct numerical comparison (such as computing the distance between samples).</span></span></span></p>
<p><span><span><span>The Adult dataset contains several categorical features, with <span><span><span><span><span><span class="packt_screen">Work-Class</span></span></span></span></span></span> being one example. While we could argue that some values are of higher rank than others (for instance, a person with a job is likely to have a better income than a person without), it doesn't make sense for all values. For example, a person working for the state government is not more or less likely to have a higher income than someone working in the private sector.</span></span></span></p>
<p><span><span><span>We can view the unique values for this feature in the dataset using the&#160;</span></span></span><kbd><span><span><span><span><span>unique()</span></span></span></span></span></kbd> function:</p>
<pre><span><span><span>adult["Work-Class"].unique()</span></span></span>
</pre>
<p><span><span><span>The result shows the unique values in this column:</span></span></span></p>
<pre><span><span><span>array([' State-gov', ' Self-emp-not-inc', ' Private', ' Federal-gov',<br/></span></span></span><span><span><span>' Local-gov', ' ?', ' Self-emp-inc', ' Without-pay',<br/></span></span></span><span><span><span>' Never-worked', nan], dtype=object)</span></span></span>
</pre>
<p><span><span><span>There are some missing values in the preceding data, but they won't affect our computations in this example. You can also use the <kbd>adult.value_counts()</kbd> function to see how frequently each value appears.</span></span></span></p>
<p>Another really useful step to take with a new dataset is to visualise it. The following code will create a swarm plot, giving a view of how education and hours-worked relate to the final classification (identified by colour):</p>
<pre>%matplotlib inline<br/>import seaborn as sns<br/>from matplotlib import pyplot as plt<br/>sns.swarmplot(x="Education-Num", y="Hours-per-week", hue="Earnings-Raw", data=adult[::50])
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="446" width="583" class=" image-border" src="images/B06162_05_03.png"/></div>
<p>In the above code, we sample the dataset to show every 50 rows, using the <kbd>adult[::50]</kbd> dataset indexing. Setting this to just <kbd>adult</kbd> will result in all samples being shown, but that may also make the graph hard to read.<br/></p>
<p><span><span><span>Similarly, we can convert numerical features to categorical features through a process called <strong>discretization</strong>, as we saw in <a href="lrn-dtmn-py-2e_ch05.html"><span><span>Chapter 1</span></span></a>, <em>Getting Started With Data Mining</em></span></span></span><span><span><span>. We can call any person who is taller than 1.7 m tall, and any person shorter than 1.7 m short. This gives us a categorical feature (although still an ordinal one). We do lose some data here. For instance, two people, one 1.69 m tall and one 1.71 m, will be in two different categories and considered <span><span>drastically</span></span> different from each other by our algorithm. In contrast, a person 1.2 m tall will be consi</span></span></span>dered of roughly the same height as <span><span><span>the person 1.69 m tall! This loss of detail is a side effect of discretization, and it is an issue that we deal with when creating models.</span></span></span></p>
<p><span><span><span>In the Adult dataset, we can create a <kbd><span><span><span><span><span>LongHours</span></span></span></span></span></kbd> feature, which tells us if a person works more than 40 hours per week. This turns our continuous feature (<kbd><span><span><span><span><span>Hours-per-week</span></span></span></span></span></kbd>) into a categorical one that is True if the number of hours is more than 40, False otherwise:</span></span></span></p>
<p><kbd><span><span><span>adult["LongHours"] = adult["Hours-per-week"] &gt; 40</span></span></span></kbd></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Creating good features</h1>
            </header>

            <article>
                
<p>Simplification due to modeling is a key<span><span><span>&#160;reason we do not have data mining methods that can just simply be applied to any dataset. A good data mining practitioner will need, or obtain, domain knowledge in the area they are applying data mining. They will look at the problem, the available data, and come up with a model that represents what they are trying to achieve.</span></span></span></p>
<p><span><span><span>For instance, a person's&#160;<span><span>height</span></span> feature may describe one component of a person, such as their&#160;ability to play basketball, but may not describe their academic performance well. If we were attempting to predict a person's grade, we may not bother measuring each person's height.</span></span></span></p>
<p><span><span><span>This is where data mining becomes more art than science. Extracting good features is difficult and is the topic of significant and ongoing research. Choosing better classification algorithms can improve the performance of a data mining application, but choosing better features is often a better option.</span></span></span></p>
<div class="packt_infobox"><span><span><span>In all data mining applications, you should first outline what you are looking for before you start designing the methodology that will find it. This will dictate the types of features you are aiming for, the types of algorithms that you can use, and the expectations in the final result.</span></span></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Feature selection</h1>
            </header>

            <article>
                
<p><span><span><span>After initial&#160;modeling, we will often have a large number of features to choose from, but we wish to select only a small subset. There are many possible reasons for this:</span></span></span></p>
<ul>
<li><strong>Reducing complexity</strong><span><span><span>: Many data mining algorithms need significantly more time and resources when the number of features increase. Reducing the number of features is a great way to make an algorithm run faster or with fewer resources.</span></span></span></li>
<li><span><span><span><strong>Reducing noise</strong>: Adding extra features doesn't always lead to better performance. Extra features may confuse the algorithm, finding correlations and patterns in training data that do not have any actual meaning. This is common in both smaller and larger datasets. Choosing only appropriate features is a good way to reduce the chance of random correlations that have no real meaning.</span></span></span></li>
<li><span><span><span><strong>Creating readable models</strong>: While many data mining algorithms will happily compute an answer for models with thousands of features, the results may be difficult to interpret for a human. In these cases, it may be worth using fewer features and creating a model that a human can understand.</span></span></span></li>
</ul>
<p><span><span><span>Some classification algorithms can handle data with issues such as those described before. Getting the data right and getting the features to effectively describe the dataset you are modeling can still assist algorithms.</span></span></span></p>
<p><span><span><span>There are some basic tests we can perform, such as ensuring that the features are at least different. If a feature's values are all the same, it can't give us extra information to perform our data mining.</span></span></span></p>
<p><span><span><span>The <span class="packt_screen"><span><span><span><span><span>VarianceThreshold</span></span></span></span></span></span></span></span></span> transformer <span><span><span>in</span></span></span> <kbd>scikit-learn</kbd>, <span><span><span>for instance, will remove any feature that doesn't have at least a minimum level of variance in the values. To show how this works, we first create a simple matrix using <span class="packt_screen">NumPy</span>:</span></span></span></p>
<pre><span><span><span>import numpy as np<br/></span></span></span><span><span><span>X = np.arange(30).reshape((10, 3))</span></span></span>
</pre>
<p><span><span><span>The result is the numbers 0 to 29, in three columns and 10 rows. This represents a synthetic dataset with 10 samples and three features:</span></span></span></p>
<pre><span><span><span>array([[ 0, 1, 2],<br/></span></span></span><span><span><span>[ 3, 4, 5],<br/></span></span></span><span><span><span>[ 6, 7, 8],<br/></span></span></span><span><span><span>[ 9, 10, 11],<br/></span></span></span><span><span><span>[12, 13, 14],<br/></span></span></span><span><span><span>[15, 16, 17],<br/></span></span></span><span><span><span>[18, 19, 20],<br/></span></span></span><span><span><span>[21, 22, 23],<br/></span></span></span><span><span><span>[24, 25, 26],<br/></span></span></span><span><span><span>[27, 28, 29]])</span></span></span>
</pre>
<p><span><span><span>Then, we set the entire second column/feature to the value 1:</span></span></span></p>
<pre>X[:,1] = 1
</pre>
<p><span><span><span>The result has lots of variance in the first and third rows, but no variance in the second row:</span></span></span></p>
<pre><span><span><span>array([[ 0, 1, 2],<br/></span></span></span><span><span><span>[ 3, 1, 5],<br/></span></span></span><span><span><span>[ 6, 1, 8],<br/></span></span></span><span><span><span>[ 9, 1, 11],<br/></span></span></span><span><span><span>[12, 1, 14],<br/></span></span></span><span><span><span>[15, 1, 17],<br/></span></span></span><span><span><span>[18, 1, 20],<br/></span></span></span><span><span><span>[21, 1, 23],<br/></span></span></span><span><span><span>[24, 1, 26],<br/></span></span></span><span><span><span>[27, 1, 29]])</span></span></span>
</pre>
<p><span><span><span>We can now create a</span></span></span> <kbd>VarianceThreshold</kbd> transformer <span><span><span>and apply it to our dataset:</span></span></span></p>
<pre><span><span><span>from sklearn.feature_selection import VarianceThreshold<br/></span></span></span><span><span><span>vt = VarianceThreshold()<br/></span></span></span><span><span><span>Xt = vt.fit_transform(X)</span></span></span>
</pre>
<p><span><span><span>Now, the result <kbd><span><span><span><span><span>Xt</span></span></span></span></span></kbd> does not have the second column:</span></span></span></p>
<pre><span><span><span>array([[ 0, 2],<br/></span></span></span><span><span><span>[ 3, 5],<br/></span></span></span><span><span><span>[ 6, 8],<br/></span></span></span><span><span><span>[ 9, 11],<br/></span></span></span><span><span><span>[12, 14],<br/></span></span></span><span><span><span>[15, 17],<br/></span></span></span><span><span><span>[18, 20],<br/></span></span></span><span><span><span>[21, 23],<br/></span></span></span><span><span><span>[24, 26],<br/></span></span></span><span><span><span>[27, 29]])</span></span></span>
</pre>
<p><span><span><span>We can observe the variances for each column by printing the <kbd><span><span><span><span><span>vt.variances_</span></span></span></span></span> attribute:</kbd></span></span></span></p>
<pre>print(vt.variances_)
</pre>
<p><span><span><span>The result shows that while the first and third column contains at least some information, the second column had no variance:</span></span></span></p>
<pre>array([ 74.25, 0. , 74.25])
</pre>
<p><span><span><span>A simple and obvious test like this is always good to run when seeing data for the first time. Features with no variance do not add any value to a data mining application; however, they can slow down the performance of the algorithm and reduce the efficacy.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Selecting the best individual features</h1>
            </header>

            <article>
                
<p><span><span><span>If we have a number of features, the problem of finding the best subset is a difficult task. It relates to solving the data mining problem itself, multiple times. As we saw in <a href="lrn-dtmn-py-2e_ch04.html"><span><span>Chapter 4</span></span></a>, <em>Recommending Movi</em></span></span></span><em>es Using Affinity Analysis</em>, subs<span><span><span>et-based tasks increase exponentially as the number of features increase. This exponential growth in the time needed is also true for finding the best subset of features.</span></span></span></p>
<p><span><span>One basic workaround to this problem is not to look for a subset that works well together, rather than just finding the best individual features. This <span><span><span>univariate</span></span></span> feature selection gives us a score based on how well a feature performs by itself. This is usually done for classification tasks, and we generally measure some type of <span>association</span> between a variable and the target class.</span></span></p>
<p><span><span><span>The</span></span></span> scikit-learn pack<span><span><span>age has a number of transformers for performing univariate feature selection. They include</span></span></span> <span class="packt_screen">SelectKBest</span>, which returns the k-best-performing features, and <span class="packt_screen">SelectPercentile</span>, which returns the top <span class="packt_screen">R%</span> of features. In both cases<span><span><span>, there are a number of methods of computing the quality of a feature.</span></span></span></p>
<p><span><span><span>There are many different methods to compute how effectively a single feature correlates with a class value. A commonly used method is the <span class="packt_screen">chi-squared (<em>χ2</em>) test.</span> Other methods include mutual information and entropy.</span></span></span></p>
<p><span><span><span>We can observe single-feature tests in action using our <span><span><span><span><span>Adult</span></span></span></span></span> dataset. First, we extract a dataset and class values from our <span class="packt_screen">pandas</span> <span><span><span><span><span>DataFrame</span></span></span></span></span>. We get a selection of the features:</span></span></span></p>
<pre><span><span><span>X = adult[["Age", "Education-Num", "Capital-gain", "Capital-loss", "Hours-per-week"]].values</span></span></span>
</pre>
<p><span><span><span>We will also create a target class array by testing whether the <span><span><span><span><span><span class="packt_screen">Earnings-Raw</span></span></span></span></span></span> value is above $50,000 or not. If it is, the class will be <span><span><span><span><span>True</span></span></span></span></span>. Otherwise, it will be <span><span><span><span><span>False</span></span></span></span></span>. Let's look at the code:</span></span></span></p>
<pre>y = (adult["Earnings-Raw"] == ' &gt;50K').values
</pre>
<p><span><span><span>Next, we create our transformer using the</span></span></span> chi2 function and a <span><span><span><span class="packt_screen"><span><span><span><span><span>SelectKBest</span></span></span></span></span></span></span></span></span> transformer<span><span><span><span class="packt_screen">:</span></span></span></span></p>
<pre><span><span><span>from sklearn.feature_selection import SelectKBest<br/></span></span></span><span><span><span>from sklearn.feature_selection import chi2<br/></span></span></span><span><span><span>transformer = SelectKBest(score_func=chi2, k=3)</span></span></span>
</pre>
<p><span><span><span>Running <kbd><span><span><span><span><span>fit_transform</span></span></span></span></span></kbd> will call fit and then transform with the same dataset.<br/>
The result will create a new dataset, choosing only the best three features.<br/>
Let's look at the code:</span></span></span></p>
<pre>Xt_chi2 = transformer.fit_transform(X, y)
</pre>
<p><span><span><span>The resulting matrix now only contains three features. We can also get the scores<br/>
for each column, allowing us to find out which features were used. Let's look at<br/>
the code:</span></span></span></p>
<pre>print(transformer.scores_)
</pre>
<p><span><span><span>The printed results give us these scores:</span></span></span></p>
<pre><span><span><span>[ 8.60061182e+03 2.40142178e+03 8.21924671e+07 1.37214589e+06</span></span></span><span><span><span>6.47640900e+03]</span></span></span>
</pre>
<p><span><span><span>The highest values are for the first, third, and fourth columns Correlates to the <span><span><span><span><span>Age</span></span></span></span></span>, <span><span><span><span><span>Capital-Gain</span></span></span></span></span>, and <span><span><span><span><span>Capital-Loss</span></span></span></span></span> features. Based on a univariate feature selection, these are the best features to choose.</span></span></span></p>
<div class="packt_tip">If you'd like to find out more about the features in the Adult dataset, take a look at the <span class="packt_screen">adult.names</span> file that comes with the dataset and the academic paper it references.</div>
<p><span><span><span>We could also implement other correlations, such as the Pearson's correlation coefficient. This is implemented in <span class="packt_screen">SciPy</span>, a library used for scientific computing (<span class="packt_screen">scikit-learn</span> uses it as a base).</span></span></span></p>
<div class="packt_tip">If scikit-learn is working on your computer, so is SciPy. You do not need to install anything further to get this sample working.</div>
<p><span><span><span>First, we import the <kbd><span><span><span><span><span>pearsonr</span></span></span></span></span></kbd> function from <span class="packt_screen">SciPy</span>:</span></span></span></p>
<pre>from scipy.stats import pearsonr
</pre>
<p><span><span><span>The preceding function almost fits the interface nee</span></span></span>ded to be used in scikit-learn's univariate transformers. The function needs to accept two arrays (x and y in our example) as parameters and returns two arrays, the scores for each feature and the corresponding p-values. The chi2 function we used earlier only uses the required interface, which allowed us to just pass it directly to <span><span><span><span class="packt_screen">SelectKBest.</span></span></span></span></p>
<p><span><span><span>The <span class="packt_screen"><span><span><span><span><span>pearsonr</span></span></span></span></span></span></span></span></span> function in SciPy <span><span><span>accepts two arrays; however, the X array it accepts is only one dimension. We will write a wrapper function that allows us to use this for multivariate arrays like the one we have. Let's look at the code:</span></span></span></p>
<pre>def multivariate_pearsonr(X, y):<br/><span><span><span>    scores, pvalues = [], []<br/></span></span></span><span><span><span>    for column in range(X.shape[1]):<br/></span></span></span><span><span><span>        # Compute the Pearson correlation for this column only<br/>        cur_score, cur_p = pearsonr(X[:,column], y)<br/>        # Record both the score and p-value.<br/></span></span></span>        scores.append(abs(cur_score))<br/><span><span><span>        pvalues.append(cur_p)<br/></span></span></span>    return (np.array(scores), np.array(pvalues))
</pre>
<div class="packt_infobox"><span><span><span>The Pearson value could be between</span></span></span> <span><span><span><span><span><span><span><span>-1</span></span></span></span></span></span></span></span> <span><span><span>and</span></span></span> <span><span><span><span><span><span><span><span>1</span></span></span></span></span></span></span></span><span><span><span>. A value of</span></span></span> <span><span><span><span><span><span><span><span>1</span></span></span></span></span></span></span></span> <span><span><span>implies a perfect correlation between two variables, while a value of -1 implies a perfect negative correlation, that is, high values in one variable give low values in the other and vice versa. Such features are really useful to have. For this reason, we have stored the absolute value in the</span></span></span> <span><span><span><span><span><span><span><span>scores</span></span></span></span></span></span></span></span> <span><span><span>array, rather than the original, signed value.</span></span></span></div>
<p><span><span><span>Now, we can use the transformer class as before to rank the features using the Pearson correlation coefficient:</span></span></span></p>
<pre><span><span><span>transformer = SelectKBest(score_func=multivariate_pearsonr, k=3)<br/></span></span></span><span><span><span>Xt_pearson = transformer.fit_transform(X, y)<br/></span></span></span><span><span><span>print(transformer.scores_)</span></span></span>
</pre>
<p><span><span><span>This returns a different set of features! The features chosen this way are the first, second, and fifth columns: the <span><span><span><span><span><span class="packt_screen">Age</span></span></span></span></span></span>, <span><span><span><span><span><span class="packt_screen">Education</span></span></span></span></span></span>, and <span><span><span><span><span><span class="packt_screen">Hours-per-week</span></span></span></span></span></span> worked. This shows that there is not a definitive answer to what the best features are— it depends on the metric used and the process undertaken.</span></span></span></p>
<p><span><span><span>We can see which feature set is better by running them through a classifier. Keep in mind that the results only indicate which subset is better for a particular classifier and/or feature combination—there is rarely a case in data mining where one method is strictly better than another in all cases! Let's look at the code:</span></span></span></p>
<pre><span><span><span>from sklearn.tree import DecisionTreeClassifier<br/></span></span></span><span><span><span>from sklearn.cross_validation import cross_val_score<br/></span></span></span><span><span><span>clf = DecisionTreeClassifier(random_state=14)<br/></span></span></span><span><span><span>scores_chi2 = cross_val_score(clf, Xt_chi2, y, scoring='accuracy')<br/></span></span></span><span><span><span>scores_pearson = cross_val_score(clf, Xt_pearson, y, scoring='accuracy')<br/><br/>print("Chi2 score: {:.3f}".format(scores_chi2.mean()))<br/>print("Pearson score: {:.3f}".format(scores_pearson.mean()))</span></span></span>
</pre>
<p>The chi2 average here is 0.83, while the Pearson score is lower at 0.77. For this combination, chi2 returns better results!</p>
<p>It is worth remembering the goal of this particular data mining activity: predicting wealth. Using a combination of good features and feature selection, we can achieve 83 percent accuracy using just three features of a person!</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Feature creation</h1>
            </header>

            <article>
                
<p><span><span><span>Sometimes, just selecting features from what we have isn't enough. We can create features in different ways from features we already have. The one-hot encoding method we saw previously is an example of this. Instead of having category features with options <span><span>A</span></span>, <span><span>B,</span></span> and <span><span>C</span></span>, we would create three new features <em>Is it A?</em>, <em>Is it B?</em>, <em>Is it C?</em>.</span></span></span></p>
<p><span><span><span>Creating new features may seem unnecessary and to have no clear benefit—after all, the information is already in the dataset and we just need to use it. However, some algorithms struggle when features correlate significantly, or if there are redundant features. They may also struggle if there are redundant features.&#160;</span></span></span><span><span><span>For this reason, there are various ways to create new features from the features we already have.</span></span></span></p>
<p><span><span><span>We are going to load a new dataset, so now is a good time to start a new Jupyter<span class="packt_screen">&#160;</span></span></span></span>Notebook<span><span><span>. Download the <span class="packt_screen">Advertisements</span> dataset from <a href="http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements" target="_blank"><span><span><span><span><span>http://archive.ics.uci.edu/ml/datasets/Internet+Advertise</span></span></span></span></span>ments</a> and save it to your <span><span><span><span><span>Data</span></span></span></span></span> folder.</span></span></span></p>
<p><span><span><span>Next, we need to load the dataset with <span class="packt_screen">pandas</span>. First, we set the data's filename as always:</span></span></span></p>
<pre><span><span><span>import os<br/></span></span></span><span><span><span>import numpy as np<br/></span></span></span><span><span><span>import pandas as pd<br/></span></span></span><span><span><span>data_folder = os.path.join(os.path.expanduser("~"), "Data")<br/></span></span></span><span><span><span>data_filename = os.path.join(data_folder, "Ads", "ad.data")</span></span></span>
</pre>
<p><span><span><span>There are a couple of issues with this dataset that stop us from loading it easily. You can see these issues by trying to load the dataset with <kbd>pd.read_csv</kbd>.&#160;</span></span></span><span><span><span>First, the first few features are numerical, but <span class="packt_screen">pandas</span> will load them as strings. To fix this, we need to write a converting function that will convert strings to numbers if possible. Otherwise, we will get a <strong>Not a Number</strong> (<span><span><span><span class="packt_screen">NaN)</span></span></span></span> - <span><span><span>an invalid value</span></span></span>, which is a special value that indicates that the value could not be interpreted as a number. It is similar to <span><span>none</span></span> or <span><span>null</span></span> in other programming languages.</span></span></span></p>
<p><span><span><span>Another issue with this dataset is that some values are missing. These are represented in the dataset using the string <span><span><span><span><span><span class="packt_screen">?</span></span></span></span></span></span>. Luckily, the question mark doesn't convert to a float, so we can convert those to <span class="packt_screen">NaNs</span> using the same concept. In further chapters, we will look at other ways of dealing with missing values like this.</span></span></span></p>
<p><span><span><span>We will create a function that will do this conversion for us. It attempts to convert the number to a float, and if that fails, it returns NumPy's special NaN value that can be stored in place of a float:</span></span></span></p>
<pre><span><span><span>def convert_number(x):<br/>    try:<br/></span></span></span><span><span><span>        return float(x)<br/></span></span></span><span><span><span>    except ValueError:<br/>        return np.nan</span></span></span>
</pre>
<p><span><span><span>Now, we create a dictionary for the conversion. We want to convert all of the features to floats:</span></span></span></p>
<pre>converters = {}<br/>for i in range(1558):<br/>    converters[i] = convert_number
</pre>
<p><span><span><span>Also, we want to set the final column, the class, (column index #1558)&#160;to a binary feature. In the Adult dataset, we created a new feature for this. In the dataset, we will convert the feature while we load it:</span></span></span></p>
<pre>converters[1558] = lambda x: 1 if x.strip() == "ad." else 0
</pre>
<p><span><span><span>Now we can load the dataset using <kbd><span><span><span><span><span>read_csv</span></span></span></span></span></kbd>. We use the converters parameter to pass our custom conversion into <span class="packt_screen">pandas</span>:</span></span></span></p>
<pre>ads = pd.read_csv(data_filename, header=None, converters=converters)
</pre>
<p><span><span><span>The resulting dataset is quite large, with 1,559 features and more than 3,000 rows. Here are some of the feature values, the first five, printed by</span></span></span> inserting <kbd>ads.head()</kbd>&#160;<span><span><span>into a new cell:</span></span></span></p>
<p><span><img class=" image-border" src="images/B06162_05_01.png"/>&#160;</span></p>
<p><span><span><span>This dataset describes images on websites, with the goal of determining whether a given image is an advertisement or not.</span></span></span></p>
<p><span><span><span>The features in this dataset are not described well by their headings. There are two files accompanying the <span class="packt_screen"><span><span><span><span><span>ad.data</span></span></span></span></span></span> file that have more information:</span></span></span> <kbd>ad.DOCUMENTATION</kbd> <span><span><span>and</span></span></span> <kbd>ad.names</kbd><span><span><span>. The first three features are the height, width, and ratio of the image size. The final feature is 1 if it is an advertisement and 0 if it is not.</span></span></span></p>
<p><span><span><span>The other features are 1 for the presence of certain words in the URL, alt text, or caption of the image. These words, such as the word <span><span>sponsor</span></span>, are used to determine if the image is likely to be an advertisement. Many of the features overlap considerably, as they are combinations of other features. Therefore, this dataset has a lot of redundant information.</span></span></span></p>
<p><span><span><span>With our dataset loaded in</span></span></span> <kbd>pandas</kbd><span><span><span>, we will now extract the <kbd><span><span><span><span><span>x</span></span></span></span></span></kbd> and <kbd><span><span><span><span><span>y</span></span></span></span></span></kbd> data for our classification algorithms. The <kbd><span><span><span><span><span>x</span></span></span></span></span></kbd> matrix will be all of the columns in our Dataframe, except for the last column. In contrast, the <kbd>y</kbd> array will be only that last column, feature <kbd>1558.</kbd> Before that though, we simplify our dataset (just for this chapter's sake) by dropping any row with a NaN value. Let's look at the code:</span></span></span></p>
<pre><span><span><span><span>ads.dropna(inplace=True)</span><br/>X = ads.drop(1558, axis=1).values<br/></span></span></span><span><span><span>y = ads[1558]<br/></span></span></span>
</pre>
<p>More than 1000 rows are dropped due to this command, which is fine for our exercise. For real-world applications, you don't want to discard data if you can help it--instead, you can use interpolation or value replacing to fill the NaN values. As an example, you can replace any missing value with the average for that column.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Principal Component Analysis</h1>
            </header>

            <article>
                
<p><span><span><span>In some datasets, features heavily correlate with each other. For example, the speed and the fuel consumption would be heavily correlated in a go-kart with a single gear. While it can be useful to find these correlations for some applications, data mining algorithms typically do not need the redundant information.</span></span></span></p>
<p><span><span><span>The ads dataset has heavily correlated features, as many of the keywords are repeated across the alt text and caption.</span></span></span></p>
<p><span><span><span>The <span><span><span>Principal Component Analysis</span></span></span> (<span><span><span>PCA</span></span></span>) algorithm aims to find combinations of features that describe the dataset in less information. It aims to discover <em>principal components</em>, which are features that do not correlate with each other and explain the information—specifically the <span><span><span>variance</span></span></span>—of the dataset. What this means is that we can often capture most of the information in a dataset in fewer features.</span></span></span></p>
<p><span><span><span>We apply PCA just like any other transformer. It has one key parameter, which is the number of components to find. By default, it will result in as many features as you have in the original dataset. However, these principal components are ranked—the first feature explains the largest amount of the variance in the dataset, the second a little less, and so on. Therefore, finding just the first few features is often enough to explain much of the dataset. Let's look at the code:</span></span></span></p>
<pre><span><span><span>from sklearn.decomposition import PCA<br/></span></span></span><span><span><span>pca = PCA(n_components=5)<br/></span></span></span><span><span><span>Xd = pca.fit_transform(X)</span></span></span>
</pre>
<p><span><span><span>The resulting matrix, <span><span><span><span><span><span class="packt_screen">Xd</span></span></span></span></span></span>, has just five features. However, let's look at the amount of variance that is explained by each of these features:</span></span></span></p>
<pre><span><span><span>np.set_printoptions(precision=3, suppress=True)<br/></span></span></span><span><span><span>pca.explained_variance_ratio_</span></span></span>
</pre>
<p><span><span><span>The result, <kbd><span><span><span><span><span>array([ 0.854, 0.145, 0.001, 0. , 0. ])</span></span></span></span></span></kbd>, shows us that the first feature accounts for 85.4 percent of the variance in the dataset, the second accounts for 14.5 percent, and so on. By the fourth feature, less than one-tenth of a percent of the variance is contained in the feature. The other 1,553 features explain even less (this is an ordered array).</span></span></span></p>
<p><span><span><span>The downside to transforming data with PCA is that these features are often complex combinations of the other features. For example, the first feature of the preceding code <span><span>starts</span></span> with <kbd><span><span><span><span><span>[-0.092, -0.995, -0.024]</span></span></span></span></span>,</kbd> that is, multiply the first feature in the original dataset by -0.092, the second by -0.995, the third by -0.024. This <span><span>feature</span></span> has 1,558 values of this form, one for each of the original datasets (although many are zeros). Such features are indistinguishable by humans and it is hard to glean much relevant information from without a lot of experience working with them.</span></span></span></p>
<p><span><span><span>Using PCA can result in models that not only approximate the original dataset,&#160;but can also improve the performance in classification tasks:</span></span></span></p>
<pre><span><span><span>clf = DecisionTreeClassifier(random_state=14)<br/></span></span></span><span><span><span>scores_reduced = cross_val_score(clf, Xd, y, scoring='accuracy')</span></span></span>
</pre>
<p><span><span><span>The resulting score is 0.9356,&#160; which is (slightly) higher than our original model's score. PCA won't always give a benefit like this, but it does more often than not.</span></span></span></p>
<div class="packt_tip"><span><span><span>We are using PCA here to reduce the number of features in our dataset. As a general rule, you shouldn't use it to reduce overfitting in your data mining experiments. The reason for this is that PCA doesn't take classes into account. A better solution is to use regularization. An introduction, with code, is available at</span></span></span> <span><span><span><span><span><a href="http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/"><span><span><span>http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/</span></span></span></a></span></span></span></span></span></div>
<p><span><span><span>Another advantage is that PCA allows you to plot datasets that you otherwise couldn't easily visualize. For example, we can plot the first two features returned by PCA.</span></span></span></p>
<p><span><span>First, we tell our Notebook to display plots inline:</span></span></p>
<pre><span><span><span>%matplotlib inline<br/></span></span></span><span><span><span>from matplotlib import pyplot as plt</span></span></span>
</pre>
<p><span><span><span>Next, we get all of the distinct classes in our dataset (there are only two: <span><span><span><span><span>is ad</span></span></span></span></span> or&#160;<span><span><span><span><span>not ad</span></span></span></span></span>):</span></span></span></p>
<pre>classes = set(y)
</pre>
<p><span><span><span>We also assign colors to each of these classes:</span></span></span></p>
<pre>colors = ['red', 'green']
</pre>
<p><span><span><span>We use zip to iterate over both lists at the same time, then extract all samples from that class, and plot them with the color appropriate to the class:</span></span></span></p>
<pre>for cur_class, color in zip(classes, colors):<br/>mask = (y == cur_class)<br/>    plt.scatter(Xd[mask,0], Xd[mask,1], marker='o', color=color, label=int(cur_class))
</pre>
<p><span><span><span>Finally, outside the loop, we create a legend and show the graph, showing where the samples from each class appear:</span></span></span></p>
<pre><span><span><span>plt.legend()<br/></span></span></span><span><span><span>plt.show()</span></span></span>
</pre>
<div class="CDPAlignCenter CDPAlign"><img height="317" width="405" class=" image-border" src="images/B06162_05_02.png"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Creating your own transformer</h1>
            </header>

            <article>
                
<p><span><span><span>As the complexity and type of dataset changes, you might find that you can't find an existing feature extraction transformer that fits your needs. We will see an example of this in <a href="lrn-dtmn-py-2e_ch07.html" target="_blank"><span><span>Chapter 7</span></span></a>, <em>Follow Recommendations Using Graph Mining</em>, where we create new features from graphs.</span></span></span></p>
<p><span><span><span>A transformer is akin to a converting function. It takes data of one form as input and returns data of another form as output. Transformers can be trained using some training dataset, and these trained parameters can be used to convert testing data.</span></span></span></p>
<p><span><span><span>The transformer API is quite simple. It takes data of a specific format as input and returns data of another format (either the same as the input or different) as output. Not much else is required of the programmer.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">The transformer API</h1>
            </header>

            <article>
                
<p><span><span><span>Transformers have two key functions:</span></span></span></p>
<ul>
<li><span><span><span><kbd><span><span><span><span><span>fit()</span></span></span></span></span>:</kbd> This takes a training set of data as input and sets internal parameters</span></span></span></li>
<li><span><span><span><kbd><span><span><span><span><span>transform()</span></span></span></span></span>:</kbd> This performs the transformation itself. This can take either the training dataset, or a new dataset of the same format</span></span></span></li>
</ul>
<p><span><span><span>Both <kbd><span><span><span><span><span>fit()</span></span></span></span></span></kbd> and <kbd><span><span><span><span><span>transform()</span></span></span></span></span></kbd> functions should take the same data type as input, but <kbd><span><span><span><span><span>transform()</span></span></span></span></span></kbd> can return data of a different type while <kbd>fit()</kbd> always returns <span class="packt_screen">self</span>.</span></span></span></p>
<p><span><span><span>We are going to create a trivial transformer to show the API in action. The transformer will take a NumPy array as input, and discretize it based on the mean. Any value higher than the mean (of the training data) will be given the value 1 and any value lower or equal to the mean will be given the value 0.</span></span></span></p>
<p><span><span><span>We did a similar transformation with the Adult dataset using pandas: we took the <span class="packt_screen"><span><span><span><span><span>Hours-per-week</span></span></span></span></span> feature</span> and created a <span><span><span><span><span><span class="packt_screen">LongHours</span></span></span></span></span></span> feature if the value was more than 40 hours per week. This transformer is different for two reasons. First, the code will conform to the <span class="packt_screen">scikit-learn</span> API, allowing us to use it in a pipeline. Second, the code will <span><span>learn</span></span> the mean, rather than taking it as a fixed value (such as 40 in the <span><span><span><span><span><span class="packt_screen">LongHours</span></span></span></span></span></span> example).</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Implementing a Transformer</h1>
            </header>

            <article>
                
<p><span><span><span>To start, open up t</span></span></span>he Jupyter Notebook <span><span><span>that we used for the Adult dataset. Then, click on the <span><span class="packt_screen">Cell</span> menu</span> item and choose <span><span class="packt_screen">Run All</span></span>. This will rerun all of the cells and ensure that the notebook is up to date.</span></span></span></p>
<p><span><span><span>First, we import the <span><span><span><span><span><span class="packt_screen">TransformerMixin</span></span></span></span></span></span>, which sets the API for us. While Python doesn't have strict interfaces (as opposed to languages like Java), using a <span><span><span><span><span>mixin&#160;</span></span></span></span></span>like this allows <span class="packt_screen">scikit-learn</span> to determine that the class is actually a transformer. We also need to import a function that checks the input is of a valid type. We will use that soon.</span></span></span></p>
<p><span><span><span>Let's look at the code:</span></span></span></p>
<pre><span><span><span>from sklearn.base import TransformerMixin<br/></span></span></span><span><span><span>from sklearn.utils import as_float_array</span></span></span>
</pre>
<p>Let's take a look at our class in entirety, and then we will revisit some of the details:</p>
<pre>class MeanDiscrete(TransformerMixin):<br/>    def fit(self, X, y=None):<br/>        X = as_float_array(X)<br/>        self.mean = X.mean(axis=0)<br/>        return self<br/><br/>    def transform(self, X, y=None):<br/>        X = as_float_array(X)<br/>        assert X.shape[1] == self.mean.shape[0]<br/>        return X &gt; self.mean
</pre>
<p>Our class will learn the mean for each feature in the fit method, by computing&#160;<kbd>X.mean(axis=0)</kbd>, which is then stored as an object attribute. After that, the <span class="packt_screen">fit</span> function returns <span class="packt_screen">self</span>, conforming to the API (scikit-learn uses this to allow for chaining function calls).</p>
<p>After fitting, the transform function takes a matrix with the same number of features (confirmed by the <kbd>assert</kbd> statement), and simply returns which values are more than the mean for a given feature.</p>
<p>Now that our class is built, we can now create an instance of this class and use it to transform our <span><span><span><span><span><span class="packt_screen">X</span></span></span></span></span></span> array:</p>
<pre>mean_discrete = MeanDiscrete()<br/><span><span><span>X_mean = mean_discrete.fit_transform(X)<br/></span></span></span>
</pre>
<p>Take a shot at implementing this Transformer into a workflow, both using a Pipeline and without. You'll see that by conforming to the Transformer API, it is quite simple to use in place of a built-in scikit-learn Transformer object.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Unit testing</h1>
            </header>

            <article>
                
<p><span><span><span>When creating your own functions and classes, it is always a good idea to do unit testing. Unit testing aims to test a single unit of your code. In this case, we want to test that our transformer does as it needs to do.</span></span></span></p>
<p><span><span><span>Good tests should be independently verifiable. A good way to confirm the legitimacy of your tests is by using another computer language or method to perform the calculations. In this case, I used Excel to create a dataset, and then computed the mean for each cell. Those values were then transferred to the unit test.</span></span></span></p>
<p><span><span><span>Unit tests should also, generally, be small and quick to run. Therefore, any data used should be of a small size. The dataset I used for creating the tests is stored in the <span><span><span><span><span><span class="packt_screen">Xt</span></span></span></span></span></span> variable from earlier, which we will recreate in our test. The mean of these two features is 13.5 and 15.5, respectively.</span></span></span></p>
<p><span><span><span>To create our unit test, we import the <kbd><span><span><span><span><span>assert_array_equal</span></span></span></span></span></kbd> function from <span class="packt_screen">NumPy's</span> testing, which checks whether two arrays are equal:</span></span></span></p>
<p><kbd><span><span><span>from numpy.testing import assert_array_equal</span></span></span></kbd></p>
<p><span><span><span>Next, we create our function. It is important that the test's name starts with <span><span><span><span><span>test_</span></span></span></span></span>,<br/>
as this nomenclature is used for tools that automatically find and run tests. We also set up our testing data:</span></span></span></p>
<pre><span><span><span>def test_meandiscrete():<br/></span></span></span><span><span><span>    X_test = np.array([[ 0, 2],<br/></span></span></span><span><span><span>                       [ 3, 5],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[ 6, 8],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[ 9, 11],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[12, 14],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[15, 17],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[18, 20],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[21, 23],<br/></span></span></span><span><span><span>                       </span></span></span><span><span><span>[24, 26],<br/></span></span></span><span><span><span>                       </span></span></span>[27, 29]])<br/><span><span><span>    # Create an instance of our Transformer<br/>    mean_discrete = MeanDiscrete()<br/></span></span></span><span><span><span>    mean_discrete.fit(X_test)<br/>    # Check that the computed mean is correct<br/></span></span></span>    assert_array_equal(mean_discrete.mean, np.array([13.5, 15.5]))<br/>    # Also test that transform works properly<br/><span><span><span>    X_transformed = mean_discrete.transform(X_test)<br/></span></span></span><span><span><span>    X_expected = np.array([[ 0, 0],<br/></span></span></span><span><span><span>                           </span></span></span><span><span><span>[ 0, 0], <br/></span></span></span><span><span><span>                           [ 0, 0],<br/></span></span></span><span><span><span>                           [ 0, 0],<br/></span></span></span><span><span><span>                           [ 0, 0],<br/></span></span></span><span><span><span>                           [ 1, 1],<br/></span></span></span><span><span><span>                           [ 1, 1],<br/></span></span></span><span><span><span>                           [ 1, 1],<br/></span></span></span><span><span><span>                           [ 1, 1],<br/></span></span></span><span><span><span>                           [ 1, 1]])<br/></span></span></span>    assert_array_equal(X_transformed, X_expected)
</pre>
<p><span><span><span>We can run the test by simply running the function itself:</span></span></span></p>
<pre>test_meandiscrete()
</pre>
<p><span><span><span>If there was no error, then the test ran without an issue! You can verify this by changing some of the tests to deliberately make values incorrect, and confirming that the test fails. Remember to change them back so that the test passes!</span></span></span></p>
<p><span><span><span>If we had multiple tests, it would be worth using a testing framework, like&#160;<span class="packt_screen">py.test</span> or&#160;<span><span><span><span><span><span class="packt_screen">nose</span></span></span></span></span></span> to run our tests. Using a framework like this is beyond the scope of this book, but they manage running tests, recording failures, and providing feedback to you, as a programmer, to help you improve your code.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p><span><span><span>Now that we have a tested transformer, it is time to put it into action. Using what we have learned so far, we create a <span><span><span><span><span>Pipeline</span></span></span></span></span>, set the first step to the <span><span><span><span><span><span class="packt_screen">MeanDiscrete</span></span></span></span></span></span> transformer, and the second step to a Decision Tree Classifier. We then run a cross-validation and print out the result. Let's look at the code:</span></span></span></p>
<pre>from sklearn.pipeline import Pipeline<br/>pipeline = Pipeline([('mean_discrete', MeanDiscrete()), ('classifier', DecisionTreeClassifier(random_state=14))])<br/>scores_mean_discrete = cross_val_score(pipeline, X, y, scoring='accuracy')<br/>print("Mean Discrete performance: {0:.3f}".format(scores_mean_discrete.mean()))
</pre>
<p><span><span><span>The result is 0.917, which is not as good as before, but very good for a simple binary feature model.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>


  <div id="sbo-rt-content"><section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p><span><span><span>In this chapter, we looked at features and transformers and how they can be used in the data mining pipeline. We discussed what makes a good feature and how to algorithmically choose good features from a standard set. However, creating good features is more art than science and often requires domain knowledge and experience.</span></span></span></p>
<p><span><span><span>We then created our own transformer using an interface that allows us to use it in</span></span></span> scikit-learn's <span><span><span>helper functions. We will be creating more transformers in later chapters so that we can perform effective testing using existing functions.</span></span></span></p>
<p>To take the lessons learned in this chapter further, I recommend signing up to the online data mining competition website <a href="http://Kaggle.com">Kaggle.com</a> and trying some of the competitions. Their recommended starting place is the Titanic dataset, which allows you to practice the feature creation aspects of this chapter. Many of the features are not numerical, requiring you to convert them to numerical features before applying a data mining algorithm.</p>
<p><span><span><span>In the next chapter, we use feature extraction on a corpus of text documents. There are many transformers and feature types for text, each with their advantages and disadvantages.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </div>
</body>
</html>