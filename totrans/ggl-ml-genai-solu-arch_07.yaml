- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Building Custom ML Models on Google Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Google Cloud上构建自定义ML模型
- en: In the last chapter, we implemented AI/ML workloads by letting Google do all
    of the work for us. Now is the point at which we’re going to elevate our knowledge
    and skills to an expert level by building our own models from scratch on Google
    Cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过让谷歌为我们完成所有工作来实现AI/ML工作负载。现在，我们将通过在Google Cloud上从头开始构建自己的模型来提升我们的知识和技能到专家水平。
- en: We will use popular software libraries that are commonly used in data science
    projects, and we will start implementing some of the concepts we discussed in
    previous chapters, such as **unsupervised ML** (**UML**) and **supervised ML**
    (**SML**), including clustering, regression, and classification.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在数据科学项目中常用的流行软件库，并将开始实现我们在前几章中讨论的一些概念，例如**无监督机器学习**（**UML**）和**监督机器学习**（**SML**），包括聚类、回归和分类。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Background information – libraries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背景信息 – 库
- en: UML with scikit-learn on Vertex AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上使用scikit-learn实现UML
- en: Implementing a regression model with scikit-learn on Vertex AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上使用scikit-learn实现回归模型
- en: Implementing a classification model with XGBoost on Vertex AI
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上使用XGBoost实现分类模型
- en: Background information – libraries
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景信息 – 库
- en: Before we dive in and start building our own models, let’s take a moment to
    discuss some of the software libraries we will use in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨并开始构建自己的模型之前，让我们花一点时间讨论一下本章中我们将使用的软件库。
- en: Definition
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: A software library is a collection of code and data that includes useful functions
    and tools for specific types of programming tasks. When common types of programming
    tasks are identified in a given industry, such as data manipulation or implementing
    complex mathematical equations, then usually somebody will eventually create a
    library that contains the code and other resources required to perform those tasks.
    The library can then easily be used by others to achieve those same tasks and
    potentially extended to add more functionality over time, rather than everybody
    needing to write the code to perform those common tasks over and over again. Without
    libraries, programmers would have to build everything from scratch all of the
    time and would waste a lot of time on rudimentary programming tasks. In this chapter,
    we will use libraries such as scikit-learn, Matplotlib, and XGBoost. Later in
    this book, we will use other libraries such as TensorFlow and PyTorch, and we
    will describe those libraries in more detail in their respective chapters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 软件库是一组代码和数据，包括用于特定类型编程任务的实用函数和工具。当在某个行业中识别出常见的编程任务类型，例如数据处理或实现复杂的数学方程式时，通常有人最终会创建一个包含执行这些任务所需的代码和其他资源的库。然后，其他人可以轻松地使用这个库来完成相同的任务，并可能随着时间的推移扩展以添加更多功能，而不是每个人都需要反复编写执行这些常见任务的代码。没有库，程序员将不得不每次从头开始构建一切，并将浪费大量时间在基础编程任务上。在本章中，我们将使用scikit-learn、Matplotlib和XGBoost等库。在本书的后期，我们将使用其他库，如TensorFlow和PyTorch，并在各自的章节中更详细地描述这些库。
- en: scikit-learn
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn
- en: scikit-learn, also referred to as `sklearn`, is an open source Python library
    that has lots of useful data science tools and practice datasets built in. It’s
    like a “Swiss Army knife” for data science, and it’s a popular starting point
    for budding data scientists to begin working on ML projects because it’s relatively
    easy to start using, as you will see in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn，也称为`sklearn`，是一个内置了许多有用的数据科学工具和实践数据集的开源Python库。它就像是数据科学的“瑞士军刀”，对于初涉数据科学领域的从业者来说，它是开始机器学习项目的热门起点，因为它相对容易上手，正如你将在本章中看到的。
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Matplotlib
- en: In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), we discussed typical stages
    that exist in almost all data science projects, and how one of those stages focuses
    on data exploration. We also discussed that “visualization” is typically an important
    part of the data exploration phase, in which data scientists and engineers use
    tools to create visual representations of the characteristics of their datasets.
    These visual representations, such as graphs, can help data scientists and engineers
    build a better understanding of the datasets, as well as how those datasets are
    affected by experiments and transformations that are performed during data science
    projects. Data scientists and engineers also often want to build visualizations
    that represent other aspects of their data science project activities, such as
    increases or decreases in metrics that help determine how a model is performing.
    In the words of Matplotlib’s developers, “*Matplotlib is a comprehensive library
    for creating static, animated, and interactive visualizations in Python.*” As
    such, Matplotlib is a commonly used library for creating visual representations
    of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中，我们讨论了几乎存在于所有数据科学项目中的典型阶段，以及其中有一个阶段专注于数据探索。我们还讨论了“可视化”通常是数据探索阶段的一个重要部分，在这个阶段，数据科学家和工程师使用工具来创建其数据集特性的视觉表示。这些视觉表示，如图表，可以帮助数据科学家和工程师更好地理解数据集，以及这些数据集如何受到在数据科学项目中执行的各种实验和转换的影响。数据科学家和工程师还经常希望构建表示其数据科学项目活动其他方面的可视化，例如帮助确定模型性能的指标的增加或减少。用Matplotlib的开发者的话说，“*Matplotlib是一个用于在Python中创建静态、动画和交互式可视化的综合性库。*”因此，Matplotlib是创建数据视觉表示的常用库。
- en: pandas
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: pandas
- en: pandas is a Python library for data manipulation and analysis. It is used for
    both data exploration and for performing transformations on data. For example,
    with pandas, we could read data in from a file or other data source, preview a
    subset of the data to understand what it contains, and perform statistical analysis
    on the dataset. Then, we could also use pandas to make changes to the data to
    prepare it for training an ML model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: pandas是一个用于数据操作和分析的Python库。它用于数据探索以及执行数据转换。例如，使用pandas，我们可以从文件或其他数据源读取数据，预览数据的一个子集以了解其内容，并对数据集进行统计分析。然后，我们还可以使用pandas对数据进行更改，以准备训练机器学习模型。
- en: XGBoost
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), we discussed how the concept
    of Gradient Descent is commonly used in the ML model training process. XGBoost
    is a popular ML library that is based on the concept of **Gradient Boosting**,
    which uses an **ensemble** approach that combines many small models (often referred
    to as **weak learners**) to create a better overall prediction model. In the case
    of Gradient Boosting, each iteration in the training process trains a small model.
    And, as is the case in almost every model training process, the resulting model
    will make some incorrect predictions. The next iteration in the Gradient Boosting
    process then trains a model on those **residual errors** made by the previous
    model. This helps to “boost” the training process in each subsequent iteration,
    with the intention of creating a stronger prediction model overall. XGBoost, which
    stands for Extreme Gradient Boosting, overcomes the sequential limitations of
    previous Gradient Boosting algorithms, and it can train thousands of weak learners
    in parallel. We will describe how XGBoost works in more detail later in this chapter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中，我们讨论了梯度下降的概念在机器学习模型训练过程中的常用性。XGBoost是一个基于**梯度提升**概念的流行机器学习库，它使用**集成**方法，结合了许多小型模型（通常被称为**弱学习器**）来创建一个更好的整体预测模型。在梯度提升的情况下，训练过程中的每一迭代都会训练一个小型模型。几乎在每一个模型训练过程中，产生的模型都会做出一些错误的预测。在梯度提升过程中的下一迭代将基于前一个模型所犯的**残差误差**来训练模型。这有助于在后续的每一迭代中“提升”训练过程，目的是创建一个更强的预测模型。XGBoost，代表极端梯度提升，克服了先前梯度提升算法的顺序限制，并且可以并行训练数千个弱学习器。我们将在本章后面更详细地描述XGBoost的工作原理。
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In general, the concept of “ensemble” model training refers to the combination
    of many weak models to create a better or “stronger” overall prediction mechanism,
    often referred to as a “meta-model.” Boosting is just one example of an ensemble
    approach. Other examples include “Bagging” (or “Bootstrap Aggregation”), which
    trains many weak learners on subsets of the data, and “Stacking,” which can be
    used to combine models trained with completely different algorithms. The intent,
    in each case, is to build a more useful prediction mechanism than could be achieved
    by any of the models individually.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，“集成”模型训练的概念指的是将许多弱模型组合起来以创建更好的或“更强”的整体预测机制，通常被称为“元模型”。提升只是集成方法的一个例子。其他例子包括“Bagging”（或“Bootstrap
    Aggregation”），它在数据的子集上训练许多弱学习器，以及“Stacking”，它可以用来结合使用完全不同算法训练的模型。在每个情况下，目的是构建比任何单个模型单独实现更有用的预测机制。
- en: Depending on the ensemble implementation, the predictions of each model in the
    ensemble could be combined in different ways, such as being summed or averaged,
    or in classification use cases, a voting mechanism may be implemented in order
    to determine the resulting best prediction.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 根据集成方法的实现，集成中每个模型的预测可以以不同的方式组合，例如求和或平均，或者在分类用例中，可能实现投票机制以确定最终的最佳预测。
- en: Prerequisites for this chapter
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的先决条件
- en: Just like in the previous chapter, we will perform some initial activities that
    are required before we can start to perform the primary activities in this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在前一章中一样，我们将在开始本章的主要活动之前执行一些初始活动。
- en: Google Cloud makes it even easier to get started with the data science libraries
    we described in the previous sections because we can use Vertex AI Workbench notebooks
    that already have these libraries installed. If you wanted to use these libraries
    outside of Vertex AI, you would need to install them yourself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud使得开始使用我们在前几节中描述的数据科学库变得更加容易，因为我们可以使用已经安装了这些库的Vertex AI Workbench笔记本。如果您想在Vertex
    AI之外使用这些库，您需要自行安装它们。
- en: 'Considering that we’re going to use Vertex AI notebooks, and we will not need
    to explicitly install the libraries, we will not include details on how to do
    that in this book. If you would like to install the libraries in another environment,
    including **Google Compute Engine** (**GCE**) or **Google Kubernetes Engine**
    (**GKE**), you can find installation instructions on the respective websites for
    each of those libraries, such as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们将使用Vertex AI笔记本，并且不需要显式安装库，因此本书中不会包含如何安装的细节。如果您想在其他环境中安装库，包括**Google Compute
    Engine**（**GCE**）或**Google Kubernetes Engine**（**GKE**），您可以在每个库的相应网站上找到安装说明，例如：
- en: '[https://scikit-learn.org/stable/install.xhtml](https://scikit-learn.org/stable/install.xhtml)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/install.xhtml](https://scikit-learn.org/stable/install.xhtml)'
- en: '[https://matplotlib.org/stable/users/installing/index.xhtml](https://matplotlib.org/stable/users/installing/index.xhtml)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://matplotlib.org/stable/users/installing/index.xhtml](https://matplotlib.org/stable/users/installing/index.xhtml)'
- en: '[https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)'
- en: '[https://xgboost.readthedocs.io/en/stable/install.xhtml](https://xgboost.readthedocs.io/en/stable/install.xhtml)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://xgboost.readthedocs.io/en/stable/install.xhtml](https://xgboost.readthedocs.io/en/stable/install.xhtml)'
- en: Next, we will discuss Vertex AI Workbench notebooks in a bit more detail, and
    how to create a notebook.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更详细地讨论Vertex AI Workbench笔记本，以及如何创建一个笔记本。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Later in this chapter, you will see pieces of code that import these libraries.
    This is not the same as installing. The libraries are already installed in our
    Vertex AI Workbench notebooks, but we just need to import them into our notebook
    context in order to use them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，您将看到导入这些库的代码片段。这并不等同于安装。这些库已经安装在我们的Vertex AI Workbench笔记本中，但我们只需要将它们导入到笔记本上下文中才能使用。
- en: Vertex AI Workbench
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI Workbench
- en: Vertex AI Workbench is a development environment in Google Cloud that enables
    you to manage all of your AI/ML development needs within Vertex AI. It is based
    on Jupyter notebooks, which provide an interactive interface for writing and running
    code. This makes notebooks extremely versatile because you can codify interactions
    not only with the broader Vertex AI ecosystem but also with other Google Cloud
    service APIs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Workbench 是 Google Cloud 中的一个开发环境，它使您能够在 Vertex AI 内管理所有 AI/ML 开发需求。它基于
    Jupyter 笔记本，提供了编写和运行代码的交互式界面。这使得笔记本非常灵活，因为您不仅可以与更广泛的 Vertex AI 生态系统进行编码交互，还可以与其他
    Google Cloud 服务 API 进行交互。
- en: Vertex AI Workbench notebooks
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Vertex AI Workbench 笔记本
- en: 'There are three types of notebooks you can use within Vertex AI Workbench:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Vertex AI Workbench 中，您可以使用以下三种类型的笔记本：
- en: Managed notebooks, which, as the name suggests, are managed for you by Google.
    These are a good default option for many use cases because they have a lot of
    useful tools built in and are ready to go on Google Cloud. They run in a JupyterLab
    environment, which is a web-based interactive development environment for notebooks.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理笔记本，正如其名所示，由 Google 为您管理。这些对于许多用例来说是一个很好的默认选项，因为它们内置了许多有用的工具，并且已经准备好在 Google
    Cloud 上使用。它们在 JupyterLab 环境中运行，这是一个基于网页的交互式开发环境，用于笔记本。
- en: User-managed notebooks, which, as the name suggests, are managed by you. These
    are suitable if we require significant customization of our environment. They
    are customizable instances of Google Cloud’s Deep Learning VMs, which are **virtual
    machine** (**VM**) images that include tools for implementing **deep learning**
    (**DL**) workloads. However, these notebooks can be used for more than just DL
    use cases.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户管理笔记本，正如其名所示，由您管理。如果需要对我们环境进行重大定制，则这些笔记本是合适的。它们是 Google Cloud 深度学习 VM 的可定制实例，这些
    VM 是包含用于实现 **深度学习**（**DL**）工作负载的工具的 **虚拟机**（**VM**）镜像。然而，这些笔记本不仅可以用于深度学习用例。
- en: Vertex AI Workbench **instances**, which are the newest option released by Google
    Cloud in late 2023\. These can be seen as a hybrid of the previous two options,
    and will likely become the primary option over time. At the time of writing this
    in September 2023, this option is only available in preview mode, so we will stick
    with *options 1* and *2* for now.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI Workbench **实例**，这是 Google Cloud 在 2023 年晚些时候发布的新选项。这些可以被视为前两种选项的混合体，并且可能会随着时间的推移成为主要选项。在撰写本文的
    2023 年 9 月，此选项目前仅以预览模式提供，因此我们现在将继续使用 *选项 1* 和 *2*。
- en: Google also provides an offering called Colab, which is a very popular service
    that allows you to run free notebooks over the public internet. In late 2023,
    Google Cloud also released an option named Colab Enterprise, which enables customers
    to use Colab within their own Google Cloud environment. At the time of writing
    this, Colab Enterprise is also only available in preview mode.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还提供了一种名为 Colab 的服务，这是一个非常流行的服务，允许您通过公共互联网运行免费的笔记本。在 2023 年晚些时候，Google
    Cloud 还发布了一个名为 Colab Enterprise 的选项，使客户能够在自己的 Google Cloud 环境中使用 Colab。在撰写本文时，Colab
    Enterprise 也仅以预览模式提供。
- en: 'Managed notebooks and user-managed notebooks are being deprecated, so in this
    book we will primarily use the newest option: Vertex AI Workbench instances. Let''s
    go ahead and create one now.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 管理笔记本和用户管理笔记本正在被弃用，因此在这本书中，我们将主要使用最新的选项：Vertex AI Workbench 实例。让我们现在就创建一个。
- en: Creating a Vertex AI Workbench instance
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Vertex AI Workbench 实例
- en: 'To create a Vertex AI Workbench instance, perform the following steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 Vertex AI Workbench 实例，请执行以下步骤：
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For configuration parameters that are not specifically called out in these instructions,
    leave those parameters at their default values
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些说明中没有明确提到的配置参数，请将这些参数保留在默认值。
- en: In the Google Cloud console, navigate to the Google Cloud services menu → **Vertex
    AI** → **Workbench**.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中，导航到 Google Cloud 服务菜单 → **Vertex AI** → **Workbench**。
- en: Select the **Instances** tab and click **Create New**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **实例** 选项卡并点击 **创建新实例**。
- en: 'In the side panel that appears (see *Figure 5**.1* for reference), enter a
    name for your notebook. Alternatively, you can use the default name that’s automatically
    generated in that input field:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的侧面板中（参见 *图 5**.1* 以获取参考），为您的笔记本输入一个名称。或者，您可以使用该输入字段中自动生成的默认名称：
- en: '![Figure 5.1: The New instance dialogue](img/B18143_05_001.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1：新建实例对话框](img/B18143_05_001.jpg)'
- en: 'Figure 5.1: The New instance dialogue'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：新建实例对话框
- en: Select your preferred region.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您首选的区域。
- en: Select the option to attach a GPU (at the time of writing, you should select
    the box that says **Attach 1 NVIDIA T4 GPU**). See *Figure 5**.1* for reference.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择附加GPU的选项（在撰写本文时，您应该选择标有**附加1个NVIDIA T4 GPU**的框）。参见*图5**.1*以获取参考。
- en: Leave the networking configuration details at their default values for now.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前请将网络配置细节保留为默认值。
- en: At the bottom of the side panel, click **Advanced Options**.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在侧面板的底部，点击**高级选项**。
- en: 'In the next screen that appears (see *Figure 5**.2* for reference), ensure
    that the option to **Enable Dataproc** is selected and click **Continue** (you
    may need to scroll down to see the **Continue** button):'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后出现的屏幕上（参见*图5**.2*以获取参考），确保选中**启用Dataproc**的选项并点击**继续**（您可能需要向下滚动以看到**继续**按钮）：
- en: '![Figure 5.2: The New instance dialogue (continued)](img/B18143_05_002.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2：新实例对话框（继续）](img/B18143_05_002.jpg)'
- en: 'Figure 5.2: The New instance dialogue (continued)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：新实例对话框（继续）
- en: On the next screen that appears (that is, the environment configuration screen),
    select **Use the latest version** and click **Continue**.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后出现的屏幕上（即环境配置屏幕），选择**使用最新版本**并点击**继续**。
- en: On the next screen that appears, you can use all of the default values (unless
    you have preferences to change anything) and click **Continue**.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后出现的屏幕上，您可以使用所有默认值（除非您有更改任何内容的偏好）并点击**继续**。
- en: You can click the **Continue** button two more times (that is, to accept the
    default values on the next two screens, unless you have preferences to change
    anything) until you reach the **Networking** configuration screen.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以点击两次**继续**按钮（即接受下一两个屏幕上的默认值，除非您有更改的偏好）直到达到**网络**配置屏幕。
- en: On the **Networking configuration** screen, ensure that the **Assign external
    IP address** option is selected.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**网络配置**屏幕上，确保选中**分配外部IP地址**的选项。
- en: Unless you have any specific networking configuration needs, you can simply
    use the default network in your project.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除非您有特定的网络配置需求，否则您可以直接使用项目中默认的网络。
- en: Click **Continue**.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**继续**。
- en: On the next screen that appears (that is, the **IAM & Security** screen), ensure
    that all the options in the **Security options** section are selected.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在随后出现的屏幕上（即**IAM & 安全**屏幕），确保**安全选项**部分中的所有选项都已选中。
- en: Select **CREATE** (at the bottom of the screen) and wait a few minutes for the
    notebook to be created.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择屏幕底部的**创建**，并等待几分钟以创建笔记本。
- en: When the notebook is up and running, a green checkmark will appear to the left
    of the notebook’s name.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当笔记本运行起来时，笔记本名称左侧将出现一个绿色的勾选标记。
- en: Finally, click **Open JupyterLab**.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击**打开JupyterLab**。
- en: At this point, I’d like to highlight an important set of integrations and features
    that Google Cloud has added to the JupyterLab interface in Vertex AI Notebooks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我想强调一组重要的集成和功能，这是Google Cloud在Vertex AI Notebooks的JupyterLab界面中添加的。
- en: Vertex AI Notebook JupyterLab integrations
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI Notebook JupyterLab集成
- en: 'In the JupyterLab interface in your Vertex AI Notebook instance, you will notice
    a set of icons on the far left-hand side of the screen, as shown in *Figure 5**.3*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的Vertex AI Notebook实例的JupyterLab界面中，您会在屏幕的左侧注意到一组图标，如图*图5**.3*所示：
- en: '![Figure 5.3: Google Cloud JupyterLab integrations](img/B18143_05_003.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3：Google Cloud JupyterLab集成](img/B18143_05_003.jpg)'
- en: 'Figure 5.3: Google Cloud JupyterLab integrations'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：Google Cloud JupyterLab集成
- en: These icons represent integrations that we can use directly within our JupyterLab
    Notebook instances in Vertex AI. For example, if we click on the BigQuery icon,
    we can see our BigQuery datasets, and we can even use the integrated SQL Editor
    to run SQL queries on our BigQuery datasets directly from the JupyterLab Notebook
    interface.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图标代表我们可以在Vertex AI的JupyterLab Notebook实例中直接使用的集成。例如，如果我们点击BigQuery图标，我们可以看到我们的BigQuery数据集，我们甚至可以使用集成的SQL编辑器直接从JupyterLab
    Notebook界面运行SQL查询。
- en: I recommend clicking on the various icons to learn more about what you can do
    with them. Other useful features include the ability to integrate with Google
    Cloud Storage and an option that enables us to schedule the execution of our notebooks.
    The latter is a very useful feature for workloads that need to be automatically
    repeated periodically, which is a common need in data science (for example, re-training,
    evaluating, and deploying a model on new data every day).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议点击各种图标来了解更多关于它们的功能。其他有用的功能包括与Google Cloud Storage集成的能力以及一个允许我们安排笔记本执行选项。后者对于需要定期自动重复的工作负载非常有用，这在数据科学中是一个常见需求（例如，每天对新数据进行重新训练、评估和部署模型）。
- en: Now that the notebook instance has been created, we will clone our GitHub repository
    so that we can access our notebook code and follow along with the activities in
    this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在笔记本实例已经创建，我们将克隆我们的GitHub仓库，以便我们可以访问我们的笔记本代码并跟随本章的活动。
- en: Cloning the GitHub repository
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克隆GitHub仓库
- en: 'Cloning our GitHub repository is the easiest way to quickly import all of the
    resources for the hands-on activities in this chapter into your notebook instance
    in Google Cloud Vertex AI. To clone our repository into your notebook, perform
    the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的GitHub仓库是将本章动手活动的所有资源快速导入到Google Cloud Vertex AI笔记本实例中的最简单方法。要将我们的仓库克隆到笔记本中，请执行以下步骤：
- en: 'Click on the **Git** symbol in the menu on the left of the screen. The symbol
    will look like the one shown in *Figure 5**.4*:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击屏幕左侧菜单中的**Git**符号。符号看起来像*图5.4*中所示：
- en: '![Figure 5.4: Git symbol](img/B18143_05_004.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4：Git符号](img/B18143_05_004.png)'
- en: 'Figure 5.4: Git symbol'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：Git符号
- en: Select **Clone Repository**.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**克隆仓库**。
- en: 'Enter our repository URL: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入我们的仓库URL：[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).
- en: If any options are displayed, leave them at their default values.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有任何选项显示，请保留它们的默认值。
- en: Select **Clone**.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**克隆**。
- en: You should see a new folder appear in your notebook, named `Google-Machine-Learning-for-Solutions-Architects`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该会在笔记本中看到一个新文件夹出现，命名为`Google-Machine-Learning-for-Solutions-Architects`。
- en: Now, we’re ready to start using our notebook instance! Let’s move on to the
    next section, in which we will perform some unsupervised training.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始使用我们的笔记本实例了！让我们继续到下一节，我们将进行一些无监督训练。
- en: UML with scikit-learn on Vertex AI
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上使用scikit-learn进行UML
- en: In this section, we will start using our Vertex AI Workbench notebook to train
    models. We will begin with a relatively simple use case in which we will create
    an unsupervised model to discover clustering patterns in our data. Before we dive
    into the code, we will first take a minute to discuss the clustering algorithm
    we will use in this section, which is called K-means.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始使用我们的Vertex AI Workbench笔记本来训练模型。我们将从一个相对简单的用例开始，我们将创建一个无监督模型来发现数据中的聚类模式。在我们深入代码之前，我们将先花一分钟讨论我们将在本节中使用的聚类算法，它被称为K-means。
- en: K-means
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means
- en: 'You may remember that we discussed **unsupervised learning** (**UL**) mechanisms
    such as clustering in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). Remember
    that in clustering, data points are grouped together based on similarities between
    features or characteristics that are observed by the model. *Figure 5**.5* provides
    a visual representation of this concept, showing the input data on the left and
    the resulting data clusters on the right:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中讨论了**无监督学习**（**UL**）机制，例如聚类。记住，在聚类中，数据点根据模型观察到的特征或特征之间的相似性被分组在一起。*图5.5*提供了这个概念的可视化表示，显示了左侧的输入数据和右侧的结果数据聚类：
- en: '![Figure 5.5: Clustering](img/B18143_05_005.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5：聚类](img/B18143_05_005.jpg)'
- en: 'Figure 5.5: Clustering'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：聚类
- en: 'K-means is an example of a clustering algorithm, and it is categorized as a
    centroid-based clustering algorithm. What this means is that it chooses a **centroid**,
    which is a point that represents the center of each of our clusters. The members
    of each cluster are data points from our dataset, and their membership in each
    cluster will depend on a mathematical evaluation of how close or far they are
    from each centroid. This proximity or distance from each centroid is generally
    calculated in terms of the Euclidean distance, represented by *Equation 5.1*:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是聚类算法的一个例子，它被归类为中心点聚类算法。这意味着它选择一个**中心点**，这是一个代表我们每个簇中心的点。每个簇的成员是我们数据集中的数据点，它们在每个簇中的成员资格将取决于它们与每个中心点距离的数学评估。从每个中心点的这种邻近度或距离通常用欧几里得距离来计算，如方程5.1所示：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>d</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow></mrow></math>](img/11.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>d</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow></mrow></math>](img/11.png)'
- en: 'Equation 5.1: Euclidean distance'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 方程5.1：欧几里得距离
- en: Don’t worry—we will discuss this in more detail. The graphs shown in *Figure
    5**.5* represent what’s called the feature space, and they show where each of
    our data points exists in the feature space. In the case of the aforementioned
    graphs, they represent a two-dimensional feature space, and each data point has
    an *x* and *y* coordinate that represents where the data point exists in the feature
    space. That is to say, the *x* and *y* coordinates are the features of each data
    point. If we take any two points on the graph, the Euclidean distance is simply
    the distance in a straight line between those two points, which is calculated
    by putting the *x* and *y* coordinates (that is, the features of each data point)
    into the equation represented in *Equation 5.1*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——我们将会更详细地讨论这个问题。图5.5中显示的图表代表所谓的特征空间，并且显示了我们的每个数据点在特征空间中的位置。在上述图表的情况下，它们代表一个二维特征空间，每个数据点都有一个
    *x* 和 *y* 坐标，这些坐标代表了数据点在特征空间中的位置。也就是说，*x* 和 *y* 坐标是每个数据点的特征。如果我们从图表上取任意两点，欧几里得距离就是这两点之间直线距离，这个距离是通过将
    *x* 和 *y* 坐标（即每个数据点的特征）代入图5.1中所示的方程来计算的。
- en: Data points can have more than *x* and *y* coordinates as their features, and
    the concept applies in higher-dimensional feature spaces also.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点可以作为其特征具有比 *x* 和 *y* 坐标更多的坐标，这个概念也适用于更高维度的特征空间。
- en: Let’s take a more concrete example than just *x* and *y* coordinates. Our dataset
    could consist of information regarding a retail company’s customers, and the features
    of each customer could include their age, the city they live in, and what they
    purchased in their most recent visit to the company’s store. Clustering algorithms
    such as K-means may then group those customers by finding similarities between
    their features and the centroids in each group.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个比 *x* 和 *y* 坐标更具体的例子来说明。我们的数据集可能包括有关零售公司客户的详细信息，每个客户的特征可能包括他们的年龄、他们居住的城市以及他们在公司商店最近一次访问中购买的商品。聚类算法如K-means可以通过寻找他们特征与每个组中心点之间的相似性来将这些客户分组。
- en: 'One of the first questions that often comes up when discussing K-means is:
    How are centroids chosen? For example, how does the algorithm know how many centroids
    to use, and where to place them in the feature space? Let’s address the number
    of centroids first. This is specified as a “hyperparameter” to the algorithm.
    This means that you can tell the algorithm how many centroids to use (and therefore
    how many clusters to form). In fact, this is reflected in the name, “K-means,”
    where K represents the number of centroids or clusters. You would generally try
    different numbers of centroids until you find a value that maximizes the amount
    of information gained and minimizes the amount of variance, in each cluster. A
    mechanism that is often used to find the optimal value for K is referred to as
    “the elbow method.” Using the elbow method, we run K-means clustering on the dataset
    for a range of different values of K (such as 1-10). Then, for each value of K,
    the average score is computed for all clusters. The default computed score is
    called the **distortion**. The distortion represents the sum of the squared distances
    from each point to the centroid in their assigned cluster, which again relates
    back to *Equation 5.1*. If you graph the value of K against the distortion score
    for each run, you will usually notice the distortion score going down each time
    you increase the value of K. The distortion score will usually go down sharply
    in the beginning and eventually start to go down in smaller increments. When adding
    new clusters (that is, increasing the value of K) stops significantly reducing
    the distortion score, then you can usually consider the optimal value of K to
    have been found.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论K-means时经常出现的一个问题是：如何选择质心？例如，算法如何知道使用多少个质心，以及在哪里放置它们在特征空间中？让我们首先解决质心的数量问题。这被指定为算法的“超参数”。这意味着你可以告诉算法使用多少个质心（因此形成多少个簇）。实际上，这反映在名称“K-means”中，其中K代表质心或簇的数量。你通常会尝试不同的质心数量，直到找到一个值，该值最大化获得的信息量并最小化每个簇中的方差。用于找到K的最佳值的机制通常被称为“肘部方法”。使用肘部方法，我们在不同K值的范围内对数据集运行K-means聚类（例如1-10）。然后，对于每个K值，计算所有簇的平均得分。默认计算的得分称为**畸变**。畸变表示每个点与其分配的簇中质心的平方距离之和，这再次与*方程5.1*相关。如果你将K值与每次运行中的畸变得分进行绘图，你通常会注意到畸变得分随着K值的增加而下降。畸变得分通常会开始急剧下降，最终开始以更小的增量下降。当添加新的簇（即增加K值）不再显著减少畸变得分时，你通常可以认为已经找到了K的最佳值。
- en: 'Now, let’s discuss how the algorithm knows where to place the centroids in
    the feature space. It begins by placing them at random locations in the feature
    space and randomly assigning data points to each centroid. It then repeats the
    following steps until no new data points are added to each cluster, at which point
    the optimal cluster positioning is believed to have been calculated:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论算法如何在特征空间中放置质心的位置。它首先在特征空间中随机放置它们，并将数据点随机分配给每个质心。然后，它重复以下步骤，直到每个簇中没有新的数据点被添加，此时认为已经计算出了最佳的聚类位置：
- en: Calculate which data points are closest to the centroid and assign them to that
    centroid.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算哪些数据点最接近质心，并将它们分配给该质心。
- en: Calculate the average (mean) between those points.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些点的平均值（均值）。
- en: Move the centroid.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动质心。
- en: Now that we’ve covered the theory of how K-means clustering works, let’s move
    on to the fun part, which is to actually implement the algorithm.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了K-means聚类的理论，让我们继续到有趣的部分，即实际实现算法。
- en: Implementing a UML workload in Vertex AI
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Vertex AI中实现UML工作负载
- en: We’re going to use the K-means algorithm within scikit-learn to start implementing
    our UML workload in Vertex AI. One of the quintessential clustering examples using
    K-means in scikit-learn is to use what’s referred to as the iris dataset, to find
    cluster patterns in the data. The iris dataset, as the name implies, contains
    data on various iris flowers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在scikit-learn中使用K-means算法来开始实现我们的Vertex AI中的UML工作负载。在scikit-learn中使用K-means的一个典型的聚类示例是使用所谓的鸢尾花数据集，以找到数据中的聚类模式。正如其名所示，鸢尾花数据集包含了各种鸢尾花的数据。
- en: Business case
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 商业案例
- en: Our company, *BigFlowers.com*, recently acquired a smaller flower company, and
    in doing so, we acquired all of their digital assets, including the datasets that
    contain their flower inventory. Unfortunately, they did not do a good job of documenting
    their datasets, so we don’t have a good idea of what’s in their inventory.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的公司 *BigFlowers.com* 最近收购了一家较小的花店，在这个过程中，我们获得了他们所有的数字资产，包括包含他们花卉库存的数据集。不幸的是，他们没有很好地记录他们的数据集，所以我们不清楚他们的库存中有什么。
- en: We’ve been tasked with finding out as much as we can about the contents of the
    flower inventory, so we’re going to use some data analytics tools and ML models
    to learn more about the dataset. One of the first things we’re going to do is
    try to find any patterns such as logical groupings, which could help us to understand
    if there are distinct categories of objects in the dataset and additional information
    such as how many distinct categories exist.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是尽可能多地了解花卉库存的内容，因此我们将使用一些数据分析工具和机器学习模型来了解更多关于数据集的信息。我们将要做的第一件事是尝试找到任何模式，例如逻辑分组，这可能有助于我们了解数据集中是否存在不同的对象类别，以及额外的信息，例如存在多少不同的类别。
- en: '*Note*: At the time of writing this, the company *BigFlowers.com* does not
    exist, and our exercises here refer to a fictitious company for example purposes
    only.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意*：在撰写本文时，公司 *BigFlowers.com* 并不存在，我们在这里的练习仅用于示例目的。'
- en: To get started with this task, navigate into the `Google-Machine-Learning-for-Solutions-Architects`
    folder in the Vertex AI notebook you created in the previous section. Then, navigate
    into the `Chapter-05` folder, double click on the `Chapter-5.ipynb` file (if prompted,
    select the `Python` kernel), and perform the following steps.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始这项任务，请导航到您在上一节中创建的 Vertex AI 笔记本中的 `Google-Machine-Learning-for-Solutions-Architects`
    文件夹。然后，导航到 `Chapter-05` 文件夹，双击 `Chapter-5.ipynb` 文件（如果提示，请选择 `Python` 内核），并执行以下步骤。
- en: 'The first thing we need to do in our notebook is to import the resources that
    we need, such as the scikit-learn K-means class, and the pandas and Matplotlib
    libraries. We will also import a function to load the iris dataset from scikit-learn.
    To perform these actions, enter the following code into the interactive prompt
    in our notebook (or use the notebook you cloned from GitHub) The following code
    in the notebook performs those steps:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的笔记本中，我们首先需要导入所需的资源，例如 scikit-learn 的 K-means 类，以及 pandas 和 Matplotlib 库。我们还将导入一个从
    scikit-learn 加载 iris 数据集的函数。要执行这些操作，请将以下代码输入到笔记本中的交互式提示（或使用您从 GitHub 克隆的笔记本）中。笔记本中的以下代码执行了这些步骤：
- en: '[PRE0]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To execute the code, hold the *Shift* key on your keyboard, and press the *Enter*
    key. Considering that the code is simply importing libraries, you will not see
    any output displayed back to the screen unless an error occurs.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行代码，请按住键盘上的 *Shift* 键，然后按 *Enter* 键。考虑到代码只是导入库，除非发生错误，否则您不会在屏幕上看到任何输出。
- en: Note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: From here onward, when we are performing activities in Vertex AI Workbench notebooks,
    we will just provide the code samples for each step, and it will imply that you
    need to enter that code into the next available empty cell in the notebook (unless
    you are using the cloned notebook that already contains the code) and execute
    the cell, just as you did with the previous piece of code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，当我们进行 Vertex AI Workbench 笔记本中的活动时，我们将只为每个步骤提供代码示例，这表示您需要将此代码输入到笔记本中下一个可用的空单元格中（除非您正在使用已包含代码的克隆笔记本），然后执行单元格，就像您之前处理那段代码一样。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'On the left side of each cell in a Jupyter notebook, there is a square bracket
    symbol that looks like this: **[ ]**'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter 笔记本中每个单元格的左侧，有一个看起来像这样的方括号符号：**[ ]**
- en: 'This can be used to understand the status of the cell, and the indicators are
    as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来了解单元格的状态，指标如下：
- en: '**[ ]** (Empty): The cell has not yet been executed.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**[ ]**（空）：该单元格尚未执行。'
- en: '**[*]** (Asterisk): The cell is currently executing.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**[*]**（星号）：该单元格正在执行。'
- en: '**[1]** (Any number): the cell has completed executing.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]**（任何数字）：该单元格已执行完成。'
- en: 'Next, we will read the iris dataset in:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将读取 iris 数据集：
- en: '[PRE1]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s use pandas to get some information about our dataset:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 pandas 获取有关我们数据集的一些信息：
- en: '[PRE2]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should look like what is shown in *Figure 5**.6*:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应类似于 *图 5.6* 中所示：
- en: '![Figure 5.6: pandas.DataFrame.info() output](img/B18143_05_006.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：pandas.DataFrame.info() 输出](img/B18143_05_006.jpg)'
- en: 'Figure 5.6: pandas.DataFrame.info() output'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：pandas.DataFrame.info() 输出
- en: 'The `info()` function output shows us what kinds of data our dataset contains.
    In this case, we can see that it contains 150 rows (indexed from 0 to 149) and
    4 columns (indexed from 0 to 3), and the data values in each cell are floating-point
    numbers. Each row in our dataset is a data point, which represents a particular
    iris flower, and each column is a feature in our dataset. If we look at the description
    for the iris dataset in the scikit-learn documentation, which can be found at
    [https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset),
    we can see that the features are:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`info()` 函数的输出显示我们的数据集包含什么类型的数据。在这种情况下，我们可以看到它包含 150 行（从 0 到 149 索引）和 4 列（从
    0 到 3 索引），并且每个单元格中的数据值是浮点数。我们的数据集中的每一行都是一个数据点，它代表一朵特定的鸢尾花，每一列是数据集中的一个特征。如果我们查看
    scikit-learn 文档中关于鸢尾花数据集的描述，该描述可以在 [https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset)
    找到，我们可以看到特征是：'
- en: Sepal length in cm
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 萼片长度（厘米）
- en: Sepal width in cm
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 萼片宽度（厘米）
- en: Petal length in cm
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 花瓣长度（厘米）
- en: Petal width in cm
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 花瓣宽度（厘米）
- en: From this, we can understand that the floating-point numbers in each cell in
    our dataset are measurements of those four aspects of each flower.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这个例子中，我们可以理解我们数据集中每个单元格中的浮点数是测量每朵花四个方面的值。
- en: 'We can also preview a subset of the data to see the actual values in each cell,
    like so:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以预览数据的一个子集，以查看每个单元格的实际值，如下所示：
- en: '[PRE3]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output should look like what is shown in *Figure 5**.7*:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应类似于 *图 5.7* 中所示：
- en: '![Figure 5.7: pandas.DataFrame.head() output](img/B18143_05_007.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7：pandas.DataFrame.head() 输出](img/B18143_05_007.jpg)'
- en: 'Figure 5.7: pandas.DataFrame.head() output'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：pandas.DataFrame.head() 输出
- en: 'Now, we’re going to use K-means to group similar data points in the dataset
    together, based on their features. First, we’re creating an instance of a K-means
    model, and we’re specifying that it will have three clusters because in this case,
    the dataset documentation told us that there are three different categories of
    iris in our dataset. If we didn’t already know how many clusters we needed to
    use, then we could try different numbers of clusters and use the elbow method
    to find the best number:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 K-means 根据数据集的特征将相似的数据点分组在一起。首先，我们创建了一个 K-means 模型的实例，并指定它将有三簇，因为在这种情况下，数据集文档告诉我们我们的数据集中有三种不同的鸢尾花类别。如果我们不知道需要使用多少簇，那么我们可以尝试不同的簇数，并使用肘部方法找到最佳数量：
- en: '[PRE4]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, we have defined our model, but it has not yet learned anything
    from our data. Next, we instruct the model to “fit” to our dataset. The term *fit*
    is used by many algorithms to refer to the training process because, during training,
    this is exactly what the algorithm is trying to do; it is trying to create a model
    that fits as accurately as possible (without overfitting) to the given dataset:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经定义了我们的模型，但它还没有从我们的数据中学习到任何东西。接下来，我们指示模型“拟合”到我们的数据集。术语 *fit* 被许多算法用来指代训练过程，因为在训练过程中，这正是算法试图做的事情；它试图创建一个尽可能精确地（不发生过拟合）拟合给定数据集的模型：
- en: '[PRE5]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When our model has completed the training process, we can now get it to cluster
    our data. We will feed our original dataset into the model, and according to the
    patterns it learned during training, it will place the data points into each of
    the clusters it defined:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们的模型完成训练过程后，我们现在可以使其对数据进行聚类。我们将原始数据集输入到模型中，根据它在训练期间学习的模式，它将根据定义的每个簇放置数据点：
- en: '[PRE6]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we store the model’s labels in a variable so that we can visualize
    the clusters in the next cell:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将模型的标签存储在一个变量中，这样我们就可以在下一个单元中可视化簇：
- en: '[PRE7]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will visualize the clusters that K-means has created:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将可视化 K-means 创建的簇：
- en: '[PRE8]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting graph should look similar to the graph depicted in *Figure 5**.8*.
    Notice how we can see three distinct clusters of data points in the graph, where
    each distinct cluster is color-coded as either purple, green, or yellow:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的图表应类似于 *图 5.8* 中所示的图表。注意我们如何在图表中看到三个不同的数据点簇，每个不同的簇都按紫色、绿色或黄色进行着色编码：
- en: '![Figure 5.8: K-means cluster graph](img/B18143_05_008.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8：K-means 聚类图](img/B18143_05_008.jpg)'
- en: 'Figure 5.8: K-means cluster graph'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：K-means 聚类图
- en: This gives us some useful information. We can see the three distinct categories
    or clusters of data points in our dataset, in which the data points share similar
    characteristics with other points in their same cluster but differ from the data
    points in other clusters. We get these insights without ever needing to label
    our dataset, which is quite useful, considering that labeling is a time-consuming
    and error-prone task.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一些有用的信息。我们可以看到数据集中的三个不同的数据点类别或聚类，其中数据点与其同一聚类中的其他点具有相似的特征，但与其他聚类中的数据点不同。我们无需对数据集进行标记就能获得这些见解，考虑到标记是一个耗时且容易出错的任务，这非常有用。
- en: Note
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Although our dataset contains four features (also referred to as “dimensions”),
    humans can only see/visualize things in three dimensions. Therefore, we only used
    three of the four features to create our graph. The following line is where we
    defined the features to use in our graph, in which the numbers represent the features
    to be used:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的数据集包含四个特征（也称为“维度”），但人类只能看到/可视化三维的东西。因此，我们只使用了四个特征中的三个来创建我们的图表。以下是我们定义用于图表中的特征的行，其中数字代表要使用的特征：
- en: '**axes.scatter(iris_data[:, 2], iris_data[:, 3], iris_data[:,** **1], c=labels.astype(float))**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**axes.scatter(iris_data[:, 2], iris_data[:, 3], iris_data[:,** **1], c=labels.astype(float))**'
- en: You can try playing around with the graph by changing each of those numbers
    to anything between 0 and 3, as long as each entry is a unique value (that is,
    don’t repeat any of the numbers more than once in the code).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试通过将每个数字更改为 0 到 3 之间的任何值来玩转这个图表，只要每个条目都是唯一的值（也就是说，在代码中不要重复任何数字）。
- en: Each time you make a change, you can execute the cell again to update the graph.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你进行更改时，都可以再次执行单元格以更新图表。
- en: Now that we’ve seen what it takes to implement a UML workload in Vertex AI,
    let’s move on and learn how to implement an SML workload in the next section.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了在 Vertex AI 中实现 UML 工作负载需要什么，接下来让我们继续学习如何在下一节中实现 SML 工作负载。
- en: Implementing a regression model with scikit-learn on Vertex AI
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Vertex AI 上使用 scikit-learn 实现回归模型
- en: The first SML model we’re going to build in our Vertex AI Workbench notebook
    is a linear regression model. You may remember that we described linear regression
    in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Vertex AI 工作台中构建的第一个 SML 模型是一个线性回归模型。你可能还记得我们在 [*第 1 章*](B18143_01.xhtml#_idTextAnchor015)
    中描述了线性回归。
- en: Business case
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 商业案例
- en: Our boss at *BigFlowers.com* is running a fun competition at work. Employees
    are asked to predict the length of an iris flower’s petal when given some other
    measurements related to that flower, such as the sepal length, sepal width, and
    petal width. The person with the most accurate estimation will get a big prize,
    and employees are allowed to use technology to help them in their estimations,
    so we’re going to build an ML model to help us make these predictions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *BigFlowers.com* 的老板在工作场所举办了一场有趣的竞赛。员工被要求预测当给出与该花朵相关的其他测量值时，鸢尾花花瓣的长度，例如萼片长度、萼片宽度和花瓣宽度。预测最准确的人将获得一份大奖，员工可以使用技术来帮助他们进行预测，因此我们将构建一个机器学习模型来帮助我们做出这些预测。
- en: In the previous section, we used the K-means algorithm within scikit-learn.
    In this section, we will use the linear regression algorithm within scikit-learn.
    As such, we will need to import the `LinearRegression` class into our notebook
    context. We will also import a function that will calculate the **Mean Squared
    Error** (**MSE**) metric, which we described in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035)
    and which is commonly used to evaluate linear regression models.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了 scikit-learn 中的 K-means 算法。在本节中，我们将使用 scikit-learn 中的线性回归算法。因此，我们需要将
    `LinearRegression` 类导入我们的笔记本上下文中。我们还将导入一个函数，该函数将计算 **均方误差**（**MSE**）指标，我们在 [*第
    2 章*](B18143_02.xhtml#_idTextAnchor035) 中描述了该指标，它通常用于评估线性回归模型。
- en: We can use the same iris dataset for our linear regression model, so we won’t
    need to repeat any of the previous dataset importing steps because it is already
    loaded into our notebook context. This is an important concept to note; we can
    use different types of models on the same data, depending on the business use
    case and desired results. However, as we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    SML model training introduces some additional requirements regarding how the dataset
    is used, which we describe next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的花瓣数据集来构建我们的线性回归模型，因此我们不需要重复之前的任何数据集导入步骤，因为它已经加载到我们的笔记本上下文中。这是一个需要注意的重要概念；我们可以根据业务用例和期望的结果，在相同的数据上使用不同类型的模型。然而，正如我们在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中讨论的，SML模型训练引入了一些关于数据集使用方面的额外要求，我们将在下面进行描述。
- en: Remember
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 记住
- en: In **supervised learning** (**SL**) use cases, we need to define which column
    in the dataset is designated as the “target” feature. This is the feature that
    we’re trying to predict based on the other features in the dataset. During training,
    some of the elements in this column are used as labels that represent known, correct
    answers from which the model learns.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在**监督学习**（SL）用例中，我们需要定义数据集中哪一列被指定为“目标”特征。这是我们试图根据数据集中的其他特征进行预测的特征。在训练过程中，该列的一些元素被用作标签，代表模型学习的已知、正确的答案。
- en: The values of the other features in the dataset are used as inputs. During the
    prediction process, the model uses the relationships it has learned between all
    of the input features and the target feature to predict the value of the target
    feature based on the values of the input features.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中其他特征的值用作输入。在预测过程中，模型使用它所学习到的所有输入特征与目标特征之间的关系，根据输入特征的值来预测目标特征的值。
- en: Also, remember that in SL use cases, we generally split our dataset into subsets
    such as training, validation, and testing. The training dataset, as the name implies,
    is what the model is trained on. The testing dataset is how we evaluate the trained
    model (based on the metrics that we’ve defined for that model), and the validation
    set is usually used in hyperparameter tuning.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，在SL用例中，我们通常将数据集分为训练、验证和测试等子集。训练数据集，正如其名所示，是模型训练所用的数据。测试数据集是我们评估训练好的模型的方式（基于为该模型定义的指标），而验证集通常用于超参数调整。
- en: '*Note*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意*：'
- en: We will explore hyperparameter tuning in a later chapter. In our current chapter,
    we will train a single regression model and we will then test it directly, so
    we will not need to split out a validation subset of our data. Therefore, we will
    just split the dataset into subsets for training and testing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中探讨超参数调整。在我们当前的章节中，我们将训练一个单一的回归模型，然后直接对其进行测试，因此我们不需要从我们的数据中分离出一个验证子集。因此，我们将仅将数据集分为训练集和测试集。
- en: 'To start building our linear regression model, perform the following steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始构建我们的线性回归模型，请执行以下步骤：
- en: 'Import the `LinearRegression` class from scikit-learn and import a function
    that will make it easy for us to split our dataset into train and test datasets,
    as well as a function that will calculate the MSE metric that we will use later
    to evaluate our model’s performance:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从scikit-learn导入`LinearRegression`类，并导入一个将使我们能够将数据集分割成训练集和测试集的函数，以及一个将计算我们稍后用于评估模型性能的MSE指标的函数：
- en: '[PRE9]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define which column in our dataset we want to use as the target feature to
    predict:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们数据集中哪一列是我们想要用作预测目标特征的列：
- en: '[PRE10]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Remember:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：
- en: 'In the previous section of this chapter, we consulted the documentation for
    the iris dataset, and we saw that the columns in the dataset are as follows: sepal
    length in cm, sepal width in cm, petal length in cm, petal width in cm. These
    columns are indexed from 0 to 3\. Considering that we want to predict the petal
    length, we have selected column 2 of our **iris_df** dataframe as our target column.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前一节中，我们查阅了iris数据集的文档，并看到数据集中的列如下：花萼长度（厘米），花萼宽度（厘米），花瓣长度（厘米），花瓣宽度（厘米）。这些列从0到3索引。考虑到我们想要预测花瓣长度，我们已选择**iris_df**数据框的列2作为我们的目标列。
- en: 'Next, define our input features. In this case, we use all feature columns except
    column 2 (because we’ve defined that column 2 is our target):'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义我们的输入特征。在这种情况下，我们使用除了列2之外的所有特征列（因为我们已定义列2是我们的目标列）：
- en: '[PRE11]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Split our dataset into separate subsets for training and testing:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的数据集分割成单独的训练集和测试集：
- en: '[PRE12]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Specifying a value of 0.2 for the **test_size** variable means that 20% of the
    original dataset will be separated to create the test dataset. The remaining 80%
    then forms the training dataset. The **input_train** and **input_test** datasets
    contain the input features used during training and the input features that will
    be used to test the trained model, respectively. The **target_train** and **target_test**
    datasets contain the respective labels (that is, the “correct answers”) for the
    training and test datasets.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将**test_size**变量的值设为0.2意味着原始数据集的20%将被分离出来以创建测试数据集。剩下的80%形成训练数据集。**input_train**和**input_test**数据集包含训练过程中使用的输入特征以及将用于测试训练模型的输入特征。**target_train**和**target_test**数据集包含训练和测试数据集的相应标签（即“正确答案”）。
- en: 'Now, let’s use the training dataset to train our linear regression model, and
    then use the test dataset to generate predictions from the model:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用训练数据集来训练我们的线性回归模型，然后使用测试数据集来生成模型预测：
- en: '[PRE13]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Remember
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 记住
- en: To test the trained model, we send the **input_test** features to it, and we
    ask it to predict what the corresponding target feature values should be, based
    on the values of the **input_test** features. We can then compare the model’s
    predictions to the **target_test** values (which are the known, correct answers)
    in order to see how close its predictions were to the correct, expected values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试训练模型，我们将**input_test**特征发送给它，并要求它根据**input_test**特征的值预测相应的目标特征值。然后我们可以将模型的预测与**target_test**值（即已知的、正确的答案）进行比较，以查看其预测与正确、预期的值有多接近。
- en: Finally, it’s time to see what kinds of predictions our model has made based
    on the `input_test` data!
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，是时候看看我们的模型基于`input_test`数据做出了哪些预测了！
- en: '[PRE14]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output should look similar to what is shown in *Figure 5**.9*, although
    the numbers may differ (the predictions are in the right-hand column, disregarding
    the number 0 at the top, which is the column header):'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果应类似于*图5.9*所示，尽管数字可能不同（预测值位于右侧列，忽略顶部的数字0，它是列标题）：
- en: '![Figure 5.9: Linear regression model predictions](img/B18143_05_009.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9：线性回归模型预测](img/B18143_05_009.jpg)'
- en: 'Figure 5.9: Linear regression model predictions'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：线性回归模型预测
- en: 'Now, let’s take a look at the corresponding known, correct values from the
    `target_test` dataset:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下`target_test`数据集中相应的已知、正确值：
- en: '[PRE15]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output should look similar to what is shown in *Figure 5**.10*, although
    the numbers may differ (the values are in the right-hand column, disregarding
    the number 2 at the top, which is the column header):'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果应类似于*图5.10*所示，尽管数字可能不同（数值位于右侧列，忽略顶部的数字2，它是列标题）：
- en: '![Figure 5.10: The known, correct values from the target_test dataset](img/B18143_05_010.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10：来自target_test数据集的已知、正确值](img/B18143_05_010.jpg)'
- en: 'Figure 5.10: The known, correct values from the target_test dataset'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：来自target_test数据集的已知、正确值
- en: 'As we can see, the numbers are pretty close in some cases. However, let’s use
    metrics to evaluate the model’s overall performance. To do this, we use the `mean_squared_error()`
    function to compare the predictions against the correct values from the `target_test`
    dataset and generate the MSE metric value:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们所见，在某些情况下数字非常接近。然而，让我们使用指标来评估模型的整体性能。为此，我们使用`mean_squared_error()`函数将预测与`target_test`数据集中的正确值进行比较，并生成MSE指标值：
- en: '[PRE16]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The value should be something like 0.08871798773326277.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值应类似于0.08871798773326277。
- en: This is a pretty good value because the error is quite low, which means that
    our model is doing a good job of predicting the target values. At this point,
    I think we might win that big prize in the competition to predict the length of
    the iris petals!
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个相当不错的数值，因为误差相当低，这意味着我们的模型在预测目标值方面做得很好。在这个阶段，我认为我们可能在预测鸢尾花花瓣长度的比赛中赢得那个大奖！
- en: Diving deeper into the various datasets mentioned in the previous section
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 深入探讨前一小节中提到的各种数据集
- en: 'The **train_test_split** function in our hands-on activity created the following
    subsets from our source dataset:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在动手活动中的**train_test_split**函数从源数据集中创建了以下子集：
- en: '**input_train**: The input features used during training.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**input_train**：训练过程中使用的输入特征。'
- en: '**input_test**: The input features that will be used to test the trained model.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**input_test**：将用于测试训练模型的输入特征。'
- en: '**target_train**: The target labels used during training. During the training
    process, the model uses these values as the known, correct answers that it is
    trying to predict. These are key to the training process because, during training,
    the model tries to learn relationships between the input features that will help
    it predict these answers as accurately as possible.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**target_train**: 在训练过程中使用的目标标签。在训练过程中，模型使用这些值作为它试图预测的已知、正确答案。这些是训练过程中的关键，因为在训练过程中，模型试图学习输入特征之间的关系，以帮助它尽可能准确地预测这些答案。'
- en: '**target_test**: The target labels used during testing. These are known, correct
    answers from the original dataset that were separated out from the training set,
    so they were not included in the training process. Therefore, the model has never
    seen these values during training. We then use these values to test the performance
    of the trained model.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**target_test**: 在测试过程中使用的目标标签。这些是从原始数据集中分离出来的已知、正确答案，因此它们没有被包含在训练过程中。因此，模型在训练过程中从未见过这些值。然后我们使用这些值来测试训练后模型的性能。'
- en: Now that we’ve seen linear regression in action, let’s move on to our first
    SL classification task.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了线性回归的实际应用，让我们继续进行我们的第一个SL分类任务。
- en: Implementing a classification model with XGBoost on Vertex AI
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上使用XGBoost实现分类模型
- en: By now, you’ve started to become familiar with many of the popular libraries
    that are commonly used in data science projects. In this section, we will start
    using another very popular library, XGBoost, which can be used for either classification
    or regression use cases.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你已经开始熟悉许多在数据科学项目中常用的流行库。在本节中，我们将开始使用另一个非常流行的库，XGBoost，它可以用于分类或回归用例。
- en: While we briefly introduced XGBoost at the beginning of this chapter, we will
    dive further into how it works here, starting with the concept of decision trees.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本章开头简要介绍了XGBoost，但在这里我们将更深入地探讨其工作原理，从决策树的概念开始。
- en: Decision trees
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: 'When we discussed the topic of Gradient Boosting earlier in this chapter, we
    mentioned that one of the components of Gradient Boosting is the concept of weak
    learners. Decision trees are one example of what could be used as a weak learner.
    Let’s start with a simple example of what a decision tree is. Refer to *Figure
    5**.11*, which shows a decision tree that is used for estimating whether a bank
    customer is likely to purchase a house, based on their age group and income:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面讨论梯度提升主题时，我们提到梯度提升的一个组成部分是弱学习者的概念。决策树是弱学习者的一个例子。让我们从一个简单的决策树例子开始。参考图5.11，它显示了一个用于估计银行客户是否可能购买房屋的决策树，基于他们的年龄组和收入：
- en: '![Figure 5.11: A decision tree](img/B18143_05_011.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11：决策树](img/B18143_05_011.jpg)'
- en: 'Figure 5.11: A decision tree'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：决策树
- en: A decision tree consists of a sequence of decisions. The process starts at what’s
    referred to as the **root node**, and at each point in the tree, a decision is
    made, which then guides us on the next step to take. The sequence of decisions
    therefore results in a path through the tree, until we reach a final point that
    contains no further decisions. Such a point is referred to as a **leaf node**.
    Following the steps in the tree in *Figure 5**.11*, it determines that a bank
    customer is likely to purchase a house if they are between 25 years old and 65
    years old and earn more than $50,000 per year. All of the other factors in the
    decision tree indicate that they are otherwise unlikely to purchase a house.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树由一系列决策组成。过程从所谓的**根节点**开始，在树的每个点上都会做出一个决策，然后引导我们采取下一步。因此，决策序列导致通过树的一条路径，直到我们达到一个不再包含进一步决策的最终点。这样的点被称为**叶节点**。按照图5.11中的步骤，它确定如果银行客户年龄在25岁到65岁之间，并且年收入超过50,000美元，他们很可能购买房屋。决策树中的所有其他因素都表明，他们不太可能购买房屋。
- en: While the example in *Figure 5**.11* is quite simple, this kind of process can
    be used algorithmically in ML applications, whereby the decision at each point
    is based on some kind of threshold. For example, if the value is less than or
    greater than a certain threshold, then move on to evaluate feature *A*; otherwise,
    evaluate feature *B* or stop evaluating features. When using decision trees, one
    of the goals is to find which features in the dataset can help make the best decisions
    and how many decisions need to be made, which is referred to as the **tree depth**,
    in order to get to the best eventual prediction.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图5.11中的例子相当简单，但这种过程可以在机器学习应用中算法化使用，其中每个点的决策基于某种阈值。例如，如果值小于或大于某个阈值，则继续评估特征A；否则，评估特征B或停止评估特征。在使用决策树时，一个目标就是找到数据集中哪些特征可以帮助做出最佳决策，以及需要做出多少决策，这被称为**树深度**，以便得到最佳预测。
- en: Two key concepts in the decision tree process are **entropy** and **information
    gain**. Entropy, in this context, is the measure of impurity in a given grouping
    of entities. For example, a grouping that perfectly captures identical entities
    in the same group would have zero entropy, whereas a grouping that has lots of
    variability between the entities would have high entropy. Information gain, in
    this context, can be seen as how much the entropy is decreased by each decision
    in our decision tree. In our bank-customer example in *Figure 5**.11*, the initial
    set of customers (before any decisions are made) consists of all of our customers,
    and this set would have a lot of entropy because it would include people of all
    ages and all kinds of economic situations and other characteristics.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树过程中的两个关键概念是**熵**和**信息增益**。在这个上下文中，熵是给定实体分组中不纯度的度量。例如，完美捕捉同一组中相同实体的分组将具有零熵，而具有大量实体之间差异的分组将具有高熵。在这个上下文中，信息增益可以看作是我们决策树中每个决策减少熵的程度。在我们的图5.11中的银行客户例子中，初始客户集（在做出任何决策之前）包括我们所有的客户，这个集合将具有很多熵，因为它将包括所有年龄段和各种经济状况以及其他特征的人。
- en: Now, as we try to figure out which customer features could help our algorithm
    decide whether they would be likely to purchase a house, we find a pattern that
    suggests that people within a certain age group (let’s say, between 25 and 65)
    and who earn over a certain amount of money per year (let’s say, more than $50,000)
    are more likely to purchase a house than other customers. Therefore, in this case,
    a customer’s age and income are examples of features that help to maximize the
    information gain and reduce the entropy and would therefore be good decision point
    features in our decision tree. On the other hand, features such as their name
    or music preferences are unlikely to have any correlation with their probability
    of purchasing a house, so those features would not result in significant information
    gain in the decision points in our decision tree, and the decision tree model
    would likely learn to ignore those features during training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们试图找出哪些客户特征可以帮助我们的算法决定他们是否可能购买房屋时，我们发现一个模式表明，一定年龄组（比如说，25至65岁之间）的人，并且每年收入超过一定数额（比如说，超过50,000美元）的人，比其他客户更有可能购买房屋。因此，在这种情况下，客户的年龄和收入是帮助最大化信息增益并减少熵的例子，因此它们将是我们决策树中良好的决策点特征。另一方面，诸如他们的名字或音乐偏好等特征不太可能与他们购买房屋的概率有任何相关性，因此这些特征不会在我们的决策树中的决策点产生显著的信息增益，决策树模型在训练过程中可能会学会忽略这些特征。
- en: 'Decision tree algorithms can be quite effective for some use cases, although
    they come with some limitations such as being prone to overfitting. This is where
    the concept of ensembles comes into play because combining many trees together
    can make a more powerful prediction model—and one that is also a lot less likely
    to overfit—than any individual tree by itself. This could be done by using a Bagging
    approach, such as the Random Forest algorithm (see *Figure 5**.12* for reference),
    which trains each tree in the ensemble on a random sub-sample (with replacement)
    from the training feature space, or by using a Boosting approach, as we described
    in the case of Gradient Boosting:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法对于某些用例可能非常有效，尽管它们有一些局限性，例如容易过拟合。这就是集成概念发挥作用的地方，因为将许多树结合起来可以创建一个更强大的预测模型——而且比单个树本身更不可能过拟合。这可以通过使用Bagging方法，例如随机森林算法（参见*图
    5.12*以供参考）来实现，该算法在集成中的每个树都使用从训练特征空间中随机子样本（带替换）进行训练，或者可以通过使用Boosting方法，正如我们在梯度提升案例中描述的那样：
- en: '![Figure 5.12: Random Forest](img/B18143_05_12.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12：随机森林](img/B18143_05_12.jpg)'
- en: 'Figure 5.12: Random Forest'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：随机森林
- en: When we use Gradient Boosting with decision trees, we refer to this as **gradient-boosted
    trees**. One of the inherent challenges with a simple gradient-boosted tree implementation
    is the sequential nature of the algorithm, whereby the errors of a tree in one
    training iteration need to be used in the next iteration to train the next tree
    in the ensemble. However, as we mentioned earlier in this chapter, XGBoost overcomes
    this limitation, and it can train thousands of trees in parallel, which vastly
    speeds up the overall training time.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用决策树进行梯度提升时，我们将其称为**梯度提升树**。简单梯度提升树实现的一个固有挑战是算法的顺序性，即一个训练迭代中树的误差需要在下一个迭代中使用来训练集成中的下一个树。然而，正如我们在本章前面提到的，XGBoost克服了这一限制，并且可以并行训练数千棵树，这大大加快了整体训练时间。
- en: 'With this background information in mind, let’s see how we can use XGBoost
    to build a classification model in Vertex AI. The business case here does not
    require its own dedicated callout section because it’s pretty straightforward:
    our model will need to predict the class for each iris, based on its petal and
    sepal measurements.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解这些背景信息的基础上，让我们看看如何使用XGBoost在Vertex AI中构建分类模型。这里的业务案例不需要专门的突出显示部分，因为它相当直接：我们的模型将需要根据每朵鸢尾花的花瓣和萼片测量值来预测其类别。
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The iris dataset contains details about irises that fit into one of three classes:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集包含关于适合三个类别之一的鸢尾花的详细信息：
- en: '- **Iris-Setosa**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '- **Iris-Setosa**'
- en: '- **Iris-Versicolour**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '- **Iris-Versicolour**'
- en: '- **Iris-Virginica**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '- **Iris-Virginica**'
- en: In our linear regression example in the previous section of this chapter, we
    decided to use the **petal length** feature as the target that we were trying
    to predict. However, in this section, we will try to predict the class (or category)
    of each iris flower based on all of the other features in the dataset. Because
    there are more than two classes, this will be a multi-class classification task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前一部分的线性回归示例中，我们决定使用**花瓣长度**特征作为我们试图预测的目标。然而，在本节中，我们将尝试根据数据集中所有其他特征来预测每朵鸢尾花的类别（或分类）。因为存在多个类别，这将是一个多类别分类任务。
- en: The class is a new feature of the iris dataset that we will introduce in this
    section, and we have not interacted with this feature yet. Next, we will talk
    about how to access this feature.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们将介绍一个新的特征——鸢尾花数据集的分类特征，我们尚未与这个特征进行交互。接下来，我们将讨论如何访问这个特征。
- en: The iris dataset in scikit-learn contains multiple objects. One of those objects
    is the **data** object, and that’s what we’ve been using so far in this chapter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的鸢尾花数据集包含多个对象。其中之一是**数据**对象，这是我们本章到目前为止一直在使用的。
- en: 'Another object in the dataset is the **target** object, which contains the
    **class** feature column. The **class** feature column represents the classes
    as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中另一个对象是**目标**对象，它包含**类别**特征列。**类别**特征列表示类别如下：
- en: '**0 =** **Iris-Setosa**'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**0 =** **Iris-Setosa**'
- en: '**1 =** **Iris-Versicolour**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**1 =** **Iris-Versicolour**'
- en: '**2 =** **Iris-Virginica**'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**2 =** **Iris-Virginica**'
- en: 'Remember the following two lines of code from earlier in this chapter:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 记住本章前面提到的以下两行代码：
- en: '**iris =** **load_iris()**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**iris =** **load_iris()**'
- en: '**iris_data =** **iris.data**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**iris_data =** **iris.data**'
- en: Those lines of code loaded the iris dataset in and specifically assigned the
    **data** object from that dataset to the **iris_data** variable. We did not reference
    the **target** object at that time because we did not need to do so.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 那些代码行将iris数据集加载进来，并将该数据集的**data**对象特别分配给**iris_data**变量。当时我们没有引用**target**对象，因为我们不需要这样做。
- en: We will start using the **target** object in this section, and in order to do
    so, we will assign it to a variable named **iris_classes**.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中开始使用**target**对象，为此，我们将将其分配给一个名为**iris_classes**的变量。
- en: 'To train the classification model, perform the following steps in your Vertex
    AI Workbench notebook:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练分类模型，请在您的Vertex AI Workbench笔记本中执行以下步骤：
- en: 'Just as with the libraries we used in the previous section, we will need to
    import the XGBoost library before we can start using it. More specifically, we
    will import the `XGBClassifier` class from the `XGBoost` library:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们在上一节中使用的库一样，在开始使用之前，我们需要导入XGBoost库。更具体地说，我们将从`XGBoost`库中导入`XGBClassifier`类：
- en: '[PRE17]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Considering that we will use it for a classification use case, we will also
    import a function that will calculate a metric that can be used to evaluate a
    classification model, which is called `accuracy`:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到我们将用它来进行分类用例，我们还将导入一个函数，该函数将计算一个可以用于评估分类模型的指标，称为`accuracy`：
- en: '[PRE18]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Assign the `target` object from the iris dataset to the `iris_classes` variable
    so that we can begin to reference it:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将iris数据集的`target`对象分配给`iris_classes`变量，以便我们可以开始引用它：
- en: '[PRE19]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Create our dataset splits for training and testing, just like we did in our
    regression example in the previous section:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建我们的训练和测试数据集拆分，就像我们在上一节中的回归示例中所做的那样：
- en: '[PRE20]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create a model instance and specify the hyperparameters:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个模型实例并指定超参数：
- en: '[PRE21]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Diving deeper
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 深入了解
- en: 'The hyperparameters we’re specifying in the previous piece of code represent
    the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中指定的超参数代表以下内容：
- en: '**n_estimators**: The number of decision trees to use in the ensemble.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**n_estimators**：在集成中使用的决策树数量。'
- en: '**max_depth**: The maximum number of decision points (or maximum depth) for
    each tree.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**max_depth**：每个树的最大决策点（或最大深度）数。'
- en: '**learning_rate**: The learning rate to use during the optimization of the
    cost function.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**learning_rate**：在成本函数优化期间使用的学习率。'
- en: '**objective**: The type of prediction we want our model to perform. In this
    case, we want it to use the **softmax** function to perform multi-class classification.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**objective**：我们希望模型执行的预测类型。在这种情况下，我们希望它使用**softmax**函数进行多类分类。'
- en: 'Let’s train our model:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练我们的模型：
- en: '[PRE22]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will see an output similar to what is shown in *Figure 5**.13*, which summarizes
    the values (including default values) of the model’s parameters:'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到类似于*图5.13*的输出，该图总结了模型参数的值（包括默认值）：
- en: '![Figure 5.13: XGBoost parameters](img/B18143_05_013.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13：XGBoost参数](img/B18143_05_013.jpg)'
- en: 'Figure 5.13: XGBoost parameters'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：XGBoost参数
- en: 'Now, it’s time to get predictions from our model by using the `xgb_input_test`
    dataset:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候使用`xgb_input_test`数据集从我们的模型中获取预测了：
- en: '[PRE23]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s see what it predicted for each item in the `xgb_input_test` dataset:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看它对`xgb_input_test`数据集中的每个项目做出了什么预测：
- en: '[PRE24]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output should be an array of predicted classes, which are represented by
    either 0, 1, or 2, similar to the output shown in *Figure 5**.14*, although the
    values you get may differ:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应该是一个预测类别的数组，这些类别由0、1或2表示，类似于*图5.14*中显示的输出，尽管你得到的值可能不同：
- en: '![Figure 5.14: XGBoost iris classification predictions](img/B18143_05_014.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14：XGBoost iris分类预测](img/B18143_05_014.jpg)'
- en: 'Figure 5.14: XGBoost iris classification predictions'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：XGBoost iris分类预测
- en: 'Did our model get it right? In order to find out, we can check what the known
    correct answers are:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型做对了没有？为了找出答案，我们可以检查已知的正确答案：
- en: '[PRE25]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of this piece of code should also be an array of classes, which
    are represented by either 0, 1, or 2, similar to the output shown in *Figure 5**.15*,
    although the values you get may differ:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码的输出也应该是一个类别的数组，这些类别由0、1或2表示，类似于*图5.15*中显示的输出，尽管你得到的值可能不同：
- en: '![Figure 5.15: The known, correct answers](img/B18143_05_015.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15：已知、正确的答案](img/B18143_05_015.jpg)'
- en: 'Figure 5.15: The known, correct answers'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：已知、正确的答案
- en: 'They look pretty similar to me, but to be sure, let’s test the accuracy by
    comparing our predictions to the known, correct answers:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它们看起来很相似，但为了确保，让我们通过将我们的预测与已知的正确答案进行比较来测试准确性：
- en: '[PRE26]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The score will be presented as a floating-point number representing the accuracy
    percentage. If the result is 1.0, it means that our predictions were 100% accurate!
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分数将以表示准确百分比的浮点数形式呈现。如果结果是1.0，这意味着我们的预测是100%准确的！
- en: Diving deeper
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 深入探讨
- en: 'The accuracy metric quantifies the ratio of correct predictions from all of
    the predictions performed by our model, as represented by the following formula:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度指标量化了我们的模型在所有预测中正确预测的比例，如下公式所示：
- en: '*Accuracy = Correct Predictions / Total Number* *of Predictions*'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确度 = 正确预测数 / 总预测数*'
- en: 'There are many other metrics for assessing a model’s prediction performance,
    and scikit-learn has built-in functions to calculate many of those metrics. As
    additional learning, we recommend you consult the scikit-learn metric documentation
    at the following link: [https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型预测性能有许多其他指标，scikit-learn内置了计算这些指标的功能。作为额外的学习，我们建议你查阅以下链接的scikit-learn指标文档：[https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics)。
- en: Great work! You have officially trained multiple of your own models on Google
    Cloud Vertex AI! Let’s recap on what we’ve learned in this chapter.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你已经在Google Cloud Vertex AI上正式训练了多个自己的模型！让我们回顾一下本章所学的内容。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took many of the ML concepts from *Chapters 1* and *2* and
    put them into practice. We used clustering to find patterns in our data in an
    unsupervised manner, and you specifically learned a lot more about how K-means
    is used for clustering and how it works.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从第1章和第2章中提取了许多机器学习概念并将其付诸实践。我们使用聚类以无监督方式在我们的数据中找到模式，并且你特别学习了更多关于K-means在聚类中的应用及其工作原理。
- en: We then dived into SL, and you explored the linear regression class within scikit-learn
    and learned how to use metrics to measure the performance of a regression model.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入探讨了SL，你探索了scikit-learn中的线性回归类，并学习了如何使用指标来衡量回归模型的性能。
- en: Next, you learned how to use XGBoost to build a classification model and classify
    items in the iris dataset based on their features.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你学习了如何使用XGBoost构建分类模型，并根据特征对鸢尾花数据集中的项目进行分类。
- en: Not only did you put all of those important concepts into practice, but you
    also learned how to create and use Vertex AI Workbench-managed notebooks.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅你将所有这些重要概念付诸实践，而且你还学会了如何创建和使用Vertex AI Workbench管理的笔记本。
- en: Additionally, you learned other important concepts in the ML industry, such
    as how decision trees work, how Gradient Boosting works, and how XGBoost enhances
    that functionality to implement one of the most effective ML algorithms in the
    industry.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还学习了机器学习行业中的其他重要概念，例如决策树的工作原理、梯度提升的工作原理以及XGBoost如何增强该功能以实现行业中最有效的机器学习算法之一。
- en: That was a lot of stuff to learn in one chapter, and you should be proud that
    you are significantly elevating your skills and knowledge in areas that are in
    high demand in today’s business world.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你需要学习很多东西，你应该为自己的技能和知识在当今商业世界中需求很高的领域得到显著提升而感到自豪。
- en: In the next chapter, you will learn another set of extremely important skills
    as we will focus on data analysis and data transformation for ML workloads.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习另一组极其重要的技能，因为我们将会关注机器学习工作负载的数据分析和数据转换。
