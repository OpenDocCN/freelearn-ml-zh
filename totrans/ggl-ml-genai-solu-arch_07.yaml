- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Custom ML Models on Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we implemented AI/ML workloads by letting Google do all
    of the work for us. Now is the point at which we’re going to elevate our knowledge
    and skills to an expert level by building our own models from scratch on Google
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: We will use popular software libraries that are commonly used in data science
    projects, and we will start implementing some of the concepts we discussed in
    previous chapters, such as **unsupervised ML** (**UML**) and **supervised ML**
    (**SML**), including clustering, regression, and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Background information – libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UML with scikit-learn on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a regression model with scikit-learn on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a classification model with XGBoost on Vertex AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Background information – libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive in and start building our own models, let’s take a moment to
    discuss some of the software libraries we will use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs: []
  type: TYPE_NORMAL
- en: A software library is a collection of code and data that includes useful functions
    and tools for specific types of programming tasks. When common types of programming
    tasks are identified in a given industry, such as data manipulation or implementing
    complex mathematical equations, then usually somebody will eventually create a
    library that contains the code and other resources required to perform those tasks.
    The library can then easily be used by others to achieve those same tasks and
    potentially extended to add more functionality over time, rather than everybody
    needing to write the code to perform those common tasks over and over again. Without
    libraries, programmers would have to build everything from scratch all of the
    time and would waste a lot of time on rudimentary programming tasks. In this chapter,
    we will use libraries such as scikit-learn, Matplotlib, and XGBoost. Later in
    this book, we will use other libraries such as TensorFlow and PyTorch, and we
    will describe those libraries in more detail in their respective chapters.
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: scikit-learn, also referred to as `sklearn`, is an open source Python library
    that has lots of useful data science tools and practice datasets built in. It’s
    like a “Swiss Army knife” for data science, and it’s a popular starting point
    for budding data scientists to begin working on ML projects because it’s relatively
    easy to start using, as you will see in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), we discussed typical stages
    that exist in almost all data science projects, and how one of those stages focuses
    on data exploration. We also discussed that “visualization” is typically an important
    part of the data exploration phase, in which data scientists and engineers use
    tools to create visual representations of the characteristics of their datasets.
    These visual representations, such as graphs, can help data scientists and engineers
    build a better understanding of the datasets, as well as how those datasets are
    affected by experiments and transformations that are performed during data science
    projects. Data scientists and engineers also often want to build visualizations
    that represent other aspects of their data science project activities, such as
    increases or decreases in metrics that help determine how a model is performing.
    In the words of Matplotlib’s developers, “*Matplotlib is a comprehensive library
    for creating static, animated, and interactive visualizations in Python.*” As
    such, Matplotlib is a commonly used library for creating visual representations
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pandas is a Python library for data manipulation and analysis. It is used for
    both data exploration and for performing transformations on data. For example,
    with pandas, we could read data in from a file or other data source, preview a
    subset of the data to understand what it contains, and perform statistical analysis
    on the dataset. Then, we could also use pandas to make changes to the data to
    prepare it for training an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015), we discussed how the concept
    of Gradient Descent is commonly used in the ML model training process. XGBoost
    is a popular ML library that is based on the concept of **Gradient Boosting**,
    which uses an **ensemble** approach that combines many small models (often referred
    to as **weak learners**) to create a better overall prediction model. In the case
    of Gradient Boosting, each iteration in the training process trains a small model.
    And, as is the case in almost every model training process, the resulting model
    will make some incorrect predictions. The next iteration in the Gradient Boosting
    process then trains a model on those **residual errors** made by the previous
    model. This helps to “boost” the training process in each subsequent iteration,
    with the intention of creating a stronger prediction model overall. XGBoost, which
    stands for Extreme Gradient Boosting, overcomes the sequential limitations of
    previous Gradient Boosting algorithms, and it can train thousands of weak learners
    in parallel. We will describe how XGBoost works in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In general, the concept of “ensemble” model training refers to the combination
    of many weak models to create a better or “stronger” overall prediction mechanism,
    often referred to as a “meta-model.” Boosting is just one example of an ensemble
    approach. Other examples include “Bagging” (or “Bootstrap Aggregation”), which
    trains many weak learners on subsets of the data, and “Stacking,” which can be
    used to combine models trained with completely different algorithms. The intent,
    in each case, is to build a more useful prediction mechanism than could be achieved
    by any of the models individually.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the ensemble implementation, the predictions of each model in the
    ensemble could be combined in different ways, such as being summed or averaged,
    or in classification use cases, a voting mechanism may be implemented in order
    to determine the resulting best prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like in the previous chapter, we will perform some initial activities that
    are required before we can start to perform the primary activities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud makes it even easier to get started with the data science libraries
    we described in the previous sections because we can use Vertex AI Workbench notebooks
    that already have these libraries installed. If you wanted to use these libraries
    outside of Vertex AI, you would need to install them yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering that we’re going to use Vertex AI notebooks, and we will not need
    to explicitly install the libraries, we will not include details on how to do
    that in this book. If you would like to install the libraries in another environment,
    including **Google Compute Engine** (**GCE**) or **Google Kubernetes Engine**
    (**GKE**), you can find installation instructions on the respective websites for
    each of those libraries, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/install.xhtml](https://scikit-learn.org/stable/install.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://matplotlib.org/stable/users/installing/index.xhtml](https://matplotlib.org/stable/users/installing/index.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://xgboost.readthedocs.io/en/stable/install.xhtml](https://xgboost.readthedocs.io/en/stable/install.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will discuss Vertex AI Workbench notebooks in a bit more detail, and
    how to create a notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, you will see pieces of code that import these libraries.
    This is not the same as installing. The libraries are already installed in our
    Vertex AI Workbench notebooks, but we just need to import them into our notebook
    context in order to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Workbench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vertex AI Workbench is a development environment in Google Cloud that enables
    you to manage all of your AI/ML development needs within Vertex AI. It is based
    on Jupyter notebooks, which provide an interactive interface for writing and running
    code. This makes notebooks extremely versatile because you can codify interactions
    not only with the broader Vertex AI ecosystem but also with other Google Cloud
    service APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Workbench notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are three types of notebooks you can use within Vertex AI Workbench:'
  prefs: []
  type: TYPE_NORMAL
- en: Managed notebooks, which, as the name suggests, are managed for you by Google.
    These are a good default option for many use cases because they have a lot of
    useful tools built in and are ready to go on Google Cloud. They run in a JupyterLab
    environment, which is a web-based interactive development environment for notebooks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-managed notebooks, which, as the name suggests, are managed by you. These
    are suitable if we require significant customization of our environment. They
    are customizable instances of Google Cloud’s Deep Learning VMs, which are **virtual
    machine** (**VM**) images that include tools for implementing **deep learning**
    (**DL**) workloads. However, these notebooks can be used for more than just DL
    use cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vertex AI Workbench **instances**, which are the newest option released by Google
    Cloud in late 2023\. These can be seen as a hybrid of the previous two options,
    and will likely become the primary option over time. At the time of writing this
    in September 2023, this option is only available in preview mode, so we will stick
    with *options 1* and *2* for now.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google also provides an offering called Colab, which is a very popular service
    that allows you to run free notebooks over the public internet. In late 2023,
    Google Cloud also released an option named Colab Enterprise, which enables customers
    to use Colab within their own Google Cloud environment. At the time of writing
    this, Colab Enterprise is also only available in preview mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Managed notebooks and user-managed notebooks are being deprecated, so in this
    book we will primarily use the newest option: Vertex AI Workbench instances. Let''s
    go ahead and create one now.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Vertex AI Workbench instance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a Vertex AI Workbench instance, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For configuration parameters that are not specifically called out in these instructions,
    leave those parameters at their default values
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the Google Cloud services menu → **Vertex
    AI** → **Workbench**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Instances** tab and click **Create New**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the side panel that appears (see *Figure 5**.1* for reference), enter a
    name for your notebook. Alternatively, you can use the default name that’s automatically
    generated in that input field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1: The New instance dialogue](img/B18143_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: The New instance dialogue'
  prefs: []
  type: TYPE_NORMAL
- en: Select your preferred region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the option to attach a GPU (at the time of writing, you should select
    the box that says **Attach 1 NVIDIA T4 GPU**). See *Figure 5**.1* for reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave the networking configuration details at their default values for now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the bottom of the side panel, click **Advanced Options**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the next screen that appears (see *Figure 5**.2* for reference), ensure
    that the option to **Enable Dataproc** is selected and click **Continue** (you
    may need to scroll down to see the **Continue** button):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2: The New instance dialogue (continued)](img/B18143_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: The New instance dialogue (continued)'
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen that appears (that is, the environment configuration screen),
    select **Use the latest version** and click **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen that appears, you can use all of the default values (unless
    you have preferences to change anything) and click **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can click the **Continue** button two more times (that is, to accept the
    default values on the next two screens, unless you have preferences to change
    anything) until you reach the **Networking** configuration screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Networking configuration** screen, ensure that the **Assign external
    IP address** option is selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unless you have any specific networking configuration needs, you can simply
    use the default network in your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Continue**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen that appears (that is, the **IAM & Security** screen), ensure
    that all the options in the **Security options** section are selected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **CREATE** (at the bottom of the screen) and wait a few minutes for the
    notebook to be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the notebook is up and running, a green checkmark will appear to the left
    of the notebook’s name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, click **Open JupyterLab**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, I’d like to highlight an important set of integrations and features
    that Google Cloud has added to the JupyterLab interface in Vertex AI Notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI Notebook JupyterLab integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the JupyterLab interface in your Vertex AI Notebook instance, you will notice
    a set of icons on the far left-hand side of the screen, as shown in *Figure 5**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Google Cloud JupyterLab integrations](img/B18143_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Google Cloud JupyterLab integrations'
  prefs: []
  type: TYPE_NORMAL
- en: These icons represent integrations that we can use directly within our JupyterLab
    Notebook instances in Vertex AI. For example, if we click on the BigQuery icon,
    we can see our BigQuery datasets, and we can even use the integrated SQL Editor
    to run SQL queries on our BigQuery datasets directly from the JupyterLab Notebook
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend clicking on the various icons to learn more about what you can do
    with them. Other useful features include the ability to integrate with Google
    Cloud Storage and an option that enables us to schedule the execution of our notebooks.
    The latter is a very useful feature for workloads that need to be automatically
    repeated periodically, which is a common need in data science (for example, re-training,
    evaluating, and deploying a model on new data every day).
  prefs: []
  type: TYPE_NORMAL
- en: Now that the notebook instance has been created, we will clone our GitHub repository
    so that we can access our notebook code and follow along with the activities in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cloning the GitHub repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cloning our GitHub repository is the easiest way to quickly import all of the
    resources for the hands-on activities in this chapter into your notebook instance
    in Google Cloud Vertex AI. To clone our repository into your notebook, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Git** symbol in the menu on the left of the screen. The symbol
    will look like the one shown in *Figure 5**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4: Git symbol](img/B18143_05_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Git symbol'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Clone Repository**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter our repository URL: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any options are displayed, leave them at their default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Clone**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see a new folder appear in your notebook, named `Google-Machine-Learning-for-Solutions-Architects`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we’re ready to start using our notebook instance! Let’s move on to the
    next section, in which we will perform some unsupervised training.
  prefs: []
  type: TYPE_NORMAL
- en: UML with scikit-learn on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will start using our Vertex AI Workbench notebook to train
    models. We will begin with a relatively simple use case in which we will create
    an unsupervised model to discover clustering patterns in our data. Before we dive
    into the code, we will first take a minute to discuss the clustering algorithm
    we will use in this section, which is called K-means.
  prefs: []
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You may remember that we discussed **unsupervised learning** (**UL**) mechanisms
    such as clustering in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015). Remember
    that in clustering, data points are grouped together based on similarities between
    features or characteristics that are observed by the model. *Figure 5**.5* provides
    a visual representation of this concept, showing the input data on the left and
    the resulting data clusters on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5: Clustering](img/B18143_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Clustering'
  prefs: []
  type: TYPE_NORMAL
- en: 'K-means is an example of a clustering algorithm, and it is categorized as a
    centroid-based clustering algorithm. What this means is that it chooses a **centroid**,
    which is a point that represents the center of each of our clusters. The members
    of each cluster are data points from our dataset, and their membership in each
    cluster will depend on a mathematical evaluation of how close or far they are
    from each centroid. This proximity or distance from each centroid is generally
    calculated in terms of the Euclidean distance, represented by *Equation 5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>d</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>)</mo><mo>=</mo><msqrt><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow></mrow></math>](img/11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 5.1: Euclidean distance'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry—we will discuss this in more detail. The graphs shown in *Figure
    5**.5* represent what’s called the feature space, and they show where each of
    our data points exists in the feature space. In the case of the aforementioned
    graphs, they represent a two-dimensional feature space, and each data point has
    an *x* and *y* coordinate that represents where the data point exists in the feature
    space. That is to say, the *x* and *y* coordinates are the features of each data
    point. If we take any two points on the graph, the Euclidean distance is simply
    the distance in a straight line between those two points, which is calculated
    by putting the *x* and *y* coordinates (that is, the features of each data point)
    into the equation represented in *Equation 5.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Data points can have more than *x* and *y* coordinates as their features, and
    the concept applies in higher-dimensional feature spaces also.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a more concrete example than just *x* and *y* coordinates. Our dataset
    could consist of information regarding a retail company’s customers, and the features
    of each customer could include their age, the city they live in, and what they
    purchased in their most recent visit to the company’s store. Clustering algorithms
    such as K-means may then group those customers by finding similarities between
    their features and the centroids in each group.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first questions that often comes up when discussing K-means is:
    How are centroids chosen? For example, how does the algorithm know how many centroids
    to use, and where to place them in the feature space? Let’s address the number
    of centroids first. This is specified as a “hyperparameter” to the algorithm.
    This means that you can tell the algorithm how many centroids to use (and therefore
    how many clusters to form). In fact, this is reflected in the name, “K-means,”
    where K represents the number of centroids or clusters. You would generally try
    different numbers of centroids until you find a value that maximizes the amount
    of information gained and minimizes the amount of variance, in each cluster. A
    mechanism that is often used to find the optimal value for K is referred to as
    “the elbow method.” Using the elbow method, we run K-means clustering on the dataset
    for a range of different values of K (such as 1-10). Then, for each value of K,
    the average score is computed for all clusters. The default computed score is
    called the **distortion**. The distortion represents the sum of the squared distances
    from each point to the centroid in their assigned cluster, which again relates
    back to *Equation 5.1*. If you graph the value of K against the distortion score
    for each run, you will usually notice the distortion score going down each time
    you increase the value of K. The distortion score will usually go down sharply
    in the beginning and eventually start to go down in smaller increments. When adding
    new clusters (that is, increasing the value of K) stops significantly reducing
    the distortion score, then you can usually consider the optimal value of K to
    have been found.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s discuss how the algorithm knows where to place the centroids in
    the feature space. It begins by placing them at random locations in the feature
    space and randomly assigning data points to each centroid. It then repeats the
    following steps until no new data points are added to each cluster, at which point
    the optimal cluster positioning is believed to have been calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate which data points are closest to the centroid and assign them to that
    centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the average (mean) between those points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve covered the theory of how K-means clustering works, let’s move
    on to the fun part, which is to actually implement the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a UML workload in Vertex AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’re going to use the K-means algorithm within scikit-learn to start implementing
    our UML workload in Vertex AI. One of the quintessential clustering examples using
    K-means in scikit-learn is to use what’s referred to as the iris dataset, to find
    cluster patterns in the data. The iris dataset, as the name implies, contains
    data on various iris flowers.
  prefs: []
  type: TYPE_NORMAL
- en: Business case
  prefs: []
  type: TYPE_NORMAL
- en: Our company, *BigFlowers.com*, recently acquired a smaller flower company, and
    in doing so, we acquired all of their digital assets, including the datasets that
    contain their flower inventory. Unfortunately, they did not do a good job of documenting
    their datasets, so we don’t have a good idea of what’s in their inventory.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve been tasked with finding out as much as we can about the contents of the
    flower inventory, so we’re going to use some data analytics tools and ML models
    to learn more about the dataset. One of the first things we’re going to do is
    try to find any patterns such as logical groupings, which could help us to understand
    if there are distinct categories of objects in the dataset and additional information
    such as how many distinct categories exist.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*: At the time of writing this, the company *BigFlowers.com* does not
    exist, and our exercises here refer to a fictitious company for example purposes
    only.'
  prefs: []
  type: TYPE_NORMAL
- en: To get started with this task, navigate into the `Google-Machine-Learning-for-Solutions-Architects`
    folder in the Vertex AI notebook you created in the previous section. Then, navigate
    into the `Chapter-05` folder, double click on the `Chapter-5.ipynb` file (if prompted,
    select the `Python` kernel), and perform the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do in our notebook is to import the resources that
    we need, such as the scikit-learn K-means class, and the pandas and Matplotlib
    libraries. We will also import a function to load the iris dataset from scikit-learn.
    To perform these actions, enter the following code into the interactive prompt
    in our notebook (or use the notebook you cloned from GitHub) The following code
    in the notebook performs those steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To execute the code, hold the *Shift* key on your keyboard, and press the *Enter*
    key. Considering that the code is simply importing libraries, you will not see
    any output displayed back to the screen unless an error occurs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: From here onward, when we are performing activities in Vertex AI Workbench notebooks,
    we will just provide the code samples for each step, and it will imply that you
    need to enter that code into the next available empty cell in the notebook (unless
    you are using the cloned notebook that already contains the code) and execute
    the cell, just as you did with the previous piece of code.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left side of each cell in a Jupyter notebook, there is a square bracket
    symbol that looks like this: **[ ]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be used to understand the status of the cell, and the indicators are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[ ]** (Empty): The cell has not yet been executed.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[*]** (Asterisk): The cell is currently executing.'
  prefs: []
  type: TYPE_NORMAL
- en: '**[1]** (Any number): the cell has completed executing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will read the iris dataset in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use pandas to get some information about our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like what is shown in *Figure 5**.6*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6: pandas.DataFrame.info() output](img/B18143_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: pandas.DataFrame.info() output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `info()` function output shows us what kinds of data our dataset contains.
    In this case, we can see that it contains 150 rows (indexed from 0 to 149) and
    4 columns (indexed from 0 to 3), and the data values in each cell are floating-point
    numbers. Each row in our dataset is a data point, which represents a particular
    iris flower, and each column is a feature in our dataset. If we look at the description
    for the iris dataset in the scikit-learn documentation, which can be found at
    [https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#iris-dataset),
    we can see that the features are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From this, we can understand that the floating-point numbers in each cell in
    our dataset are measurements of those four aspects of each flower.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can also preview a subset of the data to see the actual values in each cell,
    like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look like what is shown in *Figure 5**.7*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7: pandas.DataFrame.head() output](img/B18143_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: pandas.DataFrame.head() output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’re going to use K-means to group similar data points in the dataset
    together, based on their features. First, we’re creating an instance of a K-means
    model, and we’re specifying that it will have three clusters because in this case,
    the dataset documentation told us that there are three different categories of
    iris in our dataset. If we didn’t already know how many clusters we needed to
    use, then we could try different numbers of clusters and use the elbow method
    to find the best number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, we have defined our model, but it has not yet learned anything
    from our data. Next, we instruct the model to “fit” to our dataset. The term *fit*
    is used by many algorithms to refer to the training process because, during training,
    this is exactly what the algorithm is trying to do; it is trying to create a model
    that fits as accurately as possible (without overfitting) to the given dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When our model has completed the training process, we can now get it to cluster
    our data. We will feed our original dataset into the model, and according to the
    patterns it learned during training, it will place the data points into each of
    the clusters it defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we store the model’s labels in a variable so that we can visualize
    the clusters in the next cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will visualize the clusters that K-means has created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting graph should look similar to the graph depicted in *Figure 5**.8*.
    Notice how we can see three distinct clusters of data points in the graph, where
    each distinct cluster is color-coded as either purple, green, or yellow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.8: K-means cluster graph](img/B18143_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: K-means cluster graph'
  prefs: []
  type: TYPE_NORMAL
- en: This gives us some useful information. We can see the three distinct categories
    or clusters of data points in our dataset, in which the data points share similar
    characteristics with other points in their same cluster but differ from the data
    points in other clusters. We get these insights without ever needing to label
    our dataset, which is quite useful, considering that labeling is a time-consuming
    and error-prone task.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Although our dataset contains four features (also referred to as “dimensions”),
    humans can only see/visualize things in three dimensions. Therefore, we only used
    three of the four features to create our graph. The following line is where we
    defined the features to use in our graph, in which the numbers represent the features
    to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**axes.scatter(iris_data[:, 2], iris_data[:, 3], iris_data[:,** **1], c=labels.astype(float))**'
  prefs: []
  type: TYPE_NORMAL
- en: You can try playing around with the graph by changing each of those numbers
    to anything between 0 and 3, as long as each entry is a unique value (that is,
    don’t repeat any of the numbers more than once in the code).
  prefs: []
  type: TYPE_NORMAL
- en: Each time you make a change, you can execute the cell again to update the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen what it takes to implement a UML workload in Vertex AI,
    let’s move on and learn how to implement an SML workload in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a regression model with scikit-learn on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first SML model we’re going to build in our Vertex AI Workbench notebook
    is a linear regression model. You may remember that we described linear regression
    in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
  prefs: []
  type: TYPE_NORMAL
- en: Business case
  prefs: []
  type: TYPE_NORMAL
- en: Our boss at *BigFlowers.com* is running a fun competition at work. Employees
    are asked to predict the length of an iris flower’s petal when given some other
    measurements related to that flower, such as the sepal length, sepal width, and
    petal width. The person with the most accurate estimation will get a big prize,
    and employees are allowed to use technology to help them in their estimations,
    so we’re going to build an ML model to help us make these predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we used the K-means algorithm within scikit-learn.
    In this section, we will use the linear regression algorithm within scikit-learn.
    As such, we will need to import the `LinearRegression` class into our notebook
    context. We will also import a function that will calculate the **Mean Squared
    Error** (**MSE**) metric, which we described in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035)
    and which is commonly used to evaluate linear regression models.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the same iris dataset for our linear regression model, so we won’t
    need to repeat any of the previous dataset importing steps because it is already
    loaded into our notebook context. This is an important concept to note; we can
    use different types of models on the same data, depending on the business use
    case and desired results. However, as we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    SML model training introduces some additional requirements regarding how the dataset
    is used, which we describe next.
  prefs: []
  type: TYPE_NORMAL
- en: Remember
  prefs: []
  type: TYPE_NORMAL
- en: In **supervised learning** (**SL**) use cases, we need to define which column
    in the dataset is designated as the “target” feature. This is the feature that
    we’re trying to predict based on the other features in the dataset. During training,
    some of the elements in this column are used as labels that represent known, correct
    answers from which the model learns.
  prefs: []
  type: TYPE_NORMAL
- en: The values of the other features in the dataset are used as inputs. During the
    prediction process, the model uses the relationships it has learned between all
    of the input features and the target feature to predict the value of the target
    feature based on the values of the input features.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that in SL use cases, we generally split our dataset into subsets
    such as training, validation, and testing. The training dataset, as the name implies,
    is what the model is trained on. The testing dataset is how we evaluate the trained
    model (based on the metrics that we’ve defined for that model), and the validation
    set is usually used in hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note*:'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore hyperparameter tuning in a later chapter. In our current chapter,
    we will train a single regression model and we will then test it directly, so
    we will not need to split out a validation subset of our data. Therefore, we will
    just split the dataset into subsets for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start building our linear regression model, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `LinearRegression` class from scikit-learn and import a function
    that will make it easy for us to split our dataset into train and test datasets,
    as well as a function that will calculate the MSE metric that we will use later
    to evaluate our model’s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define which column in our dataset we want to use as the target feature to
    predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remember:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section of this chapter, we consulted the documentation for
    the iris dataset, and we saw that the columns in the dataset are as follows: sepal
    length in cm, sepal width in cm, petal length in cm, petal width in cm. These
    columns are indexed from 0 to 3\. Considering that we want to predict the petal
    length, we have selected column 2 of our **iris_df** dataframe as our target column.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, define our input features. In this case, we use all feature columns except
    column 2 (because we’ve defined that column 2 is our target):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split our dataset into separate subsets for training and testing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Specifying a value of 0.2 for the **test_size** variable means that 20% of the
    original dataset will be separated to create the test dataset. The remaining 80%
    then forms the training dataset. The **input_train** and **input_test** datasets
    contain the input features used during training and the input features that will
    be used to test the trained model, respectively. The **target_train** and **target_test**
    datasets contain the respective labels (that is, the “correct answers”) for the
    training and test datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the training dataset to train our linear regression model, and
    then use the test dataset to generate predictions from the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember
  prefs: []
  type: TYPE_NORMAL
- en: To test the trained model, we send the **input_test** features to it, and we
    ask it to predict what the corresponding target feature values should be, based
    on the values of the **input_test** features. We can then compare the model’s
    predictions to the **target_test** values (which are the known, correct answers)
    in order to see how close its predictions were to the correct, expected values.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s time to see what kinds of predictions our model has made based
    on the `input_test` data!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look similar to what is shown in *Figure 5**.9*, although
    the numbers may differ (the predictions are in the right-hand column, disregarding
    the number 0 at the top, which is the column header):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.9: Linear regression model predictions](img/B18143_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Linear regression model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look at the corresponding known, correct values from the
    `target_test` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should look similar to what is shown in *Figure 5**.10*, although
    the numbers may differ (the values are in the right-hand column, disregarding
    the number 2 at the top, which is the column header):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.10: The known, correct values from the target_test dataset](img/B18143_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: The known, correct values from the target_test dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, the numbers are pretty close in some cases. However, let’s use
    metrics to evaluate the model’s overall performance. To do this, we use the `mean_squared_error()`
    function to compare the predictions against the correct values from the `target_test`
    dataset and generate the MSE metric value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The value should be something like 0.08871798773326277.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a pretty good value because the error is quite low, which means that
    our model is doing a good job of predicting the target values. At this point,
    I think we might win that big prize in the competition to predict the length of
    the iris petals!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Diving deeper into the various datasets mentioned in the previous section
  prefs: []
  type: TYPE_NORMAL
- en: 'The **train_test_split** function in our hands-on activity created the following
    subsets from our source dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_train**: The input features used during training.'
  prefs: []
  type: TYPE_NORMAL
- en: '**input_test**: The input features that will be used to test the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**target_train**: The target labels used during training. During the training
    process, the model uses these values as the known, correct answers that it is
    trying to predict. These are key to the training process because, during training,
    the model tries to learn relationships between the input features that will help
    it predict these answers as accurately as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '**target_test**: The target labels used during testing. These are known, correct
    answers from the original dataset that were separated out from the training set,
    so they were not included in the training process. Therefore, the model has never
    seen these values during training. We then use these values to test the performance
    of the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen linear regression in action, let’s move on to our first
    SL classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a classification model with XGBoost on Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, you’ve started to become familiar with many of the popular libraries
    that are commonly used in data science projects. In this section, we will start
    using another very popular library, XGBoost, which can be used for either classification
    or regression use cases.
  prefs: []
  type: TYPE_NORMAL
- en: While we briefly introduced XGBoost at the beginning of this chapter, we will
    dive further into how it works here, starting with the concept of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we discussed the topic of Gradient Boosting earlier in this chapter, we
    mentioned that one of the components of Gradient Boosting is the concept of weak
    learners. Decision trees are one example of what could be used as a weak learner.
    Let’s start with a simple example of what a decision tree is. Refer to *Figure
    5**.11*, which shows a decision tree that is used for estimating whether a bank
    customer is likely to purchase a house, based on their age group and income:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11: A decision tree](img/B18143_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: A decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree consists of a sequence of decisions. The process starts at what’s
    referred to as the **root node**, and at each point in the tree, a decision is
    made, which then guides us on the next step to take. The sequence of decisions
    therefore results in a path through the tree, until we reach a final point that
    contains no further decisions. Such a point is referred to as a **leaf node**.
    Following the steps in the tree in *Figure 5**.11*, it determines that a bank
    customer is likely to purchase a house if they are between 25 years old and 65
    years old and earn more than $50,000 per year. All of the other factors in the
    decision tree indicate that they are otherwise unlikely to purchase a house.
  prefs: []
  type: TYPE_NORMAL
- en: While the example in *Figure 5**.11* is quite simple, this kind of process can
    be used algorithmically in ML applications, whereby the decision at each point
    is based on some kind of threshold. For example, if the value is less than or
    greater than a certain threshold, then move on to evaluate feature *A*; otherwise,
    evaluate feature *B* or stop evaluating features. When using decision trees, one
    of the goals is to find which features in the dataset can help make the best decisions
    and how many decisions need to be made, which is referred to as the **tree depth**,
    in order to get to the best eventual prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Two key concepts in the decision tree process are **entropy** and **information
    gain**. Entropy, in this context, is the measure of impurity in a given grouping
    of entities. For example, a grouping that perfectly captures identical entities
    in the same group would have zero entropy, whereas a grouping that has lots of
    variability between the entities would have high entropy. Information gain, in
    this context, can be seen as how much the entropy is decreased by each decision
    in our decision tree. In our bank-customer example in *Figure 5**.11*, the initial
    set of customers (before any decisions are made) consists of all of our customers,
    and this set would have a lot of entropy because it would include people of all
    ages and all kinds of economic situations and other characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Now, as we try to figure out which customer features could help our algorithm
    decide whether they would be likely to purchase a house, we find a pattern that
    suggests that people within a certain age group (let’s say, between 25 and 65)
    and who earn over a certain amount of money per year (let’s say, more than $50,000)
    are more likely to purchase a house than other customers. Therefore, in this case,
    a customer’s age and income are examples of features that help to maximize the
    information gain and reduce the entropy and would therefore be good decision point
    features in our decision tree. On the other hand, features such as their name
    or music preferences are unlikely to have any correlation with their probability
    of purchasing a house, so those features would not result in significant information
    gain in the decision points in our decision tree, and the decision tree model
    would likely learn to ignore those features during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree algorithms can be quite effective for some use cases, although
    they come with some limitations such as being prone to overfitting. This is where
    the concept of ensembles comes into play because combining many trees together
    can make a more powerful prediction model—and one that is also a lot less likely
    to overfit—than any individual tree by itself. This could be done by using a Bagging
    approach, such as the Random Forest algorithm (see *Figure 5**.12* for reference),
    which trains each tree in the ensemble on a random sub-sample (with replacement)
    from the training feature space, or by using a Boosting approach, as we described
    in the case of Gradient Boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12: Random Forest](img/B18143_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Random Forest'
  prefs: []
  type: TYPE_NORMAL
- en: When we use Gradient Boosting with decision trees, we refer to this as **gradient-boosted
    trees**. One of the inherent challenges with a simple gradient-boosted tree implementation
    is the sequential nature of the algorithm, whereby the errors of a tree in one
    training iteration need to be used in the next iteration to train the next tree
    in the ensemble. However, as we mentioned earlier in this chapter, XGBoost overcomes
    this limitation, and it can train thousands of trees in parallel, which vastly
    speeds up the overall training time.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this background information in mind, let’s see how we can use XGBoost
    to build a classification model in Vertex AI. The business case here does not
    require its own dedicated callout section because it’s pretty straightforward:
    our model will need to predict the class for each iris, based on its petal and
    sepal measurements.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The iris dataset contains details about irises that fit into one of three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Iris-Setosa**'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Iris-Versicolour**'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Iris-Virginica**'
  prefs: []
  type: TYPE_NORMAL
- en: In our linear regression example in the previous section of this chapter, we
    decided to use the **petal length** feature as the target that we were trying
    to predict. However, in this section, we will try to predict the class (or category)
    of each iris flower based on all of the other features in the dataset. Because
    there are more than two classes, this will be a multi-class classification task.
  prefs: []
  type: TYPE_NORMAL
- en: The class is a new feature of the iris dataset that we will introduce in this
    section, and we have not interacted with this feature yet. Next, we will talk
    about how to access this feature.
  prefs: []
  type: TYPE_NORMAL
- en: The iris dataset in scikit-learn contains multiple objects. One of those objects
    is the **data** object, and that’s what we’ve been using so far in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another object in the dataset is the **target** object, which contains the
    **class** feature column. The **class** feature column represents the classes
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0 =** **Iris-Setosa**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 =** **Iris-Versicolour**'
  prefs: []
  type: TYPE_NORMAL
- en: '**2 =** **Iris-Virginica**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the following two lines of code from earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**iris =** **load_iris()**'
  prefs: []
  type: TYPE_NORMAL
- en: '**iris_data =** **iris.data**'
  prefs: []
  type: TYPE_NORMAL
- en: Those lines of code loaded the iris dataset in and specifically assigned the
    **data** object from that dataset to the **iris_data** variable. We did not reference
    the **target** object at that time because we did not need to do so.
  prefs: []
  type: TYPE_NORMAL
- en: We will start using the **target** object in this section, and in order to do
    so, we will assign it to a variable named **iris_classes**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the classification model, perform the following steps in your Vertex
    AI Workbench notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with the libraries we used in the previous section, we will need to
    import the XGBoost library before we can start using it. More specifically, we
    will import the `XGBClassifier` class from the `XGBoost` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Considering that we will use it for a classification use case, we will also
    import a function that will calculate a metric that can be used to evaluate a
    classification model, which is called `accuracy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the `target` object from the iris dataset to the `iris_classes` variable
    so that we can begin to reference it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create our dataset splits for training and testing, just like we did in our
    regression example in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a model instance and specify the hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Diving deeper
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperparameters we’re specifying in the previous piece of code represent
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**n_estimators**: The number of decision trees to use in the ensemble.'
  prefs: []
  type: TYPE_NORMAL
- en: '**max_depth**: The maximum number of decision points (or maximum depth) for
    each tree.'
  prefs: []
  type: TYPE_NORMAL
- en: '**learning_rate**: The learning rate to use during the optimization of the
    cost function.'
  prefs: []
  type: TYPE_NORMAL
- en: '**objective**: The type of prediction we want our model to perform. In this
    case, we want it to use the **softmax** function to perform multi-class classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see an output similar to what is shown in *Figure 5**.13*, which summarizes
    the values (including default values) of the model’s parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.13: XGBoost parameters](img/B18143_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: XGBoost parameters'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, it’s time to get predictions from our model by using the `xgb_input_test`
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see what it predicted for each item in the `xgb_input_test` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be an array of predicted classes, which are represented by
    either 0, 1, or 2, similar to the output shown in *Figure 5**.14*, although the
    values you get may differ:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.14: XGBoost iris classification predictions](img/B18143_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: XGBoost iris classification predictions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Did our model get it right? In order to find out, we can check what the known
    correct answers are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this piece of code should also be an array of classes, which
    are represented by either 0, 1, or 2, similar to the output shown in *Figure 5**.15*,
    although the values you get may differ:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.15: The known, correct answers](img/B18143_05_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: The known, correct answers'
  prefs: []
  type: TYPE_NORMAL
- en: 'They look pretty similar to me, but to be sure, let’s test the accuracy by
    comparing our predictions to the known, correct answers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The score will be presented as a floating-point number representing the accuracy
    percentage. If the result is 1.0, it means that our predictions were 100% accurate!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Diving deeper
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy metric quantifies the ratio of correct predictions from all of
    the predictions performed by our model, as represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Accuracy = Correct Predictions / Total Number* *of Predictions*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other metrics for assessing a model’s prediction performance,
    and scikit-learn has built-in functions to calculate many of those metrics. As
    additional learning, we recommend you consult the scikit-learn metric documentation
    at the following link: [https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.xhtml#module-sklearn.metrics).'
  prefs: []
  type: TYPE_NORMAL
- en: Great work! You have officially trained multiple of your own models on Google
    Cloud Vertex AI! Let’s recap on what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took many of the ML concepts from *Chapters 1* and *2* and
    put them into practice. We used clustering to find patterns in our data in an
    unsupervised manner, and you specifically learned a lot more about how K-means
    is used for clustering and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: We then dived into SL, and you explored the linear regression class within scikit-learn
    and learned how to use metrics to measure the performance of a regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you learned how to use XGBoost to build a classification model and classify
    items in the iris dataset based on their features.
  prefs: []
  type: TYPE_NORMAL
- en: Not only did you put all of those important concepts into practice, but you
    also learned how to create and use Vertex AI Workbench-managed notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, you learned other important concepts in the ML industry, such
    as how decision trees work, how Gradient Boosting works, and how XGBoost enhances
    that functionality to implement one of the most effective ML algorithms in the
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: That was a lot of stuff to learn in one chapter, and you should be proud that
    you are significantly elevating your skills and knowledge in areas that are in
    high demand in today’s business world.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn another set of extremely important skills
    as we will focus on data analysis and data transformation for ML workloads.
  prefs: []
  type: TYPE_NORMAL
