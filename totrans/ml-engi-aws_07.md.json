["```py\nfrom sagemaker import PCA\n# [1] TRAINING\nestimator = PCA(\n    role=role,\n    instance_count=1,\n    instance_type='ml.c4.xlarge',\n    num_components=2,\n    sagemaker_session=session\n)\nestimator.fit(...)\n# [2] DEPLOYMENT\npredictor = estimator.deploy(\n    initial_instance_count=1,\n    instance_type='ml.t2.medium'\n)\n```", "```py\nfrom sagemaker.pytorch.model import PyTorchModel\n# [1] HERE, WE DON'T SHOW THE TRAINING STEP\nmodel_data = estimator.model_data\n# [2] DEPLOYMENT\nmodel = PyTorchModel(\n    model_data=model_data, \n    role=role, \n    source_dir=\"scripts\",\n    entry_point='inference.py', \n    framework_version='1.6.0',\n    py_version=\"py3\"\n)\npredictor = model.deploy(\n    instance_type='ml.m5.xlarge', \n    initial_instance_count=1\n)\n```", "```py\nfrom sagemaker.model import Model\n# [1] HERE, WE DON'T SHOW THE TRAINING STEP\nmodel_data = estimator.model_data\n# [2] DEPLOYMENT\nimage_uri = \"<INSERT ECR URI OF CUSTOM CONTAINER IMAGE>\"\nmodel = Model(\n    image_uri=image_uri, \n    model_data=model_data,\n    role=role,\n    sagemaker_session=session\n)\npredictor = model.deploy(\n    initial_instance_count=1, \n    instance_type='ml.m5.xlarge'\n)\n```", "```py\n    !pip3 install transformers==4.4.2\n    ```", "```py\n    !pip3 install ipywidgets --quiet\n    ```", "```py\n    import IPython\n    ```", "```py\n    kernel = IPython.Application.instance().kernel\n    ```", "```py\n    kernel.do_shutdown(True)\n    ```", "```py\n    from transformers import AutoModelForSequenceClassification as AMSC\n    ```", "```py\n    pretrained = \"distilbert-base-uncased-finetuned-sst-2-english\"\n    ```", "```py\n    model = AMSC.from_pretrained(pretrained)\n    ```", "```py\n    model.save_pretrained(save_directory=\".\")\n    ```", "```py\n    import tarfile\n    ```", "```py\n    tar = tarfile.open(\"model.tar.gz\", \"w:gz\")\n    ```", "```py\n    tar.add(\"pytorch_model.bin\")\n    ```", "```py\n    tar.add(\"config.json\")\n    ```", "```py\n    tar.close()\n    ```", "```py\n    %%bash\n    ```", "```py\n    rm pytorch_model.bin\n    ```", "```py\n    rm config.json\n    ```", "```py\n    s3_bucket = \"<INSERT S3 BUCKET NAME HERE>\"\n    ```", "```py\n    prefix = \"chapter07\"\n    ```", "```py\n    !aws s3 mb s3://{s3_bucket}\n    ```", "```py\n    model_data = \"s3://{}/{}/model/model.tar.gz\".format(\n    ```", "```py\n        s3_bucket, prefix\n    ```", "```py\n    )\n    ```", "```py\n    !aws s3 cp model.tar.gz {model_data}\n    ```", "```py\n    %store model_data\n    ```", "```py\n    %store s3_bucket\n    ```", "```py\n    %store prefix\n    ```", "```py\n    import json\n    ```", "```py\n    from transformers import AutoModelForSequenceClassification as AMSC\n    ```", "```py\n    from transformers import Trainer\n    ```", "```py\n    from transformers import TrainingArguments\n    ```", "```py\n    from torch.nn import functional as F\n    ```", "```py\n    from transformers import AutoTokenizer\n    ```", "```py\n    from time import sleep\n    ```", "```py\n    TOKENIZER = \"distilbert-base-uncased-finetuned-sst-2-english\"\n    ```", "```py\n    def model_fn(model_dir):\n    ```", "```py\n        model = AMSC.from_pretrained(model_dir)\n    ```", "```py\n        return model\n    ```", "```py\n    def humanize_prediction(output):\n    ```", "```py\n        class_a, class_b = F.softmax(\n    ```", "```py\n            output[0][0], \n    ```", "```py\n            dim = 0\n    ```", "```py\n        ).tolist()\n    ```", "```py\n        prediction = \"-\"\n    ```", "```py\n        if class_a > class_b:\n    ```", "```py\n            prediction = \"NEGATIVE\"\n    ```", "```py\n        else:\n    ```", "```py\n            prediction = \"POSITIVE\"\n    ```", "```py\n        return prediction\n    ```", "```py\n    def predict_fn(input_data, model):\n    ```", "```py\n        # sleep(30)\n    ```", "```py\n        sentence = input_data['text']\n    ```", "```py\n        tokenizer = AutoTokenizer.from_pretrained(\n    ```", "```py\n            TOKENIZER\n    ```", "```py\n        )\n    ```", "```py\n        batch = tokenizer(\n    ```", "```py\n            [sentence],\n    ```", "```py\n            padding=True,\n    ```", "```py\n            truncation=True,\n    ```", "```py\n            max_length=512,\n    ```", "```py\n            return_tensors=\"pt\"\n    ```", "```py\n        )\n    ```", "```py\n        output = model(**batch)\n    ```", "```py\n        prediction = humanize_prediction(output)\n    ```", "```py\n        return prediction\n    ```", "```py\n    def input_fn(serialized_input_data, \n    ```", "```py\n                 content_type='application/json'):\n    ```", "```py\n        if content_type == 'application/json':\n    ```", "```py\n            input_data = json.loads(serialized_input_data)\n    ```", "```py\n            return input_data\n    ```", "```py\n        else:\n    ```", "```py\n            raise Exception('Unsupported Content Type')\n    ```", "```py\n    def output_fn(prediction_output, \n    ```", "```py\n                  accept='application/json'):\n    ```", "```py\n        if accept == 'application/json':\n    ```", "```py\n            return json.dumps(prediction_output), accept\n    ```", "```py\n        raise Exception('Unsupported Content Type')\n    ```", "```py\n    transformers==4.4.2\n    ```", "```py\n    from setuptools import setup, find_packages\n    ```", "```py\n    setup(name='distillbert',\n    ```", "```py\n          version='1.0',\n    ```", "```py\n          description='distillbert',\n    ```", "```py\n          packages=find_packages(\n    ```", "```py\n              exclude=('tests', 'docs')\n    ```", "```py\n         ))\n    ```", "```py\n    %store -r model_data\n    ```", "```py\n    %store -r s3_bucket\n    ```", "```py\n    %store -r prefix\n    ```", "```py\n    from sagemaker import get_execution_role \n    ```", "```py\n    role = get_execution_role()\n    ```", "```py\n    from sagemaker.pytorch.model import PyTorchModel\n    ```", "```py\n    model = PyTorchModel(\n    ```", "```py\n        model_data=model_data, \n    ```", "```py\n        role=role, \n    ```", "```py\n        source_dir=\"scripts\",\n    ```", "```py\n        entry_point='inference.py', \n    ```", "```py\n        framework_version='1.6.0',\n    ```", "```py\n        py_version=\"py3\"\n    ```", "```py\n    )\n    ```", "```py\n    %%time\n    ```", "```py\n    from sagemaker.serializers import JSONSerializer\n    ```", "```py\n    from sagemaker.deserializers import JSONDeserializer\n    ```", "```py\n    predictor = model.deploy(\n    ```", "```py\n        instance_type='ml.m5.xlarge', \n    ```", "```py\n        initial_instance_count=1,\n    ```", "```py\n        serializer=JSONSerializer(),\n    ```", "```py\n        deserializer=JSONDeserializer()\n    ```", "```py\n    )\n    ```", "```py\n    payload = {\n    ```", "```py\n        \"text\": \"I love reading the book MLE on AWS!\"\n    ```", "```py\n    }\n    ```", "```py\n    predictor.predict(payload)\n    ```", "```py\n    payload = {\n    ```", "```py\n        \"text\": \"This is the worst spaghetti I've had\"\n    ```", "```py\n    }\n    ```", "```py\n    predictor.predict(payload)\n    ```", "```py\n    predictor.delete_endpoint()\n    ```", "```py\n    %store -r model_data\n    ```", "```py\n    %store -r s3_bucket\n    ```", "```py\n    %store -r prefix\n    ```", "```py\n    from sagemaker import get_execution_role \n    ```", "```py\n    role = get_execution_role()\n    ```", "```py\n    from sagemaker.serverless import ServerlessInferenceConfig\n    ```", "```py\n    serverless_config = ServerlessInferenceConfig(\n    ```", "```py\n      memory_size_in_mb=4096,\n    ```", "```py\n      max_concurrency=5,\n    ```", "```py\n    )\n    ```", "```py\n    from sagemaker.pytorch.model import PyTorchModel\n    ```", "```py\n    from sagemaker.serializers import JSONSerializer\n    ```", "```py\n    from sagemaker.deserializers import JSONDeserializer\n    ```", "```py\n    model = PyTorchModel(\n    ```", "```py\n        model_data=model_data, \n    ```", "```py\n        role=role, \n    ```", "```py\n        source_dir=\"scripts\",\n    ```", "```py\n        entry_point='inference.py', \n    ```", "```py\n        framework_version='1.6.0',\n    ```", "```py\n        py_version=\"py3\"\n    ```", "```py\n    )\n    ```", "```py\n    predictor = model.deploy(\n    ```", "```py\n        instance_type='ml.m5.xlarge', \n    ```", "```py\n        initial_instance_count=1,\n    ```", "```py\n        serializer=JSONSerializer(),\n    ```", "```py\n        deserializer=JSONDeserializer(),\n    ```", "```py\n        serverless_inference_config=serverless_config\n    ```", "```py\n    )\n    ```", "```py\n    payload = {\n    ```", "```py\n        \"text\": \"I love reading the book MLE on AWS!\"\n    ```", "```py\n    }\n    ```", "```py\n    predictor.predict(payload)\n    ```", "```py\n    payload = {\n    ```", "```py\n        \"text\": \"This is the worst spaghetti I've had\"\n    ```", "```py\n    }\n    ```", "```py\n    predictor.predict(payload)\n    ```", "```py\n    predictor.delete_endpoint()\n    ```", "```py\n    {\"text\": \"I love reading the book MLE on AWS!\"}\n    ```", "```py\n    %store -r model_data\n    ```", "```py\n    %store -r s3_bucket\n    ```", "```py\n    %store -r prefix\n    ```", "```py\n    input_data = \"s3://{}/{}/data/input.json\".format(\n    ```", "```py\n        s3_bucket,\n    ```", "```py\n        prefix\n    ```", "```py\n    )\n    ```", "```py\n    !aws s3 cp data/input.json {input_data}\n    ```", "```py\n    from sagemaker import get_execution_role \n    ```", "```py\n    role = get_execution_role()\n    ```", "```py\n    from sagemaker.async_inference import AsyncInferenceConfig\n    ```", "```py\n    output_path = f\"s3://{s3_bucket}/{prefix}/output\"\n    ```", "```py\n    async_config = AsyncInferenceConfig(\n    ```", "```py\n        output_path=output_path\n    ```", "```py\n    )\n    ```", "```py\n    from sagemaker.pytorch.model import PyTorchModel\n    ```", "```py\n    model = PyTorchModel(\n    ```", "```py\n        model_data=model_data, \n    ```", "```py\n        role=role, \n    ```", "```py\n        source_dir=\"scripts\",\n    ```", "```py\n        entry_point='inference.py', \n    ```", "```py\n        framework_version='1.6.0',\n    ```", "```py\n        py_version=\"py3\"\n    ```", "```py\n    )\n    ```", "```py\n    %%time\n    ```", "```py\n    from sagemaker.serializers import JSONSerializer\n    ```", "```py\n    from sagemaker.deserializers import JSONDeserializer\n    ```", "```py\n    predictor = model.deploy(\n    ```", "```py\n        instance_type='ml.m5.xlarge', \n    ```", "```py\n        initial_instance_count=1,\n    ```", "```py\n        serializer=JSONSerializer(),\n    ```", "```py\n        deserializer=JSONDeserializer(),\n    ```", "```py\n        async_inference_config=async_config\n    ```", "```py\n    )\n    ```", "```py\n    response = predictor.predict_async(\n    ```", "```py\n        input_path=input_data\n    ```", "```py\n    )\n    ```", "```py\n    from time import sleep\n    ```", "```py\n    sleep(40)\n    ```", "```py\n    response.get_result()\n    ```", "```py\n    output_path = response.output_path\n    ```", "```py\n    !aws s3 cp {output_path} sample.out\n    ```", "```py\n    !cat sample.out\n    ```", "```py\n    !rm sample.out\n    ```", "```py\n    predictor.delete_endpoint()\n    ```", "```py\nfrom sagemaker.predictor import Predictor\nfrom sagemaker.serializers import JSONSerializer\nfrom sagemaker.deserializers import JSONDeserializer\nendpoint_name = \"<INSERT NAME OF EXISTING ENDPOINT>\"\npredictor = Predictor(endpoint_name=endpoint_name)\npredictor.serializer = JSONSerializer() \npredictor.deserializer = JSONDeserializer()\npayload = {\n^  \"text\": \"I love reading the book MLE on AWS!\"\n}\npredictor.predict(payload)\n```", "```py\nimport boto3 \nimport json\nendpoint_name = \"<INSERT NAME OF EXISTING ENDPOINT>\"\nruntime = boto3.Session().client('sagemaker-runtime')\npayload = {\n    \"text\": \"I love reading the book MLE on AWS!\"\n}\nresponse = sagemaker_client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    ContentType='application/json', \n    Body=json.dumps(payload)\n)\njson.loads(response['Body'].read().decode('utf-8'))\n```", "```py\nUSING EXTERNAL FUNCTION function_name(value INT)\nRETURNS DOUBLE\nSAGEMAKER '<INSERT EXISTING ENDPOINT NAME>'\nSELECT label, value, function_name(value) AS alias\nFROM athena_db.athena_table\n```"]