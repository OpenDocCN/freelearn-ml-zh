<html><head></head><body>
		<div id="_idContainer165">
			<h1 id="_idParaDest-176"><em class="italic"><a id="_idTextAnchor178"/>Chapter 10</em>: H2O Model Deployment Patterns</h1>
			<p>In the previous chapter, we learned how easy it is to generate a ready-to-deploy scoring artifact from our model-building step and how this artifact, called a MOJO, is designed to flexibly deploy to a wide diversity of production systems. </p>
			<p>In this chapter, we explore this flexibility of MOJO deployment by surveying a wide range of MOJO deployment patterns and digging down into the details of each deployment pattern. We will see how MOJOs are implemented for scoring on either H2O software, third-party software including <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) tools, and your own software. These implementations will include scoring on real-time, batch, and streaming data. </p>
			<p>Recall from <a href="B16721_01_Final_SK_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Opportunities and Challenges</em>, how <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models achieve business value when deployed to production systems. The knowledge you gain in this chapter will allow you to find the appropriate MOJO deployment pattern for a particular business case.  For example, it will allow analysts to perform time-series forecasting from a <strong class="bold">Microsoft Excel</strong> spreadsheet, technicians to respond to predictions of product defects made on data streaming from a manufacturing process, or business stakeholders to respond to fraud predictions scored directly on Snowflake tables.</p>
			<p>The goal of this chapter is for you to implement your own H2O model scoring, whether from these examples, your web search, or your imagination, inspired by these examples.</p>
			<p>So, in this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Surveying a sample of MOJO deployment patterns</li>
				<li>Exploring examples of MOJO scoring on H2O software</li>
				<li>Exploring examples of MOJO scoring on third-party software</li>
				<li>Exploring examples of MOJO scoring on your target-system software</li>
				<li>Exploring examples of accelerators based on H2O Driverless AI integrations</li>
			</ul>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor179"/>Technical requirements</h1>
			<p>There are no technical requirements for this chapter, though we will be highlighting the technical steps to implement and execute MOJO deployment patterns.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor180"/>Surveying a sample of MOJO deployment patterns</h1>
			<p>The purpose of this chapter is to<a id="_idIndexMarker711"/> overview the diverse ways in which MOJOs can be deployed for making predictions. Enough detail is given to provide an understanding of the context of MOJO deployment and scoring. Links are provided to find low-level details.</p>
			<p>First, let's summarize our sample of MOJO scoring patterns in table form to get a sense of the many different ways you can deploy MOJOs. After this sample overview, we will elaborate on each table entry more fully. </p>
			<p>Note that the table columns for our deployment-pattern summaries are represented as follows:</p>
			<ul>
				<li><strong class="bold">Data Velocity</strong>: This refers to the size <a id="_idIndexMarker712"/>and speed of data that is scored and is categorized as either <strong class="bold">real-time</strong> (single record scored, typically in less than 100 milliseconds), <strong class="bold">batch</strong> (large numbers of records scored at one time), and <strong class="bold">streaming</strong> (a continuous flow of records that are scored).</li>
				<li><strong class="bold">Scoring Communication</strong>: This refers to<a id="_idIndexMarker713"/> how the scoring is<a id="_idIndexMarker714"/> triggered and communicated—for example, via a <strong class="bold">REpresentational State Transfer</strong> (<strong class="bold">REST</strong>) call or <a id="_idIndexMarker715"/>a <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>) statement.</li>
				<li><strong class="bold">MOJO Deployment</strong>: This is a brief <a id="_idIndexMarker716"/>description of how the MOJO is deployed on the scoring system.</li>
			</ul>
			<p>Let's take a look at some of the deployment patterns. We will break these patterns into four categories.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor181"/>H2O software</h2>
			<p>This is a sample of ways you can <a id="_idIndexMarker717"/>deploy and score MOJOs on software provided and supported by H2O.ai. The following table provides a summary of this:   </p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B16721_10_Table_01.jpg" alt=""/>
				</div>
			</div>
			<p>We will see that deploying to H2O software is super easy since all you have to do is upload the MOJO (manually or programmatically).</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor182"/>Third-party software integrations</h2>
			<p>Here are a few examples of MOJO <a id="_idIndexMarker718"/>scoring with third-party software:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B16721_10_Table_02.jpg" alt=""/>
				</div>
			</div>
			<p>Note that some third-party integrations are done by consuming scoring from MOJOs deployed to a REST server. This has the advantage of centralizing your deployment in one place (the REST server) and consuming it from many places (for example, dozens of Tableau or MS Excel instances<a id="_idIndexMarker719"/> deployed on employee personal computers).  </p>
			<p>Other third-party integrations are accomplished by deploying MOJOs directly on the third-party software system. The Snowflake integration, for example, is implemented on the Snowflake architecture and allows batch scoring that performs at a Snowflake scale (it can score hundreds of thousands of rows per second).</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor183"/>Your software integrations</h2>
			<p>We will explore the following patterns<a id="_idIndexMarker720"/> for integrating MOJOs directly into your own software:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B16721_10_Table_03.jpg" alt=""/>
				</div>
			</div>
			<p>MOJO integration into your software requires a MOJO wrapper class. We learned how to do this in <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>, <em class="italic">Production Scoring and the H2O MOJO</em>. Of course, you can take the alternative approach and integrate your software with MOJO scoring consumed from a REST endpoint.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor184"/>Accelerators based on H2O Driverless AI integrations</h2>
			<p>This book focuses on H2O Core (H2O-3 and <a id="_idIndexMarker721"/>Sparkling Water) model-building technology for building models against large data volumes. H2O provides an alternative model-building technology called Driverless AI. Driverless<a id="_idIndexMarker722"/> AI is a specialized, <strong class="bold">automated ML</strong> (<strong class="bold">AutoML</strong>) engine that allows users to find highly accurate and trusted models in extremely short amounts <a id="_idIndexMarker723"/>of time. Driverless AI cannot train on the massive datasets that H2O Core can, though. However, Driverless AI also produces a MOJO, and its flavor of MOJO deploys similarly to the H2O Core MOJO. These similarities were covered in <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>, <em class="italic">Production Scoring and the H2O MOJO</em>.</p>
			<p>There are many examples available online for deploying Driverless AI MOJOs. These examples can be followed as a guide to deploying H2O Core MOJOs in the same pattern.  Consider the following Driverless AI examples therefore as accelerators that can get you most of the way to deploying your H2O Core MOJOs, but some implementation details will differ:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B16721_10_Table_04.jpg" alt=""/>
				</div>
			</div>
			<p>The patterns shown in these four<a id="_idIndexMarker724"/> tables should provide a good sense of the many ways you can deploy MOJOs. They do not, however, represent the total set of possibilities.</p>
			<p class="callout-heading">A Note on Possibilities</p>
			<p class="callout">The patterns shown here are merely a sample of H2O MOJO scoring patterns that exist or are possible. Other MOJO scoring patterns can be found through a web search, and you can use your imagination to integrate MOJO scoring in diverse ways into your own software. Additionally, H2O.ai is rapidly expanding its third-party partner integrations for scoring, as well as expanding its own MOJO deployment, monitoring, and management capabilities. This is a rapidly moving space.</p>
			<p>Now that we have surveyed a landscape of MOJO deployment patterns, let's jump in and look at each example in detail.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor185"/>Exploring examples of MOJO scoring with H2O software </h1>
			<p>The patterns in this section represent MOJOs<a id="_idIndexMarker725"/> deployed to H2O software. There are many advantages to deploying to H2O software. First, the software is supported by H2O and their team of ML experts. Second, this deployment workflow is greatly streamlined for H2O software since all you have to do is<a id="_idIndexMarker726"/> supply the MOJO in a simple upload (via a <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>), an API, or a transfer method such as remote copy). Third, H2O scoring software has additional capabilities—such as monitoring for prediction and data drift—that are important for models deployed to production systems.</p>
			<p>Let's start by looking at H2O's flagship model-scoring platform.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor186"/>H2O MLOps</h2>
			<p>H2O MLOps is a full-featured platform for deploying, monitoring, managing, and governing ML models. H2O MLOps<a id="_idIndexMarker727"/> is dedicated to deploying <a id="_idIndexMarker728"/>models at scale (many models and model versions, enterprise-grade throughput and performance, <strong class="bold">high availability</strong>, and so on), and addressing monitoring, management, and governance concerns around models in production.  </p>
			<p>H2O MLOps and its relation to H2O's larger <strong class="bold">end-to-end</strong> ML platform will be reviewed in <a href="B16721_13_Final_SK_ePub.xhtml#_idTextAnchor241"><em class="italic">Chapter 13</em></a>, <em class="italic">Introducing H2O AI Cloud</em>. See also <a href="https://docs.h2o.ai/mlops-release/latest-stable/docs/userguide/index.html">https://docs.h2o.ai/mlops-release/latest-stable/docs/userguide/index.html</a> for the MLOps user guide to better understand H2O MLOps.</p>
			<h3>Pattern overview</h3>
			<p>The H2O MLOps scoring pattern is <a id="_idIndexMarker729"/>shown in the following diagram:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B16721_10_001.jpg" alt="Figure 10.1 – Model-scoring pattern for H2O MLOps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Model-scoring pattern for H2O MLOps</p>
			<p>We'll elaborate on this next.</p>
			<h3>Scoring context</h3>
			<p>This is H2O.ai's flagship<a id="_idIndexMarker730"/> model-deployment, model-monitoring, and model-governance platform. It can be used to host and score both H2O and third-party (non-H2O) models.  </p>
			<p>H2O MLOps scores models<a id="_idIndexMarker731"/> in real time and in batches. Predictions <a id="_idIndexMarker732"/>optionally return reason codes. Models are deployed as single-model, champion/challenger, and A/B. See the <em class="italic">Additional notes</em> section for a full description of its capabilities.</p>
			<h3>Implementation</h3>
			<p>H2O MLOps is a modern Kubernetes-based<a id="_idIndexMarker733"/> implementation deployed using Terraform scripts and Helm charts.</p>
			<h3>Scoring example</h3>
			<p>The following code snippet shows a<a id="_idIndexMarker734"/> real-time scoring request sent using the <strong class="source-inline">curl</strong> command:</p>
			<p class="source-code">curl -X POST -H "Content-Type: application/json" -d @- https://model.prod.xyz.com/9c5c3042-1f9a-42b5-ac1a-9dca19414fbb/model/score &lt;&lt; EOF</p>
			<p class="source-code">{"fields":["loan_amnt","term","int_rate","emp_length","home_ownership","annual_inc","purpose","addr_state","dti","delinq_2yrs","revol_util","total_acc","longest_credit_length","verification_status"rows":[["5000","36months","10.65","10",24000.0","RENT","AZ","27.650","0","83.7","9","26","verified"]]}EOF</p>
			<p>And here is the result:</p>
			<p class="source-code">{"fields":["bad_loan.0","bad_loan.1"],"id":"45d0677a-9327-11ec-b656-2e37808d3384","score":[["0.7730158252427003","0.2269841747572997"]]} </p>
			<p>From here, we see the probability<a id="_idIndexMarker735"/> of a loan default (<strong class="source-inline">bad_loan</strong> value of <strong class="source-inline">1</strong>) is <strong class="source-inline">0.2269841747572997</strong>. The <strong class="source-inline">id</strong> field is used to identify the REST endpoint, which is useful when models are<a id="_idIndexMarker736"/> deployed in <a id="_idIndexMarker737"/>champion/challenger or A/B test modes.</p>
			<h3>Additional notes</h3>
			<p>Here is a brief summary of key <a id="_idIndexMarker738"/>H2O MLOps capabilities:</p>
			<ul>
				<li><strong class="bold">Multiple deployment models</strong>: Standalone; champion/challenger; A/B models</li>
				<li><strong class="bold">Multiple model problems</strong>: Tabular; time-series; image; language models</li>
				<li><strong class="bold">Shapley values</strong>: On deployment, specify whether to return Shapley values (reason codes) with the prediction</li>
				<li><strong class="bold">Third-party models</strong>: Scores and monitors non-H2O models—for example, scikit-learn models</li>
				<li><strong class="bold">Model management</strong>: Model registry; versioning; model metadata; promotion and approval workflow</li>
				<li><strong class="bold">APIs</strong>: APIs<a id="_idIndexMarker739"/> and <strong class="bold">continuous integration and continuous delivery</strong> (<strong class="bold">CI/CD</strong>) integration</li>
				<li><strong class="bold">Analytics</strong>: Optionally push scoring data to your system for your own analytics</li>
				<li><strong class="bold">Lineage</strong>: Understand the lineage of data, experiment, and model </li>
				<li><strong class="bold">Model monitoring</strong>: Data drift and prediction monitoring with alert management (bias and other types <a id="_idIndexMarker740"/>of monitoring are on the MLOps roadmap)<p class="callout-heading">H2O MLOps versus Other H2O Model-Scoring Software</p><p class="callout">MLOps is H2O's flagship full-featured <a id="_idIndexMarker741"/>platform to deploy, monitor, and govern models for scoring. H2O supplies other software (overviewed next) that is specialized to address needs or constraints where MLOps may not fit.</p></li>
			</ul>
			<p>Next, let's have a look <a id="_idIndexMarker742"/>at the H2O REST scorer.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor187"/>H2O eScorer </h2>
			<p>H2O has a lightweight but <a id="_idIndexMarker743"/>powerful REST server to score MOJOs, called the H2O eScorer. This<a id="_idIndexMarker744"/> is a good alternative for serving MOJOs as REST endpoints without committing to larger infrastructure<a id="_idIndexMarker745"/> requirements of the H2O MLOps platform and therefore freeing deployment options to on-premises and lightweight deployments.  Recall that third-party software often integrates with MOJOs by way of REST endpoint integration, so this is an effective way to achieve that.</p>
			<h3>Pattern overview</h3>
			<p>The H2O REST scorer pattern<a id="_idIndexMarker746"/> is shown in the following diagram:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B16721_10_002.jpg" alt="Figure 10.2 – MOJO scoring pattern for H2O REST scorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – MOJO scoring pattern for H2O REST scorer</p>
			<p>Here is an elaboration.</p>
			<h3>Scoring context</h3>
			<p>The H2O REST scorer makes real-time and <a id="_idIndexMarker747"/>batch predictions to a REST endpoint. Predictions optionally include reason codes.</p>
			<h3>Implementation</h3>
			<p>The H2O Rest scorer is a single <strong class="bold">Java ARchive</strong> (<strong class="bold">JAR</strong>) file holding an Apache Tomcat server hosting a<a id="_idIndexMarker748"/> Spring REST services framework. A properties file configures the application to host multiple REST scoring endpoints. MOJOs are loaded<a id="_idIndexMarker749"/> either by REST itself or by other means of transferring the MOJO to the server.</p>
			<p>High throughput is <a id="_idIndexMarker750"/>achieved by placing multiple H2O REST scorers behind a load balancer. </p>
			<h3>Scoring example</h3>
			<p>Here are some examples of REST <a id="_idIndexMarker751"/>endpoints for real-time scoring:</p>
			<pre class="source-code">http://192.1.1.1:8080/model?name=riskmodel.mojo &amp;row=5000,36months,10.65,162.87,10,RENT,24000,VERIFIED-income,AZ,27.65,0,1,0,13648,83.7,0"</pre>
			<p>The REST scorer's REST API is quite flexible. For example, it includes multiple ways to structure the payload (for example, an observation<a id="_idIndexMarker752"/> input can be sent as <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>), <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>), or other structures with<a id="_idIndexMarker753"/> the scorer output returned in<a id="_idIndexMarker754"/> the same format, which is convenient when integrating with a BI tool).</p>
			<h3>Additional notes</h3>
			<p>Here is a summary of the H2O REST Scorer's set of<a id="_idIndexMarker755"/> capabilities:</p>
			<ul>
				<li>Each H2O Rest scorer can score multiple models (that is, MOJOs), each with its own REST endpoint.</li>
				<li>Typically, 1,000 scores per second<a id="_idIndexMarker756"/> are achieved for each CPU on an H2O REST scorer server.</li>
				<li>Security, monitoring, and logging settings are configurable in a properties file.</li>
				<li><strong class="bold">Java Monitoring Beans</strong> (<strong class="bold">JMX</strong>) can be<a id="_idIndexMarker757"/> configured so that your own monitoring tool can collect and analyze runtime statistics. Monitoring includes scoring errors, scoring latency, and data drift. </li>
				<li>Security features include <strong class="bold">HTTPS</strong>, administrator authentication, authenticated endpoint <strong class="bold">URIs</strong> and limited access from IP prefix. </li>
				<li>There is extensive<a id="_idIndexMarker758"/> logging.</li>
				<li>There are extensive capabilities via the REST API, including obtaining model metadata, defining prediction output formats, defining logging verbosity, and managing MOJOs on the server.</li>
				<li>The REST API can generate an example request sent from different BI tools to score a model on the H2O REST scorer—for example, sample Python code to call a model for Power BI.</li>
			</ul>
			<p>Next, we will have a look at the H2O batch database scorer.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor188"/>H2O batch database scorer</h2>
			<p>The H2O batch database<a id="_idIndexMarker759"/> scorer is a client application that can perform<a id="_idIndexMarker760"/> batch predictions<a id="_idIndexMarker761"/> against tables<a id="_idIndexMarker762"/> using a <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) connection.</p>
			<h3>Pattern overview</h3>
			<p>The H2O batch database<a id="_idIndexMarker763"/> scorer pattern is shown in the following diagram:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B16721_10_003.jpg" alt="Figure 10.3 – MOJO scoring pattern for H2O batch database scorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – MOJO scoring pattern for H2O batch database scorer</p>
			<p>We'll elaborate on this next.</p>
			<h3>Scoring context</h3>
			<p>The H2O batch database scorer<a id="_idIndexMarker764"/> performs batch predictions against database tables. Predictions optionally include reason codes. Depending on how it is configured, predictions against table rows can be inserted into a new table or updated<a id="_idIndexMarker765"/> into the same table being scored. Alternatively, it can generate a CSV file of the prediction's outcome. This CSV output can be used to manually update tables or for other downstream processing.</p>
			<p>Details of the processing sequence for H2O batch database scoring are shown in <em class="italic">Figure 10.3</em>.</p>
			<h3>Implementation</h3>
			<p>The H2O batch database scorer is a single JAR file that is available from H2O.ai. The JAR file uses a properties file to configure aspects of the database workflow.</p>
			<p>More specifically, the property file contains the following:</p>
			<ul>
				<li>SQL connection string</li>
				<li>SQL <strong class="source-inline">SELECT</strong> statement to batch-score</li>
				<li>SQL <strong class="source-inline">INSERT</strong> or <strong class="source-inline">UPDATE</strong> statement to write prediction results</li>
				<li>Number of threads during batch scoring</li>
				<li>Path to MOJO</li>
				<li>Flag to write results to CSV or not</li>
				<li>Security settings</li>
				<li>Other settings</li>
			</ul>
			<h3>Scoring example</h3>
			<p>The following command<a id="_idIndexMarker766"/> shows how a batch job is run from the <a id="_idIndexMarker767"/>command line:</p>
			<p class="source-code">java -cp /PostgresData/postgresql-42.2.5.jar:H2OBatchDB.jar \ ai.h2o.H2OBatchDB</p>
			<p>This, of course, can be integrated into a scheduler or a script to schedule and automate batch scores.</p>
			<p>Note that this command does<a id="_idIndexMarker768"/> not include anything about the database or table. The program that is kicked off from this command finds the properties file, as described in the previous <em class="italic">Implementation</em> subsection, and uses the information there to drive batch scoring.</p>
			<h3>Additional notes</h3>
			<p>A single properties file holds all the information needed to run a single batch-scoring job (the properties file maps to a SQL statement against a table that will be scored).</p>
			<p>If no properties file is stated in the Java command to score (see the <em class="italic">Scoring example</em> section), then the default properties file is used. Alternatively, a specific properties file can be specified in<a id="_idIndexMarker769"/> the Java command line to run a non-default scoring job.</p>
			<p>Next, let's have a look at the H2O batch file scorer.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor189"/>H2O batch file scorer</h2>
			<p>The H2O batch file scorer is <a id="_idIndexMarker770"/>an application that can <a id="_idIndexMarker771"/>perform batch predictions against records in a file.</p>
			<h3>Pattern overview</h3>
			<p>The H2O batch file scorer <a id="_idIndexMarker772"/>pattern is shown in the following diagram:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B16721_10_004.jpg" alt="Figure 10.4 – MOJO scoring pattern for H2O batch file scorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – MOJO scoring pattern for H2O batch file scorer</p>
			<p>This is how it is <a id="_idIndexMarker773"/>used.</p>
			<h3>Scoring context</h3>
			<p>Scoring is batch against records in a file, and<a id="_idIndexMarker774"/> the output will be a file identical to the input file but with a scored field appended to each record. The output file remains on the H2O batch-scorer system until processed by another system (for example, copied to a downstream system for processing).</p>
			<h3>Implementation</h3>
			<p>The H2O batch file scorer is a single <a id="_idIndexMarker775"/>JAR file that is<a id="_idIndexMarker776"/> available from H2O.ai. Command-line arguments are used to specify the location of the model and input file, as well as any runtime parameters such as skipping the column head if one exists<a id="_idIndexMarker777"/> in the file.</p>
			<h3>Scoring example</h3>
			<p>The following command shows <a id="_idIndexMarker778"/>how a batch-file job is run from the command line:</p>
			<p class="source-code">java -Xms10g -Xmx10g -Dskipheader=true -Dautocolumns=true -classpath mojo2-runtime.jar:DAIMojoRunner_TQ.jar daimojorunner_tq.DAIMojoRunner_TQ pipeline.mojo LoanStats4.csv</p>
			<p>A few notes are worth mentioning.</p>
			<h3>Additional notes</h3>
			<p>This scorer is ideal for processing extremely large files (&gt; GB) as a single task, making it easy to use in a traditional batch-processing workflow. If the input file contains a header, then the scorer will select the correct columns to pass to the model, and if a header is not present, then the columns can be passed as command-line parameters.</p>
			<p>Let's now take <a id="_idIndexMarker779"/>a look at the H2O Kafka scorer.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor190"/>H2O Kafka scorer</h2>
			<p>The H2O Kafka scorer is an <a id="_idIndexMarker780"/>application that integrates with the <a id="_idIndexMarker781"/>score from Kafka streams.</p>
			<h3>Pattern overview</h3>
			<p>The H2O Kafka scorer pattern is<a id="_idIndexMarker782"/> shown in the following diagram: </p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B16721_10_005.jpg" alt="Figure 10.5 – MOJO scoring pattern for H2O Kafka scorer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – MOJO scoring pattern for H2O Kafka scorer</p>
			<h3>Scoring context</h3>
			<p>Scoring against streaming data is shown in <em class="italic">Figure 10.5</em>. Specifically, the H2O Kafka scorer pulls messages from a<a id="_idIndexMarker783"/> topic queue and publishes the score outcome to another topic.</p>
			<h3>Implementation</h3>
			<p>The <a id="_idIndexMarker784"/>H2O Kafka scorer is a JAR file that is<a id="_idIndexMarker785"/> implemented on the Kafka system. A properties file is used to configure which topic to consume (and thus which messages to score) and which to publish to (where to send the results). When the H2O Kafka scorer JAR file is started, it loads the MOJO and then listens for incoming messages from the topic.</p>
			<h3>Scoring example</h3>
			<p>Scoring is done when a message<a id="_idIndexMarker786"/> arrives at the upstream topic. A prediction is appended to the last field of the original message. This new message is<a id="_idIndexMarker787"/> then sent to a topic for downstream processing.</p>
			<h3>Additional notes</h3>
			<p>Scaling throughput is done using native Kafka scaling techniques inherent in its distributed parallelized architecture.</p>
			<p>Finally, let's look at H2O batch scoring on Spark.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor191"/>H2O batch scoring on Spark</h2>
			<p>H2O MOJOs can be deployed<a id="_idIndexMarker788"/> as native Spark<a id="_idIndexMarker789"/> jobs. </p>
			<h3>Pattern overview</h3>
			<p>The H2O batch scoring on Spark <a id="_idIndexMarker790"/>pattern is shown in the following diagram: </p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B16721_10_006.jpg" alt="Figure 10.6 – MOJO scoring pattern for H2O batch scoring on Spark&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – MOJO scoring pattern for H2O batch scoring on Spark</p>
			<h3>Scoring context</h3>
			<p>Scoring is batch and on a<a id="_idIndexMarker791"/> Spark cluster. As such, the <a id="_idIndexMarker792"/>batch scoring is distributed and thus scales well to massive batch sizes. </p>
			<h3>Implementation</h3>
			<p>The required dependency to score<a id="_idIndexMarker793"/> MOJOs on the Spark <a id="_idIndexMarker794"/>cluster is distributed with the <strong class="source-inline">spark-submit</strong> command, as shown in the following section.</p>
			<h3>Scoring example</h3>
			<p>First, we'll create a <strong class="bold">PySparkling</strong> job similar to the <a id="_idIndexMarker795"/>following example. We will call this job <strong class="source-inline">myRiskScoring.py</strong>. The code is illustrated in the following snippet:</p>
			<pre class="source-code">from pysparkling.ml import *</pre>
			<pre class="source-code">settings = H2OMOJOSettings(convertUnknownCategoricalLevelsToNa = True, convertInvalidNumbersToNa = True)</pre>
			<pre class="source-code">model_location="hdfs:///models/risk/v2/riskmodel.zip"</pre>
			<pre class="source-code">model = H2OMOJOModel.createFromMojo(model_location, settings")</pre>
			<pre class="source-code">predictions = model.transform(dataset)</pre>
			<pre class="source-code">// do something with predictions, e.g. write to hdfs</pre>
			<p>Then, submit your Spark job with the H2O scoring library, as follows: </p>
			<p class="source-code">./bin/spark-submit \</p>
			<p class="source-code">    --py-files py/h2o_pysparkling_scoring.zip \</p>
			<p class="source-code">    myRiskScoring.py</p>
			<p>Note that the <strong class="source-inline">h2o_pysparkling_scoring.zip</strong> dependency will be distributed to the cluster with the<a id="_idIndexMarker796"/> job. This<a id="_idIndexMarker797"/> library is available <a id="_idIndexMarker798"/>from H2O.ai.</p>
			<h3>Additional notes</h3>
			<p>There are other scoring settings available in addition to those shown in the previous code sample. The following link will provide more details: <a href="https://docs.h2o.ai/sparkling-water/3.1/latest-stable/doc/deployment/load_mojo.html">https://docs.h2o.ai/sparkling-water/3.1/latest-stable/doc/deployment/load_mojo.html</a>.</p>
			<p>We have finished our review of <a id="_idIndexMarker799"/>some scoring patterns on H2O software. Let's now transition to scoring patterns on third-party software.   </p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor192"/>Exploring examples of MOJO scoring with third-party software</h1>
			<p>Let's now look at some examples <a id="_idIndexMarker800"/>of scoring that involve third-party software.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor193"/>Snowflake integration</h2>
			<p>H2O.ai has partnered with <a id="_idIndexMarker801"/>Snowflake to integrate MOJO scoring against Snowflake tables. It is important to note that the MOJO in this integration is deployed on the Snowflake architecture and therefore achieves Snowflake's native scalability benefits. Combined with the low latency of MOJO scoring, the<a id="_idIndexMarker802"/> result is batch scoring on massive Snowflake tables in mere seconds, though real-time scoring on a smaller number of records is achievable as well.</p>
			<h3>Pattern overview</h3>
			<p>The Snowflake<a id="_idIndexMarker803"/> integration pattern is shown in the following diagram:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B16721_10_007.jpg" alt="Figure 10.7 – MOJO scoring pattern for Snowflake Java user-defined function (UDF) integration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – MOJO scoring pattern for Snowflake Java user-defined function (UDF) integration</p>
			<p>Let's elaborate.</p>
			<h3>Scoring context</h3>
			<p>Scoring is batch against<a id="_idIndexMarker804"/> Snowflake tables and leverages the scalability of the Snowflake platform. Thus, scoring can be made against any Snowflake table, including those holding massive datasets.  </p>
			<p>Scoring is done by running a SQL statement from a Snowflake client. This can be either a native Snowflake worksheet, SnowSQL, or a SQL client with a Snowflake connector.Alternatively, scoring can be done programmatically using Snowflake's Snowpark API.</p>
			<h3>Implementation</h3>
			<p>To implement your score, create a <a id="_idIndexMarker805"/>staging table and grant permissions against it. You then copy your MOJO and H2O JAR file dependencies to the staging table. </p>
			<p>You can then use SQL to create a Java UDF that imports these dependencies and assigns a handler to the H2O dependency that does the scoring. This UDF is then referenced when making a SQL scoring statement, as shown next.</p>
			<p>You can find H2O dependencies and instructions here: <a href="https://s3.amazonaws.com/artifacts.h2o.ai/releases/ai/h2o/dai-snowflake-integration/java-udf/download/index.html">https://s3.amazonaws.com/artifacts.h2o.ai/releases/ai/h2o/dai-snowflake-integration/java-udf/download/index.html</a>.</p>
			<p>An integrated experience of using the UDF with Snowflake is also available online at <a href="https://cloud.h2o.ai/v1/latestapp/wave-snowflake">https://cloud.h2o.ai/v1/latestapp/wave-snowflake</a>.</p>
			<h3>Scoring example</h3>
			<p>This is an example of a SQL statement that <a id="_idIndexMarker806"/>performs batch scoring against a table:</p>
			<pre class="source-code">select ID, H2OScore_Java('Modelname=riskmodel.zip', ARRAY_CONSTRUCT(loan_amnt, term, int_rate, installment, emp_length, annual_inc, verification_status, addr_state, dti, inq_last_6mths, revol_bal, revol_util, total_acc)) as H2OPrediction from RiskTable;</pre>
			<p>Notice that the H2O Scoring UDF (loaded as shown in the <em class="italic">Implementation</em> section) is run and that the model name (the MOJO name) is referenced.</p>
			<h3>Additional notes</h3>
			<p>For a more programmatic approach, you can use the Snowpark API instead of a SQL statement to batch-score.</p>
			<h3>Alternative implementation – Scoring via a Snowflake external function</h3>
			<p>For cases where you do not want to deploy <a id="_idIndexMarker807"/>MOJOs directly to the Snowflake environment, you can implement an external function on Snowflake and then pass the scoring to an H2O eScorer implementation. Note that scoring itself is external to Snowflake, and batch throughput rates are determined by the H2O eScorer and not the Snowflake architecture. This is shown in the following diagram:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B16721_10_008.jpg" alt="Figure 10.8 – MOJO scoring pattern for Snowflake external function integration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – MOJO scoring pattern for Snowflake external function integration</p>
			<p>To implement this, we will <a id="_idIndexMarker808"/>use Snowflake on AWS as an example. Follow these steps:</p>
			<ol>
				<li>First, use the Snowflake client to create  <strong class="source-inline">api_integration</strong> to <strong class="source-inline">aws_api_gateway</strong>. A gateway is required to secure the external function when communicating to the H2O eScorer, which will be outside Snowflake. You will need to have the correct role to create this.</li>
				<li>Then, use SQL to create an external function on Snowflake—for example, named H2OPredict. The external function will reference the <strong class="source-inline">api_integration</strong>.</li>
				<li>You are now ready to batch score a Snowflake table via an external function pass-through to an H2O eScorer. Here is a sample SQL statement:<p class="source-code">select ID, H2OPredict('Modelname=riskmodel.zip', loan_amnt, term, int_rate, installment, emp_length, annual_inc, verification_status, addr_state, dti, inq_last_6mths, revol_bal, revol_util, total_acc) as H2OPrediction from RiskTable;</p></li>
			</ol>
			<p>Let's have a look at Teradata integration.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor194"/>Teradata integration</h2>
			<p>H2O.ai has partnered with<a id="_idIndexMarker809"/> Teradata to implement batch or<a id="_idIndexMarker810"/> real-time scoring directly against Teradata tables. This is done as shown in the following diagram: </p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B16721_10_009.jpg" alt="Figure 10.9 – MOJO scoring pattern for Teradata integration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – MOJO scoring pattern for Teradata integration</p>
			<h3>Scoring context</h3>
			<p>Scoring is batch against Teradata<a id="_idIndexMarker811"/> tables and leverages the scalability of the Teradata platform. Thus, scoring can be made against any Teradata table, including those holding massive datasets. This is similar in concept to the Snowflake UDF integration, but only in concept: the underlying architectures and implementations are fundamentally different.</p>
			<p>Scoring against Teradata tables is done by running a SQL statement from a Teradata client. This can be either a native Teradata Studio client or a SQL client with a Teradata connector.  </p>
			<h3>Implementation</h3>
			<p>To implement, you first must<a id="_idIndexMarker812"/> install Teradata Vantage <strong class="bold">Bring Your Own Model</strong> (<strong class="bold">BYOM</strong>). Then, you<a id="_idIndexMarker813"/> use SQL to create a Vantage table to store H2O MOJOs. You then use SQL to load MOJOs into the Vantage table. Details can be found at <a href="https://docs.teradata.com/r/CYNuZkahMT3u2Q~mX35YxA/WC6Ku8fmrVnx4cmPEqYoXA">https://docs.teradata.com/r/CYNuZkahMT3u2Q~mX35YxA/WC6Ku8fmrVnx4cmPEqYoXA</a>.</p>
			<h3>Scoring example</h3>
			<p>Here is an example SQL<a id="_idIndexMarker814"/> statement to batch score a Teradata table:</p>
			<pre class="source-code">select * from H2OPredict(</pre>
			<pre class="source-code">on risk_table</pre>
			<pre class="source-code">on (select * from mojo_models where model_id=riskmodel) dimension</pre>
			<pre class="source-code">using Accumulate('id')</pre>
			<pre class="source-code">) as td_alias;</pre>
			<p>In this case, the code assumes all <strong class="source-inline">risk_table</strong> fields are used as input into the MOJO.</p>
			<h3>Additional notes</h3>
			<p>Your SQL statement to batch score may include options to return reason codes, stage probabilities, and leaf-node assignments.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor195"/>BI tool integration</h2>
			<p>A powerful use of MOJO scoring is to integrate into BI tools. The most common way is to implement MOJO scoring <a id="_idIndexMarker815"/>either on a REST server or against a <a id="_idIndexMarker816"/>database, as shown in the following diagram. Note that in this pattern, MOJOs are not deployed on the BI tool itself, but rather, the tool integrates with an external scoring system. The low-latency nature of MOJO scoring allows users to interact in real time with MOJO predictions through this pattern:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B16721_10_010.jpg" alt="Figure 10.10 – MOJO scoring patterns for BI tool integration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – MOJO scoring patterns for BI tool integration</p>
			<h3>Scoring context</h3>
			<p>BI tools integrate real-time predictions from <a id="_idIndexMarker817"/>external scorers.</p>
			<h3>Implementation</h3>
			<p>An external REST or database MOJO scoring system is implemented. Integration with the external scorer is<a id="_idIndexMarker818"/> implemented in the BI tool. These integrations are specific to each BI tool, and often, a single BI tool has multiple ways to make this integration.</p>
			<h3>Scoring example – Excel</h3>
			<p>The following code block shows a<a id="_idIndexMarker819"/> formula created in a cell of <a id="_idIndexMarker820"/>an Excel spreadsheet:</p>
			<pre class="source-code">=WEBSERVICE(CONCAT("http://192.1.1.1:8080/modeltext?name=riskmodel.mojo&amp;row=",TEXTJOIN(","FALSE, $A4:$M4))))</pre>
			<p>This web service is called when the formula is applied to the target cell, or whenever a value changes in any of the cells referenced in the formula. A user can then drag the formula down a column and have predictions fill the column.</p>
			<p>Note in the preceding formula that the REST call composes the observation to be scored as CSV and not as JSON. The structuring of this payload is specific to the REST API and its endpoint.</p>
			<p>We can integrate MOJO<a id="_idIndexMarker821"/> scoring into other third-party software using REST endpoints in a similar fashion, though the semantics of the <a id="_idIndexMarker822"/>endpoint construction differ. Let's see how to do it in Tableau.  </p>
			<h3>Scoring example – Tableau</h3>
			<p>Tableau is a common <a id="_idIndexMarker823"/>dashboarding tool used within<a id="_idIndexMarker824"/> enterprises to present information to a variety of different users within the organization.</p>
			<p>Using the Tableau script syntax, a model can be invoked from the dashboard. This is very powerful as now, a business user can get current prediction results directly in the dashboard on demand. You can see an example script here:</p>
			<pre class="source-code">SCRIPT_STR(</pre>
			<pre class="source-code">'name'='riskmodel.mojo',</pre>
			<pre class="source-code">ATTR([./riskmodel.mojo]),</pre>
			<pre class="source-code">ATTR([0a4bbd12-dcad-11ea-ab05-024200eg007]),</pre>
			<pre class="source-code">ATTR([loan_amnt]),</pre>
			<pre class="source-code">ATTR([term]),</pre>
			<pre class="source-code">ATTR([int_rate]),</pre>
			<pre class="source-code">ATTR([installment]),</pre>
			<pre class="source-code">ATTR([emp_length]),</pre>
			<pre class="source-code">ATTR([annual_inc]),</pre>
			<pre class="source-code">ATTR([verification_status]),</pre>
			<pre class="source-code">ATTR([addr_state]),</pre>
			<pre class="source-code">ATTR([dti]),</pre>
			<pre class="source-code">ATTR([inq_last_6mths]),</pre>
			<pre class="source-code">ATTR([revol_bal]),</pre>
			<pre class="source-code">ATTR([revol_util]),</pre>
			<pre class="source-code">ATTR([total_acc]))</pre>
			<p>The script reads the values as attributes (<strong class="source-inline">ATTR</strong> keyword) and passes them to a script in the Tableau environment when a REST call is made to the model. Using the REST call allows a centralized model to be deployed and managed, but different applications and consumers invoke <a id="_idIndexMarker825"/>the model based on their specific needs.</p>
			<p>Now, let's see how to<a id="_idIndexMarker826"/> build a REST endpoint in Power BI.   </p>
			<h3>Scoring example – Power BI</h3>
			<p>Here is a scoring example for Power BI. In this case, we are <a id="_idIndexMarker827"/>using a <strong class="source-inline">Web.Contents</strong> Power Query M function. This function is <a id="_idIndexMarker828"/>pasted to the desired Power BI element in your Power BI dashboard:</p>
			<pre class="source-code">Web.Contents(</pre>
			<pre class="source-code">    "http://192.1.1.1:8080",</pre>
			<pre class="source-code">    [</pre>
			<pre class="source-code">        RelativePath="modeltext",</pre>
			<pre class="source-code">        Query=</pre>
			<pre class="source-code">        [</pre>
			<pre class="source-code">            name="riskmodel.mojo",</pre>
			<pre class="source-code">            loan_amnt=Loan_Ammt,</pre>
			<pre class="source-code">            term=Term,</pre>
			<pre class="source-code">            int_rate=Int_Rate,</pre>
			<pre class="source-code">            installment=Installments,</pre>
			<pre class="source-code">            emp_length=Emp_Length,</pre>
			<pre class="source-code">            annual_inc=Annual_Inc,</pre>
			<pre class="source-code">            verification_status=Verification_Status,</pre>
			<pre class="source-code">            addr_state=Addr_State,</pre>
			<pre class="source-code">            dti=DTI,</pre>
			<pre class="source-code">            inq_last_6mths= Inq_Last_6mths,</pre>
			<pre class="source-code">            revol_bal=Revol_Bal,</pre>
			<pre class="source-code">            revol_util=Revol_Util,</pre>
			<pre class="source-code">            total_acc=Total_Acc</pre>
			<pre class="source-code">        ]</pre>
			<pre class="source-code">    ]</pre>
			<pre class="source-code">)</pre>
			<p>Let's generalize a bit<a id="_idIndexMarker829"/> from these specific <a id="_idIndexMarker830"/>examples.</p>
			<h3>Additional notes</h3>
			<p>Each BI tool integrates with a REST endpoint or database in its own way and often provides multiple ways to do so. See your BI tool documentation for details. </p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor196"/>UiPath integration</h2>
			<p><strong class="bold">UiPath</strong> is an RPA platform that automates<a id="_idIndexMarker831"/> workflows based on human actions. Making predictions and responding <a id="_idIndexMarker832"/>to these predictions is a powerful part of this automation, and thus scoring models during these workflow steps is a perfect fit. You can see an example of this in the following diagram:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B16721_10_011.jpg" alt="Figure 10.11 – MOJO scoring pattern for UiPath integration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – MOJO scoring pattern for UiPath integration</p>
			<h3>Scoring context</h3>
			<p>UiPath integrates with external MOJO <a id="_idIndexMarker833"/>scoring similar to what was shown for BI tools in the previous section. In the case of UiPath, a workflow step is configured to make a REST call, receive a prediction, and respond to that prediction.</p>
			<h3>Implementation</h3>
			<p>MOJO scoring is implemented <a id="_idIndexMarker834"/>externally on a REST server, and the UiPath Request Builder wizard is used to configure a REST endpoint to return a prediction. Details can be seen here: <a href="https://www.uipath.com/learning/video-tutorials/application-integration-rest-web-service-json">https://www.uipath.com/learning/video-tutorials/application-integration-rest-web-service-json</a>.</p>
			<h3>Scoring example</h3>
			<p>This video shows how to <a id="_idIndexMarker835"/>automate a workflow using H2O MOJO scoring: <a href="https://www.youtube.com/watch?v=LRlGjphraTY">https://www.youtube.com/watch?v=LRlGjphraTY</a>.</p>
			<p>We have just finished our survey of some MOJO scoring patterns for third-party software. Let's look at a few scoring patterns with software that your organization builds itself.  </p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor197"/>Exploring examples of MOJO scoring with your target-system software</h1>
			<p>In addition to deploying MOJOs for scoring <a id="_idIndexMarker836"/>on H2O and third-party software, you can also take a <strong class="bold">Do-It-Yourself</strong> (<strong class="bold">DIY</strong>) approach and deploy scoring in <a id="_idIndexMarker837"/>your own software. Let's see how to do this.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor198"/>Your software application</h2>
			<p>There are two<a id="_idIndexMarker838"/> ways to score from your own software: integrate with an external scoring system or embed scoring directly in your software system.</p>
			<p>The following diagram shows the pattern of integrating with an external scoring system:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B16721_10_012.jpg" alt="Figure 10.12 – MOJO application-scoring pattern for external scoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – MOJO application-scoring pattern for external scoring</p>
			<p>This pattern should look familiar because it is fundamentally the same as what we saw with scoring from BI tools: your software acts as a client to consume MOJO predictions made from another system. The external prediction system can be a MOJO deployed on a REST server (for example, an H2O REST scorer) or batch database scorer (for example, a Snowflake Java UDF or an H2O batch database scorer) or another external system, and your application needs to implement the libraries to connect to that system.</p>
			<p>In contrast, the following diagram shows the pattern of embedding MOJO scoring directly into your application itself:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B16721_10_013.jpg" alt="Figure 10.13 – MOJO application-scoring pattern for embedded scoring&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – MOJO application-scoring pattern for embedded scoring</p>
			<p>Doing so requires your application to implement a Java wrapper class that uses the H2O MOJO API to load the MOJO and score data with it. This was shown in detail in <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>, <em class="italic">Production Scoring and the H2O MOJO</em>.</p>
			<p>When should you use the external versus embedded scoring pattern? There are, of course, advantages and disadvantages to each pattern.  </p>
			<p>The external scoring <a id="_idIndexMarker839"/>pattern decouples scoring from the application and thus allows each component and the personas around it to focus on what it does best. Application developers, for example, can focus on developing the application and not deploying and monitoring models. Additionally, an external scoring component can be reused so that many applications and clients can connect to the same deployed model. Finally, particularly in the case of on-database scoring (for example, Java UDF and Teradata integration) and streaming scoring with extreme batch size or throughput, it would be difficult or foolish to attempt to build this on your own.</p>
			<p>The embedded scoring pattern has the advantage of eliminating the time cost of sending observations and predictions across the network. This may or may not be important depending on<a id="_idIndexMarker840"/> your <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>). It certainly simplifies the infrastructure requirements to perform scoring, especially when network infrastructure is unavailable or unreliable. Finally, and often for regulatory reasons, it may be desirable or necessary to manage the model deployment and the application as a single entity, thus demanding the coupling of the two.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor199"/>On-device scoring</h2>
			<p>MOJOs can be deployed to devices, whether<a id="_idIndexMarker841"/> they be an office scanner/printer, a medical device, or a sensor. These can be viewed as mini-applications, and the same decision for external or embedded scoring applies to devices as with applications, as discussed previously. In the case of devices, however, the advantages and disadvantages of external versus embedded scoring can be<a id="_idIndexMarker842"/> magnified greatly. For example, devices such as <strong class="bold">internet of things</strong> (<strong class="bold">IoT</strong>) sensors may number in the thousands, and the cost of deploying and managing models on each of these may outweigh the cost of greater latency resulting from network communication to a central external scorer.</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">A rule of thumb is that the available device memory needs to be over two times the size of the MOJO.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor200"/>Exploring examples of accelerators based on H2O Driverless AI integrations </h1>
			<p>This book thus far has focused<a id="_idIndexMarker843"/> on building models at scale using H2O. We have been doing this with H2O Core (often called H2O Open<a id="_idIndexMarker844"/> Source), a distributed ML framework that scales to massive datasets. We will see in <a href="B16721_13_Final_SK_ePub.xhtml#_idTextAnchor241"><em class="italic">Chapter 13</em></a>, <em class="italic">Introducing H2O AI Cloud,</em> that H2O offers a broader set of capabilities represented by an end-to-end platform called H2O AI Cloud. One of<a id="_idIndexMarker845"/> these capabilities is a highly focused AI-based AutoML component called Driverless AI, and we will distinguish this from H2O Core in <a href="B16721_13_Final_SK_ePub.xhtml#_idTextAnchor241"><em class="italic">Chapter 13</em></a><em class="italic">, Introducing H2O AI Cloud</em>.  </p>
			<p>Driverless AI is like H2O Core because it also generates ready-to-deploy MOJOs with a generic MOJO runtime and API, though for Driverless AI a license file is required for MOJO deployment, and the MOJO and runtime are named differently than for H2O Core.  </p>
			<p>The reason for mentioning this here is that several integrations of Driverless AI have been built and are well documented but have not analogously been built for H2O Core. These integrations and their documentation can be used as accelerators to do the same for H2O Core. Just bear in mind the lack of license requirement for deploying H2O Core MOJOs, and the differently named MOJOs and runtime.</p>
			<p class="callout-heading">Approach to Describing Accelerators</p>
			<p class="callout">Accelerators are overviewed here and links are provided to allow you to understand their implementation details. As noted, these accelerators represent the deployment of MOJOs generated from the H2O Driverless AI AutoML tool. Please review <a href="B16721_09_Final_SK_ePub.xhtml#_idTextAnchor159"><em class="italic">Chapter 9</em></a>,<em class="italic"> Production Scoring and the H2O MOJO</em> to understand how MOJOs generated from H2O Core (H2O-3 or Sparkling Water) are essentially the same as those generated from Driverless AI but with differences in naming and the MOJO API. This knowledge will allow you to implement the Driverless AI MOJO details shown in the links for H2O Core MOJOs.</p>
			<p>Let's take a look at<a id="_idIndexMarker846"/> some examples.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor201"/>Apache NiFi</h2>
			<p>Apache NiFi is an <strong class="bold">open source software</strong> (<strong class="bold">OSS</strong>) designed to<a id="_idIndexMarker847"/> program the flow of data in a UI and drag-and-drop fashion. It is built <a id="_idIndexMarker848"/>around the concept of moving data through different configurable processors that act on the <a id="_idIndexMarker849"/>data in specialized ways. The resulting data flows allow forking, merging, and nesting of sub-flows <a id="_idIndexMarker850"/>of processor sequences and generally resemble complex <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>). The project's<a id="_idIndexMarker851"/> home page can be found here: <a href="https://nifi.apache.org/index.html">https://nifi.apache.org/index.html</a>.</p>
			<p>NiFi processors can be used to communicate with external REST, JDBC, and Kafka systems and thus can leverage the pattern of scoring MOJOs from external systems.  </p>
			<p>You can, however, build your own processor that embeds the MOJO in the processor to score real-time or batch. This processor requires only configurations to point to the MOJO and its dependencies. The following link shows how to do this for Driverless AI and can be used as an accelerator for doing the same with H2O Core: <a href="https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi">https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-nifi</a>.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor202"/>Apache Flink</h2>
			<p>Apache Flink is a high throughput<a id="_idIndexMarker852"/> distributed <a id="_idIndexMarker853"/>stream- and batch-processing engine with an extensive feature set to run event-driven, data analytics, and data pipeline applications in a fault-tolerant way.</p>
			<p>The following link shows how to embed Driverless AI MOJOs to score data directly against Flink data streams and can be used as an accelerator for doing the same with H2O Core: <a href="https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink">https://github.com/h2oai/dai-deployment-examples/tree/master/mojo-flink</a>.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor203"/>AWS Lambda</h2>
			<p>AWS Lambda is a serverless <a id="_idIndexMarker854"/>computing service that lets you run code without the need to stand up, manage, and pay for underlying server infrastructure. It can perform any computing task that is short-lived and stateless, and thus is a nice fit for processing scoring requests. The following accelerator shows how to implement an AWS Lambda as a REST endpoint for real-time or batch MOJO scoring: <a href="https://h2oai.github.io/dai-deployment-templates/aws_lambda_scorer/">https://h2oai.github.io/dai-deployment-templates/aws_lambda_scorer/</a>.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor204"/>AWS SageMaker</h2>
			<p>AWS SageMaker can be used to host<a id="_idIndexMarker855"/> and monitor model scoring. The following accelerator shows how to implement a REST endpoint for real-time MOJO scoring: <a href="https://h2oai.github.io/dai-deployment-templates/aws-sagemaker-hosted-scorer/">https://h2oai.github.io/dai-deployment-templates/aws-sagemaker-hosted-scorer/</a>.</p>
			<p>And now, we have finished our survey of scoring and deployment patterns for H2O MOJOs. The business value of your H2O-at-scale models is achieved when they are deployed to production systems. The examples shown here are just a few possibilities, but they should give you an idea of how diverse MOJO deployments and scoring can be. </p>
			<p>Let's summarize what we've learned in this chapter.</p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor205"/>Summary</h1>
			<p>In this chapter, we explored a wide diversity of ways to deploy MOJOs and consume predictions. This included scoring against real-time, batch, and streaming data and scoring with H2O software, third-party software (such as BI tools and Snowflake tables), and your own software and devices. It should be evident from these examples that the H2O model-deployment possibilities are extremely diverse and therefore able to fit your specific scoring needs.</p>
			<p>Now that we have learned how to deploy H2O models to production-scoring environments, let's take a step back and start seeing through the eyes of enterprise stakeholders who participate in all the steps needed to achieve success with ML at scale with H2O. In the next section, we will view H2O at scale through the needs and concerns of these stakeholders.</p>
		</div>
	</body></html>