["```py\n    // output vectors of image points \n    std::vector<cv::Point2f> imageCorners; \n    // number of inner corners on the chessboard \n    cv::Size boardSize(7,5); \n    // Get the chessboard corners \n    bool found = cv::findChessboardCorners( \n                         image,         // image of chessboard pattern \n                         boardSize,     // size of pattern \n                         imageCorners); // list of detected corners \n\n```", "```py\n    // Draw the corners \n    cv::drawChessboardCorners(image, boardSize,  \n                      imageCorners, found); // corners have been found \n\n```", "```py\n    class CameraCalibrator { \n\n      // input points: \n      // the points in world coordinates \n      // (each square is one unit) \n      std::vector<std::vector<cv::Point3f>> objectPoints; \n      // the image point positions in pixels \n      std::vector<std::vector<cv::Point2f>> imagePoints; \n      // output Matrices \n      cv::Mat cameraMatrix; \n      cv::Mat distCoeffs; \n      // flag to specify how calibration is done \n      int flag; \n\n```", "```py\n    // Open chessboard images and extract corner points \n    int CameraCalibrator::addChessboardPoints(      \n        const std::vector<std::string> & filelist, // list of filenames \n        cv::Size & boardSize) {   // calibration board size \n\n      // the points on the chessboard \n      std::vector<cv::Point2f> imageCorners; \n      std::vector<cv::Point3f> objectCorners; \n\n      // 3D Scene Points: \n      // Initialize the chessboard corners  \n      // in the chessboard reference frame \n      // The corners are at 3D location (X,Y,Z)= (i,j,0) \n      for (int i=0; i<boardSize.height; i++) { \n        for (int j=0; j<boardSize.width; j++) { \n          objectCorners.push_back(cv::Point3f(i, j, 0.0f)); \n        } \n      } \n\n      // 2D Image points: \n      cv::Mat image; //to contain chessboard image \n      int successes = 0; \n      // for all viewpoints \n      for (int i=0; i<filelist.size(); i++) { \n\n        // Open the image \n        image = cv::imread(filelist[i],0); \n\n        // Get the chessboard corners \n        bool found = cv::findChessboardCorners( \n                         image,         // image of chessboard pattern  \n                         boardSize,     // size of pattern \n                         imageCorners); // list of detected corners \n\n        // Get subpixel accuracy on the corners \n        if (found) { \n          cv::cornerSubPix(image, imageCorners,  \n               cv::Size(5, 5), // half size of serach window \n               cv::Size(-1, -1),  \n               cv::TermCriteria( cv::TermCriteria::MAX_ITER +   \n                   cv::TermCriteria::EPS, 30, // max number of iterations \n                   0.1));                     // min accuracy \n\n          // If we have a good board, add it to our data \n          if (imageCorners.size() == boardSize.area()) { \n            //Add image and scene points from one view \n            addPoints(imageCorners, objectCorners); \n            successes++; \n          } \n        } \n\n        // If we have a good board, add it to our data \n        if (imageCorners.size() == boardSize.area()) { \n          //Add image and scene points from one view \n          addPoints(imageCorners, objectCorners); \n          successes++; \n        } \n      } \n      return successes; \n    } \n\n```", "```py\n    // Calibrate the camera \n    // returns the re-projection error \n    double CameraCalibrator::calibrate(cv::Size &imageSize) { \n      // Output rotations and translations \n      std::vector<cv::Mat> rvecs, tvecs; \n\n      // start calibration \n      return \n        calibrateCamera(objectPoints,  // the 3D points \n                        imagePoints,   // the image points \n                        imageSize,     // image size \n                        cameraMatrix,  // output camera matrix \n                        distCoeffs,    // output distortion matrix \n                        rvecs, tvecs,  // Rs, Ts  \n                        flag);         // set options \n    } \n\n```", "```py\n    // remove distortion in an image (after calibration) \n    cv::Mat CameraCalibrator::remap(const cv::Mat &image) { \n\n      cv::Mat undistorted; \n\n      if (mustInitUndistort) { // called once per calibration \n\n        cv::initUndistortRectifyMap(   \n                     cameraMatrix, // computed camera matrix \n                     distCoeffs,   // computed distortion matrix \n                     cv::Mat(),    // optional rectification (none)  \n                     cv::Mat(),    // camera matrix to generate undistorted \n                     image.size(), // size of undistorted \n                     CV_32FC1,     // type of output map \n                     map1, map2);  // the x and y mapping functions \n\n        mustInitUndistort= false; \n      } \n\n      // Apply mapping functions \n      cv::remap(image, undistorted, map1, map2,        \n                cv::INTER_LINEAR);     // interpolation type \n\n      return undistorted; \n    } \n\n```", "```py\n    cv::Size boardSize(7,7); \n    std::vector<cv::Point2f> centers; \n    bool found = cv:: findCirclesGrid(image, boardSize, centers); \n\n```", "```py\n    // Input object points \n    std::vector<cv::Point3f> objectPoints; \n    objectPoints.push_back(cv::Point3f(0, 45, 0)); \n    objectPoints.push_back(cv::Point3f(242.5, 45, 0)); \n    objectPoints.push_back(cv::Point3f(242.5, 21, 0)); \n    objectPoints.push_back(cv::Point3f(0, 21, 0)); \n    objectPoints.push_back(cv::Point3f(0, 9, -9)); \n    objectPoints.push_back(cv::Point3f(242.5, 9, -9)); \n    objectPoints.push_back(cv::Point3f(242.5, 9, 44.5)); \n    objectPoints.push_back(cv::Point3f(0, 9, 44.5)); \n\n```", "```py\n    // Input image points \n    std::vector<cv::Point2f> imagePoints; \n    imagePoints.push_back(cv::Point2f(136, 113)); \n    imagePoints.push_back(cv::Point2f(379, 114)); \n    imagePoints.push_back(cv::Point2f(379, 150)); \n    imagePoints.push_back(cv::Point2f(138, 135)); \n    imagePoints.push_back(cv::Point2f(143, 146)); \n    imagePoints.push_back(cv::Point2f(381, 166)); \n    imagePoints.push_back(cv::Point2f(345, 194)); \n    imagePoints.push_back(cv::Point2f(103, 161)); \n\n    // Get the camera pose from 3D/2D points \n    cv::Mat rvec, tvec; \n    cv::solvePnP( \n                 objectPoints, imagePoints,      // corresponding 3D/2D pts  \n                 cameraMatrix, cameraDistCoeffs, // calibration  \n                 rvec, tvec);                    // output pose \n\n    //Convert to 3D rotation matrix \n    cv::Mat rotation; \n    cv::Rodrigues(rvec, rotation); \n\n```", "```py\n    // Create a viz window \n    cv::viz::Viz3d visualizer(\"Viz window\"); \n    visualizer.setBackgroundColor(cv::viz::Color::white()); \n\n```", "```py\n    // Create a virtual camera \n    cv::viz::WCameraPosition cam( \n                    cMatrix,     // matrix of intrinsics \n                    image,       // image displayed on the plane \n                    30.0,        // scale factor \n                    cv::viz::Color::black()); \n    // Add the virtual camera to the environment \n    visualizer.showWidget(\"Camera\", cam); \n\n```", "```py\n    // Create a virtual bench from cuboids \n    cv::viz::WCube plane1(cv::Point3f(0.0, 45.0, 0.0),             \n                          cv::Point3f(242.5, 21.0, -9.0),   \n                          true,     // show wire frame  \n                          cv::viz::Color::blue()); \n    plane1.setRenderingProperty(cv::viz::LINE_WIDTH, 4.0); \n    cv::viz::WCube plane2(cv::Point3f(0.0, 9.0, -9.0), \n                          cv::Point3f(242.5, 0.0, 44.5),                \n                          true,    // show wire frame  \n                          cv::viz::Color::blue()); \n    plane2.setRenderingProperty(cv::viz::LINE_WIDTH, 4.0); \n    // Add the virtual objects to the environment \n    visualizer.showWidget(\"top\", plane1); \n    visualizer.showWidget(\"bottom\", plane2); \n\n```", "```py\n    cv::Mat rotation; \n    // convert vector-3 rotation \n    // to a 3x3 rotation matrix \n    cv::Rodrigues(rvec, rotation); \n\n    // Move the bench  \n    cv::Affine3d pose(rotation, tvec); \n    visualizer.setWidgetPose(\"top\", pose); \n    visualizer.setWidgetPose(\"bottom\", pose); \n\n```", "```py\n    // visualization loop \n    while(cv::waitKey(100)==-1 && !visualizer.wasStopped()) { \n\n      visualizer.spinOnce(1,      // pause 1ms  \n                          true);  // redraw \n    } \n\n```", "```py\n    // vector of keypoints and descriptors \n    std::vector<cv::KeyPoint> keypoints1; \n    std::vector<cv::KeyPoint> keypoints2; \n    cv::Mat descriptors1, descriptors2; \n\n    // Construction of the SIFT feature detector  \n    cv::Ptr<cv::Feature2D> ptrFeature2D =   \n                           cv::xfeatures2d::SIFT::create(500); \n\n    // Detection of the SIFT features and associated descriptors \n    ptrFeature2D->detectAndCompute(image1, cv::noArray(),  \n                                   keypoints1, descriptors1); \n    ptrFeature2D->detectAndCompute(image2, cv::noArray(),  \n                                   keypoints2, descriptors2); \n\n    // Match the two image descriptors \n    // Construction of the matcher with crosscheck  \n    cv::BFMatcher matcher(cv::NORM_L2, true); \n    std::vector<cv::DMatch> matches; \n    matcher.match(descriptors1, descriptors2, matches); \n\n    // Convert keypoints into Point2f \n    std::vector<cv::Point2f> points1, points2; \n    for (std::vector<cv::DMatch>::const_iterator it =  \n           matches.begin(); it != matches.end(); ++it) { \n\n      // Get the position of left keypoints \n      float x = keypoints1[it->queryIdx].pt.x; \n      float y = keypoints1[it->queryIdx].pt.y; \n      points1.push_back(cv::Point2f(x, y)); \n      // Get the position of right keypoints \n      x = keypoints2[it->trainIdx].pt.x; \n      y = keypoints2[it->trainIdx].pt.y; \n      points2.push_back(cv::Point2f(x, y)); \n    } \n\n    // Find the essential between image 1 and image 2 \n    cv::Mat inliers; \n    cv::Mat essential = cv::findEssentialMat(points1, points2,            \n                                Matrix,         // intrinsic parameters \n                                cv::RANSAC,\n                                0.9, 1.0,       // RANSAC method \n                                inliers);       // extracted inliers \n\n```", "```py\n    // recover relative camera pose from essential matrix \n    cv::Mat rotation, translation; \n    cv::recoverPose(essential,             // the essential matrix \n                    points1, points2,      // the matched keypoints \n                    cameraMatrix,          // matrix of intrinsics \n                    rotation, translation, // estimated motion \n                    inliers);              // inliers matches \n\n```", "```py\n    // compose projection matrix from R,T \n    cv::Mat projection2(3, 4, CV_64F); // the 3x4 projection matrix \n    rotation.copyTo(projection2(cv::Rect(0, 0, 3, 3))); \n    translation.copyTo(projection2.colRange(3, 4)); \n\n    // compose generic projection matrix  \n    cv::Mat projection1(3, 4, CV_64F, 0.); // the 3x4 projection matrix \n    cv::Mat diag(cv::Mat::eye(3, 3, CV_64F)); \n    diag.copyTo(projection1(cv::Rect(0, 0, 3, 3))); \n\n    // to contain the inliers \n    std::vector<cv::Vec2d> inlierPts1; \n    std::vector<cv::Vec2d> inlierPts2; \n\n    // create inliers input point vector for triangulation \n    int j(0); \n    for (int i = 0; i < inliers.rows; i++) { \n      if (inliers.at<uchar>(i)) { \n        inlierPts1.push_back(cv::Vec2d(points1[i].x, points1[i].y)); \n        inlierPts2.push_back(cv::Vec2d(points2[i].x, points2[i].y)); \n      } \n    } \n\n    // undistort and normalize the image points \n    std::vector<cv::Vec2d> points1u; \n    cv::undistortPoints(inlierPts1, points1u,  \n                        cameraMatrix, cameraDistCoeffs); \n    std::vector<cv::Vec2d> points2u; \n    cv::undistortPoints(inlierPts2, points2u,  \n                        cameraMatrix, cameraDistCoeffs); \n\n    // triangulation \n    std::vector<cv::Vec3d> points3D; \n    triangulate(projection1, projection2,  \n                points1u, points2u, points3D); \n\n```", "```py\n    // triangulate using Linear LS-Method \n    cv::Vec3d triangulate(const cv::Mat &p1,  \n                          const cv::Mat &p2,                 \n                          const cv::Vec2d &u1,  \n                          const cv::Vec2d &u2) { \n\n    // system of equations assuming image=[u,v] and X=[x,y,z,1] \n    // from u(p3.X)= p1.X and v(p3.X)=p2.X \n    cv::Matx43d A(u1(0)*p1.at<double>(2, 0) - p1.at<double>(0, 0),  \n                  u1(0)*p1.at<double>(2, 1) - p1.at<double>(0, 1),                      \n                  u1(0)*p1.at<double>(2, 2) - p1.at<double>(0, 2),   \n                  u1(1)*p1.at<double>(2, 0) - p1.at<double>(1, 0),                 \n                  u1(1)*p1.at<double>(2, 1) - p1.at<double>(1, 1),   \n                  u1(1)*p1.at<double>(2, 2) - p1.at<double>(1, 2),  \n                  u2(0)*p2.at<double>(2, 0) - p2.at<double>(0, 0),  \n                  u2(0)*p2.at<double>(2, 1) - p2.at<double>(0, 1),  \n                  u2(0)*p2.at<double>(2, 2) - p2.at<double>(0, 2),  \n                  u2(1)*p2.at<double>(2, 0) - p2.at<double>(1, 0),          \n                  u2(1)*p2.at<double>(2, 1) - p2.at<double>(1, 1),    \n                  u2(1)*p2.at<double>(2, 2) - p2.at<double>(1, 2)); \n\n    cv::Matx41d B(p1.at<double>(0, 3) - u1(0)*p1.at<double>(2, 3), \n                  p1.at<double>(1, 3) - u1(1)*p1.at<double>(2, 3), \n                  p2.at<double>(0, 3) - u2(0)*p2.at<double>(2, 3),  \n                  p2.at<double>(1, 3) - u2(1)*p2.at<double>(2, 3)); \n\n    // X contains the 3D coordinate of the reconstructed point \n    cv::Vec3d X; \n    // solve AX=B \n    cv::solve(A, B, X, cv::DECOMP_SVD); \n    return X; \n  } \n\n```", "```py\n    // Compute homographic rectification \n    cv::Mat h1, h2; \n    cv::stereoRectifyUncalibrated(points1, points2,  \n                                  fundamental,  \n                                  image1.size(), h1, h2); \n\n    // Rectify the images through warping \n    cv::Mat rectified1; \n    cv::warpPerspective(image1, rectified1, h1, image1.size()); \n    cv::Mat rectified2; \n    cv::warpPerspective(image2, rectified2, h2, image1.size()); \n\n```", "```py\n    // Compute disparity \n    cv::Mat disparity; \n    cv::Ptr<cv::StereoMatcher> pStereo =  \n         cv::StereoSGBM::create(0,   // minimum disparity \n                                32,  // maximum disparity \n                                5);  // block size \n    pStereo->compute(rectified1, rectified2, disparity); \n\n```"]