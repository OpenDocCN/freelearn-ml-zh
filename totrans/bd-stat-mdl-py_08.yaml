- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discrete Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, we discussed models for predicting a continuous
    response variable. In this chapter, we will begin discussing models for predicting
    discrete response variables. We will start by discussing the probit and logit
    models for predicting binary outcome variables (categorical variables with two
    levels). Then, we will extend this idea to predicting categorical variables with
    multiple levels. Finally, we will look at predicting count variables, which are
    like categorical variables but only take values of integers and have an infinite
    number of levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Probit and logit models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multinomial logit model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poisson model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The negative binomial regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probit and logit models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we discussed different types of problems that can be solved with
    regression models. In particular, the dependent variable is continuous, such as
    house prices, salaries, and so on. A natural question is if dependent variables
    are not continuous – in other words, if they are categorical – how would we adapt
    our regression equation to predict a categorical response variable? For instance,
    a human resources department in a company wants to conduct an attrition study
    to predict whether an employee will stay with the company or a car dealership
    wants to know if one car can be sold or not based on prices, car models, colors,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will study **binary classification**. Here, the outcome (dependent
    variable) is a binary response such as yes/no or to do/not to do. Let’s look back
    at the simple linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: y = β 0 + β 1 x+ ϵ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the predicted outcome is a line crossing data points. With this model,
    we could build a model to predict house prices (the dependent variable) based
    on different independent variables, such as districts, units, stories, or distances.
    The same idea cannot be applied to a binary classification problem. Let’s consider
    a simple example that we want to predict if a house is sold only based on its
    price. Let’s build a model based on linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Sold = β 0 + β 1 * Price + ϵ (1)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, Sold = 1 if the house is sold and Sold=0 if it was not sold. By performing
    visualization using linear regression, we can draw a line of best fit through
    the data points. It looks like when the price increases, the chance of selling
    a house decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Line of best fit using linear regression](img/B18945_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Line of best fit using linear regression
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead of considering the equation model (*1*), we will be considering
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prob(Sold) = β 0 + β 1 * Price + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, Prob(Sold) is the probability of selling a house. The probability value
    will be between 0 and 1; the midpoint of its range is 0.5 when the probability
    of selling a house is equal to the probability of not selling a house. However,
    when the price of a house is very high, then the value of Prob(Sold) can be negative.
    To avoid this problem, we can model the problem with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: P _ 1 − P = β 0 + β 1 * Price+ ϵ (2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, P is the probability of selling a house and 1 − P is the probability
    of not selling a house. The value of  P _ 1 − P is in the range of [0, ∞) and
    when the probability of selling a car is the same as the probability of not selling
    a car, (P=0.5), then  P _ 1 − P = 1, or the midpoint of its range is 1\. This
    is also called the **odds ratio**. To interpret the odds ratio, when  P _ 1 −
    P = 1, for every house that is sold, there is a house that is not sold. When P
    = 0.75, then the odds ratio is 3\. In this case, the interpretation is that the
    odds of being able to sell the house are three times higher than the odds of not
    being able to sell the house. Going back to the equation (*2*), the range of the
    odds ratio is [0, ∞) and its lower limit is 0, but as we discussed previously
    for equation (*1*), when the price of a house is really high, then the estimate
    of the odds ratio could be negative, which contradicts the probability properties.
    In addition, because the midpoint is 1 but the range of the odds ratio is from
    0 to infinity, then the distribution is very skewed with a long right tail. However,
    we expected it to be normally distributed per one of the assumptions of linear
    regression. So, instead of using equation (*2*), we will consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: log( P _ 1 − P) = β 0 + β 1 * Price + ϵ
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of log( P _ 1 − P) is in the range of (− ∞ , ∞) and the midpoint
    is 0\. Similar to linear regression, we can use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: log(odds) = log( P _ 1 − P) = β 0 + β 1 X 1 + β 2 X 2 + … = z (3)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this to interpret how the explanatory variables are associated with
    the categorical outcome. We can rewrite the previous equation (*3*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: F(z) = P =  e z _ 1 + e z ,
  prefs: []
  type: TYPE_NORMAL
- en: The preceding formula is bounded below by 0 and above by 1 (the predicted probability,
    *P*, of the outcome takes place) and is called the logit model. It is the cumulative
    distribution function of the **logistic distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach to model the probability of a binary dependent variable is
    **probit regression**. Instead of using the cumulative distribution function of
    logistic regression, we use the cumulative distribution function of the standard
    normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: F(z) =  ∫ −∞ zϕ(u)du = ϕ(z).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding formula is bounded below by 0 and above by 1 (the predicted probability,
    *P*, of the outcome takes place), and is called the **probit model**. Remember
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: ϕ(z) = P(Z ≤ z), Z~𝒩(0,1).
  prefs: []
  type: TYPE_NORMAL
- en: Both the logit and probit models are estimated by using the `statsmodels`, we
    have classes for the probit and logit models. The documentation for these classes
    can be found at [https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Probit.xhtml](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Probit.xhtml)
    and [https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.xhtml](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Logit.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a training dataset to illustrate how to use the `Logit` class
    from `statsmodels` to conduct a logit study:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `Admitted` is the dependent variable and has two possibilities (1 – admitted
    and 0 – not admitted). `GPA` is the GPA grade and `Exp` is the number of years
    of experience. The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Training dataset on GPA grades and years of experience](img/B18945_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Training dataset on GPA grades and years of experience
  prefs: []
  type: TYPE_NORMAL
- en: 'We must also create a testing dataset for this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Testing dataset on GPA grades and years of experience](img/B18945_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Testing dataset on GPA grades and years of experience
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use `logit` from `statsmodels`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Logit Regression Output](img/B18945_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Logit Regression Output
  prefs: []
  type: TYPE_NORMAL
- en: From this output, we can see that `GPA` and `Exp` are significant at the α =
    0.05 significance level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values under the `[0.025 0.975]` heading are the 95% confidence interval
    for `Intercept`, `GPA`, and `Exp`, respectively. The next step is to use `confusion_matrix`
    and `accuracy_score` to compute the accuracy of the model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Confusion matrix on testing dataset](img/B18945_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Confusion matrix on testing dataset
  prefs: []
  type: TYPE_NORMAL
- en: By using these train and test datasets, the model can predict the outcome perfectly.
    In the next section, we will discuss **multi-class regression** using a similar
    idea as in binary logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial logit model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, there are many situations where the outcomes (dependent variables)
    are not binary but have more than two possibilities. `MNLogit` class from `statsmodels`:
    [https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.MNLogit.xhtml](https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.MNLogit.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Iris data ([https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris))
    is one of the best-known statistical and machine learning datasets for education.
    The independent variables are sepal length (in cm), sepal width (in cm), petal
    length (in cm), and petal width (in cm). The dependent variable is a categorical
    variable with three levels: Iris Setosa (0), Iris Versicolor (1), and Iris Virginia
    (2). The following Python codes illustrate how to conduct this using `sklearn`
    and `statsmodels`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Both methods give us the same test accuracy value (96.67%), with the confusion
    matrix produced as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Confusion matrices using sklearn (left) and statsmodels (right)](img/B18945_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Confusion matrices using sklearn (left) and statsmodels (right)
  prefs: []
  type: TYPE_NORMAL
- en: Poisson model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed models where the response variable was
    categorical. In this section, we will look at a model for count data. Count data
    is like categorical data (the categories are integers), but there are an infinite
    number of levels (0, 1, 2, 3, and so on). We model count data with the **Poisson
    distribution**. In this section, we will start by examining the Poisson distribution
    and its properties. Then, we will model a count variable with explanatory variables
    using the Poisson model.
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Poisson distribution is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: P(k) =  λ k e −λ _ k !
  prefs: []
  type: TYPE_NORMAL
- en: Here, λ is the average number of events and k is the number of events for which
    we would like the probability. P(k) is the probability that the k events occur.
    This distribution is used to calculate the probability of k events occurring in
    a fixed time interval or a defined space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shape of the distribution changes with the value of λ. When λ is greater
    than 10, the distribution appears approximately normal. However, as λ approaches
    0, the distribution becomes right-skewed. This is because count data cannot be
    negative. Three example Poisson distributions are shown in *Figure 8**.7* with
    means of 12, 5, and 2\. Notice that the distribution with the mean of 12 is approximately
    normally distributed, while the distributions of 5 and 2 are right-skewed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Example Poisson distributions with means of 12, 5, and 2](img/B18945_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Example Poisson distributions with means of 12, 5, and 2
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting aspect of the distribution is that the mean and variance
    are equal. This means that as the mean increases, the spread of the distribution
    also increases. We can see this in action in the examples in *Figure 8**.7*. The
    distribution with a mean of 2 has a small spread with a large peak at the mean,
    while the distribution with a mean of 12 has a much wider spread with a lower
    peak at the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed the Poisson distribution, let’s look at how to set
    up a Poisson model.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling count data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s look at how to model the response variable of counts with the Poisson
    model. As mentioned previously, count data often follows a Poisson distribution.
    The Poisson model is expressed mathematically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y = e b 0+b 1x 1+…+b nx n
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, y is the response variable, b values are model coefficients, and x variables
    represent explanatory variables. This should appear similar to the equation we
    used in the previous chapter but with the addition of the exponentiation of the
    explanatory variables. This type of model is called a **log-linear** model, which
    is a model where the logarithm of the response variable is modeled by a linear
    combination of variables. We can rewrite this equation by applying the natural
    logarithm to both sides of the equation to make it explicit:'
  prefs: []
  type: TYPE_NORMAL
- en: ln(y) = b 0 + b 1 x 1 + … + b n x n
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have the logarithm of the response variable ( ln(y) ) expressed as a
    linear combination of explanatory variables.
  prefs: []
  type: TYPE_NORMAL
- en: The natural logarithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The Poisson model uses a special logarithm called the natural logarithm. The
    natural logarithm of a number is the logarithm of that number using the mathematical
    constant e as the base. The natural logarithm is generally written as ln(x), log e(x),
    or log(x) (the first two options are explicit, but the third option can be ambiguous).
    The logarithm operation is the inverse of exponentiation. In this case, the natural
    logarithm is the inverse of the exponential function: ln(e x) = x = e ln(x). The
    natural logarithm and exponential function are commonly used in statistics, mathematics,
    and science.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. We will be using the Bike Sharing Dataset from UCI
    ([https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).).
    In this dataset, we have counts of bikes rented each day. There are two types
    of rentals: pre-registered (registered) and on-demand at a location (causal).
    In this example, we will model the weekly mean count of casually rented bikes
    over a given year. The dataset provides several explanatory variables, including
    environmental factors such as temperature and calendar information such as whether
    a holiday occurred.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by setting up the equation for our model and then take a look
    at the results from fitting the model with `statsmodels`. The model equation follows
    the form we discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: ln(weekly _ mean _ rental _ count) = b 0 + b 1(temperature) + b 1(season)
  prefs: []
  type: TYPE_NORMAL
- en: + b 2(weather _ situation) + b 3(humidity) + b 4(wind _ speed) + b 5(holiday)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can fit this model with the given data using `statsmodels`, similar to how
    we did in the previous chapter. An excerpt of the code to fit the model is shown
    here (see the Jupyter Notebook for details on preprocessing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After fitting the model, we can get details on the coefficients using the `summary()`
    method. For this model, we get the following output for the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Poisson model summary](img/B18945_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Poisson model summary
  prefs: []
  type: TYPE_NORMAL
- en: Just like the modeling example for linear regression, these coefficient values
    are estimates of the parameters listed in our model. All the explanatory variables
    in the model appear to be significant in the model. Interestingly, based on the
    value of the coefficient estimates, temperature (`atemp`) appears to be the most
    influential factor, followed by humidity (`hum`) and wind speed. With the model
    fit and no need to remove insignificant variables, we can assess the model’s performance.
    This model has an MAE of 155, which corresponds to a MAPE of 36%.
  prefs: []
  type: TYPE_NORMAL
- en: The Poisson model depends strongly on the assumption that the response variable
    has a Poisson distribution. In the next section, we will look at a similar type
    of model for count data, but with weaker distributional assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: The negative binomial regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another useful approach to **discrete regression** is the **log-linear negative
    binomial regression** model, which uses the negative binomial probability distribution.
    At a high level, negative binomial regression is useful with *over-dispersed count
    data* where the *conditional mean of the count is smaller than the conditional
    variance of the count*. Model **over-dispersion** is where the variance of the
    target variable is greater than the mean assumed by the model. In a regression
    model, the mean is the regression line. We make the determination of using the
    negative binomial model based on target variable counts analysis (mean versus
    variance) and supply a measure of model over-dispersion to the negative binomial
    model to adjust for the over-dispersion, which we will discuss here.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the negative binomial model is not for modeling
    simply discrete data, but specifically **count data** associated with a fixed
    number of **random** trials, such as modeling the number of attempts before an
    event occurs – or failing to occur – in a random sampling scenario. The model
    operates only on count data, where each count response of the target variable
    is based on a finite set of outcomes. Additionally, because the count data is
    the result of repeated binomial trials, the order of count arrangement does not
    matter.
  prefs: []
  type: TYPE_NORMAL
- en: Negative binomial distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is an example of the negative binomial distribution of failure
    counts with a conditional mean of 17 and a conditional variance of 52:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Negative binomial distribution example](img/B18945_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Negative binomial distribution example
  prefs: []
  type: TYPE_NORMAL
- en: 'The binomial distribution is a construct of counts of successes in a fixed
    count of random trials (X = ( X 1, X 2, … )) of a Bernoulli random variable, which
    is a variable that has one of two outcome values: 0 or 1\. The negative binomial
    distribution is a construct of the count of failures in a fixed count of random
    draws of the Bernoulli random variable. This distinction is important because
    a model using binomial regression models a binary outcome across observations,
    whereas a negative binomial regression model models a count outcome across observations.
    The negative binomial distribution is the inverse of the binomial distribution.
    The probability mass function for the negative binomial distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(X − x) = (x + n − 1 n − 1 ) p n (1 − p) x
  prefs: []
  type: TYPE_NORMAL
- en: Here, there are x failures and n − 1 successes in a set of x + n − 1 trials
    reaching success at trial x + n.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning the Poisson distribution, the negative binomial *does not require
    strict adherence to the assumption that conditional variance is equal to the conditional
    mean* as it includes an additional parameter, *α*, to explain the extra variance,
    whereas the Poisson model assumes the variance is equal to the mean, *μ*. The
    negative binomial model assumes the variance, here based on the Poisson-gamma
    mixture distribution, is equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: μ(1 + αμ)
  prefs: []
  type: TYPE_NORMAL
- en: 'This reduces to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: (μ + α μ 2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Stated in terms of the probability of success, *p*, the negative binomial variance
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: n  1 − p _ p 2
  prefs: []
  type: TYPE_NORMAL
- en: 'It has the following mean:'
  prefs: []
  type: TYPE_NORMAL
- en: n  1 − p _ p
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the variance is not expected to equal the mean with the negative binomial
    model, the negative binomial is likely to outperform the Poisson approach *when
    the counts are large enough that the variance in response exceeds the mean*. Similarly,
    to Poisson, a negative binomial is a log-linear model with confidence intervals
    being based on the Wald and drop-in-deviance likelihood ratio. The form of the
    negative binomial model’s regression equation, shown here, is the same as that
    for Poisson regression:'
  prefs: []
  type: TYPE_NORMAL
- en: ln(y) = b 0 + b 1 x 1 + … + b n x n
  prefs: []
  type: TYPE_NORMAL
- en: 'It reduces to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: y = e b 0 + e b 1x 1 + … + e b nx n
  prefs: []
  type: TYPE_NORMAL
- en: 'The **maximum likelihood estimate** of the probability of success for a given
    sample of the distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: n _ n +  _ x  ′
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum likelihood estimation** (**MLE**) is an underpinning of the log-odds
    (or logit) and log-likelihood approaches to statistical modeling. Likelihood is
    the probability of the known outcome of a regression model being observed given
    specific regression coefficient values. By default, a set of variables will have
    a higher likelihood than another set of variables if the set provides a higher
    probability of the observed outcome being obtained. The logarithm of the likelihood
    is taken as a measure of goodness-of-fit for a regression model. Out of a set
    of potential coefficient values for each variable, the set of coefficients with
    the maximum log-likelihood values for each variable are referred to as the **maximum
    likelihood estimates**. These values are obtained through an iterative approach
    that generates multiple possible values. If the sample size is sufficiently large
    and an appropriate set of variables has been obtained for the model, the maximum
    likelihood estimates can be considered unbiased.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculating the confidence intervals for negative binomial regression is similar
    to that for logistic regression. The **Wald** approach to the calculation leverages
    a **z-ratio**. Where there are *j* variables in the model, the z-ratio is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (  ˆ β  j− β j) _ SE(  ˆ β  j)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *SE* is the standard error. The confidence interval is the variable’s
    coefficient estimate plus and minus the half-width confidence interval percentile
    multiplied by the standard error of the coefficient estimate. The z-ratio can
    be used because it is assumed the estimates have standard normal sampling distributions.
    Therefore, we can derive the 95% confidence interval for the variable’s estimated
    coefficient as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lower 95% confidence limit:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ β  j − 0.475 × SE( ˆ β  j)
  prefs: []
  type: TYPE_NORMAL
- en: 'Upper 95% confidence limit:'
  prefs: []
  type: TYPE_NORMAL
- en: ˆ β  j + 0.475 × SE( ˆ β  j)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three required assumptions specific to negative binomial regression:'
  prefs: []
  type: TYPE_NORMAL
- en: Independence between samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A linear relationship between the log of the target variable and input variables
    (log-linear model).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conditional variance is greater than or equal to the conditional mean.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Independence between samples means there is no serial correlation nor any cluster
    or other conditional dependence between samples. A linear relationship between
    the log of the target variable and input variables means that the relationship
    between the logarithm of the target variable and changes in each input variable
    scales linearly. Except for requirement 3, which we discussed at the start of
    this section, the requirements are essentially the same as for the Poisson model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through an example in Python using statsmodels. For this, let’s
    load the statsmodels affairs dataset to model child count (the `children` variable)
    using the remaining variables. In line three, we must add the constant required
    to generate an intercept coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let’s numerically confirm there is over-dispersion present in the target
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the variance is greater than the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mean count of children per marriage:` `1.3968740182218033`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Variance of the count of children per marriage:` `2.054838616333698`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the conditional mean per marriage is smaller than the
    conditional variance. While not a massive difference, it is enough to consider
    using negative binomial regression. Let’s visually observe the distribution of
    the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Distribution of children](img/B18945_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Distribution of children
  prefs: []
  type: TYPE_NORMAL
- en: 'The first five rows of the data can be seen here. Note that the first column
    of the design matrix should always contain the constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – First five records of example data set, including the added
    constant](img/B18945_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – First five records of example data set, including the added constant
  prefs: []
  type: TYPE_NORMAL
- en: 'In our visual inspection of the distribution of children in *Figure 8**.10*,
    we identified that there is a value of 5.5 for children. This may be the result
    of averaging or an error. A subject matter expert may help determine this, but
    for our analysis, let’s assume it was a mistake and round to a whole number of
    children since people are not fractional. Let’s set up the target array, *y*,
    and design matrix, *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create a train and test split for regression modeling. Note that
    `shuffle=True` will provide different results. To obtain a representative sample,
    the data should be randomly shuffled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the negative binomial model is based on a Poisson-gamma mixture model,
    we need to estimate the model’s measure of over-dispersion using a Poisson model.
    A method referred to as **auxiliary OLS regression (without constant)** is provided
    by A. Colin Cameron and Pravin K. Trivedi in *Microeconometrics: Methods and Applications*.
    The authors propose the creation of an over-dispersion test statistic where the
    null hypothesis is α=0 and the alternate hypothesis is α ≠ 0, where *α* is the
    estimate of over-dispersion. The auxiliary OLS regression formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ( y i −  ˆ μ  i) 2 − y i  _  ˆ μ  i  = α  g(  ˆ μ  i) _  ˆ μ  i  + μ i
  prefs: []
  type: TYPE_NORMAL
- en: Here, μ i is an error term and g(  ˆ μ  i) is  ˆ μ  i 2\. Thus, the right-hand
    operand reduces to α  ˆ μ  i + μ i. In terms of negative binomial regression,
    we consider the error to equal zero, so we can factor α in as the over-dispersion
    estimate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we have fit our training data to a generalized linear
    model using the Poisson model for the linkage. Then, we used the regression mean
    of the model to build the estimated auxiliary target variable. Because the method
    is “without constant,” we subtract 1 to remove the constant from the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the estimated dispersion for the negative binomial model is
    `0.622034`. Now, we need to assess if the auxiliary estimate is statistically
    significant. We can do this using the p-value from the OLS model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – OLS regression results](img/B18945_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – OLS regression results
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the coefficient is significant and greater than 0, we can confirm the
    model has over-dispersion based on the target. The coefficient can be used as
    the measure for that over-dispersion in the negative binomial model, which we
    can use to adjust the variance in the `alpha` argument here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Generalized linear model regression results](img/B18945_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Generalized linear model regression results
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s use the model we built on the training data to predict the training
    data, then again to predict on the test data so that we can compare generalizability
    on unseen data using residual error as a basis of comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe from the root mean squared error that the model’s performance
    is approximately constant across training and test data, indicating a consistent
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Training Root Mean Squared` `Error: 1.2553439918425695`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Testing Root Mean Squared` `Error: 1.266620561303553`'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained the issue of encountering negative raw probabilities
    that are generated by building a binary classification probability model based
    strictly on linear regression, where probabilities in a range of [0, 1] are expected.
    We provided an overview of the log-odds ratio and probit and logit modeling using
    the cumulative distribution function of both the standard normal distribution
    and logistic distribution, respectively. We also demonstrated methods for applying
    logistic regression to solve binary and multinomial classification problems. Lastly,
    we covered count-based regression using the log-linear Poisson and negative binomial
    models, which can also be logically extended to rate data without modification.
    We provided examples of their implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will introduce conditional probability using Bayes’
    theorem in addition to dimension reduction and classification modeling using linear
    discriminant analysis and quadratic discriminant analysis.
  prefs: []
  type: TYPE_NORMAL
