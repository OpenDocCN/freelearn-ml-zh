- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging Simulators and Rendering Engines to Generate Synthetic Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce a well-known method for synthetic data generation
    using simulators and rendering engines. We will explore the main pipeline for
    creating a simulator and generating automatically annotated synthetic data. Following
    this, we will highlight the challenges and briefly discuss two simulators for
    synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simulators and rendering engines: definitions, history, and evolution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to generate synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case studies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to simulators and rendering engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will dive into the world of simulators and rendering engines.
    We will look at the history and evolution of these powerful tools for synthetic
    data generation.
  prefs: []
  type: TYPE_NORMAL
- en: Simulators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **simulator** is software or a program written to imitate or simulate certain
    processes or phenomena of the real world. Simulators usually create a virtual
    world where scientists, engineers, and other users can test their algorithms,
    products, and hypotheses. At the same time, you can use this virtual environment
    to help you learn about and practice complex tasks. These tasks are usually dangerous
    and very expensive to perform in the real world. For example, driving simulators
    teach learners how to drive and how to react to unexpected scenarios such as a
    child suddenly crossing the street, which is extremely dangerous to do in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: Simulators are used in various fields, such as aviation, healthcare, engineering,
    driving, space, farming, and gaming. In *Figure 6**.1*, you can find examples
    of these simulators.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Examples of simulators utilized in driving, engineering, healthcare,
    and farming](img/Figure_06_01_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Examples of simulators utilized in driving, engineering, healthcare,
    and farming
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce rendering and game engines.
  prefs: []
  type: TYPE_NORMAL
- en: Rendering and game engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rendering and game engines are software used mainly to generate images or videos.
    They are composed of various subsystems responsible for simulating, for example,
    physics, lighting, and sound. They are usually used in fields such as gaming,
    animation, virtual reality, augmented reality, and the metaverse. Unlike simulators,
    game engines can be used to create virtual worlds that may or may not be designed
    to mimic the real world. Game engines are mainly used to develop games. However,
    they can be utilized for training and simulation, films and television, and visualization.
    In *Figure 6**.2*, you can see some examples of modern rendering and game engines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Examples of modern rendering and game engines](img/Figure_06_02_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Examples of modern rendering and game engines
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll learn more about the history of rendering and game engines.
  prefs: []
  type: TYPE_NORMAL
- en: History and evolution of simulators and game engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Game engines roughly started to appear in the 1970s. The computers of that era
    were limited in terms of processing capabilities and memory. At that time, most
    games were 2D games, such as *Pong* and *Spacewar!*. They were limited to simple
    graphics, basic lighting, elementary shading, and limited visual and sound effects.
  prefs: []
  type: TYPE_NORMAL
- en: The great advancements in hardware led to more sophisticated game engines. These
    game engines, such as *Unreal* ([https://www.unrealengine.com](https://www.unrealengine.com))
    and *Unity* ([https://unity.com](https://unity.com)) facilitated the creation
    of rich, photorealistic virtual worlds. As physics simulations became more sophisticated
    and advanced, the development of photorealistic graphics also progressed simultaneously.
    This allowed the simulation of complex physics and interactions between scene
    elements, such as fluid dynamics and cloth simulation. In more recent years, many
    photorealistic, complex games have been released, such as *Call of Duty* ([https://www.callofduty.com](https://www.callofduty.com))
    and *Grand Theft* *Auto* ([https://www.rockstargames.com/games/vicecity](https://www.rockstargames.com/games/vicecity)).
  prefs: []
  type: TYPE_NORMAL
- en: The availability of complex and easy-to-use game engines such as Unity has democratized
    game development and made it more accessible than ever before. Thus, game development
    is not just limited to big tech companies but is also available to independent
    companies and artists.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the huge sudden increase in the number of mobile phones recently
    made mobile games a more attractive destination for research and industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, synthetic data researchers started to experiment with using game
    engines and video games to generate rich synthetic data. Two of the pioneer works
    in this area are *Playing for Data: Ground Truth from Computer Games* ([https://arxiv.org/abs/1608.02192](https://arxiv.org/abs/1608.02192))
    and *Domain Randomization for Transferring Deep Neural Networks from Simulation
    to the Real* *World* ([https://arxiv.org/pdf/1703.06907.pdf](https://arxiv.org/pdf/1703.06907.pdf)).'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will explore exactly how to generate synthetic
    data using simulators and game engines.
  prefs: []
  type: TYPE_NORMAL
- en: Generating synthetic data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to generate synthetic data using modern game
    engines such as Unity and Unreal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate synthetic data with its corresponding ground truth, it is recommended
    that we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the task and ground truth to generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the 3D virtual world in the game engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the virtual camera.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add noise and anomalies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the labeling pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the training data with the ground truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Throughout this section, we will thoroughly discuss each facet of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Identify the task and ground truth to generate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step in the synthetic data generation process is defining the task,
    the type of the data, and the ground truth to generate. For example, the data
    could be images, videos, or audio. At the same time, you need to identify what
    ground truth to generate for your problem. For example, you can generate semantic
    segmentation, instance segmentation, depth maps, normal maps, human poses, and
    human body parts semantic segmentation, just to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we need to understand how to create the 3D virtual world, which we will
    explore in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Create the 3D virtual world in the game engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, we must define the environment, its elements, and the interactions
    between these elements. You may need to decide on the level of photorealism, the
    degree of visual complexity, and the range of variations and diversity that you
    need to attain for your virtual scenes, and thus the synthetic data. In general,
    and for a typical plan to generate synthetic data, we can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparation and conceptualization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Materialization and texturing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Integration into the game engine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Polishing and testing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will delve into each of these aspects and provide deeper insight.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation and conceptualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before creating the 3D virtual world, we need to examine our ideas about the
    virtual world to be created. It is suggested to make simple drawings and sketches
    to visualize the elements of the world and how they will interact with each other.
    You may need to jot down the following: weather conditions to simulate, whether
    an indoor or outdoor environment, the time of day, and the scenes’ crowdedness,
    just to mention a few. Additionally, you need to decide which game engine to use,
    for instance, **Unity**, **Unreal**, or **CryEngine**. You also need to decide
    which rendering pipeline to utilize, which depends on the game engine itself.
    For example, the Unity game engine has different rendering pipelines, such as
    **Built-in Render Pipeline (BRP)**, **High-Definition Render Pipeline (HDRP)**,
    **Universal Render Pipeline (URP)**, and **Scriptable Render Pipeline (SRP)**.
    The selection of the rendering pipeline also depends on the degree of photorealism
    that you want to achieve. Moreover, some game engines may support various programming
    languages, such as CryEngine, which supports C++ and C#. Thus, you may need to
    decide which language to use as well.'
  prefs: []
  type: TYPE_NORMAL
- en: After this, we need to determine the assets to use, such as objects, materials,
    visual effects, and sound effects. At the same time, you may need to consider
    the budget, timeframe, and the skills of your team.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The next step, after creating a solid idea about the 3D virtual world, is to
    start the modeling stage. **3D modeling** is the process of creating 3D objects
    using appropriate modeling software. 3D modeling is widely used in the game and
    entertainment industries, engineering fields, and architecture. To build the 3D
    virtual world, we need to create its elements, such as buildings, trees, pedestrians,
    and vehicles. Thus, we need to decide whether to import or model these elements.
    We can do 3D modeling using software such as *Blender* ([https://www.blender.org](https://www.blender.org)),
    *ZBrush* ([https://pixologic.com](https://pixologic.com)), and *3ds Max* ([https://www.autodesk.co.uk/products/3ds-max](https://www.autodesk.co.uk/products/3ds-max)).
    As you may expect, a straightforward solution is importing these elements from
    websites such as *Adobe 3D Substance* ([https://substance3d.adobe.com](https://substance3d.adobe.com))
    and *Turbosquid* ([https://www.turbosquid.com](https://www.turbosquid.com)). However,
    high-quality 3D models are usually expensive. Additionally, it should be noted
    that modeling complex 3D objects can be a challenging and time-consuming process
    that requires technical skills, effort, and time, but that depends on the object
    being modeled and technical constraints such as the polygon count.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – An example of a 3D car model (right) created from a car sketch
    (left)](img/Figure_06_03_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – An example of a 3D car model (right) created from a car sketch
    (left)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.3* shows an example of the output that we get after the modeling
    stage, which is a car in this instance.'
  prefs: []
  type: TYPE_NORMAL
- en: Materialization and texturing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After creating a 3D model or a mesh, we need to add the physical properties
    of this object, such as the color, transparency, and reflectivity. These properties
    simulate the matter and the surface of the objects. On the other hand, texturing
    is used to simulate surface details such as scratches and patterns and to give
    the object a non-uniform appearance similar to the object’s appearance in the
    real world. In most game engines, this information is encoded using a texture
    map. *Figure 6**.4* shows a 3D object after the materialization and texturing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – A 3D object before (left) and after (right) materialization
    and texturing](img/Figure_06_04_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – A 3D object before (left) and after (right) materialization and
    texturing
  prefs: []
  type: TYPE_NORMAL
- en: Integration into the game engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When the elements of the 3D virtual world are ready, we need to add them to
    our scene. We also need to configure and set up lighting and a virtual camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lighting**: This is an essential step for creating photorealistic scenes.
    Lights are added to the virtual worlds to give a sense of depth and atmosphere.
    There are usually two options for lighting: **pre-rendered lighting** using lightmaps
    and **real-time lighting**. Lighting is fundamental but it is expensive computation-wise.
    Thus, you should pay attention to this step to achieve your target photorealism
    and frame rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Virtual camera**: Once the virtual world is generated, a virtual camera is
    utilized to capture the required synthetic data. The behavior of the camera is
    usually controlled using a script. The camera parameters and behaviors can be
    configured to match the real-world scenario and to achieve the intended behavior.
    Camera parameters include **Field of View (FoV)**, **Depth of Field (DoF)**, **Sensor
    Size**, and **Lens Shift**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polishing and testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step is to examine the generated synthetic data and iterate on the
    virtual world design. In this stage, you can fix bugs and optimize the performance.
    As expected, creating a 3D virtual world is not a simple process. It requires
    effort, time, and technical skills. However, once the virtual world is created,
    it can be leveraged to generate large-scale synthetic datasets for enormous applications.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the virtual camera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the virtual world, the camera plays a vital role in the synthetic data generation
    process. It represents the observer, and it is usually utilized to capture images,
    audio, and videos. The captured data may be used for training and testing ML models.
  prefs: []
  type: TYPE_NORMAL
- en: As we have mentioned previously, camera properties and attributes can be customized
    and configured to achieve the target behavior. For example, the camera FoV controls
    how much of the world your observer agent can perceive, and therefore, how much
    visual information you can capture in a single generated image. *Figure 6**.5*
    shows two images generated with different FoV values.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – A scene captured using two different FoVs in the Unreal game
    engine](img/Figure_06_05_B18494.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – A scene captured using two different FoVs in the Unreal game engine
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the camera position is fixed, and we only changed the FoV.
    Additionally, we can control the camera motion and transition to achieve the required
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The camera can take different setups in the 3D virtual world to imitate the
    relevant ones in the real world. These are some examples of camera setups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed camera**: The camera does not change its location or orientation while
    it captures the scene. This camera setup can be used to record the scene from
    a specific viewpoint. It is the simplest setup; it is easy to implement, and it
    does not require scripting. However, you need to pay attention to the position
    and the attributes of the camera. Otherwise, dynamic objects may accidentally
    block the view of the camera. In simulators and game engines, a fixed camera can
    be used, for instance, to imitate a traffic monitoring camera or a fixed camera
    used in sports broadcasting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PTZ camera**: This is a special type of camera setup in which the camera
    can pan, tilt, and zoom. In the real world, this type is usually controlled by
    an operator to capture an object of interest or a specific event. Thus, the camera
    can change its orientation and FoV to realize that. In a virtual world, the camera
    can be programmed to achieve that, or it can be controlled by a human operator
    during simulation or the synthetic data generation process. This setup gives you
    more freedom to capture the scene. However, it may require scripting to achieve
    the intended camera behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First-person camera**: First-person vision is a fundamental field in computer
    vision. This camera setup simulates an agent observing the world by wearing a
    camera. It has enormous applications in gaming and virtual reality, law enforcement,
    medicine, and education. For example, an ML model trained on first-person data
    can be used to assist surgeons and improve training, decision-making, performance,
    and accuracy. For a detailed discussion, refer to *Artificial Intelligence for
    Intraoperative Guidance: Using Semantic Segmentation to Identify Surgical Anatomy
    During Laparoscopic* *Cholecystectomy* ([https://pubmed.ncbi.nlm.nih.gov/33196488](https://pubmed.ncbi.nlm.nih.gov/33196488)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aerial or UAV camera**: This camera setup is key for flight and drone simulators.
    It simulates a camera mounted on a drone or UAV. Usually, it is used to simulate
    a birds-eye view of the scene. It has a wide spectrum of applications and can
    be used to enhance the performance of ML models that require training images captured
    using drones. It supports various computer vision tasks, such as object detection,
    classification, tracking, and recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stereoscopic camera**: A stereo camera is a special type of camera with two
    lenses separated by a short distance that can be leveraged to simulate how humans
    perceive depth. The distance between the two lenses is called **intra-ocular distance**
    and it is usually similar to the distance between a human’s eyes: approximately
    6.35 cm. This distance is essential for creating a sense of depth in the vision
    system. This type of camera is important for VR and immersive 3D experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking camera**: This type is used to track an object of interest. In the
    virtual world, this camera can be programmed to follow the desired object, which
    facilitates creating large-scale synthetic data focused on a target object. For
    example, it is possible to track a human in the virtual world for an action recognition
    task. This will help you to generate large-scale training data focused on your
    subject (human). It is possible to use other camera setups, but you will end up
    having many videos with no actions or where your object of interest is not apparent.
    Additionally, you may use this camera setup for visual object tracking and other
    similar tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is adding noise.
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise and anomalies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The real world is not perfect and has anomalies. In the context of image generation,
    noise and anomalies refer to a deviation from the main pattern, process, and phenomenon.
    For example, when we observe street light poles at night, a small portion of them
    may have been accidentally turned off, the light may be flickering, the pole may
    be slightly rotated, painted a different color, or have different dimensions.
    Adding noise and anomalies to the attributes and behaviors of virtual world elements
    improves realism and boosts the usability of the generated synthetic data. Another
    example regarding anomalies in behavior can be seen when observing pedestrians
    crossing the road. The majority wait for the green light or walk signal, look
    both ways, and cross on the crosswalk. On the other hand, a small portion may
    cross the road when the red light is on or may cross without paying attention
    to oncoming cars. This behavior anomaly, for example, should be simulated in the
    virtual world to ensure that the generated training data is diverse. Thus, training
    an ML model on this data will ensure a robust ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the labeling pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The labeling pipeline depends on your problem and how you plan to utilize the
    synthetic data. You may want to just generate the training data because it is
    too expensive in the real world, and you may prefer to ask human annotators to
    annotate your data. On the other hand, it is possible that you want to automate
    the annotation process in the simulator or the rendering engine. Simulators such
    as CARLA, NOVA, and Silver support generating the data with its corresponding
    ground truth for various computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the training data with the ground truth
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, the synthetic data generation pipeline should be ready. The previous
    steps can be challenging, costly, and time-consuming. However, they are only required
    to set up the system. Following this, you can leverage the system to generate
    your task-specific, automatically annotated, large-scale datasets. Changing the
    annotating protocol is simple and is not expensive compared to real-world datasets.
    Please note that we have not provided hands-on examples on how to generate synthetic
    data using game engines or simulators because the focus of the book is not on
    the implementation and coding of synthetic data generation approaches. However,
    it is committed to the theoretical, conceptual, and design aspects of the process.
    For more details about the implementation and coding aspects, please refer to
    Unity Computer Vision ([https://unity.com/products/computer-vision](https://unity.com/products/computer-vision))
    and Synthetic for Computer Vision ([https://github.com/unrealcv/synthetic-computer-vision](https://github.com/unrealcv/synthetic-computer-vision)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about the main limitations of deploying
    this synthetic data generation approach.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will highlight the main challenges in using this approach
    for synthetic data generation. We will look at realism, diversity, and complexity
    issues that present some difficulties in utilizing this approach for synthetic
    data generation.
  prefs: []
  type: TYPE_NORMAL
- en: Realism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **domain gap** between synthetic and real data is one of the main issues
    that limit the usability of synthetic data. For synthetic data to be useful, it
    should mimic the distribution and statistical characteristics of its real counterparts.
    Thus, for computer vision problems, we need to ensure a high degree of photorealism,
    otherwise, ML models trained on synthetic data may not generalize well to real
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving a high degree of photorealism using game engines and simulators is
    not a simple task. Even with the help of contemporary game engines such as CryEngine,
    Unreal, and Unity, we need effort, skill, and time to create photorealistic scenes.
  prefs: []
  type: TYPE_NORMAL
- en: The three essential elements for approaching realism and thus mitigating the
    domain gap problem for synthetic data generated by game engines and simulators
    are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Photorealism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generating photorealistic images, for computer vision problems, is vital for
    training and testing ML models. Building photorealistic scenes requires the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High-quality assets**: The 3D models, textures, and materials should be detailed
    and realistic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lighting**: It is essential for rendering photorealistic scenes. You may
    need to use physically based rendering and physically based materials. Additionally,
    you need to use suitable light sources and carefully configure their parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Postprocessing**: Game engines such as Unreal and Unity support postprocessing
    effects to improve photorealism. For example, you can use these techniques to
    simulate motion blur, gloom, and depth of field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistic behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve this, we need to ensure realistic camera behavior. For example, the
    camera should not penetrate walls and its parameters should be close to real-world
    camera ones. Additionally, character animations should be realistic and emulate
    human body movement. Furthermore, scene element interactions should obey physics
    rules, for example, when objects collide.
  prefs: []
  type: TYPE_NORMAL
- en: Realistic distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Objects and attributes are not randomly distributed in the real world. Thus,
    when building virtual worlds, we need to pay attention to matching these distributions
    as well. For example, people walking near shopping malls may have a higher probability
    of carrying objects. At the same time, under certain weather conditions, specific
    actions and objects may become more frequent or less. For example, in rainy weather
    conditions, pedestrians may carry umbrellas.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The world around us is remarkably varied and contains a multitude of elements
    that come in diverse colors, shapes, and behaviors, and possess different properties.
    Attaining a diverse virtual world requires time and effort. The usability of synthetic
    data comes from its primary advantage of generating large-scale datasets for training
    ML models. If the data is not diverse enough, this will cause ML models to overfit
    limited scenarios and attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nonlinearity, interdependence, uncertainty, and the dynamic nature of the real
    world make creating a realistic virtual world rather a complex task. Creating
    and simulating a realistic environment requires approximations, simplifications,
    and generalizations. This is necessary because of the trade-off between realism
    and computational complexity. Building a realistic virtual world that captures
    all real-world properties, phenomena, and processes is simply not feasible, even
    with state-of-the-art software and hardware. However, we can still approach an
    acceptable level of realism with a careful understanding of the ML problem: what
    is essential and what is auxiliary for this particular application? For example,
    if we would like to generate synthetic data for a face recognition task, we may
    need to pay extra attention to simulating photorealistic faces as compared to
    other elements in the scene.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at two case studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will briefly discuss two well-known simulators for synthetic
    data generation, and comment on the potential of using these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: AirSim
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AirSim is an open source, cross-platform simulator developed by Microsoft using
    the Unreal game engine. It simulates drones and cars, opening the door for enormous
    applications in computer vision for DL and RL approaches for autonomous driving.
    Some of the key features of this simulator include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Various weather effects and conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIDAR and infrared sensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizable environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistic physics, environments, and sensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, AirSim can be leveraged to generate rich, large-scale, and high-quality
    synthetic data from various sensors. Researchers in ML can train their models
    to fuse the different data modalities to develop more robust autonomous driving
    algorithms. Additionally, AirSim provides automatically labeled synthetic data
    for depth estimation, semantic segmentation, and surface normal estimation tasks.
    For more information about this simulator, please refer to *AirSim* ([https://www.microsoft.com/en-us/AI/autonomous-systems-project-airsim](https://www.microsoft.com/en-us/AI/autonomous-systems-project-airsim)).
  prefs: []
  type: TYPE_NORMAL
- en: CARLA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CARLA is an open source simulator for autonomous driving. It was developed
    using the Unreal game engine by **Computer Vision Centre** (**CVC**), Intel, and
    Toyota. CARLA is a well-known simulator for synthetic data generation. It has
    a traffic manager system and users can configure several sensors, which include
    depth sensors, LIDARs, multiple cameras, and **Global Positioning System** (**GPS**).
    CARLA generates synthetic data for a number of computer vision tasks, such as
    semantic segmentation, depth estimation, object detection, and visual object tracking.
    In addition to generating automatically labeled and large-scale synthetic data,
    CARLA can be deployed to generate diverse traffic scenarios. Then, researchers
    can utilize the generated synthetic data to train more accurate and robust ML
    models on a myriad of driving scenarios. Please check the project’s *CARLA* web
    page ([https://carla.org](https://carla.org)) and the *CARLA: An Open Urban Driving
    Simulator* paper ([http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf](http://proceedings.mlr.press/v78/dosovitskiy17a/dosovitskiy17a.pdf))
    for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other simulators, such as Silver, AI Habitat, SynthCity, and
    IGibson. Creating a more realistic simulator, supporting more tasks, making the
    simulator easier to use, and the virtual environment more customizable are the
    main research directions in developing future synthetic data generators using
    game engines and simulators.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a well-known method for synthetic data generation
    based on simulators and rendering engines. We learned how to generate synthetic
    data. We highlighted the main challenges and we discussed AirSim and CARLA simulators
    as examples of this data generation approach. We have seen that by using simulators
    and game engines, we can generate large-scale, rich, and automatically annotated
    synthetic data for many applications. It reduces the cost and effort and provides
    an ideal solution for training robust ML models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about a new method for synthetic data generation
    using **Generative Adversarial** **Networks** (**GANs**).
  prefs: []
  type: TYPE_NORMAL
