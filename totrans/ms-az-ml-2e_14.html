<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer240">
			<h1 id="_idParaDest-179"><em class="italic"><a id="_idTextAnchor178"/>Chapter 11</em>: Hyperparameter Tuning and Automated Machine Learning</h1>
			<p>In the previous chapter, we learned how to train <strong class="bold">convolutional neural networks</strong> and complex <strong class="bold">deep neural networks</strong>. When training these models, we are often confronted with difficult choices in terms of the various parameters we should use, such as the number of layers, filter dimensions, the type and order of layers, regularization, batch size, learning rate, the number of epochs, and many more. And this is not only the case for DNNs – the same challenges arise when we need to select the correct preprocessing steps, features, models, and model parameters in statistical ML approaches.</p>
			<p>In this chapter, we will look at optimizing the training process to remove some of the non-optimal human choices in ML. This will help you train better models faster and more efficiently without manual intervention. First, we<a id="_idIndexMarker1323"/> will explore <strong class="bold">hyperparameter optimization</strong> (also called <strong class="bold">HyperDrive</strong> in Azure <a id="_idIndexMarker1324"/>Machine Learning), a standard technique for optimizing parameters in an ML process. By evaluating different sampling techniques for hyperparameter sampling, such as random sampling, grid sampling, and Bayesian optimization, you will learn how to efficiently trade model runtime for model performance.</p>
			<p>In the second half of this chapter, we will look at model optimization by automating the complete end-to-end ML training process using <strong class="bold">Automated Machine Learning</strong>. This process is also often referred to as <strong class="bold">AutoML</strong>. Using Automated Machine Learning, we can optimize preprocessing, feature engineering, model selection, hyperparameter tuning, and model stacking all in one abstract optimization pipeline.</p>
			<p>One benefit of Azure Machine Learning is that both parameter optimization (HyperDrive) and model optimization (Automated Machine Learning) are supported in the same generalized way. This means we can deploy both to an auto-scaling training cluster, store the best model or parameter combination on disk, and then deploy the best model to production without ever leaving our notebook environment.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Finding the optimal model parameters with HyperDrive</li>
				<li>Finding the optimal model with Automated Machine Learning</li>
			</ul>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create decision-tree based ensemble classifiers:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0</strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0</strong></li>
			</ul>
			<p>Similar to the previous chapters, you can run this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. However, all the scripts need to be scheduled in Azure Machine Learning training clusters.</p>
			<p>All the code examples in this chapter can be found in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter11</a>.</p>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Finding the optimal model parameters with HyperDrive</h1>
			<p>In ML, we typically deal with either <a id="_idIndexMarker1325"/>parametric or non-parametric models. Models represent the distribution of the training data to make <a id="_idIndexMarker1326"/>predictions for unseen data from the same distribution. While parametric models (such as linear regression, logistic regression, and neural networks) represent the training data distribution by using a learned set of parameters, non-parametric models describe the training data distribution through other traits, such as decision trees (all tree-based classifiers), training samples (k-nearest neighbors), or weighted training samples (support vector machine).</p>
			<p><strong class="bold">Parametric models</strong> such as linear or <a id="_idIndexMarker1327"/>logistic regression are typically defined by a constant number of parameters that are independent of the training data. These models make strong assumptions about the training data, so they often require fewer training samples. As a result, both training and inferencing are usually very fast.</p>
			<p>In <a id="_idIndexMarker1328"/>comparison, for <strong class="bold">non-parametric models</strong> such as decision trees or k-nearest neighbors, the number of traits usually increases with the number of training samples. While these models don't<a id="_idIndexMarker1329"/> assume anything about the training <a id="_idIndexMarker1330"/>data distribution, many training samples are required. This often leads to slow training and slow interference performance.</p>
			<p>The term <strong class="bold">hyperparameter</strong> refers to all the <a id="_idIndexMarker1331"/>parameters that are used to configure and tune the training process of parametric or non-parametric models. The following is a list of some typical hyperparameters in a neural network:</p>
			<ul>
				<li>The number of hidden layers</li>
				<li>The number of units per layer</li>
				<li>Batch size</li>
				<li>Filter dimensions</li>
				<li>Learning rate</li>
				<li>Regularization terms</li>
				<li>Dropout</li>
				<li>Loss metric</li>
			</ul>
			<p>The number of hyperparameters and parameter values for training a simple ML model is astonishing. Have you ever caught yourself manually tweaking a parameter in your training processes, such as the number of splits in a decision-based classifier or the number of units in a neural network classifier? If so, you are not alone! However, it's very important to accept that manually tweaking parameters requires deep expertise in the specific model or model configuration. However, we can't possibly be an expert in every type of statistical modeling, ML, and optimization to tune all the possible parameters manually. Given that the number of parameter choices is enormous, it is not feasible to try all possible combinations, so we need to find a better way to optimize them.</p>
			<p>Not only can we not possibly try all the distinct combinations of parameters manually, but in many cases, we also can't possibly predict the outcome of a tweak in a hyperparameter, even with <a id="_idIndexMarker1332"/>expert knowledge. In such scenarios, we can start looking at finding the optimal set of parameters automatically. This <a id="_idIndexMarker1333"/>process is called <strong class="bold">hyperparameter tuning</strong> or <strong class="bold">hyperparameter search</strong>.</p>
			<p>Hyperparameter tuning entails automatically testing a model's performance against different sets of hyperparameter combinations and ultimately choosing the best combination of <a id="_idIndexMarker1334"/>hyperparameters. The definition of the <em class="italic">best performance</em> depends on the chosen metric and validation method. For example, stratified-fold cross-validation with the f1-score <a id="_idIndexMarker1335"/>metric will yield a different set of optimal parameters than the accuracy metric with k-fold cross-validation.</p>
			<p>One reason why we are discussing hyperparameter tuning (and Automated Machine Learning later) in this book is that we have a competitive advantage from using elastic cloud computing infrastructure. While it is difficult to train hundreds of models sequentially on your laptop, it is easy to train thousands of models in parallel in the cloud using cheap auto-scaling compute. Using cheap cloud storage, we can also persist all potentially good models for later analysis. Many of the recent ML papers have shown that we can often achieve better results by using more compute power and/or better parameter choices.</p>
			<p>Before we begin tuning hyperparameters, we want to remind you of the importance of a baseline model. For many practical ML tasks, you should be able to achieve good performance using a single tree-based ensemble classifier or a pre-trained neural network with default parameters. If this is not the case, hyperparameter tuning won't magically output parameters for a top-performing best-in-class model. In this case, it would be better to go back to data preprocessing and feature engineering to build a better baseline model first, before tuning the batch sizes, the number of hidden units, or the number of trees.</p>
			<p>Another issue to avoid with hyperparameter tuning is overfitting and focusing on the wrong performance metric or validation method. As with any other optimization technique, hyperparameter tuning will yield the best parameter combination for a given loss function or metric. Therefore, it is essential to validate your loss function before starting hyperparameter tuning.</p>
			<p>As with most other techniques in ML, there are multiple ways to find the best hyperparameters for a model. The most popular techniques are <em class="italic">grid search</em>, <em class="italic">random search</em>, and <em class="italic">Bayesian optimization</em>. In this<a id="_idIndexMarker1336"/> chapter, we will investigate all three of them, discuss their strengths and <a id="_idIndexMarker1337"/>weaknesses, and experiment with practical examples.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor181"/>Sampling all possible parameter combinations using grid search</h2>
			<p><strong class="bold">Grid search</strong> (or <strong class="bold">grid sampling</strong>) is a popular<a id="_idIndexMarker1338"/> technique for finding the optimal hyperparameters from a parameter grid by testing every possible parameter combination of the multi-dimensional parameter grid. For every parameter (continuous or categorical), we need to define<a id="_idIndexMarker1339"/> all the values or value ranges that should be tested. Popular ML libraries provide tools to create these parameter grids efficiently.</p>
			<p>Two properties differentiate grid search from <a id="_idIndexMarker1340"/>other hyperparameter sampling methods:</p>
			<ul>
				<li>All parameter combinations are assumed to be independent of each other, which means they can be tested in parallel. Therefore, given a set of 100 possible parameter combinations, we can start 100 models to test all the combinations in parallel.</li>
				<li>By testing all possible parameter combinations, we can ensure that we search for a global optimum rather than a local optimum.</li>
			</ul>
			<p>Grid search works perfectly for smaller ML models with only a few hyperparameters but grows exponentially with every additional parameter because it adds a new dimension to the parameter grid.</p>
			<p>Let's look at how grid search can be implemented using Azure Machine Learning. In Azure Machine Learning, the hyperparameter tuning functionality lives in the <strong class="source-inline">hyperdrive</strong> package. Here is what we are going to do:</p>
			<ol>
				<li>Create a grid sampling configuration</li>
				<li>Define a primary metric to define the tuning goal</li>
				<li>Create a <strong class="source-inline">hyperdrive</strong> configuration</li>
				<li>Submit the <strong class="source-inline">hyperdrive</strong> configuration as an experiment to Azure Machine Learning</li>
			</ol>
			<p>Let's look at these steps in more detail:</p>
			<ol>
				<li value="1">First, we must create the grid sampling configuration by defining the parameter choices and ranges for grid sampling, as shown in the following code block:<p class="source-code">from azureml.train.hyperdrive import \ </p><p class="source-code">  GridParameterSampling</p><p class="source-code">from azureml.train.hyperdrive.parameter_expressions \ </p><p class="source-code">  import *</p><p class="source-code">grid_sampling = <strong class="bold">GridParameterSampling</strong>({</p><p class="source-code">    "--first-layer-neurons": choice(16, 32, 64, 128),</p><p class="source-code">    "--second-layer-neurons": choice(16, 32, 64, 128),</p><p class="source-code">    "--batch-size": choice(16, 32)</p><p class="source-code">})</p></li>
			</ol>
			<p>In the preceding <a id="_idIndexMarker1341"/>code, we defined a parameter grid using discrete parameter choices along three parameter dimensions – the number of neurons in the first layer, the number of neurons in the second layer, and the training batch size.</p>
			<ol>
				<li value="2">The parameter names are formatted as command-line arguments because they will be passed as arguments to the training script. So, we need to make sure that the training script can configure parameters via command-line arguments. The following code shows what this could look like in your training example:<p class="source-code">import argparse</p><p class="source-code">parser = argparse.ArgumentParser()</p><p class="source-code">parser.add_argument('--batch-size', type=int, </p><p class="source-code">  default=50)</p><p class="source-code">parser.add_argument('--epochs', type=int, default=30)</p><p class="source-code">parser.add_argument('--first-layer-neurons', type=int, </p><p class="source-code">  dest='n_hidden_1', default=100)</p><p class="source-code">parser.add_argument('--second-layer-neurons', </p><p class="source-code">  type=int, </p><p class="source-code">  dest='n_hidden_2', default=100)</p><p class="source-code">parser.add_argument('--learning-rate', type=float, </p><p class="source-code">  default=0.1)</p><p class="source-code">parser.add_argument('--momentum', type=float, </p><p class="source-code">  default=0.9)</p><p class="source-code">args = parser.parse_args()</p></li>
			</ol>
			<p>With grid sampling, we<a id="_idIndexMarker1342"/> can test all the possible combinations of these parameters. This will result in a total of 32 runs (<em class="italic">4 x 4 x 2</em>) that we could theoretically run in parallel, as the training runs, and the parameter configurations are independent of each other. In this case, the total number of required training runs is obvious as we are only using discrete parameter ranges. Later, we will see that this is not the case for random sampling and Bayesian optimization. For these other methods, we sample from a continuous distribution, so the number of training runs won't be bounded. We will also see that the number of parallel runs can affect the optimization process when parameter choices are not independent. So, let's appreciate the simplicity of the grid sampling solution for a small number of discrete parameters.</p>
			<ol>
				<li value="3">Next, we need to define a metric that measures the performance of each parameter combination. This metric can be any numeric value that is logged by the training script. Please note that this metric does not need to be the same as the loss function – it can be any measurement that you would like to use to compare different parameter pairs. Have a look at the following example. Here, we have decided to maximize the <strong class="source-inline">accuracy</strong> metric and defined the following parameters:<p class="source-code">from azureml.train.hyperdrive import PrimaryMetricGoal</p><p class="source-code">primary_metric_name = "<strong class="bold">accuracy</strong>"</p><p class="source-code">primary_metric_goal = <strong class="bold">PrimaryMetricGoal.MAXIMIZE</strong></p></li>
			</ol>
			<p>In the preceding code, we chose the <strong class="source-inline">accuracy</strong> metric, which is what we want to maximize. Here, you can see that we simply specified any metric name as a string. To use this metric to evaluate hyperparameter optimization runs, the training script needs to log a metric with this exact name. We saw this in the previous chapters, where we emitted metrics for an Azure Machine Learning run.</p>
			<ol>
				<li value="4">We must use the same metric name of <strong class="source-inline">primary_metric_name</strong> to define and log a metric that can be picked up by <strong class="source-inline">hyperdrive</strong> to evaluate the run in the training script:<p class="source-code">from azureml.core.run import Run</p><p class="source-code">run = Run.get_context()</p><p class="source-code">run.log("<strong class="bold">accuracy</strong>", float(val_accuracy))</p></li>
				<li>Before we continue, recall the <a id="_idIndexMarker1343"/>script run configuration from the previous chapters. Similar to the previous chapters, we must configure a CPU-based Azure Machine Learning training cluster defined as <strong class="source-inline">aml_cluster</strong> and an environment called <strong class="source-inline">tf_env</strong> containing all the relevant packages for running TensorFlow:<p class="source-code">src = ScriptRunConfig(source_directory="train", </p><p class="source-code">    script="train.py",</p><p class="source-code">    compute_target=aml_cluster,</p><p class="source-code">    environment=tf_env) </p></li>
				<li>Now, we can initialize the <strong class="source-inline">hyperdrive</strong> configuration, which consists of the estimator, the<a id="_idIndexMarker1344"/> sampling grid, the optimization metric, and the number of runs and concurrent runs:<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p><p class="source-code">hyperdrive_run_config = HyperDriveConfig(</p><p class="source-code">    run_config=src,</p><p class="source-code">    hyperparameter_sampling=grid_sampling, </p><p class="source-code">    primary_metric_name=primary_metric_name, </p><p class="source-code">    primary_metric_goal=primary_metric_goal,</p><p class="source-code">    max_total_runs=32,</p><p class="source-code">    max_concurrent_runs=4)</p></li>
			</ol>
			<p>In grid sampling, the number of runs should correspond with the number of possible parameter combinations. As it is a required attribute, we need to compute this value and pass it here. The maximum number of concurrent runs in grid sampling is limited only by the number of nodes in your Azure Machine Learning cluster. We are using a four-node cluster, so we have set the number to <strong class="source-inline">4</strong> to maximize concurrency.</p>
			<ol>
				<li value="7">Finally, we can submit the <strong class="source-inline">hyperdrive</strong> configuration to an experiment, which will execute all the concurrent child runs on the specified compute target:<p class="source-code">from azureml.core.experiment import Experiment</p><p class="source-code">experiment = Experiment(workspace, experiment_name)</p><p class="source-code">hyperdrive_run = experiment.submit(hyperdrive_run_config)</p><p class="source-code">print(hyperdrive_run.get_portal_url())</p></li>
			</ol>
			<p>The preceding snippet will kick off the<a id="_idIndexMarker1345"/> training process, build and register new Docker images if needed, initialize and scale up the nodes in the cluster, and finally run the training scripts on the cluster. Each script will be parameterized using a unique parameter combination from the sampling grid. The following screenshot shows the resulting experiment run. We can go to this page by clicking on the link that is returned from the preceding code snippet:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/B17928_11_01.jpg" alt="Figure 11.1 – Grid sampling overview " width="963" height="612"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 11.1 – Grid sampling overview</p>
			<p>Here, we can see the sampling policy's name, which is <strong class="bold">GRID</strong>, and the configured parameter space. These parameters will be applied as command-line arguments to the training script.</p>
			<p>As you may have guessed already, not everything is great when you must sample all the possible parameter combinations from a multi-dimensional grid. As the number of hyperparameters grows, so do the dimensions of the grid. And each dimension of parameters adds a magnitude of new parameter configurations that need to be tested. And don't forget that testing a parameter's configuration usually means performing training, cross-validation, and test set predictions on your model, which can take a significant number of resources.</p>
			<p>Imagine that you want to <a id="_idIndexMarker1346"/>search for the best parameter combination for five parameters with 10 different values for each parameter. Let's assume the following:</p>
			<ul>
				<li>We are testing 10<span class="superscript">5</span> (<em class="italic">10 x 10 x 10 x 10 x 10</em>) parameter combinations.</li>
				<li>One training run takes only 2 minutes.</li>
				<li>We are performing four-fold cross-validation.</li>
			</ul>
			<p>Here, we would end up with 555 days (<em class="italic">2min x 4 x 10^5 = 800,000min</em>) of combined training time. While you could decrease the total runtime by running parameter combinations in parallel, other methods exist that are better suited for large numbers of parameters, such as random sampling. Let's see how we can limit the required runtime of the parameter optimization search by sampling parameter configurations at random.</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor182"/>Testing random combinations using random search</h2>
			<p><strong class="bold">Random search</strong> is another popular<a id="_idIndexMarker1347"/> hyperparameter sampling method that's similar to grid search. The main difference is that instead of testing all the possible parameter combinations, only a few combinations are randomly selected and tested. The main idea is that grid search often samples nearby parameter configurations that have little effect on model performance. Therefore, we waste a lot of time chasing similarly bad solutions where we could use our time to try diverse and hopefully more successful parameter configurations.</p>
			<p>When you're dealing with large amounts of hyperparameters (for example, more than 5), random search will find a good set of hyperparameters much faster than grid search – however, it might not be the optimal result. Even so, in many cases, it will be a reasonable trade-off to use random search over grid search to improve prediction performance with hyperparameter tuning.</p>
			<p>In random search, parameters are usually sampled from a continuous distribution instead of discrete parameter choices being used. This leads to a slightly different way of defining the parameter grid. Instead of providing choices for distinct values, we can define a distribution function for each parameter to draw random values from a continuous range.</p>
			<p>Like grid search, all parameter<a id="_idIndexMarker1348"/> combinations are independent if they're drawn with replacement, which means they can be fully parallelized. If a parameter grid with 10,000 distinct parameter configurations is provided, we can run and test all the models in parallel.</p>
			<p>Let's look at random search in Azure Machine Learning:</p>
			<ol>
				<li value="1">As with all other hyperparameter optimization methods, we find the random sampling method in the <strong class="source-inline">hyperdrive</strong> package. As we discussed previously, we can now define probability distribution functions such as <strong class="source-inline">normal</strong> and <strong class="source-inline">uniform</strong> for each parameter instead of choosing only discrete parameters:<p class="source-code">from azureml.train.hyperdrive import \ </p><p class="source-code">  RandomParameterSampling</p><p class="source-code">from azureml.train.hyperdrive.parameter_expressions \</p><p class="source-code">   import *</p><p class="source-code">random_sampling = <strong class="bold">RandomParameterSampling</strong>({</p><p class="source-code">    "--learning-rate": normal(10, 3),</p><p class="source-code">    "--momentum": uniform(0.5, 1.0),</p><p class="source-code">    "--batch-size": choice(16, 32, 64)</p><p class="source-code">})</p></li>
			</ol>
			<p>Using continuous parameter ranges is not the only difference in random sampling. Due to the possibility of sampling an infinite amount of parameter configurations from a continuous range, we need a way to specify the duration of the search. We can use the <strong class="source-inline">max_total_runs</strong> and <strong class="source-inline">max_duration_minutes</strong> parameters to define the expected runtime in minutes or to limit the amount of sampled parameter configurations.</p>
			<ol>
				<li value="2">Let's test 25 different configurations and run the hyperparameter tuning process for a maximum of 60 minutes. We must set the following parameters:<p class="source-code">max_total_runs = 25</p><p class="source-code">max_duration_minutes = 60</p></li>
				<li>We will reuse the same metric that we defined in the previous section, namely <em class="italic">accuracy</em>. The <strong class="source-inline">hyperdrive</strong> configuration<a id="_idIndexMarker1349"/> looks as follows:<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p><p class="source-code">hyperdrive_run_config = HyperDriveConfig(</p><p class="source-code">    run_config=src,</p><p class="source-code">    hyperparameter_sampling=random_sampling, </p><p class="source-code">    primary_metric_name=primary_metric_name, </p><p class="source-code">    primary_metric_goal=primary_metric_goal,</p><p class="source-code">    max_total_runs=max_total_runs,</p><p class="source-code">    max_duration_minutes=max_duration_minutes)</p></li>
				<li>Similar to the previous example, we must submit the <strong class="source-inline">hyperdrive</strong> configuration to Azure Machine Learning from the authoring runtime, which will schedule all the optimization runs on the compute target:<p class="source-code">from azureml.core.experiment import Experiment</p><p class="source-code">experiment = Experiment(workspace, experiment_name)</p><p class="source-code">hyperdrive_run = experiment.submit(hyperdrive_run_config)</p><p class="source-code">print(hyperdrive_run.get_portal_url())</p></li>
			</ol>
			<p>Random sampling is an excellent choice for testing large numbers of tunable hyperparameters or sampling values from a continuous range. However, instead of optimizing the parameter configurations step by step, we simply try all those configurations at random and compare how they perform.</p>
			<p>In the next section, we will learn <a id="_idIndexMarker1350"/>how to find a good parameter combination faster by stopping training runs early. In the subsequent section, <em class="italic">Optimizing parameter choices using Bayesian optimization</em>, we will look at a more elegant way of navigating through the parameter space in hyperparameter tuning by using optimization.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor183"/>Converging faster using early termination</h2>
			<p>Both the grid and random <a id="_idIndexMarker1351"/>sampling techniques test models for poor parameter choices and hence spend precious compute resources on fitting poorly parameterized models to your training data. <strong class="bold">Early termination</strong> is a technique that stops a training run early if the intermediate results look worse than other runs. It is a great solution for speeding up expensive hyperparameter optimization techniques.</p>
			<p>In general, you should always try to use early termination when using either grid or random sampling. You get no benefit from training all the parameter combinations if the results are a lot worse than for some of the existing runs.</p>
			<p>Once we understand the idea of canceling poor-performing runs, we need to find a way to specify a threshold of when a run should be canceled – we refer to this threshold as the <strong class="bold">termination policy</strong>. Azure Machine Learning provides the most popular termination <a id="_idIndexMarker1352"/>policies, namely <strong class="bold">bandit</strong>, <strong class="bold">median stopping</strong>, and <strong class="bold">truncation selection</strong>. Let's take a look at them and see what their differences are.</p>
			<p>Before we get into the details, though, let's learn how to configure early termination. In Azure Machine Learning, we can parameterize the different early termination policies with two global properties, namely <strong class="source-inline">evaluation_interval</strong> and <strong class="source-inline">delay_evaluation</strong>. These parameters control how often the early termination policy is tested. An example of using these parameters are as follows:</p>
			<p class="source-code">evaluation_interval = 1</p>
			<p class="source-code">delay_evaluation = 10</p>
			<p>The units of both parameters <a id="_idIndexMarker1353"/>are in intervals. An <strong class="bold">interval</strong> is defined by the training code and corresponds to one invocation of <strong class="source-inline">run.log()</strong>. For example, when you're training a neural network, an interval will equal one training epoch. The <strong class="source-inline">delay_evaluation</strong> parameter <a id="_idIndexMarker1354"/>controls how many intervals we want to<a id="_idIndexMarker1355"/> wait after the start to test the early termination policy for the first time. In the preceding example, we configured it as <strong class="source-inline">10</strong>, so we wait for 10 epochs before testing the early termination policy.</p>
			<p>Then, every other policy evaluation is configured using the <strong class="source-inline">evaluation_interval</strong> parameter. It describes how many iterations need to pass until the next test. In the preceding example, we set <strong class="source-inline">evaluation_interval</strong> to <strong class="source-inline">1</strong>, which is also the default value. This means that we test the early termination policy every interval after the <strong class="source-inline">delay_evaluation</strong> interval – here, every 1 iteration. Let's look into the three termination policies in more detail.</p>
			<h3>The median stopping policy</h3>
			<p>Let's start with the easiest termination policy – the <strong class="bold">median stopping policy</strong>. It takes no other arguments than the <a id="_idIndexMarker1356"/>two default arguments, which<a id="_idIndexMarker1357"/> control when and how often the policy should be tested. The median stopping policy keeps track of the running average of the primary metric across all experiment runs. Whenever the median policy is evaluated, it will test whether the current metric is above the median of all running experiments and stop those runs that are below. The following code shows how to create a median stopping early termination policy for any hyperparameter tuning script:</p>
			<p class="source-code">from azureml.train.hyperdrive import MedianStoppingPolicy</p>
			<p class="source-code">early_termination_policy = <strong class="bold">MedianStoppingPolicy</strong>(</p>
			<p class="source-code">    evaluation_interval=evaluation_interval,</p>
			<p class="source-code">    delay_evaluation=delay_evaluation)</p>
			<p>As we can see, it's quite simple to construct a median stopping policy as it is only configured by the two default parameters. Due to its simplicity, it is a very effective method for reducing the runtime of your hyperparameter optimization script. The early termination policy is then applied to the <strong class="source-inline">hyperdrive</strong> configuration file using the <strong class="source-inline">policy</strong> parameter. Now, let's look at the truncation selection policy.</p>
			<h3>The truncation selection policy</h3>
			<p>Unlike the median stopping <a id="_idIndexMarker1358"/>policy, the <strong class="bold">truncation selection policy</strong> will always kill runs<a id="_idIndexMarker1359"/> when evaluated. It will kill a percentage of runs with the lowest primary metric. The percentage is defined using the <strong class="source-inline">truncation_percentage</strong> parameter:</p>
			<p class="source-code">truncation_percentage = 10</p>
			<p class="source-code">evaluation_interval = 5</p>
			<p class="source-code">delay_evaluation = 10</p>
			<p>In the preceding example, we set the <strong class="source-inline">truncation_percentage</strong> value to <strong class="source-inline">10</strong>. This means that whenever the early termination policy is executed, it will kill the lowest-performing 10% of runs. We must also increase the <strong class="source-inline">evaluation_interval</strong> value to <strong class="source-inline">5</strong> as we don't want to kill runs every epoch, as shown in the following example:</p>
			<p class="source-code">from azureml.train.hyperdrive import TruncationSelectionPolicy</p>
			<p class="source-code">early_termination_policy = <strong class="bold">TruncationSelectionPolicy</strong>(</p>
			<p class="source-code">    truncation_percentage=truncation_percentage,</p>
			<p class="source-code">    evaluation_interval=evaluation_interval,</p>
			<p class="source-code">    delay_evaluation=delay_evaluation)</p>
			<p>This early termination policy makes sense when only very few training resources are available, and we want to aggressively prune the number of runs each time the early termination policy is evaluated. Let's look at the final policy – the bandit policy.</p>
			<h3>The bandit policy</h3>
			<p>The <strong class="bold">bandit policy</strong> works similarly but inverse<a id="_idIndexMarker1360"/> to the truncation policy. Instead of stopping a percentage of the<a id="_idIndexMarker1361"/> lowest-performing runs, it kills all the runs that are worse than the best current run. In contrast to the previous policies, the bandit policy is not configured using a percentage value, but rather a <strong class="source-inline">slack_factor</strong> or <strong class="source-inline">slack_amount</strong> parameter. The <strong class="source-inline">slack_factor</strong> parameter describes the relative deviation from the best metric, whereas the <strong class="source-inline">slack_amount</strong> parameter describes the absolute deviation from the best primary metric.</p>
			<p>Let's look at an example. Here, we will configure <strong class="source-inline">hyperdrive</strong> by configuring a <strong class="source-inline">slack_factor</strong> parameter of <strong class="source-inline">0.2</strong> and testing an accuracy value (<em class="italic">bigger is better</em>). As we did previously, we will set the <strong class="source-inline">evaluation_interval</strong> value to <strong class="source-inline">5</strong> and the <strong class="source-inline">evaluation_delay</strong> value to <strong class="source-inline">10</strong> intervals:</p>
			<p class="source-code">slack_factor = 0.2</p>
			<p class="source-code">evaluation_interval = 5</p>
			<p class="source-code">delay_evaluation = 10</p>
			<p class="source-code">from azureml.train.hyperdrive import BanditPolicy</p>
			<p class="source-code">early_termination_policy = <strong class="bold">BanditPolicy</strong>(</p>
			<p class="source-code">    slack_factor = slack_factor,</p>
			<p class="source-code">    evaluation_interval=evaluation_interval,</p>
			<p class="source-code">    delay_evaluation=delay_evaluation)</p>
			<p>Let's say that the best-performing <a id="_idIndexMarker1362"/>run yields an accuracy of 0.8 after epoch 10, which is when the early termination policy gets applied for the first time. Now, all the runs that are performing up to 20% worse than the best metric are killed. We can <a id="_idIndexMarker1363"/>compute the relative deviation from an accuracy of 0.8 by using the following function: </p>
			<p><em class="italic">0.8/(1 + 0.2) = 0.67</em></p>
			<p>Hence, all the runs that yield a performance that's lower than 0.67 will get canceled by the early termination policy.</p>
			<h3>A HyperDrive configuration with the termination policy</h3>
			<p>To create a <strong class="source-inline">hyperdrive</strong> configuration, we <a id="_idIndexMarker1364"/>need to pass the early termination policy using the <strong class="source-inline">policy</strong> parameter. Here is an example of using grid search sampling and the previously defined bandit policy:</p>
			<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p>
			<p class="source-code">hyperdrive_run_config = HyperDriveConfig(</p>
			<p class="source-code">    run_config=src,</p>
			<p class="source-code">    hyperparameter_sampling=grid_sampling, </p>
			<p class="source-code">    <strong class="bold">policy=early_termination_policy</strong>,</p>
			<p class="source-code">    primary_metric_name="accuracy", </p>
			<p class="source-code">    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE)</p>
			<p>The bandit policy is a good trade-off between the median stopping and the truncation selection policy that works well in<a id="_idIndexMarker1365"/> many cases. You can rest assured that only a well-performing subset of all the hyperparameter configurations will be run and evaluated for multiple intervals.</p>
			<p>Let's submit this HyperDrive configuration as an experiment to Azure Machine Learning. We can use the <strong class="source-inline">RunDetails</strong> method that we saw in the previous chapters to output additional information about the hyperparameter tuning experiment, such as scheduling and parameter information, a visualization of the training performance, and a parallel coordinate chart showing the parameter dimensions:</p>
			<p class="source-code">from azureml.widgets import RunDetails</p>
			<p class="source-code">hyperdrive_run = exp.submit(hyperdrive_run_config)</p>
			<p class="source-code">RunDetails(hyperdrive_run).show()</p>
			<p>If you run the preceding code, it will run the hyperparameter search for the configured policies. Once the experiment is running, you will see the specified metric for the individual parameter combinations and iterations as a chart in a widget:</p>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/B17928_11_02.jpg" alt="Figure 11.2 – HyperDrive – the performance of runs " width="891" height="410"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – HyperDrive – the performance of runs</p>
			<p>Besides looking at the defined metric, you<a id="_idIndexMarker1366"/> can select other visualizations that show the sampled parameters, such as on a parallel coordinates plot, or as two- and three-dimensional scatter plots. Here, you can see which parameter combinations yield high model accuracy:</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/B17928_11_03.jpg" alt="Figure 11.3 – HyperDrive – visualization of the results " width="868" height="348"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – HyperDrive – visualization of the results</p>
			<p>In this section, you learned that applying an early termination policy to your hyperparameter optimization script is a simple but extremely effective way to reduce the number of poorly performing<a id="_idIndexMarker1367"/> training runs. With just a few lines of code, we can reduce the number of training runs to a minimum and only finish those that are yielding promising results.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">When you're using hyperparameter optimization with random or grid sampling, <em class="italic">always</em> use an early termination policy.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor184"/>Optimizing parameter choices using Bayesian optimization</h2>
			<p>In the previous examples, we<a id="_idIndexMarker1368"/> evaluated different parameter configurations sampled from a grid or at random without any optimization or strategic parameter choice. This had the benefit that all the configurations were independent and could be evaluated in parallel. However, imagine using an ML model to help us find the best parameter combination for a large multi-dimensional parameter space. That's exactly what <strong class="bold">Bayesian optimization</strong> does in the domain of hyperparameter tuning.</p>
			<p>The job of an optimization method is to find the optimal value (that is, a minimum or maximum) of a predefined objective function. In hyperparameter tuning, we are faced with a very similar problem: we want to find the parameter configuration that yields the best-predefined evaluation metric for an ML model.</p>
			<p>So, how does optimization work for hyperparameter search? First, we must define a hyperplane – a multi-dimensional grid where we can sample our parameter configurations. In the following diagram, we can see such a plane for two parameters along the <em class="italic">x</em> and <em class="italic">y</em> axes. The <em class="italic">z</em>-axis represents the performance of the model that is being tested using the parameters at this specific location:</p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/B17928_11_04.jpg" alt="Figure 11.4 – The Rastrigin function " width="1254" height="729"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The Rastrigin function</p>
			<p>The preceding diagram<a id="_idIndexMarker1369"/> shows the multi-dimensional Rastrigin function, as an example of something extremely hard to optimize. In hyperparameter tuning, we often face a similar problem in that finding the optimal solution is difficult – just like finding the global minimum in the Rastrigin function.</p>
			<p>Then, we must sample points from this plane and test the first (few) parameter configurations. We assume that the parameters are not independent and that the model will have similar performance when using similar nearby parameters. However, each evaluation only yields a noisy value of the true model performance. Using these assumptions, we can use <strong class="bold">Gaussian processes</strong> to combine<a id="_idIndexMarker1370"/> the model evaluations into a multi-variate continuous Gaussian. Next, we can compute the points for the highest expected improvements on this Gaussian. These points will yield new samples to test with our model.</p>
			<p>Luckily, we don't have to implement the algorithm ourselves, but many ML libraries provide a hyperparameter optimization <a id="_idIndexMarker1371"/>algorithm out of the box. In Azure Machine Learning, we can use the <strong class="bold">Bayesian sampling method</strong>, which helps us pick good parameter configurations to optimize the predefined metric.</p>
			<p>The parameter grid is defined similarly to the random sampling technique – that is, by using a continuous or discrete parameter space for all the parameter values, as shown in the following code block:</p>
			<p class="source-code">from azureml.train.hyperdrive import BayesianParameterSampling</p>
			<p class="source-code">from azureml.train.hyperdrive.parameter_expressions import *</p>
			<p class="source-code">bayesian_sampling = <strong class="bold">BayesianParameterSampling</strong>({</p>
			<p class="source-code">    "--learning-rate": normal(10, 3),</p>
			<p class="source-code">    "--momentum": uniform(0.5, 1.0),</p>
			<p class="source-code">    "--batch-size": choice(16, 32, 64)</p>
			<p class="source-code">})</p>
			<p>Before we continue, we <a id="_idIndexMarker1372"/>need to keep one thing in mind. The Bayesian sampling technique tries to predict well-performing parameter configurations based on the results of the previously tested parameters. This means that the parameter choices and runs are not independent anymore. We can't run all the experiments in parallel at the same time as we need the results of some experiments to sample new parameters. Therefore, we need to set an additional parameter to control how many training runs should run concurrently.</p>
			<p>We can do this using the <strong class="source-inline">max_concurrent_runs</strong> parameter. To let the Bayesian optimization technique converge, it is recommended to set this value to a small value, for example, in the range of 2-10. Let's set the value to 4 for this experiment and the number of total runs to 100. This means that we are using 25 iterations for the Bayesian optimization method, where we explore four parameter configurations concurrently at a time:</p>
			<p class="source-code">max_concurrent_runs = 4</p>
			<p class="source-code">max_total_runs = 100</p>
			<p>Let's kick off the experiment with Bayesian sampling:</p>
			<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p>
			<p class="source-code">from azureml.core.experiment import Experiment</p>
			<p class="source-code">hyperdrive_run_config = HyperDriveConfig(</p>
			<p class="source-code">    estimator=estimator,</p>
			<p class="source-code">    <strong class="bold">hyperparameter_sampling=bayesian_sampling</strong>, </p>
			<p class="source-code">    primary_metric_name=primary_metric_name, </p>
			<p class="source-code">    primary_metric_goal=primary_metric_goal,</p>
			<p class="source-code">    max_total_runs=max_total_runs,</p>
			<p class="source-code">    max_concurrent_runs=max_concurrent_runs)</p>
			<p class="source-code">experiment = Experiment(workspace, experiment_name)</p>
			<p class="source-code">hyperdrive_run = experiment.submit(hyperdrive_run_config)</p>
			<p class="source-code">print(hyperdrive_run.get_portal_url())</p>
			<p>Unfortunately, this technique can't be parallelized further to finish faster as all the parameter choices are dependent<a id="_idIndexMarker1373"/> on the results of the previous iteration. However, due to the optimization step, it generally yields good results in a relatively short amount of time. </p>
			<p>Another downside of Bayesian optimization or optimization for hyperparameter tuning is that the optimization requires each result of each run with the defined parameter configuration to compute the new parameter choices. Therefore, we can't use early termination together with Bayesian sampling as the training would be stopped earlier, which means no accurate metric can be computed.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Early termination doesn't work for optimization techniques such as Bayesian optimization because it requires the final testing score to compute the parameter gradient.</p>
			<p>Once you've played <a id="_idIndexMarker1374"/>around with using ML to optimize an ML model, you may already think about taking it one step further: why should we stop at optimizing hyperparameters, and why shouldn't we optimize model choices, network structures, or model stacking altogether?</p>
			<p>And this is a perfectly valid thought. No human can test all the variations of different ML models, different parameter configurations, and different nested models together. In the next section, we will do exactly this and optimize not just parameters but also model architecture and preprocessing steps using Automated Machine Learning</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/>Finding the optimal model with Automated Machine Learning</h1>
			<p><strong class="bold">Automated Machine Learning</strong> is an exciting new <a id="_idIndexMarker1375"/>trend that many (if not all) cloud providers follow. The aim is to provide a service to users that automatically preprocesses your <a id="_idIndexMarker1376"/>data, selects an ML model, and trains and optimizes the model to fit your training data to optimize a specified error metric. This will create and train a fully automated end-to-end ML pipeline that only needs your labeled training data and target metric as input. Here is a list of steps that Automated Machine Learning optimizes for you:</p>
			<ul>
				<li>Data preprocessing</li>
				<li>Feature engineering</li>
				<li>Model selection</li>
				<li>Hyperparameter tuning</li>
				<li>Model ensembling</li>
			</ul>
			<p>While most experienced ML engineers or data scientists would be very cautious about the effectiveness of such an automated approach, it still has a ton of benefits, which will be explained in this section. If you like the idea of hyperparameter tuning, then you will find value in Automated Machine Learning.</p>
			<p>A good way to think about Automated Machine Learning is that it performs a hyperparameter search over the complete end-to-end ML pipeline, similar to Bayesian optimization, but over a much larger parameter space. The parameters are now individual steps in the end-to-end ML pipeline, which should be automated. The great thing about Automated Machine Learning is that instead of going through the dumb<a id="_idIndexMarker1377"/> sampling of all possible parameter choices, it will predict how well certain <a id="_idIndexMarker1378"/>preprocessing steps and models will perform on a dataset before actually training a model. This<a id="_idIndexMarker1379"/> process is called <strong class="bold">meta-learning</strong> and will help the optimization process yield great candidate solutions for the pipeline without spending time being evaluated.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor186"/>The unfair advantage of Automated Machine Learning</h2>
			<p>Let's evaluate the advantages of <a id="_idIndexMarker1380"/>Automated Machine Learning If we look at the list of automated steps we mentioned earlier, each one requires days for an experienced data scientist to explore, evaluate, and fine-tune. Even steps such as selecting the correct model, such as either LightGBM or XGBoost for gradient-based tree ensemble classification, are non-trivial as they require experience and knowledge of both tools. Moreover, we all know that those two are only a tiny subset of all the possible options for a classification model. If we look at hyperparameter tuning and model stacking, we can immediately tell that the amount of work that's required to build a great ensemble model is non-trivial.</p>
			<p>This is not only a problem of knowledge or expertise. It's also very time-consuming. Automated Machine Learning aims to replace manual steps with automated best practices, applying continuously improving rules, and heavily optimizing every possible human choice. It's very similar to hyperparameter tuning but for the complete end-to-end process. A machine will find the best parameters much faster and much more accurately than a human by using optimization instead of manual selection.</p>
			<p>We can also look at Automated Machine Learning from a different <a id="_idIndexMarker1381"/>perspective, namely as a <strong class="bold">machine learning as a service</strong> (<strong class="bold">MLaaS</strong>) product: data in, model (or prediction endpoint) out. By now, you should be aware that each step of building an end-to-end ML pipeline is a thorough, complicated, and time-consuming task. Even when you can choose the correct model and tuning parameters using Bayesian optimization, the cost of building this infrastructure and operating it is significant. In this case, choosing MLaaS would provide you with an ML infrastructure for a fraction of the usual cost.</p>
			<p>There is another reason why the idea of Automated Machine Learning is very interesting. It separates the ML part from your data-fitting problem and leaves you with what you should know best – the data. Similar to using a managed service in the cloud (for example, a managed database), which lets you focus on implementing business logic rather than operating infrastructure, Automated Machine Learning will allow you to use a managed ML pipeline built on best practices and optimization by using data instead of specific ML algorithms.</p>
			<p>This also leads to the reason why Automated Machine Learning is still a great fit for many (mature) companies – it reduces a prediction problem to the most important tasks:</p>
			<ul>
				<li>Data acquisition</li>
				<li>Data cleansing</li>
				<li>Data labeling</li>
				<li>Selecting an error metric</li>
			</ul>
			<p>We don't want to<a id="_idIndexMarker1382"/> judge anyone, but ML practitioners often like to skip these topics and dive right into the fun parts, namely feature engineering, model selection, parameterization, stacking, and tuning. Therefore, a good start for every ML project is to start with an Automated Machine Learning baseline model, because it will force you to focus only on the data side. After achieving a good initial score, you can always go ahead and start further feature engineering and build a model if needed.</p>
			<p>Now that we've talked about the Automated Machine Learning trend being reasonable and that you could benefit from it in one way or another, let's dive deep into some examples and code. We will look at the different capabilities of Azure Automated Machine Learning, a product of Azure Machine Learning, as applied in a standard end-to-end ML pipeline.</p>
			<p>Before we jump into the code, let's take a look at what problem Azure Automated Machine Learning can tackle. In general, we can decide between <em class="italic">classification</em>, <em class="italic">regression</em>, and <em class="italic">time series forecasting</em> in Automated Machine Learning As we know from the previous chapters, time series forecasting is simply a variant of regression, where all the predicted values are in the future.</p>
			<p>Hence, the most important task after choosing the correct ML task is choosing the proper error metric that should be optimized. The following list shows all the error metrics that are supported:</p>
			<ul>
				<li><strong class="bold">Classification</strong>: <strong class="source-inline">accuracy</strong>, <strong class="source-inline">AUC_weighted</strong>, <strong class="source-inline">average_precision_score_weighted</strong>, <strong class="source-inline">norm_macro_recall</strong>, and <strong class="source-inline">precision_score_weighted</strong></li>
				<li><strong class="bold">Regression and time series forecasting</strong>: <strong class="source-inline">spearman_correlation</strong>, <strong class="source-inline">normalized_root_mean_squared_error</strong>, <strong class="source-inline">r2_score</strong>, and <strong class="source-inline">normalized_mean_absolute_error</strong></li>
			</ul>
			<p>You should be familiar with most of these metrics as they are variants of the most popular error metrics for classification and regression.</p>
			<p>Among the supported models, there's LogisticRegression, SGD, MultinomialNaiveBayes, SVM, KNN, Random Forest, ExtremeRandomTrees, LigthtGBM, GradientBoosting, DNN, Lasso, Arima, Prophet, and more. The great thing about a managed service in the cloud is that this list will most likely grow in the future and add the most recent state-of-the-art models. However, this list should be thought of just as additional information for you, since the idea of Automated Machine Learning is that the models are <a id="_idIndexMarker1383"/>automatically chosen for you. However, according to the user's preference, individual models can be allow- or deny-listed for Automated Machine Learning. </p>
			<p>With all this in mind, let's look at a classification example that uses Automated Machine Learning</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor187"/>A classification example with Automated Machine Learning</h2>
			<p>When you're using <a id="_idIndexMarker1384"/>new technology, it's always good to take a step back and think about what the technology could be capable of. Let's use the same approach to figure out how automated preprocessing could help us in a typical ML project and where its limitations will be.</p>
			<p>Automated Machine Learning is great for applying best-practice transformations to your dataset: applying date/time transformations, as well as the normalization and standardization of your data when using linear regression, handling missing data or dropping low-variance features, and so on. A long list of features is provided by Microsoft that is expected to grow in the future.</p>
			<p>Let's recall what we learned in <a href="B17928_07_ePub.xhtml#_idTextAnchor112"><em class="italic">Chapter 7</em></a>, <em class="italic">Advanced Feature Extraction with NLP</em>. While Automated Machine Learning can detect free text and convert it into a numeric feature vector, it won't be able to understand the semantic meaning of the data in your business domain. Therefore, it will be able to transform your textual data, but if you need to semantically encode your text or categorical data, you have to implement that yourself.</p>
			<p>Another thing to remember is<a id="_idIndexMarker1385"/> that Automated Machine Learning will not try to infer any correlations between different feature dimensions in your training data. Hence, if you want to combine two categorical columns into a combined feature column (for example, using one-hot-encoding, mean embedding, and so on), then you will have to implement this on your own.</p>
			<p>In Automated Machine Learning there are two different sets of <a id="_idIndexMarker1386"/>preprocessors – the <strong class="bold">simple</strong> ones and the <strong class="bold">complex</strong> ones. Simple preprocessing is just referred to as <strong class="bold">preprocessing</strong>. The<a id="_idIndexMarker1387"/> following list shows all the simple preprocessing<a id="_idIndexMarker1388"/> techniques that will be evaluated during Automated Machine Learning training if the <strong class="source-inline">preprocess</strong> argument is specified. If you have worked with scikit-learn before, then most of the following preprocessing techniques should be fairly familiar to you:</p>
			<ul>
				<li><strong class="source-inline">StandardScaler</strong>: Normalization – mean subtraction and scaling a feature to unit variance.</li>
				<li><strong class="source-inline">MinMaxScaler</strong>: Normalization – scaling a feature by the minimum and maximum.</li>
				<li><strong class="source-inline">MaxAbsScaler</strong>: Normalization – scaling a feature by the maximum absolute value.</li>
				<li><strong class="source-inline">RobustScaler</strong>: Normalization – scaling a feature to the quantile range.</li>
				<li><strong class="source-inline">PCA</strong>: Linear dimensionality reduction<a id="_idIndexMarker1389"/> based on PCA.</li>
				<li><strong class="source-inline">TruncatedSVD</strong>: Linear dimensionality<a id="_idIndexMarker1390"/> reduction-based truncated <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>). Contrary to PCA, this estimator does not center the data beforehand.</li>
				<li><strong class="source-inline">SparseNormalizer</strong>: Normalization – each sample is normalized independently.</li>
			</ul>
			<p>Complex <a id="_idIndexMarker1391"/>preprocessing is referred to as <strong class="bold">featurization</strong>. These preprocessing steps are more complicated and apply various tasks during Automated Machine Learning optimization. As a user of Azure Automated Machine Learning, you can expect this list to grow and include new state-of-the-art transformations as they become available. The following list shows the various featurization steps:</p>
			<ul>
				<li><strong class="bold">Drop high cardinality or no variance features</strong>: Drops high cardinality (for example, hashes, IDs, or GUIDs) or <a id="_idIndexMarker1392"/>no variance (for example, all values missing or the same value across all rows) features.</li>
				<li><strong class="bold">Impute missing values</strong>: Imputes missing values <a id="_idIndexMarker1393"/>for numerical features (mean imputation) and categorical features (mode imputation).</li>
				<li><strong class="bold">Generate additional features</strong>: Generates additional features derived from date/time (for example, year, month, day, day of the week, day of the year, quarter, week of the year, hour, minute, and second) and text features (term frequency based on n-grams).</li>
				<li><strong class="bold">Transform and encode</strong>: Encodes categorical features using one-hot encoding (low cardinality) and one-hot-hash encoding (high cardinality). Transforms numeric features with few unique values into categorical features.</li>
				<li><strong class="bold">Word embeddings</strong>: Uses a pre-trained embedding model to convert text into aggregated feature vectors using mean embeddings.</li>
				<li><strong class="bold">Target encodings</strong>: Performs target encoding on categorical features.</li>
				<li><strong class="bold">Text target encoding</strong>: Performs target encoding on text features using a bag-of-words model.</li>
				<li><strong class="bold">Weight of evidence</strong>: Calculates the correlation of categorical columns to the target column through the weight of evidence and outputs a new feature per column per class.</li>
				<li><strong class="bold">Cluster distance</strong>: Trains a k-means clustering model on all the numerical columns and computes the distance of each feature to its centroid before outputting a new feature per column per cluster.</li>
			</ul>
			<p>Let's start with a simple Automated Machine Learning classification task that also uses preprocessing.</p>
			<p>We will start by defining a dictionary containing the Automated Machine Learning configuration. To enable standard preprocessing such as scaling, normalization, and PCA/SVD, we need to set the <strong class="source-inline">preprocess</strong> property to <strong class="source-inline">true</strong>. For <a id="_idIndexMarker1394"/>advanced preprocessing and feature engineering, we need to set the <strong class="source-inline">featurization</strong> property to <strong class="source-inline">auto</strong>. The following code block shows all these settings:</p>
			<p class="source-code">automl_settings = {</p>
			<p class="source-code">  "experiment_timeout_minutes": 15,</p>
			<p class="source-code">  "n_cross_validations": 3,</p>
			<p class="source-code">  "primary_metric": 'accuracy',</p>
			<p class="source-code">  "featurization": 'auto',</p>
			<p class="source-code">  "preprocess": True,</p>
			<p class="source-code">  "verbosity": logging.INFO,</p>
			<p class="source-code">}</p>
			<p>Using this configuration, we can now load a dataset using <strong class="source-inline">pandas</strong>. As shown in the following snippet, we are loading the <strong class="source-inline">titanic</strong> dataset and specifying the target column as a string. This column is required later for configuring Automated Machine Learning:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">df = pd.read_csv("train.csv")</p>
			<p class="source-code">target_column = "survival"</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">When you're using Automated Machine Learning and the local execution context, you can use a pandas DataFrame as the input source. However, when you execute the training process on a remote cluster, you need to wrap the data in an Azure Machine Learning dataset.</p>
			<p>Whenever we use a black-box classifier, we should also hold out a test set to verify the test performance of the model to validate generalization. Therefore, we must split the data into training and test sets:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">df_train, df_test = train_test_split(df, test_size=0.2)</p>
			<p>Finally, we can supply all the<a id="_idIndexMarker1395"/> required parameters to the Automated Machine Learning configuration constructor. In this example, we are using a local execution target to train the Automated Machine Learning experiment. However, we can also provide an Azure Machine Learning dataset and submit the experiment to our training cluster:</p>
			<p class="source-code">from azureml.train.automl import AutoMLConfig</p>
			<p class="source-code">automl_config = AutoMLConfig(</p>
			<p class="source-code">    task='classification',</p>
			<p class="source-code">    debug_log='debug.log',</p>
			<p class="source-code">    compute_target=aml_cluster,</p>
			<p class="source-code">    training_data=df_train,</p>
			<p class="source-code">    label_column_name=target_column,</p>
			<p class="source-code">    **automl_settings)</p>
			<p>Let's submit the Automated Machine Learning configuration as an experiment to the defined compute target and wait for completion. We can output the run details:</p>
			<p class="source-code">from azureml.widgets import RunDetails</p>
			<p class="source-code">automl_run = experiment.submit(automl_config,</p>
			<p class="source-code">    show_output=False)</p>
			<p class="source-code">RunDetails(automl_run).show()</p>
			<p>Similar to <strong class="source-inline">HyperDriveConfig</strong>, we can see that <strong class="source-inline">RunDetails</strong> for Automated Machine Learning shows a lot of useful information about your current experiment. Not only can you see all of your scheduled and running models, but you also get a nice visualization of the trained models and their training <a id="_idIndexMarker1396"/>performance. The following screenshot shows the accuracy of the first 14 runs of the Automated Machine Learning experiment:</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/B17928_11_05.jpg" alt="Figure 11.5 – Automated Machine Learning – visualization of the results " width="1650" height="576"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Automated Machine Learning – visualization of the results</p>
			<p>Finally, after 15 minutes, we can retrieve the best ML pipeline from the Automated Machine Learning run. From now on, we will refer to this pipeline simply as the <strong class="bold">model</strong>, as all the preprocessing steps are packed into the model, which itself is a pipeline of operations. We can use the following code to retrieve the pipeline:</p>
			<p class="source-code">best_run, best_model = remote_run.get_output()</p>
			<p>The resulting fitted pipeline (called <strong class="source-inline">best_model</strong>) can now be used exactly like a scikit-learn estimator. We can store it on disk, register it to the model store, deploy it to a <em class="italic">container instance</em>, or simply evaluate it on the test set. We will see this in more detail in <a href="B17928_14_ePub.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Model Deployment, Endpoints, and Operations</em>. Finally, we want to evaluate the best model. To do so, we will take the testing set that we separated from the dataset beforehand and predict the output on the fitted model:</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">y_test = df_test[target_column]</p>
			<p class="source-code">X_test = df_test.drop(target_column, axis=1)</p>
			<p class="source-code">y_pred = fitted_model.predict(X_test)</p>
			<p class="source-code">accuracy_score(y_test, y_pred)</p>
			<p>In the preceding code, we used the <strong class="source-inline">accuracy_score</strong> function from scikit-learn to compute the accuracy <a id="_idIndexMarker1397"/>of the final model. These steps are all you need to perform classification on a dataset using automatically preprocessed data and fitted models.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/>Summary</h1>
			<p>In this chapter, we introduced hyperparameter optimization through <strong class="bold">HyperDrive</strong> and model optimization through <strong class="bold">Automated Machine Learning</strong> Both techniques can help you efficiently retrieve the best model for your ML task.</p>
			<p><strong class="bold">Grid sampling</strong> works great with classical ML models, and also when the number of tunable parameters is fixed. All the values on a discrete parameter grid are evaluated. In <strong class="bold">random sampling</strong>, we can apply a continuous distribution for the parameter space and select as many parameter choices as we can fit into the configured training duration. Random sampling performs better on a large number of parameters. Both sampling techniques can/should be tuned using an <strong class="bold">early stopping criterion</strong>.</p>
			<p>Unlike random and grid sampling, <strong class="bold">Bayesian optimization</strong> probes the model performance to optimize the following parameter choices. This means that each set of parameter choices and the resulting model performance are used to compute the next best parameter choices. Therefore, Bayesian optimization uses ML to optimize parameter choices for your ML model. Since the underlying Gaussian process requires the resulting model performance, early stopping does not work with Bayesian optimization.</p>
			<p>We also learned that Automated Machine Learning is a generalization of Bayesian optimization on the complete end-to-end ML pipeline. Instead of choosing only hyperparameters, we also choose pre-processing, feature engineering, model selection, and model stacking methods and optimize those together. Automated Machine Learning speeds up this process by predicting which models will perform well on your data instead of blindly trying all possible combinations. Both techniques are essential for a great ML project; Automated Machine Learning lets you focus on the data and labeling first, while hyperparameter tuning lets you optimize a specific model.</p>
			<p>In the next chapter, we will look at training DNNs where the data or the model parameters don't fit into the memory of a single machine anymore, and therefore distributed learning is required.</p>
		</div>
	</div>
</div>
</body></html>