- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summary and Where to Go Next
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the last chapter of this book. We’ve learned a lot – starting with understanding
    the differences between traditional and machine learning-based software. We’ve
    learned how to handle data and how to work with algorithms. We’ve also looked
    at how to deploy models and how to work ethically with machine learning. In this
    chapter, we’ll summarize the best practices and try to get a glimpse of future
    developments in the area of machine learning overlapped with software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: To know where we are going, we need to know where we’ve been
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Current developments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: My view on the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "To know where we’re going, we need to know where \Lwe’ve been"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'My journey with computers started in the early 1990s, with Atari 800XL. Once
    I got my hands on that computer, I was amazed by the sheer fact that it could
    do what I told it to do. My first program was, of course, a program in BASIC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It’s neither well-formed nor a very useful program, but that was all I could
    do at the time. This first program shaped my entire career as it sparked my interest
    in software development. Later on, during my professional career, I realized that
    professional software engineering is much more than just writing source code and
    compiling it. The programs need to be well-formed, well-documented, well-designed,
    and well-tested (among many other things). This observation, in turn, shaped my
    view on software engineering as a discipline that can turn homebrewed software
    into a piece of art that can be used over long periods if well maintained. That
    was in the early 2000s. Around 2015, I saw a potential in machine learning that
    also sparked my interest. The first project that I took part in was done by a
    colleague of mine who showed me how a random forest classifier can interpret programming
    language code and make inferences about defects. Fast forward to today, we have
    transformers, diffusers, autoencoders, and hybrid networks that can do amazing
    things with data.
  prefs: []
  type: TYPE_NORMAL
- en: However, we need to move from AI development to AI engineering, as we did from
    software development to software engineering. That brings us to this book, where
    I showed over 70 best practices in AI engineering. This book has taken us from
    the basic concepts of machine learning and AI in *Part I*, through handling data
    in *Part II* to algorithms and models in *Part III*. We also explored the ethical
    and legal aspects of designing, using, and deploying machine learning systems
    in *Part IV*.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of this book contains significantly more best practices, which
    is because these best practices relate to engineering software, designing it,
    and making crucial decisions about machine learning – for example, the first best
    practice tells us when to use (and not to use) machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As this part of this book was about the *machine learning landscape in software
    engineering*, we’ll discuss different types of models and data and show how they
    come together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of best practices from the first part of this book is presented in
    *Table 17.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Best Practice** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Use machine learning algorithms when your problem is focused on data,
    not on the algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Before you start developing a machine learning system, do due diligence
    and identify the right group of algorithms to use. |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | If your software is safety-critical, make sure that you can design mechanisms
    to prevent hazards caused by the probabilistic nature of machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Test the machine learning software as an addition to the typical train-validation-evaluation
    process of machine learning model development. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | When designing machine learning software, focus on your data and the
    problem to solve first and on the algorithm second. |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Use data imputation only when you know which properties of data you need
    to strengthen. |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Once you have explored the problem to solve and understood the data available,
    decide whether you want to use supervised, self-supervised, unsupervised, or reinforcement
    learning algorithms. |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Choose the data validation attributes that are the most relevant for
    your system. |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Use GridSearch and other algorithms after you have explored the parameter
    search space manually. |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Always include monitoring mechanisms in your machine learning systems.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Choose the right database for your data – look at this from the perspective
    of the data, not the system. |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Use cloud infrastructure if you can as it saves resources and reduces
    the need for specialized competence. |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | Decide on your production environment early and align your process with
    that environment. |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | Design the entire software system based on the task that you need to
    solve, not only the machine learning model. |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | Downsize the size of your images and use as few colors as possible to
    reduce the computational complexity of your system. |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | Use a reference dataset for benchmarking whenever you can. |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | Whenever possible, use models that are already pre-trained for specific
    tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | Visualize your raw data to get an understanding of patterns in your
    data. |'
  prefs: []
  type: TYPE_TB
- en: '| 19 | Visualize your data when it has been turned into features to monitor
    if the same patterns are still observable. |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | Only use the necessary information as input to machine learning models.
    Too much information may require additional processing and pose unnecessary requirements
    on the system. |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | Use bounding boxes in the data when the task requires detecting and
    tracking objects. |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | Use semantic maps when you need to get the context of the image or you
    need details of a specific area. |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | Use a pre-trained embedding model such as GPT-3 or an existing BERT
    model to vectorize your text. |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | Use role labels when designing software that needs to provide grounded
    decisions. |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | Identify the origin of the data used in your software and create your
    data processing pipeline accordingly. |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | Extract as much data as you need and store it locally to reduce the
    disturbances for the software engineers who will use the tool for their work.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 27 | When accessing data from public repositories, please check the licenses
    and ensure you acknowledge the contribution of the community that created the
    analyzed code. |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | The best strategy to reduce the impact of noise on machine learning
    classifiers is to remove the noisy data points. |'
  prefs: []
  type: TYPE_TB
- en: '| 29 | Balance the number of features with the number of data points. Having
    more features is not always better. |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | Use `KNNImputer` for data for classification tasks and `IterativeImputer`
    for data for regression tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| 31 | Use the random forest classifier to reduce the attribute noise as it
    provides very good performance. |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | Retain the original distribution of the data as much as possible as
    it reflects the empirical observations. |'
  prefs: []
  type: TYPE_TB
- en: '| 33 | In large-scale software systems, if possible, rely on the machine learning
    models to handle noise in the data. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17.1: Best practices in the first part of this book'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of this book was about *data acquisition and management*. We
    focused on machine learning from the perspective of the data – how to get orientation
    in the data, what kind of data exists, and how to process it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the following best practices are related to data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Best Practice** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 34 | When working with numerical data, visualize it first, starting with
    the summary views of the data. |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | When visualizing data on the aggregate level, focus on the strength
    of relationships and connections between the values. |'
  prefs: []
  type: TYPE_TB
- en: '| 36 | Diving deeper into individual analyses should be guided by the machine
    learning task at hand. |'
  prefs: []
  type: TYPE_TB
- en: '| 37 | When visualizing the metadata for images, make sure you visualize the
    images themselves. |'
  prefs: []
  type: TYPE_TB
- en: '| 38 | Summary statistics for text data help with performing a sanity check
    of the data. |'
  prefs: []
  type: TYPE_TB
- en: '| 39 | Use feature engineering techniques if the data is complex but the task
    is simple – for example, creating a classification model. |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | Use PCA if the data is somehow linear and on similar scales. |'
  prefs: []
  type: TYPE_TB
- en: '| 41 | Use t-SNE if you do not know the properties of the data and the dataset
    is large (>1,000 data points). |'
  prefs: []
  type: TYPE_TB
- en: '| 42 | Use autoencoders for numerical data when the dataset is really large
    since autoencoders are complex and require a lot of data for training. |'
  prefs: []
  type: TYPE_TB
- en: '| 43 | Start with a small number of neurons in the bottleneck – usually one
    third of the number of columns. If the autoencoder does not learn, increase the
    number gradually. |'
  prefs: []
  type: TYPE_TB
- en: '| 44 | Use tokenizers for large language models such as BERT and word embeddings
    for simple tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | Use BoW tokenizers together with dictionaries when your task requires
    a pre-defined set of words. |'
  prefs: []
  type: TYPE_TB
- en: '| 46 | Use the WordPiece tokenizer as your first choice. |'
  prefs: []
  type: TYPE_TB
- en: '| 47 | Use BPE when you’re working with large language models and large corpora
    of text. |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | Use the sentence piece tokenizer when whitespaces are important. |'
  prefs: []
  type: TYPE_TB
- en: '| 49 | Use word embeddings (FastText) as a go-to model for designing classifiers
    or text. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17.2: Best practices in the second part of this book'
  prefs: []
  type: TYPE_NORMAL
- en: Once we worked with the data, we focused on algorithms, which comprise the other
    part of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the third part of this book, *Designing and Developing Machine Learning Systems*,
    our focus was on the software. We started by exploring which algorithms exist
    and how to choose the best ones. Starting from AutoML and working with Hugging
    Face provided us with a good platform to learn how to train and deploy machine
    learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are its best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Best Practice** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | Use AutoML as the initial choice when training the classical machine
    learning models. |'
  prefs: []
  type: TYPE_TB
- en: '| 51 | Use pre-trained models from Hugging Face or TensorFlow Hub to start
    with. |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | Work with the pre-trained networks to identify their limitations and
    then train the network on your dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| 53 | Instead of looking for more complex models, create a smarter pipeline.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 54 | If you want to understand your numerical data, use models that provide
    explainability, such as decision tree or random forest. |'
  prefs: []
  type: TYPE_TB
- en: '| 55 | The best models are those that capture the empirical phenomena in the
    data. |'
  prefs: []
  type: TYPE_TB
- en: '| 56 | Simple, but explainable, models can often capture the data in a good
    way. |'
  prefs: []
  type: TYPE_TB
- en: '| 57 | Always make sure that the data points in both the train and test sets
    are separate. |'
  prefs: []
  type: TYPE_TB
- en: '| 58 | Use NVidia CUDA (accelerated computing) for training advanced models
    such as BERT, GPT-3, and Autoencoders. |'
  prefs: []
  type: TYPE_TB
- en: '| 59 | In addition to monitoring the loss, make sure you visualize the actual
    results of the generation. |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | Check the output of generative AI models so that it does not break the
    entire system or does not provide unethical responses. |'
  prefs: []
  type: TYPE_TB
- en: '| 61 | The model card should contain information about how the model was trained,
    how to use it, which tasks it supports, and how to reference the model. |'
  prefs: []
  type: TYPE_TB
- en: '| 62 | Experiment with different models to find the best pipeline. |'
  prefs: []
  type: TYPE_TB
- en: '| 63 | Use a professional testing framework such as Pytest. |'
  prefs: []
  type: TYPE_TB
- en: '| 64 | Set up your test infrastructure based on your training data. |'
  prefs: []
  type: TYPE_TB
- en: '| 65 | Treat models as units and prepare unit tests for them accordingly. |'
  prefs: []
  type: TYPE_TB
- en: '| 66 | Identify the key aspects of the machine learning deployment and monitor
    these aspects accordingly. |'
  prefs: []
  type: TYPE_TB
- en: '| 67 | Focus on the user task when designing the user interface of the machine
    learning model. |'
  prefs: []
  type: TYPE_TB
- en: '| 68 | Prepare your models for web deployment. |'
  prefs: []
  type: TYPE_TB
- en: '| 69 | Try to work with in-memory databases and dump them to disk very often.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17.3: Best practices in the third part of this book'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last part of this book, *Ethical Aspects of Data Management and Machine
    Learning System Development*, we explored the challenges that relate to machine
    learning from the perspective of ethics and legal aspects. We offered certain
    solutions, but they do not replace human judgment and human intelligence. In the
    final chapter, we returned to a more technical part and taught you how to work
    with ecosystems, closing this book by opening up new alleys – microservices and
    Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Best Practice** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 70 | If your model/software aims to help with the daily tasks of your users,
    make sure that you develop it as an add-in. |'
  prefs: []
  type: TYPE_TB
- en: '| 71 | If you create your own data for non-commercial purposes, use one of
    the permissive derivative licenses that limit your liability. |'
  prefs: []
  type: TYPE_TB
- en: '| 72 | Limit yourself to studying source code and other artifacts, and only
    use personal data with the consent of the subjects. |'
  prefs: []
  type: TYPE_TB
- en: '| 73 | Any personal data should be stored behind authentication and access
    control to prevent malicious actors from accessing it. |'
  prefs: []
  type: TYPE_TB
- en: '| 74 | If a dataset contains variables that can be prone to bias, use the disparity
    metric to get a quick orientation about the data. |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | Complement automated bias management with regular audits. |'
  prefs: []
  type: TYPE_TB
- en: '| 76 | Use web services when deploying machine learning models to production.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 77 | Use both a website and a Python API for the web services. |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | Dockerize your web services for both version control and portability.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17.4: Best practices in the fourth part of this book'
  prefs: []
  type: TYPE_NORMAL
- en: The best practices presented in this book are based on my experiences in designing,
    developing, testing, and deploying machine learning systems. These best practices
    are not exhaustive, and we could find more, depending on our interests.
  prefs: []
  type: TYPE_NORMAL
- en: To look for these best practices, I strongly suggest taking a look at the research
    publications that appear continuously in this area. In particular, I recommend
    following the main conferences in AI – that is, NeurIPS and AAAI – as well as
    the main conferences in software engineering – that is, ICSE, ASE, and ESEC/FSE.
  prefs: []
  type: TYPE_NORMAL
- en: Current developments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing this book, the Technology Innovation Institute ([https://www.tii.ae/](https://www.tii.ae/))
    has just released its largest model – Falcon 170B. It is the largest fully open
    source model that is similar to the GPT-3.5 model. It shows the current direction
    of the research in large language models.
  prefs: []
  type: TYPE_NORMAL
- en: Although GPT-4 exists, which is larger by a factor of 1,000, we can develop
    very good software with moderately large models such as GPT-3.5\. This brings
    us to some of the current topics that we, as a community, need to discuss. One
    of them is the energy sustainability of these models. Falcon-170B requires 400
    GB of RAM (eight times that of an Nvidia A100 GPU) to execute (according to Hugging
    Face). We do not know how much hardware the GPT-4 model needs. The amount of electricity
    that it takes and the resources that it uses must be on par with what we get as
    value from that model.
  prefs: []
  type: TYPE_NORMAL
- en: We also approach limits to the conventional computational power when it comes
    to machine learning, which brings us to quantum computing (Cerezo, Verdon et al.
    2022). However, although the idea of embedding machine learning in quantum computing
    formalism is quite appealing, it is not without its challenges. Machine learning
    systems/models need to be re-designed to fit the new paradigm and the new paradigm
    needs to be more accessible for AI engineers and software engineers.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to algorithms, it is *swarming* that gets increasingly more attention,
    mostly due to the attention behind the **Internet of Things** (**IoT**) (Rosenberg
    2016, Rajeesh Kumar, Jaya Lakshmi, et al. 2023). With the support of technologies
    such as 5G and 6G in telecommunication, these developments are clearly on the
    rise and will continue to rise. However, one of the current challenges is fragmenting
    the protocols and technologies, which leads to challenges related to cybersecurity
    and attacks on critical infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another alley of research that is currently growing is the area of *graph
    neural networks* (Gao, Zheng, et al. 2023). As the current architectures are limited
    to linear data or data or data that can be linearized, they cannot handle certain
    tasks. Programming language models are one example of such an area – although
    they are capable of many tasks, they cannot consider execution graphs or similar
    elements at the moment. Therefore, the graph neural models are seen to be the
    next development, although the main challenge to address there is related to the
    computational power needed for training and inference in the graph neural models.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, increasingly more studies are tackling the development of
    models and softer aspects of it, such as ethics, explainability, and their impact
    on society at large (Meskó and Topol 2023). Medicine, large language models, and
    the digitalization of society are just a few areas where intensive standardization
    of the work is ongoing.
  prefs: []
  type: TYPE_NORMAL
- en: My view on the future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on my observations of the machine learning landscape today, with a particular
    focus on software engineering, I can see a few trends.
  prefs: []
  type: TYPE_NORMAL
- en: Language models will get better at completing software engineering tasks, such
    as requirements, testing, and documenting. This means that software engineers
    will be able to focus on their core work – engineering software – rather than
    on tedious, repetitive tasks. We will see models that will test software, document
    it, explain it, and maybe even repair it. The latest advancements in this field
    are very promising.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid models will be more popular. Combining symbolic analysis and neural networks
    will gain traction and be able to assist us in finding advanced vulnerabilities
    in software, as well as identifying them before they are exploited. This will
    make our software more robust and more resilient over time.
  prefs: []
  type: TYPE_NORMAL
- en: Large models and the availability of significant computational power will help
    us also to detect anomalies in software operations. By analyzing logs, we will
    be able to predict aging and we will be able to direct maintenance effort. Again,
    we will be able to focus on software engineering rather than on analysis.
  prefs: []
  type: TYPE_NORMAL
- en: And last, but not least, hybrid models will help us with design tasks. Combining
    image generation and other types of methods will help us in designing software
    – its architecture and detailed design, and even predicting the operational performance
    of the software.
  prefs: []
  type: TYPE_NORMAL
- en: Final remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope that this book was interesting for you and that you have gained new knowledge
    in machine learning, AI, and engineering software. I hope that you can use this
    book as a reference and that you will use the associated code to create new products.
    I would also greatly appreciate it if you could let me know if you liked it, connect
    via LinkedIn, and contribute to this book’s GitHub repository. I will monitor
    it and integrate all pull requests that you may have.
  prefs: []
  type: TYPE_NORMAL
- en: Before we part, I have one last best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #79'
  prefs: []
  type: TYPE_NORMAL
- en: Never stop learning.
  prefs: []
  type: TYPE_NORMAL
- en: Take this from a university professor. The field of machine learning grows quickly,
    with new models being introduced almost every week. Make sure that you observe
    the scientific publications in this area and commercial developments. This will
    help you keep your knowledge up-to-date and help you advance in your professional
    career.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Cerezo, M., G. Verdon, H.-Y. Huang, L. Cincio and P. J. Coles (2022). Challenges
    and opportunities in quantum machine learning. Nature Computational Science* *2(9):
    567-576.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Gao, C., Y. Zheng, N. Li, Y. Li, Y. Qin, J. Piao, Y. Quan, J. Chang, D. Jin
    and X. He (2023). A survey of graph neural networks for recommender systems: Challenges,
    methods, and directions**. ACM Transactions on Recommender Systems* *1(1): 1-51.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meskó, B. and E. J. Topol (2023). The imperative for regulatory oversight
    of large language models (or generative AI) in healthcare. npj Digital Medicine*
    *6(1): 120.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rajeesh Kumar, N. V., N. Jaya Lakshmi, B. Mallala and V. Jadhav (2023). Secure
    trust aware multi-objective routing protocol based on battle competitive swarm
    optimization in IoT. Artificial* *Intelligence Review.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rosenberg, L. (2016). Artificial Swarm Intelligence, a Human-in-the-loop approach
    to AI. Proceedings of the AAAI conference on* *artificial intelligence.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
