<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-83"><a id="_idTextAnchor087"/>5</h1>
<h1 id="_idParaDest-84"><a id="_idTextAnchor088"/>Non-Parametric Tests</h1>
<p>In the previous chapter, we discussed parametric tests. Parametric tests are useful when test assumptions are met. However, there are cases where those assumptions are not met. In this chapter, we will discuss several non-parametric alternatives to the parametric tests presented in the previous chapter. We start by introducing the concept of a non-parametric test. Then, we will discuss several non-parametric tests that can be used when t-test or z-test assumptions are not met.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>When parametric test assumptions are violated</li>
<li>The rank-sum test</li>
<li>The signed-rank test</li>
<li>The Kruskal-Wallis test</li>
<li>The chi-square test</li>
<li>Spearman’s correlation analysis</li>
<li>Chi-square power analysis</li>
</ul>
<h1 id="_idParaDest-85"><a id="_idTextAnchor089"/>When parametric test assumptions are violated</h1>
<p>In the<a id="_idIndexMarker426"/> previous chapter, we discussed parametric tests. Parametric tests have strong statistical power but also require adherence to strong assumptions. When the assumptions are not satisfied, the test results are not valid. Fortunately, we have alternative tests that can be used when the assumptions of a parametric test are not satisfied. These <a id="_idIndexMarker427"/>tests are called <strong class="bold">non-parametric</strong> tests, meaning <a id="_idIndexMarker428"/>that they make <em class="italic">no assumptions about the underlying distribution of the data</em>. While non-parametric tests do not require distributional assumptions, these <em class="italic">tests will still require the samples to </em><em class="italic">be independent</em>.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor090"/>Permutation tests</h2>
<p>For the first <a id="_idIndexMarker429"/>non-parametric test, let’s look more deeply at the definition of a p-value. A p-value is the <em class="italic">probability of obtaining a test statistic at least as extreme as the observed value</em> under the assumption of the null hypothesis. Then, to calculate a p-value, we need the null distribution and an observed statistic. The p-value is the proportion of samples with a test statistic more extreme than the observed statistic. It turns out that we can construct the null distribution using permutations from data. Let’s see how to construct the null distribution using the following dataset. This dataset could represent counts of machine failures at low and high temperatures. We assume that the samples are independent:</p>
<pre class="source-code">
low_temp = np.array([0, 0, 0, 0, 0, 1, 1])
high_temp = np.array([1, 2, 3, 1])</pre>
<p>To construct the null hypothesis, we first need to decide on a statistical measure. In this case, we will look for a difference in the mean of the two distributions. So, our statistical measure will be as follows:</p>
<p> _ x  lowtemp −  _ x  hightemp</p>
<p>Now, to calculate the distribution values, calculate the statistical measure for all permutations of the dataset. Here are a couple of examples of the permutations of the dataset.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Label</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Observed</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">P1</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">P2</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">P3</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">P4</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">P5</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">…</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>low</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>high</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>high</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>high</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>high</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>mean difference</p>
</td>
<td class="No-Table-Style">
<p>-1.46</p>
</td>
<td class="No-Table-Style">
<p>-0.68</p>
</td>
<td class="No-Table-Style">
<p>0.5</p>
</td>
<td class="No-Table-Style">
<p>0.89</p>
</td>
<td class="No-Table-Style">
<p>0.5</p>
</td>
<td class="No-Table-Style">
<p>0.11</p>
</td>
<td class="No-Table-Style">
<p>…</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – First five permutations of the observed data and the mean difference</p>
<p>The table shows the observed data with five randomly generated permutations of the values. We calculate the difference in the mean for each permutation. The differences in means are the values of the null distribution. Once we have the distribution, we can calculate the p-value as the proportion of values more extreme than the observed value.</p>
<p class="callout-heading">Scaling of permutation calculations</p>
<p class="callout">In general, permutation tests <a id="_idIndexMarker430"/>can be expensive to calculate because the number of permutations grows quickly with the size of the distribution. For example, dataset sizes of 3, 5, and 7 samples correspond to permutation sizes of 6, 120, and 5,040. The dataset shown here has more than 39 million permutations! The runtime performance of a permutation test on a large dataset will likely be slow due to the number of permutations necessary to compute the null distribution.</p>
<p>We can perform<a id="_idIndexMarker431"/> a permutation test in Python using the <code>permutation_test</code> function from <code>scipy</code>. This function calculates the test statistic, the null distribution, and the p-value:</p>
<pre class="source-code">
def statistic_function(set_one, set_two):
      return np.mean(set_one) - np.mean(set_two)
random_gen=42
perm_result = sp.stats.permutation_test(
      (low_temp, high_temp) ,
      statistic_function, random_state=random_gen,
)</pre>
<p>This function<a id="_idIndexMarker432"/> produces the following distribution from the dataset.</p>
<div><div><img alt="Figure 5.2 – Null distribution from the permutation test" height="469" src="img/B18253_05_002.jpg" width="657"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Null distribution from the permutation test</p>
<p>The null distribution from the permutation test and the observed difference in the mean of the two groups is shown in <em class="italic">Figure 5</em><em class="italic">.2</em>. The p-value from the permutation test is 0.036. The permutation test is the first of several non-parametric tests that will be covered in this section. Again, these types of tests are useful when the assumptions for parametric tests are not met. <em class="italic">However, if a parametric test can be used, it should be used</em>; non-parametric tests should not be used as default methods.</p>
<p>In this section, we introduced non-parametric tests with permutation tests. The permutation test is a widely applicable non-parametric test, but the computations for permutations grow quickly with the size of the dataset, which may make its use impractical in some situations. In the following sections, we will cover several other non-parametric tests that do not require computing a null distribution.</p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor091"/>The Rank-Sum test</h1>
<p>When the assumptions <a id="_idIndexMarker433"/>of the t-test are not met, the Rank-Sum test is often a good non-parametric alternative test. While the t-test can be used to test for the <em class="italic">difference between the means of two distributions</em>, the Rank-Sum test is used to test for the <em class="italic">difference between the locations of two distributions</em>. This difference in the test utility is due to the lack of parametric assumptions in the Rank-Sum test. The null hypothesis of the Rank-Sum test is that the distribution underlying the first sample is the same as the second sample. If the sample distributions appear to be similar, this allows us to use the Rank-Sum test to test for the difference in the locations of the two samples. As stated, the Rank-Sum test cannot specifically be used for testing the difference between means because it does not require assumptions about the sample distributions.</p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor092"/>The test statistic procedure</h2>
<p>The<a id="_idIndexMarker434"/> test procedure is straightforward. The process is outlined here and an example is shown in the following table:</p>
<ol>
<li>Combine all sample values into one set and sort the samples in ascending order, keeping track of their labels.</li>
<li>Assign ranks to all samples starting with rank 1 for the lowest sample value.</li>
<li>Where ties occur, replace the rank of the tied values with the mean rank of the tied values.</li>
<li>Sum the ranks for the smallest sample group, which is the test statistic T.</li>
</ol>
<p>Once the test statistic is calculated, the p-value can be calculated. The p-value can be done with a normal approximation or with an exact method. Generally, the exact method is only used when the sample size is small (less than 8 samples).</p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor093"/>Normal approximation</h2>
<p>Once <a id="_idIndexMarker435"/>the T statistic is calculated for the Rank-Sum test, we can determine a p-value with an exact method or with a normal approximation. We will cover the approximation method here as the exact method requires a <a id="_idIndexMarker436"/>permutation test, which will require the use of software. We approximate the p-value with a z-score (recall the z-score from <a href="B18945_03.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, </em><em class="italic">Hypothesis Testing</em>):</p>
<p>Z =  T − Mean(T) _ STDEV(T) </p>
<p>where</p>
<p>Mean(T) = n T _ R </p>
<p>and</p>
<p>STDEV(T) = s R √ _  n T n O _ n T + n O  </p>
<p>In the equations here, n T is the number of samples in the group used to calculate T, n O is the number of samples in the other group,  _ R  is the mean of the corrected ranks, and s Ris the standard deviation of the corrected ranks. Once Z is calculated, the corresponding p-value can be looked up for the z-distribution. Having described the method, let’s look at an example. The data shown in the following table can be found in the accompanying Jupyter notebook.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor094"/>Rank-Sum example</h2>
<p>The following table <a id="_idIndexMarker437"/>shows this process performed on a set of data for two groups labeled with “L” and “H,” where a Rank-Sum test will be used to test for the difference in location of the two sample distributions. The test statistic for this table is 42.5, which is the sum of the corrected ranks of group L. This test statistic corresponds to an approximate p-value of 0.00194 (a two-sided test). The data for this table was downloaded from <a href="https://github.com/OpenIntroStat/openintro/raw/master/data/gpa_iq.rda">https://github.com/OpenIntroStat/openintro/raw/master/data/gpa_iq.rda</a>.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">IQ</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Group</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Rank</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Corrected Rank</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>77</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>79</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>93</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>96</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>104</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>105</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>106</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>107</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>109</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">IQ</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Group</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Rank</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Corrected Rank</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>111</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>10</p>
</td>
<td class="No-Table-Style">
<p>10.5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>111</p>
</td>
<td class="No-Table-Style">
<p>L</p>
</td>
<td class="No-Table-Style">
<p>11</p>
</td>
<td class="No-Table-Style">
<p>10.5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>112</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>12</p>
</td>
<td class="No-Table-Style">
<p>12</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>116</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>13</p>
</td>
<td class="No-Table-Style">
<p>13</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>118</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>14</p>
</td>
<td class="No-Table-Style">
<p>14</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>124</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>15</p>
</td>
<td class="No-Table-Style">
<p>15</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>126</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>16</p>
</td>
<td class="No-Table-Style">
<p>16</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>127</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>17</p>
</td>
<td class="No-Table-Style">
<p>17</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>128</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>18</p>
</td>
<td class="No-Table-Style">
<p>18.5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>128</p>
</td>
<td class="No-Table-Style">
<p>H</p>
</td>
<td class="No-Table-Style">
<p>19</p>
</td>
<td class="No-Table-Style">
<p>18.5</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Corrected ranks for Rank-Sum test</p>
<p>We can <a id="_idIndexMarker438"/>also perform this test with software using <code>mannwhitenyu</code> from <code>scipy</code>. For the following code sample, the values corresponding to L and H from the preceding table are contained in the <code>lower_score_iqs</code> and <code>higher_score_iqs</code> variables, respectively:</p>
<pre class="source-code">
mannwhitneyu(higher_score_iqs, lower_score_iqs).pvalue
# 0.00222925216588146</pre>
<p>In this <a id="_idIndexMarker439"/>section, we discussed the Rank-Sum test, which is a non-parametric alternative to the t-test. The Rank-Sum test is used to test for a difference in the locations of two distributions of sample data. In the next section, we will look at a similar rank-based test, which is used to compare paired data like the paired t-test from <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, </em><em class="italic">Parametric Tests</em>.</p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor095"/>The Signed-Rank test</h1>
<p>The <a id="_idIndexMarker440"/>Wilcoxon Signed-Rank test <a id="_idIndexMarker441"/>is a non-parametric alternative version of the paired t-test that is used when the assumption of normality is violated. This test is robust to outliers because of the use of ranks and medians instead of means in the null and alternative hypotheses. As indicated by the name of the test, it uses the magnitudes of differences between two stages and their signs.</p>
<p>In research, a null hypothesis considers that the median difference between stage 1 and stage 2 is zero. Similarly, as in a paired t-test, for the alternative hypothesis, for a two-tailed test, the median difference between Stage 1 and Stage 2 is considered not to be zero, or for a one-tailed test, the median difference between Stage 1 and Stage 2 is greater (or less) than zero.</p>
<p>Though the normality requirement is relaxed, the test requires independence between paired observations and these observations to be from the same population. In addition, the dependent variable is required to be continuous.</p>
<p>To compute the test statistic, there are the following procedures:</p>
<ol>
<li>Calculate the differences in each pair between the two stages.</li>
<li>Drop pairs with zero difference, if they exist.</li>
<li>Take the absolute difference between each pair and rank them from smallest to largest.</li>
<li>Calculate the signed-rank statistic S by summing the ranks with a positive sign.</li>
</ol>
<p>Let us consider the simple example that follows. We consider generic data for two stages (before and after the treatment) of nine samples.</p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Pair</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Before </strong><strong class="bold">the treatment</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">After </strong><strong class="bold">the treatment</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Absolute </strong><strong class="bold">difference</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Sign</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Rank</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>37</p>
</td>
<td class="No-Table-Style">
<p>38</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>-</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>14</p>
</td>
<td class="No-Table-Style">
<p>17</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>-</p>
</td>
<td class="No-Table-Style">
<p>2.5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>22</p>
</td>
<td class="No-Table-Style">
<p>19</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>2.5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>12</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>24</p>
</td>
<td class="No-Table-Style">
<p>15</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>35</p>
</td>
<td class="No-Table-Style">
<p>25</p>
</td>
<td class="No-Table-Style">
<p>10</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>35</p>
</td>
<td class="No-Table-Style">
<p>24</p>
</td>
<td class="No-Table-Style">
<p>11</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>51</p>
</td>
<td class="No-Table-Style">
<p>38</p>
</td>
<td class="No-Table-Style">
<p>13</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p>39</p>
</td>
<td class="No-Table-Style">
<p>19</p>
</td>
<td class="No-Table-Style">
<p>20</p>
</td>
<td class="No-Table-Style">
<p>+</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Demonstration of rank calculation for sign-rank test</p>
<p>By observing the<a id="_idIndexMarker442"/> preceding table, we can see the second and the third pairs have the same absolute difference. Therefore, their ranks are computed by averaging their original rank, (2+3)/2 = 2.5. The null and alternative hypotheses for a one-sided test are as follows:</p>
<p>H 0 : The median difference between before and after treatment is zero</p>
<p>H a : The median difference between before treatment and after treatment is positive</p>
<p>The signed-rank statistic is the sum of ranks for positive differences and it is given as follows:</p>
<p>S = 2.5 + 4 + 5 + 6 + 7 + 8 + 9 = 41.5.</p>
<p>The mean of S is:</p>
<p>Mean(S) =  n(n + 1) _ 4  =  9.10 _ 4  = 22.5</p>
<p>And the standard deviation of S is:</p>
<p>SD(S) = √ ____________   n(n + 1)(2n + 1)  ____________ 24   = √ _  9 * 10 * 19 _ 24   = 8.44.</p>
<p>Then, the <a id="_idIndexMarker443"/>test statistic is calculated by the following formula:</p>
<p>Z statistic = S − Mean(S) _ SD(S)  = 41.5 − 22.5 _ 8.44  = 2.2511.</p>
<p>Referring to <a href="B18945_03.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Hypothesis Testing</em>, we could use <code>scipy.stats.norm.sf()</code> to calculate the approximate one-sided <code>p-value 0.012</code> using the Z statistic.</p>
<p>At α = 0.05 – level of significance – with <code>p-value &lt; </code>α, we reject the null hypothesis. There is strong evidence that the median before the treatment is greater than the median after the treatment. In Python, it is simple to implement the test as follows:</p>
<pre class="source-code">
import scipy.stats as stats
import numpy as np
before_treatment = np.array([37, 14, 22, 12, 24, 35, 35, 51, 39])
after_treatment = np.array([38,17, 19, 7, 15, 25, 24, 38, 19])
# Signed Rank Test
stats.wilcoxon(before_treatment, after_treatment, alternative = 'greater')</pre>
<p>The documentation for this test can be found at the following link:</p>
<p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.xhtml">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.xhtml</a></p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor096"/>The Kruskal-Wallis test</h1>
<p>Another <a id="_idIndexMarker444"/>non-parametric test we will now discuss is the Kruskal-Wallis test. It is an alternative to the one-way ANOVA test when the normality assumption is not satisfied. It uses the medians instead of the means to test whether there are statistically significant differences between two or more independent groups. Let us consider a generic example of three independent groups:</p>
<pre class="source-code">
group1 = [8, 13, 13, 15, 12, 10, 6, 15, 13, 9]
group2 = [16, 17, 14, 14, 15, 12, 9, 12, 11, 9]
group3 = [7, 8, 9, 9, 4, 15, 13, 9, 11, 9]</pre>
<p>The null and alternative hypotheses are stated as follows.</p>
<p>H 0 : The medians are equal among these three groups</p>
<p>H a : The medians are not equal among these three groups</p>
<p>In Python, it is easy to implement by using the <code>scipy.stats.kruskal</code> function. The documentation can be found at the following link:</p>
<p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.xhtml">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.xhtml</a></p>
<pre class="source-code">
from scipy import stats
group1 = [8, 13, 13, 15, 12, 10, 6, 15, 13, 9]
group2 = [16, 17, 14, 14, 15, 12, 9, 12, 11, 9]
group3 = [7, 8, 9, 9, 4, 15, 13, 9, 11, 9]
#Kruskal-Wallis Test
stats.kruskal(group1, group2, group3)</pre>
<p>The output of the preceding code is as follows:</p>
<p><code>KruskalResult(statistic=5.7342701722574905, pvalue=0.056861597028239855)</code></p>
<p>As α = 0.05 – the level of significance – with <code>p-value &gt; </code>α, we fail to reject the null hypothesis. There is no strong evidence to show that the medians are not equal across these three groups.</p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor097"/>Chi-square distribution</h1>
<p>Researchers <a id="_idIndexMarker445"/>are often faced with the need to test hypotheses on categorical data. The parametric tests covered in <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, Parametric Tests</em>, are often not very helpful for this type of analysis. In the last chapter, we discussed using an F-test to compare sample variances. Extending that concept, we can consider the non-parametric and non-symmetric chi-square probability distribution, which is a distribution useful for comparing the means of sampling distribution variances to their population variances, specifically when the mean of a sampling distribution of sample variances is expected to equal the population variance under the null hypothesis. Because variance cannot be negative, the distribution starts at an origin of 0. Here, we can see the <strong class="bold">chi-square distribution</strong>:</p>
<div><div><img alt="Figure 5.5 – Chi-square distribution with seven degrees of freedom" height="557" src="img/B18253_05_005.jpg" width="775"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Chi-square distribution with seven degrees of freedom</p>
<p>The shape of the chi-square distribution does not represent an assumption that percentiles are fixed to standard deviations, as with standard normal distribution; it is expected to change with each additional sample variance calculated. When the original population data from which the sampling distribution of variances is calculated can be assumed to be normally distributed, the chi-square standardized test statistic is calculated as:</p>
<p>χ 2 =  (n − 1) s 2 _ σ 2 </p>
<p>Where (n-1) is used <a id="_idIndexMarker446"/>to calculate the degrees of freedom, which are used to explain errors in sampling when building the test statistic. The critical values are based on table lookups using degrees of freedom and desired levels of significance. The null hypothesis is always H 0 : σ χ 2 = σ 0 2, where σ χ 2 is the observed distribution variance and σ 0 2 is the expected variance. The alternative hypothesis for a one-tailed test is H a : σ χ 2 ≥ σ 0 2 when testing the right tail and H a : σ χ 2 ≤ σ 0 2 when testing the left tail. The alternative hypothesis for a two-tailed test is H a : σ χ 2 ≠ σ 0 2. This test, when the population data can be assumed to be normally distributed, is similar to the F-test in that the two variances are being compared and the result is 1 when both are the same. The difference is that the chi-square test statistic factors in the degrees of freedom and is therefore less sensitive to differences.</p>
<p>Understanding how this test is useful for comparing variances is helpful for understanding how the chi-square distribution also relates to frequencies of occurrences of categorical data. In the preceding chi-square statistic, we can relate the sample statistic for variance as an observed value whereas the population parameter for variance is what we expect to occur. When determining whether there is statistical significance in the occurrence of categorical factor levels, we can <em class="italic">compare the observed occurrences to expected occurrences</em>. We will illustrate examples for arguably the two most widely used versions of this test, the <strong class="bold">chi-square goodness-of-fit test</strong> and the <strong class="bold">chi-square test of independence</strong>, in the following sections. These tests are considered non-parametric as they do not require the assumptions stated in the first section of <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, </em><em class="italic">Parametric Tests</em>.</p>
<p class="callout-heading">Tailedness of chi-square goodness-of-fit and independence tests</p>
<p class="callout">The chi-square goodness-of-fit test and chi-square test of independence in the following two sections are always right-tailed tests. The null hypothesis in these tests states a difference of zero between the observed and expected frequencies. The alternative hypothesis is that the observed and expected frequencies are not the same. The closer to 0 the χ 2 test statistic, the more likely the observed and expected frequencies approximate each other.</p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor098"/>Chi-square goodness-of-fit</h1>
<p>The <strong class="bold">chi-square goodness-of-fit test</strong> compares <a id="_idIndexMarker447"/>the count of occurrences of multiple factor levels for a single variable (factor) to determine whether the levels are statistically equal. For example, a vendor offers three models of phones – three levels (brands) of the single factor (phone) – to customers, who purchase in total an average of 90 phones per week. We can say the expected frequency is 1/3 – so, 30 phones of each model are sold per week, on average. Pearson’s chi-square test statistic, which is calculated by measuring the observed frequencies against expected frequencies, is the test statistic used for the chi-square goodness-of-fit test. The linear equation for this test statistic is as follows:</p>
<p>χ 2 = ∑ (O i − E i) 2 _ E i , degrees of freedom = k-1</p>
<p>Where O i is the observed frequency, E i, is the expected frequency, and <em class="italic">k</em> is the number of factor levels. Using our phone example, we learn from the vendor the expected frequency of 1/3 is not actually what is observed; the vendor sells an average of 45, 30, and 15 phones of models A, B, and C, respectively. Suppose we want to know whether this observed frequency differs with statistical significance from the expected frequency. The null hypothesis<a id="_idIndexMarker448"/> is that the frequencies are equal. We formulate the <strong class="bold">Pearson’s chi-square test statistic</strong> as follows:</p>
<p>χ2 =  (45 − 30) 2 _ 30  +  (30 − 30) 2 _ 30  +  (15 − 30) 2 _ 30  = 15</p>
<p>Suppose we want to use a hypothesis test (right-tailed) with a 0.05 level of significance, thus forming the alternative hypothesis that the observed and expected frequencies are not equal. The chi-square critical value table shows that for a level of significance of 0.05 with df : 3 − 1 = 2 degrees of freedom, the critical value is 5.9915. Because 15 &gt; 5.9915, we may conclude to reject the null hypothesis based on the critical value test.</p>
<p>To perform this operation in Python, we can use the <code>statsmodels.stats.gof</code> module’s <code>chisquare</code> function. Each group volume needs to be passed into a list or array, both for the observed frequencies and the expected frequencies. The <code>statsmodels</code> <code>chisquare</code> test will automatically calculate the k-1 degrees of freedom based on the counts of values in the observed (<code>f_obs</code>) list, but the degrees of freedom must be provided for the <code>scipy chi2.ppf</code> function, which provides the critical value, bypassing the need for a manual table lookup:</p>
<pre class="source-code">
from statsmodels.stats.gof import chisquare
from scipy.stats import chi2
chi_square_stat, p_value = chisquare(f_obs=[45, 30, 15],
    f_exp=[30, 30, 30])
chi_square_critical_value = chi2.ppf(1-.05, df=2)
print('Chi-Square Test Statistic: %.4f'%chi_square_stat)
print('Chi-Square Critical Value: %.4f'%chi_square_critical_value)
print('P-Value: %.4f'%p_value)</pre>
<p>As noted<a id="_idIndexMarker449"/> previously, since the p-value is below the 0.05 level of significance and the test statistic is larger than the critical value, we can assume we have a reasonable amount of evidence to reject the null hypothesis and conclude we have statistical significance indicating the phone models are not purchased in equal quantities:</p>
<p><code>Chi-Square Test </code><code>Statistic: 15.0000</code></p>
<p><code>Chi-Square Critical </code><code>Value: 5.9915</code></p>
<p><code>P-Value: 0.0006</code></p>
<p>We could then rerun the test for two frequencies at that point, comparing 45 to 30 or 45 to 15, for example, with expected frequencies of 37.5 and 37.5 or 30 and 30, respectively, to identify which phone may be the largest influencer on sales imbalance.</p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor099"/>Chi-square test of independence</h1>
<p>Suppose <a id="_idIndexMarker450"/>we have a dataset of observed vehicle crashes in the state of Texas in 2021, <em class="italic">Restraint Use by Injury Severity and Seat Position</em> (<a href="https://www.txdot.gov/data-maps/crash-reports-records/motor-vehicle-crash-statistics.xhtml">https://www.txdot.gov/data-maps/crash-reports-records/motor-vehicle-crash-statistics.xhtml</a>), and want to know whether using a seat belt resulted in a statistically significant difference in fatalities. We have the table of observed values as follows:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table004-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">Restrained</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Unrestrained</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Total</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Fatal</p>
</td>
<td class="No-Table-Style">
<p>1,429</p>
</td>
<td class="No-Table-Style">
<p>1,235</p>
</td>
<td class="No-Table-Style">
<p>2,664</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Not Fatal</p>
</td>
<td class="No-Table-Style">
<p>1,216,934</p>
</td>
<td class="No-Table-Style">
<p>22,663</p>
</td>
<td class="No-Table-Style">
<p>1,239,597</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Total</p>
</td>
<td class="No-Table-Style">
<p>1,218,363</p>
</td>
<td class="No-Table-Style">
<p>23,898</p>
</td>
<td class="No-Table-Style">
<p>1,242,261</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 5.6 – Chi-square test of independence “observed” table</p>
<p>Let us now create a table of expected values using this equation:</p>
<p>E ij =  T i T j _ N </p>
<p>Where T i is the total in the <em class="italic">i</em>th row, T j is the total in the <em class="italic">j</em>th column, and N is the total number of observations. This yields the following:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table005-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">Restrained</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Unrestrained</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Total</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Fatal</p>
</td>
<td class="No-Table-Style">
<p>(2,664 *1,218,363)/1,242,261 = 2,612.75</p>
</td>
<td class="No-Table-Style">
<p>(2,664 *23,898)/1,242,261 = 51.25</p>
</td>
<td class="No-Table-Style">
<p>2,664</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Not Fatal</p>
</td>
<td class="No-Table-Style">
<p>(1,216,934* 1,239,597) / 1,242,261 = 1,214,324.31</p>
</td>
<td class="No-Table-Style">
<p>(1,239,597 * 23,898) / 1,242,261 = 23,846.75</p>
</td>
<td class="No-Table-Style">
<p>1,239,597</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Total</p>
</td>
<td class="No-Table-Style">
<p>1,218,363</p>
</td>
<td class="No-Table-Style">
<p>23,898</p>
</td>
<td class="No-Table-Style">
<p>1,242,261</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 5.7 – Chi-square test of independence “expected” table</p>
<p>The modified version of the Pearson’s chi-square test statistic we used in the goodness-of-fit test that extends to the chi-square test of independence follows:</p>
<p>χ 2 = ∑ (O ij − E ij) 2 _ E ij , degrees of freedom = (r-1)(c-1)</p>
<p>The subscript <em class="italic">j</em> corresponds to columnar data and the subscript <em class="italic">i</em> corresponds to the row data for the observed and expected values, denoted as <em class="italic">O</em> and <em class="italic">E</em>, respectively. The values <em class="italic">r</em> and <em class="italic">c</em> in the degrees of freedom calculation correspond to the number of rows and the number of columns in the table (this is 2x2, so the degrees of freedom will equal (2-1)(2-1)=1.</p>
<p>Using the<a id="_idIndexMarker451"/> values from the observed and expected tables, we find the chi-square test statistic to equal the following:</p>
<p>χ 2 =  (1,429 − 2,612.75) 2  ______________ 2,612.75  +  (1,216,934 − 1,214,324.31) 2  ____________________  1,214,324.31  +  (1,235 − 51.25) 2  ____________ 51.25  +</p>
<p> (22, 663 − 23,846.75) 2  ________________  23,846.75  = 27,942.43</p>
<p>The table lookup test statistic using the one degree of freedom calculated here is 3.84. We can reasonably conclude that since the test statistic of 27,942.43 is greater than the critical value of 3.84, we can conclude that in the state of Texas in 2021, there was a statistically significant difference in the rate of fatalities in vehicle crashes where safety restraints were used compared to where they were not used.</p>
<p class="callout-heading">Chi-square contingency tables</p>
<p class="callout">The test we performed uses two 2x2 tables called contingency tables. The chi-square test of independence is frequently referred to <a id="_idIndexMarker452"/>as the <strong class="bold">chi-squared contingency test</strong>. The failure to reject the null hypothesis is <strong class="bold">contingent</strong> upon the observed table’s values matching the expected table’s values within a level of statistical confidence. However, the chi-square test of independence can be extended to <strong class="bold">tables of any combination of rows and columns</strong>. However, as the rows and columns increase in value, the tables may be less useful to interpret.</p>
<p>To perform this test in Python, we can use the <code>chi2_contingency</code> function from the <code>scipy</code> <code>stats</code> module. Inputting only the observed frequencies as a 2x2 <code>numpy</code> array, the <code>chi2_contingency</code> test provides the expected frequencies should the null hypothesis be true– the p-value, degrees of freedom used, and the chi-square test statistic:</p>
<pre class="source-code">
from scipy.stats import chi2_contingency
from scipy.stats import chi2
import numpy as np
observed_frequencies = np.array([[1429, 1235], [1216934, 22663]])
chi_Square_test_statistic, p_value, degrees_of_freedom, expected_frequencies = chi2_contingency(observed_frequencies)
chi_square_critical_value = chi2.ppf(1-.05, df=degrees_of_freedom)
print('Chi-Square Test Statistic: %.4f'%chi_Square_test_statistic)
print('Chi-Square Critical Value: %.4f'%chi_square_critical_value)
print('P-Value: %.4f'%p_value)</pre>
<p>Here, we <a id="_idIndexMarker453"/>see the chi-square test statistic is much larger than the chi-square critical value at the 0.05 level of significance and the p-value is significant at p &lt; 0.0000. Therefore, we can conclude there is strong statistical evidence to suggest a large difference in car crash fatalities when using a safety restraint compared to not using one:</p>
<p><code>Chi-Square Test </code><code>Statistic: 27915.1221</code></p>
<p><code>Chi-Square Critical </code><code>Value: 3.8415</code></p>
<p><code>P-Value: 0.0000</code></p>
<p class="callout-heading">Yates’ continuity correction</p>
<p class="callout">An additional argument can optionally be added to the <code>chi2_contingency</code> test with the <code>correction</code> <code>=</code> argument and a bool (<code>True</code> or <code>False</code>) input. This applies Yates’ continuity correction, which, per the <code>scipy.stats</code> documentation, adjusts each observed <a id="_idIndexMarker454"/>value by 0.5 toward the corresponding expected value. The purpose of the adjustment is to avoid incorrectly detecting the presence of statistical significance due to small expected sample sizes. Subjectively, this is useful for less than 10 samples in an expected frequency cell. However, many practitioners and researchers argue against its use.</p>
<h1 id="_idParaDest-96"><a id="_idTextAnchor100"/>Chi-square goodness-of-fit test power analysis</h1>
<p>Let’s use an<a id="_idIndexMarker455"/> example where a phone vendor sells four popular models of phones, models A, B, C, and D. We want to determine how many samples are required to produce a power of 0.8 so we can understand whether there is a statistically significant difference between the popularity of different phones so the vendor can more properly invest in phone acquisitions. In this case, the null hypothesis asserts that 25% of phones from each model were sold. In reality, 20% of phones sold were model A, 30% were model B, 19% were model C, and 31% were model D phones.</p>
<p>Testing different values for the <code>nobs</code> argument (number of observations), we find that a minimum of 224 samples produces a power just greater than 0.801. Adding more samples will only improve this. If the true distribution were more divergent from the hypothesized 25% even split, fewer samples would be required. However, since the splits are relatively close to 25%, a high volume of samples is needed:</p>
<pre class="source-code">
from statsmodels.stats.power import GofChisquarePower
from statsmodels.stats.gof import chisquare_effectsize
# probs0 asserts 25% of each brand are sold
# In reality, 12% of Brand A, 25% of Brand B, 33% sold were Brand C, and 1% were Brand D.
effect_size = chisquare_effectsize(probs0=[25, 25, 25, 25], probs1=[20, 30, 19, 31], cohen=True)
alpha = 0.05
n_bins=4 # 4 brands of phones
analysis = GofChisquarePower()
result = analysis.solve_power(effect_size, nobs=224, alpha=alpha, n_bins=n_bins)
print('Sample Size Required in Sample 1: {:.3f}'.format(
    result))
# Sample Size Required in Sample 1: 0.801</pre>
<h1 id="_idParaDest-97"><a id="_idTextAnchor101"/>Spearman’s rank correlation coefficient</h1>
<p>In <a href="B18945_04.xhtml#_idTextAnchor070"><em class="italic">Chapter 4</em></a><em class="italic">, Parametric Tests</em>, we looked at the parametric correlation coefficient, Pearson’s correlation, where the coefficient is calculated from independently sampled, continuous<a id="_idIndexMarker456"/> data. However, <strong class="bold">when we have ranked, ordinal data</strong>, such as that from a satisfaction survey, we would not want to use Pearson’s correlation as it cannot be assumed to guarantee the preservation of order. As with Pearson’s correlation coefficient, <strong class="bold">Spearman’s correlation coefficient</strong> results in a coefficient, <em class="italic">r</em>, that ranges from -1 to 1, with -1 being a strong inverse correlation and 1 being a strong direct correlation. Spearman’s is derived by dividing the covariance of the two variables’ ranks by the product of their standard deviations. The equation for the correlation coefficient, <em class="italic">r</em>, is as follows:</p>
<p>r s =  S xy _ √ _ S xx S yy  </p>
<p>Where</p>
<p>S xy = ∑     (x i −  _ x )(y i −  _ y )</p>
<p>S xx = ∑     (x i −  _ x ) 2</p>
<p>S yy = ∑     (y i −  _ y ) 2</p>
<p>The preceding correlation equation is safe to use under all ranking scenarios. However, when there are no ties or a minimal proportion of ties compared to the overall sample volume, the following equation may be used:</p>
<p>r s =  6∑ d i 2 _ n(n 2 − 1), where d i = (x i − y i)</p>
<p>However, we <a id="_idIndexMarker457"/>will perform this test in Python. Therefore, the more complex and less error-prone formula will be applied regardless since computational power enables us to bypass the manual process entirely.</p>
<p>Suppose we have students being judged in a competition by two judges and there is a concern that one of the judges may be biased toward some of the participants based on confounding factors, such as family ties, rather than performance alone. We decide to run a correlation analysis on the scores to test the hypothesis the two judges scored similarly for each contestant:</p>
<pre class="source-code">
from scipy.stats import spearmanr
import pandas as pd
df_scores = pd.DataFrame({'Judge A':[1, 3, 5, 7, 8, 3, 9],
                          'Judge B':[2, 5, 3, 9, 6, 1, 7]})</pre>
<p>We have the following table of contestants:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table006-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold">Student 1</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 2</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 3</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 4</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 5</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 6</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Student 7</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Judge A</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Judge B</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 5.8 – Judge rankings of student contestants for Spearman’s correlation analysis</p>
<pre class="source-code">
correlation, p_value = spearmanr(df_scores['Judge A'],
    df_scores['Judge B'])
print('Spearman Correlation Coefficient: %.4f'%correlation)
print('P-Value: %.4f'%p_value)</pre>
<p>Based on <a id="_idIndexMarker458"/>the p-value of 0.04 here, which is less than a significance level of 0.05, we can say the correlation produced is significant beyond a degree of random chance. Had the p-value been greater than 0.05, we could say with a 95% level of confidence that the correlation may have been purely spurious and may not have enough statistical evidence to support determinism. Spearman’s correlation (r) is 0.7748. <code>r_squared</code> is therefore approximately 0.6003, meaning approximately 60.3% of the variation in the score is determined by the judge’s scoring:</p>
<p><code>Spearman Correlation </code><code>Coefficient: 0.7748</code></p>
<p><code>P-Value: 0.0408</code></p>
<p>Since the p-value is significant and the correlation coefficient is 0.77 – and a strong correlation coefficient starts at approximately 0.7 – we may conclude that the judges’ scores are directly correlated enough to assume there is no bias in scoring present, assuming a relatively objective method for ranking exists; something more subjective may not be as suitable for correlation analysis.</p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor102"/>Summary</h1>
<p>In this chapter, we discussed some of the most commonly used non-parametric hypothesis tests performed when required assumptions for parametric hypothesis testing cannot be prudently guaranteed. We discussed two-sample Wilcoxon Rank-Sum – also called Mann-Whitney U – tests to draw inferences from medians when two-sample t-testing cannot be performed. Next, we walked through the Wilcoxon Sign-Rank test’s paired comparison of medians when a paired t-test comparison of means cannot be performed. After, we looked at the non-parametric chi-square goodness-of-fit test and the chi-square Test of independence for comparing observed frequencies against expected frequencies, both useful for identifying the presence of statistically significant differences in counts of categorical data. Additionally, we discussed the Kruskal-Wallis test, a non-parametric alternative to the analysis of variance (ANOVA). Finally, we discussed Spearman’s correlation coefficient and how to derive correlation based on rank when parametric assumptions cannot safely support using Pearson’s correlation. Closing out the chapter, we provided an example of using power analysis for the chi-square test.</p>
<p>In the next chapter, we will begin our discussion of predictive analytics, starting with simple linear regression. There, we will discuss methods for fitting a model, interpreting linear coefficients, understanding the required assumptions of linear regression, assessing model performance, and considering modified versions of linear regression.</p>
</div>
</div>

<div><div><h1 id="_idParaDest-99" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor103"/>Part 2:Regression Models</h1>
<p>In this part, we discuss the types of problems that can be solved with regression, coefficients of correlation and determination, multivariate modeling, model selection and variable adjustment with regularization.</p>
<p>It includes the following chapters:</p>
<ul>
<li><a href="B18945_06.xhtml#_idTextAnchor104"><em class="italic">Chapter 6</em></a>, <em class="italic">Simple Linear Regression</em></li>
<li><a href="B18945_07.xhtml#_idTextAnchor118"><em class="italic">Chapter 7</em></a>, <em class="italic">Multiple Linear Regression</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</div></body></html>