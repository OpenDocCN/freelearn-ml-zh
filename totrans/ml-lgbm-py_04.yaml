- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Comparing LightGBM, XGBoost, and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous chapter introduced LightGBM for building **gradient-boosted decision
    trees** (**GBDTs**). In this chapter, we compare LightGBM against two other methods
    for modeling tabular data: XGBoost, another library for building gradient-boosted
    trees, and **deep neural networks** (**DNNs**), a state-of-the-art machine learning
    technique.'
  prefs: []
  type: TYPE_NORMAL
- en: We compare LightGBM, XGBoost, and DNNs on two datasets, focusing on complexity,
    dataset preparation, model performance, and training time.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is aimed at advanced readers, and some understanding of deep learning
    is required. However, the primary purpose of the chapter is not to understand
    XGBoost or DNNs in detail (neither technique is used in subsequent chapters).
    Instead, by the end of the chapter, you should have some understanding of how
    competitive LightGBM is within the machine-learning landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main topics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning and TabTransformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing LightGBM, XGBoost, and TabTransformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter includes examples and code excerpts illustrating how to train LightGBM,
    XGBoost, and TabTransformer models in Python. Complete examples and instructions
    for setting up a suitable environment for this chapter are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-4](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-4).
  prefs: []
  type: TYPE_NORMAL
- en: An overview of XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**XGBoost**, short for **eXtreme Gradient Boosting**, is a widely popular open
    source gradient boosting library with similar goals and functionality to LightGBM.
    XGBoost is older than LightGBM and was developed by Tianqi Chen and initially
    released in 2014 *[1]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, XGBoost implements GBDTs and supports building them highly efficiently.
    Some of the main features of XGBoost are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization**: XGBoost incorporates both L1 and L2 regularization to avoid
    overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparsity awareness**: XGBoost efficiently handles sparse data and missing
    values, automatically learning the best imputation strategy during training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelization**: The library employs parallel and distributed computing
    techniques to train multiple trees simultaneously, significantly reducing training
    time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: XGBoost provides an option to halt the training process
    if there is no significant improvement in the model’s performance, improving performance
    and preventing overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-platform compatibility**: XGBoost is available for many programming
    languages, including Python, R, Java, and Scala, making it accessible to a diverse
    user base'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the years, XGBoost has gained popularity in the machine learning community
    due to its support for various applications and the library’s ease of use and
    efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing XGBoost and LightGBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is considerable overlap in functionality between XGBoost and LightGBM.
    Both libraries implement GBDTs and DART and support building random forests. Both
    have similar techniques to avoid overfitting and handle missing values and sparse
    data automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, some of the differences between XGBoost and LightGBM are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tree-growing strategy**: XGBoost employs a level-wise tree growth approach,
    where trees are built level by level, while LightGBM uses a leaf-wise tree growth
    strategy that focuses on growing the tree by choosing the leaf with the highest
    delta loss. This difference in growth strategy generally makes LightGBM faster,
    especially for large datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed and scalability**: LightGBM is designed to be more efficient regarding
    memory usage and computation time, making it a better choice for large-scale datasets
    or when training time is critical. However, this speed advantage can sometimes
    come at the cost of higher variance in model predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling categorical features**: LightGBM has built-in support for categorical
    features, meaning it can handle them directly without needing one-hot encoding
    or other preprocessing techniques. XGBoost, on the other hand, requires the user
    to preprocess categorical features before feeding them into the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: XGBoost provides an option to halt the training process
    if there is no significant improvement in the model’s performance. LightGBM does
    not have this feature built in, although it can be implemented manually using
    callbacks, as seen in earlier chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, LightGBM and XGBoost provide similar functionality. LightGBM performs
    better on large datasets with many features, whereas XGBoost may provide more
    stable and accurate results on smaller or medium-sized datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Python XGBoost example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'XGBoost provides a scikit-learn-based interface for building models. The following
    example shows how to use XGBoost on the Forest Cover dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The scikit-learn interface should be familiar to you at this stage. The preceding
    code shows that XGBoost supports similar hyperparameters as we used to train LightGBM-based
    models. A full list of parameters is av[ailable at https://xgboost.readthedocs.io/en/stable/par](https://xgboost.readthedocs.io/en/stable/parameter.xhtml)ameter.xhtml.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost represents a direct alternative to LightGBM as another gradient-boosting
    library. In the next section, we look at deep learning, a wholly different but
    extremely popular learning technique, and how it compares to gradient boosting
    on tabular learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and TabTransformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now look at an approach to solving tabular-based data problems using deep
    learning. Deep learning has gained immense popularity in recent years due to the
    performance of deep-learning-based models. Deep-learning-based techniques such
    as AlphaZero, Stable Diffusion, and the GPT series of language models have achieved
    human or superhuman performance in gameplay, art generation, and language-based
    reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Deep learning** is a subfield of the broader machine learning field of artificial
    neural networks. Artificial neural networks are mathematical mimics of the human
    brain and consist of interconnected layers of nodes (or “neurons” in biological
    parlance) that process and transmit information.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple artificial neural networks consist of only a few layers. The term “deep”
    in deep learning refers to using neural networks of many more layers, each with
    potentially thousands of neurons. These layers are organized hierarchically, with
    *input* layers at the bottom, *output* at the top, and *hidden* between. Each
    layer extracts and refines features as data passes through the network, allowing
    the model to learn complex patterns and representations.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram depicts a simple neural network called a multilayer perceptron
    with a single hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A multilayer perceptron with a single hidden layer and an output
    layer. The layers are fully connected](img/B16690_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A multilayer perceptron with a single hidden layer and an output
    layer. The layers are fully connected
  prefs: []
  type: TYPE_NORMAL
- en: Each neuron receives input from other neurons, performs a mathematical operation,
    and then passes the result to the next layer of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical operation involves two main steps – weighted sum and activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weighted sum**: The neuron takes the inputs (input data or outputs from previous
    neurons), multiplies each input by its corresponding weight, and then adds them
    together. A bias term is often added to the weighted sum for better control over
    the neuron’s output. Mathematically, this can be represented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: z j = ∑ i (w ij x i) + b j
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, x i represents all inputs to the neuron, w ij is the weight associated
    with the i th input, and b j is the bias for the neuron.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Activation function**: The weighted sum is then passed through an activation
    function, determining the neuron’s output. The purpose of the activation function
    is to introduce non-linearity into the mathematical operation. The non-linearity
    allows the neural network to model non-linear and, therefore, complex relationships
    between inputs and outputs. There are various activation functions, such as **sigmoid**
    (logistic function), **hyperbolic tangent** (**tanh**), and **Rectified Linear
    Unit** (**ReLU**), each with its own properties and use cases. This can be represented
    as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a j = σ( z j)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where a j is the neuron output and σ is the activation function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Combining these two steps, a neuron in a neural network processes the input
    data, allowing the network to learn and model complex patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are trained by adjusting the weights associated with the neurons.
    The algorithm can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Weights are initialized to small, random values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A **forward pass** is performed: for each example in a batch, the input features
    are passed through the entire network (calculating the sum and activation at each
    neuron) to produce a prediction at the output layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is then calculated by comparing the output against the actual output
    for each example in a batch. Like GBDTs, the loss function has to be differentiable,
    and standard loss functions include the MSE and cross-entropy loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Backpropagation** is performed: the gradient of the loss function with respect
    to the weights is calculated using the calculus chain rule. The process is started
    at the output layer and works backward through the network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weights are then updated using gradient descent or one of the modern variants,
    such as Adam, based on the backpropagated gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The process is repeated for a set number of epochs (each epoch running through
    the entire dataset) to minimize the loss function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A unique property of neural networks is that neural networks have been proven
    to be **universal function approximators**. DNNs have the theoretical capability
    to approximate any continuous function to a desired level of accuracy, given a
    sufficient number of hidden neurons and an appropriate activation function. This
    property is based on the **Universal Approximation Theorem**, which has been proven
    for various types of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: This means that a neural network can learn to represent complex relationships
    between input and output data, no matter how intricate or non-linear these relationships
    might be. This capability is one of the reasons why neural networks, especially
    DNNs, have successfully solved a wide range of problems across different domains.
    However, this guarantee is theoretical. In practice, finding the correct network
    architecture, hyperparameters, and training techniques to achieve the desired
    level of approximation can be challenging. The process often requires experimentation,
    expertise, and prohibitive computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and disadvantages of deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the capabilities of DNNs, we might believe that they should be our first
    port of call for all machine learning problems. The primary advantage of using
    DNNs is their high accuracy in very complex domains: the current state-of-the-art
    performance on a wide range of complex tasks, natural language processing, generative
    AI, image recognition, and speech recognition are all achieved by DNNs due to
    their ability to learn complex and hidden patterns in large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage is automatic feature extraction. With the correct architecture,
    a DNN can extract complex or higher-order features automatically, alleviating
    the need for a data scientist to perform feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, DNNs can also transfer learning: pre-trained deep learning models
    can be fine-tuned on a smaller dataset for a specific task, leveraging the knowledge
    acquired during the initial training. Transfer learning can significantly reduce
    training time and data requirements for new tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, deep learning is not a panacea to all machine learning problems. Some
    of the disadvantages of using DNNs include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational resources**: Deep learning models often require significant
    computing power and memory for training, especially when dealing with large datasets
    and complex architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Large datasets**: DNNs usually perform well when trained on large datasets,
    but their performance can degrade when trained on smaller datasets. When a dataset
    is too small, the DNN overfits the training data and cannot generalize to unseen
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability**: DNNs are often considered “black boxes” due to their
    complex architectures and the large number of parameters involved. The complexity
    can make understanding how the model makes decisions difficult, which may be a
    concern for applications requiring transparency or regulatory compliance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: DNNs involve numerous hyperparameters, such as network
    architecture, learning rate, and activation functions. Coupled with longer training
    times and resource needs, finding the optimal combination of these hyperparameters
    can be expensive and time-consuming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TabTransformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We want to apply deep learning to tabular data, as most practical machine learning
    problems have tabular data. To this end, we use a new deep learning architecture
    called **TabTransformer**: a deep neural network model designed to handle tabular
    data specifically.'
  prefs: []
  type: TYPE_NORMAL
- en: Like the GPT family of DNNs, TabTransformer is based on the transformer architecture
    originally introduced by Vaswani et al. *[2]*. TabTransformer adapts the transformer
    architecture to work effectively with tabular data, providing an alternative to
    other machine learning models such as decision trees and gradient boosting machines
    for such data *[3]*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The TabTransformer architecture as implemented in Keras [3]](img/B16690_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The TabTransformer architecture as implemented in Keras [3]
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture for TabTransformer is shown in *Figure 4**.2*. With TabTransformer,
    each feature in the tabular data is treated as a token, similar to how words are
    treated as tokens in natural language processing. The model applies self-attention
    mechanisms to learn complex interactions and dependencies between features in
    the input data. The token embedding and attention mechanism allows the model to
    capture global and local relationships between features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TabTransformer model has several key components: token embedding, positional
    encoding, transformer layers, pooling, and output layers. The token embedding
    converts each feature value into a continuous vector representation, combined
    with positional information through positional encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Moving through the layers, shown in *Figure 4**.2*, we can see that the categorical
    and numerical features are split.
  prefs: []
  type: TYPE_NORMAL
- en: The categorical features first go through an embedding layer, as implemented
    by `layers.Embedding` in Keras, and are then passed along to the transformer blocks.
    A variable number of transformer blocks can be implemented (as set using a hyperparameter),
    but each consists of a `layers.MultiHeadAttention` layer and a `layers.Dense`
    layer with `Dropout`. The output values are added and normalized after passing
    through the attention and dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: Due to their performance, the transformer feedforward layer uses the **Gaussian
    error linear unit** (**GELU**) activation function in our implementation. However,
    other activation functions may be used *[5]*.
  prefs: []
  type: TYPE_NORMAL
- en: The numerical features are passed through a normalization layer (normalizing
    the numerical input ranges) and then concatenated with output from the transformers.
  prefs: []
  type: TYPE_NORMAL
- en: The concatenated results are passed through a `Dropout`. Our implementation
    uses **scaled exponential linear unit** (**SELU**), which causes the activations
    to self-normalize *[6]*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output from the MLP block is passed to the loss function, the implementation
    of which depends on the learning problem (classification or regression).
  prefs: []
  type: TYPE_NORMAL
- en: TabTransformers are much more complex to implement and train than gradient-boosted
    trees. Like other DNNs, TabTransformers require more data preparation and computational
    power than gradient-boosted trees.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to TabTransformers, this section introduced deep learning alongside
    its advantages and disadvantages. In the next section, we use a practical example
    to compare the different approaches, including the complexity of working with
    TabTransformers.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing LightGBM, XGBoost, and TabTransformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we compare the performance of LightGBM, XGBoost, and TabTransformers
    on two different datasets. We also look at more data preparation techniques for
    unbalanced classes, missing values, and categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting census income
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first dataset we use is the Census Income dataset, which predicts whether
    personal income will exceed $50,000 based on attributes such as education, marital
    status, occupation, and others *[4]*. The dataset has 48,842 instances, and as
    we’ll see, some missing values and unbalanced classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is available from the following URL: [https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data).
    The data has already been split into a training set and a test set. Once loaded,
    we can sample the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The data sample for the selected columns is shown in *Table 4.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **age** | **education** | **marital_status** | **hours_per_week** | **income_bracket**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12390 | 34 | Some-college | Never-married | 40 | <=50K |'
  prefs: []
  type: TYPE_TB
- en: '| 20169 | 41 | Assoc-acdm | Married-civ-spouse | 45 | >50K |'
  prefs: []
  type: TYPE_TB
- en: '| 17134 | 35 | Doctorate | Never-married | 60 | >50K |'
  prefs: []
  type: TYPE_TB
- en: '| 23452 | 49 | HS-grad | Married-civ-spouse | 40 | >50K |'
  prefs: []
  type: TYPE_TB
- en: '| 22372 | 31 | HS-grad | Separated | 45 | <=50K |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Sample data from the Census Income dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 4.1* shows that we have mixed data types: some features are numeric,
    and others are text. Notably, some columns in the dataset are **categorical features**:
    string-based features with a fixed set of values. Next, we look at encoding these
    features for use in machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding categorical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most machine learning algorithms need string-based features to be encoded to
    numbers; in some cases, this can be done automatically. We discuss automatic encoding
    for LightGBM in [*Chapter 6*](B16690_06.xhtml#_idTextAnchor094)*, Solving Real-World
    Data Science Problems with LightGBM*. In this example, we encode the features
    to understand what this entails.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to map each categorical value to a unique number; therefore, we first
    build a vocabulary of all values for each feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code extracts unique values for each column into a list and sorts
    the list, putting `null` values last. When working with pandas DataFrames, it’s
    also useful to explicitly set the data type for categorical columns to `category`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using our vocabulary, we can now update the values in each column to numbers
    representing their category (using the index in the vocabulary list as the numeric
    value):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a DataFrame where all features are now numeric:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **age** | **education** | **marital_status** | **hours_per_week** | **income_bracket**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 18545 | 37 | 11 | 2 | 40 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 26110 | 51 | 14 | 0 | 60 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 21905 | 36 | 11 | 5 | 32 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1496 | 32 | 1 | 0 | 43 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3148 | 47 | 15 | 2 | 40 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Encoded categorical data from the Census Income dataset
  prefs: []
  type: TYPE_NORMAL
- en: Our categorical features are now encoded, and we can proceed with further data
    cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Missing values and duplicates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to check for missing values, duplicates, and outliers. We can use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We drop the duplicate data, and reviewing the output of `describe` shows us
    there are no significant outliers. However, there are **missing values** in the
    dataset. LightGBM and XGBoost can deal with missing values automatically, a significant
    advantage of tree-based algorithms. However, for TabTransformers, we need to implement
    particular logic to deal with the missing values, as we’ll see next.
  prefs: []
  type: TYPE_NORMAL
- en: Unbalanced data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The dataset is also skewed: the number of examples of each class is not balanced.
    We can calculate the skew using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output shows a roughly 75%/25% skew toward the negative (`0`) class. One
    of the simplest ways of dealing with unbalanced classes (if we have binary classes)
    is to weigh the positive class more strongly than the negative class. Therefore,
    when calculating the loss, a prediction that misses the positive class has a more
    significant impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'LightGBM and XGBoost both support this through the `scale_pos_weight` parameter,
    which can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training LightGBM and XGBoost models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the data cleaned and prepared, we can now train our models. Training the
    LightGBM and XGBoost models is straightforward. For LightGBM, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And for XGBoost, we can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code highlights the simplicity of working with both libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Training a TabTransformer model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll now build a TabTransformer model. We’ll use **TensorFlow’s Keras** to
    define the model based on the example code: [https://keras.io/examples/structured_data/tabtransformer/](https://keras.io/examples/structured_data/tabtransformer/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our dataset preparation remains mostly the same, with two key differences:
    we don’t encode the categorical features and must handle the missing values explicitly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t encode the categorical features because Keras provides a special layer
    to perform string lookup and conversion to a numerical value. However, we must
    still supply the vocabulary. The following code illustrates creating a lookup
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `num_oov_indices` parameter is set to `0`, meaning no indices are used if
    an `mask_token` parameter is also set to `None,` as we aren’t masking any string
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to supply a default value for each column in the dataset to handle
    missing values. Our strategy is to replace string values with a default string
    value, `NA`. We use the statistical mean for the numeric columns to fill in the
    missing value. The following code creates a list of the default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The Keras code for implementing a TabTransformer model is roughly 100 lines
    long and is available in our GitHub repository: [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/blob/main/chapter-4/tabtransformer-census-income.ipynb](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/blob/main/chapter-4/tabtransformer-census-income.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sets up the gradient optimizer and data that we can use
    with the TabTransformer model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We use an `AdamW` optimizer with weight decay *[7]* with a binary cross-entropy
    loss function to fit the binary classification problem. We can then train and
    evaluate our model with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We also add early stopping via the Keras callback with a patience of `3` epochs.
    During training and for validation, we track the accuracy and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Training takes significantly longer than either gradient-boosting framework
    and requires a GPU (training on a CPU is technically possible but takes an inordinate
    amount of time).
  prefs: []
  type: TYPE_NORMAL
- en: We can now look at the results of the three algorithms on the Census Income
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parameter optimization was performed for all three algorithms using the grid
    search technique discussed in the previous chapter. The learning rate, bin size,
    and number of trees were optimized for the two boosting algorithms. For the TabTransformer,
    both parameters and aspects of the architecture must be optimized. In terms of
    parameters, the learning rate, weight decay, and dropout rate were optimized,
    while for the architecture, the number of transformer blocks and hidden layers
    (in the MLP) had to be chosen. The optimized parameters are available in the source
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The following table shows the results of the validation set for the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Training Time** | **Accuracy** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| LightGBM GBDT | **1.05s** | **84.46%** | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost GBDT | 5.5s | 84.44% | **0.72** |'
  prefs: []
  type: TYPE_TB
- en: '| TabTransformer | 113.63s | 77.00% | 0.64 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Results from training the three models on the Census Income dataset
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost and LightGBM performed similarly on the dataset, reaching an accuracy
    of 84% and an F1 score of 0.7\. The TabTransformer model performed worse, with
    a lower accuracy and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding training time, LightGBM was much faster than the other approaches.
    The LightGBM model was trained 5.23 times faster than XGBoost and 108.22 times
    faster than the TabTransformer. The TabTransformer was trained for 15 epochs on
    an 8-core P4000 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: For another point of comparison and to illustrate how the TabTransformer architecture
    can be adapted when categorical features aren’t present, we look at solving a
    second problem using the three algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting credit card fraud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our second task is detecting fraudulent transactions in a credit card transaction
    dataset *[8]*. The dataset is available at [https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
    The task is a binary classification problem, with the training data transactions
    labeled non-fraudulent (`0`) and fraudulent (`1`). The dataset consists only of
    numerical features that have been anonymized for confidentiality. Notably, the
    dataset is highly unbalanced, with fraudulent transactions making up only 0.17%
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Training LightGBM and XGBoost models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the values are all numeric, very little data preparation is required
    for the gradient-boosting models. To counteract the imbalance in the dataset,
    we again calculate `scale_pos_weight` and pass it to the model as a parameter.
    We perform a grid search with cross-validation to find good hyperparameters for
    both the LightGBM and XGBoost models. For LightGBM, both DART and a GBDT model
    were tried, with DART performing better. Unlike the Census Income dataset, the
    credit cards dataset is not pre-split into a training and test set. We, therefore,
    apply five-fold cross-validation to measure performance on unseen data. The following
    code trains the LightGBM model, with the XGBoost code being very similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The results for both LightGBM and XGBoost are shown in *Table 4.4* alongside
    the TabTransformer results.
  prefs: []
  type: TYPE_NORMAL
- en: Training a TabTransformer model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Without categorical features, the TabTransformer architecture can be significantly
    simplified. Let’s look at the architecture as shown in *Figure 4**.2*. We can
    see that the embedding and attention layers are no longer required. Indeed, *the
    model simplifies to a regular MLP* (it is disingenuous to still call the model
    a transformer as the attention layers are not used at all).
  prefs: []
  type: TYPE_NORMAL
- en: Besides removing the unneeded layers, the rest of the architecture and process
    remain the same as for the Census Income problem. `AdamW` is again used as the
    optimizer, and we perform grid search optimization of the hyperparameters and
    the number of hidden layers in the model. As with the gradient-boosting models,
    five-fold cross-validation is performed to measure the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the accuracy is also reported next, it is essential to note that it
    is not a good performance indicator with unbalanced data. In the dataset, 99.82%
    of the samples are of one class, and a model that predicts only that class will
    have a 99.82% accuracy and be completely pointless. The F1 score is unaffected
    by the class imbalance and remains a good performance indicator for classification
    performance in unbalanced datasets. The following table shows the results for
    all three algorithms with five-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Training Time** | **Accuracy** | **F1 score** |'
  prefs: []
  type: TYPE_TB
- en: '| LightGBM GBDT | **113s** | **99.88%** | 0.80 |'
  prefs: []
  type: TYPE_TB
- en: '| XGBoost GBDT | 351s | 98.41% | **0.82** |'
  prefs: []
  type: TYPE_TB
- en: '| TabTransformer | 528.59s | 93.37% | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.4 – Results from training the three models on the Credit Card Fraud
    dataset. Training time includes five-fold cross-validation
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost and LightGBM performed very similarly on the dataset, obtaining F1 scores
    of 0.82 and 0.80, respectively. The DNN struggles significantly with the problem,
    obtaining an F1 score of only 0.05 even using class weights to compensate for
    the imbalanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging performance issues in DNNs are notoriously tricky. Due to the complexity
    and opaqueness of building and training DNN models, small changes can have a significant
    effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Possible reasons for the poor performance include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inadequate model architecture**: This is the most likely cause. The architecture
    is not well suited to the problem. Further experimentation with the architecture,
    the size of layers, or even the type of neural network is needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insufficient training**: The model might not be trained long enough. Increasing
    the training epochs can improve performance. However, in our experiments, the
    loss stagnated after 10 epochs (while training continued to 15).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BinaryCrossentropy` loss function with class weights. However, a more advanced
    loss function, such as a focal loss, may be tried *[9]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of the training and validation time, it is a similar story as with
    the Census Income dataset. The LightGBM model was trained and validated significantly
    faster than the other approaches: 3.1 times faster than the XGBoost and 4.62 times
    faster than the DNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed two additional algorithms that may be used to
    solve tabular learning problems: XGBoost, another gradient-boosting framework,
    and TabTransformer, a deep learning approach.'
  prefs: []
  type: TYPE_NORMAL
- en: We showed how to set up and train both XGBoost models and TabTransformer on
    two datasets. We also showed how to encode categorical features for tree-based
    and neural network models. Both datasets also had imbalanced classes, which we
    had to compensate for during training.
  prefs: []
  type: TYPE_NORMAL
- en: We found that LightGBM and XGBoost produced similarly accurate models but that
    LightGBM trained models much faster and more efficiently. We also saw the complexity
    of training DNNs and the lackluster performance on these problems. Deep learning
    is an extremely powerful technique, but tree-based approaches are often more applicable
    when working with tabular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we focus on more effective parameter optimization with
    LightGBM using a framework called **Optuna**.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| *[**1]* | *T. Chen and C. Guestrin, “XGBoost,” in Proceedings of the 22nd
    ACM SIGKDD International Conference on Knowledge Discovery and Data* *Mining,
    2016.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**2]* | *A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.
    N. Gomez, L. Kaiser, and I. Polosukhin, Attention Is All You* *Need, 2017.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**3]* | *X. Huang, A. Khetan, M. Cvitkovic, and Z. Karnin, TabTransformer:
    Tabular Data Modeling Using Contextual* *Embeddings, 2020.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**4]* | *R. Becker, Adult, UCI Machine Learning* *Repository, 1996.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**5]* | *D. Hendrycks and K. Gimpel, Gaussian Error Linear Units (**GELUs),
    2020.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**6]* | *G. Klambauer, T. Unterthiner, A. Mayr and S. Hochreiter, Self-Normalizing
    Neural* *Networks, 2017.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**7]* | *I. Loshchilov and F. Hutter, Decoupled Weight Decay* *Regularization,
    2019.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**8]* | *A. Dal Pozzolo, O. Caelen, R. Johnson and G. Bontempi, “Calibrating
    Probability with Undersampling for Unbalanced* *Classification,” 2015.* |'
  prefs: []
  type: TYPE_TB
- en: '| *[**9]* | *T.-Y. Lin, P. Goyal, R. Girshick, K. He and P. Dollár, Focal Loss
    for Dense Object* *Detection, 2018.* |'
  prefs: []
  type: TYPE_TB
- en: 'Part 2: Practical Machine Learning with LightGBM'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Part 2 delves into the intricate processes that underpin practical machine learning
    engineering, starting with a look at efficient hyperparameter optimization via
    a framework called **Optuna**. We will then transition into a comprehensive exploration
    of the data science lifecycle, illustrating the rigorous steps from problem definition
    and data handling to practical data science modeling applications. Concluding
    this part, the focus will shift to automated machine learning, spotlighting the
    FLAML library, which aims to simplify and streamline model selection and tuning.
    Throughout this part, a blend of case studies and hands-on examples will provide
    a clear roadmap to harnessing the full potential of these advanced tools, underscoring
    the themes of efficiency and optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part will include the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16690_05.xhtml#_idTextAnchor083)*, LightGBM Parameter Optimization
    with Optuna*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16690_06.xhtml#_idTextAnchor094)*,* *Solving Real-World Data
    Science Problems with LightGBM*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16690_07.xhtml#_idTextAnchor116)*, AutoML with LightGBM and
    FLAML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
