<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Homogeneous Ensembles Using Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">An ensemble of homogeneous models for energy prediction</li>
<li class="calibre11">An ensemble of homogeneous models for handwritten digit classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the case of ensemble models, each <span class="calibre5">base classifier must have some degree of diversity within itself. This diversity can be obtained in one of the following manners:</span></p>
<ul class="calibre10">
<li class="calibre11"><span>By using different subsets of training data through various resampling methods or randomization of the training data</span></li>
<li class="calibre11">By using different learning hyperparameters for different base learners</li>
<li class="calibre11">By using different learning algorithms </li>
</ul>
<p class="calibre2">In the case of ensemble models, where different algorithms are used for the base learners, the ensemble is called a <strong class="calibre4">heterogeneous ensemble method</strong>. <span class="calibre5">If the same algorithm is used for all the base learners on </span><span class="calibre5">different distributions of the training set, </span><span class="calibre5">the ensemble is called a <strong class="calibre4">homogeneous ensemble</strong>. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An ensemble of homogeneous models for energy prediction</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the following example, we will use the Keras API. <span class="calibre5">Keras is an open source high-level framework for building deep neural networks.</span> It's written on top of TensorFlow or Theano and uses them for its calculations behind the scenes. Keras can run on both CPU and GPU. The default settings of Keras are designed to deliver good results in most cases.</p>
<p class="calibre2"/>
<p class="calibre2">The focus of Keras is the idea of a model. Keras supports two types of models. The main type of model is a sequence of layers, called <strong class="calibre4">sequential</strong>. The other type of model in Keras is the non-sequential model, called <strong class="calibre4">model</strong>. </p>
<p class="calibre2">To build a sequential model, carry out the following steps:</p>
<ol class="calibre14">
<li class="calibre11">Instantiate a sequential model using <kbd class="calibre12">Sequential()</kbd></li>
<li class="calibre11">Add layers to it one by one using the <kbd class="calibre12">Dense</kbd> class</li>
<li class="calibre11">Compile the model with the following:
<ul class="calibre41">
<li class="calibre11">A mandatory loss function</li>
<li class="calibre11">A mandatory optimizer</li>
<li class="calibre11">Optional evaluation parameters</li>
</ul>
</li>
<li class="calibre11">Use data to fit the model</li>
<li class="calibre11">Evaluate the model</li>
</ol>
<p class="calibre2">Here's a diagrammatic flow of the preceding steps:</p>
<p class="CDPAlignCenter"><img src="assets/f89f4516-767a-4737-a6a6-dd1e6b30bd1d.png" class="calibre56"/></p>
<p class="calibre2">In the following code block, we can see a short code example:</p>
<pre class="calibre18"># Instantiate a sequential model<br class="title-page-name"/>seqmodel = Sequential()<br class="title-page-name"/><br class="title-page-name"/># Add layers using the Dense class<br class="title-page-name"/>seqmodel.add(Dense8, activation='relu')<br class="title-page-name"/><br class="title-page-name"/># Compile the model<br class="title-page-name"/>seqmodel.compile(loss='binary_crossentropy, optimizer='adam', metric=['accuracy'])<br class="title-page-name"/><br class="title-page-name"/># Fit the model<br class="title-page-name"/>seqmodel.fit(X_train, Y_train, batch_size=10)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">We'll start by installing Keras. In order to install Keras, you will need to have Theano or TensorFlow installed in your system. In this example, we'll go with TensorFlow as the backend for Keras.</p>
<p class="calibre2">There are two variants of TensorFlow: a CPU version and a GPU version.</p>
<p class="calibre2">To install the current CPU-only version, use the following command:</p>
<pre class="calibre15">pip install tensorflow</pre>
<p class="calibre2">If you have to install the GPU package, use the following command:</p>
<pre class="calibre15">pip install tensorflow-gpu</pre>
<p class="calibre2">Once you've installed TensorFlow, you'll need to install Keras using the following command:</p>
<pre class="calibre15">sudo pip install keras</pre>
<p class="calibre2">In order to upgrade your already-installed Keras library, use the following command:</p>
<pre class="calibre15">sudo pip install --upgrade keras</pre>
<p class="calibre2"><span class="calibre5">Once we're done with installing the libraries, let's import the required libraries:</span></p>
<pre class="calibre15">import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/>from sklearn.metrics import mean_squared_error<br class="title-page-name"/><br class="title-page-name"/>from keras.models import Sequential<br class="title-page-name"/>from keras.layers import Dense</pre>
<p class="calibre2">We set our working directory according to our requirements:</p>
<pre class="calibre15">os.chdir("..../Chapter 9")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">We read our <kbd class="calibre12">energydata.csv</kbd> dataset:</p>
<pre class="calibre15">df_energydata = pd.read_csv("energydata.csv")</pre>
<p class="calibre2">We check whether there are any null values in our dataset:</p>
<pre class="calibre15">df_energydata.isnull().sum() </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">We'll now build our <kbd class="calibre12">test</kbd> subset and train our neural network models:</span></p>
<ol class="calibre14">
<li class="calibre11">Separate the <kbd class="calibre12">test</kbd> subset to apply the models in order to make predictions:</li>
</ol>
<pre class="calibre18">df_traindata, df_testdata = train_test_split(df_energydata, test_size=0.3)</pre>
<ol start="2" class="calibre14">
<li class="calibre11">Check the shape of the <kbd class="calibre12">train </kbd>and <kbd class="calibre12">test</kbd> subsets:</li>
</ol>
<pre class="calibre18">print(df_traindata.shape)<br class="title-page-name"/>print(df_testdata.shape)</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Take the <kbd class="calibre12">test</kbd> subset and split it into target and feature variables:</li>
</ol>
<pre class="calibre18">X_test = df_testdata.iloc[:,3:27] <br class="title-page-name"/>Y_test = df_testdata.iloc[:,28] </pre>
<ol start="4" class="calibre14">
<li class="calibre11">Validate the preceding split by checking the shape of <kbd class="calibre12">X_test</kbd> and <kbd class="calibre12">Y_test</kbd>:</li>
</ol>
<pre class="calibre18">print(X_test.shape)<br class="title-page-name"/>print(Y_test.shape)</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Let's create multiple neural network models using Keras. We use <kbd class="calibre12">For...Loop</kbd> to build multiple models:</li>
</ol>
<pre class="calibre18">ensemble = 20<br class="title-page-name"/>frac = 0.7<br class="title-page-name"/><br class="title-page-name"/>predictions_total = np.zeros(5921, dtype=float)<br class="title-page-name"/><br class="title-page-name"/>for i in range(ensemble):<br class="title-page-name"/>    print("number of iteration:", i)<br class="title-page-name"/>    print("predictions_total", predictions_total)<br class="title-page-name"/><br class="title-page-name"/>    # Sample randomly the train data<br class="title-page-name"/>    Traindata = df_traindata.sample(frac=frac)<br class="title-page-name"/>    X_train = Traindata.iloc[:,3:27] <br class="title-page-name"/>    Y_train = Traindata.iloc[:,28] <br class="title-page-name"/>    <br class="title-page-name"/><br class="title-page-name"/>    ############################################################<br class="title-page-name"/>    <br class="title-page-name"/>    model = Sequential()<br class="title-page-name"/>    # Adding the input layer and the first hidden layer<br class="title-page-name"/>    model.add(Dense(units=16, kernel_initializer = 'normal', activation = 'relu', input_dim = 24))<br class="title-page-name"/><br class="title-page-name"/>    # Adding the second hidden layer<br class="title-page-name"/>    model.add(Dense(units = 24, kernel_initializer = 'normal', activation = 'relu'))<br class="title-page-name"/>    <br class="title-page-name"/>    # Adding the third hidden layer<br class="title-page-name"/>    model.add(Dense(units = 32, kernel_initializer = 'normal', activation = 'relu'))<br class="title-page-name"/><br class="title-page-name"/>    # Adding the output layer<br class="title-page-name"/>    model.add(Dense(units = 1, kernel_initializer = 'normal', activation = 'relu'))<br class="title-page-name"/><br class="title-page-name"/>    # Compiling the ANN<br class="title-page-name"/>    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.9, epsilon=None, decay=0.0)<br class="title-page-name"/>    model.compile(loss='mse', optimizer=adam, metrics=['mean_squared_error'])<br class="title-page-name"/>    # Fitting the ANN to the Training set<br class="title-page-name"/><br class="title-page-name"/>    model.fit(X_train, Y_train, batch_size = 16, epochs = 25)<br class="title-page-name"/><br class="title-page-name"/>    ############################################################<br class="title-page-name"/>    <br class="title-page-name"/>    # We use predict() to predict our values<br class="title-page-name"/>    model_predictions = model.predict(X_test)<br class="title-page-name"/>    <br class="title-page-name"/>    model_predictions = model_predictions.flatten()<br class="title-page-name"/>    print("TEST MSE for individual model: ", mean_squared_error(Y_test, model_predictions))<br class="title-page-name"/>    print("")<br class="title-page-name"/>    print(model_predictions)<br class="title-page-name"/>    print("")<br class="title-page-name"/><br class="title-page-name"/>predictions_total = np.add(predictions_total, model_predictions)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">Take the summation of the predicted values and divide them by the number of iterations to get the average predicted values. We use the average predicted values to calculate the <strong class="calibre1">mean-squared error</strong> (<strong class="calibre1">MSE</strong>) for our ensemble:</li>
</ol>
<pre class="calibre18">predictions_total = predictions_total/ensemble<br class="title-page-name"/>print("MSE after ensemble: ", mean_squared_error(np.array(Y_test), predictions_total))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Here's a diagrammatic representation of the ensemble homogeneous model workflow:</p>
<p class="CDPAlignCenter"><img src="assets/e78d32ca-81b3-482a-9bd6-5ece7e724693.png" class="calibre33"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">In the preceding diagram, we assume that we have 100 train samples. We train 100 models on our 100 train samples and apply them to our test sample. We get 100 sets of predictions, which we ensemble by averaging whether the target variable is a numeric variable or whether we are calculating probabilities for a classification problem. In the case of class predictions, we would opt for max voting.</p>
<p class="calibre2">In <em class="calibre13">Step 1</em>, we separated our train and test samples. This is the same test sample that we used for our predictions with all the models we built in this recipe. In <em class="calibre13">Step 2</em>, we checked the shape of the <kbd class="calibre12">train</kbd> and <kbd class="calibre12">test</kbd> subsets. In <em class="calibre13">Step 3</em>, we split our test subset into target and predictor variables, and then checked the shape again in <em class="calibre13">Step 4</em> to ensure we got the right split.</p>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we used the Keras library to build our neural network models. We initialized two variables, <kbd class="calibre12">ensemble</kbd> and <kbd class="calibre12">frac</kbd>. We used the <kbd class="calibre12">ensemble</kbd> variable to run a <kbd class="calibre12">for</kbd> loop for a certain number of iterations (in our case, we set it to <kbd class="calibre12">200</kbd>). We then used the <kbd class="calibre12">frac</kbd> variable to assign the proportion of data we took for our bootstrap samples from the training subset. In our example, we set <kbd class="calibre12">frac</kbd> to <kbd class="calibre12">0.8</kbd>.</p>
<p class="calibre2">In <em class="calibre13">Step 5</em>, within the <kbd class="calibre12">for...loop</kbd> iteration, we built multiple neural network models and applied the models to our test subset to get the predictions. We created sequential models by passing a list of layers using the <kbd class="calibre12">add()</kbd> method. In the first layer, we specified the input dimensions using the <kbd class="calibre12">input_dim</kbd> argument. Because we have 24 input dimensions, we set <kbd class="calibre12">input_dim</kbd> to <kbd class="calibre12">24</kbd>. We also mentioned the <kbd class="calibre12">Activation</kbd> function to use in each layer by setting the <kbd class="calibre12">Activation</kbd> argument. </p>
<p class="calibre2">You can also set the <kbd class="calibre12">Activation</kbd> function through an <kbd class="calibre12">Activation</kbd> layer, as follows:</p>
<pre class="calibre15"># Example code to set activation function through the activation layer<br class="title-page-name"/><br class="title-page-name"/>from keras.layers import Activation, Dense <br class="title-page-name"/><br class="title-page-name"/>model.add(Dense(64)) <br class="title-page-name"/>model.add(Activation('tanh'))</pre>
<p class="calibre2">In this step, before we build our model, we configure the learning process using the <kbd class="calibre12">compile</kbd> method. The <kbd class="calibre12">compile</kbd> method takes the mandatory <kbd class="calibre12">loss function</kbd>, the mandatory <kbd class="calibre12">optimizer</kbd>, and the optional <kbd class="calibre12">metrics</kbd> as an argument.</p>
<p class="calibre2">The <kbd class="calibre12">optimizer</kbd> argument can take values such as <strong class="calibre4">S<span class="calibre5">tochastic Gradient Descent </span></strong>(<strong class="calibre4"><span class="calibre5">SGD</span></strong><span class="calibre5">)</span>, <kbd class="calibre12">RMSprop</kbd>, <kbd class="calibre12">Adagrad</kbd>, <kbd class="calibre12">Adadelta</kbd>, <kbd class="calibre12">Adam</kbd>, <kbd class="calibre12">Adamax</kbd>, or <kbd class="calibre12">Nadam</kbd>. </p>
<p class="calibre2"/>
<p class="calibre2"><kbd class="calibre12">loss function</kbd> can take values such as <kbd class="calibre12">mean_squared_error</kbd>, <kbd class="calibre12">mean_absolute_error</kbd>, <kbd class="calibre12">mean_absolute_percentage_error</kbd>, <kbd class="calibre12">mean_squared_logarithmic_error</kbd>, <kbd class="calibre12">squared_hinge</kbd>, <kbd class="calibre12">categorical_hinge</kbd>, or <kbd class="calibre12">binary_crossentropy</kbd>. More details are available at <a href="https://keras.io/losses/" class="calibre9">https://keras.io/losses/</a>.</p>
<p class="calibre2">We also keep adding the predictions array to an array variable, called <kbd class="calibre12">predictions_total</kbd>, using the <kbd class="calibre12">np.add()</kbd> method.</p>
<p class="calibre2">Once we finished all the iterations in the <kbd class="calibre12">for</kbd> loop in <em class="calibre13">Step 5</em>, we divided the summation of predictions by the number of iterations, which is held in the <kbd class="calibre12">ensemble</kbd> variable and set to <kbd class="calibre12">200</kbd>, to get the average predictions. We used the average predictions to calculate the MSE of the ensemble result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">If you have high computational requirements, you can use Google Colaboratory. <span class="calibre5">Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.</span> It's a free cloud service that supports free GPU. You can use Google Colab to build your deep learning applications using TensorFlow, Keras, PyTorch, and OpenCV.</p>
<p class="calibre2">Once you create your account with <a href="https://colab.research.google.com/" class="calibre9">https://colab.research.google.com/</a>, you can log in using your credentials.</p>
<p class="calibre2">Once you're logged in, you can <span class="calibre5">move</span><span class="calibre5"> </span><span class="calibre5">straight to the</span> <kbd class="calibre12">File</kbd> <span class="calibre5">menu to create your Python notebook:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter87" src="assets/931b17de-964b-4ef9-a5ca-fbefd119a528.png"/></p>
<p class="calibre2">Once you click on the <span class="calibre5">File</span> tab, you'll see <span class="calibre5">New Python 3 notebook</span>; a new notebook is created that supports Python 3.</p>
<p class="calibre2">You can click on <span class="calibre5">Untitled0.ipynb</span> in the top-left corner to rename the file:</p>
<p class="CDPAlignCenter"><img class="aligncenter88" src="assets/b6b14a5c-e286-4f02-93f5-b9c58805fef2.png"/></p>
<p class="calibre2">Go to <span class="calibre5">Edit</span> and then <span class="calibre5">Notebook settings.</span> A window pops up to indicate the different settings that you can have:</p>
<p class="CDPAlignCenter"><img class="aligncenter89" src="assets/1c957538-b972-4980-a900-d69544ed9fda.png"/></p>
<p class="calibre2"><span class="calibre5">Choose the <strong class="calibre4">Graphics Processing Unit</strong> (</span><strong class="calibre4">GPU</strong><span class="calibre5">)</span> <span class="calibre5">option as the <span class="calibre5">Hardware accelerator</span>, as shown in the preceding screenshot, in order to use the free GPU.</span></p>
<p class="calibre2"><span class="calibre5">One neat thing about Google Colab is it can work on your own Google Drive. </span>You can choose to create your own folder in your Google Drive or use the default Colab Notebooks folder. In order to use the default Google Colab Notebooks folder, follow the steps shown in the following screenshot:</p>
<p class="CDPAlignCenter"><img class="aligncenter90" src="assets/b21d3679-750f-4335-9a7f-74af17b93341.png"/></p>
<p class="calibre2">To start reading your datasets, you can store them in folders in Google Drive.</p>
<p class="calibre2">After you have logged in to Google Colab and created a new notebook, you will have to mount the drive by executing the following code in your notebook:</p>
<pre class="calibre15">from google.colab import drive<br class="title-page-name"/><br class="title-page-name"/># This can be your folder path as per your drive<br class="title-page-name"/>drive.mount('/content/drive')</pre>
<p class="calibre2">When the preceding code is run, it will ask for the authorization code to be entered, as shown here:</p>
<p class="CDPAlignCenter"><img class="aligncenter91" src="assets/4cad1459-aa85-4a26-92a2-e1ba88e9d95e.png"/></p>
<p class="calibre2">Click on the preceding URL to get an authorization code:</p>
<p class="CDPAlignCenter"><img class="aligncenter92" src="assets/e49f2e8c-79d1-4314-abf4-74b6df76ed6e.png"/></p>
<p class="calibre2">Paste the <span class="calibre5">authorization code</span> into the text box. You'll get a different authorization code each time. Upon authorization, the drive is mounted.</p>
<p class="calibre2">Once the drive is mounted, you can read <kbd class="calibre12">.csv</kbd> file using <kbd class="calibre12">pandas</kbd>, as we showed earlier in the chapter. The rest of the code, as shown in the <em class="calibre13">How to do it</em> section, runs as it is. If you use the GPU, you'll notice that there is a substantial increase in the speed of your computational performance.</p>
<div class="packtinfobox">In order to install additional libraries in Google Colab, you'll need to run the <kbd class="calibre19">pip install</kbd> command with a ! sign before it. For example, you can run <kbd class="calibre19">!pip install utils</kbd> to install utils in the Google Colab instance.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="calibre2">There are various activation functions available for use with the Keras library:</p>
<ul class="calibre10">
<li class="calibre11">Softmax activation function </li>
<li class="calibre11">Exponential linear unit</li>
<li class="calibre11"><span>Scaled exponential linear unit </span></li>
<li class="calibre11">Softplus activation function</li>
<li class="calibre11">Rectified linear unit</li>
<li class="calibre11"><span>Hyperbolic tangent activation function</span></li>
<li class="calibre11">Sigmoid activation function</li>
<li class="calibre11"><span>Linear activation function</span></li>
<li class="calibre11"><span>Exponential activation function</span></li>
</ul>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">For more information about the preceding activation functions, visit <a href="https://keras.io/activations/" class="calibre9">https://keras.io/activations/</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An ensemble of homogeneous models for handwritten digit classification</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this example, we will use a dataset called The <strong class="calibre4">Street View House Numbers</strong> (<strong class="calibre4">SVHN</strong>) dataset from <a href="http://ufldl.stanford.edu/housenumbers/" class="calibre9">http://ufldl.stanford.edu/housenumbers/</a>. The dataset is also provided in the GitHub in <kbd class="calibre12">.hd5f</kbd> format.</p>
<p class="calibre2">This dataset is a real-world dataset and<span class="calibre5"> is obtained from house numbers in Google Street View images.</span></p>
<p class="calibre2">We use Google Colab to train our models. In the first phase, we build a single model using Keras. In the second phase, we ensemble multiple homogeneous models and ensemble the results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">The dataset has 60,000 house number images. Each image is labeled between 1 and 10. Digit 1 is labelled as 1, digit 9 is labelled as 9, and digit 0 is labelled as 10. The images are <span class="calibre5">32 x 32 images centered around a single character. In some cases, we can see the images are visually indistinct.</span></p>
<p class="CDPAlignLeft3">We import the required libraries:</p>
<pre class="calibre15">import os<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from numpy import array<br class="title-page-name"/><br class="title-page-name"/>from sklearn.metrics import accuracy_score<br class="title-page-name"/><br class="title-page-name"/>from keras.models import Sequential, load_model<br class="title-page-name"/>from keras.layers.core import Dense, Dropout, Activation<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">We mount the Google Drive:</p>
<pre class="calibre15">from google.colab import drive<br class="title-page-name"/>drive.mount('/content/drive')</pre>
<p class="calibre2">Now, we import a library called <kbd class="calibre12">h5py</kbd> to read the HDF5 format file and our data file, which is called <kbd class="calibre12">SVHN_single_grey.h5</kbd>:</p>
<pre class="calibre15">import h5py<br class="title-page-name"/><br class="title-page-name"/># Open the file as readonly<br class="title-page-name"/>h5f = h5py.File('/content/drive/My Drive/DLCP/SVHN_single_grey.h5', 'r')</pre>
<p class="calibre2">We load the training and test subsets and close the file:</p>
<pre class="calibre15"># Load the training and test set<br class="title-page-name"/>x_train = h5f['X_train'][:]<br class="title-page-name"/>y_train = h5f['y_train'][:]<br class="title-page-name"/>x_test = h5f['X_test'][:]<br class="title-page-name"/>y_test = h5f['y_test'][:]<br class="title-page-name"/><br class="title-page-name"/># Close this file<br class="title-page-name"/>h5f.close()</pre>
<p class="calibre2">We reshape our train and test subsets. We also change the datatype to float:</p>
<pre class="calibre15">x_train = x_train.reshape(x_train.shape[0], 1024)<br class="title-page-name"/>x_test = x_test.reshape(x_test.shape[0], 1024)</pre>
<p class="calibre2">We now normalize our data by dividing it by 255.0. This also converts the data type of the values to float:</p>
<pre class="calibre15"># normalize inputs from 0-255 to 0-1<br class="title-page-name"/>x_train = x_train / 255.0<br class="title-page-name"/>x_test = x_test / 255.0</pre>
<p class="calibre2">We check the shape of the train and test subsets:</p>
<pre class="calibre15">print("X_train shape", x_train.shape)<br class="title-page-name"/>print("y_train shape", y_train.shape)<br class="title-page-name"/>print("X_test shape", x_test.shape)<br class="title-page-name"/>print("y_test shape", y_test.shape)</pre>
<p class="calibre2">We see that the shape of the <kbd class="calibre12">train</kbd> and <kbd class="calibre12">test</kbd> features and our target subsets are as follows:</p>
<p class="CDPAlignCenter"><img src="assets/cfdbb7d5-3001-4dc6-b046-01dde8bf8b48.png" class="calibre57"/></p>
<p class="calibre2">We visualize some of the images. We also print labels on top of the images:</p>
<pre class="calibre15"># Visualizing the 1st 10 images in our dataset<br class="title-page-name"/># along with the labels<br class="title-page-name"/>%matplotlib inline<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>plt.figure(figsize=(10, 1))<br class="title-page-name"/>for i in range(10):<br class="title-page-name"/> plt.subplot(1, 10, i+1)<br class="title-page-name"/> plt.imshow(x_train[i].reshape(32,32), cmap="gray")<br class="title-page-name"/> plt.title(y_train[i], color='r')<br class="title-page-name"/> plt.axis("off")<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">The first 10 images are shown as follows:</p>
<p class="CDPAlignCenter"><img src="assets/ecbd6145-9449-460e-9e18-1cd953487adc.png" class="calibre33"/></p>
<p class="calibre2">We now perform one-hot encoding on our target variable. We also store our <kbd class="calibre12">y_test</kbd> labels in another variable, called <kbd class="calibre12">y_test_actuals</kbd>, for later use:</p>
<pre class="calibre15"># Let us store the original y_test to another variable y_test_actuals<br class="title-page-name"/>y_test_actuals = y_test<br class="title-page-name"/><br class="title-page-name"/># one-hot encoding using keras' numpy-related utilities<br class="title-page-name"/>n_classes = 10<br class="title-page-name"/><br class="title-page-name"/>print("Before one-hot encoding:")<br class="title-page-name"/>print("Shape of Y_TRAIN before one-hot encoding: ", y_train.shape)<br class="title-page-name"/>print("Shape of Y_TEST before one-hot encoding: ", y_test.shape)<br class="title-page-name"/><br class="title-page-name"/>y_train = np_utils.to_categorical(y_train, n_classes)<br class="title-page-name"/>y_test = np_utils.to_categorical(y_test, n_classes)<br class="title-page-name"/><br class="title-page-name"/>print("After one-hot encoding:")<br class="title-page-name"/>print("Shape of Y_TRAIN after one-hot encoding: ", y_train.shape)<br class="title-page-name"/>print("Shape of Y_TRAIN after one-hot encoding: ", y_test.shape)</pre>
<p class="calibre2">The shapes before and after one-hot encoding are as follows:</p>
<p class="CDPAlignCenter"><img src="assets/81fa3c37-968c-4a24-8d35-a2a672133a44.png" class="aligncenter11"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">We'll now build a single model with the Keras library:</span></p>
<ol start="1" class="calibre14">
<li class="calibre11">Build a linear stack of layers with the sequential model:</li>
</ol>
<pre class="calibre18"># building a linear stack of layers with the sequential model<br class="title-page-name"/>model = Sequential()<br class="title-page-name"/>model.add(Dense(512, input_shape=(1024,)))<br class="title-page-name"/>model.add(Activation('relu')) <br class="title-page-name"/><br class="title-page-name"/>model.add(Dense(512))<br class="title-page-name"/>model.add(Activation('relu'))<br class="title-page-name"/><br class="title-page-name"/>model.add(Dense(10))<br class="title-page-name"/>model.add(Activation('softmax'))</pre>
<ol start="2" class="calibre14">
<li class="calibre11">Compile the model:</li>
</ol>
<pre class="calibre18"># compiling the sequential model<br class="title-page-name"/>model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Fit the model to the <kbd class="calibre12">train</kbd> data and validate it with the <kbd class="calibre12">test</kbd> data:</li>
</ol>
<pre class="calibre18"># training the model and saving metrics in history<br class="title-page-name"/>svhn_model = model.fit(x_train, y_train,<br class="title-page-name"/>          batch_size=128, epochs=100,<br class="title-page-name"/>          verbose=2,<br class="title-page-name"/>          validation_data=(x_test, y_test))</pre>
<ol start="4" class="calibre14">
<li class="calibre11">Plot the model's accuracy at every epoch:</li>
</ol>
<pre class="calibre18"># plotting the metrics<br class="title-page-name"/>fig = plt.figure(figsize=(12,4))<br class="title-page-name"/><br class="title-page-name"/>#plt.subplot(2,1,1)<br class="title-page-name"/>plt.plot(svhn_model.history['acc'])<br class="title-page-name"/>plt.plot(svhn_model.history['val_acc'])<br class="title-page-name"/>plt.title('Model Accuracy')<br class="title-page-name"/>plt.ylabel('Accuracy')<br class="title-page-name"/>plt.xlabel('Epochs')<br class="title-page-name"/>plt.legend(['Train', 'Test'], loc='uppper left')<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()</pre>
<p class="calibre20">We see the following model accuracy plot:</p>
<p class="CDPAlignCenter"><img src="assets/a5b170b0-ef5f-41cc-9915-340b5d345445.png" class="calibre33"/></p>
<p class="calibre2"/>
<ol start="5" class="calibre14">
<li class="calibre11">Plot the loss at every epoch:</li>
</ol>
<pre class="calibre18"># plotting the metrics<br class="title-page-name"/>fig = plt.figure(figsize=(12,4))<br class="title-page-name"/><br class="title-page-name"/>plt.plot(svhn_model.history['loss'])<br class="title-page-name"/>plt.plot(svhn_model.history['val_loss'])<br class="title-page-name"/>plt.title('Model Loss')<br class="title-page-name"/>plt.ylabel('Loss')<br class="title-page-name"/>plt.xlabel('Epochs')<br class="title-page-name"/>plt.legend(['Train', 'Test'], loc='upper right')<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()</pre>
<p class="calibre20">We see the following model loss plot:</p>
<p class="CDPAlignCenter"><img src="assets/480f23ae-eb8f-43f0-ab1a-8f0f7347fa55.png" class="calibre58"/></p>
<ol start="6" class="calibre14">
<li class="calibre11">Reuse the code from the scikit-learn website to plot the confusion matrix:</li>
</ol>
<pre class="calibre18"># code from http://scikit-learn.org<br class="title-page-name"/>def plot_confusion_matrix(cm, classes,<br class="title-page-name"/>normalize=False,<br class="title-page-name"/>title='Confusion matrix',<br class="title-page-name"/>cmap=plt.cm.Blues):<br class="title-page-name"/>"""<br class="title-page-name"/>This function prints and plots the confusion matrix.<br class="title-page-name"/>"""<br class="title-page-name"/>plt.imshow(cm, cmap=cmap)<br class="title-page-name"/>plt.title(title)<br class="title-page-name"/>plt.colorbar()<br class="title-page-name"/>tick_marks = np.arange(len(classes))<br class="title-page-name"/>plt.xticks(tick_marks, classes, rotation=45)<br class="title-page-name"/>plt.yticks(tick_marks, classes)<br class="title-page-name"/><br class="title-page-name"/>thresh = cm.max() / 2.<br class="title-page-name"/>for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br class="title-page-name"/>plt.text(j, i, cm[i, j],<br class="title-page-name"/>horizontalalignment="center",<br class="title-page-name"/>color="white" if cm[i, j] &gt; thresh else "black")<br class="title-page-name"/><br class="title-page-name"/>plt.ylabel('Actuals')<br class="title-page-name"/>plt.xlabel('Predicted')</pre>
<ol start="7" class="calibre14">
<li class="calibre11">Plot the confusion matrix both numerically and graphically:</li>
</ol>
<pre class="calibre18">target_names = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']<br class="title-page-name"/><br class="title-page-name"/># Formulating the Confusion Matrix<br class="title-page-name"/>import itertools<br class="title-page-name"/>from sklearn.metrics import confusion_matrix<br class="title-page-name"/><br class="title-page-name"/>cm = confusion_matrix(y_test_actuals, predicted_classes)<br class="title-page-name"/>print(cm)<br class="title-page-name"/><br class="title-page-name"/>plt.figure(figsize=(10,10))<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names, normalize=False)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">The confusion matrix appears as follows:</p>
<p class="CDPAlignCenter"><img src="assets/0744e47b-b9c7-464e-8753-4eed29c9d736.png" class="calibre59"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="8" class="calibre14">
<li class="calibre11">We'll now look at how to ensemble the results of multiple homogeneous models. Define a function to fit the model to the training data:</li>
</ol>
<pre class="calibre18"># fit model on dataset<br class="title-page-name"/>def train_models(x_train, y_train):<br class="title-page-name"/>  # building a linear stack of layers with the sequential model<br class="title-page-name"/>  model = Sequential()<br class="title-page-name"/>  model.add(Dense(512, input_shape=(1024,)))<br class="title-page-name"/>  model.add(Activation('relu')) <br class="title-page-name"/>  model.add(Dropout(0.2))<br class="title-page-name"/><br class="title-page-name"/>  model.add(Dense(512))<br class="title-page-name"/>  model.add(Activation('relu'))<br class="title-page-name"/>  model.add(Dropout(0.2))<br class="title-page-name"/><br class="title-page-name"/>  model.add(Dense(10))<br class="title-page-name"/>  model.add(Activation('softmax'))<br class="title-page-name"/>  <br class="title-page-name"/>  # compiling the sequential model<br class="title-page-name"/>  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br class="title-page-name"/>  <br class="title-page-name"/>  # training the model and saving metrics in history<br class="title-page-name"/>  svhn_model = model.fit(x_train, y_train, batch_size=32, epochs=25)<br class="title-page-name"/>  <br class="title-page-name"/>  return model</pre>
<ol start="9" class="calibre14">
<li class="calibre11">Write a function to ensemble the predictions of all the models:</li>
</ol>
<pre class="calibre18"># make an ensemble prediction for multi-class classification<br class="title-page-name"/>def ensemble_predictions(models, x_test):<br class="title-page-name"/>  # make predictions<br class="title-page-name"/>  y_predicted = [model.predict(x_test) for model in models]<br class="title-page-name"/>  y_predicted = np.array(y_predicted)<br class="title-page-name"/>  <br class="title-page-name"/>  # sum predictions from all ensemble models<br class="title-page-name"/>  predicted_total = np.sum(y_predicted, axis=0)<br class="title-page-name"/>  <br class="title-page-name"/>  # argmax across classes<br class="title-page-name"/>  result = np.argmax(predicted_total, axis=1)<br class="title-page-name"/>  <br class="title-page-name"/>  return result</pre>
<div class="packtinfobox"><span><kbd class="calibre19">numpy.argmax</kbd><strong class="calibre1"> </strong>returns indices of the max element of the array in a particular axis.</span></div>
<p class="calibre2"/>
<ol start="10" class="calibre14">
<li class="calibre11">Write a function to evaluate the models and get the accuracy score of each model:</li>
</ol>
<pre class="calibre18"># evaluate a specific number of members in an ensemble<br class="title-page-name"/>def evaluate_models(models, no_of_models, x_test, y_test):<br class="title-page-name"/> # select a subset of members<br class="title-page-name"/> subset = models[:no_of_models]<br class="title-page-name"/> <br class="title-page-name"/> # make prediction<br class="title-page-name"/> y_predicted_ensemble = ensemble_predictions(subset, x_test)<br class="title-page-name"/> <br class="title-page-name"/> # calculate accuracy<br class="title-page-name"/> return accuracy_score(y_test_actuals, y_predicted_ensemble)</pre>
<ol start="11" class="calibre14">
<li class="calibre11">Fit all the models:</li>
</ol>
<pre class="calibre18"># fit all models<br class="title-page-name"/>no_of_models = 50<br class="title-page-name"/><br class="title-page-name"/>models = [train_models(x_train, y_train) for _ in range(no_of_models)]<br class="title-page-name"/><br class="title-page-name"/># evaluate different numbers of ensembles<br class="title-page-name"/>all_scores = list()<br class="title-page-name"/>for i in range(1, no_of_models+1):<br class="title-page-name"/>  score = evaluate_models(models, i, x_test, y_test)<br class="title-page-name"/>  print("Accuracy Score of model ", i, " ", score)<br class="title-page-name"/>  all_scores.append(score)</pre>
<ol start="12" class="calibre14">
<li class="calibre11">Plot the accuracy score against each epoch:</li>
</ol>
<pre class="calibre18"># plot score vs number of ensemble members<br class="title-page-name"/>x_axis = [i for i in range(1, no_of_models+1)]<br class="title-page-name"/>plt.plot(x_axis, all_scores)<br class="title-page-name"/>plt.show()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em> to <em class="calibre13"><span class="calibre5">Step </span>7</em>, we built a single neural network model to see how to use a labelled image dataset to train our model and predict the actual label for an unseen image.</p>
<p class="calibre2">In <em class="calibre13">Step 1</em>, we <span class="calibre5">built a linear stack of layers with the sequential model using Keras. We defined the three layers: one input layer, one hidden layer, and one output layer. We provided <kbd class="calibre12">input_shape=1024</kbd> to the input layer since we have 32 x 32 images. We used the relu activation function in the first and second layers. Because ours is a multi-class classification problem, we used softmax as the activation function for our output layer. </span></p>
<p class="calibre2">In <em class="calibre13">Step 2</em>, we compiled the model with <kbd class="calibre12">loss='categorical_crossentropy'</kbd> and <kbd class="calibre12">optimizer='adam'</kbd>. In <em class="calibre13">Step 3</em>, we fitted our model to our train data and validated it on our test data.</p>
<p class="calibre2">In <em class="calibre13">Step 4</em> and<em class="calibre13"> <span class="calibre5">Step </span>5</em>, we plotted the model accuracy and the loss metric for every epoch.</p>
<p class="calibre2">In <em class="calibre13">Step 6</em> and <em class="calibre13">Step<span class="calibre5"> </span>7</em>, we reused a <kbd class="calibre12">plot_confusion_matrix()</kbd><span class="calibre5"> function</span><span class="calibre5"> </span><span class="calibre5">from the scikit-learn website to plot our confusion matrix both numerically and visually.</span></p>
<p class="calibre2">From <em class="calibre13">Step 8</em> onward, we ensembled multiple models. We wrote three custom functions:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">train_models()</kbd>: To train and compile our model using sequential layers.</li>
<li class="calibre11"><kbd class="calibre12">ensemble_predictions()</kbd>: To ensemble the predictions and find the maximum value across classes for all observations.</li>
<li class="calibre11"><kbd class="calibre12">evaluate_models()</kbd>: To calculate the accuracy score for every model.</li>
</ul>
<p class="calibre2">In <em class="calibre13">Step 11</em>, we fitted all the models. We set the <kbd class="calibre12">no_of_models</kbd> <span class="calibre5">variable</span><span class="calibre5"> </span><span class="calibre5">to <kbd class="calibre12">50</kbd>. We trained our models in a loop by calling the</span> <kbd class="calibre12">train_models()</kbd> <span class="calibre5">function</span><span class="calibre5">. We then passed</span> <kbd class="calibre12">x_train</kbd> <span class="calibre5">and</span> <kbd class="calibre12">y_train</kbd> <span class="calibre5">to the</span> <kbd class="calibre12">train_models()</kbd> <span class="calibre5">function for every model built at every iteration. We also called </span><kbd class="calibre12">evaluate_models()</kbd><span class="calibre5">, which returned the accuracy scores of each model built. We then appended all the accuracy scores. </span></p>
<p class="calibre2">In <em class="calibre13">Step 12</em>, we plotted the accuracy scores for all the models.</p>


            </article>

            
        </section>
    </body></html>