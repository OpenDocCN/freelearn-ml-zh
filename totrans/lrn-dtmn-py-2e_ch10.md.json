["```py\nCLIENT_ID = \"<Enter your Client ID here>\" \nCLIENT_SECRET = \"<Enter your Client Secret here>\"\n\n```", "```py\nUSER_AGENT = \"python:<your unique user agent> (by /u/<your reddit username>)\"\n\n```", "```py\nfrom getpass import getpass\nUSERNAME = \"<your reddit username>\" \nPASSWORD = getpass(\"Enter your reddit password:\")\n\n```", "```py\nimport requests\ndef login(username, password):\n    if password is None:\n        password = getpass.getpass(\"Enter reddit password for user {}: \".format(username))    \n    headers = {\"User-Agent\": USER_AGENT}\n    # Setup an auth object with our credentials\n    client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)\n    # Make a post request to the access_token endpoint\n    post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\n    response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth,     \n                             data=post_data, headers=headers) \n    return response.json()\n\n```", "```py\ntoken = login(USERNAME, PASSWORD)\n\n```", "```py\n{'access_token': '<semi-random string>', 'expires_in': 3600, 'scope': '*', 'token_type': 'bearer'}\n\n```", "```py\nsubreddit = \"worldnews\"\n\n```", "```py\nurl = \"https://oauth.reddit.com/r/{}\".format(subreddit)\n\n```", "```py\nheaders = {\"Authorization\": \"bearer {}\".format(token['access_token']), \n\"User-Agent\": USER_AGENT}\n\n```", "```py\nresponse = requests.get(url, headers=headers)\n\n```", "```py\nresult = response.json()\nfor story in result['data']['children']: \n    print(story['data']['title'])\n\n```", "```py\nfrom time import sleep\n\n```", "```py\ndef get_links(subreddit, token, n_pages=5):\n    stories = []\n    after = None\n    for page_number in range(n_pages):\n        # Sleep before making calls to avoid going over the API limit\n        sleep(2)\n        # Setup headers and make call, just like in the login function\n        headers = {\"Authorization\": \"bearer {}\".format(token['access_token']), \"User-Agent\": USER_AGENT} \n        url = \"https://oauth.reddit.com/r/{}?limit=100\". format(subreddit)\n        if after:\n            # Append cursor for next page, if we have one\n            url += \"&after={}\".format(after)\n        response = requests.get(url, headers=headers)\n        result = response.json()\n        # Get the new cursor for the next loop\n        after = result['data']['after']\n        # Add all of the news items to our stories list\n        for story in result['data']['children']:\n            stories.append((story['data']['title'], story['data']['url'], story['data']['score']))\n    return stories\n\n```", "```py\nstories = get_links(\"worldnews\", token)\n\n```", "```py\nimport os \ndata_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"websites\", \"raw\")\n\n```", "```py\nimport hashlib\n\n```", "```py\nnumber_errors = 0\n\n```", "```py\nfor title, url, score in stories:\n    output_filename = hashlib.md5(url.encode()).hexdigest() \n    fullpath = os.path.join(data_folder, output_filename + \".txt\")\n    try: \n        response = requests.get(url) \n        data = response.text \n        with open(fullpath, 'w') as outf: \n            outf.write(data)\n        print(\"Successfully completed {}\".format(title))\n    except Exception as e:\n        number_errors += 1\n        # You can use this to view the errors, if you are getting too many:\n        # raise\n\n```", "```py\nfilenames = [os.path.join(data_folder, filename) for filename in os.listdir(data_folder)]\n\n```", "```py\ntext_output_folder = os.path.join(os.path.expanduser(\"~\"), \"Data\", \"websites\", \"textonly\")\n\n```", "```py\nfrom lxml import etree\n\n```", "```py\nskip_node_types = [\"script\", \"head\", \"style\", etree.Comment]\n\n```", "```py\nparser = etree.HTMLParser()\n\ndef get_text_from_file(filename):\n    with open(filename) as inf:\n        html_tree = etree.parse(inf, parser) \n    return get_text_from_node(html_tree.getroot())\n\n```", "```py\ndef get_text_from_node(node):\n    if len(node) == 0: \n        # No children, just return text from this item\n        if node.text: \n            return node.text \n        else:\n            return \"\"\n    else:\n        # This node has children, return the text from it:\n        results = (get_text_from_node(child)\n                   for child in node\n                   if child.tag not in skip_node_types)\n    result = str.join(\"n\", (r for r in results if len(r) > 1))\n    if len(result) >= 100:\n        return result\n    else:\n        return \"\"\n\n```", "```py\nfor filename in os.listdir(data_folder):\n    text = get_text_from_file(os.path.join(data_folder, filename)) \n    with open(os.path.join(text_output_folder, filename), 'w') as outf: \n        outf.write(text)\n\n```", "```py\nfrom sklearn.cluster import KMeans\n\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n```", "```py\nfrom sklearn.pipeline import Pipeline\nn_clusters = 10 \npipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n                                     ('clusterer', KMeans(n_clusters=n_clusters)) ])\n\n```", "```py\ndocuments = [open(os.path.join(text_output_folder, filename)).read()\n             for filename in os.listdir(text_output_folder)]\n\n```", "```py\npipeline.fit(documents)\nlabels = pipeline.predict(documents)\n\n```", "```py\nfrom collections import Counter\nc = Counter(labels) \nfor cluster_number in range(n_clusters): \n    print(\"Cluster {} contains {} samples\".format(cluster_number, c[cluster_number]))\n\nCluster 0 contains 1 samples\nCluster 1 contains 2 samples\nCluster 2 contains 439 samples\nCluster 3 contains 1 samples\nCluster 4 contains 2 samples\nCluster 5 contains 3 samples\nCluster 6 contains 27 samples\nCluster 7 contains 2 samples\nCluster 8 contains 12 samples\nCluster 9 contains 1 samples\n\n```", "```py\npipeline.named_steps['clusterer'].inertia_\n\n```", "```py\ninertia_scores = [] \nn_cluster_values = list(range(2, 20)) \nfor n_clusters in n_cluster_values: \n    cur_inertia_scores = [] \n    X = TfidfVectorizer(max_df=0.4).fit_transform(documents) \n for i in range(10): \n        km = KMeans(n_clusters=n_clusters).fit(X) \n        cur_inertia_scores.append(km.inertia_) \n    inertia_scores.append(cur_inertia_scores)\n\n```", "```py\n%matplotlib inline\nfrom matplotlib import pyplot as plt\ninertia_means = np.mean(inertia_scores, axis=1)\ninertia_stderr = np.std(inertia_scores, axis=1)\nfig = plt.figure(figsize=(40,20))\nplt.errorbar(n_cluster_values, inertia_means, inertia_stderr, color='green')\nplt.show()\n\n```", "```py\nn_clusters = 6 \npipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n                     ('clusterer', KMeans(n_clusters=n_clusters)) ])\npipeline.fit(documents) \nlabels = pipeline.predict(documents)\n\n```", "```py\nterms = pipeline.named_steps['feature_extraction'].get_feature_names()\n\n```", "```py\nc = Counter(labels)\n\n```", "```py\nfor cluster_number in range(n_clusters): \n    print(\"Cluster {} contains {} samples\".format(cluster_number, c[cluster_number]))\n    print(\" Most important terms\")\n    centroid = pipeline.named_steps['clusterer'].cluster_centers_[cluster_number]\n    most_important = centroid.argsort()\n    for i in range(5):\n        term_index = most_important[-(i+1)]\n        print(\" {0}) {1} (score: {2:.4f})\".format(i+1, terms[term_index], centroid[term_index]))\n\n```", "```py\nfunction_words = [... list from Chapter 9 ...]\n\npipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4, stop_words=function_words)),\n                     ('clusterer', KMeans(n_clusters=n_clusters)) ])\n\n```", "```py\nX = pipeline.transform(documents)\n\n```", "```py\nfrom scipy.sparse import csr_matrix\n\n```", "```py\nimport numpy as np\ndef create_coassociation_matrix(labels):\n    rows = [] \n    cols = []\n    unique_labels = set(labels) \n    for label in unique_labels:\n        indices = np.where(labels == label)[0]\n        for index1 in indices:\n            for index2 in indices:\n                rows.append(index1)\n                cols.append(index2)\n    data = np.ones((len(rows),)) \n    return csr_matrix((data, (rows, cols)), dtype='float')\n\n```", "```py\nC = create_coassociation_matrix(labels)\n\n```", "```py\nfrom scipy.sparse.csgraph import minimum_spanning_tree\n\n```", "```py\nmst = minimum_spanning_tree(C)\n\n```", "```py\nmst = minimum_spanning_tree(-C)\n\n```", "```py\npipeline.fit(documents) \nlabels2 = pipeline.predict(documents) \nC2 = create_coassociation_matrix(labels2) \nC_sum = (C + C2) / 2\n\n```", "```py\nmst = minimum_spanning_tree(-C_sum) \nmst.data[mst.data > -1] = 0\n\n```", "```py\nfrom scipy.sparse.csgraph import connected_components \nnumber_of_clusters, labels = connected_components(mst)\n\n```", "```py\nfrom sklearn.base import BaseEstimator, ClusterMixin\nclass EAC(BaseEstimator, ClusterMixin):\n    def __init__(self, n_clusterings=10, cut_threshold=0.5, n_clusters_range=(3, 10)): \n        self.n_clusterings = n_clusterings\n        self.cut_threshold = cut_threshold\n        self.n_clusters_range = n_clusters_range\n\n    def fit(self, X, y=None):\n        C = sum((create_coassociation_matrix(self._single_clustering(X))\n                 for i in range(self.n_clusterings)))\n        mst = minimum_spanning_tree(-C)\n        mst.data[mst.data > -self.cut_threshold] = 0\n        mst.eliminate_zeros()\n        self.n_components, self.labels_ = connected_components(mst)\n        return self\n\n    def _single_clustering(self, X):\n        n_clusters = np.random.randint(*self.n_clusters_range)\n        km = KMeans(n_clusters=n_clusters)\n        return km.fit_predict(X)\n\n    def fit_predict(self, X):\n        self.fit(X)\n        return self.labels_\n\n```", "```py\npipeline = Pipeline([('feature_extraction', TfidfVectorizer(max_df=0.4)),\n                     ('clusterer', EAC()) ])\n\n```", "```py\nvec = TfidfVectorizer(max_df=0.4) \nX = vec.fit_transform(documents)\n\n```", "```py\nfrom sklearn.cluster import MiniBatchKMeans \nmbkm = MiniBatchKMeans(random_state=14, n_clusters=3)\n\n```", "```py\nbatch_size = 10 \nfor iteration in range(int(X.shape[0] / batch_size)): \n    start = batch_size * iteration \n    end = batch_size * (iteration + 1) \n    mbkm.partial_fit(X[start:end])\n\n```", "```py\nlabels = mbkm.predict(X)\n\n```", "```py\nclass PartialFitPipeline(Pipeline):\n    def partial_fit(self, X, y=None):\n        Xt = X\n        for name, transform in self.steps[:-1]:\n            Xt = transform.transform(Xt)\n        return self.steps[-1][1].partial_fit(Xt, y=y)\n\n```", "```py\nfrom sklearn.feature_extraction.text import HashingVectorizer\n\npipeline = PartialFitPipeline([('feature_extraction', HashingVectorizer()),\n                               ('clusterer', MiniBatchKMeans(random_state=14, n_clusters=3)) ])\nbatch_size = 10 \nfor iteration in range(int(len(documents) / batch_size)): \n    start = batch_size * iteration end = batch_size * (iteration + 1)\n    pipeline.partial_fit(documents[start:end]) \nlabels = pipeline.predict(documents)\n\n```"]