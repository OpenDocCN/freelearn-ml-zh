- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Pragmatic Data Processing and Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实用数据处理和分析
- en: Data needs to be analyzed, transformed, and processed first before using it
    when training **machine learning** (**ML**) models. In the past, data scientists
    and ML practitioners had to write custom code from scratch using a variety of
    libraries, frameworks, and tools (such as **pandas** and **PySpark**) to perform
    the needed analysis and processing work. The custom code prepared by these professionals
    often needed tweaking since different variations of the steps programmed in the
    data processing scripts had to be tested on the data before being used for model
    training. This takes up a significant portion of an ML practitioner’s time, and
    since this is a manual process, it is usually error-prone as well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 **机器学习**（**ML**）模型之前，需要先分析、转换和处理数据。在过去，数据科学家和机器学习从业者必须从头开始编写自定义代码，使用各种库、框架和工具（如
    **pandas** 和 **PySpark**）来执行所需的分析和处理工作。这些专业人士准备的自定义代码通常需要调整，因为数据处理脚本中编程的步骤的不同变体必须在用于模型训练之前在数据上测试。这占据了机器学习从业者的大量时间，而且由于这是一个手动过程，通常也容易出错。
- en: One of the more practical ways to process and analyze data involves the usage
    of no-code or low-code tools when loading, cleaning, analyzing, and transforming
    the raw data from different data sources. Using these types of tools will significantly
    speed up the process since we won’t need to worry about coding the data processing
    scripts from scratch. In this chapter, we will use **AWS Glue DataBrew** and **Amazon
    SageMaker Data Wrangler** to load, analyze, and process a sample dataset. After
    cleaning, processing, and transforming the data, we will download and inspect
    the results in an **AWS CloudShell** environment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 处理和分析数据的一种更实际的方法是在加载数据、清洗、分析和转换来自不同数据源的原生数据时使用无代码或低代码工具。使用这些类型的工具将显著加快处理过程，因为我们不需要从头编写数据处理脚本。在本章中，我们将使用
    **AWS Glue DataBrew** 和 **Amazon SageMaker Data Wrangler** 来加载数据、分析和处理一个示例数据集。在清理、处理和转换数据后，我们将在
    **AWS CloudShell** 环境中下载并检查结果。
- en: 'That said, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们将涵盖以下主题：
- en: Getting started with data processing and analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始数据处理和分析
- en: Preparing the essential prerequisites
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备基本先决条件
- en: Automating data preparation and analysis with AWS Glue DataBrew
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Glue DataBrew 自动化数据准备和分析
- en: Preparing ML data with Amazon SageMaker Data Wrangler
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Data Wrangler 准备机器学习数据
- en: While working on the hands-on solutions in this chapter, you will notice that
    there are several similarities when using **AWS Glue DataBrew** and **Amazon SageMaker
    Data Wrangler**, but of course, you will notice several differences as well. Before
    we dive straight into using and comparing these services, let’s have a short discussion
    first regarding data processing and analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的动手解决方案中工作期间，您会注意到在使用 **AWS Glue DataBrew** 和 **Amazon SageMaker Data Wrangler**
    时存在一些相似之处，但当然，您也会注意到一些差异。在我们直接使用和比较这些服务之前，让我们首先就数据处理和分析进行简短讨论。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before we start, it is important that we have the following ready:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，确保我们准备好了以下内容：
- en: A web browser (preferably Chrome or Firefox)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器（最好是 Chrome 或 Firefox）
- en: Access to the AWS account used in the first four chapters of the book
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问书中前四章使用的 AWS 账户
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每章使用的 Jupyter 笔记本、源代码和其他文件都存放在这个仓库中：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。
- en: Important Note
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure to sign out and NOT use the IAM user created in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079),
    *Serverless Data Management on AWS*. In this chapter, you should use the root
    account or a new IAM user with a set of permissions to create and manage the **AWS
    Glue DataBrew**, **Amazon S3**, **AWS CloudShell**, and **Amazon SageMaker** resources.
    It is recommended to use an IAM user with limited permissions instead of the root
    account when running the examples in this book. We will discuss this along with
    other security best practices in further detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 确保注销并**不使用**在[*第 4 章*](B18638_04.xhtml#_idTextAnchor079)，*AWS 上的无服务器数据管理*中创建的
    IAM 用户。在本章中，你应该使用根账户或具有一组权限的新 IAM 用户来创建和管理 **AWS Glue DataBrew**、**Amazon S3**、**AWS
    CloudShell** 和 **Amazon SageMaker** 资源。在运行本书中的示例时，建议使用具有有限权限的 IAM 用户而不是根账户。我们将在[*第
    9 章*](B18638_09.xhtml#_idTextAnchor187)，*安全、治理和合规策略*中进一步详细讨论这一点以及其他安全最佳实践。
- en: Getting started with data processing and analysis
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始数据处理和分析
- en: 'In the previous chapter, we utilized a data warehouse and a data lake to store,
    manage, and query our data. Data stored in these data sources generally must undergo
    a series of data processing and data transformation steps similar to those shown
    in *Figure 5.1* before it can be used as a training dataset for ML experiments:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们利用数据仓库和数据湖来存储、管理和查询我们的数据。存储在这些数据源中的数据通常必须经过一系列类似于 *图 5.1* 中所示的数据处理和转换步骤，才能用作
    ML 实验的训练数据集：
- en: '![Figure 5.1 – Data processing and analysis ](img/B18638_05_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 数据处理和分析](img/B18638_05_001.jpg)'
- en: Figure 5.1 – Data processing and analysis
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 数据处理和分析
- en: 'In *Figure 5.1*, we can see that these data processing steps may involve merging
    different datasets, along with cleaning, converting, analyzing, and transforming
    the data using a variety of options and techniques. In practice, data scientists
    and ML engineers generally spend a lot of hours cleaning the data and getting
    it ready for use in ML experiments. Some professionals may be used to writing
    and running custom Python or R scripts to perform this work. However, it may be
    more practical to use no-code or low-code solutions such as AWS Glue DataBrew
    and Amazon SageMaker Data Wrangler when dealing with these types of requirements.
    For one thing, these solutions are more convenient to use since we won’t need
    to worry about managing the infrastructure, as well as coding the data processing
    scripts from scratch. We would also be using an easy-to-use visual interface that
    will help speed up the work significantly. Monitoring and security management
    are easier as well since these are integrated with other AWS services such as
    the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.1* 中，我们可以看到这些数据处理步骤可能涉及合并不同的数据集，以及使用各种选项和技术进行数据清理、转换、分析和转换。在实践中，数据科学家和
    ML 工程师通常花费大量时间清理数据，使其准备好用于 ML 实验的使用。一些专业人士可能习惯于编写和运行定制的 Python 或 R 脚本来执行这项工作。然而，当处理这些类型的要求时，使用无代码或低代码解决方案，如
    AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler，可能更为实用。一方面，这些解决方案更易于使用，因为我们不需要担心管理基础设施，也不需要从头编写数据处理脚本。我们还将使用易于使用的可视化界面，这将显著加快工作速度。监控和安全管理也更容易，因为这些功能与以下
    AWS 服务集成在一起：
- en: '**AWS Identity and Access Management** (**IAM**) – for controlling and limiting
    access to AWS services and resources'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS 身份和访问管理**（**IAM**）- 用于控制和限制对 AWS 服务和资源的访问'
- en: '**Amazon Virtual Private Cloud** (**VPC**) – for defining and configuring a
    logically isolated network that dictates how resources are accessed and how each
    resource communicates with the others within the network'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊虚拟私有云**（**VPC**）- 用于定义和配置一个逻辑上隔离的网络，该网络决定了资源如何访问以及网络内每个资源如何与其他资源通信。'
- en: '**Amazon CloudWatch** – for monitoring the performance and managing the logs
    of the resources used'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊云监控**（**CloudWatch**）- 用于监控资源的性能和管理使用的日志'
- en: '**AWS CloudTrail** – for monitoring and auditing account activity'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS CloudTrail** – 用于监控和审计账户活动'
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on how these services are used to secure and manage the
    resources in the AWS account, feel free to check out [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何使用这些服务来确保和管理 AWS 账户中的资源，更多信息请参阅[*第 9 章*](B18638_09.xhtml#_idTextAnchor187)，*安全、治理和合规策略*。
- en: 'It is important to note that there are also other options in AWS that can help
    us when processing and analyzing our data. These include the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，AWS中还有其他选项可以帮助我们在处理和分析数据时。这些包括以下内容：
- en: '**Amazon Elastic MapReduce** (**EMR**) and **EMR Serverless** – for large-scale
    distributed data processing workloads using a variety of open source tools such
    as Apache Spark, Apache Hive, and Presto'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic MapReduce**（**EMR**）和**EMR Serverless** – 用于使用Apache Spark、Apache
    Hive和Presto等多种开源工具进行大规模分布式数据处理工作负载'
- en: '**Amazon Kinesis** – for processing and analyzing real-time streaming data'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Kinesis** – 用于处理和分析实时流数据'
- en: '**Amazon QuickSight** – for enabling advanced analytics and self-service business
    intelligence'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon QuickSight** – 用于启用高级分析和自助式商业智能'
- en: '**AWS Data Pipeline** – for processing and moving data across a variety of
    services (for example, **Amazon S3**, **Amazon Relational Database Service**,
    and **Amazon DynamoDB**) using features that help with the scheduling, dependency
    tracking, and error handling of custom pipeline resources'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS数据管道** – 用于跨各种服务（例如，**Amazon S3**、**Amazon关系数据库服务**和**Amazon DynamoDB**）处理和移动数据，使用有助于自定义管道资源调度、依赖关系跟踪和错误处理的特性'
- en: '**SageMaker Processing** – for running custom data processing and analysis
    scripts (including bias metrics and feature importance computations) on top of
    the managed infrastructure of AWS with SageMaker'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker处理** – 在SageMaker管理的AWS基础设施上运行自定义数据处理和分析脚本（包括偏差指标和特征重要性计算）'
- en: Note that this is not an exhaustive list and there are more services and capabilities
    that can be used for these types of requirements. *What’s the advantage of using
    these services?* When dealing with relatively small datasets, performing data
    analysis and transformations on our local machine may do the trick. However, once
    we need to work with much larger datasets, we may need to use a more dedicated
    set of resources with more computing power, as well as features that allow us
    to focus on the work we need to do.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这并不是一个详尽的列表，还有更多服务和功能可以用于这些类型的需求。*使用这些服务的优势是什么？*当处理相对较小的数据集时，在我们的本地机器上执行数据分析和处理可能就足够了。然而，一旦我们需要处理更大的数据集，我们可能需要使用更专业的资源集，这些资源集拥有更多的计算能力，以及允许我们专注于我们需要完成的工作的功能。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: We will discuss bias detection and feature importance in more detail in [*Chapter
    9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and Compliance Strategies*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第9章*](B18638_09.xhtml#_idTextAnchor187)“安全、治理和合规策略”中更详细地讨论偏差检测和特征重要性。
- en: 'In this chapter, we will focus on AWS Glue DataBrew and Amazon SageMaker Data
    Wrangler and we will show a few examples of how to use these when processing and
    analyzing our data. We will start with a “dirty” dataset (containing a few rows
    with invalid values) and perform the following types of transformations, analyses,
    and operations on this dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注AWS Glue DataBrew和Amazon SageMaker Data Wrangler，并展示一些如何在处理和分析数据时使用这些工具的示例。我们将从一个“脏”数据集（包含一些包含无效值的行）开始，并对该数据集执行以下类型的转换、分析和操作：
- en: Running a data profiling job that analyzes the dataset
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行一个分析数据集的数据概要作业
- en: Filtering out rows that contain invalid values
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉包含无效值的行
- en: Creating a new column from an existing one
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有列创建新列
- en: Exporting the results after the transformations have been applied
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用转换后导出结果
- en: Once the file containing the processed results has been uploaded to the output
    location, we will verify the results by downloading the file and checking whether
    the transformations have been applied.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦包含处理结果的文件已上传到输出位置，我们将通过下载文件并检查是否已应用转换来验证结果。
- en: Preparing the essential prerequisites
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备必要的先决条件
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with the hands-on solutions of this chapter:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，在继续本章的动手解决方案之前，我们将确保以下先决条件已准备就绪：
- en: The Parquet file to be analyzed and processed
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要分析和处理Parquet文件
- en: The S3 bucket where the Parquet file will be uploaded
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Parquet文件上传到的S3存储桶
- en: Downloading the Parquet file
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载Parquet文件
- en: In this chapter, we will work with a similar `bookings` dataset as the one used
    in previous chapters. However, the source data is stored in a Parquet file this
    time, and we have modified some of the rows so that the dataset will have dirty
    data. That said, let’s download the `synthetic.bookings.dirty.parquet` file onto
    our local machine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用与之前章节中使用的类似 `bookings` 数据集。然而，这次源数据存储在一个 Parquet 文件中，并且我们对一些行进行了修改，以便数据集将包含脏数据。因此，让我们将
    `synthetic.bookings.dirty.parquet` 文件下载到我们的本地机器上。
- en: 'You can find it here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到它：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet).
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that storing data using the Parquet format is preferable to storing data
    using the CSV format. Once you need to work with much larger datasets, the difference
    in the file sizes of generated Parquet and CSV files becomes apparent. For example,
    a 1 GB CSV file may end up as just 300 MB (or even less) as a Parquet file! For
    more information on this topic, feel free to check the following link: [https://parquet.apache.org/docs/](https://parquet.apache.org/docs/).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用 Parquet 格式存储数据比使用 CSV 格式存储数据更可取。一旦您需要处理更大的数据集，生成的 Parquet 和 CSV 文件大小的差异就会变得明显。例如，一个
    1 GB 的 CSV 文件最终可能只有 300 MB（甚至更少）作为 Parquet 文件！有关此主题的更多信息，请随时查看以下链接：[https://parquet.apache.org/docs/](https://parquet.apache.org/docs/).
- en: Make sure to download the `synthetic.bookings.dirty.parquet` file to your local
    machine before proceeding.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续操作之前，请确保将 `synthetic.bookings.dirty.parquet` 文件下载到您的本地机器上。
- en: Preparing the S3 bucket
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备 S3 存储桶
- en: You can create a new S3 bucket for the hands-on solutions in this chapter or
    you can reuse an existing one that was created in previous chapters. This S3 bucket
    will be used to store both the `synthetic.bookings.dirty.parquet` source file
    and the output destination results after running the data processing and transformation
    steps using AWS Glue DataBrew and Amazon SageMaker Data Wrangler.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为本章的动手解决方案创建一个新的 S3 存储桶，或者您可以使用之前章节中创建的现有存储桶。这个 S3 存储桶将用于存储 `synthetic.bookings.dirty.parquet`
    源文件以及使用 AWS Glue DataBrew 和 Amazon SageMaker Data Wrangler 运行数据处理和转换步骤后的输出目标结果。
- en: Once both prerequisites are ready, we can proceed with using AWS Glue DataBrew
    to analyze and process our dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦准备就绪，我们就可以使用 AWS Glue DataBrew 来分析和处理我们的数据集。
- en: Automating data preparation and analysis with AWS Glue DataBrew
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AWS Glue DataBrew 自动化数据准备和分析
- en: AWS Glue DataBrew is a no-code data preparation service built to help data scientists
    and ML engineers clean, prepare, and transform data. Similar to the services we
    used in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*, Glue DataBrew is *serverless* as well. This means that we won’t need
    to worry about infrastructure management when using this service to perform data
    preparation, transformation, and analysis.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue DataBrew 是一个无代码的数据准备服务，旨在帮助数据科学家和 ML 工程师清理、准备和转换数据。类似于我们在 [*第 4 章*](B18638_04.xhtml#_idTextAnchor079)
    中使用的服务，*AWS 上的无服务器数据管理*，Glue DataBrew 也是无服务器的。这意味着当使用此服务进行数据准备、转换和分析时，我们不需要担心基础设施管理。
- en: '![Figure 5.2 – The core concepts in AWS Glue DataBrew ](img/B18638_05_002.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – AWS Glue DataBrew 的核心概念](img/B18638_05_002.jpg)'
- en: Figure 5.2 – The core concepts in AWS Glue DataBrew
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – AWS Glue DataBrew 的核心概念
- en: 'In *Figure 5.2*, we can see that there are different concepts and resources
    involved when using AWS Glue DataBrew. We need to have a good idea of what these
    are before using the service. Here is a quick overview of the concepts and terms
    used:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.2* 中，我们可以看到在使用 AWS Glue DataBrew 时涉及不同的概念和资源。在使用此服务之前，我们需要对这些有一个很好的了解。以下是概念和术语的快速概述：
- en: '**Dataset** – Data stored in an existing data source (for example, **Amazon
    S3**, **Amazon Redshift**, or **Amazon RDS**) or uploaded from the local machine
    to an S3 bucket.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集** – 存储在现有数据源（例如，**Amazon S3**、**Amazon Redshift** 或 **Amazon RDS**）中的数据，或者从本地机器上传到
    S3 存储桶。'
- en: '**Recipe** – A set of data transformation or data preparation steps to be performed
    on a dataset.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配方** – 在数据集上执行的数据转换或数据准备步骤的集合。'
- en: '**Job** – The process of running certain instructions to profile or transform
    a dataset. Jobs that are used to evaluate a dataset are called **profile jobs**.
    On the other hand, jobs that are used to run a set of instructions to clean, normalize,
    and transform data are called **recipe jobs**. We can use a view called a **data
    lineage** to keep track of the transformation steps that a dataset has been through,
    along with the origin and destination configured in a job.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业** – 运行某些指令以分析或转换数据集的过程。用于评估数据集的作业称为**分析作业**。另一方面，用于运行一组指令以清理、归一化和转换数据的作业称为**食谱作业**。我们可以使用一个称为**数据血缘**的视图来跟踪数据集所经历的转换步骤，以及作业中配置的源和目标。'
- en: '**Data profile** – A report generated after running a profile job on a dataset.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据概要** – 在数据集上运行分析作业后生成的报告。'
- en: '**Project** – A managed collection of data, transformation steps, and jobs.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**项目** – 数据、转换步骤和作业的受管理集合。'
- en: Now that we have a good idea of what the concepts and terms are, let’s proceed
    with creating a new dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对概念和术语有了很好的了解，让我们继续创建一个新的数据集。
- en: Creating a new dataset
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建新的数据集
- en: 'In the *Preparing the essential prerequisites* section of this chapter, we
    downloaded a Parquet file to our local machine. In the next set of steps, we will
    create a new dataset by uploading this Parquet file from the local machine to
    an existing Amazon S3 bucket:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的*准备基本先决条件*部分，我们下载了一个Parquet文件到本地机器。在接下来的步骤中，我们将通过从本地机器上传这个Parquet文件到现有的Amazon
    S3存储桶来创建一个新的数据集：
- en: Navigate to the **AWS Glue DataBrew** console using the search bar of the **AWS
    Management Console**.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**AWS管理控制台**的搜索栏导航到**AWS Glue DataBrew**控制台。
- en: Important Note
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed if certain resources need to be
    transferred to the region of choice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设我们在使用服务来管理和创建不同类型的资源时使用的是`us-west-2`区域。您可以使用不同的区域，但请确保如果某些资源需要转移到所选区域，则进行任何必要的调整。
- en: 'Go to the **DATASETS** page by clicking the sidebar icon highlighted in *Figure
    5.3*:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击如图5.3所示的侧边栏图标，转到**DATASETS**页面：
- en: '![Figure 5.3 – Navigating to the DATASETS page ](img/B18638_05_003.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 导航到DATASETS页面](img/B18638_05_003.jpg)'
- en: Figure 5.3 – Navigating to the DATASETS page
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 导航到DATASETS页面
- en: Click **Connect to new dataset**.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**连接到新数据集**。
- en: 'Click **File upload**, as highlighted in *Figure 5.4*:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**文件上传**，如图5.4所示：
- en: '![Figure 5.4 – Locating the File upload option ](img/B18638_05_004.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 定位文件上传选项](img/B18638_05_004.jpg)'
- en: Figure 5.4 – Locating the File upload option
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 定位文件上传选项
- en: Note that there are different ways to load data and connect to your dataset.
    We can connect and load data stored in Amazon Redshift, Amazon RDS, and AWS Glue
    using **AWS Glue Data Catalog**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有不同方式来加载数据并连接到您的数据集。我们可以使用**AWS Glue数据目录**连接并加载数据存储在Amazon Redshift、Amazon
    RDS和AWS Glue中。
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Feel free to check out [https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)
    for more information.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎查看[https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)获取更多信息。
- en: Specify `bookings` as the value for the **Dataset name** field (under **New
    dataset details**).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**数据集名称**字段的值指定为`bookings`（在**新数据集详细信息**下）。
- en: Under `synthetic.bookings.dirty.parquet` file from your local machine.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的本地机器上的`synthetic.bookings.dirty.parquet`文件下。
- en: Next, locate and click the **Browse S3** button under **Enter S3 destination**.
    Select the S3 bucket that you created in the *Preparing the essential prerequisites*
    section of [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在**输入S3目标**下找到并点击**浏览S3**按钮。选择在*准备基本先决条件*部分[*第4章*](B18638_04.xhtml#_idTextAnchor079)中创建的S3存储桶，*AWS上的无服务器数据管理*。
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the `synthetic.bookings.dirty.parquet` file in your local machine
    will be uploaded to the S3 bucket selected in this step. You may create and use
    a different S3 bucket when working on the hands-on solutions in this chapter.
    Feel free to create a new S3 bucket using the AWS Management Console or through
    AWS CloudShell using the AWS CLI.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您本地机器上的`synthetic.bookings.dirty.parquet`文件将被上传到本步骤中选择的S3存储桶。当您在本章的动手练习中工作时，您可以创建并使用不同的S3存储桶。请随意使用AWS管理控制台或通过AWS
    CloudShell使用AWS CLI创建一个新的S3存储桶。
- en: Under **Additional configurations**, make sure that the **Selected file type**
    field is set to **PARQUET**.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**附加配置**下，确保**所选文件类型**字段设置为**PARQUET**。
- en: Click the **Create dataset** button (located at the lower right of the page).
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击页面右下角的**创建数据集**按钮。
- en: 'At this point, the `bookings` dataset has been created and it should appear
    in the list of datasets as shown in *Figure 5.5*:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，`bookings`数据集已创建，并应如图5.5所示出现在数据集列表中：
- en: '![Figure 5.5 – Navigating to the Datasets preview page ](img/B18638_05_005.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 导航到数据集预览页面](img/B18638_05_005.jpg)'
- en: Figure 5.5 – Navigating to the Datasets preview page
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 导航到数据集预览页面
- en: 'With that, let’s click the `bookings` dataset name as highlighted in *Figure
    5.5*. This will redirect you to the **Dataset preview** page as shown in *Figure
    5.6*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们点击如图5.5所示的高亮显示的`bookings`数据集名称。这将带您转到如图5.6所示的**数据集预览**页面：
- en: '![Figure 5.6 – The dataset preview ](img/B18638_05_006.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 数据集预览](img/B18638_05_006.jpg)'
- en: Figure 5.6 – The dataset preview
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 数据集预览
- en: Feel free to check the **Schema**, **Text**, and **Tree** views by clicking
    the appropriate button on the upper-right side of the **Dataset preview** pane.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意通过点击**数据集预览**面板右上角的相应按钮来检查**模式**、**文本**和**树**视图。
- en: Now that we have successfully uploaded the Parquet file and created a new dataset,
    let’s proceed with creating and running a profile job to analyze the data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功上传Parquet文件并创建了一个新的数据集，让我们继续创建并运行一个概要作业来分析数据。
- en: Creating and running a profile job
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和运行概要作业
- en: Before performing any data cleaning and data transformation steps, it would
    be a good idea to analyze the data first and review the properties and statistics
    of each column in the dataset. Instead of doing this manually, we can use the
    capability of AWS Glue DataBrew to automatically generate different analysis reports
    for us. We can generate these reports automatically by running a profile job.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任何数据清洗和数据转换步骤之前，先分析数据并查看数据集中每一列的属性和统计信息是一个好主意。我们不必手动进行此操作，可以使用AWS Glue DataBrew的能力为我们自动生成不同的分析报告。我们可以通过运行概要作业自动生成这些报告。
- en: 'In the next set of steps, we will create and run a profile job to generate
    a data profile of the dataset we uploaded:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将创建并运行一个概要作业来生成我们上传的数据集的数据概览：
- en: First, click the **Data profile overview** tab to navigate to the **Data profile
    overview** page.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，点击**数据概览概述**选项卡以导航到**数据概览概述**页面。
- en: Next, click the **Run data profile** button. This will redirect you to the **Create
    job** page.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**运行数据概览**按钮。这将带您转到**创建作业**页面。
- en: On the **Create job** page, scroll down and locate the **Job output** settings
    section, and then click the **Browse** button to set the **S3 location** field
    value.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**创建作业**页面，向下滚动并定位到**作业输出**设置部分，然后点击**浏览**按钮以设置**S3位置**字段值。
- en: In the `synthetic.bookings.dirty.parquet` file was uploaded in an earlier step.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在早期步骤中已上传到`synthetic.bookings.dirty.parquet`文件。
- en: Under `mle` as the value for **New IAM role suffix**.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`mle`下作为**新IAM角色后缀**的值。
- en: Click the **Create and run job** button.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建并运行作业**按钮。
- en: Note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step may take 3 to 5 minutes to complete. Feel free to grab a cup of coffee
    or tea! Note that you may see a **1 job in progress** loading message while waiting
    for the results to appear.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要3到5分钟才能完成。请随意拿一杯咖啡或茶！请注意，在等待结果出现时，您可能会看到一条**1个作业正在进行**的加载信息。
- en: 'Once the profile job is complete, scroll down, and view the results:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦概要作业完成，向下滚动，查看结果：
- en: '![Figure 5.7 – An overview of the data profile ](img/B18638_05_007.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 数据概览概述](img/B18638_05_007.jpg)'
- en: Figure 5.7 – An overview of the data profile
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 数据概览
- en: 'You should see a set of results similar to those shown in *Figure 5.7*. Feel
    free to check the following reports generated by the profile job:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会看到一组类似于图5.7中所示的结果。请随意检查概要作业生成的以下报告：
- en: '**Summary** – shows the total number of rows, total number of columns, missing
    cells, and duplicate rows'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要** – 显示总行数、总列数、缺失单元格和重复行数'
- en: '**Correlations** – shows a correlation matrix (displaying how each of the variables
    is related)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性** – 显示相关性矩阵（显示每个变量之间的关系）'
- en: '**Compare value distribution** – shows a comparative view of the distributions
    across columns'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较值分布** – 显示跨列的分布比较视图'
- en: '**Columns summary** – shows summary statistics for each of the columns'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列摘要** – 显示每列的摘要统计信息'
- en: Optionally, you can navigate to the **Column statistics** tab and review the
    reports in that tab as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您可以导航到**列统计**选项卡并查看该选项卡中的报告。
- en: As you can see, it took us just a couple of clicks to generate a data profile
    that can be used to analyze our dataset. Feel free to review the different reports
    and statistics generated by the profile job before proceeding with the next part
    of this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们只需点击几下就能生成一个可用于分析数据集的数据概要。在继续本章下一部分之前，请随意查看由概要作业生成的不同报告和统计数据。
- en: Creating a project and configuring a recipe
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建项目和配置配方
- en: 'It is time that we create and use an AWS Glue DataBrew project. Creating a
    project involves working with a dataset and a recipe to perform the desired data
    processing and transformation work. Since we don’t have a recipe yet, a new recipe
    will be created while we are creating and configuring the project. In this section,
    we will configure a recipe that does the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候创建并使用AWS Glue DataBrew项目了。创建项目涉及处理数据集和配方以执行所需的数据处理和转换工作。由于我们还没有配方，因此在创建和配置项目的同时将创建一个新的配方。在本节中，我们将配置一个执行以下操作的配方：
- en: Filters out the rows containing invalid `children` column values
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉包含无效`children`列值的行
- en: Creates a new column (`has_booking_changes`) based on the value of an existing
    column (`booking_changes`)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于现有列（`booking_changes`）的值创建一个新的列（`has_booking_changes`）
- en: 'In the next set of steps, we will create a project and use the interactive
    user interface to configure a recipe for cleaning and transforming the data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将创建一个项目并使用交互式用户界面配置一个用于清理和转换数据的配方：
- en: In the upper-right-hand corner of the page, locate and click the **Create project
    with this dataset** button. This should redirect you to the **Create project**
    page.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面右上角，找到并点击**使用此数据集创建项目**按钮。这应该会重定向到**创建项目**页面。
- en: Under `bookings-project` as the value for the **Project name** field.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`bookings-project`作为**项目名称**字段的值。
- en: Scroll down and locate the **Role name** drop-down field under **Permissions**.
    Select the existing IAM role created in an earlier step.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并找到**权限**下的**角色名称**下拉字段。选择在早期步骤中创建的现有IAM角色。
- en: 'Click the **Create project** button afterward:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后点击**创建项目**按钮：
- en: '![Figure 5.8 – Waiting for the project to be ready ](img/B18638_05_008.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 等待项目准备就绪](img/B18638_05_008.jpg)'
- en: Figure 5.8 – Waiting for the project to be ready
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 等待项目准备就绪
- en: After clicking the **Create project** button, you should be redirected to a
    page similar to that shown in *Figure 5.8*. After creating a project, we should
    be able to use a highly interactive workspace where we can test and apply a variety
    of data transformations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**创建项目**按钮后，您应该会被重定向到一个类似于*图5.8*中所示的页面。创建项目后，我们应该能够使用一个高度交互的工作空间，在那里我们可以测试和应用各种数据转换。
- en: Note
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step may take 2 to 3 minutes to complete
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要2到3分钟才能完成
- en: 'Once the project session is ready, we’ll do a quick check of the data to find
    any erroneous entries and spot issues in the data (so that we can filter these
    out). In the left pane showing a grid view of the data, locate and scroll (either
    left or right) to the **children** column similar to what is shown in *Figure
    5.9*:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦项目会话准备就绪，我们将快速检查数据以查找任何错误条目并发现数据中的问题（以便我们可以过滤掉这些条目）。在显示数据网格视图的左侧面板中，找到并滚动（向左或向右）到类似于*图5.9*中所示的两个**children**列：
- en: '![Figure 5.9 – Filtering out the rows with invalid cell values ](img/B18638_05_009.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 过滤掉包含无效单元格值的行](img/B18638_05_009.jpg)'
- en: Figure 5.9 – Filtering out the rows with invalid cell values
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 过滤掉包含无效单元格值的行
- en: We should see the `children` column between the `adults` and `babies``-1` and
    there are cells under the `children` column with a value of `-1`. Once you have
    reviewed the different values under the `children` column, click the **FILTER**
    button as highlighted in *Figure 5.9*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在`adults`和`babies`之间的`children`列之间看到`children`列，并且在该列下有值为`-1`的单元格。一旦你查看了`children`列下的不同值，点击如图5.9所示的高亮**过滤**按钮。
- en: Note
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Note that we have intentionally added a certain number of `-1` values under
    the `children` column in the Parquet file used in this chapter. Given that it
    is impossible to have a value less than `0` for the `children` column, we will
    filter out these rows in the next set of steps.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在本章使用的Parquet文件中故意在`children`列下添加了一定数量的`-1`值。鉴于`children`列的值不可能小于`0`，我们将在下一步中过滤掉这些行。
- en: After clicking the **FILTER** button, a drop-down menu should appear. Locate
    and select **Greater than or equal to** from the list of options under **By condition**.
    This should update the pane on the right-hand side of the page and show the list
    of configuration options for the **Filter values** operation.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**过滤**按钮后，应该会出现一个下拉菜单。在**条件**下的选项列表中定位并选择**大于等于**。这应该更新页面右侧的面板，并显示**过滤值**操作的配置选项列表。
- en: In the `children` column from the list of options for the `0` in the field with
    the placeholder background text of **Enter a filter value**.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在具有占位文本**输入一个过滤值**的**0**字段下的选项列表中，从`children`列中选择。
- en: 'Click **Preview changes**. This should update the left-hand pane and provide
    a grid view of the dataset:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**预览更改**。这应该更新左侧面板，并提供数据集的网格视图：
- en: '![Figure 5.10 – Previewing the results ](img/B18638_05_010.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – 预览结果](img/B18638_05_010.jpg)'
- en: Figure 5.10 – Previewing the results
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 预览结果
- en: We should see that the rows with a value of `-1` under the `children` column
    have been filtered out, similar to what is shown in *Figure 5.10*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到在`children`列下值为`-1`的行已被过滤掉，类似于*图5.10*中所示。
- en: Next, click the **Apply** button.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击**应用**按钮。
- en: 'Let’s proceed with adding a step for creating a new column (from an existing
    one). Locate and click the **Add step** button as highlighted in *Figure 5.11*:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续添加一个步骤来创建一个新列（从现有列）。定位并点击如图5.11所示的高亮**添加步骤**按钮：
- en: '![Figure 5.11 – Adding a step ](img/B18638_05_011.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 添加步骤](img/B18638_05_011.jpg)'
- en: Figure 5.11 – Adding a step
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 添加步骤
- en: The **Add step** button should be in the same row where the **Clear all** link
    is located.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**添加步骤**按钮应该位于**清除所有**链接所在的同一行。'
- en: 'Open the drop-down field with the `create` in the search field. Select the
    **Based on conditions** option from the list of results:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索字段中带有`create`的下拉字段中打开。从结果列表中选择**基于条件**选项：
- en: '![Figure 5.12 – Locating the Based on conditions option ](img/B18638_05_012.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – 定位基于条件选项](img/B18638_05_012.jpg)'
- en: Figure 5.12 – Locating the Based on conditions option
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – 定位基于条件选项
- en: If you are looking for the search field, simply refer to the highlighted box
    (at the top) in *Figure 5.12*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找搜索字段，只需参考*图5.12*中高亮的框（在顶部）。
- en: In the `booking_changes`
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`booking_changes`列中。
- en: '`Greater than`'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`大于`'
- en: '`0`'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`0`'
- en: '`True or False`'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`真或假`'
- en: '`has_booking_changes`'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`has_booking_changes`'
- en: 'Click `has_booking_changes`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`has_booking_changes`：
- en: '![Figure 5.13 – Previewing the changes ](img/B18638_05_013.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图5.13 – 预览更改](img/B18638_05_013.jpg)'
- en: Figure 5.13 – Previewing the changes
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – 预览更改
- en: As we can see in *Figure 5.13*, this new column has a value of `true` if the
    `booking_changes` column has a value greater than `0`, and `false` otherwise.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图5.13*所示，如果`booking_changes`列的值大于`0`，则新列的值为`true`，否则为`false`。
- en: Review the preview results before clicking the **Apply** button.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在点击**应用**按钮之前，请先查看预览结果。
- en: 'At this point, we should have two applied steps in our recipe. Click **Publish**,
    as highlighted in *Figure 5.14*:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该在我们的配方中有了两个应用步骤。点击**发布**，如图5.14所示：
- en: '![Figure 5.14 – Locating and clicking the Publish button ](img/B18638_05_014.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – 定位并点击发布按钮](img/B18638_05_014.jpg)'
- en: Figure 5.14 – Locating and clicking the Publish button
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 定位并点击发布按钮
- en: This should open the **Publish recipe** window.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会打开**发布配方**窗口。
- en: In the **Publish recipe** pop-up window, click **Publish**. Note that we can
    specify an optional version description when publishing the current recipe.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**发布配方**弹出窗口中，点击**发布**。请注意，在发布当前配方时，我们可以指定可选的版本描述。
- en: Now that we have a published recipe, we can proceed with creating a recipe job
    that will execute the different steps configured in the recipe.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经发布了一个配方，我们可以继续创建一个将执行配方中配置的不同步骤的配方作业。
- en: Note
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: After publishing the recipe, we still need to run a recipe job before the changes
    are applied (resulting in the generation of a new file with the applied data transformations).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布配方后，我们仍然需要在更改生效之前运行一个配方作业（这将生成一个应用了数据转换的新文件）。
- en: Creating and running a recipe job
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和运行配方作业
- en: As we can see in *Figure 5.15*, a recipe job needs to be configured with a source
    and a destination. The job reads the data stored in the source, performs the transformation
    steps configured in the associated recipe, and stores the processed files in the
    destination.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图5.15*中可以看到的，配方作业需要配置一个源和一个目标。作业读取存储在源中的数据，执行关联配方中配置的转换步骤，并将处理后的文件存储在目标中。
- en: '![Figure 5.15 – A job needs to be configured with a source and a destination
    ](img/B18638_05_015.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图5.15 – 作业需要配置源和目标](img/B18638_05_015.jpg)'
- en: Figure 5.15 – A job needs to be configured with a source and a destination
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 – 作业需要配置源和目标
- en: It is important to note that the source data is not modified since the recipe
    job only connects in a read-only manner. After the recipe job has finished processing
    all the steps, the job results are stored in one or more of the configured output
    destinations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，源数据不会被修改，因为配方作业仅以只读方式连接。配方作业完成所有步骤的处理后，作业结果将存储在配置的输出目标之一或多个中。
- en: 'In the next set of steps, we will create and run a recipe job using the recipe
    we published in the previous section:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用上一节中发布的配方创建并运行一个配方作业：
- en: Navigate to the **Recipes** page using the sidebar on the left-hand side.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用左侧边栏导航到**配方**页面。
- en: Select the row named `bookings-project-recipe` (which should toggle the checkbox
    and highlight the entire row).
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择名为`bookings-project-recipe`的行（这将切换复选框并突出显示整个行）。
- en: Click the **Create job with this recipe** button. This will redirect you to
    the **Create job** page.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**使用此配方创建作业**按钮。这将带您转到**创建作业**页面。
- en: On the `bookings-clean-and-add-column`
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`bookings-clean-and-add-column`
- en: '`Project`'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`项目`'
- en: '`bookings-project`.'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`bookings-project`。'
- en: '**Job output settings**'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**作业输出设置**'
- en: '**S3 location**: Click **Browse**. Locate and choose the same S3 bucket used
    in the previous steps in this chapter.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3位置**：单击**浏览**。找到并选择本章此步骤中使用的相同S3存储桶。'
- en: '**Permissions**:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**权限**：'
- en: '**Role name**: Choose the IAM role created in an earlier step.'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**角色名称**：选择在早期步骤中创建的IAM角色。'
- en: Note
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'We are not limited to storing the job output results in S3\. We can also store
    the output results in **Amazon Redshift**, **Amazon RDS** tables, and more. For
    more information, feel free to check the following link: [https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅限于将作业输出结果存储在S3中。我们还可以将输出结果存储在**Amazon Redshift**、**Amazon RDS**表和其他地方。有关更多信息，请随意查看以下链接：[https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)。
- en: Review the specified configuration and then click the **Create and run job**
    button. If you accidentally clicked on the **Create job** button (beside the **Create
    and run job** button), you may click the **Run job** button after the job has
    been created.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查指定的配置，然后单击**创建并运行作业**按钮。如果您意外单击了**创建作业**按钮（位于**创建并运行作业**按钮旁边），在作业创建后，您可以单击**运行作业**按钮。
- en: Note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Wait for 3 to 5 minutes for this step to complete. Feel free to grab a cup of
    coffee or tea while waiting!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 等待3到5分钟以完成此步骤。在等待时，请随意拿一杯咖啡或茶！
- en: 'Wasn’t that easy? Creating, configuring, and running a recipe job is straightforward.
    Note that we can configure this recipe job and automate the job runs by associating
    a schedule. For more information on this topic, you can check the following link:
    [https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml#jobs.scheduling).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不是很简单吗？创建、配置和运行一个配方作业是直接的。请注意，我们可以通过关联一个计划来自动化配置这个配方作业的运行。有关此主题的更多信息，您可以查看以下链接：[https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml#jobs.scheduling)。
- en: Verifying the results
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证结果
- en: 'Now, let’s proceed with inspecting the recipe job output results in AWS CloudShell,
    a free browser-based shell we can use to manage our AWS resources using a terminal.
    In the next set of steps, we will download the recipe job output results into
    the CloudShell environment and check whether the expected changes are reflected
    in the downloaded file:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续在 AWS CloudShell 中检查配方作业输出结果，AWS CloudShell 是一个基于浏览器的免费 shell，我们可以使用终端来管理我们的
    AWS 资源。在接下来的步骤中，我们将下载配方作业输出结果到 CloudShell 环境中，并检查预期的更改是否反映在下载的文件中：
- en: Once **Last job run status** of the job has changed to **Succeeded**, click
    the **1 output** link under the **Output** column. This should open the **Job
    output locations** window. Click the S3 link under the **Destination** column
    to open the S3 bucket page in a new tab.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当作业的 **最后运行状态** 变为 **成功** 后，点击 **输出** 列下的 **1 输出** 链接。这应该会打开 **作业输出位置** 窗口。点击
    **目的地** 列下的 S3 链接，在新标签页中打开 S3 存储桶页面。
- en: Use the `bookings-clean-and-add-column`. Make sure to press the *ENTER* key
    to filter the list of objects. Navigate to the `bookings-clean-and-add-column`
    and ends with `part00000`.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `bookings-clean-and-add-column` 命令。确保按下 *ENTER* 键以过滤对象列表。导航到 `bookings-clean-and-add-column`
    并以 `part00000` 结尾。
- en: Select the CSV file (which should toggle the checkbox) and then click the **Copy
    S3 URI** button.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 CSV 文件（这将切换复选框）然后点击 **复制 S3 URI** 按钮。
- en: 'Navigate to **AWS CloudShell** by clicking the icon, as highlighted in *Figure
    5.16*:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击图标，导航到 **AWS CloudShell**，如图 5.16 所示：
- en: '![Figure 5.16 – Navigating to CloudShell ](img/B18638_05_016.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 导航到 CloudShell](img/B18638_05_016.jpg)'
- en: Figure 5.16 – Navigating to CloudShell
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 导航到 CloudShell
- en: We can find this button in the upper-right-hand corner of the AWS Management
    Console. You can also use the search bar to navigate to the CloudShell console.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 AWS 管理控制台的右上角找到这个按钮。您也可以使用搜索栏导航到 CloudShell 控制台。
- en: When you see the **Welcome to AWS CloudShell** window, click the **Close** button.
    Wait for the environment to run (for about 1 to 2 minutes) before proceeding.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你看到 **欢迎使用 AWS CloudShell** 窗口时，点击 **关闭** 按钮。在继续之前，等待环境运行（大约 1 到 2 分钟）。
- en: 'Run the following commands in the CloudShell environment (after `<PASTE COPIED
    S3 URL>` with what was copied to the clipboard in an earlier step:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 CloudShell 环境中运行以下命令（在 `<PASTE COPIED S3 URL>` 处粘贴之前步骤中复制到剪贴板的内容）：
- en: '[PRE0]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This should download the output CSV file from S3 into the CloudShell environment.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该将输出 CSV 文件从 S3 下载到 CloudShell 环境中。
- en: 'Use the `head` command to inspect the first few rows of the `bookings.csv`
    file:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `head` 命令检查 `bookings.csv` 文件的前几行：
- en: '[PRE2]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This should return the first row containing the header of the CSV file, along
    with the first few records of the dataset:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该返回包含 CSV 文件标题的第一行，以及数据集的前几条记录：
- en: '![Figure 5.17 – Verifying the job results ](img/B18638_05_017.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 验证作业结果](img/B18638_05_017.jpg)'
- en: Figure 5.17 – Verifying the job results
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 验证作业结果
- en: In *Figure 5.17*, we can see that the processed dataset now includes the `has_booking_changes`
    column containing `true` or `false` values. You can inspect the CSV file further
    and verify that there are no more `-1` values under the `children` column. We
    will leave this to you as an exercise.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.17* 中，我们可以看到处理后的数据集现在包括包含 `true` 或 `false` 值的 `has_booking_changes` 列。您可以进一步检查
    CSV 文件，并验证 `children` 列下没有更多的 `-1` 值。我们将把这个留给你作为练习。
- en: Now that we’re done using AWS Glue DataBrew to analyze and process our data,
    we can proceed with using Amazon SageMaker Data Wrangler to perform a similar
    set of operations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经使用 AWS Glue DataBrew 分析和加工了我们的数据，我们可以继续使用 Amazon SageMaker Data Wrangler
    来执行类似的操作集。
- en: Important Note
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Do not forget to delete all Glue DataBrew resources (such as the recipe job,
    profile job, recipe, project, and dataset) once you are done working on the hands-on
    solutions in this chapter.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章中实际解决方案的工作后，不要忘记删除所有 Glue DataBrew 资源（例如，配方作业、配置文件作业、配方、项目和数据集）。
- en: Preparing ML data with Amazon SageMaker Data Wrangler
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Data Wrangler 准备 ML 数据
- en: 'Amazon SageMaker has a lot of capabilities and features to assist data scientists
    and ML engineers with the different ML requirements. One of the capabilities of
    SageMaker focused on accelerating data preparation and data analysis is SageMaker
    Data Wrangler:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 拥有大量功能和特性，以帮助数据科学家和 ML 工程师满足不同的 ML 需求。SageMaker 的一个功能是专注于加速数据准备和数据分析，即
    SageMaker Data Wrangler：
- en: '![Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler
    ](img/B18638_05_018.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – SageMaker Data Wrangler 中可用的主要功能](img/B18638_05_018.jpg)'
- en: Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – SageMaker Data Wrangler 中可用的主要功能
- en: 'In *Figure 5.18*, we can see what we can do with our data when using SageMaker
    Data Wrangler:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*图 5.18*](img/B18638_05_018.jpg)中，我们可以看到使用 SageMaker Data Wrangler 可以对我们的数据进行哪些操作：
- en: First, we can import data from a variety of data sources such as Amazon S3,
    Amazon Athena, and Amazon Redshift.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们可以从各种数据源导入数据，例如 Amazon S3、Amazon Athena 和 Amazon Redshift。
- en: Next, we can create a data flow and transform the data using a variety of data
    formatting and data transformation options. We can also analyze and visualize
    the data using both inbuilt and custom options in just a few clicks.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建数据流，并使用各种数据格式化和数据转换选项来转换数据。我们还可以通过内置和自定义选项在几秒钟内分析和可视化数据。
- en: Finally, we can automate the data preparation workflows by exporting one or
    more of the transformations configured in the data processing pipeline.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过导出数据处理管道中配置的一个或多个转换来自动化数据准备工作流程。
- en: SageMaker Data Wrangler is integrated into SageMaker Studio, which allows us
    to use this capability to process our data and automate our data processing workflows
    without having to leave the development environment. Instead of having to code
    everything from scratch using a variety of tools, libraries, and frameworks such
    as pandas and PySpark, we can simply use SageMaker Data Wrangler to help us prepare
    custom data flows using an interface and automatically generate reusable code
    within minutes!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Data Wrangler 集成到 SageMaker Studio 中，这使得我们可以使用这项功能来处理我们的数据，并自动化我们的数据处理工作流程，而无需离开开发环境。我们无需从头开始使用各种工具、库和框架（如
    pandas 和 PySpark）编写所有代码，只需简单地使用 SageMaker Data Wrangler 来帮助我们使用界面准备自定义数据流，并在几分钟内自动生成可重用的代码！
- en: Important Note
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure to sign out and *NOT* use the IAM user created in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079),
    *Serverless Data Management on AWS*. You should use the root account or a new
    IAM user with a set of permissions to create and manage the AWS Glue DataBrew,
    Amazon S3, AWS CloudShell, and Amazon SageMaker resources. It is recommended to
    use an IAM user with limited permissions instead of the root account when running
    the examples in this book. We will discuss this along with other security best
    practices in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security,
    Governance, and Compliance Strategies*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 确保注销并*不*使用[*第 4 章*](B18638_04.xhtml#_idTextAnchor079)“在 AWS 上无服务器数据管理”中创建的 IAM
    用户。您应该使用根账户或具有创建和管理 AWS Glue DataBrew、Amazon S3、AWS CloudShell 和 Amazon SageMaker
    资源权限的新 IAM 用户。在运行本书中的示例时，建议使用具有有限权限的 IAM 用户而不是根账户。我们将在[*第 9 章*](B18638_09.xhtml#_idTextAnchor187)“安全、治理和合规策略”中详细讨论这一点以及其他安全最佳实践。
- en: Accessing Data Wrangler
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问 Data Wrangler
- en: We need to open SageMaker Studio to access SageMaker Data Wrangler.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要打开 SageMaker Studio 以访问 SageMaker Data Wrangler。
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Make sure that you have completed the hands-on solutions in the *Getting started
    with SageMaker and SageMaker Studio* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, before proceeding. You can also update
    SageMaker Studio along with Studio Apps (in case you’re using an old version).
    For more information about this topic, you can check the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml).
    Note that the steps in this section assume that we’re using JupyterLab 3.0\. If
    you are using a different version, you may experience a few differences in terms
    of layout and user experience.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已完成了[*第 1 章*](B18638_01.xhtml#_idTextAnchor017)“在 AWS 上开始使用 SageMaker
    和 SageMaker Studio”部分中的动手练习。如果您正在使用旧版本，您还可以更新 SageMaker Studio 以及 Studio Apps。有关此主题的更多信息，您可以查看以下链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml)。请注意，本节中的步骤假设我们正在使用
    JupyterLab 3.0。如果您使用的是不同版本，您可能会在布局和用户体验方面遇到一些差异。
- en: 'In the next set of steps, we will launch SageMaker Studio and access Data Wrangler
    from the **File** menu:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将启动 SageMaker Studio 并从**文件**菜单访问 Data Wrangler：
- en: Navigate to `sagemaker studio` into the search bar of the AWS Management Console
    and selecting **SageMaker Studio** from the list of results under **Features**.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS管理控制台的搜索栏中的`sagemaker studio`，并从**功能**下的结果列表中选择**SageMaker Studio**。
- en: Important Note
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed in case certain resources need
    to be transferred to the region of choice.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设我们在使用服务管理和创建不同类型的资源时，正在使用`us-west-2`区域。您可以使用不同的区域，但请确保在需要将某些资源转移到所选区域时进行任何必要的调整。
- en: Next, click **Studio** under **SageMaker Domain** in the sidebar.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在侧边栏中点击**SageMaker Domain**下的**工作室**。
- en: 'Click **Launch app**, as highlighted in *Figure 5.19*. Select **Studio** from
    the list of drop-down options:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**启动应用**，如*图5.19*中所示。从下拉选项列表中选择**工作室**：
- en: '![Figure 5.19 – Opening SageMaker Studio ](img/B18638_05_019.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图5.19 – 打开SageMaker Studio](img/B18638_05_019.jpg)'
- en: Figure 5.19 – Opening SageMaker Studio
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 – 打开SageMaker Studio
- en: This will redirect you to **SageMaker Studio**. Wait for a few seconds for the
    interface to load.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带您到**SageMaker Studio**。等待几秒钟，直到界面加载完成。
- en: Open the `ml.m5.4xlarge` instance is being provisioned to run Data Wrangler.
    Once ready, you’ll see the **Import data** page.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开正在配置的`ml.m5.4xlarge`实例以运行Data Wrangler。一旦准备就绪，您将看到**导入数据**页面。
- en: With that, let’s proceed with importing our data in the following section.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，让我们在下一节中继续导入我们的数据。
- en: Important Note
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Once you are done with the hands-on solutions in this chapter, the `ml.m5.4xlarge`
    instance used to run Data Wrangler needs to be turned off immediately to avoid
    any additional charges. Click and locate the circle icon on the left-hand sidebar
    to show the list of running instances, apps, kernel sessions, and terminal sessions.
    Make sure to shut down all running instances under **RUNNING INSTANCES** whenever
    you are done using SageMaker Studio.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成本章的动手实践，用于运行Data Wrangler的`ml.m5.4xlarge`实例需要立即关闭，以避免产生额外费用。点击并定位左侧侧边栏上的圆形图标以显示正在运行的实例、应用、内核会话和终端会话的列表。确保在完成使用SageMaker
    Studio后，在**正在运行的实例**下关闭所有运行实例。
- en: Importing data
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入数据
- en: There are several options when importing data for use in Data Wrangler. We can
    import and load data from a variety of sources including Amazon S3, Amazon Athena,
    Amazon Redshift, Databricks (JDBC), and Snowflake.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Data Wrangler导入数据时，有多种选项。我们可以从包括Amazon S3、Amazon Athena、Amazon Redshift、Databricks（JDBC）和Snowflake在内的各种来源导入和加载数据。
- en: 'In the next set of steps, we will focus on importing the data stored in the
    Parquet file uploaded in an S3 bucket in our account:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将专注于导入存储在我们账户中S3存储桶上传的Parquet文件中的数据：
- en: On the **Import data** page (under the **Import** tab), click **Amazon S3**.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**导入数据**页面（位于**导入**选项卡下），点击**Amazon S3**。
- en: On the `synthetic.bookings.dirty.parquet` file uploaded in one of the S3 buckets
    in your AWS account.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您AWS账户中S3存储桶上传的`synthetic.bookings.dirty.parquet`文件。
- en: Important Note
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In case you skipped the *Automating data preparation and analysis with AWS Glue
    DataBrew* section of this chapter, you need to upload the Parquet file downloaded
    in the *Preparing the essential prerequisites* section of this chapter to a new
    or existing Amazon S3 bucket.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您跳过了本章的*使用AWS Glue DataBrew自动化数据准备和分析*部分，您需要将本章*准备基本先决条件*部分下载的Parquet文件上传到新的或现有的Amazon
    S3存储桶。
- en: If you see a **Preview Error** notification similar to what is shown in *Figure
    5.20*, you may remove this by opening the **File type** dropdown and choosing
    **parquet** from the list of options.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您看到一个类似于*图5.20*中所示的**预览错误**通知，您可以通过打开**文件类型**下拉菜单并从选项列表中选择**parquet**来移除它。
- en: '![Figure 5.20 – Setting the file type to Parquet ](img/B18638_05_020.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图5.20 – 设置文件类型为Parquet](img/B18638_05_020.jpg)'
- en: Figure 5.20 – Setting the file type to Parquet
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 – 设置文件类型为Parquet
- en: The **Preview Error** message should disappear after the **parquet** option
    is selected from the **File type** drop-down menu.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 选择**文件类型**下拉菜单中的**parquet**选项后，**预览错误**消息应该会消失。
- en: If you’re using JupyterLab version 3.0, the **parquet** option is already preselected.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用JupyterLab版本3.0，**parquet**选项已经预先选中。
- en: If you’re using JupyterLab version 1.0, the **csv** option may be preselected
    instead of the **parquet** option. That said, regardless of the version, we should
    set the **File type** drop-down value to **parquet**. Click the **Import** button
    located in the upper-right corner of the page. This will redirect you back to
    the **Data flow** page.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用的是JupyterLab版本1.0，**csv**选项可能被预先选中，而不是**parquet**选项。但不管版本如何，我们应该将**文件类型**下拉值设置为**parquet**。点击页面右上角的**导入**按钮。这将把你重定向回**数据流**页面。
- en: Note that we are not limited to importing from Amazon S3\. We can also import
    data from Amazon Athena, Amazon Redshift, and other data sources. You may check
    [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml)
    for more information.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不仅限于从Amazon S3导入。我们还可以从Amazon Athena、Amazon Redshift和其他数据源导入数据。你可以查看[https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml)获取更多信息。
- en: Transforming the data
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: There are many built-in options in SageMaker Data Wrangler when processing and
    transforming our data. In this chapter, we will show a quick demo of how to use
    a custom PySpark script to transform data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker Data Wrangler中处理和转换我们的数据时，有许多内置选项。在本章中，我们将展示如何使用自定义PySpark脚本来转换数据的快速演示。
- en: Note
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on the numerous data transforms available, feel free to
    check the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可用的众多数据转换的更多信息，请随时查看以下链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml)。
- en: 'In the next set of steps, we will add and configure a custom PySpark transform
    to clean and process our data:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将添加并配置一个自定义PySpark转换来清理和处理我们的数据：
- en: 'If you can see the **Data types · Transform: synthetic.bookings.dirty.parquet**
    page, navigate back to the **Data flow** page by clicking the **< Data flow**
    button located in the top-left-hand corner of the page. We’ll go back to this
    page in a bit, after having a quick look at the current configuration of the data
    flow in the next step.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你可以看到**数据类型 · 转换：synthetic.bookings.dirty.parquet**页面，通过点击页面左上角的**< 数据流**按钮导航回**数据流**页面。我们将在查看下一步中数据流当前配置的快速查看后回到这个页面。
- en: 'On the **Data flow** page, click the **+** button, as highlighted in *Figure
    5.21*. Select **Add transform** from the list of options:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**数据流**页面，点击如图5.21所示的高亮**+**按钮。从选项列表中选择**添加转换**：
- en: '![Figure 5.21 – Adding a transform ](img/B18638_05_021.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图5.21 – 添加转换](img/B18638_05_021.jpg)'
- en: Figure 5.21 – Adding a transform
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 – 添加转换
- en: In this chapter, we will only work with a single dataset. However, it is important
    to note that we can work on and merge two datasets using the **Join** option as
    we have in *Figure 5.21*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只将与一个数据集工作。然而，重要的是要注意，我们可以使用如图5.21所示的**连接**选项来处理和合并两个数据集。
- en: In the **ALL STEPS** pane on the left side of the page, click the **Add step**
    button. This should show a list of options for transforming the dataset.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面左侧的**所有步骤**面板中，点击**添加步骤**按钮。这将显示用于转换数据集的选项列表。
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using **JupyterLab 1.0**, you should see the left pane labeled as
    **TRANSFORMS** instead of **ALL STEPS**.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是**JupyterLab 1.0**，你应该看到左侧面板上标记为**TRANSFORMS**而不是**所有步骤**。
- en: Select **Custom transform** from the list of options available.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从选项列表中选择**自定义转换**。
- en: 'In **CUSTOM TRANSFORM**, enter the following code into the code editor:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**自定义转换**中，将以下代码输入到代码编辑器中：
- en: '[PRE3]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What this code block does is select and retain all rows where the value of the
    `children` column is `0` or higher, and creates a new column, `has_booking_changes,`
    that has a value of `true` if the value in the `booking_changes` column is greater
    than `0` and `false` otherwise.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码块的功能是选择并保留所有`children`列的值等于`0`或更高的行，并创建一个新列`has_booking_changes`，如果`booking_changes`列的值大于`0`，则该列的值为`true`，否则为`false`。
- en: Note
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using **JupyterLab 1.0**, you should see the left-hand pane labeled
    **CUSTOM PYSPARK** instead of **CUSTOM TRANSFORM**.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是**JupyterLab 1.0**，你应该看到左侧面板上标记为**CUSTOM PYSPARK**而不是**CUSTOM TRANSFORM**。
- en: 'Click the `has_booking_changes` column, similar to what we have in *Figure
    5.22*:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`has_booking_changes`列，类似于图5.22中的操作：
- en: '![Figure 5.22 – Previewing the changes ](img/B18638_05_022.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.22 – 预览更改](img/B18638_05_022.jpg)'
- en: Figure 5.22 – Previewing the changes
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22 – 预览更改
- en: You should find the `has_booking_changes` column beside the `total_of_special_requests`
    column (which is the leftmost column in the preview).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在**total_of_special_requests**列旁边找到**has_booking_changes**列（这是预览中最左侧的列）。
- en: Once you have finished reviewing the preview of the data, you can provide an
    optional **Name** value before clicking the **Add** button.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成数据预览的审查后，你可以在点击**添加**按钮之前提供一个可选的**名称**值。
- en: After clicking the **Add** button in the previous step, locate and click the
    **< Data flow** link (or the **Back to data flow** link) located on the upper-right-hand
    side of the page.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一步点击**添加**按钮后，找到并点击页面右上角的**<数据流**链接（或**返回数据流**链接）。
- en: Note
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to note that the steps are not executed yet, as we’re just defining
    what will run in a later step.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这些步骤尚未执行，因为我们只是在定义稍后将要运行的步骤。
- en: 'Note that we are just scratching the surface of what we can do with SageMaker
    Data Wrangler here. Here are a few other examples of the other transforms available
    as well:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里使用 SageMaker Data Wrangler 只是在触及我们能做的事情的表面。以下是一些其他可用的转换示例：
- en: Balancing the data (for example, random oversampling, random undersampling,
    and SMOTE)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡数据（例如，随机过采样、随机欠采样和SMOTE）
- en: Encoding categorical data (for example, one-hot encoding, similarity encoding)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对分类数据进行编码（例如，独热编码、相似度编码）
- en: Handling missing time series data
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失的时间序列数据
- en: Extracting features from time series data
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从时间序列数据中提取特征
- en: 'For a more complete list of transforms, feel free to check the following link:
    [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml).'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更完整的转换列表，请随意查看以下链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml)。
- en: Note
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Feel free to check the following blog post in case you’re interested in diving
    a bit deeper into how to balance data using a variety of techniques (such as random
    oversampling, random undersampling, and SMOTE): [https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/](https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对如何使用各种技术（如随机过采样、随机欠采样和SMOTE）平衡数据感兴趣，请随意查看以下博客文章：[https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/](https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/)。
- en: Analyzing the data
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据
- en: 'It is critical that we analyze the data we will use in later steps to train
    ML models. We need to have a good idea of the properties that could inadvertently
    affect the behavior and performance of the ML models trained using this data.
    There are a variety of ways to analyze a dataset and the great thing about SageMaker
    Data Wrangler is that it allows us to choose from a list of pre-built analysis
    options and visualizations, including the ones in the following list:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 分析我们将在后续步骤中用于训练机器学习模型的 数据至关重要。我们需要对可能无意中影响使用此数据训练的机器学习模型的行为和性能的属性有一个良好的了解。分析数据集有多种方法，而
    SageMaker Data Wrangler 的好处在于它允许我们从一系列预构建的分析选项和可视化中选择，包括以下列表中的选项：
- en: '**Histograms** – can be used to show the “shape” of the distribution of the
    data'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直方图**——可以用来显示数据的“形状”'
- en: '**Scatter plots** – can be used to show the relationship between two numeric
    variables (using dots representing each data point from the dataset)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**散点图**——可以用来显示两个数值变量之间的关系（使用代表数据集中每个数据点的点）'
- en: '**Table summary** – can be used to show the summary statistics of the dataset
    (for example, the number of records or the minimum and maximum values in each
    column)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格摘要**——可以用来显示数据集的摘要统计信息（例如，记录数或每列的最小值和最大值）'
- en: '**Feature importance scores** (using Quick Model) – used to analyze the impact
    of each feature when predicting a target variable'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性分数**（使用快速模型）——用于分析每个特征在预测目标变量时的影响'
- en: '**Target leakage analysis** – can be used to detect columns in the dataset
    that are strongly correlated with the column we want to predict'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标泄漏分析**——可以用来检测数据集中与我们要预测的列强相关的列'
- en: '**Anomaly detection for time series data** – can be used to detect outliers
    in time series data'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列数据的异常检测** – 可以用来检测时间序列数据中的异常值'
- en: '**Bias reports** – can be used to detect potential biases in the dataset (by
    calculating different bias metrics)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差报告** – 可以用来检测数据集中潜在的偏差（通过计算不同的偏差指标）'
- en: Note
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that this is not an exhaustive list and you may see other options when
    you are working on the hands-on portion of this section.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这并不是一个详尽的列表，当你在这个部分的实践部分工作时，你可能会看到其他选项。
- en: 'In the next set of steps, we will create an analysis and generate a bias report:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将创建一个分析和生成偏差报告：
- en: 'Click the **+** button and select **Add analysis** from the list of options,
    similar to that in *Figure 5.23*:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**+**按钮，从选项列表中选择**添加分析**，类似于*图5.23*中的操作：
- en: '![Figure 5.23 – Adding an analysis ](img/B18638_05_023.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图5.23 – 添加分析](img/B18638_05_023.jpg)'
- en: Figure 5.23 – Adding an analysis
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 – 添加分析
- en: You should see the **Create analysis** pane located on the left-hand side of
    the page.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到位于页面左侧的**创建分析**面板。
- en: Specify the following configuration options in the `Bias report`
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`偏差报告`中指定以下配置选项
- en: '`Sample analysis`'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`样本分析`'
- en: '`is_cancelled`'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`is_cancelled`'
- en: '`1`'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`1`'
- en: '`babies`'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`babies`'
- en: '`Threshold`'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`阈值`'
- en: '`1`'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`1`'
- en: Scroll down to the bottom of the page. Locate and click the **Check for bias**
    button (beside the **Save** button).
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到页面底部。定位并点击**检查偏差**按钮（在**保存**按钮旁边）。
- en: 'Scroll up and locate the bias report, similar to what is shown in *Figure 5.24*:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向上滚动并定位偏差报告，类似于*图5.24*中所示：
- en: '![Figure 5.24 – The bias report ](img/B18638_05_024.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图5.24 – 偏差报告](img/B18638_05_024.jpg)'
- en: Figure 5.24 – The bias report
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24 – 偏差报告
- en: Here, we can see that the `0.92`. This means that the dataset is highly imbalanced
    and the advantaged group (`is_cancelled = 1`) is represented at a much higher
    rate compared to the disadvantaged group (`is_cancelled = 0`).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`0.92`。这意味着数据集高度不平衡，优势组（`is_cancelled = 1`）的代表性远高于劣势组（`is_cancelled
    = 0`）。
- en: Note
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will dive deeper into the details of how bias metrics are computed and interpreted
    in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and
    Compliance Strategies*.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第9章](B18638_09.xhtml#_idTextAnchor187)“安全、治理和合规策略”的详细内容中深入了解偏差指标的计算和解释。
- en: Scroll down and click the **Save** button (beside the **Check for bias** button)
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并点击**保存**按钮（在**检查偏差**按钮旁边）
- en: Locate and click the **< Data flow** link (or the **Back to data flow** link)
    to return to the **Data flow** page.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位并点击**<数据流**链接（或**返回数据流**链接）以返回到**数据流**页面。
- en: 'In addition to bias reports, we can generate data visualizations such as histograms
    and scatter plots to help us analyze our data. We can even generate a quick model
    using the provided dataset and generate a feature importance report (with scores
    showing the impact of each feature when predicting a target variable). For more
    information, feel free to check out the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 除了偏差报告外，我们还可以生成数据可视化，如直方图和散点图，以帮助我们分析数据。我们甚至可以使用提供的数据集快速生成一个模型，并生成一个特征重要性报告（显示每个特征在预测目标变量时的作用）。更多信息，请查看以下链接：[https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml)。
- en: Exporting the data flow
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出数据流
- en: With everything ready, let’s proceed with exporting the data flow we prepared
    in the previous sections. There are different options when performing the export
    operation. This includes exporting the data to an Amazon S3 bucket. We can also
    choose to export one or more steps from the data flow to **SageMaker Pipelines**
    using a Jupyter notebook that includes the relevant blocks of code. Similarly,
    we also have the option to export the features we have prepared to **SageMaker
    Feature Store**. There’s an option to export the data flow steps directly to Python
    code as well.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 准备就绪后，让我们继续导出在前几节中准备的数据流。在执行导出操作时，有多种选择。这包括将数据导出到Amazon S3存储桶。我们还可以选择使用包含相关代码块的Jupyter笔记本，将数据流中的一个或多个步骤导出到**SageMaker
    Pipelines**。同样，我们也有将准备好的特征导出到**SageMaker Feature Store**的选项。还有一个选项可以直接将数据流步骤导出到Python代码。
- en: Note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Once the data flow steps have been exported and converted to code, the generated
    code and the Jupyter notebooks can be run to execute the different steps configured
    in the data flow. Finally, experienced ML practitioners may opt to modify the
    generated notebooks and code if needed.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据流步骤被导出并转换为代码，生成的代码和 Jupyter 笔记本就可以运行以执行数据流中配置的不同步骤。最后，经验丰富的机器学习从业者可能会选择在需要时修改生成的笔记本和代码。
- en: 'In the next set of steps, we will perform the export operation and generate
    a Jupyter notebook that will utilize a **SageMaker processing** job to process
    the data and save the results to an S3 bucket:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将执行导出操作并生成一个将利用**SageMaker 处理**作业处理数据并将结果保存到 S3 桶中的 Jupyter Notebook：
- en: 'Click the **+** button after the third box, **Python (PySpark)** (or using
    the custom name that you specified in an earlier step), as highlighted in *Figure
    5.25*, and then open the list of options under **Export to**:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三个框之后点击**+**按钮，选择**Python (PySpark)**（或使用你在早期步骤中指定的自定义名称），如*图 5.25*中所示，然后打开**导出到**下的选项列表：
- en: '![Figure 5.25 – Exporting the step ](img/B18638_05_025.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.25 – 导出步骤](img/B18638_05_025.jpg)'
- en: Figure 5.25 – Exporting the step
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25 – 导出步骤
- en: 'This should give us a list of options that includes the following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们一个包含以下选项的列表：
- en: '**Amazon S3 (via Jupyter Notebook)**'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3（通过 Jupyter Notebook）**'
- en: '**SageMaker Pipelines (via Jupyter Notebook)**'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker 流水线（通过 Jupyter Notebook）**'
- en: '**Python Code**'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 代码**'
- en: '**SageMaker Feature Store (via Jupyter Notebook)**'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker 特征存储（通过 Jupyter Notebook）**'
- en: Note
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re using JupyterLab 1.0, you will need to navigate to the **Export data
    flow** page first by clicking the **Export** tab beside the **Data Flow** tab.
    After that, you’ll need to click the third box (under **Custom PySpark**) and
    then click the **Export Step** button (which will open the drop-down list of options).
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是 JupyterLab 1.0，你首先需要通过点击**数据流**标签旁边的**导出**标签来导航到**导出数据流**页面。之后，你需要点击**自定义
    PySpark**下的第三个框，然后点击**导出步骤**按钮（这将打开选项的下拉列表）。
- en: Select **Amazon S3 (via Jupyter Notebook)** from the list of options. This should
    generate and open the **Save to S3 with a SageMaker Processing Job** Jupyter notebook.
    Note that at this point, the configured data transformations have not been applied
    yet and we would need to run the cells in the generated notebook file to apply
    the transformations.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从选项列表中选择**Amazon S3（通过 Jupyter Notebook）**。这应该会生成并打开**使用 SageMaker 处理作业保存到 S3**的
    Jupyter Notebook。请注意，在此阶段，配置的数据转换尚未应用，我们需要运行生成的笔记本文件中的单元格以应用转换。
- en: 'Locate and click the first runnable cell. Run it using the **Run the selected
    cells and advance** button, as highlighted in *Figure 5.26*:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位并点击第一个可运行的单元格。使用**运行选定的单元格并前进**按钮运行它，如*图 5.26*中所示：
- en: '![Figure 5.26 – Running the first cell ](img/B18638_05_026.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.26 – 运行第一个单元格](img/B18638_05_026.jpg)'
- en: Figure 5.26 – Running the first cell
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26 – 运行第一个单元格
- en: 'As we have in *Figure 5.26*, we can find the first runnable cell under **Inputs
    and Outputs**. You may see a “**Note: The kernel is still starting. Please execute
    this cell again after the kernel is started.**” message while waiting for the
    kernel to start.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 5.26*所示，我们可以在**输入和输出**下找到第一个可运行的单元格。在等待内核启动时，你可能会看到一个“**注意：内核仍在启动中。请在内核启动后再次执行此单元格。**”的消息。
- en: Note
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Wait for the kernel to start. This step may take around 3 to 5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells. Once you
    are done with the hands-on solutions in this chapter, the ML instance used to
    run the Jupyter Notebook cells needs to be turned off immediately to avoid any
    additional charges. Click and locate the circle icon on the left-hand sidebar
    to show the list of running instances, apps, kernel sessions, and terminal sessions.
    Make sure to shut down all running instances under **RUNNING INSTANCES** whenever
    you are done using SageMaker Studio.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 等待内核启动。这一步骤可能需要大约 3 到 5 分钟，因为正在配置机器学习实例以运行 Jupyter Notebook 单元格。一旦你完成本章的动手实践，用于运行
    Jupyter Notebook 单元格的机器学习实例需要立即关闭，以避免产生额外费用。点击并定位左侧侧边栏上的圆形图标以显示运行实例、应用程序、内核会话和终端会话的列表。确保在完成使用
    SageMaker Studio 后，关闭**正在运行实例**下的所有运行实例。
- en: Once the kernel is ready, click on the cell containing the first code block
    under `run_optional_steps` variable is set to `False`.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦内核准备好，点击在`run_optional_steps`变量设置为`False`下包含第一个代码块的单元格。
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are wondering what a SageMaker processing job is, it is a job that utilizes
    the managed infrastructure of AWS to run a script. This script is coded to perform
    a set of operations defined by the user (or creator of the script). You can check
    [https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml)
    for more information on this topic.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在 wondering 什么是 SageMaker 处理作业，它是一种利用 AWS 管理基础设施运行脚本的作业。这个脚本被编码成执行用户定义的一系列操作（或脚本的创建者）。你可以查看
    [https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml)
    了解更多关于这个主题的信息。
- en: 'It may take about 10 to 20 minutes to run all the cells in the **Save to S3
    with a SageMaker Processing Job** Jupyter notebook. While waiting, let’s quickly
    check the different sections in the notebook:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 **Save to S3 with a SageMaker Processing Job** Jupyter 笔记本中所有单元格可能需要大约 10
    到 20 分钟。在等待的同时，让我们快速检查笔记本中的不同部分：
- en: '**Inputs and Outputs** – where we specify the input and output configuration
    for the flow export'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入和输出** – 在这里我们指定流程导出的输入和输出配置'
- en: '**Run Processing Job** – where we configure and run a SageMaker processing
    job'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行处理作业** – 在这里我们配置并运行一个 SageMaker 处理作业'
- en: '**(Optional)Next Steps** – where we can optionally load the processed data
    into pandas for further inspection and train a model with SageMaker'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(可选)下一步操作** – 在这里我们可以选择将处理后的数据加载到 pandas 中进行进一步检查，并使用 SageMaker 训练模型'
- en: Note
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you encounter an error with a message similar to `input_name` values of the
    `ProcessingInput` objects stored in the `data_sources` list (and we should only
    have a single `ProcessingInput` object in the list). If you encounter other unexpected
    errors, feel free to troubleshoot the Python code as needed.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到类似 `input_name` 的错误消息，该错误消息与存储在 `data_sources` 列表中的 `ProcessingInput` 对象的值（并且列表中应该只有一个
    `ProcessingInput` 对象）。如果你遇到其他意外的错误，请根据需要自由地调试 Python 代码。
- en: 'Once `SystemExit` has been raised under **(Optional)Next Steps**, locate and
    scroll to the cell under **Job Status & S3 Output Location** and copy the S3 path
    highlighted in *Figure 5.27* to a text editor (for example, VS Code) on your local
    machine:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦在 **(可选)下一步操作** 中抛出了 `SystemExit`，找到并滚动到 **作业状态 & S3 输出位置** 下的单元格，并将 *图 5.27*
    中高亮显示的 S3 路径复制到你的本地机器上的文本编辑器（例如，VS Code）中：
- en: '![Figure 5.27 – Copying the S3 path where the job results are stored ](img/B18638_05_027.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.27 – 复制存储作业结果的 S3 路径](img/B18638_05_027.jpg)'
- en: Figure 5.27 – Copying the S3 path where the job results are stored
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27 – 复制存储作业结果的 S3 路径
- en: You should find the S3 path right after `SystemExit` to be raised before proceeding.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你应该找到 `SystemExit` 之前抛出的 S3 路径。
- en: 'Now that we have finished running the cells from the generated Jupyter notebook,
    you might be wondering, *what is the point of generating a Jupyter notebook in
    the first place*? Why not run the data flow steps directly without having to generate
    a script or a notebook? The answer to this is simple: these generated Jupyter
    notebooks are meant to serve as initial templates that can be customized to fit
    the requirements of the work that needs to be done.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经运行完生成的 Jupyter 笔记本中的单元格，你可能想知道，*最初生成 Jupyter 笔记本的意义是什么*？为什么不直接运行数据流步骤，而无需生成脚本或笔记本？这个答案很简单：这些生成的
    Jupyter 笔记本是作为初始模板，可以根据需要的工作要求进行定制。
- en: Note
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Wait! Where’s the processed version of the dataset? In the next section, we
    will quickly turn off the instances automatically launched by SageMaker Studio
    to manage costs. After turning off the resources, we will proceed with downloading
    and checking the output CSV file saved in S3 in the *Verifying the results* section
    near the end of the chapter.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！数据集的处理版本在哪里？在下一节中，我们将快速关闭 SageMaker Studio 自动启动的实例以管理成本。关闭资源后，我们将继续在章节末尾的
    *验证结果* 部分下载并检查保存在 S3 中的输出 CSV 文件。
- en: Turning off the resources
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关闭资源
- en: 'It is important to note that SageMaker Studio automatically launches an `ml.m5.4xlarge`
    instance (at the time of writing) whenever we use and access SageMaker Data Wrangler.
    In addition to this, another ML instance is provisioned when running one or more
    cells inside a Jupyter notebook. If we were to create and run ML experiments on
    a Jupyter notebook using an AWS Deep Learning Container similar to that in *Figure
    5.28*, then an `ml.g4dn.xlarge` instance may be provisioned as well. These instances
    and resources need to be manually turned off and removed since these are not turned
    off automatically even during periods of inactivity:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – A high-level view of how SageMaker Studio operates ](img/B18638_05_028.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – A high-level view of how SageMaker Studio operates
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning off these resources is crucial since we do not want to pay for the
    time when these resources are not being used. In the next set of steps, we will
    locate and turn off the running instances in SageMaker Studio:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the circle icon in the sidebar, as highlighted in *Figure 5.29*:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Turning off the running instances ](img/B18638_05_029.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – Turning off the running instances
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the circle icon should open and show the running instances, apps, and
    terminals in SageMaker Studio.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Turn off all running instances under **RUNNING INSTANCES** by clicking the **Shut
    down** button for each of the instances as highlighted in *Figure 5.29*. Clicking
    the **Shut down** button will open a pop-up window verifying the instance shutdown
    operation. Click the **Shut down all** button to proceed.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Moving forward, you may want to install and use a JupyterLab extension that
    automatically turns off certain resources during periods of inactivity similar
    to the **SageMaker Studio auto-shutdown extension**. You can find the extension
    here: [https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension](https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension).'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Even after installing the extension, it is still recommended to manually check
    and turn off the resources after using SageMaker Studio. Make sure to perform
    regular inspections and cleanup of resources as well.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the results
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, the processed version of the dataset should be stored in the
    destination S3 path you copied to a text editor in your local machine. In the
    next set of steps, we will download this into the AWS CloudShell environment and
    check whether the expected changes are reflected in the downloaded file:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: In SageMaker Studio, open the **File** menu and select **Log out** from the
    list of options. This will redirect you back to the **SageMaker Domain** page.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to **CloudShell** by clicking the icon highlighted in *Figure 5.30*:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Navigating to CloudShell ](img/B18638_05_030.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – Navigating to CloudShell
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: We can find this button in the upper-right-hand corner of the AWS Management
    Console. You may also use the search bar to navigate to the CloudShell console.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 AWS 管理控制台的右上角找到这个按钮。您也可以使用搜索栏导航到 CloudShell 控制台。
- en: 'Once the terminal is ready, move all files in the CloudShell environment to
    the `/tmp` directory by running the following (after **$**):'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦终端准备好，请通过运行以下命令将 CloudShell 环境中的所有文件移动到 `/tmp` 目录（在 **$** 之后）：
- en: '[PRE6]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Use the `aws s3 cp` command to copy the generated CSV file stored in S3 to
    the CloudShell environment. Make sure to replace `<PASTE S3 URL>` with the S3
    URL copied from the **Save to S3 with a SageMaker Processing Job** notebook to
    a text editor on your local machine:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `aws s3 cp` 命令将存储在 S3 中的生成的 CSV 文件复制到 CloudShell 环境中。请确保将 `<PASTE S3 URL>`
    替换为您从 **Save to S3 with a SageMaker Processing Job** 笔记本复制到您本地机器上的文本编辑器中的 S3 URL：
- en: '[PRE7]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Recursively list the files and directories using the following command:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令递归列出文件和目录：
- en: '[PRE9]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You should see a CSV file stored inside `<UUID>/default`.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能看到存储在 `<UUID>/default` 中的 CSV 文件。
- en: 'Finally, use the `head` command to inspect the CSV file:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `head` 命令检查 CSV 文件：
- en: '[PRE10]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This should give us the first few lines of the CSV file, similar to what we
    have in *Figure 5.31*:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们 CSV 文件的前几行，类似于我们在 *图 5.31* 中看到的那样：
- en: '![Figure 5.31 – Verifying the changes ](img/B18638_05_031.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.31 – 验证更改](img/B18638_05_031.jpg)'
- en: Figure 5.31 – Verifying the changes
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.31 – 验证更改
- en: Here, we can see that the dataset has the new `has_booking_changes` column containing
    `true` and `false` values. You may inspect the CSV file further and verify that
    there are no more `-1` values under the `children` column. We will leave this
    to you as an exercise (that is, verifying that there are no more `-1` values under
    the `children` column of the CSV file).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到数据集有一个新的 `has_booking_changes` 列，包含 `true` 和 `false` 值。您可以进一步检查 CSV
    文件，并验证在 `children` 列下没有更多的 `-1` 值。我们将把这个留给你作为练习（即，验证 CSV 文件中的 `children` 列下没有更多的
    `-1` 值）。
- en: 'Now that we have finished using both Amazon SageMaker Data Wrangler and AWS
    Glue DataBrew to process and analyze a sample dataset, you might be wondering
    when to use one of these tools over the other. Here are some general recommendations
    when deciding:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用 Amazon SageMaker Data Wrangler 和 AWS Glue DataBrew 处理和分析了一个样本数据集，您可能想知道何时使用其中一个工具而不是另一个。以下是一些在做出决定时的通用建议：
- en: If you are planning to use custom transforms using PySpark similar to those
    we performed in this chapter, then you may want to use Amazon SageMaker Data Wrangler.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您计划使用类似于我们在本章中执行的自定义转换使用 PySpark，那么您可能想使用 Amazon SageMaker Data Wrangler。
- en: If the source, connection, or file type format is not supported in SageMaker
    Data Wrangler (for example, Microsoft Excel workbook format or `.xlsx` files),
    then you may want to use AWS Glue Data Brew.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果源、连接或文件类型格式在 SageMaker Data Wrangler 中不受支持（例如，Microsoft Excel 工作簿格式或 `.xlsx`
    文件），那么您可能想使用 AWS Glue Data Brew。
- en: If you want to export the data processing workflow and automatically generate
    a Jupyter notebook, then you may want to use Amazon SageMaker Data Wrangler.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您想导出数据处理工作流程并自动生成 Jupyter 笔记本，那么您可能想使用 Amazon SageMaker Data Wrangler。
- en: If the primary users of the tool have minimal coding experience and would prefer
    processing and analyzing the data without reading, customizing, or writing a single
    line of code, then AWS Glue Data Brew may be used instead of Amazon SageMaker
    Data Wrangler.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果工具的主要用户编码经验有限，并且更愿意在不阅读、定制或编写任何代码的情况下处理和分析数据，那么可以使用 AWS Glue Data Brew 而不是
    Amazon SageMaker Data Wrangler。
- en: Of course, these are just some of the guidelines you can use but the decision
    on which tool to use will ultimately depend on the context of the work that needs
    to be done, along with the limitations of the tools at the time that the decision
    needs to be made. Features and limitations change over time, so make sure to review
    as many angles as possible when deciding.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些只是一些您可以使用的指南，但最终决定使用哪个工具将取决于需要完成的工作的上下文，以及做出决定时工具的限制。功能和限制会随时间变化，所以确保在做出决定时尽可能多地审查各个角度。
- en: Summary
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Data needs to be cleaned, analyzed, and prepared before it is used to train
    ML models. Since it takes time and effort to work on these types of requirements,
    it is recommended to use no-code or low-code solutions such as AWS Glue DataBrew
    and Amazon SageMaker Data Wrangler when analyzing and processing our data. In
    this chapter, we were able to use these two services to analyze and process our
    sample dataset. Starting with a sample “dirty” dataset, we performed a variety
    of transformations and operations, which included (1) profiling and analyzing
    the data, (2) filtering out rows containing invalid data, (3) creating a new column
    from an existing one, (4) exporting the results into an output location, and (5)
    verifying whether the transformations have been applied to the output file.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据训练机器学习模型之前，数据需要被清理、分析和准备。由于处理这些类型的要求需要时间和精力，因此建议在分析和处理我们的数据时使用无代码或低代码解决方案，例如AWS
    Glue DataBrew和Amazon SageMaker Data Wrangler。在本章中，我们能够使用这两项服务来分析和处理我们的样本数据集。从样本“脏”数据集开始，我们执行了各种转换和操作，包括（1）对数据进行配置文件分析和分析，（2）过滤掉包含无效数据的行，（3）从现有列创建新列，（4）将结果导出到输出位置，以及（5）验证转换是否已应用于输出文件。
- en: In the next chapter, we will take a closer look at Amazon SageMaker and we will
    dive deeper into how we can use this managed service when performing machine learning
    experiments.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地了解Amazon SageMaker，并深入探讨在进行机器学习实验时如何使用这项托管服务。
- en: Further reading
  id: totrans-409
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information on the topics covered in this chapter, feel free to check
    out the following resources:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章涵盖的主题的更多信息，您可以自由地查看以下资源：
- en: '*AWS Glue DataBrew product and service integrations* ([https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml))'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS Glue DataBrew产品和服务集成* ([https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml))'
- en: '*Security in AWS Glue DataBrew* ([https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml))'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS Glue DataBrew中的安全性* ([https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml))'
- en: '*Create and Use a Data Wrangler Flow* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml))'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建和使用Data Wrangler流程* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml))'
- en: '*Data Wrangler – Transform* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml))'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Data Wrangler – 转换* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml))'
- en: '*Data Wrangler – Troubleshooting* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml))'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Data Wrangler – 故障排除* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml))'
- en: 'Part 3: Diving Deeper with Relevant Model Training and Deployment Solutions'
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：使用相关模型训练和部署解决方案深入探讨
- en: In this section, readers will learn the relevant model training and deployment
    solutions using the different capabilities and features of Amazon SageMaker, along
    with other AWS services.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，读者将学习使用Amazon SageMaker的不同功能和特性，以及其他AWS服务，来了解相关的模型训练和部署解决方案。
- en: 'This section comprises the following chapters:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括以下章节：
- en: '[*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and Debugging
    Solutions*'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18638_06.xhtml#_idTextAnchor132)，*SageMaker训练和调试解决方案*'
- en: '[*Chapter 7*](B18638_07.xhtml#_idTextAnchor151), *SageMaker Deployment Solutions*'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18638_07.xhtml#_idTextAnchor151)，*SageMaker部署解决方案*'
