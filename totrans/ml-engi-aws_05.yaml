- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pragmatic Data Processing and Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data needs to be analyzed, transformed, and processed first before using it
    when training **machine learning** (**ML**) models. In the past, data scientists
    and ML practitioners had to write custom code from scratch using a variety of
    libraries, frameworks, and tools (such as **pandas** and **PySpark**) to perform
    the needed analysis and processing work. The custom code prepared by these professionals
    often needed tweaking since different variations of the steps programmed in the
    data processing scripts had to be tested on the data before being used for model
    training. This takes up a significant portion of an ML practitioner’s time, and
    since this is a manual process, it is usually error-prone as well.
  prefs: []
  type: TYPE_NORMAL
- en: One of the more practical ways to process and analyze data involves the usage
    of no-code or low-code tools when loading, cleaning, analyzing, and transforming
    the raw data from different data sources. Using these types of tools will significantly
    speed up the process since we won’t need to worry about coding the data processing
    scripts from scratch. In this chapter, we will use **AWS Glue DataBrew** and **Amazon
    SageMaker Data Wrangler** to load, analyze, and process a sample dataset. After
    cleaning, processing, and transforming the data, we will download and inspect
    the results in an **AWS CloudShell** environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with data processing and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating data preparation and analysis with AWS Glue DataBrew
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing ML data with Amazon SageMaker Data Wrangler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While working on the hands-on solutions in this chapter, you will notice that
    there are several similarities when using **AWS Glue DataBrew** and **Amazon SageMaker
    Data Wrangler**, but of course, you will notice several differences as well. Before
    we dive straight into using and comparing these services, let’s have a short discussion
    first regarding data processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, it is important that we have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account used in the first four chapters of the book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks, source code, and other files used for each chapter are
    available in this repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to sign out and NOT use the IAM user created in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079),
    *Serverless Data Management on AWS*. In this chapter, you should use the root
    account or a new IAM user with a set of permissions to create and manage the **AWS
    Glue DataBrew**, **Amazon S3**, **AWS CloudShell**, and **Amazon SageMaker** resources.
    It is recommended to use an IAM user with limited permissions instead of the root
    account when running the examples in this book. We will discuss this along with
    other security best practices in further detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with data processing and analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we utilized a data warehouse and a data lake to store,
    manage, and query our data. Data stored in these data sources generally must undergo
    a series of data processing and data transformation steps similar to those shown
    in *Figure 5.1* before it can be used as a training dataset for ML experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Data processing and analysis ](img/B18638_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Data processing and analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5.1*, we can see that these data processing steps may involve merging
    different datasets, along with cleaning, converting, analyzing, and transforming
    the data using a variety of options and techniques. In practice, data scientists
    and ML engineers generally spend a lot of hours cleaning the data and getting
    it ready for use in ML experiments. Some professionals may be used to writing
    and running custom Python or R scripts to perform this work. However, it may be
    more practical to use no-code or low-code solutions such as AWS Glue DataBrew
    and Amazon SageMaker Data Wrangler when dealing with these types of requirements.
    For one thing, these solutions are more convenient to use since we won’t need
    to worry about managing the infrastructure, as well as coding the data processing
    scripts from scratch. We would also be using an easy-to-use visual interface that
    will help speed up the work significantly. Monitoring and security management
    are easier as well since these are integrated with other AWS services such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Identity and Access Management** (**IAM**) – for controlling and limiting
    access to AWS services and resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Virtual Private Cloud** (**VPC**) – for defining and configuring a
    logically isolated network that dictates how resources are accessed and how each
    resource communicates with the others within the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon CloudWatch** – for monitoring the performance and managing the logs
    of the resources used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS CloudTrail** – for monitoring and auditing account activity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on how these services are used to secure and manage the
    resources in the AWS account, feel free to check out [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that there are also other options in AWS that can help
    us when processing and analyzing our data. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon Elastic MapReduce** (**EMR**) and **EMR Serverless** – for large-scale
    distributed data processing workloads using a variety of open source tools such
    as Apache Spark, Apache Hive, and Presto'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Kinesis** – for processing and analyzing real-time streaming data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon QuickSight** – for enabling advanced analytics and self-service business
    intelligence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS Data Pipeline** – for processing and moving data across a variety of
    services (for example, **Amazon S3**, **Amazon Relational Database Service**,
    and **Amazon DynamoDB**) using features that help with the scheduling, dependency
    tracking, and error handling of custom pipeline resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Processing** – for running custom data processing and analysis
    scripts (including bias metrics and feature importance computations) on top of
    the managed infrastructure of AWS with SageMaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list and there are more services and capabilities
    that can be used for these types of requirements. *What’s the advantage of using
    these services?* When dealing with relatively small datasets, performing data
    analysis and transformations on our local machine may do the trick. However, once
    we need to work with much larger datasets, we may need to use a more dedicated
    set of resources with more computing power, as well as features that allow us
    to focus on the work we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss bias detection and feature importance in more detail in [*Chapter
    9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on AWS Glue DataBrew and Amazon SageMaker Data
    Wrangler and we will show a few examples of how to use these when processing and
    analyzing our data. We will start with a “dirty” dataset (containing a few rows
    with invalid values) and perform the following types of transformations, analyses,
    and operations on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Running a data profiling job that analyzes the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering out rows that contain invalid values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new column from an existing one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting the results after the transformations have been applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the file containing the processed results has been uploaded to the output
    location, we will verify the results by downloading the file and checking whether
    the transformations have been applied.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with the hands-on solutions of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The Parquet file to be analyzed and processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The S3 bucket where the Parquet file will be uploaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading the Parquet file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will work with a similar `bookings` dataset as the one used
    in previous chapters. However, the source data is stored in a Parquet file this
    time, and we have modified some of the rows so that the dataset will have dirty
    data. That said, let’s download the `synthetic.bookings.dirty.parquet` file onto
    our local machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find it here: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/raw/main/chapter05/synthetic.bookings.dirty.parquet).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that storing data using the Parquet format is preferable to storing data
    using the CSV format. Once you need to work with much larger datasets, the difference
    in the file sizes of generated Parquet and CSV files becomes apparent. For example,
    a 1 GB CSV file may end up as just 300 MB (or even less) as a Parquet file! For
    more information on this topic, feel free to check the following link: [https://parquet.apache.org/docs/](https://parquet.apache.org/docs/).'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to download the `synthetic.bookings.dirty.parquet` file to your local
    machine before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the S3 bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can create a new S3 bucket for the hands-on solutions in this chapter or
    you can reuse an existing one that was created in previous chapters. This S3 bucket
    will be used to store both the `synthetic.bookings.dirty.parquet` source file
    and the output destination results after running the data processing and transformation
    steps using AWS Glue DataBrew and Amazon SageMaker Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Once both prerequisites are ready, we can proceed with using AWS Glue DataBrew
    to analyze and process our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Automating data preparation and analysis with AWS Glue DataBrew
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Glue DataBrew is a no-code data preparation service built to help data scientists
    and ML engineers clean, prepare, and transform data. Similar to the services we
    used in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*, Glue DataBrew is *serverless* as well. This means that we won’t need
    to worry about infrastructure management when using this service to perform data
    preparation, transformation, and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – The core concepts in AWS Glue DataBrew ](img/B18638_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – The core concepts in AWS Glue DataBrew
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5.2*, we can see that there are different concepts and resources
    involved when using AWS Glue DataBrew. We need to have a good idea of what these
    are before using the service. Here is a quick overview of the concepts and terms
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset** – Data stored in an existing data source (for example, **Amazon
    S3**, **Amazon Redshift**, or **Amazon RDS**) or uploaded from the local machine
    to an S3 bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recipe** – A set of data transformation or data preparation steps to be performed
    on a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job** – The process of running certain instructions to profile or transform
    a dataset. Jobs that are used to evaluate a dataset are called **profile jobs**.
    On the other hand, jobs that are used to run a set of instructions to clean, normalize,
    and transform data are called **recipe jobs**. We can use a view called a **data
    lineage** to keep track of the transformation steps that a dataset has been through,
    along with the origin and destination configured in a job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data profile** – A report generated after running a profile job on a dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project** – A managed collection of data, transformation steps, and jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a good idea of what the concepts and terms are, let’s proceed
    with creating a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the *Preparing the essential prerequisites* section of this chapter, we
    downloaded a Parquet file to our local machine. In the next set of steps, we will
    create a new dataset by uploading this Parquet file from the local machine to
    an existing Amazon S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **AWS Glue DataBrew** console using the search bar of the **AWS
    Management Console**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed if certain resources need to be
    transferred to the region of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the **DATASETS** page by clicking the sidebar icon highlighted in *Figure
    5.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Navigating to the DATASETS page ](img/B18638_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Navigating to the DATASETS page
  prefs: []
  type: TYPE_NORMAL
- en: Click **Connect to new dataset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **File upload**, as highlighted in *Figure 5.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Locating the File upload option ](img/B18638_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Locating the File upload option
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are different ways to load data and connect to your dataset.
    We can connect and load data stored in Amazon Redshift, Amazon RDS, and AWS Glue
    using **AWS Glue Data Catalog**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check out [https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Specify `bookings` as the value for the **Dataset name** field (under **New
    dataset details**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `synthetic.bookings.dirty.parquet` file from your local machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, locate and click the **Browse S3** button under **Enter S3 destination**.
    Select the S3 bucket that you created in the *Preparing the essential prerequisites*
    section of [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management
    on AWS*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `synthetic.bookings.dirty.parquet` file in your local machine
    will be uploaded to the S3 bucket selected in this step. You may create and use
    a different S3 bucket when working on the hands-on solutions in this chapter.
    Feel free to create a new S3 bucket using the AWS Management Console or through
    AWS CloudShell using the AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Under **Additional configurations**, make sure that the **Selected file type**
    field is set to **PARQUET**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create dataset** button (located at the lower right of the page).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, the `bookings` dataset has been created and it should appear
    in the list of datasets as shown in *Figure 5.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Navigating to the Datasets preview page ](img/B18638_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Navigating to the Datasets preview page
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let’s click the `bookings` dataset name as highlighted in *Figure
    5.5*. This will redirect you to the **Dataset preview** page as shown in *Figure
    5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The dataset preview ](img/B18638_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The dataset preview
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to check the **Schema**, **Text**, and **Tree** views by clicking
    the appropriate button on the upper-right side of the **Dataset preview** pane.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully uploaded the Parquet file and created a new dataset,
    let’s proceed with creating and running a profile job to analyze the data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running a profile job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before performing any data cleaning and data transformation steps, it would
    be a good idea to analyze the data first and review the properties and statistics
    of each column in the dataset. Instead of doing this manually, we can use the
    capability of AWS Glue DataBrew to automatically generate different analysis reports
    for us. We can generate these reports automatically by running a profile job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and run a profile job to generate
    a data profile of the dataset we uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: First, click the **Data profile overview** tab to navigate to the **Data profile
    overview** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Run data profile** button. This will redirect you to the **Create
    job** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Create job** page, scroll down and locate the **Job output** settings
    section, and then click the **Browse** button to set the **S3 location** field
    value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `synthetic.bookings.dirty.parquet` file was uploaded in an earlier step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `mle` as the value for **New IAM role suffix**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create and run job** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 3 to 5 minutes to complete. Feel free to grab a cup of coffee
    or tea! Note that you may see a **1 job in progress** loading message while waiting
    for the results to appear.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the profile job is complete, scroll down, and view the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – An overview of the data profile ](img/B18638_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – An overview of the data profile
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see a set of results similar to those shown in *Figure 5.7*. Feel
    free to check the following reports generated by the profile job:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary** – shows the total number of rows, total number of columns, missing
    cells, and duplicate rows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correlations** – shows a correlation matrix (displaying how each of the variables
    is related)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compare value distribution** – shows a comparative view of the distributions
    across columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Columns summary** – shows summary statistics for each of the columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optionally, you can navigate to the **Column statistics** tab and review the
    reports in that tab as well.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, it took us just a couple of clicks to generate a data profile
    that can be used to analyze our dataset. Feel free to review the different reports
    and statistics generated by the profile job before proceeding with the next part
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a project and configuring a recipe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is time that we create and use an AWS Glue DataBrew project. Creating a
    project involves working with a dataset and a recipe to perform the desired data
    processing and transformation work. Since we don’t have a recipe yet, a new recipe
    will be created while we are creating and configuring the project. In this section,
    we will configure a recipe that does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Filters out the rows containing invalid `children` column values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates a new column (`has_booking_changes`) based on the value of an existing
    column (`booking_changes`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create a project and use the interactive
    user interface to configure a recipe for cleaning and transforming the data:'
  prefs: []
  type: TYPE_NORMAL
- en: In the upper-right-hand corner of the page, locate and click the **Create project
    with this dataset** button. This should redirect you to the **Create project**
    page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under `bookings-project` as the value for the **Project name** field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down and locate the **Role name** drop-down field under **Permissions**.
    Select the existing IAM role created in an earlier step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the **Create project** button afterward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Waiting for the project to be ready ](img/B18638_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Waiting for the project to be ready
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the **Create project** button, you should be redirected to a
    page similar to that shown in *Figure 5.8*. After creating a project, we should
    be able to use a highly interactive workspace where we can test and apply a variety
    of data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 2 to 3 minutes to complete
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the project session is ready, we’ll do a quick check of the data to find
    any erroneous entries and spot issues in the data (so that we can filter these
    out). In the left pane showing a grid view of the data, locate and scroll (either
    left or right) to the **children** column similar to what is shown in *Figure
    5.9*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Filtering out the rows with invalid cell values ](img/B18638_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Filtering out the rows with invalid cell values
  prefs: []
  type: TYPE_NORMAL
- en: We should see the `children` column between the `adults` and `babies``-1` and
    there are cells under the `children` column with a value of `-1`. Once you have
    reviewed the different values under the `children` column, click the **FILTER**
    button as highlighted in *Figure 5.9*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have intentionally added a certain number of `-1` values under
    the `children` column in the Parquet file used in this chapter. Given that it
    is impossible to have a value less than `0` for the `children` column, we will
    filter out these rows in the next set of steps.
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the **FILTER** button, a drop-down menu should appear. Locate
    and select **Greater than or equal to** from the list of options under **By condition**.
    This should update the pane on the right-hand side of the page and show the list
    of configuration options for the **Filter values** operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `children` column from the list of options for the `0` in the field with
    the placeholder background text of **Enter a filter value**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Preview changes**. This should update the left-hand pane and provide
    a grid view of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Previewing the results ](img/B18638_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Previewing the results
  prefs: []
  type: TYPE_NORMAL
- en: We should see that the rows with a value of `-1` under the `children` column
    have been filtered out, similar to what is shown in *Figure 5.10*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, click the **Apply** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s proceed with adding a step for creating a new column (from an existing
    one). Locate and click the **Add step** button as highlighted in *Figure 5.11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Adding a step ](img/B18638_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Adding a step
  prefs: []
  type: TYPE_NORMAL
- en: The **Add step** button should be in the same row where the **Clear all** link
    is located.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the drop-down field with the `create` in the search field. Select the
    **Based on conditions** option from the list of results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Locating the Based on conditions option ](img/B18638_05_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Locating the Based on conditions option
  prefs: []
  type: TYPE_NORMAL
- en: If you are looking for the search field, simply refer to the highlighted box
    (at the top) in *Figure 5.12*.
  prefs: []
  type: TYPE_NORMAL
- en: In the `booking_changes`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Greater than`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`0`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`True or False`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`has_booking_changes`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click `has_booking_changes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Previewing the changes ](img/B18638_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Previewing the changes
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 5.13*, this new column has a value of `true` if the
    `booking_changes` column has a value greater than `0`, and `false` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Review the preview results before clicking the **Apply** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, we should have two applied steps in our recipe. Click **Publish**,
    as highlighted in *Figure 5.14*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Locating and clicking the Publish button ](img/B18638_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Locating and clicking the Publish button
  prefs: []
  type: TYPE_NORMAL
- en: This should open the **Publish recipe** window.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Publish recipe** pop-up window, click **Publish**. Note that we can
    specify an optional version description when publishing the current recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a published recipe, we can proceed with creating a recipe job
    that will execute the different steps configured in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: After publishing the recipe, we still need to run a recipe job before the changes
    are applied (resulting in the generation of a new file with the applied data transformations).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and running a recipe job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we can see in *Figure 5.15*, a recipe job needs to be configured with a source
    and a destination. The job reads the data stored in the source, performs the transformation
    steps configured in the associated recipe, and stores the processed files in the
    destination.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – A job needs to be configured with a source and a destination
    ](img/B18638_05_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – A job needs to be configured with a source and a destination
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the source data is not modified since the recipe
    job only connects in a read-only manner. After the recipe job has finished processing
    all the steps, the job results are stored in one or more of the configured output
    destinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create and run a recipe job using the recipe
    we published in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Recipes** page using the sidebar on the left-hand side.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the row named `bookings-project-recipe` (which should toggle the checkbox
    and highlight the entire row).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create job with this recipe** button. This will redirect you to
    the **Create job** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `bookings-clean-and-add-column`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Project`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`bookings-project`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Job output settings**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**S3 location**: Click **Browse**. Locate and choose the same S3 bucket used
    in the previous steps in this chapter.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Permissions**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Role name**: Choose the IAM role created in an earlier step.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not limited to storing the job output results in S3\. We can also store
    the output results in **Amazon Redshift**, **Amazon RDS** tables, and more. For
    more information, feel free to check the following link: [https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/supported-data-connection-sources.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Review the specified configuration and then click the **Create and run job**
    button. If you accidentally clicked on the **Create job** button (beside the **Create
    and run job** button), you may click the **Run job** button after the job has
    been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait for 3 to 5 minutes for this step to complete. Feel free to grab a cup of
    coffee or tea while waiting!
  prefs: []
  type: TYPE_NORMAL
- en: 'Wasn’t that easy? Creating, configuring, and running a recipe job is straightforward.
    Note that we can configure this recipe job and automate the job runs by associating
    a schedule. For more information on this topic, you can check the following link:
    [https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/jobs.recipe.xhtml#jobs.scheduling).'
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s proceed with inspecting the recipe job output results in AWS CloudShell,
    a free browser-based shell we can use to manage our AWS resources using a terminal.
    In the next set of steps, we will download the recipe job output results into
    the CloudShell environment and check whether the expected changes are reflected
    in the downloaded file:'
  prefs: []
  type: TYPE_NORMAL
- en: Once **Last job run status** of the job has changed to **Succeeded**, click
    the **1 output** link under the **Output** column. This should open the **Job
    output locations** window. Click the S3 link under the **Destination** column
    to open the S3 bucket page in a new tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `bookings-clean-and-add-column`. Make sure to press the *ENTER* key
    to filter the list of objects. Navigate to the `bookings-clean-and-add-column`
    and ends with `part00000`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the CSV file (which should toggle the checkbox) and then click the **Copy
    S3 URI** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to **AWS CloudShell** by clicking the icon, as highlighted in *Figure
    5.16*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Navigating to CloudShell ](img/B18638_05_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Navigating to CloudShell
  prefs: []
  type: TYPE_NORMAL
- en: We can find this button in the upper-right-hand corner of the AWS Management
    Console. You can also use the search bar to navigate to the CloudShell console.
  prefs: []
  type: TYPE_NORMAL
- en: When you see the **Welcome to AWS CloudShell** window, click the **Close** button.
    Wait for the environment to run (for about 1 to 2 minutes) before proceeding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following commands in the CloudShell environment (after `<PASTE COPIED
    S3 URL>` with what was copied to the clipboard in an earlier step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should download the output CSV file from S3 into the CloudShell environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `head` command to inspect the first few rows of the `bookings.csv`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should return the first row containing the header of the CSV file, along
    with the first few records of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Verifying the job results ](img/B18638_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Verifying the job results
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5.17*, we can see that the processed dataset now includes the `has_booking_changes`
    column containing `true` or `false` values. You can inspect the CSV file further
    and verify that there are no more `-1` values under the `children` column. We
    will leave this to you as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re done using AWS Glue DataBrew to analyze and process our data,
    we can proceed with using Amazon SageMaker Data Wrangler to perform a similar
    set of operations.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not forget to delete all Glue DataBrew resources (such as the recipe job,
    profile job, recipe, project, and dataset) once you are done working on the hands-on
    solutions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing ML data with Amazon SageMaker Data Wrangler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Amazon SageMaker has a lot of capabilities and features to assist data scientists
    and ML engineers with the different ML requirements. One of the capabilities of
    SageMaker focused on accelerating data preparation and data analysis is SageMaker
    Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler
    ](img/B18638_05_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – The primary functionalities available in SageMaker Data Wrangler
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5.18*, we can see what we can do with our data when using SageMaker
    Data Wrangler:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we can import data from a variety of data sources such as Amazon S3,
    Amazon Athena, and Amazon Redshift.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we can create a data flow and transform the data using a variety of data
    formatting and data transformation options. We can also analyze and visualize
    the data using both inbuilt and custom options in just a few clicks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can automate the data preparation workflows by exporting one or
    more of the transformations configured in the data processing pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SageMaker Data Wrangler is integrated into SageMaker Studio, which allows us
    to use this capability to process our data and automate our data processing workflows
    without having to leave the development environment. Instead of having to code
    everything from scratch using a variety of tools, libraries, and frameworks such
    as pandas and PySpark, we can simply use SageMaker Data Wrangler to help us prepare
    custom data flows using an interface and automatically generate reusable code
    within minutes!
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to sign out and *NOT* use the IAM user created in [*Chapter 4*](B18638_04.xhtml#_idTextAnchor079),
    *Serverless Data Management on AWS*. You should use the root account or a new
    IAM user with a set of permissions to create and manage the AWS Glue DataBrew,
    Amazon S3, AWS CloudShell, and Amazon SageMaker resources. It is recommended to
    use an IAM user with limited permissions instead of the root account when running
    the examples in this book. We will discuss this along with other security best
    practices in detail in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security,
    Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Data Wrangler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to open SageMaker Studio to access SageMaker Data Wrangler.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that you have completed the hands-on solutions in the *Getting started
    with SageMaker and SageMaker Studio* section of [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017),
    *Introduction to ML Engineering on AWS*, before proceeding. You can also update
    SageMaker Studio along with Studio Apps (in case you’re using an old version).
    For more information about this topic, you can check the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tasks-update-studio.xhtml).
    Note that the steps in this section assume that we’re using JupyterLab 3.0\. If
    you are using a different version, you may experience a few differences in terms
    of layout and user experience.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will launch SageMaker Studio and access Data Wrangler
    from the **File** menu:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to `sagemaker studio` into the search bar of the AWS Management Console
    and selecting **SageMaker Studio** from the list of results under **Features**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes that we are using the `us-west-2`) region when using services
    to manage and create different types of resources. You may use a different region
    but make sure to perform any adjustments needed in case certain resources need
    to be transferred to the region of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Next, click **Studio** under **SageMaker Domain** in the sidebar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Launch app**, as highlighted in *Figure 5.19*. Select **Studio** from
    the list of drop-down options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Opening SageMaker Studio ](img/B18638_05_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Opening SageMaker Studio
  prefs: []
  type: TYPE_NORMAL
- en: This will redirect you to **SageMaker Studio**. Wait for a few seconds for the
    interface to load.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `ml.m5.4xlarge` instance is being provisioned to run Data Wrangler.
    Once ready, you’ll see the **Import data** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, let’s proceed with importing our data in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Once you are done with the hands-on solutions in this chapter, the `ml.m5.4xlarge`
    instance used to run Data Wrangler needs to be turned off immediately to avoid
    any additional charges. Click and locate the circle icon on the left-hand sidebar
    to show the list of running instances, apps, kernel sessions, and terminal sessions.
    Make sure to shut down all running instances under **RUNNING INSTANCES** whenever
    you are done using SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several options when importing data for use in Data Wrangler. We can
    import and load data from a variety of sources including Amazon S3, Amazon Athena,
    Amazon Redshift, Databricks (JDBC), and Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will focus on importing the data stored in the
    Parquet file uploaded in an S3 bucket in our account:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Import data** page (under the **Import** tab), click **Amazon S3**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `synthetic.bookings.dirty.parquet` file uploaded in one of the S3 buckets
    in your AWS account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In case you skipped the *Automating data preparation and analysis with AWS Glue
    DataBrew* section of this chapter, you need to upload the Parquet file downloaded
    in the *Preparing the essential prerequisites* section of this chapter to a new
    or existing Amazon S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: If you see a **Preview Error** notification similar to what is shown in *Figure
    5.20*, you may remove this by opening the **File type** dropdown and choosing
    **parquet** from the list of options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Setting the file type to Parquet ](img/B18638_05_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Setting the file type to Parquet
  prefs: []
  type: TYPE_NORMAL
- en: The **Preview Error** message should disappear after the **parquet** option
    is selected from the **File type** drop-down menu.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using JupyterLab version 3.0, the **parquet** option is already preselected.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using JupyterLab version 1.0, the **csv** option may be preselected
    instead of the **parquet** option. That said, regardless of the version, we should
    set the **File type** drop-down value to **parquet**. Click the **Import** button
    located in the upper-right corner of the page. This will redirect you back to
    the **Data flow** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that we are not limited to importing from Amazon S3\. We can also import
    data from Amazon Athena, Amazon Redshift, and other data sources. You may check
    [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-import.xhtml)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many built-in options in SageMaker Data Wrangler when processing and
    transforming our data. In this chapter, we will show a quick demo of how to use
    a custom PySpark script to transform data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the numerous data transforms available, feel free to
    check the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will add and configure a custom PySpark transform
    to clean and process our data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you can see the **Data types · Transform: synthetic.bookings.dirty.parquet**
    page, navigate back to the **Data flow** page by clicking the **< Data flow**
    button located in the top-left-hand corner of the page. We’ll go back to this
    page in a bit, after having a quick look at the current configuration of the data
    flow in the next step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Data flow** page, click the **+** button, as highlighted in *Figure
    5.21*. Select **Add transform** from the list of options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Adding a transform ](img/B18638_05_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Adding a transform
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will only work with a single dataset. However, it is important
    to note that we can work on and merge two datasets using the **Join** option as
    we have in *Figure 5.21*.
  prefs: []
  type: TYPE_NORMAL
- en: In the **ALL STEPS** pane on the left side of the page, click the **Add step**
    button. This should show a list of options for transforming the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using **JupyterLab 1.0**, you should see the left pane labeled as
    **TRANSFORMS** instead of **ALL STEPS**.
  prefs: []
  type: TYPE_NORMAL
- en: Select **Custom transform** from the list of options available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In **CUSTOM TRANSFORM**, enter the following code into the code editor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What this code block does is select and retain all rows where the value of the
    `children` column is `0` or higher, and creates a new column, `has_booking_changes,`
    that has a value of `true` if the value in the `booking_changes` column is greater
    than `0` and `false` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using **JupyterLab 1.0**, you should see the left-hand pane labeled
    **CUSTOM PYSPARK** instead of **CUSTOM TRANSFORM**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the `has_booking_changes` column, similar to what we have in *Figure
    5.22*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.22 – Previewing the changes ](img/B18638_05_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – Previewing the changes
  prefs: []
  type: TYPE_NORMAL
- en: You should find the `has_booking_changes` column beside the `total_of_special_requests`
    column (which is the leftmost column in the preview).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have finished reviewing the preview of the data, you can provide an
    optional **Name** value before clicking the **Add** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After clicking the **Add** button in the previous step, locate and click the
    **< Data flow** link (or the **Back to data flow** link) located on the upper-right-hand
    side of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the steps are not executed yet, as we’re just defining
    what will run in a later step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we are just scratching the surface of what we can do with SageMaker
    Data Wrangler here. Here are a few other examples of the other transforms available
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Balancing the data (for example, random oversampling, random undersampling,
    and SMOTE)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding categorical data (for example, one-hot encoding, similarity encoding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling missing time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features from time series data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a more complete list of transforms, feel free to check the following link:
    [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to check the following blog post in case you’re interested in diving
    a bit deeper into how to balance data using a variety of techniques (such as random
    oversampling, random undersampling, and SMOTE): [https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/](https://aws.amazon.com/blogs/machine-learning/balance-your-data-for-machine-learning-with-amazon-sagemaker-data-wrangler/).'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is critical that we analyze the data we will use in later steps to train
    ML models. We need to have a good idea of the properties that could inadvertently
    affect the behavior and performance of the ML models trained using this data.
    There are a variety of ways to analyze a dataset and the great thing about SageMaker
    Data Wrangler is that it allows us to choose from a list of pre-built analysis
    options and visualizations, including the ones in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Histograms** – can be used to show the “shape” of the distribution of the
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scatter plots** – can be used to show the relationship between two numeric
    variables (using dots representing each data point from the dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table summary** – can be used to show the summary statistics of the dataset
    (for example, the number of records or the minimum and maximum values in each
    column)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature importance scores** (using Quick Model) – used to analyze the impact
    of each feature when predicting a target variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Target leakage analysis** – can be used to detect columns in the dataset
    that are strongly correlated with the column we want to predict'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection for time series data** – can be used to detect outliers
    in time series data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias reports** – can be used to detect potential biases in the dataset (by
    calculating different bias metrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is not an exhaustive list and you may see other options when
    you are working on the hands-on portion of this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will create an analysis and generate a bias report:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **+** button and select **Add analysis** from the list of options,
    similar to that in *Figure 5.23*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.23 – Adding an analysis ](img/B18638_05_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – Adding an analysis
  prefs: []
  type: TYPE_NORMAL
- en: You should see the **Create analysis** pane located on the left-hand side of
    the page.
  prefs: []
  type: TYPE_NORMAL
- en: Specify the following configuration options in the `Bias report`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Sample analysis`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`is_cancelled`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`1`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`babies`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Threshold`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`1`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the bottom of the page. Locate and click the **Check for bias**
    button (beside the **Save** button).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll up and locate the bias report, similar to what is shown in *Figure 5.24*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.24 – The bias report ](img/B18638_05_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – The bias report
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the `0.92`. This means that the dataset is highly imbalanced
    and the advantaged group (`is_cancelled = 1`) is represented at a much higher
    rate compared to the disadvantaged group (`is_cancelled = 0`).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deeper into the details of how bias metrics are computed and interpreted
    in [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187), *Security, Governance, and
    Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: Scroll down and click the **Save** button (beside the **Check for bias** button)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate and click the **< Data flow** link (or the **Back to data flow** link)
    to return to the **Data flow** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In addition to bias reports, we can generate data visualizations such as histograms
    and scatter plots to help us analyze our data. We can even generate a quick model
    using the provided dataset and generate a feature importance report (with scores
    showing the impact of each feature when predicting a target variable). For more
    information, feel free to check out the following link: [https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-analyses.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Exporting the data flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With everything ready, let’s proceed with exporting the data flow we prepared
    in the previous sections. There are different options when performing the export
    operation. This includes exporting the data to an Amazon S3 bucket. We can also
    choose to export one or more steps from the data flow to **SageMaker Pipelines**
    using a Jupyter notebook that includes the relevant blocks of code. Similarly,
    we also have the option to export the features we have prepared to **SageMaker
    Feature Store**. There’s an option to export the data flow steps directly to Python
    code as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Once the data flow steps have been exported and converted to code, the generated
    code and the Jupyter notebooks can be run to execute the different steps configured
    in the data flow. Finally, experienced ML practitioners may opt to modify the
    generated notebooks and code if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next set of steps, we will perform the export operation and generate
    a Jupyter notebook that will utilize a **SageMaker processing** job to process
    the data and save the results to an S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **+** button after the third box, **Python (PySpark)** (or using
    the custom name that you specified in an earlier step), as highlighted in *Figure
    5.25*, and then open the list of options under **Export to**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Exporting the step ](img/B18638_05_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – Exporting the step
  prefs: []
  type: TYPE_NORMAL
- en: 'This should give us a list of options that includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Amazon S3 (via Jupyter Notebook)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Pipelines (via Jupyter Notebook)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python Code**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker Feature Store (via Jupyter Notebook)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using JupyterLab 1.0, you will need to navigate to the **Export data
    flow** page first by clicking the **Export** tab beside the **Data Flow** tab.
    After that, you’ll need to click the third box (under **Custom PySpark**) and
    then click the **Export Step** button (which will open the drop-down list of options).
  prefs: []
  type: TYPE_NORMAL
- en: Select **Amazon S3 (via Jupyter Notebook)** from the list of options. This should
    generate and open the **Save to S3 with a SageMaker Processing Job** Jupyter notebook.
    Note that at this point, the configured data transformations have not been applied
    yet and we would need to run the cells in the generated notebook file to apply
    the transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Locate and click the first runnable cell. Run it using the **Run the selected
    cells and advance** button, as highlighted in *Figure 5.26*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.26 – Running the first cell ](img/B18638_05_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.26 – Running the first cell
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have in *Figure 5.26*, we can find the first runnable cell under **Inputs
    and Outputs**. You may see a “**Note: The kernel is still starting. Please execute
    this cell again after the kernel is started.**” message while waiting for the
    kernel to start.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait for the kernel to start. This step may take around 3 to 5 minutes while
    an ML instance is being provisioned to run the Jupyter notebook cells. Once you
    are done with the hands-on solutions in this chapter, the ML instance used to
    run the Jupyter Notebook cells needs to be turned off immediately to avoid any
    additional charges. Click and locate the circle icon on the left-hand sidebar
    to show the list of running instances, apps, kernel sessions, and terminal sessions.
    Make sure to shut down all running instances under **RUNNING INSTANCES** whenever
    you are done using SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Once the kernel is ready, click on the cell containing the first code block
    under `run_optional_steps` variable is set to `False`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are wondering what a SageMaker processing job is, it is a job that utilizes
    the managed infrastructure of AWS to run a script. This script is coded to perform
    a set of operations defined by the user (or creator of the script). You can check
    [https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.xhtml)
    for more information on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may take about 10 to 20 minutes to run all the cells in the **Save to S3
    with a SageMaker Processing Job** Jupyter notebook. While waiting, let’s quickly
    check the different sections in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs and Outputs** – where we specify the input and output configuration
    for the flow export'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run Processing Job** – where we configure and run a SageMaker processing
    job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(Optional)Next Steps** – where we can optionally load the processed data
    into pandas for further inspection and train a model with SageMaker'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter an error with a message similar to `input_name` values of the
    `ProcessingInput` objects stored in the `data_sources` list (and we should only
    have a single `ProcessingInput` object in the list). If you encounter other unexpected
    errors, feel free to troubleshoot the Python code as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once `SystemExit` has been raised under **(Optional)Next Steps**, locate and
    scroll to the cell under **Job Status & S3 Output Location** and copy the S3 path
    highlighted in *Figure 5.27* to a text editor (for example, VS Code) on your local
    machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.27 – Copying the S3 path where the job results are stored ](img/B18638_05_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.27 – Copying the S3 path where the job results are stored
  prefs: []
  type: TYPE_NORMAL
- en: You should find the S3 path right after `SystemExit` to be raised before proceeding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have finished running the cells from the generated Jupyter notebook,
    you might be wondering, *what is the point of generating a Jupyter notebook in
    the first place*? Why not run the data flow steps directly without having to generate
    a script or a notebook? The answer to this is simple: these generated Jupyter
    notebooks are meant to serve as initial templates that can be customized to fit
    the requirements of the work that needs to be done.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Wait! Where’s the processed version of the dataset? In the next section, we
    will quickly turn off the instances automatically launched by SageMaker Studio
    to manage costs. After turning off the resources, we will proceed with downloading
    and checking the output CSV file saved in S3 in the *Verifying the results* section
    near the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Turning off the resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is important to note that SageMaker Studio automatically launches an `ml.m5.4xlarge`
    instance (at the time of writing) whenever we use and access SageMaker Data Wrangler.
    In addition to this, another ML instance is provisioned when running one or more
    cells inside a Jupyter notebook. If we were to create and run ML experiments on
    a Jupyter notebook using an AWS Deep Learning Container similar to that in *Figure
    5.28*, then an `ml.g4dn.xlarge` instance may be provisioned as well. These instances
    and resources need to be manually turned off and removed since these are not turned
    off automatically even during periods of inactivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.28 – A high-level view of how SageMaker Studio operates ](img/B18638_05_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.28 – A high-level view of how SageMaker Studio operates
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning off these resources is crucial since we do not want to pay for the
    time when these resources are not being used. In the next set of steps, we will
    locate and turn off the running instances in SageMaker Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the circle icon in the sidebar, as highlighted in *Figure 5.29*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.29 – Turning off the running instances ](img/B18638_05_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.29 – Turning off the running instances
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the circle icon should open and show the running instances, apps, and
    terminals in SageMaker Studio.
  prefs: []
  type: TYPE_NORMAL
- en: Turn off all running instances under **RUNNING INSTANCES** by clicking the **Shut
    down** button for each of the instances as highlighted in *Figure 5.29*. Clicking
    the **Shut down** button will open a pop-up window verifying the instance shutdown
    operation. Click the **Shut down all** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Moving forward, you may want to install and use a JupyterLab extension that
    automatically turns off certain resources during periods of inactivity similar
    to the **SageMaker Studio auto-shutdown extension**. You can find the extension
    here: [https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension](https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension).'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Even after installing the extension, it is still recommended to manually check
    and turn off the resources after using SageMaker Studio. Make sure to perform
    regular inspections and cleanup of resources as well.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, the processed version of the dataset should be stored in the
    destination S3 path you copied to a text editor in your local machine. In the
    next set of steps, we will download this into the AWS CloudShell environment and
    check whether the expected changes are reflected in the downloaded file:'
  prefs: []
  type: TYPE_NORMAL
- en: In SageMaker Studio, open the **File** menu and select **Log out** from the
    list of options. This will redirect you back to the **SageMaker Domain** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to **CloudShell** by clicking the icon highlighted in *Figure 5.30*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.30 – Navigating to CloudShell ](img/B18638_05_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.30 – Navigating to CloudShell
  prefs: []
  type: TYPE_NORMAL
- en: We can find this button in the upper-right-hand corner of the AWS Management
    Console. You may also use the search bar to navigate to the CloudShell console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the terminal is ready, move all files in the CloudShell environment to
    the `/tmp` directory by running the following (after **$**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `aws s3 cp` command to copy the generated CSV file stored in S3 to
    the CloudShell environment. Make sure to replace `<PASTE S3 URL>` with the S3
    URL copied from the **Save to S3 with a SageMaker Processing Job** notebook to
    a text editor on your local machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Recursively list the files and directories using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a CSV file stored inside `<UUID>/default`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, use the `head` command to inspect the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should give us the first few lines of the CSV file, similar to what we
    have in *Figure 5.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.31 – Verifying the changes ](img/B18638_05_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.31 – Verifying the changes
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the dataset has the new `has_booking_changes` column containing
    `true` and `false` values. You may inspect the CSV file further and verify that
    there are no more `-1` values under the `children` column. We will leave this
    to you as an exercise (that is, verifying that there are no more `-1` values under
    the `children` column of the CSV file).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have finished using both Amazon SageMaker Data Wrangler and AWS
    Glue DataBrew to process and analyze a sample dataset, you might be wondering
    when to use one of these tools over the other. Here are some general recommendations
    when deciding:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are planning to use custom transforms using PySpark similar to those
    we performed in this chapter, then you may want to use Amazon SageMaker Data Wrangler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the source, connection, or file type format is not supported in SageMaker
    Data Wrangler (for example, Microsoft Excel workbook format or `.xlsx` files),
    then you may want to use AWS Glue Data Brew.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to export the data processing workflow and automatically generate
    a Jupyter notebook, then you may want to use Amazon SageMaker Data Wrangler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the primary users of the tool have minimal coding experience and would prefer
    processing and analyzing the data without reading, customizing, or writing a single
    line of code, then AWS Glue Data Brew may be used instead of Amazon SageMaker
    Data Wrangler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, these are just some of the guidelines you can use but the decision
    on which tool to use will ultimately depend on the context of the work that needs
    to be done, along with the limitations of the tools at the time that the decision
    needs to be made. Features and limitations change over time, so make sure to review
    as many angles as possible when deciding.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data needs to be cleaned, analyzed, and prepared before it is used to train
    ML models. Since it takes time and effort to work on these types of requirements,
    it is recommended to use no-code or low-code solutions such as AWS Glue DataBrew
    and Amazon SageMaker Data Wrangler when analyzing and processing our data. In
    this chapter, we were able to use these two services to analyze and process our
    sample dataset. Starting with a sample “dirty” dataset, we performed a variety
    of transformations and operations, which included (1) profiling and analyzing
    the data, (2) filtering out rows containing invalid data, (3) creating a new column
    from an existing one, (4) exporting the results into an output location, and (5)
    verifying whether the transformations have been applied to the output file.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a closer look at Amazon SageMaker and we will
    dive deeper into how we can use this managed service when performing machine learning
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics covered in this chapter, feel free to check
    out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*AWS Glue DataBrew product and service integrations* ([https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/databrew-integrations.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security in AWS Glue DataBrew* ([https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml](https://docs.aws.amazon.com/databrew/latest/dg/security.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Create and Use a Data Wrangler Flow* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-data-flow.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Wrangler – Transform* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-transform.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data Wrangler – Troubleshooting* ([https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/data-wrangler-trouble-shooting.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 3: Diving Deeper with Relevant Model Training and Deployment Solutions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, readers will learn the relevant model training and deployment
    solutions using the different capabilities and features of Amazon SageMaker, along
    with other AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18638_06.xhtml#_idTextAnchor132), *SageMaker Training and Debugging
    Solutions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18638_07.xhtml#_idTextAnchor151), *SageMaker Deployment Solutions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
