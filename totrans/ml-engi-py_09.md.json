["```py\nconda env create â€“f mlewp-chapter09.yml \n```", "```py\nairflow standalone \n```", "```py\n    pip install apache-airflow\n    pip install pyspark\n    pip install apache-airflow-providers-apache-spark \n    ```", "```py\n    from datetime import datetime\n    from airflow.models import DAG\n    from airflow.providers.apache.spark.operators.spark_jdbc import SparkJDBCOperator\n    from airflow.providers.apache.spark.operators.spark_sql import SparkSqlOperator\n    from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n\n    DAG_ID = \"spark_example\"\n    with DAG(\n        dag_id=DAG_ID,\n        schedule=None,\n        start_date=datetime(2023, 5, 1),\n        catchup=False,\n        tags=[\"example\"],\n    ) as dag:\n        submit_job = SparkSubmitOperator(\n            application=\\\n               \"${SPARK_HOME}/examples/src/main/python/spark-script.py\",\n               task_id=\"submit_job\"\n        ) \n    ```", "```py\n    if __name__ == \"__main__\":\n        dag.test() \n    ```", "```py\nfrom __future__ import annotations\nimport datetime\nimport pendulum\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom utils.summarize import LLMSummarizer\nfrom utils.cluster import Clusterer\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\n# Bucket name could be read in as an environment variable.\nbucket_name = \"etml-data\"\ndate = datetime.datetime.now().strftime(\"%Y%m%d\")\nfile_name = f\"taxi-rides-{date}.json\"\n\nwith DAG(\n    dag_id=\"etml_dag\",\n    start_date=pendulum.datetime(2021, 10, 1),\n    schedule_interval=\"@daily\",\n    catchup=False,\n) as dag:\n    logging.info(\"DAG started ...\")\n    logging.info(\"Extracting and clustering data ...\")\n    extract_cluster_load_task = PythonOperator(\n        task_id=\"extract_cluster_save\",\n        python_callable=Clusterer(bucket_name, file_name).\\\n                                  cluster_and_label,\n        op_kwargs={\"features\": [\"ride_dist\", \"ride_time\"]}\n    )\n\n    logging.info(\"Extracting and summarizing data ...\")\n    extract_summarize_load_task = PythonOperator(\n        task_id=\"extract_summarize\",\n        python_callable=LLMSummarizer(bucket_name, file_name).summarize\n    )\n\n    extract_cluster_load_task >> extract_summarize_load_task \n sequentially after one another using the >> operator. Each task performs the following pieces of work:\n```", "```py\nimport boto3\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom utils.extractor import Extractor\n\nmodel_params = {\n    'eps': 0.3,\n    'min_samples': 10,\n}\n\nclass Clusterer:\n    def __init__(\n        self, bucket_name: str, \n        file_name: str, \n        model_params: dict = model_params\n    ) -> None:\n        self.model_params = model_params\n        self.bucket_name = bucket_name\n        self.file_name = file_name\n\n    def cluster_and_label(self, features: list) -> None:\n        extractor = Extractor(self.bucket_name, self.file_name)\n        df = extractor.extract_data()\n        df_features = df[features]\n        df_features = StandardScaler().fit_transform(df_features)\n        db = DBSCAN(**self.model_params).fit(df_features)\n        # Find labels from the clustering\n        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n        core_samples_mask[db.core_sample_indices_] = True\n        labels = db.labels_\n        # Add labels to the dataset and return.\n        df['label'] = labels\n\n        date = datetime.datetime.now().strftime(\"%Y%m%d\")\n        boto3.client('s3').put_object(\n            Body=df.to_json(orient='records'), \n            Bucket=self.bucket_name, \n            Key=f\"clustered_data_{date}.json\"\n        ) \n```", "```py\nimport pandas as pd\nimport boto3\n\nclass Extractor:\n    def __init__(self, bucket_name: str, file_name: str) -> None:\n        self.bucket_name = bucket_name\n        self.file_name = file_name\n\n    def extract_data(self) -> pd.DataFrame:\n        s3 = boto3.client('s3')\n        obj = s3.get_object(Bucket=self.bucket_name, Key=self.file_name)\n        df = pd.read_json(obj['Body'])\n        return df \n```", "```py\nopenai.api_key = os.environ['OPENAI_API_KEY']\n\nclass LLMSummarizer:\n    def __init__(self, bucket_name: str, file_name: str) -> None:\n        self.bucket_name = bucket_name\n        self.file_name = file_name\n\n    def summarize(self) -> None:\n        extractor = Extractor(self.bucket_name, self.file_name)\n        df = extractor.extract_data()\n        df['summary'] = ''\n\n        df['prompt'] = df.apply(\n                    lambda x:self.format_prompt(\n                        x['news'],\n                        x['weather'],\n                        x['traffic']\n                    ),\n                    axis=1\n        )\n        df.loc[df['label']==-1, 'summary'] = df.loc[df['label']==-1,\n               'prompt'].apply(lambda x: self.generate_summary(x))\n        date = datetime.datetime.now().strftime(\"%Y%m%d\")\n        boto3.client('s3').put_object(\n            Body=df.to_json(orient='records'), \n            Bucket=self.bucket_name, \n            Key=f\"clustered_summarized_{date}.json\"\n        )\n\n    def format_prompt(self, news: str, weather: str, traffic: str) -> str:\n        prompt = dedent(f'''\n            The following information describes conditions relevant to\n            taxi journeys through a single day in Glasgow, Scotland.\n            News: {news}\n            Weather: {weather}\n            Traffic: {traffic}\n            Summarise the above information in 3 sentences or less.\n            ''')\n        return prompt\n\n    def generate_summary(self, prompt: str) -> str:\n        # Try chatgpt api and fall back if not working\n        try:\n            response = openai.ChatCompletion.create(\n                model = \"gpt-3.5-turbo\",\n                temperature = 0.3,\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n            )\n            return response.choices[0].message['content']\n        except:\n            response = openai.Completion.create(\n                model=\"text-davinci-003\",\n                prompt = prompt\n            )\n            return response['choices'][0]['text'] \n```", "```py\nairflow info \n```"]