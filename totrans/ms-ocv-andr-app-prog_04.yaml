- en: Chapter 4. Drilling Deeper into Object Detection – Using Cascade Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at some very sophisticated algorithms used
    for object detection. In this chapter, we plan to look further into a different
    set of algorithms, known as cascade classifiers and HOG descriptors. These algorithms
    are widely used to detect human expressions and find application in surveillance
    systems, face recognition systems, and other simple biometric systems. Face detection
    was one of the first applications of **cascade classifiers** (Haar-cascade classifier)
    and from then on, there have been many different applications that have been developed.
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever wondered how cameras detect smiling faces in an image and click
    a picture automatically? It is no rocket science. This chapter will talk about
    the different ways of detecting human expressions, using which you can build your
    own version of the aforementioned applications on an Android platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take a look at the following algorithms in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Cascade classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HOG descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to cascade classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are cascade classifiers? Let's take a look at both the words individually
    and then combine them to see what the phrase actually means. Classifiers are like
    black boxes that classify objects into various classes on the basis of a training
    set. Initially, we take a large set of training data, feed it to any learning
    algorithm, and compute a trained model (classifier), which is capable of classifying
    new unknown data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand the word cascade. In the literal sense of the word, cascading
    means to form a chain. In the current context, cascading implies forming a multistage
    classifier, where the output of one stage is passed on to the next stage, and
    so on. Cascade classifiers are used in situations where you have low computational
    power and you do not want to compromise on the speed of your algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cascade classifiers that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Haar cascades (Viola and Jones – face detection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LBP cascades
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's briefly understand Haar and LBP Cascades and then build an Android application
    that uses these cascades to detect faces in images.
  prefs: []
  type: TYPE_NORMAL
- en: Haar cascades
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first real-time face detection algorithms, developed by Viola and
    Jones, was inspired by the concept of Haar wavelets. The algorithm exploits the
    inherent structure and similarities in human faces. For example, in every human
    face, the eye region is darker than the cheeks, and the nose bridge region is
    darker than the eyes. Using such characteristics of a human face, we learn the
    generic models of the face and then use these trained models to detect faces in
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we feed a learning algorithm with positive images (images with faces)
    and negative images (images with out faces) and learn the classifier. Then we
    extract Haar features from the images using convolutional kernels (as shown in
    the following image). Feature values are obtained by subtracting the sum of white
    pixels under the white rectangle from the sum of pixels under the black rectangle.
    We slide these kernels (nothing but Haar features) over the entire image and calculate
    the feature values. If the value is above a certain user-defined threshold, we
    say that there is a match, otherwise we reject that region. To reduce calculations,
    we make use of integral images.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An explanation of integral images can be found at [http://en.wikipedia.org/wiki/Summed_area_table](http://en.wikipedia.org/wiki/Summed_area_table).
  prefs: []
  type: TYPE_NORMAL
- en: '![Haar cascades](img/B02052_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Haar features
  prefs: []
  type: TYPE_NORMAL
- en: Training the classifier every time before using it is unacceptable in terms
    of the performance because it takes a lot of time; sometimes up to 6-7 hours or
    more. Hence, we use the pretrained classifiers provided by OpenCV (or any other
    source).
  prefs: []
  type: TYPE_NORMAL
- en: LBP cascades
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Local Binary Patterns** (**LBP**) cascade is another type of a cascade
    classifier that is used widely in computer vision. Compared to Haar cascades,
    LBP cascades deal with integers rather than double values. So, both training and
    testing is faster with LBP cascades and hence is preferred while developing embedded
    applications. Another important property of LBP is their tolerance against illumination
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: In LBP, an 8-bit binary feature vector is created for each pixel in the image
    by considering the eight neighboring pixels (top-left, top-right, left, right,
    bottom-left, and bottom-right). For every neighboring pixel, there is a corresponding
    bit which is assigned a value 1 if the pixel value is greater than the center
    pixel's value, otherwise 0\. The 8-bit feature vector is treated as a binary number
    (later convert it to a decimal value), and using the decimal values for each pixel,
    a 256-bin histogram is computed. This histogram is used as a representative of
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'LBP features have some primitives coded in them, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![LBP cascades](img/B02052_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Examples of texture primitives
  prefs: []
  type: TYPE_NORMAL
- en: For Haar cascade, we also make a set of positive images (with faces) and negative
    images (without faces). We compute histograms for each image and feed it to any
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection using the cascade classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most common applications of the cascade classifier is face detection.
    Implementation for both Haar and LBP classifiers on Android using OpenCV is very
    similar; the only difference is in the model that we use to detect faces. Let''s
    work on a generic application for face detection and make relevant changes to
    the application to accommodate both Haar and LBP cascades. The application will
    display the camera preview on the entire screen (landscape orientation) and make
    rectangles around faces in each frame. It will also provide an option to switch
    between the front and back camera. Following are the steps to create this application:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new Eclipse (or Android Studio) project with a blank activity and call
    the application *Face Detection*. It will be a landscape application with a fullscreen
    camera preview.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the application tag, add the following line to make a fullscreen application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Give the following permissions in `AndroidManifest.xml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the main activity, add a camera preview view. This will display the camera''s
    output on the screen. Add the view using the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCV provides two camera preview views: `JavaCameraView` and `NativeCameraView`.
    Both the views work in a similar way except for a few differences. Refer to [http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html?highlight=nativecameraview](http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html?highlight=nativecameraview)
    for a detailed explanation of the differences.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this application, we will implement the `CvCameraViewListener2` interface
    that has function definitions that provide some control over the camera (refer
    to the camera preview tutorial of OpenCV). We will take a look at these functions
    later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other applications seen so far in this book, this application has a different
    implementation of the `BaseLoaderCallback` class (for those who are not able to
    recollect, the `BaseLoaderCallback` class initializes and loads OpenCV modules
    in the application).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this application, we will load the cascade classifiers after we have loaded
    OpenCV in our application. Here is the `BaseLoaderCallback` class for this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we first check whether OpenCV was successfully
    loaded. After doing this, we copy the cascade file from the project resources
    to our application using `InputStream` and `FileOutputStream`, as shown next.
    Create a new folder `cascade` and copy the contents of the cascade file to a new
    file in that folder. Now comes the difference between using Haar cascades and
    LBP cascades. Replace `<INSERT_RESOURCE_IDENTIFIER>` with your favorite cascade
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The rest of the code works independently of your choice of the type of
    cascade.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV provides pre-learnt cascades for both Haar and LBP. Copy the cascade
    file to the `res/raw` folder in your Android project. Let's assume that your cascade
    files for Haar and LBP are named `haar_cascade.xml` and `lbp_cascade.xml` respectively.
    Replace `<INSERT_RESOURCE_IDENTIFIER>` with `R.raw.id.haar_casacde` or `R.raw.id.lbp_cascade`,
    depending on which classifier you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why we copy and save at the same time is to bring the file from
    your project directory into your phone''s filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After this is done, create a new `CascadeClassifier` object that will be used
    later to detect faces in the camera feed, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, we have been able to initialize OpenCV in our project, and we have
    loaded our favorite cascade classifier in to the application. The next step is
    to get our camera preview ready. As mentioned earlier, we are implementing the
    `CvCameraViewListener2` interface and hence, we need to implement its member functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Another function that needs to be implemented is `onCameraFrame()`. This is
    where all the magic happens. In this function, we will process each frame and
    find faces in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we first store the output of the camera in `mRgba`, and `mGray` stores
    the grayscale image of the camera output. Then we check whether we are using the
    front camera or the back camera of our phone (how to handle the front camera is
    explained later in this chapter) through a Boolean value `mIsFrontCamera` (data
    member of the class). If the front camera is being used, just flip the image.
    Now create a `MatOfRect` object that will store the rectangles that bound the
    faces in the frame. Then, call the magical function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `detectMultiScale()` function takes in a grayscale image and returns rectangles
    that bound the faces (if any). The third parameter of the function is the scaling
    factor that specifies how much the image size is reduced at each image scale.
    For more accurate results, face detection happens at different scales. The last
    two parameters are the minimum and maximum size of the face that can be detected.
    These parameters sort of decide the speed at which your application runs. Having
    a minimum size can make your application perform poorly, that is, have very few
    frames per second. Be careful while setting these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Done! The application is almost complete with just one bit of functionality
    remaining: handling the front camera. In order to do this, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first add a menu option in the application''s menu that allows the user
    to switch between the front and back camera, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `onOptionsItemSelected()` function, add the functionality to switch
    between cameras:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Whenever the user selects this option, we first toggle the `isFrontCamera`
    value. After this, we change the camera index of the `mOpenCvCameraView` object
    by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The default camera index in Android is `-1`, which represents the back camera.
    The front camera's index is 1 (this is not a fixed number; it can vary from one
    phone to another). Set the camera index according to the `isFrontCamera` value,
    as shown in the preceding code, and set the toast message to notify the user.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have successfully built our own version of a face detection application!
  prefs: []
  type: TYPE_NORMAL
- en: HOG descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Histogram of Oriented Gradients** (**HOG**) descriptors are feature descriptors
    that use the direction of intensity of the gradients and edge directions. For
    HOG descriptors, we divide the image into small cells, compute a histogram for
    each cell, and further combine these histograms to compute one single descriptor.
    They are similar to SIFT descriptors in the sense that both use image gradients
    and both divide the image into spatial bins and form a histogram, but SIFT descriptors
    help you to match local regions (using keypoint locations), while HOG descriptors
    use sliding windows to detect objects. The HOG descriptor works well with geometric
    and illumination transformations, but does not work well with object orientations
    (unlike SIFT, which works well with change in orientations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The HOG descriptor is divided into multiple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computing gradient**: We first calculate the gradient values for all the
    pixels in the image using any derivative mask over the image in horizontal and
    vertical directions (you can choose from either one direction or both directions).
    Some common derivative masks are the Sobel operator, Prewitt operator, and the
    likes, but the original algorithm recommends that you use a 1D derivative mask,
    that is, [-1, 0, +1].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orientation binning**: Create a histogram of the weighted gradients that
    were computed in the previous step. The gradient values are divided into bin values,
    ranging from 0 to 180, or 0 to 360 (depending on whether we are using signed or
    unsigned gradient values).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combining cells to form blocks**: After computing histograms for each cell,
    we combine these cells into blocks and form a combined histogram of the block
    using its constituent cell''s normalized histograms. The final HOG descriptor
    is a vector of the normalized histograms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building the classifier**: In the final step of the algorithm, feed the HOG
    feature vectors that were computed in the previous step in to your favorite learning
    algorithm, and build a model that will later be used to detect objects in images:![HOG
    descriptors](img/B02052_04_03.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flowchart of a HOG Descriptor
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now take a look at an Android application that detects objects using HOG
    descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: Since OpenCV provides a pretrained HOG descriptor to detect people in images,
    we will write an Android application that can detect people in images (we won't
    have to train our descriptor). Since the calculations involved in computing HOG
    descriptors are expensive, making a real-time application for a mobile platform
    with limited computational resources turns out to be a difficult task. So instead,
    we will build an application that will only detect people in single images.
  prefs: []
  type: TYPE_NORMAL
- en: For this, let's refer to [Chapter 2](ch02.html "Chapter 2. Detecting Basic Features
    in Images"), *Detecting Basic Features in Images*, where we built an application
    that could read images from your phone's gallery and perform any operation based
    on the user's choice (hopefully, you still have that project saved somewhere in
    your computer). We won't need the entire application. We will only take the base
    of that application and make a new function to detect people in any image from
    the gallery.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have the project from [Chapter 2](ch02.html "Chapter 2. Detecting Basic
    Features in Images"), *Detecting Basic Features in Images*, saved, make the following
    changes to it. Add a new menu option *Detect Face* to the application menu (refer
    to [Chapter 2](ch02.html "Chapter 2. Detecting Basic Features in Images"), *Detecting
    Basic Features in Images*), and in the `onSelectedOptionItem()` function, add
    the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Make a new function `HOGDescriptor()`, where we''ll implement people detection
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we first convert the image to a grayscale image.
    Then, we initialize `HOGDescriptor` with a pretrained model (using SVM) using
    the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The next step is simple; we will call the `detectMultiScale()` function, which
    will return all the faces in the image. The second parameter in the function stores
    the regions where people were detected. We will then iterate through all such
    regions and draw rectangles around them on the image.
  prefs: []
  type: TYPE_NORMAL
- en: Project – Happy Camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practice is better than theory. It's time to apply your learning from this chapter
    and build a cool camera application, which automatically clicks a picture when
    it detects smiling faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'The trick is that we will use two different types of cascade classifiers. First,
    we will use Haar cascades to find faces on the image and store the positions of
    all the faces. Then we will use the Haar cascades to detect smiles in an image
    and store them. Now we try to match the face with a smile. For each smile, we
    find the corresponding face in the image. This is simple: if the smiling region
    is within any detected face region, we say that it''s a match.'
  prefs: []
  type: TYPE_NORMAL
- en: After locating all the smiling faces in the image, find the ratio of the smiling
    faces to all faces to nonsmiling faces, and if that ratio is greater than a certain
    threshold we say that it's a happy picture and click the image. Though one thing
    to note here is the ratio that we are using. We can use a different metric to
    tag an image as a happy image. If we calculate the ratio of smiling faces to total
    faces, there is a problem that if you have just two people in the image and one
    of them is not smiling (or has a standard expression), then our application will
    not click an image. Hence, to avoid such situations, we choose to have a relaxed
    ratio of smiling faces to nonsmiling faces in order to classify images as happy
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we go about building this application? Most parts of the application
    have already been discussed in this chapter. The remaining parts of the application
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a smile detector**: This is very simple. It is exactly the same as
    what we did to detect faces; instead here, we will use Haar cascades for smiles.
    You can find a pretrained model at [https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_smile.xml](https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_smile.xml).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Correlating faces and smiles**: Once we have faces and smiles, we need to
    find matching pairs of faces and smiles in the image. Why do we want to correlate
    them? Why not use the number of smiles directly? Yes, we can do that. It is not
    necessary to correlate faces and smiles. The only advantage of doing this extra
    step is to reduce the false positives. If there is no corresponding face for a
    smile, we can choose to ignore that smile in our calculations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tagging happy images**: Once you have the face and smile pairs ready, calculate
    the ratio (explained earlier) and make a decision on whether you want to save
    that image or not.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Actually saving the image**: After tagging the image as a happy image, make
    a function that will actually save the image to your phone.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You just made a cool camera application!
  prefs: []
  type: TYPE_NORMAL
- en: Only after you have tried to build this application yourself, you can take a
    look at a sample implementation from the code bundle that accompanies this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was a continuation of the last chapter, where we saw some basic
    feature detection algorithms. Here we have learnt a few more algorithms that can
    be used in face, eye, and person detection. Cascade classifiers are a type of
    supervised learning models, where we first train a classifier with some labelled
    data, and then use the trained model to detect new unencountered data.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will take a look at topics such as image stitching
    and how to use machine learning in computer vision algorithms.
  prefs: []
  type: TYPE_NORMAL
