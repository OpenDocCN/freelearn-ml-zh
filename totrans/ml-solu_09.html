<html><head></head><body><div id="sbo-rt-content"><div class="chapter" title="Chapter 9. Building a Real-Time Object Recognition App"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Building a Real-Time Object Recognition App</h1></div></div></div><p>In this chapter, we will build an application that can detect objects. This application will help us recognize<a id="id1039" class="indexterm"/> the object present in an image or a video feed. We will be using real-time input, such as a live video stream from our webcam, and our real-time object detection application will detect the objects present in the video stream. We will be using a live video stream, which is the main reason why this kind of object detection is called <span class="strong"><strong>Real-Time Object Detection<a id="id1040" class="indexterm"/></strong></span>. In this chapter, we will be using the <span class="strong"><strong>Transfer Learning</strong></span> methodology to <a id="id1041" class="indexterm"/>build Real-Time Object Detection. I will explain Transfer Learning in detail during the course of the chapter.</p><p>In this chapter, we will cover the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing the problem statement</li><li class="listitem" style="list-style-type: disc">Understanding the dataset</li><li class="listitem" style="list-style-type: disc">Transfer Learning</li><li class="listitem" style="list-style-type: disc">Setting up the coding environment</li><li class="listitem" style="list-style-type: disc">Features engineering for the baseline model</li><li class="listitem" style="list-style-type: disc">Selecting the <span class="strong"><strong>Machine Learning</strong></span> (<span class="strong"><strong>ML</strong></span>) algorithm</li><li class="listitem" style="list-style-type: disc">Building the baseline model</li><li class="listitem" style="list-style-type: disc">Understanding the testing metrics </li><li class="listitem" style="list-style-type: disc">Testing the baseline model </li><li class="listitem" style="list-style-type: disc">Problems with the existing approach</li><li class="listitem" style="list-style-type: disc">How to optimize the existing approach<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the process for optimization</li></ul></div></li><li class="listitem" style="list-style-type: disc">Implementing the revised approach <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Testing the revised approach </li><li class="listitem" style="list-style-type: disc">Understanding problems with the revised approach </li></ul></div></li><li class="listitem" style="list-style-type: disc">The best approach <div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the best approach</li></ul></div></li><li class="listitem" style="list-style-type: disc">Summary</li></ul></div><div class="section" title="Introducing the problem statement"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec87"/>Introducing the problem statement</h1></div></div></div><p>In this<a id="id1042" class="indexterm"/> chapter, we will be building an object detection application. We won't just be detecting objects, but we will be building the application that detects the objects in real time. This application can be used in self-driving cars, for segregation tasks in the agricultural field, or even in the robotics field. Let's understand our goal and what we are actually building.</p><p>We want to build an application in which we will provide the live webcam video stream or the live video stream as the input. Our application will use pre-trained Machine Learning models, which will help us predict the objects that appear in the video. This means that, if there is a person in the video, then our application can identify the person as a person. If the video contains a chair or a cup or a cell phone, then our application should identify all these objects in the correct manner. So, our main goal in this chapter is to build an application that can detect the objects in images and videos. In this chapter, you will also learn the concept of Transfer Learning. All our approaches are based on Deep Learning techniques.</p><p>In the next section, we will be discussing the dataset.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec88"/>Understanding the dataset</h1></div></div></div><p>In this section, we <a id="id1043" class="indexterm"/>will cover the dataset on which the Deep Learning models have been trained. There are two datasets that are heavily<a id="id1044" class="indexterm"/> used when we are trying to build the object detection application, and those datasets are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The COCO dataset</li><li class="listitem" style="list-style-type: disc">The PASCAL VOC dataset</li></ul></div><p>We will look at each of the datasets one by one.</p><div class="section" title="The COCO dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec151"/>The COCO dataset</h2></div></div></div><p>COCO stands for Common<a id="id1045" class="indexterm"/> object in context. So, the short form for this dataset is the COCO dataset. Many tech giants, such<a id="id1046" class="indexterm"/> as Google, Facebook, Microsoft, and so on are using COCO data to build amazing applications for object detection, object segmentation, and so on. You can find details regarding this dataset at this official web page:</p><p>
<a class="ulink" href="http://cocodataset.org/#home">http://cocodataset.org/#home</a>
</p><p>The COCO dataset is<a id="id1047" class="indexterm"/> a large-scale object detection, segmentation, and captioning dataset. In this dataset, there are a total of 330,000 images, of which  more than 200,000  are labeled. These images contain 1.5 million object instances with 80 object categories. All the labeled images have five different captions; so, our machine learning approach is able to generalize the object detection and segmentation effectively.</p><p>By using COCO Explorer, we can explore the COCO dataset. You can use the<a class="ulink" href="http://cocodataset.org/#explore"> http://cocodataset.org/#explore</a> URL to explore the dataset. COCO Explorer is great user interface. You just need to select objects tags such as <span class="emphasis"><em>I want to see images with a person, a bicycle, and a bus in the picture</em></span> and the explorer provides you images with a person, a bicycle, and a bus in it. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_01.jpg" alt="The COCO dataset" width="801" height="843"/><div class="caption"><p>Figure 9.1: COCO Dataset explorer snippet</p></div></div><p>In each image, the proper object boundary has been provided for each of the major objects. This is<a id="id1048" class="indexterm"/> the main reason why this dataset is great if you want to build your own computer vision application from scratch.</p><p>Here, we are not <a id="id1049" class="indexterm"/>downloading the dataset because if we need to train the models from scratch on this dataset, then it will require a lot of time and lots of GPUs in order to get good accuracy. So, we will be using a pre-trained model, and using Transfer Learning, we will implement real-time object detection. Now let's move on to the PASCAL VOC dataset.</p></div><div class="section" title="The PASCAL VOC dataset"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec152"/>The PASCAL VOC dataset</h2></div></div></div><p>PASCAL stands for Pattern Analysis, Statistical Modeling, and Computational Learning and VOC stands for Visual Object Classes. In this dataset, images are tagged for 20 classes<a id="id1050" class="indexterm"/> for object<a id="id1051" class="indexterm"/> detection. Action classes and person layout taster tagging are available as well. In the person layout taster tagging, the bounding box is all about the label of each part of a person (head, hands, feet). You can refer to the details of this dataset at <a class="ulink" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html</a>.</p><div class="section" title="PASCAL VOC classes"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec119"/>PASCAL VOC classes</h3></div></div></div><p>Images have <a id="id1052" class="indexterm"/>been categorized into four major classes: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Person</li><li class="listitem" style="list-style-type: disc">Animal</li><li class="listitem" style="list-style-type: disc">Vehicle</li><li class="listitem" style="list-style-type: disc">Indoor</li></ul></div><p>Each image is tagged with the preceding major classes, plus there are specific tags given to the objects in the images. Each of the preceding four categories has specific tags, which I have described in the following list:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Person</strong></span>: person</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Animal</strong></span>: bird, cat, cow, dog, horse, sheep</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Vehicle</strong></span>: aero plane, bicycle, boat, bus, car, motorbike, train</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Indoor</strong></span>: bottle, chair, dining table, potted plant, sofa, tv/monitor</li></ul></div><p>You can refer to the following figure, which will help you understand the tagging in this PASCAL VOC dataset:</p><div class="mediaobject"><img src="Images/B08394_09_02.jpg" alt="PASCAL VOC classes" width="502" height="329"/><div class="caption"><p>Figure 9.2: PASCAL VOC tagged image example</p></div></div><p>As you can see<a id="id1053" class="indexterm"/> in the preceding figure, two major classes have been tagged: Person and Animal. Specific tagging has been given for objects appearing in the image, that is, the person and the sheep. We are not downloading this dataset; we will be using the pre-trained model, which was trained using this dataset.</p><p>You have heard the terms Transfer Learning and pre-trained model a lot until now. Let's understand what they are.</p></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Transfer Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec89"/>Transfer Learning</h1></div></div></div><p>In this <a id="id1054" class="indexterm"/>section, we will look at what Transfer Learning is and how it is going to be useful for us as we build real-time object detection. We <a id="id1055" class="indexterm"/>divide this section into the following parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is Transfer Learning?</li><li class="listitem" style="list-style-type: disc">What is a pre-trained model?</li><li class="listitem" style="list-style-type: disc">Why should we use a pre-trained model?</li><li class="listitem" style="list-style-type: disc">How can we use the pre-trained model?</li></ul></div><p>Let's start with the first question.</p><div class="section" title="What is Transfer Learning?"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec153"/>What is Transfer Learning?</h2></div></div></div><p>We will be <a id="id1056" class="indexterm"/>looking at the intuition behind Transfer Learning first and, then, we will cover its technical definition. Let me explain this concept through a simple teacher-student analogy. A teacher has many years of experience in teaching certain specific topics or subjects. Whatever information the teacher has, they deliver it to their students. So, the process of teaching is all about transferring knowledge from the teacher to the student. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_03.jpg" alt="What is Transfer Learning?" width="663" height="670"/><div class="caption"><p>Figure 9.3: An overview of Transfer Learning</p></div></div><p>Now, remember this<a id="id1057" class="indexterm"/> analogy; we will apply it to neural networks. When we train a neural network, it gains knowledge from the given dataset. This trained neural network has some weights that help it learn from the given dataset; after training, we can store these weights in a binary format. The weight that we have stored in the binary format can be extracted and then transferred to any other neural network. So, instead of training the neural network from scratch, we transfer the knowledge that the previously trained model gained. We are transferring the learned features to the new neural network and this will save a lot of our time. If we have an already trained model for a particular application, then we will apply it to the new but similar type of application, which in turn will help save time.</p><p>Now, it's time <a id="id1058" class="indexterm"/>to define Transfer Learning in more technical terms. Transfer Learning is a research problem in Machine Learning that focuses on storing knowledge gained while solving a particular problem and applying it to a different but related problem. Sometimes, Transfer Learning is also called inductive transfer. Let's take a solid example to solidify your vision. If we build a Machine Learning model that gained knowledge while learning to recognize cars, it can also be applied when we are trying to recognize trucks. In this chapter, we are using Transfer Learning in order to build this real-time object detection application.</p></div><div class="section" title="What is a pre-trained model?"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec154"/>What is a pre-trained model?</h2></div></div></div><p>I want to <a id="id1059" class="indexterm"/>give you a simple explanation. A pre-trained model is one that is created and built by someone else to solve a specific problem. This means that we are using a model which has already been trained and plugging and playing with it. By using the pre-trained model, we can build new applications with similar domains.</p><p>Let me give you an example: suppose we want to create a self-driving car. In order to build it, our first step would be to build a decent object recognition system. You can spend a year or more to build the decent image and object recognition algorithm from scratch, or you can use a pre-trained model, such as the Google inception model or the YOLO model, which has been built using the PASCAL VOC dataset.</p><p>There are some<a id="id1060" class="indexterm"/> advantages of using a pre-trained model, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A pre-trained model may not give you 100% accuracy, but it saves a lot of effort.</li><li class="listitem" style="list-style-type: disc">You can optimize the accuracy of the real problem on which you are working rather than making an algorithm from scratch; as we say sometimes, there is no need to reinvent the wheel.</li><li class="listitem" style="list-style-type: disc">There are many libraries available that can help us save trained models, so we can load and use them easily whenever we need them.</li></ul></div><p>Apart from the advantages the pre-trained models provide us, we need to understand other real reasons why we should use pre-trained models. So, let's discuss that in our next section.</p></div><div class="section" title="Why should we use a pre-trained model?"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec155"/>Why should we use a pre-trained model?</h2></div></div></div><p>When we are<a id="id1061" class="indexterm"/> focusing on developing the existing algorithms in a different manner, our goal would be to build an algorithm that would outperform every other existing algorithm and making it more efficient. If we just focus on the research part, then this can be a nice approach to develop an efficient and accurate algorithm, but if your vision is to make an application and this algorithm is one part of the entire application, then you should focus on how quickly and efficiently you can build the application. Let's understand this through an example.</p><p>In this chapter, we want to build real-time object detection techniques. Now, my primary focus would be on building an application that would detect objects in real time. Not just that; I need to combine object detection and real-time tracking activity as well. If I ignore my primary goal and start making the new but effective object detection algorithm, then I will lose focus. My focus will be on building the entire real-time object detection application and not just a certain algorithm.</p><p>Sometimes, if we lose focus and try to build the algorithm from scratch, then it will take a lot of time. We will also waste our efforts, because some smart people in the world may have already built it for us. When we use this already developed algorithm, we will save time and focus on our application. After building a prototype of our actual application, we will have time to improvise it so that it can be used by many other people.   </p><p>Let me tell you my story. I started building an image detection application that could be used in the security domain a couple of months ago. At that time, I didn't want to use a pre-trained model, because I wanted to explore how much effort it would take to build an algorithm from scratch. So, I started making one on my own. I tried several different algorithms, such as  SVM, <span class="strong"><strong>Multilayer Perceptron</strong></span> (<span class="strong"><strong>MLP</strong></span>), and <span class="strong"><strong>Convolution Neural Network</strong></span> (<span class="strong"><strong>CNN</strong></span>) models, but I<a id="id1062" class="indexterm"/> got really low accuracy. I lost focus<a id="id1063" class="indexterm"/> on building an image-detection algorithm application that could be used in the security domain and just started focusing on making the algorithm better. After some time, I realized that it would be better if I used a pre-trained model with an optimization technique that would save my time and enable me to build a better solution. After this experience, I tried to explore the option of using Transfer Learning in problem statements I was solving, and if I found that there would be no scope for Transfer Learning, then I would make the algorithms from scratch; otherwise, I preferred using the pre-trained model. So, always explore all the options before building the algorithm. Understand the application usage and build your solution based on that. Suppose you are building your own self-driving car; then, real-time object detection would become an important part of a self-driving car, but it would be just a part of the entire application; therefore, it would be better if we were to use a pre-trained model to detect objects, so that we could use our time to build a quality self-driving car.</p></div><div class="section" title="How can we use a pre-trained model?"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec156"/>How can we use a pre-trained model?</h2></div></div></div><p>Generally, a <a id="id1064" class="indexterm"/>pre-trained model is in the binary format, which can be downloaded and then used in our applications. Some libraries, such as Keras, TensorFlow, Darknet, and so on, already have those pre-models that you can load and use with certain available APIs. These pre-trained networks have the ability to generalize images that are not part of the PASCAL VOC or COCO dataset via Transfer Learning. We can modify the pre-existing model by fine-tuning the model. We don't want to modify the weights too much, because it has been trained on a large dataset using lots of GPUs. The pre-trained model has the ability to generalize the prediction and classification of objects, so we know that this pre-trained model can be generalized enough to give us the best possible outcome. However, we can change some hyperparameters if we want to train the model from scratch. These parameters can be the learning rate, epochs, layer size, and so on. Let's discuss some of them:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Learning Rate: Learning rate <a id="id1065" class="indexterm"/>basically controls how much we should update the weights of neurons. We can use fixed learning rate, decreassing learning rate, momentum-based methods, or adaptive learning rates.  </li><li class="listitem" style="list-style-type: disc">Number of epochs: The <a id="id1066" class="indexterm"/>number of epochs indicates the number of times the entire training dataset should pass through the neural network. We need to increase the number of epochs in order to decrease gap between the test error and the training error.</li><li class="listitem" style="list-style-type: disc">Batch Size:  For <a id="id1067" class="indexterm"/>convolutional neural networks, mini-batch size is usually more preferable. A range of 16 to 128 is really a good choice to start from for convolutional neural networks.    </li><li class="listitem" style="list-style-type: disc">Activation function: As we <a id="id1068" class="indexterm"/>know, activation functions introduce non-linearity to the model. ReLU activation function is the first choice for convolutional neural networks. You can use other activation functions, such as tanh, sigmoid, and so on, as well.</li><li class="listitem" style="list-style-type: disc">Dropout for regularization: Regularization techniques are used to prevent overfitting problems. Dropout is<a id="id1069" class="indexterm"/> the regularization technique for deep neural networks. In this technique, we are dropping out some of the neurons or units in neural networks. The drop out of neurons is based on probability value. The default value for this is 0.5, which is good choice to start with, but we can change the value for regularization after observing training error and testing error.</li></ul></div><p>Here, we will be using pre-trained models such <a id="id1070" class="indexterm"/>as the Caffe pre-trained model, the TensorFow object detraction model, and <span class="strong"><strong>You Only Look Once</strong></span> (<span class="strong"><strong>YOLO</strong></span>). For real-time streaming from our webcam, we are using OpenCV, which is also useful in order to draw bounding boxes.  So first, let's set up the OpenCV environment.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Setting up the coding environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec90"/>Setting up the coding environment </h1></div></div></div><p>In this section, we <a id="id1071" class="indexterm"/>will list down the libraries and equipment you need in order to run the upcoming code. You need to have a webcam that can at least stream the video with good clarity. We will be using OpenCV, TensorFlow, YOLO, Darkflow, and Darknet libraries. I'm not going to explain how to install TensorFlow, because it is an easy process and you can find the documentation for the installation by clicking on<a class="ulink" href="https://www.tensorflow.org/install/install_linux"> https://www.tensorflow.org/install/install_linux</a>.</p><p>In this section, we will be looking at how to set up OpenCV first and, in the upcoming sections, we will see how to set up YOLO, Darkflow, and DarkNet.</p><div class="section" title="Setting up and installing OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec157"/>Setting up and installing OpenCV</h2></div></div></div><p>OpenCV stands for <a id="id1072" class="indexterm"/>Open Source<a id="id1073" class="indexterm"/> Computer Vision. It is designed for computational efficiency, with a strong focus on real-time applications. In this section, you will learn how to<a id="id1074" class="indexterm"/> set up OpenCV. I'm using Ubuntu 16.04 and I have a GPU, so I have already installed CUDA and CUDNN. If you haven't installed CUDA and CUDNN, then you can refer to this GitHub link: <a class="ulink" href="https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64">https://gist.github.com/vbalnt/a0f789d788a99bfb62b61cb809246d64</a>. Once you are done with that, start executing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">This will update the <a id="id1075" class="indexterm"/>software and libraries: <code class="literal">$ sudo apt-get update</code></li><li class="listitem">This will upgrade the OS and install OS-level updates: <code class="literal">$ sudo apt-get upgrade</code></li><li class="listitem">This is for compiling the software: <code class="literal">$ sudo apt-get install build-essential</code></li><li class="listitem">This command installs prerequisites for OpenCV: <code class="literal">$ sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</code></li><li class="listitem">This command installs optional prerequisites for OpenCV: <code class="literal">$ sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev    libdc1394-22-dev</code></li><li class="listitem">Create a directory using this command: <code class="literal">$ sudo mkdir ~/opencv</code></li><li class="listitem">Jump to the directory that we just created: <code class="literal">$ cd ~/opencv</code></li><li class="listitem">Clone the following OpenCV projects from GitHub inside the opencv
 directory:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">$ sudo git clone https://github.com/opencv/opencv.git</code></li><li class="listitem"><code class="literal">$ sudo git clone https://github.com/opencv/opencv_contrib.git</code></li></ol></div></li><li class="listitem">Inside the opencv folder, create another directory named build: <code class="literal">$ sudo mkdir ~/opencv/build</code></li><li class="listitem">Jump to the build directory or folder: <code class="literal">$ cd ~/opencv/build</code></li><li class="listitem">Once you <a id="id1076" class="indexterm"/>are in the build folder location, run this command. It may take some time. If you run this <a id="id1077" class="indexterm"/>command without any error, then proceed to the next step:  <div class="informalexample"><pre class="programlisting">$ sudo cmake -D CMAKE_BUILD_TYPE=RELEASE \
-D CMAKE_INSTALL_PREFIX=/usr/local \
-D INSTALL_C_EXAMPLES=ON \
-D INSTALL_PYTHON_EXAMPLES=ON \
-D WITH_TBB=ON \
-D WITH_V4L=ON \
-D WITH_QT=ON \
-D WITH_OPENGL=ON \
-D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules \
-D BUILD_EXAMPLES=ON ..</pre></div></li><li class="listitem">Identify how many CPU cores you have by using this command.: <code class="literal">$ nproc</code></li><li class="listitem">Once you know the number of cores of the CPU, you can use it to process multi-threading. I have allocated four CPU cores, which means there are four threads running simultaneously. The command for this is <code class="literal">$ make -j4</code> and it compiles all of the classes written in C for OpenCV.</li><li class="listitem">Now, execute this command for the actual installation of OpenCV: <code class="literal">$ sudo make install</code></li><li class="listitem">Add the path to the configuration file: <code class="literal">$ sudo sh -c 'echo "/usr/local/lib" &gt;&gt; /etc/ld.so.conf.d/opencv.conf'</code></li><li class="listitem">Check for the proper configuration using this command: <code class="literal">$ sudo ldconfig</code></li></ol></div><p>Once you have installed OpenCV successfully, we can use this library to stream real-time video. Now, we will start building our baseline model. Let's begin!</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Features engineering for the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec91"/>Features engineering for the baseline model</h1></div></div></div><p>In order to build the <a id="id1078" class="indexterm"/>baseline model, we will use the Caffe implementation of the Google MobileNet SSD detection <a id="id1079" class="indexterm"/>network with<a id="id1080" class="indexterm"/> pre-trained weights. This model has been trained on the PASCAL VOC dataset. So, in this section, we will look at the approach with which this model has been trained by Google. We will understand the basic approach behind MobileNet SSD and use the pre-trained model to help save time. To create this kind of accurate model, we need to have lots of GPUs and training time, so we are using a pre-trained model. This pre-trained MobileNet model uses <span class="strong"><strong>Convolution Neural Net</strong></span> (<span class="strong"><strong>CNN</strong></span>).</p><p>Let's look at <a id="id1081" class="indexterm"/>how the features have been extracted by the MobileNet using CNN. This will help<a id="id1082" class="indexterm"/> us understand the basic idea behind CNN, as well as how MobileNet has been used. The CNN network is made of layers and, when we provide the images to CNN, it scans the region of <a id="id1083" class="indexterm"/>the images and tries to extract the possible objects using the region proposal method. Then, it finds the region with objects, uses the warped region, and generates the CNN features. These features can be the position of the pixels, edges, the length of the edges, the texture of the images, the scale of the region, the lightness or darkness of the picture, object parts, and so on. The CNN network learns these kinds of features by itself. You can refer to the following figure: </p><div class="mediaobject"><img src="Images/B08394_09_04.jpg" alt="Features engineering for the baseline model" width="1000" height="709"/><div class="caption"><p>Figure 9.4: Understanding features extraction in CNN</p></div></div><p>As you can see in the <a id="id1084" class="indexterm"/>preceding image, the region of the image has been scanned and the first CNN layer, made up of C1 and S1, will generate the features. These features can identify the representation of edges that eventually build the whole object. In the second stage, CNN layers <a id="id1085" class="indexterm"/>learn the feature representation that can help the neural network identify parts of the objects. In<a id="id1086" class="indexterm"/> the last stage, it learns all the features that are necessary in order to identify the objects present in the given input images. If you want to explore each and every aspect of the CNN network, you can refer to <a class="ulink" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a>. Don't worry; we will cover the overview of the CNN architecture in the upcoming section, so that you can understand how object detection will work. Now, it's time to explore CNN.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Selecting the machine learning algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec92"/>Selecting the machine learning algorithm</h1></div></div></div><p>We already know<a id="id1087" class="indexterm"/> that we are using <span class="strong"><strong>Convolution Neural Networks</strong></span> (<span class="strong"><strong>CNN</strong></span>) for developing this application. You<a id="id1088" class="indexterm"/> might wonder why we have chosen CNN and not another neural net. You might already know the answer to this question. There are three reasons why we have chosen CNN:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The amount of visual data present nowadays, which is carefully hand-labeled</li><li class="listitem" style="list-style-type: disc">The affordable computation machines through which GPUs open the door for optimization</li><li class="listitem" style="list-style-type: disc">The various kinds of architecture of CNN outperforms the other algorithms </li></ul></div><p>Due to these reasons, we have chosen the CNN with SSD. During the development of the baseline model, we will be using MobileNet, which uses CNN with <span class="strong"><strong>Single Shot Detector </strong></span>(<span class="strong"><strong>SSD</strong></span>) techniques <a id="id1089" class="indexterm"/>underneath. So, in this section, we will look at the architecture of the CNN used during the development of the MobileNet. This will help us understand the pre-trained model.</p><div class="section" title="Architecture of the MobileNet SSD model"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec158"/>Architecture of the MobileNet SSD model</h2></div></div></div><p>MobileNet SSD  is fast and <a id="id1090" class="indexterm"/>does the job<a id="id1091" class="indexterm"/> of object detection in images and video well. This model is faster than <span class="strong"><strong>Region-based Convolution Neural Network</strong></span> (<span class="strong"><strong>R-CNN</strong></span>). SSD achieves<a id="id1092" class="indexterm"/> this speed because it scans images and video frames quite differently.</p><p>In R-CNN, models performed <a id="id1093" class="indexterm"/>region proposals and region classifications in two different steps, given as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, they used a region proposal network in order to generate regions of interest</li><li class="listitem" style="list-style-type: disc">After that, fully connected layers or positive sensitive constitutional layers classified the objects in the selected regions.</li></ul></div><p>These steps <a id="id1094" class="indexterm"/>are at par with the R-CNN, but SSD performs them in a single shot, which means it simultaneously predicts the bounding boxes and the classes of the objects appearing in the bounding boxes. SDD performs the following steps when an image or video streams and set of basic truth labels has been given as the input:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Pass the images through the series of convolution layers that generate the sets of features map in the form of a different size of matrix. The output can be in the form of a 10×10 matrix, a 6×6 matrix, or a 3×3 matrix.</li><li class="listitem" style="list-style-type: disc">For each location in each of the feature maps, use the 3×3 convolution filter in order to evaluate the small set of default bound boxes. These generated default bounding boxes are equivalent to anchor boxes, which are generated using Faster R-CNN.</li><li class="listitem" style="list-style-type: disc">For each box, simultaneously predict the bounding box offset and the class probability for objects.</li><li class="listitem" style="list-style-type: disc">During the training, match the ground truth box with these predicted boxes. This matching is performed by using Intersection of Union (IoU). The best predicted box will be labeled as a positive bounding box. This happens to every boundary box. An IoU with more than 50% truth value has been considered here.</li></ul></div><p>Take a look at the following figure for a graphical representation. MobileNets have streamlined architecture that uses depth-wise separable convolutions in order to build light weight deep neural networks for mobile and embedded vision application. MobileNets are a more efficient ML model for computer vision application.  You can also refer to the original paper for MobileNet at <a class="ulink" href="https://arxiv.org/pdf/1704.04861.pdf">https://arxiv.org/pdf/1704.04861.pdf</a>.</p><div class="mediaobject"><img src="Images/B08394_09_05.jpg" alt="Architecture of the MobileNet SSD model" width="333" height="222"/><div class="caption"><p>Figure 9.5: Basic architectural building blocks for MobileNet SSD</p><p>Image Source: https://arxiv.org/pdf/1704.04861.pdf</p></div></div><p>As you <a id="id1095" class="indexterm"/>can see in the preceding figure, we <a id="id1096" class="indexterm"/>used the standard convolution network with the depth-wise convolution network. MobileNet SDD used the ReLU activation function. You can refer to the following figure to get an idea about what kind of filter shape this network has:</p><div class="mediaobject"><img src="Images/B08394_09_06.jpg" alt="Architecture of the MobileNet SSD model" width="334" height="369"/><div class="caption"><p>Figure 9.6, MobileNet body architecture</p><p>Image Source: https://arxiv.org/pdf/1704.04861.pdf</p></div></div><p>If you want to<a id="id1097" class="indexterm"/> interpret this table, then let's <a id="id1098" class="indexterm"/>consider an example. If we have an original image with a pixel sixe of 224×224, then this Mobilenet network shrinks the image down to 7×7 pixels; it also has 1,024 channels. After this, there is an average pooling layer that works on all the images and generates the vector of a 1×1×1,024 size, which is just a vector of 1,024 elements in reality. If you want to learn more about MobileNet SSD, refer to the following resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9">https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19">https://medium.com/ilenze-com/object-detection-using-deep-learning-for-advanced-users-part-1-183bbbb08b19</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/">http://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/</a></li></ul></div><p>Now, let's move on to the implementation part.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Building the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec93"/>Building the baseline model</h1></div></div></div><p>In this<a id="id1099" class="indexterm"/> section, we will be looking at the coding part. You can refer to the code given at this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model">https://github.com/jalajthanaki/Real_time_object_detection/tree/master/base_line_model</a>.</p><p>First, download the project from the given link and install OpenCV, as per the information given earlier in this chapter. When you download this project folder, there is a <a id="id1100" class="indexterm"/>pre-trained MobileNet SSD that has been implemented using the caffe library, but here, we are using the pre-trained binary model.  We are using OpenCV for loading the pre-trained model as well as streaming the video feeds from the webcam.</p><p>In the code, first, we specify the libraries that we need to import and define the command-line arguments that will be used to run the script. We need to provide the parameter file and the pre-trained model. The name of the parameter file is <code class="literal">MobileNetSSD_deploy.prototxt.txt</code> and the filename for the pre-trained model is <code class="literal">MobileNetSSD_deploy.caffemodel</code>. We have also defined the classes that can be identified by the model. After this, we will load the pre-trained model using OpenCV. You can refer to the coding up to this stage in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_09_07.jpg" alt="Building the baseline model" width="1000" height="505"/><div class="caption"><p>Figure 9.7: Code snippet for the baseline model</p></div></div><p>Now, let's look at how we can stream the video from our webcam. Here, we are using library <code class="literal">imutils</code> and its video API to stream the video from the webcam. Using the start function, we will start the streaming and, after that, we will define the frame size. We grab the frame size and convert it into a blob format. This code always verifies that the detected object confidence score <a id="id1101" class="indexterm"/>will be higher than the minimum confidence score or the minimum threshold of the <a id="id1102" class="indexterm"/>confidence score. Once we get a higher confidence score, we will draw the bounding box for those objects. We can see the objects that have been detected so far. You can refer to the following figure for the video streaming of the baseline model:</p><div class="mediaobject"><img src="Images/B08394_09_08.jpg" alt="Building the baseline model" width="654" height="852"/><div class="caption"><p>Figure 9.8: Code snippet for the baseline model video streaming</p></div></div><p>In order to stop the streaming, we need to break the loop by pressing Q or Ctrl + C and we need to take care that when<a id="id1103" class="indexterm"/> we close the<a id="id1104" class="indexterm"/> program, all windows and processes will stop appropriately. You can see this in the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_09_09.jpg" alt="Building the baseline model" width="553" height="172"/><div class="caption"><p>Figure 9.9: Code snippet for ending the script</p></div></div><p>Before we run the testing of the script, let's understand the testing metrics for the object detection application. Once we understand the testing metrics, we will run the code as well as checking how much accuracy we are getting.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Understanding the testing metrics"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec94"/>Understanding the testing metrics</h1></div></div></div><p>In this <a id="id1105" class="indexterm"/>section, we will cover the testing<a id="id1106" class="indexterm"/> metrics. We will look at the two matrices that will help us understand how to test the object detection application. These testing matrices are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Intersection over Union (IoU)</li><li class="listitem" style="list-style-type: disc">mean Average Precision (mAP)</li></ul></div><div class="section" title="Intersection over Union (IoU)"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec159"/>Intersection over Union (IoU)</h2></div></div></div><p>For <a id="id1107" class="indexterm"/>detection, IoU is<a id="id1108" class="indexterm"/> used in order to find out whether the object proposal is right or not. This is a regular way to determine whether object detection is done perfectly or not. IoU generally takes the set, A, of proposed object pixels and the set of true object pixels, B, and calculates IoU based on the following formula: </p><div class="mediaobject"><img src="Images/B08394_09_30.jpg" alt="Intersection over Union (IoU)" width="164" height="52"/></div><p>Generally, IoU &gt;0.5, which<a id="id1109" class="indexterm"/> means that it was a hit or that it identified the object pixels or boundary box for the object; otherwise, it fails. This is a more formal understanding of the IoU. Now, let's<a id="id1110" class="indexterm"/> look at the intuition and the meaning behind it. Let's take an image as reference to help us understand the intuition behind this matrix. You can refer to the following screenshot:</p><div class="mediaobject"><img src="Images/B08394_09_10.jpg" alt="Intersection over Union (IoU)" width="684" height="569"/><div class="caption"><p>Figure 9.10: Understanding the intuition behind IoU</p></div></div><p>The preceding<a id="id1111" class="indexterm"/> screenshot is an example of detecting a stop sign in an image. The predicted bounding box is drawn in red and pixels belonging to this red box are considered part of set A, while the ground-truth bounding box is drawn in green and pixels belong to this <a id="id1112" class="indexterm"/>green box are considered part of set B. Our goal is to compute the Intersection of Union between these bounding boxes. So, when our application draws a boundary box, it should match the ground-truth boundary box at least more than 50%, which is considered a good prediction. The equation for IoU is given in the following figure: </p><div class="mediaobject"><img src="Images/B08394_09_11.jpg" alt="Intersection over Union (IoU)" width="654" height="464"/><div class="caption"><p>Figure 9.11: Equation of IoU based on intuitive understanding</p></div></div><p>There are few chances in reality where the (x, y) coordinate of our predicted bounding box will exactly match the (x, y) coordinates of the ground-truth bounding box. In the following figure, you <a id="id1113" class="indexterm"/>can see various examples for poor, good, and excellent IoUs:</p><div class="mediaobject"><img src="Images/B08394_09_12.jpg" alt="Intersection over Union (IoU)" width="664" height="341"/><div class="caption"><p> Figure 9.12: Various IoU boundary box examples</p></div></div><p>IoUs help us <a id="id1114" class="indexterm"/>to determine how well the application identifies the object boundaries and differentiates the various objects from each other. Now, it's time to understand the next testing metrics.</p></div><div class="section" title="mean Average Precision"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec160"/>mean Average Precision</h2></div></div></div><p>In this<a id="id1115" class="indexterm"/> section, we will cover the <span class="strong"><strong>mean Average Precision</strong></span> (<span class="strong"><strong>mAP</strong></span>). In object detection, first, we <a id="id1116" class="indexterm"/>identify the object boundary box and then we classify it into a category. These categories have some labels, and we provide the appropriate label to the identified objects. Now, we need to test how well the application can assign these labels, which means how well we can classify the objects into different predefined categories. For each class, we will calculate the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">True Positive TP(c): A predicted class was C and the object actually belongs to class C </li><li class="listitem" style="list-style-type: disc">False Positive FP(c): A predicted class was C but in reality, the object does not belong to class C </li><li class="listitem" style="list-style-type: disc">Average Precision for class C is given by the following equation:<div class="mediaobject"><img src="Images/B08394_09_31.jpg" alt="mean Average Precision" width="415" height="62"/></div></li></ul></div><p>So, for all the <a id="id1117" class="indexterm"/>classes, we need to calculate the mAP and the equation for that is as follows:</p><div class="mediaobject"><img src="Images/B08394_09_32.jpg" alt="mean Average Precision" width="338" height="62"/></div><p>If we <a id="id1118" class="indexterm"/>want better prediction, then we need to increase the IoU from 0.5 to a higher value (up to 1.0, which would be perfect). We can denote this with this equation: mAP<sub>@p</sub>, where p ∈ (0,1)  is the IoU. mAP<sub>@[0.5:0.95]</sub> means that the mAP is calculated over multiple thresholds and then it is averaged again.</p><p>Now, let's test the baseline model and check the mAP for this implementation.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Testing the baseline model"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec95"/>Testing the baseline model </h1></div></div></div><p>In this section, we<a id="id1119" class="indexterm"/> will run the <a id="id1120" class="indexterm"/>baseline model. In order to run the script, we need to jump to the location where we put the script titled <code class="literal">real_time_object_detection.py</code> and, on Command Prompt, we need to execute the following command:</p><div class="mediaobject"><img src="Images/B08394_09_13.jpg" alt="Testing the baseline model" width="1000" height="81"/><div class="caption"><p>Figure 9.13: Execution of the baseline approach</p></div></div><p>Take a look at the following figure. Here, I have just placed example images, but you can see the entire<a id="id1121" class="indexterm"/> video when you run the script. Here is the link to see the entire video for real-time object detection using the baseline approach: <a class="ulink" href="https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing">https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing</a>.</p><div class="mediaobject"><img src="Images/B08394_09_14.jpg" alt="Testing the baseline model" width="502" height="377"/><div class="caption"><p>Figure 9.14: Output of the baseline approach (image is part of the video stream)</p></div></div><p>Here, the<a id="id1122" class="indexterm"/> mAP for the MobileNet SSD is 71.1% . You will learn how to optimize this approach in the upcoming section. First, we will list down the points that we can improve in the next iteration. So, let's jump to our next section.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Problem with existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec96"/>Problem with existing approach</h1></div></div></div><p>Although the<a id="id1123" class="indexterm"/> MobileNet SSD is fast and gives us good results, it still can't identify classes such as cup, pen, and so on. So, we need to use the pre-trained model that has been trained on a variety of objects. In this upcoming iteration, we need to use the pre-trained model, for example, the TensorFlow object detection API, which will able to identify the different objects compared to the baseline approach. So now, let's look at how we will optimize the existing approach.</p></div></div>



  
<div id="sbo-rt-content"><div class="section" title="How to optimize the existing approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec97"/>How to optimize the existing approach </h1></div></div></div><p>As mentioned <a id="id1124" class="indexterm"/>earlier, in order to optimize the existing approach, I will be using the TensorFlow Object Detection API. You can refer to Google's TensorFlow GitHub repo for this API at the following link: <a class="ulink" href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a>. This API is trained using the COCO dataset as well as the PASCAL VOC dataset; so, it will have the capability of identifying the variety of classes.</p><div class="section" title="Understanding the process for optimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec161"/>Understanding the process for optimization </h2></div></div></div><p>The most <a id="id1125" class="indexterm"/>important part <a id="id1126" class="indexterm"/>for us is how to use the various pre-trained models. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, pull the TensorFlow models repository using this link: <a class="ulink" href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a></li><li class="listitem">Once you pull the repository, you can find the iPython Notebook that I have referred to in order to understand how to use the pre-trained model and to find the link for the iPython notebook at <a class="ulink" href="https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb">https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb</a>.</li><li class="listitem">Here, SSD with MobileNet has been used, but we are using the detection model zoo. This model is trained on the COCO dataset and their versions are given based on the speed and performance of the model. You can download the pre-trained model from this link: <a class="ulink" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a>. I have already placed all these parts together, so it is easy for everyone to implement the code.</li><li class="listitem">The main thing is that this model is trained using the SSD approach, but it has taken datasets such as the kitti dataset and the Open Image dataset. So, this model is able to detect more objects and is more generalized. The link for the Kitti dataset is  <a class="ulink" href="http://www.cvlibs.net/datasets/kitti/">http://www.cvlibs.net/datasets/kitti/</a> and the link for the Open Image dataset is <a class="ulink" href="https://github.com/openimages/dataset">https://github.com/openimages/dataset</a>.</li></ol></div><p>Once we <a id="id1127" class="indexterm"/>download the repository and the pre-trained model, we will load the pre-trained model. In TensorFlow, as we know, the models are saved as a .pb file. Once we load the model, we will be using OpenCV to stream the video. In the upcoming section, we will be implementing the code for the revised approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Implementing the revised approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec98"/>Implementing the revised approach</h1></div></div></div><p>In this section, we <a id="id1128" class="indexterm"/>will understand the implementation of the revised approach. You can refer to this GitHub link: <a class="ulink" href="https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach">https://github.com/jalajthanaki/Real_time_object_detection/tree/master/revised_approach</a>, which has the pre-trained model and the TensorFlow's Object <a id="id1129" class="indexterm"/>detection folder. Before we even begin with the code, I will provide information regarding the folder structure of this approach. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_15.jpg" alt="Implementing the revised approach" width="319" height="154"/><div class="caption"><p>Figure 9.15: Understanding the folder structure for the revised approach </p></div></div><p>Here is the object detection folder downloaded from the TensorFlow model repository: <a class="ulink" href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a>. In the <code class="literal">utils</code> folder, there are some helper functions to help us stream the video. The main script that helps us run the script is <code class="literal">object_detection_app.py</code>. The pre-trained model has been saved inside the object detection folder. The path for pre-trained model in this folder is this: <code class="literal">~/PycharmProjects/Real_time_object_detection/revised_approach/object_detection/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb</code>.</p><p>Now, let's look<a id="id1130" class="indexterm"/> at the coding implementation step by step. In the first step, we will import the dependency libraries and load the pre-trained model. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_16.jpg" alt="Implementing the revised approach" width="914" height="614"/><div class="caption"><p>Figure 9.16: Code snippet for loading the pre-trained model in the revised approach</p></div></div><p>In this model, there are 90 different types of objects that can be identified. Once we load the model, the next step is the <code class="literal">detect_objects()</code> function, which is used to identify the objects. Once the object is identified, the bounding boxes for that object are drawn and we simultaneously run the pre-trained model on those objects, so that we can get the identification labels for the object, whether it's a cup, bottle, person, and so on. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_17.jpg" alt="Implementing the revised approach" width="822" height="550"/><div class="caption"><p>Figure 9.17: Code snippet for the detect_objects() function</p></div></div><p>After this, we have the <code class="literal">worker()</code> function, which helps us stream the video as well as perform some GPU memory <a id="id1131" class="indexterm"/>management. You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_18.jpg" alt="Implementing the revised approach" width="670" height="482"/><div class="caption"><p>Figure 9.18: Code snippet for the worker() function</p></div></div><p>As you can see, we have <a id="id1132" class="indexterm"/>defined the GPU memory fraction, as well as the kind of colors used when it detects the objects. Now, let's look at the main function of the script. In the main function, we define some optional arguments and their default values.  The list of these arguments is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Device index of the camera: <code class="literal">--source=0</code></li><li class="listitem" style="list-style-type: disc">Width of the frames in the video stream <code class="literal">--width= 500</code></li><li class="listitem" style="list-style-type: disc">Height of the frames in the video stream <code class="literal">--height= 500</code></li><li class="listitem" style="list-style-type: disc">Number of workers <code class="literal">--num-workers=2</code></li><li class="listitem" style="list-style-type: disc">Size of the queue <code class="literal">--queue-size=5</code></li></ul></div><p>You can refer to the implementation of the main function shown in the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_19.jpg" alt="Implementing the revised approach" width="816" height="899"/><div class="caption"><p>Figure 9.19: Code snippet for the main function</p></div></div><p>When we run the script, we can<a id="id1133" class="indexterm"/> see the output given in the upcoming figure. Here, we have placed the image, but you can see the video by using this link:</p><p>
<a class="ulink" href="https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing">https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing</a>
</p><div class="mediaobject"><img src="Images/B08394_09_20.jpg" alt="Implementing the revised approach" width="490" height="428"/><div class="caption"><p>Figure 9.20: Output of the revised approach</p></div></div><p>
</p><p>Once we end the video, the resources and the process <a id="id1134" class="indexterm"/>should end as well. For that, we will be using the code that has been provided to us through the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_21.jpg" alt="Implementing the revised approach" width="235" height="61"/><div class="caption"><p> Figure 9.21: Code snippet to release the resources once the script is terminated</p></div></div><div class="section" title="Testing the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec162"/>Testing the revised approach </h2></div></div></div><p>Once we <a id="id1135" class="indexterm"/>have executed this approach, we can identify objects such as cups, pens, and so on. So, we can say that our baseline approach is definitely improvised and there are some modified labels, such as a sofa (in this approach, identified as a couch). Apart from this, if we talk about the mAP of this pre-trained model, then as per the documentation, on the COCO dataset, this model gets around 52.4% accuracy. In our input, we will get around 73% accuracy. This approach identifies more objects with different categories, which is a great advantage.</p><p>In the upcoming section, we will discuss the points that we can use to come up with the best possible solution.</p></div><div class="section" title="Understanding problems with the revised approach"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec163"/>Understanding problems with the revised approach</h2></div></div></div><p>We have<a id="id1136" class="indexterm"/> tried approaches that are fast and accurate, but we need an approach that is fast as well as accurate and optimized. These are the points we need to keep in our mind when we develop the best possible solution:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The SSD-based approach is great, but is not that accurate when you train your own model using COCO or the PASCAL VOC dataset.</li><li class="listitem" style="list-style-type: disc">The TensorFlow detection model zoo will have a mAP score on the COCO test dataset from 20 to 40%. So, we need to explore other techniques that can help us give a better result in terms of processing and object detection accuracy.</li></ul></div><p>So, in the upcoming section, we will look at the approach that can help us optimize the revised approach.</p></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="The best approach"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec99"/>The best approach</h1></div></div></div><p>In this section, we <a id="id1137" class="indexterm"/>will be trying the approach named <span class="strong"><strong>YOLO</strong></span>. YOLO stands for You Only Look Once. This <a id="id1138" class="indexterm"/>technique gives us good accuracy, is fast, and its memory management is easy. This section will be divided into two parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding  YOLO</li><li class="listitem" style="list-style-type: disc">Implementing the best approach using YOLO</li></ul></div><p>In the first section, we will understand the basics about YOLO. During the implementation, we will be use YOLO with the pre-trained YOLO model. So, let's begin!</p><div class="section" title="Understanding YOLO"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec164"/>Understanding YOLO </h2></div></div></div><p>YOLO is a<a id="id1139" class="indexterm"/> state-of-the-art, real-time<a id="id1140" class="indexterm"/> object detection system. On GPU Titan X, it processes images at 40-90 FPS and has a mAP on the PASCAL VOC dataset of 78.6% and a mAP of 48.1% on the coco test-dev dataset. So, now, we will look at how YOLO works and processes the images in order to identify the objects. We are using YOLOv2 (YOLO version 2) as it is a faster version.</p></div><div class="section" title="The working of YOLO"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec165"/>The working of YOLO</h2></div></div></div><p>YOLO reframes the<a id="id1141" class="indexterm"/> object detection problem. It considers the object recognition task as single regression problem, right from the image pixels to the bounding box coordinates and class probabilities. A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance. This approach has several advantages compared to traditional methods, as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">YOLO is extremely fast. This<a id="id1142" class="indexterm"/> is because, in YOLO, frame detection is a regression problem, and we do not need to use a complex pipeline.</li><li class="listitem" style="list-style-type: disc">We can simply run our neural network on a new image at the time of testing to predict detection. Our base network runs at 45 frames per second with no batch processing on a Titan X GPU and a fast version runs at more than 150 fps. This means that we can process streaming video in real time with less than 25 milliseconds of latency.</li></ul></div><p>YOLO processes images globally when it makes a prediction regarding information about classes as well as their appearance. It also learns the generalization of objects.</p><p>YOLO divides the input image into an S×S grid. If the center of an object falls into a grid cell, then that grid cell is responsible for detecting the object. Each grid cell predicts the B bounding boxes and confidence scores for those boxes. These confidence scores reflect how confident the model is that the box contains an object and how accurate it thinks the box that it predicts is. So, formally, we can define the confidence mechanism for YOLO by using this notation. We define the confidence as Pr(object) * IoU.  If no object exists in that cell, then the confidence scores should be zero; otherwise, we want the confidence score to equal the IoU between predicted box and the ground truth. Each bounding box consists of five predictions: x, y, w, h, and confidence. The (x, y) coordinates represent the center of the box relative to the bounds of the grid cell. The w and h represents the width and height. They are predicted in relativity to the whole image. Finally, the confidence <a id="id1143" class="indexterm"/>score represents the IoU. For each grid cell, it predicts C conditional class probabilities, <span class="emphasis"><em>Pr(Classi|Object)</em></span>. These probabilities are conditioned on the grid cell that contains an object. We predict only one set of class probabilities per grid cell regardless of the number of bounding boxes B. At the time of testing, we multiply the conditional class probabilities and the individual box confidence prediction that is defined by this equation:</p><div class="mediaobject"><img src="Images/B08394_09_33.jpg" alt="The working of YOLO" width="488" height="34"/></div><p>The preceding equation gives us the class-specific confidence score for each of the boxes. This score contains both the probability of the class appearing in the box and how well the predicted box fits the object.  The pictorial representation of the whole process of YOLO is shown in the following figure: </p><div class="mediaobject"><img src="Images/B08394_09_22.jpg" alt="The working of YOLO" width="711" height="632"/><div class="caption"><p>Figure 9.22: Pictorial representation of YOLO object detection</p></div></div><p>Now, let's get to basic knowledge about the YOLO architecture.</p></div><div class="section" title="The architecture of YOLO"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec166"/>The architecture of YOLO</h2></div></div></div><p>In this section, you<a id="id1144" class="indexterm"/> will the learn basics about the YOLO architecture. In YOLO, there are 24 convolutional layers followed by two fully connected layers. Instead of the inception modules used by GoogleNet, YOLO simply uses 1×1 reduction layers followed by 3×3 convolutional layers. Fast YOLO uses a neural network with fewer convolutional layers. We use nine layers instead of 24 layers. We also use fewer filters in these layers. Apart from this, all parameters are the same for YOLO and Fast YOLO during training and testing. You can refer to the architecture in the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_23.jpg" alt="The architecture of YOLO" width="1000" height="698"/><div class="caption"><p>Figure 9.23: Architecture of YOLO</p></div></div><p>Now, let's move on to the implementation part.</p></div><div class="section" title="Implementing the best approach using YOLO"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec167"/>Implementing the best approach using YOLO</h2></div></div></div><p>In order to<a id="id1145" class="indexterm"/> implement YOLO, we need to install the Cython module. Apart from that, you can use either Darknet or the Darkflow, which is the TensorFlow wrapper on Darknet. Darknet is written in C and CUDA, so it is quite fast. Here, we will be implementing both the options. Before implementation, we need to set up the environment. The implementation part should be divided into two sections here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementation using Darknet</li><li class="listitem" style="list-style-type: disc">Implementation using Darkflow</li></ul></div><p>You can refer to this GitHub repository for all the code: <a class="ulink" href="https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO">https://github.com/jalajthanaki/Real_time_object_detection_with_YOLO</a>.</p><div class="section" title="Implementation using Darknet"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec120"/>Implementation using Darknet</h3></div></div></div><p>We are<a id="id1146" class="indexterm"/> following <a id="id1147" class="indexterm"/>these steps in order to implement <a id="id1148" class="indexterm"/>YOLO using Darknet:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Environment setup for Darknet</li><li class="listitem" style="list-style-type: disc">Compile the Darknet</li><li class="listitem" style="list-style-type: disc">Download the pre-trained weight</li><li class="listitem" style="list-style-type: disc">Run object detection for the image</li><li class="listitem" style="list-style-type: disc">Run object detection for the video stream</li></ul></div><div class="section" title="Environment setup for Darknet"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec53"/>Environment setup for Darknet</h4></div></div></div><p>In this step, we<a id="id1149" class="indexterm"/> need to download the GitHub repository of Darknet. We can do that using the following command. You can download this repository at any path: </p><div class="informalexample"><pre class="programlisting">$ git clone https://github.com/pjreddie/darknet</pre></div><p>Once you run this command, the directory named Darknet is created. After that, you can jump to the next step.</p></div><div class="section" title="Compiling the Darknet"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec54"/>Compiling the Darknet</h4></div></div></div><p>Once we <a id="id1150" class="indexterm"/>have downloaded the Darknet, we need to jump to the directory named Darknet. After that, we need to compile the Darknet. So, we need to execute the following commands sequentially:</p><div class="informalexample"><pre class="programlisting">
$ cd darknet
$ make
</pre></div></div><div class="section" title="Downloading the pre-trained weight"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec55"/>Downloading the pre-trained weight</h4></div></div></div><p>Configuration files <a id="id1151" class="indexterm"/>are already inside the <code class="literal">cfg/</code> subdirectory inside the darknet directory. So, by executing the following command, you can download the pre-trained weight for the YOLO model:</p><div class="informalexample"><pre class="programlisting">$ wegt https://pjreddie.com/media/files/yolo.weights</pre></div><p>This download may take some time. Once we have the pre-trained weight with us, we can run the Darknet.</p></div><div class="section" title="Running object detection for the image"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec56"/>Running object detection for the image</h4></div></div></div><p>If you want to identify the objects in the image, then you need to execute the following command:</p><div class="informalexample"><pre class="programlisting">./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg</pre></div><p>You can refer to the output of this<a id="id1152" class="indexterm"/> command in the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_24.jpg" alt="Running object detection for the image" width="1000" height="562"/><div class="caption"><p>Figure 9.24 : Output of object detection for the image using Darknet</p></div></div><p>Now, let's implement YOLO on the video stream.</p></div><div class="section" title="Running the object detection on the video stream"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec57"/>Running the object detection on the video stream</h4></div></div></div><p>We can run YOLO on the video stream using this command: </p><div class="informalexample"><pre class="programlisting">./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights &lt;video file&gt;</pre></div><p>Here, we need to pass the path of the video. For more information, you can refer to this Darknet documentation: <a class="ulink" href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a>. Now, let's understand the implementation of Darkflow.</p></div></div><div class="section" title="Implementation using Darkflow"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec121"/>Implementation using Darkflow</h3></div></div></div><p>In this<a id="id1153" class="indexterm"/> implementation, you need to refer to the code given in the folder named Darkflow. We need to perform the <a id="id1154" class="indexterm"/>following<a id="id1155" class="indexterm"/> steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Installing Cython</li><li class="listitem">Building the already provided setup file</li><li class="listitem">Testing the environment</li><li class="listitem">Loading the model and run object detection on the images </li><li class="listitem">Loading the model and run object detection on the video stream</li></ol></div><div class="section" title="Installing Cython"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec58"/>Installing Cython</h4></div></div></div><p>In order to<a id="id1156" class="indexterm"/> install Cython, we need to <a id="id1157" class="indexterm"/>execute the following command. This Cython package is needed because the Darkflow is a Python wrapper that uses C code from Darknet: </p><div class="informalexample"><pre class="programlisting">$ sudo apt-get install Cython</pre></div><p>Once Cython is installed, we can build the other setup.</p></div><div class="section" title="Building the already provided setup file"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec59"/>Building the already provided setup file</h4></div></div></div><p>In this stage, we <a id="id1158" class="indexterm"/>will be executing the command that will set up the necessary Cython environment for us. The command is as follows:</p><div class="informalexample"><pre class="programlisting">$ python setup.py build_ext --inplace</pre></div><p>When we execute this command, we will have to use <code class="literal">./flow</code> in the cloned Darkflow directory instead of flow, as Darkflow is not installed globally. Once this command runs successfully, we need to test whether we installed all the dependencies perfectly. You can download the pre-trained weight by using the following command:</p><div class="informalexample"><pre class="programlisting">$ wegt https://pjreddie.com/media/files/yolo.weights</pre></div></div><div class="section" title="Testing the environment"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec60"/>Testing the environment</h4></div></div></div><p>In this stage, we will test whether Darkflow <a id="id1159" class="indexterm"/>runs perfectly or not. In order to check that, we need to execute the following command:</p><div class="informalexample"><pre class="programlisting">$ ./flow --h</pre></div><p>You can refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_25.jpg" alt="Testing the environment" width="1000" height="697"/><div class="caption"><p>Figure 9.25 : Successful testing outcome of Darkflow</p></div></div><p>Once you can see the preceding <a id="id1160" class="indexterm"/>output, you'll know that you have successfully configured Darkflow. Now, let's run it.</p></div><div class="section" title="Loading the model and running object detection on images"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec61"/>Loading the model and running object detection on images </h4></div></div></div><p>We can run <a id="id1161" class="indexterm"/>the Darkflow on images. For that, we need to load YOLO pre-trained weights, configuration files, and path of images so you can execute the following command:</p><div class="informalexample"><pre class="programlisting">./flow --imgdir sample_img/ --model cfg/yolo.cfg --load ../darknet/yolo.weights</pre></div><p>If you want to save the object detection in the json format, then that is possible as well. You need to execute the following command:</p><div class="informalexample"><pre class="programlisting">./flow --imgdir sample_img/ --model cfg/yolo.cfg --load ../darknet/yolo.weights --json</pre></div><p>You can see the output inside the <code class="literal">sample_img/out</code> folder; refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_26.jpg" alt="Loading the model and running object detection on images" width="1000" height="580"/><div class="caption"><p>Figure 9.26 : Output image with predicted objects using Darkflow</p></div></div><p>You can<a id="id1162" class="indexterm"/> also refer to the following figure:</p><div class="mediaobject"><img src="Images/B08394_09_27.jpg" alt="Loading the model and running object detection on images" width="1000" height="95"/><div class="caption"><p>Figure 9.27 : json output of the object detected in the image</p></div></div></div><div class="section" title="Loading the model and running object detection on the video stream"><div class="titlepage"><div><div><h4 class="title"><a id="ch09lvl4sec62"/>Loading the model and running object detection on the video stream</h4></div></div></div><p>In this section, we <a id="id1163" class="indexterm"/>will run object detection on the video stream. First, we will see how to use the webcam and perform object detection. The command for that is as follows:</p><div class="informalexample"><pre class="programlisting">./flow --model cfg/yolo.cfg --load ../darknet/yolo.weights --demo camera --saveVideo --gpu 0.60</pre></div><p>You can refer to the following figure. You can see the video at this link: <a class="ulink" href="https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing">https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing</a>
</p><div class="mediaobject"><img src="Images/B08394_09_28.jpg" alt="Loading the model and running object detection on the video stream" width="1000" height="708"/><div class="caption"><p>Figure 9.28 : Output of the object detection using Darkflow for the webcam video stream</p></div></div><p>
</p><p>We can also run<a id="id1164" class="indexterm"/> the Darkflow for a prerecorded video. For that, you need to run the following command: </p><div class="informalexample"><pre class="programlisting">./flow --model cfg/yolo.cfg --load ../darknet/yolo.weights --demo ~/Downloads/Traffic.avi --saveVideo --gpu 0.60</pre></div><p>You can refer to the following figure. You can see the video at this link: <a class="ulink" href="https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing">https://drive.google.com/drive/folders/1RwKEUaxTExefdrSJSy44NugqGZaTN_BX?usp=sharing</a>.</p><div class="mediaobject"><img src="Images/B08394_09_29.jpg" alt="Loading the model and running object detection on the video stream" width="1000" height="505"/><div class="caption"><p>Figure 9.29 : Output of the object detection using Darkflow for the prerecorded video </p></div></div><p>In both <a id="id1165" class="indexterm"/>commands we have used the—save Video flag to save the video and the—gpu 0.60 flag, which use 60% memory of GPU.  Using this approach, we will get an accuracy of 78%.</p></div></div></div></div></div>



  
<div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec100"/>Summary</h1></div></div></div><p> In this chapter, you learned about Transfer Learning. We explored different libraries and approaches in order to build a real-time object detection application. You learned how to set up OpenCV and looked at how it is rather useful in building the baseline application. In this baseline approach, we used the model that is trained using the caffe deep learning library. After that, we used TensorFlow to build real-time object detection, but in the end, we used a pre-trained YOLO model, which outperformed every other approach. This YOLO-based approach gave us more generalized approach for object detection applications. If you are interested in building innovative solutions for computer vision, then you can enroll yourself in the VOC challenges. This boosts your skills and gives you a chance to learn. You can refer to this link for more information: <a class="ulink" href="http://host.robots.ox.ac.uk/pascal/VOC/">http://host.robots.ox.ac.uk/pascal/VOC/</a> (PASCAL VOC Challenges 2005-2012).  You can also build your own algorithm and check the result and compare your result with the existing approach and, if it outperforms the existing approaches, you can definitely publish the paper in reputed journals. By using the YOLO approach, we get the Mean Average Precision of 78% on the PASCAL VOC dataset and it works pretty well when you apply this model to any video or image. The code credit for this chapter goes to Adrian Rosebrock, Dat Tran, and Trieu. We defined the mAP score based on the mAP it gets for either the COCO dataset or the PASCAL VOC dataset.</p><p>In the next chapter, we will explore another application that belongs to the computer vision domain: face detection and facial expression detection. In order to build this application, we will be using Deep Learning techniques. So, keep reading!</p></div></div>



  </body></html>