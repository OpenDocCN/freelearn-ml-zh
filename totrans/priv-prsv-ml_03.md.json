["```py\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n# Prepare sample input data with two features (feature 1, feature 2\nX = np.array([[10, 10], [10, 20], [20, 20], [20, 30]])\n# Assume that target feature has some relationship with the input features with the formula y = 3 * x_1 + 5 * x_2 + 50\ny = np.dot(X, np.array([3, 5])) + 50\n# Run the model with the input data and output values\nreg = LinearRegression().fit(X, y)\nreg.score(X, y)\nreg.coef_\nreg.intercept_\nreg.predict(np.array([[30, 50]]))\nOutput: array([390.])\n```", "```py\n# Persist the model in python Joblib file\n# Retrieve the model weights and use it further prediction\nimport joblib\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nX = np.array([[10, 10], [10, 20], [20, 20], [20, 30]])\n# Assume that target feature has some relationship with the input features with the formula y = 3 * x_1 + 5 * x2 + 50\ny = np.dot(X, np.array([3, 5])) + 50# Run the model with the input data and output values\nreg = LinearRegression().fit(X, y)\n# Persist the model in python Joblib file\nfilename = \"sample_model.sav\"\njoblib.dump(reg, filename)\n# regression model which is used earlier \n# Share this file alone to others for deploying in to production and for inferencing/predictions\n```", "```py\nimport joblib\nimport numpy as np\nfilename = \"sample_model.sav\"\n# Load the model from disk\nloaded_model = joblib.load(filename)\nresult = loaded_model.predict(np.array([[30, 50]]))\nprint(result)\nOutput: [390.]\n```", "```py\nfrom sklearn import tree\nX = [[10,1],[20,1],[30,1],[80,1],[75,0],[78,0]]\nY = [1,1,1,0,0,0]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\nclf.predict([[35,1]])\nOutput: array([1])\nclf.predict([[78,1]])\nOutput: array([0])\n```", "```py\ntree.plot_tree(clf)\n```", "```py\n[Text(167.4, 163.07999999999998, ‘X[0] <= 52.5\\ngini = 0.5\\nsamples = 6\\nvalue = [3, 3]’),\n Text(83.7, 54.360000000000014, ‘gini = 0.0\\nsamples = 3\\nvalue = [0, 3]’),\n Text(251.10000000000002, 54.360000000000014, ‘gini = 0.0\\nsamples = 3\\nvalue = [3, 0]’)]\n```", "```py\npip3 install onnx --user\npip3 install onnxruntime --user\npip3 install skl2onnx --user\n```", "```py\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\ninitial_type = [('float_input', FloatTensorType([None, 2]))]\nonx = convert_sklearn(clf, initial_types=initial_type)\nwith open(\"survive.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())\n```", "```py\ninitial_type = [(‘float_input’, FloatTensorType([None, 2]))]\n```", "```py\nonx = convert_sklearn(clf, initial_types=initial_type)\n```", "```py\nwith open(“survive.onnx”, “wb”) as f:\n    f.write(onx.SerializeToString())\n```", "```py\nimport onnxruntime as rt\nimport numpy as np\nsess = rt.InferenceSession(\"survive.onnx\")\ninput_name = sess.get_inputs()[0].name\n# To test whether the patient will survive or not with 78 years age and doesn’t have prior cancer\nX_test = np.array([[78.0, 0.0]])\npred_onx = sess.run(None, {input_name: X_test.astype(np.float32)})[0]\nprint(pred_onx)\nOutput: [0]\n```", "```py\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport numpy as np\nX = np.array([\n              [12.93,77.4472],\n              [12.32,77.4472],\n              [12.51,77.4472],\n              [12.62,77.4472],\n              [12.73,77.4472],\n              [12.84,76.4158],\n              [12.91,76.4158],\n              [12.41,76.4158],\n              [12.92,76.4158],\n              [12.55,76.4158],\n             ])\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\nkmeans.labels_\narray([1, 1, 1, 1, 1, 0, 0, 0, 0, 0], dtype=int32)\n      kmeans.cluster_centers_\narray([[12.726 , 76.4158],\n       [12.622 , 77.4472]])\nkmeans.predict([[12.88, 76.88]])\n      array([0], dtype=int32\n```", "```py\nimport numpy as np\n# This example will use a 4-armed bandit.\n# Initialization\nnum_trials = 1000\nbandit_probabilities = [0.1, 0.15, 0.3, 0.35]  # The probability of each bandit to give reward\nreward_counts = np.zeros(4)\nselected_bandit = 0\nfor i in range(num_trials):\n    # Select a bandit\n    selected_bandit = np.random.randint(0, 4)\n    # Pull bandit’s arm\n    random_num = np.random.random()\n    if random_num <= bandit_probabilities[selected_bandit]:\n         reward = 1\n    else:\n         reward = 0\n    reward_counts[selected_bandit] += reward\nprint(“Most successful bandit:”, np.argmax(reward_counts))\nMost successful bandit: 3\n```", "```py\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n# Load the dataset (e.g., Iris dataset)\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n# Train a decision tree classifier on the training set\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nfor i in range (0,75):\n    # Select a data point from the test set\n    target_data = X_test[i]\n    #Determine if the target data point was in the training set\n    predicted_class = clf.predict([target_data])\n    is_in_training_set = predicted_class == y_test[i]\n    # Print the result\n    if is_in_training_set:\n        print(target_data, \"Membership Inference Attack successful! Target data point was in the training set.\")\n    else:\n        print( target_data, \"Membership Inference Attack unsuccessful. Target data point was not in the training set.\")\nOutput:\n[6.9 3.1 5.1 2.3] Membership Inference Attack successful! Target data point was in the training set.\n[6.2 2.2 4.5 1.5] Membership Inference Attack unsuccessful. Target data point was not in the training set.\n```", "```py\nfrom sklearn import tree\nX = [[10,1],[20,1],[30,1],[80,1],[75,0],[78,0]]\nY = [1,1,1,0,0,0]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\nclf.predict([[35,1]])\n```", "```py\ntestY=[\n       [25,1],[25,0],[30,1],[30,0],[45,0],[45,1],\n       [50,1],[50,0],[60,1],[60,0],[75,0],[75,1],\n       [80,1],[80,0],[90,1],[90,0],[100,0],[100,1],\n       [10,1],[20,1],[30,1],[78,0]\n      ]\nclf.predict(testY)\narray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0])\nclf.predict_proba(testY)\narray([[0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.]])\n```", "```py\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n# Create a sample dataset for demonstration\nX = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\ny = np.array([0, 0, 0, 1, 1])\n# Train a simple logistic regression model on the dataset\nmodel = LogisticRegression()\nmodel.fit(X, y)\n# Attacker's code to perform model extraction attack\nextracted_model = LogisticRegression()\nextracted_model.fit(X, y)\n# Extracted model can be further used for unauthorized purposes\n# such as making predictions on new data without access to the original model\nnew_data = np.array([[6, 6], [7, 7]])\npredictions = extracted_model.predict(new_data)\nprint(predictions)\n[1 1]\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n# Create a sample dataset\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])  # Input features\ny = np.array([0, 0, 1, 1])  # Corresponding labels\n# Train a logistic regression model on the dataset\nmodel = LogisticRegression()\nmodel.fit(X, y)\n# Function to perform model inversion attack\ndef model_inversion_attack(model, output):\n    # Generate a random input within a certain range\n    input_range = np.arange(0, 10, 0.01)\n    best_input = None\n    best_loss = float('inf')\n    # Find the input that minimizes the loss function\n    for i in input_range:\n        input_guess = np.array([[i, i]])\n        predicted_output = model.predict_proba(input_guess)\n        loss = abs(predicted_output[0][1] - output)\nif loss < best_loss:\n            best_input = input_guess\n            best_loss = loss\n\n    return best_input\n# Perform model inversion attack on a specific output\ntarget_output = 0.8\ninverted_input = model_inversion_attack(model, target_output)\nprint(\"Inverted Input:\", inverted_input)\ntarget_output = 1\ninverted_input = model_inversion_attack(model, target_output)\nprint(\"Inverted Input:\", inverted_input)\ntarget_output = 0.5\ninverted_input = model_inversion_attack(model, target_output)\nprint(\"Inverted Input:\", inverted_input)\ntarget_output = 0\ninverted_input = model_inversion_attack(model, target_output)\nprint(\"Inverted Input:\", inverted_input)\nInverted Input: [[5.64 5.64]]\nInverted Input: [[9.99 9.99]]\nInverted Input: [[4.5 4.5]]\nInverted Input: [[0\\. 0.]]\n```", "```py\nmnist_data.pkl. We open the file in binary read mode and load the dataset using the pickle.load() function. The dataset is then split into the training and test sets, with the corresponding labels.\n```", "```py\nfrom pathlib import Path\nimport requests\nimport pickle\nimport gzip\nDATA_PATH = Path(“data”)\nPATH = DATA_PATH / “mnist”\nPATH.mkdir(parents=True, exist_ok=True)\n# 10.195.33.40 - Github\nURL = \"https://10.195.33.40/pytorch/tutorials/raw/main/_static/\"\nFILENAME = “mnist.pkl.gz”\nwith gzip.open((PATH / FILENAME).as_posix(), “rb”) as f:\n((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=”latin-1”)\n```", "```py\nfrom matplotlib import pyplot\nimport numpy as np\npyplot.imshow(x_train[0].reshape((28, 28)), cmap=”gray”)\nprint(x_train.shape, y_train[0])\n```", "```py\nimport torch\nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nn, c = x_train.shape\nprint(x_train, y_train)\nprint(x_train.shape)\nprint(y_train.min(), y_train.max())\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\ntensor([5, 0, 4,  ..., 8, 4, 8])\nprint(x_train.shape)\ntorch.Size([50000, 784])\nprint(y_train.min(), y_train.max())\ntensor(0) tensor(9)\n```", "```py\nfrom torch import nn, optim\nclass AuthorsNN(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.first_sec = nn.Sequential(\n                           nn.Linear(784, 450),\n                           nn.ReLU(),\n                         )\n    self.second_sec = nn.Sequential(\n                           nn.Linear(450, 450),\n                           nn.ReLU(),\n                           nn.Linear(450, 10),\n                           nn.Softmax(dim=-1),\n                         )\n  def forward(self, x):\n    return self.second_sec(self.first_sec(x))\n```", "```py\nauth_nn = AuthorsNN()\nauth_nn\n```", "```py\nAuthorsNN(\n  (first_sec): Sequential(\n    (0): Linear(in_features=784, out_features=450, bias=True)\n    (1): ReLU()\n  )\n  (second_sec): Sequential(\n    (0): Linear(in_features=450, out_features=450, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=450, out_features=10, bias=True)\n    (3): Softmax(dim=-1)\n  )\n)\n```", "```py\nloss_func = nn.CrossEntropyLoss()\nloss_func\nCrossEntropyLoss()\n```", "```py\nfrom torch import optim\noptimizer = optim.Adam(auth_nn.parameters(), lr = 0.01)\n```", "```py\noptimizer\n```", "```py\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: False\n    lr: 0.01\n    maximize: False\n    weight_decay: 0\n)\n```", "```py\ndef train(num_epochs, ann):\n    ann.train()\n    for epoch in range(num_epochs):\n            output = ann(x_train)\n            loss = loss_func(output, y_train)\n            # clear gradients for this training step\n            optimizer.zero_grad()\n            # backpropagation, compute gradients\n            loss.backward()\n            # apply gradients\n            optimizer.step()\n            print(epoch, loss.item())\n    pass\n```", "```py\ntrain(100,auth_nn)\n0 1.6085695028305054\n1 1.6047792434692383\n2 1.59657621383667\n….\n100 1.4667187929153442\n```", "```py\nclass Adversary(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.layers= nn.Sequential(\n                      nn.Linear(450, 800),\n                      nn.ReLU(),\n                      nn.Linear(800, 784),\n                    )\n  def forward(self, x):\n    return self.layers(x)\n```", "```py\nadversary = Adversary()\noptimizer = optim.Adam(adversary.parameters(), lr=1e-4)\nfor i in range (0,1000):\n    optimiser.zero_grad()\n    #print(x_train[i])\n    target_outputs  = auth_nn.first_sec(x_valid[i])\n    adversary_outputs = adversary(target_outputs)\n    #print(adversary_outputs)\n    loss = ((x_valid[i] - adversary_outputs)**2).mean()\n    #print(loss.item())\n    loss.backward()\n    optimiser.step()\nNow, let’s test the adversary model:\nfor i in range (1000,2000):\n    target_outputs = auth_nn.first_sec(x_train[i])\n    recreated_data = adversary(target_outputs)\n    #print(recreated_data)\n```", "```py\nWith torch.no_grad():\n pyplot.imshow(recreated_data.reshape((28, 28)), cmap=”gray”)\n```"]