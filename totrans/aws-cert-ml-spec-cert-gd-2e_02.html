<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer022">
			<h1 class="chapter-number"><a id="_idTextAnchor161"/>2</h1>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor162"/>AWS Services for Data Storage</h1>
			<p>AWS provides a wide range of services to store your data safely and securely. There are various storage options available on AWS, such as block storage, file storage, and object storage. It is expensive to manage on-premises data storage due to the higher investment in hardware, admin overheads, and managing system upgrades. With AWS storage services, you just pay for what you use, and you don’t have to manage the hardware. You will also learn about various storage classes offered by Amazon S3 for intelligent access to data and to reduce costs. You can expect questions in the exam on storage classes. As you continue through this chapter, you will master the <strong class="bold">single-AZ </strong>and <strong class="bold">multi-AZ</strong> instances, and <strong class="bold">Recovery Time Objective</strong> (<strong class="bold">RTO</strong>) and <strong class="bold">Recovery Point Objective</strong> (<strong class="bold">RPO</strong>) concepts of <span class="No-Break">Amazon RDS.</span></p>
			<p>In this chapter, you will learn about storing your data securely for further analytical purposes throughout the <span class="No-Break">following sections:</span></p>
			<ul>
				<li>Storing data on <span class="No-Break">Amazon S3</span></li>
				<li>Controlling access on S3 buckets <span class="No-Break">and objects</span></li>
				<li>Protecting data on <span class="No-Break">Amazon S3</span></li>
				<li>Securing S3 objects at rest and <span class="No-Break">in transit</span></li>
				<li>Using other types of <span class="No-Break">data stores</span></li>
				<li><strong class="bold">Relational Database </strong><span class="No-Break"><strong class="bold">Services</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RDSes</strong></span><span class="No-Break">)</span></li>
				<li>Managing failover in <span class="No-Break">Amazon RDS</span></li>
				<li>Taking automatic backup, RDS snapshots, and restore and <span class="No-Break">read replicas</span></li>
				<li>Writing to Amazon Aurora with <span class="No-Break">multi-master capabilities</span></li>
				<li>Storing columnar data on <span class="No-Break">Amazon Redshift</span></li>
				<li>Amazon DynamoDB for NoSQL databases as <span class="No-Break">a service</span></li>
			</ul>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor163"/><a id="_idTextAnchor164"/>Technical requirements</h1>
			<p>All you will need for this chapter is an AWS account and the AWS CLI configured. The steps to configure the AWS CLI for your account are explained in detail by Amazon <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html"><span class="No-Break">https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html</span></a><span class="No-Break">.</span></p>
			<p>You can download the code examples from GitHub, <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor165"/><a id="_idTextAnchor166"/>Storing Data on Amazon S3</h1>
			<p>S3 is Amazon’s <a id="_idTextAnchor167"/>cloud-based object storage service, and it can be <a id="_idTextAnchor168"/>accessed from anywhere via the internet. It is an ideal storage option for large datasets. It is region-based, as your data is stored in a particular region until you move the data to a different region. Your data will never leave that region until it is configured to do so. In a particular region, data is replicated in the availabi<a id="_idTextAnchor169"/>lity zones of that region; this makes S3 regionally resilient. If any of the availability zones fail in a region, then other availability zones will serve your requests. S3 can be accessed via the AWS console UI, AWS CLI, AWS API requests, or via standard <span class="No-Break">HTTP methods.</span></p>
			<p>S3 has two main components: <strong class="bold">buckets</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">objects</strong></span><span class="No-Break">.</span></p>
			<ul>
				<li>Buckets <a id="_idTextAnchor170"/>are created in a specific AWS region. Buckets <a id="_idTextAnchor171"/>can contain objects but cannot contain <span class="No-Break">other buckets.</span></li>
				<li>Objects have two <a id="_idTextAnchor172"/>main attributes. One is the <strong class="bold">key</strong>, and the <a id="_idTextAnchor173"/>other is the <strong class="bold">value</strong>. The value is <a id="_idTextAnchor174"/>the content being stored, and the key is the name. The maximum size of an object can be 5 TB. As per the Amazon S3 documentation (<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html</a>), objects also have a version ID, metadata, access control information, <span class="No-Break">and sub-resources.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">As per Amazon’s docs, S3 provides read-after-write consistency for PUTs of new objects, which means that if you upload a new object or create a new object and you immediately try to read the object using its key, then you get the exact data that you just uploaded. However, for overwrites and deletes, it behaves in an <strong class="bold">eventually consistent manner</strong>. This means that if you read an object straight after the delete or overwrite operation, then you may read an old copy or a stale version of the object. It takes some time to replicate the content of the object across three <span class="No-Break">Availability Zones.</span></p>
			<p>A folder structure can be maintained logically by using a prefix. Take an example where an image is <a id="_idTextAnchor175"/>uploaded into a bucket, <strong class="source-inline">bucket-name-example</strong>, with the prefix <strong class="source-inline">folder-name</strong> and the object name <strong class="source-inline">my-image.jpg</strong>. The entire structure looks like <span class="No-Break">this: </span><span class="No-Break"><strong class="source-inline">/bucket-name-example/folder-name/my-image.jpg</strong></span><span class="No-Break">.</span></p>
			<p>The content of the object can be read by using the bucket name of <strong class="source-inline">bucket-name-example</strong> and the key <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">/folder-name/my-image.jpg</strong></span><span class="No-Break">.</span></p>
			<p>There are several storage classes offered by Amazon for objects stored <span class="No-Break">in S3:</span></p>
			<ul>
				<li><strong class="bold">Standard Storage (S3 Standard):</strong> This is the <a id="_idTextAnchor176"/>storage class for frequently <a id="_idTextAnchor177"/>accessed objects and for quick access. S3 Standard has a millisecond first-byte latency and objects can be made <span class="No-Break">publicly available.</span></li>
				<li><strong class="bold">Standard Infrequent Access (S3 Standard-IA):</strong> This option is <a id="_idTextAnchor178"/>used when you need data to be returned quickly, but <a id="_idTextAnchor179"/>not for frequent access. The object size has to be a minimum of 128 KB. The minimum storage timeframe is 30 days. If the object is deleted before 30 days, you are still charged for 30 days. Standard-IA objects are resilient to the loss of <span class="No-Break">Availability Zones.</span></li>
				<li><strong class="bold">One Zone Infrequent Access (S3 One Zone-IA):</strong> Objects <a id="_idTextAnchor180"/>in this storage class are <a id="_idTextAnchor181"/>stored in just one Availability Zone, which makes it cheaper than <strong class="bold">Standard-IA</strong>. The minimum object size and storage timeframe are the same as Standard-IA. Objects from this storage class are less available and less resilient. This storage class is used when you have another copy, or if the data can be recreated. A <strong class="bold">One Zone-IA</strong> storage class should <a id="_idTextAnchor182"/>be used for long-lived data that is non-critical and replaceable, and where access <span class="No-Break">is infrequent.</span></li>
				<li><strong class="bold">Amazon S3 Glacier Flexible Retrieval (formerly S3 Glacier): </strong>This option is used for long-term <a id="_idTextAnchor183"/>archiving and backup. It can take anything <a id="_idTextAnchor184"/>from minutes to hours to retrieve objects in this storage class. The minimum storage timeframe is 90 days. For archived data that doesn’t need to be accessed right away but requires the ability to retrieve extensive data sets without incurring additional charges, like in backup or disaster-recovery scenarios, S3 Glacier Flexible Retrieval (formerly known as S3 Glacier) is the perfect <span class="No-Break">storage option.</span></li>
				<li><strong class="bold">Amazon S3 Glacier Instant Retrieval:</strong> This storage class offers cost-effective, high-speed storage for seldom-accessed, long-term data. Compared to S3 Standard-Infrequent Access, it can cut storage expenses by up to 68% when data is accessed once per quarter. This storage class is perfect for swiftly retrieving archive data like medical images, news media assets, or user-generated content archives. You can upload data directly or use S3 Lifecycle policies to move it from other S3 <span class="No-Break">storage classes.</span></li>
				<li><strong class="bold">Glacier Deep Archive:</strong> The minimum <a id="_idTextAnchor185"/>storage duration of this <a id="_idTextAnchor186"/>class is 180 days. This is the least expensive storage class and has a default retrieval time of <span class="No-Break">12 hours.</span></li>
				<li><strong class="bold">S3 Intelligent-Tiering:</strong> This storage class <a id="_idTextAnchor187"/>is designed to reduce operational overheads. Users pay a <a id="_idTextAnchor188"/>monitoring fee and AWS selects a storage class between Standard (a frequent-access tier) and Standard-IA (a lower-cost, infrequent-access tier) based on the access pattern of an object. This option is designed for long-lived data with unknown or unpredictable <span class="No-Break">access patterns.</span></li>
			</ul>
			<p>Through <a id="_idTextAnchor189"/>sets of rules, the transition between storage classes and deletion of the objects can be managed easily and are <a id="_idTextAnchor190"/>referred to as <strong class="bold">S3 Lifecycle configurations</strong>. These rules consist of actions. These can be applied to a bucket or a group of objects in that bucket defined by prefixes or tags. Actions can either be <strong class="bold">transition actions</strong> or <strong class="bold">expiration actions</strong>. Transition actions <a id="_idTextAnchor191"/>define the storage class transition of the objects following the creation of <em class="italic">a user-defined</em> number of days. Expiration <a id="_idTextAnchor192"/>actions configure the deletion of versioned objects, or the deletion of delete markers or incomplete multipart uploads. This is very useful for <span class="No-Break">managing costs.</span></p>
			<p>An illustration is given in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>. You can find more details <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html</span></a><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B21197_02_01.jpg" alt="Figure 2.1 – A comparison table of S3 Storage classes" width="1650" height="499"/>
				</div>
			</div>
			<p class="IMG---Figure"><a id="_idTextAnchor193"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – A comparison table of S3 Storage classes</p>
			<h2 id="_idParaDest-42">C<a id="_idTextAnchor194"/><a id="_idTextAnchor195"/>reating buckets to hold data</h2>
			<p>Now, y<a id="_idTextAnchor196"/>ou will see how to create a bucket, upload a<a id="_idTextAnchor197"/>n object, and read the object using the <span class="No-Break">AWS CLI:</span></p>
			<ol>
				<li>In the first step, check whether you have any buckets created by using the <strong class="source-inline">aws s3 </strong><span class="No-Break"><strong class="source-inline">ls</strong></span><span class="No-Break"> command:</span><pre class="source-code"><strong class="bold">$ pwd</strong></pre><pre class="source-code"><strong class="bold">/Users/baba/AWS-Certified-Machine-Learning-Specialty-</strong></pre><pre class="source-code"><strong class="bold">2020-Certification-Guide/Chapter-5/s3demo/demo-files</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 ls</strong></pre></li>
				<li>This command returns nothing here. So, create a bucket now by using the <strong class="source-inline">mb</strong> argument. Let’s say the bucket name is <strong class="source-inline">demo-bucket-baba</strong> in the <span class="No-Break"><strong class="source-inline">us-east-1</strong></span><span class="No-Break"> Region:</span><pre class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba --region us-east-1</strong></pre><pre class="source-code"><strong class="bold">make_bucket: demo-bucket-baba</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 ls</strong></pre><pre class="source-code"><strong class="bold">2020-11-04 14:39:50 demo-bucket-baba</strong></pre></li>
				<li>As you have created a<a id="_idTextAnchor198"/> bucket now, your next s<a id="_idTextAnchor199"/>tep is to copy a file to your bucket using the <strong class="source-inline">cp</strong> argument, as shown in the <span class="No-Break">following code:</span><pre class="source-code"><strong class="bold">$ aws s3 cp sample-file.txt s3://demo-bucket-baba/</strong></pre><pre class="source-code"><strong class="bold">upload: ./sample-file.txt to s3://demo-bucket-</strong></pre><pre class="source-code"><strong class="bold">baba/sample-file.txt</strong></pre></li>
				<li>To validate the file upload operation via the AWS console, please log in to your AWS account and go to the AWS S3 console to see the same. The AWS S3 console lists the result as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em>. The console may have changed by the time you are reading <span class="No-Break">this book!</span></li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B21197_02_02.jpg" alt="Figure 2.2 – AWS S3 listing your files" width="1179" height="663"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor200"/>Figure 2.2 – AWS S3 listing your files</p>
			<p class="list-inset">You ca<a id="_idTextAnchor201"/>n also li<a id="_idTextAnchor202"/>st the files in your S3 bucket from the command line, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></pre>
			<pre class="source-code"><strong class="bold">2020-11-04 14:50:02         99 sample-file.txt</strong></pre>
			<ol>
				<li value="5">If you want to upload your filesystem directories and files to the S3 bucket, then <strong class="source-inline">--recursive</strong> will do the job <span class="No-Break">for you:</span><pre class="source-code"><strong class="bold">$ aws s3 cp . s3://demo-bucket-baba/ --recursive</strong></pre><pre class="source-code"><strong class="bold">upload: folder-1/a.txt to s3://demo-bucket-baba/folder-1/a.txt</strong></pre><pre class="source-code"><strong class="bold">upload: folder-2/sample-image.jpg to s3://demo-bucket-baba/folder-2/sample-image.jpg</strong></pre><pre class="source-code"><strong class="bold">upload: ./sample-file.txt to s3://demo-bucket-baba/sample-file.txt</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></pre></li>
				<li>The contents of one bucket can be copied/moved to another bucket via the <strong class="source-inline">cp </strong>command and the <strong class="source-inline">--recursive</strong> parameter. To achieve this, you will have to create tw<a id="_idTextAnchor203"/>o buckets, <strong class="source-inline">demo-bucket-baba-copied</strong> and <strong class="source-inline">demo-bucket-baba-moved</strong>. The steps are <span class="No-Break">as<a id="_idTextAnchor204"/> follows:</span><pre class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba-copied --region us-east-2</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 mb s3://demo-bucket-baba-moved --region us-east-2</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 cp s3://demo-bucket-baba s3://demo-bucket-baba-copied/ --recursive</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 mv s3://demo-bucket-baba s3://demo-bucket-baba-moved/ --recursive</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 ls</strong></pre><pre class="source-code"><strong class="bold">2020-11-04 14:39:50 demo-bucket-baba</strong></pre><pre class="source-code"><strong class="bold">2020-11-04 15:44:28 demo-bucket-baba-copied</strong></pre><pre class="source-code"><strong class="bold">2020-11-04 15:44:37 demo-bucket-baba-moved</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 ls s3://demo-bucket-baba/</strong></pre><p class="list-inset">If all the commands are run successfully, then the original bucket should be empty at the end (as all the files have now <span class="No-Break">been moved).</span></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">In the certification exam, you will not find many questions on bucket- and object-level operations. However, it is always better to know the basic operations and the <span class="No-Break">required steps.</span></p>
			<ol>
				<li value="7">The buckets must be deleted to avoid costs as soon as the hands-on work is finished. The bucket has to be empty before you run the <span class="No-Break"><strong class="source-inline">rb</strong></span><span class="No-Break"> command:</span><pre class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-moved</strong></pre><pre class="source-code"><strong class="bold">remove_bucket failed: s3://demo-bucket-baba-moved An error occurred (BucketNotEmpty) when calling the DeleteBucket operation: The bucket you tried to delete is not empty</strong></pre></li>
				<li>The <strong class="source-inline">demo-bucket-baba-moved</strong> bucket is not empty, so you couldn’t remove the bucket. In such scenarios, use the <strong class="source-inline">--force</strong> parameter to delete the entire bucket and all its contents, as <span class="No-Break">shown here:</span><pre class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-moved --force</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 rb s3://demo-bucket-baba-copied--force</strong></pre><p class="list-inset">If yo<a id="_idTextAnchor205"/>u want to de<a id="_idTextAnchor206"/>lete all the content from a specific prefix inside a bucket using the CLI, then it is easy to use the <strong class="source-inline">rm</strong> command with the <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">recursive</strong></span><span class="No-Break"> parameter.</span></p></li>
				<li>Let’s take an example of a bucket, <strong class="source-inline">test-bucket</strong>, that has a prefix, <strong class="source-inline">images</strong>. This prefix contains four image files named <strong class="source-inline">animal.jpg</strong>, <strong class="source-inline">draw-house.jpg</strong>, <strong class="source-inline">cat.jpg</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">human.jpg</strong></span><span class="No-Break">.</span></li>
				<li>Now, to delete the contents inside the images, the command will be as follows: <strong class="source-inline">aws s3 rm </strong><span class="No-Break"><strong class="source-inline">s3://test-bucket/images –recursive</strong></span></li>
				<li>The bucket should now <span class="No-Break">be empty.</span></li>
			</ol>
			<p>In the next section, you are going to learn about object tags and <span class="No-Break">object metadata.</span></p>
			<h2 id="_idParaDest-43">Di<a id="_idTextAnchor207"/><a id="_idTextAnchor208"/>stinguishing between object tags and object metadata</h2>
			<p>Let’s co<a id="_idTextAnchor209"/>mpare these <span class="No-Break">two te<a id="_idTextAnchor210"/>rms:</span></p>
			<ul>
				<li><strong class="bold">Object tag</strong>: An object tag is a <strong class="bold">key-value</strong> pair. AWS S3 object tags can help you filter analytics an<a id="_idTextAnchor211"/>d metrics, categorize storage, secure objects based on certain categorizations, track costs based on certain categorization of objects, and much more besides. Object tags can be used to create life cycle rules to move objects to cheaper storage tiers. You can have a maximum of 10 tags added to an object and 50 tags to a bucket. A tag key can contain 128 Unicode characters, while a tag value can contain 256 <span class="No-Break">Unicode characters.</span></li>
				<li><strong class="bold">Object metadata</strong>: Object metadata is<a id="_idTextAnchor212"/> descriptive data describing an object. It consists of <strong class="bold">name-value</strong> pairs. Object metadata is returned as HTTP headers on objects. They are of two types: one is <strong class="bold">system metadata</strong>, and the other is <strong class="bold">user-defined metadata</strong>. User-defined metadata is a custom name-value pair added to an object by the user. The name must begin with <strong class="bold">x-amz-meta</strong>. You can change all system metadata such as storage class, versioning, and encryption attributes on an object. Further details are available <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html</span></a><span class="No-Break">.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">Metadata names are case-insensitive, whereas tag names <span class="No-Break">are case-sensitive.</span></p>
			<p>In the next section, you are going to learn about controlling access to buckets and objects on Amazon S3 through different policies, including the resource policy and the <span class="No-Break">identity policy.</span></p>
			<h1 id="_idParaDest-44">Co<a id="_idTextAnchor213"/><a id="_idTextAnchor214"/>ntrolling access to buckets and objects on Amazon S3</h1>
			<p>Once the ob<a id="_idTextAnchor215"/>ject is stored in<a id="_idTextAnchor216"/> the bucket, the next ma<a id="_idTextAnchor217"/>jor step is to manage ac<a id="_idTextAnchor218"/>cess. S3 is private by default, and access is given to other users, groups, or resources via se<a id="_idTextAnchor219"/>veral methods. This means that access to the objects can be managed via <strong class="bold">Access Control Lists (ACLs)</strong>, <strong class="bold">Public Access Settings</strong>, <strong class="bold">Identity Policies</strong>, and <span class="No-Break"><strong class="bold">Bucket Policies</strong></span><span class="No-Break">.</span></p>
			<p>Let’s look at some of these <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-45">S3<a id="_idTextAnchor220"/><a id="_idTextAnchor221"/> bucket policy</h2>
			<p>An <strong class="bold">S3 bucket policy</strong> is a resource po<a id="_idTextAnchor222"/>licy that is attached to a bucket. Resource policies decide who can access that resource. It differs from identity policies in that identity policies can be attached or assigned to the identities inside an account, whereas resource policies can control identities from the same account or different accounts. Resource policies control anonymous principals too, which means an object can be made public through resource policies. The following example policy allows everyone in the world to read the bucket because <strong class="source-inline">Principal</strong> is <span class="No-Break">rendered </span><span class="No-Break"><strong class="source-inline">*</strong></span><span class="No-Break">:</span></p>
			<pre class="console"><strong class="source-inline">{</strong></pre>
			<pre class="console"><strong class="source-inline">  "Version":"2012-10-17"</strong></pre>
			<pre class="console"><strong class="source-inline">  "Statement":[</strong></pre>
			<pre class="console"><strong class="source-inline">    {</strong></pre>
			<pre class="console"><strong class="source-inline">      "Sid":"AnyoneCanRead",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Effect":"Allow",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Principal":"*",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Action":["s3:GetObject"],</strong></pre>
			<pre class="console"><strong class="source-inline">      "Resource":["arn:aws:s3:::my-bucket/*"]</strong></pre>
			<pre class="console"><strong class="source-inline">    }</strong></pre>
			<pre class="console"><strong class="source-inline">    ]</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<p>By default, everything in S3 is private to the owner. If you want to make a prefix public to the world, then <strong class="source-inline">Resource</strong> changes to <strong class="source-inline">arn:aws:s3:::my-bucket/some-prefix/*</strong>, and similarly, if it is intended for a specific IAM user or IAM group, then those details go in the principal part in <span class="No-Break">the policy.</span></p>
			<p>There can be conditions added to the bucket policy too. Let’s examine a use case where the organization wants to<a id="_idTextAnchor223"/> keep a bucket public and whitelist particular IP addresses. The policy would look something <span class="No-Break">like this:</span></p>
			<pre class="console"><strong class="source-inline">{</strong></pre>
			<pre class="console"><strong class="source-inline">  "Version":"2012-10-17"</strong></pre>
			<pre class="console"><strong class="source-inline">  "Statement":[</strong></pre>
			<pre class="console"><strong class="source-inline">    {</strong></pre>
			<pre class="console"><strong class="source-inline">      "Sid":"ParticularIPRead",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Effect":"Allow",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Principal":"*",</strong></pre>
			<pre class="console"><strong class="source-inline">      "Action":["s3:GetObject"],</strong></pre>
			<pre class="console"><strong class="source-inline">      "Resource":["arn:aws:s3:::my-bucket/*"],</strong></pre>
			<pre class="console"><strong class="source-inline">      "Condition":{</strong></pre>
			<pre class="console"><strong class="source-inline">        "NotIpAddress":{"aws:SourceIp":"2.3.3.6/32"}</strong></pre>
			<pre class="console"><strong class="source-inline">      }</strong></pre>
			<pre class="console"><strong class="source-inline">    }</strong></pre>
			<pre class="console"><strong class="source-inline">    ]</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<p>More examples are available in the AWS S3 developer guide, which can be found <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html</span></a><span class="No-Break">.</span></p>
			<p><strong class="bold">Block public access</strong> is a separate setting gi<a id="_idTextAnchor224"/>ven to the bucket owner to avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket can be made public through bucket policy by mistake; to avoid such mistakes, or data leaks, AWS has provided this setting. It provides a further level of security, irrespective of the bucket policy. You can choose this while creating a bucket, or it can be set after creating <span class="No-Break">a bucket.</span></p>
			<p><strong class="bold">Identity policies</strong> are meant for IAM users and IAM roles. Identity policies are validated when an IAM id<a id="_idTextAnchor225"/>entity (user or role) requests to access a resource. All requests are denied by default. If an identity intends to access any services, resources, or actions, then access must be provided explicitly through identity policies. The example policy that follows can be attached to an IAM user and the IAM user will be allowed to have full RDS access within a specific Region (<strong class="source-inline">us-east-1</strong> in <span class="No-Break">this example):</span></p>
			<pre class="console"><strong class="source-inline">{</strong></pre>
			<pre class="console"><strong class="source-inline">    "Version": "2012-10-17",</strong></pre>
			<pre class="console"><strong class="source-inline">    "Statement": [</strong></pre>
			<pre class="console"><strong class="source-inline">        {</strong></pre>
			<pre class="console"><strong class="source-inline">            "Effect": "Allow",</strong></pre>
			<pre class="console"><strong class="source-inline">            "Action": "rds:*",</strong></pre>
			<pre class="console"><strong class="source-inline">            "Resource": ["arn:aws:rds:us-east-1:*:*"]</strong></pre>
			<pre class="console"><strong class="source-inline">        },</strong></pre>
			<pre class="console"><strong class="source-inline">        {</strong></pre>
			<pre class="console"><strong class="source-inline">            "Effect": "Allow",</strong></pre>
			<pre class="console"><strong class="source-inline">            "Action": ["rds:Describe*"],</strong></pre>
			<pre class="console"><strong class="source-inline">            "Resource": ["*"]</strong></pre>
			<pre class="console"><strong class="source-inline">        }</strong></pre>
			<pre class="console"><strong class="source-inline">    ]</strong></pre>
			<pre class="console"><strong class="source-inline">}</strong></pre>
			<p><strong class="bold">ACLs</strong> are used to gr<a id="_idTextAnchor226"/>ant high-level permissions, typically for granting access to other AWS accounts. ACLs are one of the <strong class="bold">sub-resources</strong> of a bucket or an object. A bucket or object can be made public quickly via ACLs. AWS doesn’t suggest doing this, and you shouldn’t expect questions about this on the test. It is good to know about this, but it is not as flexible as the <strong class="bold">S3 </strong><span class="No-Break"><strong class="bold">bucket policy</strong></span><span class="No-Break">.</span></p>
			<p>Now, let’s learn about the methods to protect our data in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-46">Pr<a id="_idTextAnchor227"/><a id="_idTextAnchor228"/>otecting data on Amazon S3</h1>
			<p>In this section, you wi<a id="_idTextAnchor229"/>ll learn how to record every version of an ob<a id="_idTextAnchor230"/>ject. Along with durability, Amazon provides several techniques to secure the data in S3. Some of those techniques involve enabling versioning and encrypting <span class="No-Break">the objects.</span></p>
			<p>Versioning helps you to roll back to a previous version if any problem occurs with the current object during update, delete, or <span class="No-Break">put operations.</span></p>
			<p>Through encryption, you can control the access of an object. You need the appropriate key to read and write an object. You wi<a id="_idTextAnchor231"/>ll also learn <strong class="bold">Multi-Factor Authentication (MFA</strong>) for delete operations. Amazon also allows <strong class="bold">Cross-Region Replication (CRR)</strong> to maintain a <a id="_idTextAnchor232"/>copy of an object in another Region, which can be used for data backup during any disaster, for further redundancy, or for the enhancement of data access speed in <span class="No-Break">different Regions.</span></p>
			<h2 id="_idParaDest-47">Ap<a id="_idTextAnchor233"/><a id="_idTextAnchor234"/>plying bucket versioning</h2>
			<p>Let’s now un<a id="_idTextAnchor235"/>derstand how you can enable bu<a id="_idTextAnchor236"/>cket versioning with the help of some hands-on examples. Bucket versioning can be applied while creating a bucket from the AWS <span class="No-Break">S3 console:</span></p>
			<ol>
				<li>To enable versioning on a bucket from the command line, a bucket must be created first and then versioning can be enabled, as shown in the following example. In this example, I have created a bucket, <strong class="source-inline">version-demo-mlpractice</strong>, and enabled versioning through the <span class="No-Break"><strong class="source-inline">put-bucket-versioning</strong></span><span class="No-Break"> command:</span><pre class="source-code"><strong class="bold">$ aws s3 mb s3://version-demo-mlpractice/</strong></pre><pre class="source-code"><strong class="bold">$ aws s3api put-bucket-versioning --bucket version-demo-mlpractice --versioning-configuration Status=Enabled</strong></pre><pre class="source-code"><strong class="bold">$ aws s3api get-bucket-versioning --bucket version-demo-mlpractice</strong></pre><pre class="source-code">{</pre><pre class="source-code">    "Status": "Enabled"</pre><pre class="source-code">}</pre></li>
				<li>You have not created this bucket with any kind of encryption. So, if you run <strong class="bold">aws s3api get-bucket-encryption --bucket version-demo-mlpractice</strong>, then it will ou<a id="_idTextAnchor237"/>tput an error that says <span class="No-Break">the fo<a id="_idTextAnchor238"/>llowing:</span><pre class="source-code"><strong class="bold">The server side encryption configuration was not found</strong></pre></li>
				<li><strong class="bold">Server-Side Encryption (SSE)</strong> can be applied fr<a id="_idTextAnchor239"/>om the AWS S3 console while creating a bucket. This is called bucket default encryption. You can also apply SSE via the command line using the <strong class="source-inline">put-bucket-encryption</strong> API. The command will look <span class="No-Break">like this:</span><pre class="source-code"><strong class="bold">$ aws s3api put-bucket-encryption --bucket version-demo-mlpractice --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":</strong></pre><pre class="source-code"><strong class="bold">{"SSEAlgorithm":"AES256"}}]}'</strong></pre></li>
				<li>This can be verified using the following command: <strong class="source-inline">aws s3api get-bucket-encryption --</strong><span class="No-Break"><strong class="source-inline">bucket version-demo-mlpractice</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>You will learn more about encryption in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-48">Ap<a id="_idTextAnchor240"/><a id="_idTextAnchor241"/>plying encryption to buckets</h2>
			<p>You also ne<a id="_idTextAnchor242"/>ed to understand how en<a id="_idTextAnchor243"/>abling versioning on a bucket would help. There are use cases where a file is updated regularly, and versions will be created for the same file. To simulate this scenario, try the <span class="No-Break">following example:</span></p>
			<ol>
				<li>In this example, you will create a file with versions written in it. You will overwrite it and retrieve it to check the versions in <span class="No-Break">that file:</span><pre class="source-code"><strong class="bold">$ echo "Version-1"&gt;version-doc.txt</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 cp version-doc.txt s3://version-demo-mlpractice</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 cp s3://version-demo-mlpractice/version-doc.txt</strong></pre><pre class="source-code"><strong class="bold">check.txt</strong></pre><pre class="source-code"><strong class="bold">$ cat check.txt</strong></pre><pre class="source-code"><strong class="bold">Version-1</strong></pre><pre class="source-code"><strong class="bold">$ echo "Version-2"&gt;version-doc.txt</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 cp version-doc.txt s3://version-demo-mlpractice</strong></pre><pre class="source-code"><strong class="bold">$ aws s3 cp s3://version-demo-mlpractice/version-doc.txt</strong></pre><pre class="source-code"><strong class="bold">check.txt</strong></pre><pre class="source-code"><strong class="bold">$ cat check.txt</strong></pre><pre class="source-code"><strong class="bold">Version-2</strong></pre></li>
				<li>Upon retrieval, you go<a id="_idTextAnchor244"/>t the latest version of the file, in other words, <strong class="source-inline">Version-2</strong> in this case. To check each of<a id="_idTextAnchor245"/> the versions and the latest one of them, S3 provides the <strong class="source-inline">list-object-versions</strong> API, as shown here. From the JSON results, you can deduce the <span class="No-Break">latest version:</span><pre class="source-code"><strong class="bold">$ aws s3api list-object-versions</strong></pre><pre class="source-code"><strong class="bold">--bucket version-demo-mlpractice</strong></pre><pre class="source-code"><strong class="source-inline">{</strong></pre><pre class="source-code"><strong class="source-inline">    "Versions": [</strong></pre><pre class="source-code"><strong class="source-inline">        {</strong></pre><pre class="source-code"><strong class="source-inline">            "ETag":</strong></pre><pre class="source-code"><strong class="source-inline">"\"b6690f56ca22c410a2782512d24cdc97\"",</strong></pre><pre class="source-code"><strong class="source-inline">            "Size": 10,</strong></pre><pre class="source-code"><strong class="source-inline">            "StorageClass": "STANDARD",</strong></pre><pre class="source-code"><strong class="source-inline">            "Key": "version-doc.txt",</strong></pre><pre class="source-code"><strong class="source-inline">            "VersionId":</strong></pre><pre class="source-code"><strong class="source-inline">"70wbLG6BMBEQhCXmwsriDgQoXafFmgGi",</strong></pre><pre class="source-code"><strong class="source-inline">            "IsLatest": true,</strong></pre><pre class="source-code"><strong class="source-inline">            "LastModified": "2020-11-07T15:57:05+00:00",</strong></pre><pre class="source-code"><strong class="source-inline">            "Owner": {</strong></pre><pre class="source-code"><strong class="source-inline">                "DisplayName": "baba",</strong></pre><pre class="source-code"><strong class="source-inline">                "ID": "XXXXXXXXXXXX"</strong></pre><pre class="source-code"><strong class="source-inline">            }</strong></pre><pre class="source-code"><strong class="source-inline">        },</strong></pre><pre class="source-code"><strong class="source-inline">        {</strong></pre><pre class="source-code"><strong class="source-inline">            "ETag": "\"5022e6af0dd3d2ea70920438271b21a2\"",</strong></pre><pre class="source-code"><strong class="source-inline">            "Size": 10,</strong></pre><pre class="source-code"><strong class="source-inline">            "StorageClass": "STANDARD",</strong></pre><pre class="source-code"><strong class="source-inline">            "Key": "version-doc.txt",</strong></pre><pre class="source-code"><strong class="source-inline">            "VersionId": "f1iC.9L.MsP00tIb.sUMnfOEae240sIW",</strong></pre><pre class="source-code"><strong class="source-inline">            "IsLatest": false,</strong></pre><pre class="source-code"><strong class="source-inline">            "LastModified": "2020-11-07T15:56:27+00:00",</strong></pre><pre class="source-code"><strong class="source-inline">            "Owner": {</strong></pre><pre class="source-code"><strong class="source-inline">                "DisplayName": "baba",</strong></pre><pre class="source-code"><strong class="source-inline">                "ID": " XXXXXXXXXXXX"</strong></pre><pre class="source-code"><strong class="source-inline">            }</strong></pre><pre class="source-code"><strong class="source-inline">        }</strong></pre><pre class="source-code"><strong class="source-inline">    ]</strong></pre><pre class="source-code"><strong class="source-inline">}</strong></pre></li>
				<li>There may be a situation where you have to roll back to the earlier version of the current object. In the preceding example, the latest one is <strong class="source-inline">Version-2.</strong> You can make any desired version the latest or current version by parsing the <strong class="source-inline">VersionId</strong> sub-resource to the <strong class="source-inline">get-object</strong> API call and uploading that object again. The other way is to delete the current or latest version by passing <strong class="source-inline">versionId</strong> to the <strong class="source-inline">–version-id</strong> parameter in the <strong class="source-inline">delete-object</strong> API request. More details about the API are available <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html"><span class="No-Break">https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html</span></a><span class="No-Break">.</span></li>
				<li>When you delete an object in a ve<a id="_idTextAnchor246"/>rsioning-enabled bucket, it does not delete the object from the bu<a id="_idTextAnchor247"/>cket. It just creates a marker called <strong class="source-inline">DeleteMarker</strong>. It looks <span class="No-Break">like this:</span><pre class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt</strong></pre><pre class="source-code"><strong class="source-inline">{</strong></pre><pre class="source-code"><strong class="source-inline">    "DeleteMarker": true,</strong></pre><pre class="source-code"><strong class="source-inline">    "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"</strong></pre><pre class="source-code"><strong class="source-inline">}</strong></pre></li>
				<li>This means that the object is not deleted. You can list it by using <span class="No-Break">this command:</span><pre class="source-code"><strong class="bold">aws s3api list-object-versions --bucket version-demo-mlpractice</strong></pre></li>
				<li>Now the bucket has no objects as <strong class="source-inline">version-doc.txt</strong>, and you can verify this using the <strong class="source-inline">aws s3 ls</strong> command because that marker became the current version of the object with a new ID. If you try to retrieve an object that is deleted, which means a delete marker is serving the current version of the object, then you will get a <strong class="bold">404 error</strong>. Hence, the permanent deletion of an object in a versioning-enabled bucket can only be achieved by deleting the object using their version IDs against each version. If a situation arises to get the object back, then the same object can be retrieved by deleting the delete marker, <strong class="source-inline">VersionId</strong>, as shown in the following example commands. A simple delete request <strong class="bold">(without the version ID)</strong> will not delete the delete marker and create another delete marker with a unique version ID. So, it’s possible to have multiple delete markers for the same object. It is important to note at this point that it will consume your storage and you will be billed <span class="No-Break">for it:</span><pre class="source-code"><strong class="bold">$ aws s3 ls s3://version-demo-mlpractice/</strong></pre><pre class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP</strong></pre><pre class="source-code"><strong class="source-inline">{</strong></pre><pre class="source-code"><strong class="source-inline">    "DeleteMarker": true,</strong></pre><pre class="source-code"><strong class="source-inline">    "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"</strong></pre><pre class="source-code"><strong class="source-inline">}</strong></pre></li>
				<li>Upon li<a id="_idTextAnchor248"/>sting the bucket now, the older objects ca<a id="_idTextAnchor249"/>n <span class="No-Break">be seen:</span><pre class="source-code"><strong class="bold">$ aws s3 ls s3://version-demo-mlpractice/</strong></pre><pre class="source-code"><strong class="bold">2020-11-07 15:57:05         10 version-doc.txt</strong></pre><p class="list-inset">As you have already covered the exam topics and practiced most of the required concepts, you should delete the objects in the bucket and then delete the bucket to save on costs. This step deletes the versions of the object and, in turn, removes the <span class="No-Break">object permanently.</span></p></li>
				<li>Here, the latest version is deleted by giving the version ID to it, followed by the other <span class="No-Break">version ID:</span><pre class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id 70wbLG6BMBEQhCXmwsriDgQoXafFmgGi</strong></pre><pre class="source-code"><strong class="bold">$ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id f1iC.9L.MsP00tIb.sUMnfOEae240sIW</strong></pre><pre class="source-code"><strong class="bold">$ aws s3api list-object-versions --bucket version-demo-mlpractice</strong></pre><p class="list-inset">You can clearly see the empty <span class="No-Break">bucket now.</span></p></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">AWS best practices suggest adding another layer of protection through <strong class="bold">MFA delete.</strong> Accidental bucket deletions can be prevented, and the security of the objects in the bucket is ensured. MFA delete can be enabled or disabled via the console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication together: your security credentials, and the concatenation of a valid serial number, a space, and the six-digit code displayed on an approved <span class="No-Break">authentication device.</span></p>
			<p>CRR helps you to separate data between different geographical Regions. A typical use case is the maintenance business-as-usual activities during a disaster. If a Region goes down, then another Region can su<a id="_idTextAnchor250"/>pport the users if CRR is enabled. This improves the availability of the data. Another use case is to re<a id="_idTextAnchor251"/>duce latency if the same data is used by another compute resource, such as EC2 or AWS Lambda being launched in another Region. You can also use CRR to copy objects to another AWS account that belongs to a different owner. There are a few important points that are worth noting down for the <span class="No-Break">certification exam:</span></p>
			<ul>
				<li>In order to use CRR, versioning has to be enabled on both the source and <span class="No-Break">destination bucket.</span></li>
				<li>Replication is enabled on the source bucket by adding rules. As the source, either an entire bucket, a prefix, or tags can <span class="No-Break">be replicated.</span></li>
				<li>Encrypted objects can also be replicated by assigning an appropriate <span class="No-Break">encryption key.</span></li>
				<li>The destination bucket can be in the same account or in another account. You can change the storage type and ownership of the object in the <span class="No-Break">destination bucket.</span></li>
				<li>For CRR, an existing role can be chosen or a new IAM role can be <span class="No-Break">created too.</span></li>
				<li>There can be multiple replication rules on the source bucket, with priority accorded to it. Rules with higher priority override rules with <span class="No-Break">lower priority.</span></li>
				<li>When you add a replication rule, only new versions of an object that are created after the rules are enabled <span class="No-Break">get replicated.</span></li>
				<li>If versions are deleted from the source bucket, then they are not deleted from the <span class="No-Break">destination bucket.</span></li>
				<li>When you delete an object fr<a id="_idTextAnchor252"/>om the source bucket, it creates a delete marker in said source bucket. That delete ma<a id="_idTextAnchor253"/>rker is not replicated to the destination bucket <span class="No-Break">by S3.</span></li>
			</ul>
			<p>In the next section, you will cover the concept of securing <span class="No-Break">S3 objects.</span></p>
			<h1 id="_idParaDest-49">Se<a id="_idTextAnchor254"/><a id="_idTextAnchor255"/>curing S3 objects at rest and in transit</h1>
			<p>In the previous se<a id="_idTextAnchor256"/>ction, you learned about bucket default encryption, which is co<a id="_idTextAnchor257"/>mpletely different from object-level encryption. Buckets are not encrypted, whereas objects are. A question may arise here: <em class="italic">what is the default bucket encryption?</em> You will learn these co<a id="_idTextAnchor258"/>ncepts in this se<a id="_idTextAnchor259"/>ction. Data during transmission can be protected by using <strong class="bold">Secure Socket Layer (SSL)</strong> or <strong class="bold">Transport Layer Security (TLS)</strong> for the transfer of HTTPS requests. The next step is to protect the data, where the authorized person can encode and decode <span class="No-Break">the data.</span></p>
			<p>It is possible to have different en<a id="_idTextAnchor260"/>cryption settings on different objects in<a id="_idTextAnchor261"/> the same bucket. S3 supports <strong class="bold">Client-Side Encryption (CSE)</strong> and <strong class="bold">Server-Side Encryption (SSE)</strong> for objects <span class="No-Break">at rest:</span></p>
			<ul>
				<li><strong class="bold">CSE</strong>: A client uploads the object to S3 via the S3 endpoint. In CSE, the data is en<a id="_idTextAnchor262"/>crypted by the client before uploading to S3. Although the transit between the user and the S3 endpoint happens in an encrypted channel, the data in the channel is already encrypted by the client and can’t be seen. In transit, encryption takes place by default through HTTPS. So, AWS S3 stores the encrypted object and cannot read the data in any format at any point in time. In CSE, the client takes care of encrypting the object’s content. So, control stays with the client in terms of key management and the encryption-decryption process. This leads to a huge amount of CPU usage. S3 is only used <span class="No-Break">for storage.</span></li>
				<li><strong class="bold">SSE</strong>: A client uploads the object to S3 via the S3 endpoint. Even though the da<a id="_idTextAnchor263"/>ta in transit is through an encrypted channel that uses HTTPS, the objects themselves are not encrypted inside the channel. Once the data hits S3, then it is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption, object storage, and key management. There are three types of SSE techniques available for <span class="No-Break">S3 objects:</span><ul><li><span class="No-Break">SSE-C</span></li><li><span class="No-Break">SSE-S3</span></li><li><span class="No-Break">SSE-KMS</span></li></ul></li>
				<li><strong class="bold">SSE with Customer-Provided Keys (SSE-C): </strong>With SSE-C, the user is re<a id="_idTextAnchor264"/>sponsible for the key that is used for encryption and decryption. S3 manages the encryption and decryption process. In CSE, the client handles the encryption-decryption process, but in SSE-C, S3 handles the cryptographic operations. This potentially decreases the CPU requirements for these processes. The only overhead here is to manage the keys. Ideally, when a user is doing a <strong class="source-inline">PUT</strong> operation, the user has to provide a key and an object to S3. S3 encrypts the object using the key provided and attaches the hash (a cipher text) to the object. As soon as the object is stored, S3 discards the encryption key. This generated hash is on<a id="_idTextAnchor265"/>e-way and cannot be used to generate a new key. When the user provides a <strong class="source-inline">GET</strong> operation request along with the decryption key, the hash identifies whether the specific key was used for encryption. Then, S3 decrypts and discards <span class="No-Break">the key.</span></li>
				<li><strong class="bold">SSE with Amazon S3-Managed Keys (SSE-S3):</strong> With SSE-S3, AWS handles bo<a id="_idTextAnchor266"/>th the management of the key and the process of encryption and decryption. When the user uploads an object using a <strong class="source-inline">PUT</strong> operation, the user just provides the unencrypted object. S3 creates a master key to be used for the encryption process. No one can change anything on this master key as this is created, rotated internally, and managed by S3 from end to end. This is a unique key for the object. It uses the <a id="_idTextAnchor267"/>AES-256 algorithm <span class="No-Break">by default.</span></li>
				<li><strong class="bold">SSE with Customer Master Keys stored in AWS Key Management Service (SSE-KMS):</strong> AWS Key Management Service (KMS) manages the Customer Master Key (CMK). AWS S3<a id="_idTextAnchor268"/> collaborates with AWS KM<a id="_idTextAnchor269"/>S and generates an<a id="_idTextAnchor270"/> AWS-managed CMK. This is the default master key used for SSE-KMS. Every time an object is uploaded, S3 uses a dedicated key to encrypt that object, and that ke<a id="_idTextAnchor271"/>y is a <strong class="bold">Data Encryption Key (DEK).</strong> The DEK is generated by KMS using the CMK. S3 is provided with both a plain-text version and an encrypted version of the DEK. The plain-text version of DEK is used to encrypt the object and then discarded. The encrypted version of DEK is stored along with the encrypted object. When you are using SSE-KMS, it is not necessary to use the default CMK that is created by S3. You can create and use a customer-managed CMK, which means you can control the permission on it as well as the rotation of the key material. So, if you have a regulatory board in your organization that is concerned with the rotation of the key or the separation of roles between encryption users and decryption users, then SSE-KMS is the solution. Logging and auditing are also possible on SSE-KMS to track the API calls made <span class="No-Break">against keys.</span></li>
				<li><strong class="bold">Default bucket encryption:</strong> If you set AE<a id="_idTextAnchor272"/>S-256 while creating a bucket, or you enable it after creating a bucket, then SSE-S3 will be used (when you don’t set something at the object level while performing a <span class="No-Break"><strong class="source-inline">PUT</strong></span><span class="No-Break"> operation).</span></li>
			</ul>
			<p>In the next section, you will learn about some of the data stores used with <span class="No-Break">EC2 instances.</span></p>
			<h1 id="_idParaDest-50">Us<a id="_idTextAnchor273"/><a id="_idTextAnchor274"/>ing other types of data stores</h1>
			<p><strong class="bold">Elastic Block Store (EBS)</strong> is used to cr<a id="_idTextAnchor275"/>eate volumes in<a id="_idTextAnchor276"/> an Availability Zone. The vo<a id="_idTextAnchor277"/>lume can only be at<a id="_idTextAnchor278"/>tached to an EC2 instance in the same Availability Zone. Amazon EBS provides both <strong class="bold">Solid-State Drive (SSD)</strong> and <strong class="bold">Hard Disk Drive (HDD)</strong> types of volumes. For SSD-based volumes, the dominant pe<a id="_idTextAnchor279"/>rformance attribute is <strong class="bold">Input-Output Per Second (IOPS)</strong>, and for HDD it is throughput, which is generally measured as MiB/s. You can choose between different volume types, such as General Purpose SSD (gp2), Provisioned IOPS SSD (io1), or Throughput Optimized HDD (st1), depending on your requirements. Provisioned IOPS volumes are often used for high-performance workloads, such as deep learning training, where low latency and high throughput are critical. <em class="italic">Table 2.1</em> provides an overview of the different volumes <span class="No-Break">and types:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><a id="_idTextAnchor280"/></strong><span class="No-Break"><strong class="bold">Volume Types</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Use cases</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>General Purpose <span class="No-Break">SSD (gp2)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Useful for maintaining balance between price and performance. Good for most workloads, system boot volumes, dev, and <span class="No-Break">test environments</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Provisioned IOPS SSD (<span class="No-Break">io2, io1)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Useful for mission-critical, high-throughput or low-latency workloads. For example, I/O intensive database workloads like MongoDB, <span class="No-Break">Cassandra, Oracle</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Throughput Optimized <span class="No-Break">HDD (st1)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Useful for frequently accessed, throughput-intensive workloads. For example, big data processing, data warehouses, <span class="No-Break">log processing</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Cold <span class="No-Break">HDD (sc1)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Useful for less frequently <span class="No-Break">accessed workloads</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – Different volumes and their use cases</p>
			<p><strong class="bold">EBS</strong> is designed to be resilient within an <a id="_idTextAnchor281"/><strong class="bold">Availability Zone (AZ)</strong>. If, for some reason, an AZ fails, then the volume cannot be accessed. To prevent such scenarios, <strong class="bold">snapshots</strong> can be created from the EBS volumes, and they are stored in S3. Once the snapshot arrives in S3, the data in the snapshot becomes Region-resilient. The first snapshot is a full copy of data on the volume and, from then onward, snapshots are incremental. Snapshots can be used to clone a volume. As the snapshot is stored in S3, a volume can be cloned in any AZ in that Region. Snapshots can be shared between Regions and volumes can be cloned from them during disaster recovery. Even after the EC2 instance is stopped/terminated, EBS volumes can retain data through an easy restoration process from <span class="No-Break">backed-up snapshots.</span></p>
			<p>Multiple EC2 instances can be attached via <strong class="bold">EBS Multi-Attach</strong> for concurrent EBS volume access. If the use case demands multiple instances to access the training dataset simultaneously (distributed training scenarios), then EBS Multi-Attach will provide the solution with improved performance <span class="No-Break">and scalability.</span></p>
			<p>AWS KMS manages <a id="_idTextAnchor282"/>the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS can use a customer-managed CMK. The CMK is used by EBS when an encrypted volume is created. The CMK is used to create an encrypted DEK, which is stored with the volume on the physical disk. This DEK can only be decrypted using KMS, assuming the entity has access to decrypt. When a snapshot is created from the encrypted volume, the snapshot is encrypted with the same DEK. Any volume created from this snapshot also uses <span class="No-Break">that DEK.</span></p>
			<p>Instance Store volumes are the <a id="_idTextAnchor283"/>block storage devices <a id="_idTextAnchor284"/>physically connected to the EC2 instance. They provide the highest <a id="_idTextAnchor285"/>performance, as the ephemeral storage attached to the instance is from the same host where the instance is launched. EBS can be attached to the instance at any time, but the instance store must be attached to the instance at the time of its <a id="_idTextAnchor286"/>launch; it cannot be attached once the instance is launched. If there is an issue on the underlying host of an EC2 instance, then the same instance will be launched on another host with a new instance store volume and the earlier instance store (ephemeral storage) and old data will be lost. The size and capabilities of the attached volumes depend on the instance types and can be found in more detail <span class="No-Break">here: </span><a href="https://aws.amazon.com/ec2/instance-types/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/</span></a><span class="No-Break">.</span></p>
			<p><strong class="bold">Elastic File System (EFS)</strong> provides a <a id="_idTextAnchor287"/>network-based filesystem <a id="_idTextAnchor288"/>that can be mounted within Linux EC2 instances and can be used by multiple instances at once. It is an implementation of <strong class="bold">NFSv4</strong>. It can be used <a id="_idTextAnchor289"/>in general-purpose mode, max I/O performance mode (for scientific analysis or parallel computing), bursting mode, and provisioned throughput mode. This makes it ideal for scenarios where multiple instances need to train on large datasets or share model artifacts. With EFS, you can store training datasets, pre-trained models, and other data centrally, ensuring consistency and reducing data duplication. Additionally, EFS provides high throughput and low-latency access, enabling efficient data access during training and inference processes. By leveraging EFS with SageMaker, machine learning developers can seamlessly scale their workloads, collaborate effectively, and accelerate model development <span class="No-Break">and training.</span></p>
			<p>As you know, in the case of instance stores, the data is volatile. As soon as the instance is lost, the data is lost from the instance store. That is not the case for EFS. EFS is separate from the EC2 instance <a id="_idTextAnchor290"/>storage. EFS is a file store and is accessed by multiple EC2 instances via mount targets inside a VPC. On-premises systems can access EFS storage via hybrid networking to the VPC, such as <strong class="bold">VPN</strong> or <strong class="bold">Direct Connect</strong>. EFS also supports two types of storage classes: Standard and Infrequent Access. Standard is used for frequently accessed data. Infrequent Access is the cost-effective storage class for long-lived, less frequently accessed data. Lifecycle policies can be used for the transition of data between storage classes. EFS offers a pay-as-you-go pricing model, where you only pay for the storage capacity you use. It eliminates the need to provision and manage separate storage volumes for each instance, reducing storage costs and simplifying storage management for your machine <span class="No-Break">learning workloads.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An instance store is preferred for max I/O requirements and if the data is replaceable <span class="No-Break">and temporary.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor291"/><a id="_idTextAnchor292"/>Relational Database Service (RDS)</h1>
			<p>This is one of the <a id="_idTextAnchor293"/>most commonly featured topics in AWS exams. You should have sufficient knowledge prior to the exam. In this section, you will learn about <span class="No-Break">Amazon’s RDS.</span></p>
			<p>AWS provides several relational databases as a service to its users. Users can run their desired database on EC2 instances, too. The biggest drawback is that the instance is only available in one Availability Zone in a Region. The EC2 instance has to be administered and monitored to avoid any kind of failure. Custom scripts will be required to maintain a data backup over time. Any database major or minor version update would result in downtime. Database instances running on an EC2 instance cannot be easily scaled if the load increases on the database as replication is not an <span class="No-Break">easy task.</span></p>
			<p>RDS provides managed database instances that can themselves hold one or more databases. Imagine a database server running on an EC2 instance that you do not have to manage or maintain. You need only access the server and create databases in it. AWS will manage everything else, such as the security of the instance, the operating system running on the instance, the database versions, and high availability of the database server. RDS supports multiple engines, such as MySQL, Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can choose any of these based on <span class="No-Break">your requirements.</span></p>
			<p>The foundation of <a id="_idTextAnchor294"/>Amazon RDS is a database instance, which can support multiple engines and can have multiple databases created by the user. One database instance can be accessed only by using the database DNS endpoint (the CNAME, which is an <a id="_idTextAnchor295"/>alias for the canonical name in a domain name system database) of the primary instance. RDS uses standard database engines. So, accessing the database using some sort of tool in a self-managed database server is the same as accessing <span class="No-Break">Amazon RDS.</span></p>
			<p>As you have now understood the requirements of Amazon RDS, let’s understand the failover process in Amazon RDS. You will cover what services Amazon offers if something goes wrong with the <span class="No-Break">RDS instance.</span></p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor296"/><a id="_idTextAnchor297"/><a id="_idTextAnchor298"/>Managing failover in Amazon RDS</h1>
			<p>RDS instances can be <strong class="bold">Single-AZ</strong> or <strong class="bold">Multi-AZ</strong>. In Multi-AZ, multiple instances work together, similar to an active-passive <span class="No-Break">failover design.</span></p>
			<p>For a Single-AZ RDS instance, storage can be allocated for that instance to use. In a nutshell, a Single-AZ RDS instance has one attached block store (EBS storage) available in the same Availability Zone. This makes the databases and the storage of the RDS instance vulnerable to Availability Zone failure. The storage allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure the RDS instance, it is advised to use a security group and provide access based <span class="No-Break">on requirements.</span></p>
			<p>Multi-AZ is always the best way to design the architecture to prevent failures and keep the applications highly available. With Multi-AZ features, a standby replica is kept in sync synchronously with the primary instance. The standby instance has its own storage in the assigned Availability Zone. A standby replica cannot be accessed directly, because all RDS access is via a single database DNS endpoint (CNAME). You can’t access the standby unless a failover happens. The standby provides no performance benefit, but it does constitute an improvement in terms of the availability of the RDS instance. It can only happen in the same Region, another AZ’s subnet in the same Region inside the VPC. When a Multi-AZ RDS instance is online, you can take a backup from the standby replica without affecting the performance. In a Single-AZ instance, availability and performance issues can be significant during <span class="No-Break">backup operation.</span></p>
			<p>To understand the workings of Multi-AZ, let’s take an example of a Single-AZ instance and expand it <span class="No-Break">to Multi-AZ.</span></p>
			<p>Imagine you have an RDS instance running in Availability Zone <strong class="source-inline">AZ-A</strong> of the <strong class="source-inline">us-east-1</strong> Region inside a VPC named <strong class="source-inline">db-vpc</strong>. This becomes a primary instance in a Single-AZ design of an RDS instance. In this case, there will be storage allocated to the instance in the <strong class="source-inline">AZ-A</strong> Availability Zone. Once you opt for Multi-AZ deployment in another Availability Zone called <strong class="source-inline">AZ-B</strong>, AWS creates a standby instance in Availability Zone <strong class="source-inline">AZ-B</strong> of the <strong class="source-inline">us-east-1</strong> Region inside the <strong class="source-inline">db-vpc</strong> VPC and allocates storage for the standby instance in <strong class="source-inline">AZ-B</strong> of the <strong class="source-inline">us-east-1</strong> Region. Along with that, RDS will enable <strong class="bold">synchronous replication</strong> from the primary instance to the standby replica. As you learned earlier, the only way to access our RDS instance is via the database CNAME, hence, the access request goes to the RDS primary instance. As soon as a write request comes to the endpoint, it writes to the primary instance. Then it writes the data to the hardware, which is the block storage attached to the primary instance. At the same time, the primary instance replicates the same data to the standby instance. Finally, the standby instance commits the data to its <span class="No-Break">block storage.</span></p>
			<p>The primary instance writes the data into the hardware and replicates the data to the standby instance in parallel, so there is a minimal time lag (almost nothing) between the data commit operations in their respective hardware. If an error occurs with the primary instance, then RDS detects this and changes the database endpoint to the standby instance. The clients accessing the database may experience a very short interruption with this. This failover occurs within 60-120 seconds. It does not provide a fault-tolerant system because there will be some impact during the <span class="No-Break">failover operation.</span></p>
			<p>You should now understand failover management on Amazon RDS. Let’s now learn about taking automatic RDS backups and using snapshots to restore in the event of a failure, and read replicas in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor299"/>Taking automatic backups, RDS snapshots, and restore and read replicas</h1>
			<p>In this section, you <a id="_idTextAnchor300"/>will see how RDS <strong class="bold">automatic backups</strong> and <strong class="bold">manual snapshots</strong> work. These <a id="_idTextAnchor301"/>features come <a id="_idTextAnchor302"/>with <span class="No-Break">Amazon <a id="_idTextAnchor303"/>RDS.</span></p>
			<p>Let’s consider a database that is scheduled to take a backup at 5 A.M. every day. If the application fails at 11 A.M., then it is possible to restart the application from the backup taken at 11 A.M. with the loss of 6 hours’ worth of data. This is <a id="_idTextAnchor304"/>called a 6-hour <strong class="bold">Recovery Point Objective (RPO)</strong>. The RPO is defined as the time between the most recent backup and the incident, and this determines the amount of data loss. If you want to reduce this, then you have to schedule more incremental backups, which increases the cost and backup frequency. If your business demands a lower RPO value, then the business must spend more to provide the necessary <span class="No-Break">technical solutions.</span></p>
			<p>Now, according to our example, an engineer was assigned the task of bringing the system back online as soon as the disaster occurred. The engineer managed to bring the database online at 2 P.M. on the same day by adding a few extra hardware components to the current system and installing some updated versions of the software. This is called a 3-hour <strong class="bold">Recovery Time Objective (RTO).</strong> The RTO is <a id="_idTextAnchor305"/>determined as the time between the disaster recovery and full recovery. RTO values can be reduced by having spare hardware and documenting the restoration process. If the business demands a lower RTO value, then your business must spend more money on spare hardware and an effective system setup to perform the <span class="No-Break">restoration process.</span></p>
			<p>In RDS, the RPO and RTO play an important role in the selection of automatic backups and manual snapshots. Both of these backup services use AWS-managed S3 buckets, which means they cannot be visible in the user’s AWS S3 console. They areRegion-resilient because the backup is replicated into multiple Availability Zones in the AWS Region. In the case of a Single-AZ RDS instance, the backup happens from the single available data store, and for a Multi-AZ enabled RDS instance, the backup happens from the standby data store (the primary store remains untouched as regards <span class="No-Break">the backup).</span></p>
			<p>The snapshots are manual <a id="_idTextAnchor306"/>for RDS instances, and they are stored in the AWS-managed S3 bucket. The first snapshot of an RDS instance is a full copy of the data and the onward snapshots are incremental, reflecting the change in the data. In terms of the time taken for the snapshot process, it is high for the first one and, from then on, the incremental backup is quicker. When any snapshot occurs, it can impact the performance of the Single-AZ RDS instance, but not the performance of a Multi-AZ RDS instance as this happens on the standby data storage. Manual snapshots do not expire, have to be cleared automatically, and live past the termination of an RDS instance. When you delete an RDS instance, it suggests making one final snapshot on your behalf and it will contain all the databases inside your RDS instance (there is not just a single database in an RDS instance). When you restore from a manual snapshot, you restore to a single point in time, and that affects <span class="No-Break">the RPO.</span></p>
			<p>To automate this entire process, you can choose a time window when these snapshots can be taken. This is called an automatic <a id="_idTextAnchor307"/>backup. These time windows can be managed carefully to essentially lower the RPO value of the business. Automatic backups have a retention period of 0 to 35 days, with 0 being disabled and the maximum is 35 days. To quote AWS documentation, retained automated backups contain system snapshots and transaction logs from a database instance. They also include database instance properties such as allocated storage and a database instance class, which are required to restore it to an active instance. Databases generate transaction logs, which contain the actual change in data in a particular database. These transaction logs are also written to S3 every 5 minutes by RDS. Transaction logs can also be replayed on top of the snapshots to restore to a point in time of 5 minutes’ granularity. Theoretically, the RPO can be a 5-minute point <span class="No-Break">in time.</span></p>
			<p>When you <a id="_idTextAnchor308"/>perform a restore, RDS creates a new RDS instance, which means a new database endpoint to access the instance. The applications using the instances have to point to the new address, which significantly affects the RTO. This means that the restoration process is not very fast, which affects the RTO. To minimize the RTO during a failure, you may consider replicating the data. With replicas, there is a high chance of replicating the corrupted data. The only way to overcome this is to have snapshots and restore an RDS instance to a particular point in time prior to the corruption. <strong class="bold">Amazon RDS Read Replicas</strong> are unlike the Multi-AZ replicas. In Multi-AZ RDS instances, the standby replicas cannot be used directly for anything unless a primary instance fails, whereas <strong class="bold">Read Replicas</strong> can <a id="_idTextAnchor309"/>be used directly, but only for read operations. Read replicas have their own database endpoints and read-heavy applications can directly point to this address. They are kept in sync <strong class="bold">asynchronously</strong> with the primary instance. Read Replicas can be created in the same Region as the primary instance or in a different Region. Read Replicas in other Regions are called <strong class="bold">Cross-Region Read Replicas</strong> and this <a id="_idTextAnchor310"/>improves the global performance of <span class="No-Break">the application.</span></p>
			<p>As per AWS documentation, five direct Read Replicas are allowed per database instance and this helps to scale out the read performances. Read Replicas have a very low RPO value due to asynchronous replication. They can be promoted to a read-write database instance in the case of a primary instance failure. This can be done quickly and it offers a fairly low <span class="No-Break">RTO value.</span></p>
			<p>In the next section, you will learn about Amazon’s database engine, <span class="No-Break">Amazon Aurora.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor311"/><a id="_idTextAnchor312"/>Writing to Amazon Aurora with multi-master capabilities</h1>
			<p>Amazon Aurora <a id="_idTextAnchor313"/>is the most reliable relational <a id="_idTextAnchor314"/>database engine developed by Amazon to deliver speed in a simple and cost-effective manner. Aurora uses a cluster of single primary instances and zero or more replicas. Aurora’s replicas can give you the advantage of both read replicas and Multi-AZ instances in RDS. Aurora uses a shared cluster volume for storage and is available to all compute instances of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision faster and improves availability and performance. Aurora uses SSD-based storage, which provides high IOPS and low latency. Aurora does not ask you to allocate storage, unlike other RDS instances; it is based on the storage that <span class="No-Break">you use.</span></p>
			<p>Aurora clusters have multiple <a id="_idTextAnchor315"/>endpoints, including the <strong class="bold">cluster endpoint</strong> <strong class="bold">and reader endpoint.</strong> If there are zero replicas, then <a id="_idTextAnchor316"/>the cluster endpoint is the same as the reader endpoint. If there are replicas available, then the reader endpoint is load-balanced across the reader endpoints. Cluster endpoints are used for reading/writing, while reader endpoints are intended for reading from the cluster. If you add more replicas, then AWS manages load balancing under the hood for the <span class="No-Break">new replicas.</span></p>
			<p>When failover occurs, the replicas are promoted to read/write mode, and this takes some time. This can be prevented in a <strong class="bold">Multi-Master</strong> mode of an Aurora cluster. This allows multiple instances to perform reads and writes at the <span class="No-Break">same time.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor317"/><a id="_idTextAnchor318"/>Storing columnar data on Amazon Redshift</h1>
			<p>Amazon <a id="_idTextAnchor319"/>Redshift is not used for real-time transactions, but it is <a id="_idTextAnchor320"/>used for data warehouse purposes. It is designed to support huge volumes of data at a petabyte scale. It is a column-based database used for analytics, long-term processing, tending, and aggregation. <strong class="bold">Redshift Spectrum</strong> can be <a id="_idTextAnchor321"/>used to query data on S3 without loading data to the Redshift cluster (a Redshift cluster is required, though). It’s not an OLTP, but an OLAP. <strong class="bold">AWS QuickSight</strong> can be integrated with Redshift for visualization, with a SQL-like interface that allows you to connect using JDBC/ODBC connections to query <span class="No-Break">the data.</span></p>
			<p>Redshift uses a clustered architecture in one AZ in a VPC with faster network connectivity between the nodes. It is not high availability by design as it is tightly coupled to the AZ. A Redshift cluster <a id="_idTextAnchor322"/>has a leader node, and this node is responsible for all the communication between the client and the computing nodes of the cluster, query planning, and aggregation. Compute nodes are responsible <a id="_idTextAnchor323"/>for running the queries submitted by the leader lode and for storing the data. By default, Redshift uses a public network for communicating with external services or any AWS services. With <strong class="bold">enhanced VPC routing</strong>, it can be controlled via customized <span class="No-Break">networking settings.</span></p>
			<p>By combining Redshift with SageMaker, data scientists and analysts can leverage the scalability and computational power of Redshift to preprocess and transform data before training machine learning models. They can utilize Redshift’s advanced SQL capabilities to perform aggregations, joins, and filtering operations, enabling efficient feature engineering and data preparation. The processed data can then be seamlessly fed into SageMaker for model training, hyperparameter tuning, <span class="No-Break">and evaluation.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor324"/><a id="_idTextAnchor325"/>Amazon DynamoDB for NoSQL Database-as-a-Service</h1>
			<p>Amazon <a id="_idTextAnchor326"/>DynamoDB is a NoSQL <a id="_idTextAnchor327"/>database-as-a-service product within AWS. It’s a fully managed key/value and document database. Accessing DynamoDB is easy via its endpoint. The input and output throughputs can be managed or scaled manually or automatically. It also supports data backup, point-in-time recovery, and <span class="No-Break">data encryption.</span></p>
			<p>One example where Amazon DynamoDB can be used with Amazon SageMaker in a cost-efficient way is for real-time prediction applications. DynamoDB can serve as a storage backend for storing and retrieving input data for prediction models built using SageMaker. Instead of continuously running and scaling an inference endpoint, which can be costlier, you can leverage DynamoDB’s low-latency access and scalability to retrieve the required input data <span class="No-Break">on demand.</span></p>
			<p>In this setup, the input data for predictions can be stored in DynamoDB tables, where each item represents a unique data instance. When a prediction request is received, the application can use DynamoDB’s efficient querying capabilities to retrieve the required input data item(s) based on specific attributes or conditions. Once the data is retrieved, it can be passed to the SageMaker endpoint for <span class="No-Break">real-time predictions.</span></p>
			<p>By using DynamoDB in this way, you can dynamically scale your application’s read capacity based on the incoming prediction requests, ensuring that you only pay for the read capacity you actually need. This approach offers a cost-efficient solution as it eliminates the need for running and managing a continuously running inference endpoint, which may incur high costs even during periods of low prediction demand. With DynamoDB and SageMaker working together, you can achieve scalable and cost-efficient real-time prediction applications while maintaining low latency and <span class="No-Break">high availability.</span></p>
			<p>You will not cover the DynamoDB table structure or key structure in this chapter as this is not required for the certification exam. However, it is good to have a basic knowledge of them. For more details, please refer to the AWS docs available <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html"><span class="No-Break">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor328"/><a id="_idTextAnchor329"/>Summary</h1>
			<p>In this chapter, you learned about various data storage services from Amazon, and how to secure data through various policies and use these services. If you are working on machine learning use cases, then you may encounter such scenarios where you have to choose an effective data storage service for <span class="No-Break">your requirements.</span></p>
			<p>In the next chapter, you will learn about the migration and processing of <span class="No-Break">stored data.</span></p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor330"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH02"><span class="No-Break">https://packt.link/MLSC01E2_CH02</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B21197_02_03.jpg" alt="Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B21197_02_04.jpg" alt="Figure 2.4 – Chapter Review Questions for Chapter 2" width="1422" height="752"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Chapter Review Questions for Chapter 2</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor331"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-60"><a id="_idTextAnchor332"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-61"><a id="_idTextAnchor333"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-62"><a id="_idTextAnchor334"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor335"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>