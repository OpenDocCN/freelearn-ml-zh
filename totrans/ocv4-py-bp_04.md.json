["```py\nimport cv2\nimport numpy as np\nimport wx\n\nfrom wx_gui import BaseLayout\n```", "```py\n def main():\n```", "```py\ncapture = cv2.VideoCapture(0)\nassert capture.isOpened(), \"Can not connect to camera\"\ncapture.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\ncapture.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n```", "```py\napp = wx.App()\nlayout = CameraCalibration(capture, title='Camera Calibration', fps=2)\n```", "```py\nlayout.Show(True)\napp.MainLoop()\n```", "```py\nclass CameraCalibration(BaseLayout): \n```", "```py\n    def augment_layout(self):\n        pnl = wx.Panel(self, -1)\n        self.button_calibrate = wx.Button(pnl, label='Calibrate Camera')\n        self.Bind(wx.EVT_BUTTON, self._on_button_calibrate)\n        hbox = wx.BoxSizer(wx.HORIZONTAL)\n        hbox.Add(self.button_calibrate)\n        pnl.SetSizer(hbox)\n```", "```py\nself.panels_vertical.Add(pnl, flag=wx.EXPAND | wx.BOTTOM | wx.TOP,\n                                 border=1)\n```", "```py\nself.chessboard_size = (9, 6) \n```", "```py\n        # prepare object points\n        self.objp = np.zeros((np.prod(self.chessboard_size), 3),\n                             dtype=np.float32)\n        self.objp[:, :2] = np.mgrid[0:self.chessboard_size[0],\n                                    0:self.chessboard_size[1]]\n                                    .T.reshape(-1, 2)\n```", "```py\n        # prepare recording\n        self.recording = False\n        self.record_min_num_frames = 15\n        self._reset_recording()\n```", "```py\n    def _on_button_calibrate(self, event):\n        \"\"\"Enable recording mode upon pushing the button\"\"\"\n        self.button_calibrate.Disable()\n        self.recording = True\n        self._reset_recording()\n```", "```py\ndef _reset_recording(self): \n    self.record_cnt = 0 \n    self.obj_points = [] \n    self.img_points = [] \n```", "```py\n    def process_frame(self, frame):\n        \"\"\"Processes each frame\"\"\"\n        # if we are not recording, just display the frame\n        if not self.recording:\n            return frame\n\n        # else we're recording\n        img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                   .astype(np.uint8)\n        if self.record_cnt < self.record_min_num_frames:\n            ret, corners = cv2.findChessboardCorners(\n                               img_gray,\n                               self.chessboard_size,\n                               None)\n```", "```py\n            if ret:\n                print(f\"{self.record_min_num_frames - self.record_cnt} chessboards remain\")\n                cv2.drawChessboardCorners(frame, self.chessboard_size, corners, ret)\n```", "```py\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,\n            30, 0.01)\ncv2.cornerSubPix(img_gray, corners, (9, 9), (-1, -1), criteria)\n```", "```py\nself.obj_points.append(self.objp) \nself.img_points.append(corners) \nself.record_cnt += 1 \n```", "```py\nelse:\n    print(\"Calibrating...\")\n    ret, K, dist, rvecs, tvecs = cv2.calibrateCamera(self.obj_points,\n                                                     self.img_points,\n                                                     (self.imgHeight,\n                                                      self.imgWidth),\n                                                     None, None)\n```", "```py\nprint(\"K=\", K)\nprint(\"dist=\", dist)\n```", "```py\nK= [[ 3.36696445e+03 0.00000000e+00 2.99109943e+02] \n    [ 0.00000000e+00 3.29683922e+03 2.69436829e+02] \n    [ 0.00000000e+00 0.00000000e+00 1.00000000e+00]] \ndist= [[ 9.87991355e-01 -3.18446968e+02 9.56790602e-02 \n         -3.42530800e-02 4.87489304e+03]]\n```", "```py\nmean_error = 0 \nfor obj_point, rvec, tvec, img_point in zip(\n        self.obj_points, rvecs, tvecs, self.img_points):\n    img_points2, _ = cv2.projectPoints(\n        obj_point, rvec, tvec, K, dist)\n    error = cv2.norm(img_point, img_points2,\n                     cv2.NORM_L2) / len(img_points2)\n    mean_error += error\n\nprint(\"mean error=\", mean_error)\n```", "```py\nimport numpy as np \n\nfrom scene3D import SceneReconstruction3D\n\n```", "```py\ndef main():\n```", "```py\nK = np.array([[2759.48 / 4, 0, 1520.69 / 4, 0, 2764.16 / 4,\n               1006.81 / 4, 0, 0, 1]]).reshape(3, 3)\nd = np.array([0.0, 0.0, 0.0, 0.0, 0.0]).reshape(1, 5) \n```", "```py\nscene = SceneReconstruction3D(K, d) \nscene.load_image_pair(\"fountain_dense/0004.png\", \n     \"fountain_dense/0005.png\")\n```", "```py\nscene.plot_rectified_images()\nscene.plot_optic_flow()\nscene.plot_point_cloud()\n```", "```py\nimport cv2 \nimport numpy as np \nimport sys \n\nfrom mpl_toolkits.mplot3d import Axes3D \nimport matplotlib.pyplot as plt \nfrom matplotlib import cm\n\nclass SceneReconstruction3D: \n    def __init__(self, K, dist): \n        self.K = K \n        self.K_inv = np.linalg.inv(K) \n        self.d = dist \n```", "```py\n    @staticmethod\n    def load_image(\n            img_path: str,\n            use_pyr_down: bool,\n            target_width: int = 600) -> np.ndarray:\n\n        img = cv2.imread(img_path, cv2.CV_8UC3)\n        # make sure image is valid\n        assert img is not None, f\"Image {img_path} could not be loaded.\"\n        if len(img.shape) == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\n        while use_pyr_down and img.shape[1] > 2 * target_width:\n            img = cv2.pyrDown(img)\n        return img\n```", "```py\n    def load_image_pair(\n            self,\n            img_path1: str,\n            img_path2: str,\n            use_pyr_down: bool = True) -> None:\n\n        self.img1, self.img2 = [cv2.undistort(self.load_image(path, \n                                                              use_pyr_down), \n                                              self.K, self.d)\n            for path in (img_path1,img_path2)]\n```", "```py\n    def _extract_keypoints(self, feat_mode):\n        # extract features\n        if feat_mode.lower() == \"sift\":\n            # feature matching via sift and BFMatcher\n            self._extract_keypoints_sift()\n        elif feat_mode.lower() == \"flow\":\n            # feature matching via optic flow\n            self._extract_keypoints_flow()\n        else:\n            sys.exit(f\"Unknown feat_mode {feat_mode}. Use 'SIFT' or \n                     'FLOW'\")\n```", "```py\n    def _extract_keypoints_sift(self):\n        # extract keypoints and descriptors from both images\n        detector = cv2.xfeatures2d.SIFT_create()\n        first_key_points, first_desc = detector.detectAndCompute(self.img1,\n                                                                 None)\n        second_key_points, second_desc = detector.detectAndCompute(self.img2,\n                                                                   None)\n```", "```py\n        matcher = cv2.BFMatcher(cv2.NORM_L1, True)\n        matches = matcher.match(first_desc, second_desc)\n```", "```py\n        # generate lists of point correspondences\n        self.match_pts1 = np.array(\n            [first_key_points[match.queryIdx].pt for match in matches])\n        self.match_pts2 = np.array(\n            [second_key_points[match.trainIdx].pt for match in matches])\n```", "```py\ndef _extract_keypoints_flow(self): \n    fast = cv2.FastFeatureDetector() \n    first_key_points = fast.detect(self.img1, None) \n```", "```py\nfirst_key_list = [i.pt for i in first_key_points] \nfirst_key_arr = np.array(first_key_list).astype(np.float32) \n```", "```py\nsecond_key_arr, status, err = \n     cv2.calcOpticalFlowPyrLK(self.img1, self.img2, \n         first_key_arr)\n```", "```py\ncondition = (status == 1) * (err < 5.) \nconcat = np.concatenate((condition, condition), axis=1) \nfirst_match_points = first_key_arr[concat].reshape(-1, 2) \nsecond_match_points = second_key_arr[concat].reshape(-1, 2) \n\nself.match_pts1 = first_match_points \nself.match_pts2 = second_match_points \n```", "```py\n    def plot_optic_flow(self):\n        self._extract_keypoints_flow()\n\n        img = np.copy(self.img1)\n        for pt1, pt2 in zip(self.match_pts1, self.match_pts2):\n            cv2.arrowedLine(img, tuple(pt1), tuple(pt2),\n                     color=(255, 0, 0))\n\n        cv2.imshow(\"imgFlow\", img)\n        cv2.waitKey()\n```", "```py\ndef _find_fundamental_matrix(self): \n    self.F, self.Fmask = cv2.findFundamentalMat(self.match_pts1, \n         self.match_pts2, cv2.FM_RANSAC, 0.1, 0.99)\n```", "```py\ndef _find_essential_matrix(self): \n    self.E = self.K.T.dot(self.F).dot(self.K) \n```", "```py\ndef _find_camera_matrices(self): \n    U, S, Vt = np.linalg.svd(self.E) \n    W = np.array([0.0, -1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, \n         1.0]).reshape(3, 3)\n```", "```py\n        first_inliers = []\n        second_inliers = []\n        for pt1,pt2, mask in \n        zip(self.match_pts1,self.match_pts2,self.Fmask):\n            if mask:\n                first_inliers.append(self.K_inv.dot([pt1[0], pt1[1], 1.0]))\n                second_inliers.append(self.K_inv.dot([pt2[0], pt2[1], \n                                      1.0]))\n```", "```py\n        R = T = None\n        for r in (U.dot(W).dot(Vt), U.dot(W.T).dot(Vt)):\n            for t in (U[:, 2], -U[:, 2]):\n                if self._in_front_of_both_cameras(\n                        first_inliers, second_inliers, r, t):\n                    R, T = r, t\n\n        assert R is not None, \"Camera matricies were never found!\"\n```", "```py\nself.Rt1 = np.hstack((np.eye(3), np.zeros((3, 1)))) \n```", "```py\nself.Rt2 = np.hstack((R, T.reshape(3, 1))) \n```", "```py\n    def _in_front_of_both_cameras(self, first_points, second_points, rot,\n                                  trans):\n        \"\"\"Determines whether point correspondences are in front of both\n           images\"\"\"\n        rot_inv = rot\n        for first, second in zip(first_points, second_points):\n            first_z = np.dot(rot[0, :] - second[0] * rot[2, :],\n                             trans) / np.dot(rot[0, :] - second[0] * rot[2, \n                             :],\n                                             second)\n            first_3d_point = np.array([first[0] * first_z,\n                                       second[0] * first_z, first_z])\n            second_3d_point = np.dot(rot.T, first_3d_point) - np.dot(rot.T,\n                                                                     trans)\n```", "```py\nif first_3d_point[2] < 0 or second_3d_point[2] < 0: \n    return False \nreturn True \n```", "```py\ndef plot_rectified_images(self, feat_mode=\"SIFT\"): \n    self._extract_keypoints(feat_mode) \n    self._find_fundamental_matrix() \n    self._find_essential_matrix() \n    self._find_camera_matrices_rt() \n\n    R = self.Rt2[:, :3] \n    T = self.Rt2[:, 3] \n```", "```py\n        R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(\n            self.K, self.d, self.K, self.d, \n            self.img1.shape[:2], R, T, alpha=1.0)\n        mapx1, mapy1 = cv2.initUndistortRectifyMap(\n            self.K, self.d, R1, self.K, self.img1.shape[:2],\n            cv2.CV_32F)\n        mapx2, mapy2 = cv2.initUndistortRectifyMap(\n            self.K, self.d, R2, self.K,\n            self.img2.shape[:2],\n            cv2.CV_32F)\n        img_rect1 = cv2.remap(self.img1, mapx1, mapy1, \n                              cv2.INTER_LINEAR)\n        img_rect2 = cv2.remap(self.img2, mapx2, mapy2, \n                              cv2.INTER_LINEAR)\n```", "```py\n        total_size = (max(img_rect1.shape[0], img_rect2.shape[0]),\n                      img_rect1.shape[1] + img_rect2.shape[1], 3)\n        img = np.zeros(total_size, dtype=np.uint8)\n        img[:img_rect1.shape[0], :img_rect1.shape[1]] = img_rect1\n        img[:img_rect2.shape[0], img_rect1.shape[1]:] = img_rect2\n```", "```py\n        for i in range(20, img.shape[0], 25):\n            cv2.line(img, (0, i), (img.shape[1], i), (255, 0, 0))\n\n        cv2.imshow('imgRectified', img)\n        cv2.waitKey()\n```", "```py\nfirst_inliers = np.array(self.match_inliers1).reshape\n     (-1, 3)[:, :2]second_inliers = np.array(self.match_inliers2).reshape\n     (-1, 3)[:, :2]\n```", "```py\npts4D = cv2.triangulatePoints(self.Rt1, self.Rt2, first_inliers.T,\n     second_inliers.T).T\n```", "```py\npts3D = pts4D[:, :3]/np.repeat(pts4D[:, 3], 3).reshape(-1, 3) \n```", "```py\n\n    def plot_point_cloud(self, feat_mode=\"SIFT\"):\n        self._extract_keypoints(feat_mode)\n        self._find_fundamental_matrix()\n        self._find_essential_matrix()\n        self._find_camera_matrices_rt()\n\n        # triangulate points\n        first_inliers = np.array(self.match_inliers1)[:, :2]\n        second_inliers = np.array(self.match_inliers2)[:, :2]\n        pts4D = cv2.triangulatePoints(self.Rt1, self.Rt2, first_inliers.T,\n                                      second_inliers.T).T\n\n        # convert from homogeneous coordinates to 3D\n        pts3D = pts4D[:, :3] / pts4D[:, 3, None]\n```", "```py\n\n        Xs, Zs, Ys = [pts3D[:, i] for i in range(3)]\n\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(Xs, Ys, Zs, c=Ys, cmap=cm.hsv, marker='o')\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        ax.set_zlabel('Z')\n        plt.title('3D point cloud: Use pan axes button below to inspect')\n        plt.show()\n```"]