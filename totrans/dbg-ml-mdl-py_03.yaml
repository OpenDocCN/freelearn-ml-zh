- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Debugging toward Responsible AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试以实现负责任人工智能
- en: Developing successful machine learning models is not solely about achieving
    high performance. We all get excited when we improve the performance of our models.
    We feel responsible for developing a high-performance model. But we are also responsible
    for building fair and secure models. These goals, which are beyond performance
    improvement, are among the objectives of *responsible machine learning*, or more
    broadly, *responsible artificial intelligence*. As part of responsible machine
    learning modeling, we should consider transparency and accountability when training
    and making predictions for our models and consider governance systems for our
    data and modeling processes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 开发成功的机器学习模型并不仅仅是关于实现高性能。当我们提高我们模型的表现时，我们都感到兴奋。我们觉得自己有责任开发一个高性能的模型。但我们也有责任构建公平和安全的模型。这些超越性能改进的目标，是*负责任机器学习*或更广泛地，*负责任人工智能*的目标之一。作为负责任机器学习建模的一部分，我们在训练和预测模型时应该考虑透明度和问责制，并考虑我们数据和建模过程的治理系统。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Impartial modeling fairness in machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的无偏建模公平性
- en: Security and privacy in machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的安全和隐私
- en: Transparency in machine learning modeling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习建模中的透明度
- en: Accountable and open to inspection modeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可问责和可检查的建模
- en: Data and model governance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据和模型治理
- en: By the end of this chapter, you will understand the need and different concerns
    and challenges in responsible machine learning modeling. You will have also learned
    about different techniques that can help us in responsible modeling and ensuring
    privacy and security while developing machine learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解负责任机器学习建模中的需求和不同关注点以及挑战。您还将了解不同的技术，这些技术可以帮助我们在负责任建模的同时确保隐私和安全，在开发机器学习模型时。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You need to understand the components of machine learning life cycles before
    reading this chapter as this will help you better understand the concepts and
    be able to use them in your projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之前，您需要了解机器学习生命周期的组成部分，因为这将帮助您更好地理解这些概念，并能够在您的项目中使用它们。
- en: Impartial modeling fairness in machine learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的无偏建模公平性
- en: Machine learning models make mistakes. But when a mistake happens, they could
    have biases, such as in the COMPAS example provided in [*Chapter 1*](B16369_01.xhtml#_idTextAnchor015),
    *Beyond Code Debugging*. We need to investigate our models for the existence of
    such biases and revise them to eliminate these biases. Let’s go through more examples
    to clarify the importance of investigating our data and models for the existence
    of such biases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型会犯错误。但是当错误发生时，它们可能存在偏见，例如在[*第一章*](B16369_01.xhtml#_idTextAnchor015)中提供的COMPAS示例，*超越代码调试*。我们需要调查我们的模型是否存在这种偏见，并修改它们以消除这些偏见。让我们通过更多示例来阐明调查我们的数据和模型是否存在这种偏见的重要性。
- en: Recruiting is a challenging process for every company as they must identify
    the most suitable candidates to interview from hundreds of applicants who have
    submitted resumes and cover letters. In 2014, Amazon started to develop a hiring
    tool using machine learning to screen job applicants and select the best ones
    to pursue based on the information provided in their resumes. This was a text
    processing model that used the text in resumes to identify the key information
    and select the top candidates. But eventually, Amazon decided to abandon the system
    as the model was biased in selecting men over women in the hiring process. The
    main reason behind this bias was the data, which contained mainly resumes of men,
    that was fed into the machine learning model. The model learned how to identify
    language and key information in men’s resumes, but it was not effective when it
    came to women’s resumes. Hence, the model couldn’t rank candidates for a job application
    while remaining unbiased in terms of gender.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 招聘对每家公司来说都是一个具有挑战性的过程，因为他们必须从提交简历和求职信的数百名申请人中确定最合适的候选人进行面试。2014年，亚马逊开始开发一个招聘工具，使用机器学习来筛选求职者，并根据他们在简历中提供的信息选择最佳候选人。这是一个文本处理模型，它使用简历中的文本来识别关键信息并选择顶级候选人。但最终，亚马逊决定放弃该系统，因为该模型在招聘过程中对男性比对女性有偏见。这种偏见背后的主要原因是数据，这些数据主要是男性简历，被输入到机器学习模型中。模型学会了如何识别男性简历中的语言和关键信息，但面对女性简历时并不有效。因此，该模型在保持性别无偏见的同时，无法对工作申请的候选人进行排名。
- en: Some machine learning models are designed to predict the likelihood of hospitalization.
    These models can help reduce individual and population healthcare costs. However,
    such beneficial models can have their own biases. For example, hospitalization
    requires access to and the use of health care services, which is influenced by
    differences in socioeconomic conditions. This means that the datasets that are
    available for building models to predict the likelihood of hospitalization would
    have more positive data on people of high socioeconomic conditions compared to
    poor families. This inequality could cause biases in decision-making by machine
    learning models for hospitalization, which results in limiting the access of low
    socioeconomic people to hospitalization even further.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习模型被设计用来预测住院的可能性。这些模型可以帮助降低个人和人群的医疗保健成本。然而，这样的有益模型也可能存在自己的偏差。例如，住院需要获得和使用医疗服务，这受到社会经济条件差异的影响。这意味着用于构建预测住院可能性的模型的数据集，与贫困家庭相比，将包含更多社会经济条件较高人群的正面数据。这种不平等可能导致机器学习模型在住院决策中的偏差，从而进一步限制社会经济条件较低人群获得住院的机会。
- en: Another example of biases in machine learning applications in the healthcare
    setting has been in genetic studies. These studies have been criticized for biases
    due to them not properly accounting for diversity in populations, which could
    result in misdiagnosis in the studied diseases.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗保健环境中机器学习应用中偏差的另一个例子出现在遗传研究中。这些研究因未能妥善考虑人群的多样性而受到批评，这可能导致所研究疾病的误诊。
- en: Two main sources of bias include data, which either originated from the data
    source or was introduced in data processing before model training, and algorithmic
    bias. Let’s review both.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差的两个主要来源包括数据，这些数据可能源自数据源，也可能在模型训练之前的数据处理中引入，以及算法偏差。让我们来回顾一下这两者。
- en: Data bias
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据偏差
- en: You might have heard of the “garbage in, garbage out” concept in computer science.
    This concept is about the fact that if nonsense data gets into a computer tool,
    such as a machine learning model, the output will be nonsense. The data that gets
    fed to help train machine learning algorithms could have all sorts of issues that
    eventually result in biases, as mentioned previously. For example, the data could
    under-represent a group, similar to women in the hiring data fed into the Amazon
    model. Recall that having this biased data shouldn’t stop us from building models,
    but we have to design our life cycle components, such as data selection and wrangling
    or model training, while considering these biases and testing our models for bias
    detection before bringing a model into production. The following are some of the
    sources of data biases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经听说过计算机科学中的“垃圾输入，垃圾输出”概念。这个概念是关于这样一个事实：如果无意义的数据进入计算机工具，例如机器学习模型，输出也将是无意义的。用于帮助训练机器学习算法的数据可能存在各种问题，最终导致偏差，正如之前提到的。例如，数据可能未能充分代表某个群体，类似于亚马逊模型中输入的招聘数据中的女性。回想一下，拥有这种偏差数据不应该阻止我们构建模型，但我们必须在设计生命周期组件时考虑到这些偏差，例如数据选择和整理或模型训练，并在将模型投入生产之前测试我们的模型以检测偏差。以下是一些数据偏差的来源。
- en: Data collection bias
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集偏差
- en: Data that is collected could contain biases, such as gender bias, as in the
    Amazon applicant sorting example, race bias, as in COMPAS, socioeconomic biases,
    as in hospitalization examples, or other kinds of biases. As another example,
    imagine that a machine learning model for autonomous driving is trained only on
    images of streets, cars, people, and traffic signs taken in the daytime. The model
    will be biased and not reliable in the nighttime. This kind of bias can be removed
    after providing feedback from data exploration or data wrangling steps to data
    collection and selection in the machine learning life cycle. But if it is not
    revised before a model gets trained, tested, and deployed, then the feedback needs
    to be immediately provided from model monitoring, when biases in predictions get
    detected, and used in the life cycle to provide less biased data for modeling.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的数据可能包含偏差，例如，在亚马逊申请人分类示例中存在的性别偏差，在COMPAS中存在的种族偏差，在医院化示例中存在的社会经济偏差，或其他类型的偏差。作为另一个例子，想象一个用于自动驾驶的机器学习模型仅训练于白天拍摄的街道、汽车、人和交通标志的图像。该模型在夜间将具有偏差且不可靠。这种偏差可以通过在机器学习生命周期中的数据探索或数据整理步骤提供反馈后从数据收集和选择中移除。但如果在模型训练、测试和部署之前没有修订，那么在检测到预测偏差时，需要立即从模型监控中提供反馈，并在生命周期中使用以提供更少偏差的数据进行建模。
- en: Sampling bias
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本偏差
- en: Another source of data bias could be in the process of sampling data points
    or sampling the population in the *data collection* stage of the life cycle. For
    example, when sampling students to fill in a survey, our sampling process could
    be biased toward girls or boys, rich or poor student families, or high versus
    low-grade students. These kinds of biases cannot be easily fixed by adding samples
    of other groups. Sampling processes for filling surveys or designing clinical
    trials for new drug testing on patients are among examples of data collection
    processes where adding data to them is not necessarily allowed. Some of these
    data collection processes need a prior definition of the population that cannot
    be changed in the middle of the process. In such cases, different kinds of possible
    biases need to be determined and considered when designing the data sampling process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据偏差的另一个来源可能是生命周期中数据收集阶段的样本数据点或人群样本。例如，当抽样学生填写调查问卷时，我们的抽样过程可能会偏向于女孩或男孩、富裕或贫困的学生家庭，或者高年级或低年级学生。这类偏差仅通过添加其他群体的样本难以轻易纠正。填写调查问卷或为新药测试设计临床试验的抽样过程就是数据收集过程中添加数据并不一定被允许的例子。其中一些数据收集过程需要在过程中预先定义人群，且人群定义在过程中不能改变。在这种情况下，在设计数据抽样过程时需要确定和考虑不同类型的可能偏差。
- en: Exclusion bias
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排除偏差
- en: In the process of data cleaning and wrangling, before you start training and
    testing a machine learning model, features could be removed because of statistical
    reasoning, such as low information content or variance across data points or not
    having a desired characteristic. These feature removals can sometimes cause biases
    in our modeling. Although not excluded, some of the features could also cause
    biases in the eventual machine learning model predictions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '在数据清洗和整理过程中，在开始训练和测试机器学习模型之前，可能由于统计推理（如数据点的低信息含量或方差，或没有期望的特征）而删除特征。这些特征删除有时会导致我们的建模偏差。尽管不是排除在外，一些特征也可能导致最终机器学习模型预测的偏差。 '
- en: Measurement or labeling bias
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测量或标注偏差
- en: Measurement and annotation biases could be caused by issues or differences in
    terms of technologies, experts, or non-expert data annotators, who generated or
    annotated the data that’s used for model training, testing, and prediction in
    production. For example, if one camera type is used to collect the data to train
    a machine learning model for image classification, predictions in production might
    have lower reliability if images in production will be captured by another camera
    that generates images with a different quality.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 测量和标注偏差可能由技术、专家或非专家数据标注员在生成或标注用于模型训练、测试和生产预测的数据时遇到的问题或差异引起。例如，如果使用一种类型的相机收集数据来训练用于图像分类的机器学习模型，那么在生产中如果使用另一种生成不同质量的图像的相机进行图像捕捉，生产中的预测可能可靠性较低。
- en: Algorithmic bias
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法偏差
- en: There could be systematic errors associated with the algorithm and training
    process of a machine learning model. For example, instead of the data being biased
    to a specific race or skin color in face recognition tools, the algorithm might
    result in biased predictions regarding a group with a specific skin color or race.
    Keeping the machine learning life cycle in mind, in the modular way it was presented
    in [*Chapter 2*](B16369_02.xhtml#_idTextAnchor076), *Machine Learning Life Cycle*,
    will help you identify the issues in a stage such as model monitoring. Then, the
    feedback can be provided for the relevant step, such as data collection or data
    wrangling, to eliminate the identified biases. There are methodologies to detect
    biases and resolve them that we will go through in future chapters. For example,
    we can use machine learning explainability techniques to identify the contributions
    of features, or their combinations, that could cause biases in predictions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习模型的算法和训练过程相关的系统错误可能是存在的。例如，在人脸识别工具中，数据可能被偏向于特定的种族或肤色，而算法可能会对具有特定肤色或种族的群体产生有偏见的预测。考虑到机器学习生命周期，在[*第二章*](B16369_02.xhtml#_idTextAnchor076)中展示的模块化方式，*机器学习生命周期*将帮助你在模型监控等阶段识别问题。然后，可以为相关步骤提供反馈，例如数据收集或数据处理，以消除已识别的偏见。我们将在未来的章节中介绍检测偏见和解决偏见的方法。例如，我们可以使用机器学习可解释性技术来识别可能导致预测偏见的特征或其组合的贡献。
- en: In addition to eliminating biases in our models, we also need to take into account
    security and privacy concerns while going through a machine learning life cycle,
    which is our next topic.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了消除模型中的偏见外，在经历机器学习生命周期时，我们还需要考虑安全和隐私问题，这是我们接下来要讨论的主题。
- en: Security and privacy in machine learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的安全和隐私
- en: 'Security is a concern for all businesses with physical or virtual products
    and services. 60 years ago, each bank had to ensure the security of physical assets,
    such as cash and important documents, in its branches. But after moving to the
    digital world, they had to build new security systems to make sure that the data
    of their clients and their money and assets, which can now be transferred and
    changed digitally, were secure. Machine learning products and technologies are
    no exception and need to have proper security systems. Security concerns in machine
    learning settings could be related to the security of the data, the models themselves,
    or model predictions. In this section, we will introduce three important subjects
    regarding security and privacy in machine learning modeling: **data privacy**,
    **data poisoning**, and **adversarial attacks**.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有拥有物理或虚拟产品和服务的企业来说，安全都是一个关注点。60年前，每家银行都必须确保其分支机构中现金和重要文件等实物资产的安全。但是，在进入数字世界后，他们必须建立新的安全系统，以确保其客户的数据、金钱和资产的安全，这些资产现在可以以数字方式传输和更改。机器学习产品和技术也不例外，需要拥有适当的安全系统。机器学习环境中的安全担忧可能与数据的安全、模型本身或模型预测有关。在本节中，我们将介绍与机器学习建模中的安全和隐私相关的三个重要主题：**数据隐私**、**数据中毒**和**对抗攻击**。
- en: Data privacy
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据隐私
- en: 'The privacy of the user data in production or the data you have stored and
    used for model training and testing is an important aspect of security system
    design for machine learning technologies. The data needs to be secure for many
    reasons:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中用户数据的隐私性，或者您用于模型训练和测试存储和使用的数据库是机器学习技术安全系统设计的重要方面。出于许多原因，数据需要保持安全：
- en: If the data includes confidential information of users, people, or organizations
    the data has been received from
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据包含从用户、个人或组织接收的机密信息
- en: If the data is licensed from a commercial data provider under legal contracts
    and should not become accessible through your services or technologies with others
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据是根据法律合同从商业数据提供商那里许可的，并且不应通过您的服务或技术对他人可访问
- en: If the data is generated for you and considered one of the assets of your team
    and organization
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据是为您生成的，并被认为是您团队和组织的资产之一
- en: In all these cases, you need to make sure the data is secure. You can use security
    systems for your databases and datasets. You can also design encryption processes
    on top of this if part of the data needs to be transferred digitally between two
    servers, for example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，您需要确保数据的安全。您可以使用数据库和数据集的安全系统。如果部分数据需要在两个服务器之间以数字方式传输，您还可以在此之上设计加密过程。
- en: Data privacy attacks
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据隐私攻击
- en: Some attacks are designed to access private and confidential data in your datasets
    and databases, such as patient information in hospitals, customer data in banking
    systems, or the personal information of employees of governmental organizations.
    Three of these attacks are *data reconstruction attacks*, *identity recognition
    attacks*, and *individual tracing attacks*, all of which can be done through **internet
    protocol** (**IP**) tracking, for example.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些攻击旨在访问您数据集和数据库中的私有和机密数据，例如医院中的患者信息、银行系统中的客户数据或政府机构员工的个人信息。其中三种攻击是*数据重建攻击*、*身份识别攻击*和*个人追踪攻击*，所有这些都可以通过**互联网协议**（**IP**）跟踪来实现，例如。
- en: Data poisoning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中毒
- en: Change in the meaning and quality of data is another concern in data security.
    Data could be poisoned and the resulting changes in prediction could have drastic
    consequences financially, legally, and ethically for individuals, teams, and organizations.
    Imagine you designed a machine learning model with your friends for stock market
    prediction and your model uses news feeds and stock prices in previous days as
    input features. This data gets extracted from different resources such as Yahoo
    Finance and different sources of news. If your database gets poisoned, by changing
    the values of some of the features or changes in the collected data, such as the
    price history of a piece of stock, you might go through serious financial losses
    as your model might suggest that you buy stocks that will lose their value by
    more than 50% in a week rather than going up. This is an example that has financial
    consequences. However, data poisoning could have life-threatening consequences
    if, for example, it happens in healthcare or military systems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的意义和质量的变化是数据安全中的另一个担忧。数据可能会被中毒，预测结果的变化可能对个人、团队和组织在财务、法律和道德方面产生严重影响。想象一下，你和你的朋友们设计了一个用于股票市场预测的机器学习模型，并且你的模型使用过去几天的新闻和股票价格作为输入特征。这些数据是从雅虎财经和不同的新闻来源等不同资源中提取的。如果你的数据库被中毒，通过改变某些特征值或收集的数据的变化，例如某只股票的价格历史，你可能会遭受严重的经济损失，因为你的模型可能会建议你购买一周内价值下降超过50%的股票，而不是上涨。这是一个有财务后果的例子。然而，如果发生在医疗或军事系统中，数据中毒可能会产生致命的后果。
- en: Adversarial attacks
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗性攻击
- en: Sometimes, you can fool machine learning models by making very simple changes,
    such as adding small amounts of noise or perturbation to feature values. This
    is the concept behind generating adversarial examples and adversarial attacks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，通过在特征值上做出非常简单的改变，例如添加少量的噪声或扰动，你可以欺骗机器学习模型。这是生成对抗样本和对抗攻击背后的概念。
- en: For example, in a medical AI system, an image of a benign (that is, not harmful)
    mole could be diagnosed as malignant (that is, harmful and dangerous in general
    terms) by adding adversarial noise in the image that would not be recognizable
    by the human eye or simply rotating the image. Synonymous text substitution such
    as changing “*The patient has a history of back pain and chronic alcohol abuse
    and more recently has been seen in several...*” to “*The patient has a history
    of lumbago and chronic alcohol dependence and more recently has been seen in several...*”
    could change the diagnosis from benign to malignant (Finlayson et al., 2019).
    In other applications of image classification, such as in self-driving cars, simple
    black and white stickers could sometimes fool models into classifying images of
    stop signs or frames of videos of stop signs (Eykholt et al., 2018).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个医疗人工智能系统中，一个良性（即，无害）的痣的图像可能会被诊断为恶性（即，在一般意义上有害和危险的）通过在图像中添加人眼无法识别的对立噪声或简单地旋转图像。将“*患者有背部疼痛和慢性酒精滥用史，最近还出现过几次...*”替换为“*患者有腰痛和慢性酒精依赖史，最近还出现过几次...*”这样的同义文本替换可能会将诊断从良性变为恶性（Finlayson等人，2019年）。在其他图像分类的应用中，例如在自动驾驶汽车中，简单的黑白贴纸有时会欺骗模型将停车标志的图像或停车标志视频帧分类（Eykholt等人，2018年）。
- en: 'Adversarial examples could mislead your system in inference or training and
    validating whether they get injected into your modeling data and poison it. There
    are three important aspects of knowing your adversary that can help you in protecting
    your systems – that is, the attacker’s goal, knowledge, and capability (*Table
    3.1*):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性样本可能会在推理或训练过程中误导你的系统，并验证它们是否被注入到你的建模数据中并使其中毒。了解你的对手有三个重要方面可以帮助你保护你的系统——即攻击者的目标、知识和能力（*表3.1*）：
- en: '| **Type of Knowledge about** **the Adversary** | **Aspects of Different Types**
    **of Knowledge** | **Definition** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **关于对手的知识类型** | **不同类型知识的方面** | **定义** |'
- en: '| The attacker’s goal | Security violation | The attacker tries to do the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '| 攻击者的目标 | 安全违规 | 攻击者试图做以下事情：'
- en: Evade detection
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免检测
- en: Compromise system functionalities
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 破坏系统功能
- en: Get access to private information
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取访问私有信息
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Attack specificity | Targeting specific or random data points to generate
    wrong results |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 攻击特异性 | 针对特定或随机数据点以生成错误结果 |'
- en: '| The attacker’s knowledge | Perfect-knowledge white-box attacks | The attacker
    knows everything about the system |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 攻击者的知识 | 完美知识白盒攻击 | 攻击者了解系统的一切 |'
- en: '| Zero-knowledge black-box attacks | The attacker doesn’t have any knowledge
    of the system itself but collects information through predictions of the model
    in production |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 零知识黑盒攻击 | 攻击者对系统本身没有任何了解，但通过生产中模型的预测收集信息 |'
- en: '| Limited-knowledge gray-box attacks | The attacker has limited knowledge |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 有限知识灰盒攻击 | 攻击者有有限的知识 |'
- en: '| The attacker’s capability | Attack influence | **Causative**: Attackers can
    poison train data and manipulate test data**Exploratory**: The attacker can manipulate
    test data only |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 攻击者的能力 | 攻击影响 | **因果性**：攻击者可以毒害训练数据并操纵测试数据**探索性**：攻击者只能操纵测试数据 |'
- en: '| Data manipulation constraints | Constraints on data manipulation to eliminate
    data manipulation or make it challenging |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 数据操纵约束 | 对数据操纵的约束，以消除数据操纵或使其具有挑战性 |'
- en: Table 3.1 – Types of knowledge about adversaries (Biggio et al., 2018)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 – 关于对手的知识类型（Biggio等人，2018）
- en: Output integrity attacks
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出完整性攻击
- en: This type of attack usually doesn’t affect data processing, model training and
    testing, or even prediction in production. It comes between the output of your
    model and what will be shown to the user. Based on this definition, this attack
    is not specific to machine learning settings. But in our machine learning systems,
    understanding this type of attack solely based on the outputs shown to the users
    might be challenging. For example, if the prediction probabilities or labels of
    your model in classification settings get changed once in a while, the results
    that are shown to the users will be wrong, but the user might accept them if they
    believe in our systems. It is our responsibility to make sure such kinds of attacks
    don’t challenge the integrity of the results of our model in production.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击通常不会影响数据处理、模型训练和测试，甚至不会影响生产中的预测。它出现在你的模型输出和展示给用户的内容之间。根据这个定义，这种攻击并不特定于机器学习环境。但在我们的机器学习系统中，仅基于展示给用户的输出理解这种攻击可能具有挑战性。例如，如果你的模型在分类设置中的预测概率或标签偶尔发生变化，展示给用户的结果将是错误的，但如果用户相信我们的系统，他们可能会接受这些结果。确保这类攻击不会挑战我们模型在生产中的结果完整性是我们的责任。
- en: System manipulation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 系统操纵
- en: Your machine learning system could be manipulated by intentionally designed
    synthetic data, which either does not exist or might not have existed in the model
    training and test sets. This manipulation in the prediction level could not only
    have consequences such as time wasted for investigating wrong predictions of the
    model, but it could also poison your models and change the performance of your
    model in testing and production if the data enters your training, evaluation,
    or test data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习系统可能被故意设计的合成数据操纵，这些数据可能不存在，或者可能从未存在于模型训练和测试集中。这种在预测层面的操纵不仅可能导致调查模型错误预测的时间浪费，还可能毒害你的模型，并改变你的模型在测试和生产中的性能，如果数据进入你的训练、评估或测试数据。
- en: Secure and private machine learning techniques
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全和隐私机器学习技术
- en: 'Some techniques help us in developing secure and privacy-preserving processes
    and tools for data storage, transfer, and use in machine learning modeling:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些技术帮助我们开发用于数据存储、传输以及在机器学习建模中使用的数据的安全和隐私保护流程和工具：
- en: '**Anonymization**: This technique focuses on removing information that helps
    in identifying individual data points, such as individual patients, within a healthcare
    dataset. This information could be very specific, such as health card numbers,
    which could have different names in different countries, or more general information,
    such as gender and age.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匿名化**：这项技术侧重于删除有助于识别个人数据点（如医疗数据集中的单个患者）的信息。这些信息可能非常具体，例如健康卡号码，在不同国家可能有不同的名称，或者更一般的信息，如性别和年龄。'
- en: '**Pseudonymization**: Instead of removing information, as in anonymization,
    the personally identifiable data could be replaced with synthetic substitutes
    as part of pseudonymization.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脱敏**：与匿名化不同，脱敏不是删除信息，而是将可识别的个人数据替换为合成替代品作为脱敏的一部分。'
- en: '**Data and algorithm encryption**: The encryption process transforms the information
    – be it data or an algorithm – into a new (encrypted) form. The encrypted data
    can be decrypted (so that it becomes human-readable or machine understandable)
    if the individual has access to the encryption key (that is, a password-style
    key necessary for the decryption process). In this way, getting access to the
    data and algorithm without the encryption key will be almost impossible or very
    difficult. We will review encryption techniques such as **Advanced Encryption
    Standard** (**AES**) in [*Chapter 16*](B16369_16.xhtml#_idTextAnchor429), *Security
    and Privacy in* *Machine Learning*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据和算法加密**：加密过程将信息（无论是数据还是算法）转换成新的（加密）形式。如果个人有权访问加密密钥（即用于解密过程的密码式密钥），则加密数据可以被解密（使其成为人类可读或机器可理解）。这样，在没有加密密钥的情况下获取数据和算法将几乎不可能或非常困难。我们将在第16章“机器学习中的安全和隐私”中回顾加密技术，如**高级加密标准**（**AES**）[*第16章*](B16369_16.xhtml#_idTextAnchor429)。'
- en: '**Homomorphic encryption**: This is an encryption technique that eliminates
    the need for data decryption at the time of prediction by a machine learning model.
    The model uses the encrypted data for predictions, so the data can be kept encrypted
    through the whole data transfer and usage process in a machine learning pipeline.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同态加密**：这是一种加密技术，通过机器学习模型进行预测时无需对数据进行解密。模型使用加密数据进行预测，因此数据可以在机器学习管道中的整个数据传输和使用过程中保持加密状态。'
- en: '**Federated machine learning**: Federated machine learning relies on the idea
    of decentralizing learning, data analysis, and inference, thus allowing the user
    data to be kept within individual devices or local databases.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联邦机器学习**：联邦机器学习依赖于将学习、数据分析、推理去中心化的想法，从而允许用户数据保持在单个设备或本地数据库中。'
- en: '**Differential privacy**: Differential privacy tries to ensure that the removal
    or addition of individual data points does not affect the outcome of modeling.
    It attempts to learn from patterns within groups of data points. For example,
    by adding random noise from a normal distribution, it tries to make features of
    individual data points obscure. The effect of noise in learning could be eliminated
    based on the law of large numbers ([https://www.britannica.com/science/law-of-large-numbers](https://www.britannica.com/science/law-of-large-numbers))
    if a large number of data points is accessible.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**差分隐私**：差分隐私试图确保删除或添加单个数据点不会影响建模的结果。它试图从数据点的组内模式中学习。例如，通过添加来自正态分布的随机噪声，它试图使单个数据点的特征变得模糊。如果可以访问大量数据点，基于大数定律（[https://www.britannica.com/science/law-of-large-numbers](https://www.britannica.com/science/law-of-large-numbers)），可以消除学习中的噪声影响。'
- en: These techniques are not applicable and useful in all settings. For example,
    federated machine learning will not be helpful when you have an internal database
    and need to just be sure about its security. Differential privacy for small data
    sources could also be unreliable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术并不适用于所有环境，也不一定有用。例如，当你有一个内部数据库并且只需要确保其安全性时，联邦机器学习将不会有所帮助。对于小型数据源，差分隐私也可能不可靠。
- en: Encryption and decryption processes
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 加密和解密过程
- en: Encryption is the process to transform readable data into a human-unreadable
    form. On the other hand, decryption is the process of transforming encrypted data
    back into its original readable format. You can find more information on this
    topic at [https://docs.oracle.com/](https://docs.oracle.com/) and [https://learn.microsoft.com/en-ca/](https://learn.microsoft.com/en-ca/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 加密是将可读数据转换为人类不可读形式的过程。另一方面，解密是将加密数据转换回其原始可读格式的过程。您可以在[https://docs.oracle.com/](https://docs.oracle.com/)和[https://learn.microsoft.com/en-ca/](https://learn.microsoft.com/en-ca/)找到更多关于这个主题的信息。
- en: In this section, we talked about privacy and security in machine learning modeling.
    Even if we build a secure system with minimum privacy concerns, we need to consider
    other factors to build trust in our models. Transparency is one of those factors.
    We will introduce this next.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了机器学习建模中的隐私和安全问题。即使我们构建了一个具有最小隐私担忧的安全系统，我们也需要考虑其他因素来建立对我们模型的信任。透明度就是这些因素之一。我们将在下一节介绍这一点。
- en: Transparency in machine learning modeling
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习建模中的透明度
- en: 'Transparency helps users of your model trust it by helping them understand
    how it works and how it was built. It also helps you, your team, your collaborators,
    and your organization to collect feedback on different components of your machine
    learning life cycle. It is worth understanding the transparency requirements in
    different stages of a life cycle and the challenges in achieving them:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度通过帮助用户理解模型的工作原理和构建方式，使用户信任您的模型。它还帮助您、您的团队、您的合作者和您的组织收集关于机器学习生命周期不同组件的反馈。了解生命周期不同阶段的透明度要求以及实现它们的挑战是值得的：
- en: '**Data collection**: Transparency in data collection needs to answer two major
    questions:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集**：数据收集的透明度需要回答两个主要问题：'
- en: What data are you collecting?
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在收集哪些数据？
- en: What do you want to use that data for?
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想用这些数据做什么？
- en: For example, when users click on the agreement button for data usage when registering
    for a mobile phone app, they are giving consent for the information they provide
    in the app to be used. But the agreement needs to be clear on the part of the
    user data that is going to be used and for what purposes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当用户在注册手机应用时点击数据使用协议按钮，他们是在同意将他们在应用中提供的信息用于。但协议需要明确说明将要使用哪些用户数据以及用于什么目的。
- en: '**Data selection and exploration**: In these stages of the life cycle, your
    process of data selection and how you achieved your exploratory results need to
    be clear. This helps you collect feedback from other collaborators and colleagues
    on your project.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据选择和探索**：在这些生命周期阶段，您数据选择的过程以及您如何实现探索性结果需要清晰。这有助于您从其他项目合作者和同事那里收集反馈。'
- en: '**Data wrangling and modeling data preparation**: Before this step, data is
    almost like the so-called raw data, without any changes in feature definition
    or data being split into train and test sets. If you design these components of
    the life cycle as a black box and it’s not transparent, you might lose both trust
    and the opportunity for feedback from other experts with future access to your
    data and results. For example, imagine you are supposed to not use the genetic
    information of patients in hospitals, and you provide features called Feature1,
    Feature2, and so on after these steps in the life cycle. Without explaining how
    those features were generated and using what original features, people cannot
    be sure if you used patients’ genetic information or not. You also need to be
    transparent about how you designed your testing strategy and separated your training
    data from validation and testing.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据整理和建模数据准备**：在此步骤之前，数据几乎就像所谓的原始数据，没有对特征定义进行任何更改，也没有将数据分成训练集和测试集。如果您将这些生命周期组件设计成一个黑盒且不透明，您可能会失去其他专家在将来访问您的数据和结果时的信任和反馈机会。例如，想象一下，您本不应该使用医院的患者的遗传信息，而在生命周期这些步骤之后，您提供了称为Feature1、Feature2等特征。如果没有解释这些特征是如何生成的以及使用了哪些原始特征，人们就不能确定您是否使用了患者的遗传信息。您还需要对您如何设计测试策略以及如何将训练数据与验证和测试数据分开进行透明度说明。'
- en: '**Model training and evaluation**: Transparency in model training helps in
    understanding the decision-making and pattern recognition aspects of models when
    learning from data. Transparency in both training and evaluation builds trust
    for direct users, developers, and auditors to better assess these processes. It
    is true that, for example, >99% of Amazon Prime users never want to understand
    the machine learning modeling behind the scenes. However, our models are sometimes
    used directly by users, such as doctors in hospitals for diagnosis or employees
    of manufacturing facilities. *Explainability*, which we will discuss in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine Learning Modeling*, is an important concept in machine learning modeling
    that has a close relationship with transparency and helps users better understand
    how the model works.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练和评估**：模型训练的透明度有助于在从数据中学习时理解模型的决策和模式识别方面。训练和评估的透明度有助于直接用户、开发人员和审计员更好地评估这些过程。例如，确实有超过99%的亚马逊Prime用户不想了解背后的机器学习建模。然而，我们的模型有时会被用户直接使用，例如医院中的医生进行诊断或制造设施中的员工。我们将讨论的[*第6章*](B16369_06.xhtml#_idTextAnchor201)，“机器学习建模中的可解释性和可理解性”，是机器学习建模中的一个重要概念，它与透明度密切相关，并有助于用户更好地理解模型的工作原理。'
- en: '**Testing**: Transparency in decision-making in the testing stage of the life
    cycle that helps in better understanding the process of selecting or rejecting
    a model, or planning for improving and debugging the related code, data processing,
    or model training and evaluation.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：在生命周期测试阶段的决策透明度有助于更好地理解选择或拒绝模型的过程，或规划改进和调试相关代码、数据处理、或模型训练和评估。'
- en: '**Model deployment and monitoring**: In addition to getting feedback from other
    developers and experts in your organization and team, or public experts, you need
    to make the effect of the model in production and what data is collected for monitoring
    clear for users and auditors within or outside of your organization.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署和监控**：除了从组织或团队中的其他开发人员和专家，或公众专家那里获取反馈外，您还需要让用户和审计员（无论在组织内部还是外部）清楚模型在生产中的效果以及收集用于监控的数据。'
- en: Despite the need for transparency, an increase in transparency might make your
    system vulnerable from the perspectives of security and privacy. So, you cannot
    aim for maximum transparency without keeping privacy and security in mind.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管需要透明度，但透明度的增加可能会从安全和隐私的角度使您的系统变得脆弱。因此，您不能在不考虑隐私和安全的情况下追求最大程度的透明度。
- en: Fairness and bias removal, privacy, and transparency are all factors in responsible
    machine learning modeling. But we need to remember that the accountability for
    achieving fair and secure models and systems is on us.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 公平性、偏差消除、隐私和透明度都是负责任机器学习建模的因素。但我们需要记住，实现公平和安全模型及系统的责任在我们身上。
- en: Accountable and open to inspection modeling
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任且开放接受检查的建模
- en: The models we develop as employees of different businesses or incorporations,
    research institutes or universities, or as freelancers could eventually get into
    production in different applications, such as healthcare, finance, manufacturing,
    marketing, retail, transportation, media, and entertainment. Our models could
    use patient data to predict whether they will get cancer or diabetes or whether
    they will respond to therapy. Alternatively, they could use the financial history
    and other information on the clients of a bank to assess their eligibility for
    loans. Another example is that our model can use the history of people’s purchases
    to recommend new products to them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们作为不同企业或公司、研究机构或大学，或自由职业者的员工开发的模型，最终可能会应用于不同的应用，如医疗保健、金融、制造、营销、零售、交通、媒体和娱乐。我们的模型可以使用患者数据来预测他们是否会患上癌症或糖尿病，或者他们是否会响应治疗。或者，它们可以使用银行客户的财务历史和其他信息来评估他们获得贷款的资格。另一个例子是，我们的模型可以使用人们购买的历史记录向他们推荐新产品。
- en: 'As we discussed in this chapter, we have to take care of the privacy of data
    and models, provide a fair and impartial model, and make our models as transparent
    as possible. But we have to remember that we are accountable for managing all
    these concerns in developing reliable and responsible machine learning models.
    I asked ChatGPT, the famous machine learning chatbot developed by OpenAI, “*Are
    you accountable for your responses?*” It responded as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中讨论的，我们必须注意数据的隐私和模型的隐私，提供一个公平无私的模型，并尽可能使我们的模型透明。但我们必须记住，我们在开发可靠和负责任的机器学习模型时，必须对这些所有问题负责。我向著名的机器学习聊天机器人ChatGPT提问，“*你对你的回应负责吗？*”它的回答如下：
- en: '*“As a machine learning model, I do not have the ability to be held accountable
    for my responses in the same way that a human would. I am designed to generate
    responses based on the input I receive, but I do not have personal beliefs, intentions,
    and motivations. I am not capable of making decisions or choices on my own. My
    responses are a reflection of the training data and algorithms that were used
    to create me, and my creators are responsible for ensuring that I am functioning*
    *as intended.”*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*“作为一个机器学习模型，我没有能力像人类一样对我的回应承担责任。我被设计成根据接收到的输入生成回应，但我没有个人信念、意图和动机。我无法独立做出决策或选择。我的回应是我被创建时使用的训练数据和算法的反映，我的创造者负责确保我按预期运行。”*'
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: There are quotes in the next few paragraphs of this chapter from legal sources
    that haven’t been changed to make sure the legal content is not rephrased inappropriately.
    However, this is not a legal book and no content of this book should be considered
    as legal advice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本章接下来的几段引用了法律来源的引文，这些引文未作改动，以确保法律内容不会被不恰当地改写。然而，这不是一本法律书籍，本书的任何内容都不应被视为法律建议。
- en: 'But accountability is not optional. There have been legislations and regulations
    in the last few years to keep the developers and owners of machine learning models
    and products accountable regarding the concerns we talked about in this chapter.
    For example, the **European Union’s** (**EU’s**) **General Data Protection Regulation**
    (**GDPR**) lists the rights of the individuals whose personal data is being processed
    to give them control over their data. It does this through the following aspects:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但问责制不是可选择的。在过去的几年中，已经出台了立法和法规，以使机器学习模型和产品的开发者和所有者对我们本章中讨论的担忧负责。例如，**欧盟的**（**EU的**）**通用数据保护条例**（**GDPR**）列出了正在处理个人数据的个人的权利，以赋予他们对其数据的控制权。它是通过以下方面实现的：
- en: The need for an individual’s clear consent to process their data
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要个人明确同意处理其数据
- en: Easier access for the data subject to their data
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据主体更容易访问其数据
- en: The rights to rectification, to erasure, and to be forgotten
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纠正权、删除权和被遗忘权
- en: The right to object, including to the use of personal data, for profiling
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括对个人数据进行画像在内的反对权
- en: The right to data portability from one service provider to another
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个服务提供商到另一个服务提供商的数据可移植权
- en: 'The EU also established a judicial remedy and compensation system (source:
    [https://www.consilium.europa.eu/en/policies/data-protection/](https://www.consilium.europa.eu/en/policies/data-protection/)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟还建立了一个司法救济和赔偿系统（来源：[https://www.consilium.europa.eu/en/policies/data-protection/](https://www.consilium.europa.eu/en/policies/data-protection/))。
- en: 'The EU later developed the **Artificial Intelligence** (**AI**) **Act** as
    the first law on AI by a major regulator (source: [https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟后来制定了**人工智能**（**AI**）**法案**，这是主要监管机构制定的第一部人工智能法律（来源：[https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/))。
- en: 'But these regulations are not limited to the EU. For example, the *White House
    Office of Science and Technology Policy* released the following blueprint for
    an AI Bill of Rights to protect the American public in the age of AI (source:
    [https://www.whitehouse.gov/ostp/ai-bill-of-rights/](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些法规不仅限于欧盟。例如，*白宫科学和技术政策办公室*发布了以下人工智能权利法案蓝图，以保护人工智能时代的美国公众（来源：[https://www.whitehouse.gov/ostp/ai-bill-of-rights/](https://www.whitehouse.gov/ostp/ai-bill-of-rights/))。
- en: 'Canada also later proposed the C-27 AI law, which “creates its baseline obligations
    through a set of primary offenses, protecting citizens from errant AI and a universal
    record-keeping obligation on the use of data" (source: [https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624](https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 加拿大后来还提出了C-27人工智能法案，该法案“通过一系列主要罪行建立其基本义务，保护公民免受错误的人工智能侵害，并要求对数据使用进行普遍记录的义务”（来源：[https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624](https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624))。
- en: The last topic we want to discuss in this chapter is governance in machine learning
    modeling. In the next section, you will learn how governance can help you and
    your organizations in developing your machine learning models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们要讨论的最后一个主题是机器学习建模中的治理。在下一节中，你将了解治理如何帮助你和你所在的组织开发机器学习模型。
- en: Data and model governance
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和模型治理
- en: 'Governance in machine learning modeling is about the use of tools and procedures
    to help you, your team, and your organization in developing reliable and responsible
    machine learning models. You shouldn’t consider it as any sort of restriction
    on how to conduct your projects but as an opportunity to reduce the risk of undetected
    mistakes. The governance in machine learning is supposed to be designed to help
    you and your organization achieve your objectives in helping humanity and business
    and avoid processes and models that could have ethical, legal, or financial consequences.
    Here are some examples of ways to establish governance systems in a team and organization:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习建模中的治理是关于使用工具和程序来帮助你、你的团队和你的组织开发可靠和负责任的机器学习模型。你不应该将其视为对如何进行项目的任何限制，而应将其视为减少未检测错误风险的机会。机器学习中的治理应该设计成帮助你和你所在的组织实现帮助人类和商业的目标，避免可能产生道德、法律或财务后果的过程和模型。以下是一些在团队和组织中建立治理体系的方法示例：
- en: '**Define guidelines and protocols**: As we want to detect issues in our models
    and improve our models in terms of both performance and responsibility, we need
    to design guidelines and protocols for simplification and consistency. We need
    to define criteria and methods for what are considered issues with models, such
    as from a security perspective, and what is considered an opportunity for model
    improvement that’s worth spending time and effort on. We need to remember that
    machine learning modeling, considering the topics we talked about in this chapter
    and the different steps of the life cycle, is not an easy task and you shouldn’t
    expect that every developer you work with will know all of them like a specialist.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义指南和协议**：鉴于我们希望检测模型中的问题，并在性能和责任方面改进模型，我们需要设计用于简化和一致性的指南和协议。我们需要定义被认为是模型问题的标准和方法，例如从安全角度考虑，以及被认为是值得花费时间和精力进行模型改进的机会。我们需要记住，考虑到本章讨论的主题和生命周期中的不同步骤，机器学习建模并非易事，你不应该期望与你合作的每个开发者都像专家一样了解所有这些。'
- en: '**Training and mentorship**: You need to look for mentorship and training programs
    and read books and articles, and then provide these opportunities for your team
    if you are a manager. But you also need to bring what you or your team learn into
    practice. Each concept in machine learning modeling has its challenges. For example,
    if you decide to use defense mechanisms against adversarial attacks, it is not
    as simple as loading a Python library and hoping nothing happens for eternity.
    So, practice what you learn and provide opportunities for your team to bring what
    they learn into practice.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培训和指导**：如果你是管理者，你需要寻找导师和培训项目，阅读书籍和文章，然后为你的团队提供这些机会。但你也需要将你或你的团队学到的知识应用到实践中。机器学习建模中的每个概念都有其挑战。例如，如果你决定使用防御机制来对抗对抗性攻击，这并不像加载一个Python库并希望永远都不会发生任何事情那样简单。所以，实践你所学的，并为你的团队提供将所学知识应用到实践中的机会。'
- en: '**Define responsibilities and accountabilities**: It is not a one-person job
    to take care of all aspects of the machine learning life cycle to build a technology
    and take care of all the responsibility topics we talked about in this chapter.
    That being said, the responsibilities and accountabilities of individuals within
    teams and organizations need to be clearly defined to reduce the redundancy of
    effort while making sure nothing gets missed.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义责任和问责制**：照顾机器学习生命周期的所有方面以构建技术和处理本章中讨论的所有责任主题，并不是一个人的工作。话虽如此，团队和组织中个人的责任和问责制需要明确界定，以减少工作冗余并确保没有遗漏任何事项。'
- en: '**Use feedback collection systems**: We need to design simple-to-use and preferably
    automated systems to collect feedback and act upon it throughout the machine learning
    life cycle. This feedback will help developers that are responsible for each step
    of a life cycle and eventually result in a better model being brought up in production.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用反馈收集系统**：我们需要设计简单易用且最好是自动化的系统来收集反馈并在机器学习生命周期中采取行动。这种反馈将帮助负责生命周期每个步骤的开发者，并最终导致在生产中推出更好的模型。'
- en: '**Use a quality control process**: We need quantitative and predefined methods
    and protocols to assess the quality of machine learning models after training
    or in production or to assess processed data coming out of each stage of a machine
    learning life cycle. Having the quality control processes defined and documented
    help us in attaining a scalable system for a faster and more consistent quality
    assessment. However, these processes can be revised and adapted according to new
    criteria and the risks associated with data and the corresponding machine learning
    models.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用质量控制流程**：我们需要定量和预定义的方法和协议来评估训练后或生产中的机器学习模型的质量，或者评估机器学习生命周期每个阶段输出的处理数据。拥有定义和记录的质量控制流程有助于我们实现可扩展的系统，以便更快、更一致地进行质量评估。然而，这些流程可以根据新的标准和与数据和相应的机器学习模型相关的风险进行修订和调整。'
- en: Now that we understand the importance of responsible machine learning modeling
    and reviewed important factors and techniques to achieve it, we are ready to move
    on to the next part of this book and get into more technical details concerning
    developing reliable, high-performance, and fair machine learning models and technologies.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了负责任机器学习建模的重要性，并回顾了实现它的关键因素和技术，我们准备进入本书的下一部分，并深入了解开发可靠、高性能和公平的机器学习模型和技术的技术细节。
- en: Summary
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we talked about the different elements of responsible AI, such
    as data privacy, security in machine learning systems, the different types of
    attacks and designing defense systems against them, transparency and accountability
    in the machine learning era, and how to use data and model governance to develop
    reliable and responsible models in practice.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了负责任人工智能的不同要素，例如数据隐私、机器学习系统的安全性、不同类型的攻击以及设计针对它们的防御系统、机器学习时代的透明度和问责制，以及如何在实际中利用数据和管理模型来开发可靠和负责任的模式。
- en: This chapter and the two previous chapters, which make up *Part 1* of this book,
    introduced important concepts in machine learning modeling and model debugging.
    *Part 2* includes topics on *h*ow to improve machine learning models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和前两章，构成了本书的*第一部分*，介绍了机器学习建模和模型调试的重要概念。*第二部分*包括如何改进机器学习模型的主题。
- en: In the next chapter, you will learn about methods for detecting issues in machine
    learning models and opportunities for improving the performance and generalizability
    of such models. We will cover statistical, mathematical, and visualization techniques
    for model debugging with real-life examples to help you quickly start implementing
    these methods so that you can investigate and improve your models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解检测机器学习模型问题以及提高此类模型性能和泛化性的方法。我们将涵盖使用真实生活示例进行模型调试的统计、数学和可视化技术，以帮助您快速实施这些方法，以便您可以调查和改进您的模型。
- en: Questions
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can you explain two types of data biases?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释两种类型的数据偏差吗？
- en: What is the difference between white-box and black-box adversarial attacks?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 白盒攻击和黑盒攻击有什么区别？
- en: Can you explain how data and algorithm encryption can help in securing the privacy
    and security of your systems?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释一下数据与算法加密如何帮助保护你系统的隐私和安全吗？
- en: Can you explain the difference between differential privacy and federated machine
    learning?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能解释差分隐私和联邦机器学习之间的区别吗？
- en: How does transparency help you in increasing the number of users of your machine
    learning models?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 透明度如何帮助你在增加你的机器学习模型用户数量方面？
- en: References
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Zou, James, and Londa Schiebinger. *AI can be sexist and racist – it’s time
    to make it fair*. (2018): 324-326.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou, James, 和 Londa Schiebinger. *AI 可能是性别歧视和种族歧视的 – 是时候让它变得公平了*。（2018）：324-326.
- en: 'Nushi, Besmira, Ece Kamar, and Eric Horvitz. *Towards accountable ai: Hybrid
    human-machine analyses for characterizing system failure*. Proceedings of the
    AAAI Conference on Human Computation and Crowdsourcing. Vol. 6\. 2018.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nushi, Besmira, Ece Kamar, 和 Eric Horvitz. *迈向可问责的 AI：用于表征系统失败的混合人机分析.* AAAI
    人机计算与众包会议论文集。第 6 卷。2018.
- en: 'Busuioc, Madalina. *Accountable artificial intelligence: Holding algorithms
    to account.* Public Administration Review 81.5 (2021): 825-836.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Busuioc, Madalina. *可问责的人工智能：对算法负责.* 公共行政评论 81.5 (2021): 825-836.'
- en: 'Unceta, Irene, Jordi Nin, and Oriol Pujol. *Risk mitigation in algorithmic
    accountability: The role of machine learning copies.* Plos one 15.11 (2020): e0241286.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Unceta, Irene, Jordi Nin, 和 Oriol Pujol. *算法问责制中的风险缓解：机器学习副本在其中的作用.* Plos one
    15.11 (2020): e0241286.'
- en: 'Leonelli, Sabina. *Data governance is key to interpretation: Reconceptualizing
    data in data science.* Harvard Data Science Review 1.1 (2019): 10-1162.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Leonelli, Sabina. *数据治理是关键：重新概念化数据在数据科学中的角色.* 哈佛数据科学评论 1.1 (2019): 10-1162.'
- en: 'Sridhar, Vinay, et al. *Model governance: Reducing the anarchy of production
    {ML}.* 2018 USENIX Annual Technical Conference (USENIX ATC 18). 2018.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sridhar, Vinay, 等人. *模型治理：减少生产 {ML} 的无政府状态.* 2018 USENIX 年度技术会议 (USENIX ATC
    18)。2018.
- en: 'Stilgoe, Jack. *Machine learning, social learning, and the governance of self-driving
    cars.* Social studies of science 48.1 (2018): 25-56.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stilgoe, Jack. *机器学习、社会学习和自动驾驶汽车的治理.* 科学研究 48.1 (2018): 25-56.'
- en: 'Reddy, Sandeep, et al. *A governance model for the application of AI in health
    care.* Journal of the American Medical Informatics Association 27.3 (2020): 491-497.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reddy, Sandeep, 等人. *AI 在医疗保健中的应用治理模型.* 美国医学信息学协会杂志 27.3 (2020): 491-497.'
- en: 'Gervasi, Stephanie S., et al. *The Potential For Bias In Machine Learning And
    Opportunities For Health Insurers To Address It: Article examines the potential
    for bias in machine learning and opportunities for health insurers to address
    it.* Health Affairs 41.2 (2022): 212-218.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gervasi, Stephanie S., 等人. *机器学习中的潜在偏见及其对健康保险公司的应对机会：文章探讨了机器学习中的潜在偏见以及健康保险公司应对它的机会.*
    健康事务 41.2 (2022): 212-218.'
- en: Gianfrancesco, M. A., Tamang, S., Yazdany, J., & Schmajuk, G. (2018). *Potential
    Biases in Machine Learning Algorithms Using Electronic Health Record Data*. JAMA
    internal medicine, 178(11), 1544.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gianfrancesco, M. A., Tamang, S., Yazdany, J., & Schmajuk, G. (2018). *使用电子健康记录数据的机器学习算法中的潜在偏见.*
    JAMA 内科学杂志，178(11)，1544.
- en: 'Finlayson, Samuel G., et al. *Adversarial attacks on medical machine learning.*
    Science 363.6433 (2019): 1287-1289.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Finlayson, Samuel G., 等人. *对抗医疗机器学习攻击.* 科学 363.6433 (2019): 1287-1289.'
- en: Eykholt, Kevin, et al. *Robust physical-world attacks on deep learning visual
    classification.* Proceedings of the IEEE conference on computer vision and pattern
    recognition. 2018.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eykholt, Kevin, 等人. *对深度学习视觉分类的鲁棒物理世界攻击.* IEEE 计算机视觉与模式识别会议论文集。2018.
- en: 'Biggio, Battista, and Fabio Roli. *Wild patterns: Ten years after the rise
    of adversarial machine learning.* Pattern Recognition 84 (2018): 317-331.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biggio, Battista, 和 Fabio Roli. *野模式：对抗机器学习兴起十年后.* 模式识别 84 (2018): 317-331.'
- en: 'Kaissis, Georgios A., et al. *Secure, privacy-preserving and federated machine
    learning in medical imaging.* Nature Machine Intelligence 2.6 (2020): 305-311.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaissis, Georgios A., 等人. *医学成像中的安全、隐私保护和联邦机器学习.* 自然机器智能 2.6 (2020): 305-311.'
- en: 'Acar, Abbas, et al. *A survey on homomorphic encryption schemes: Theory and
    implementation.* ACM Computing Surveys (Csur) 51.4 (2018): 1-35.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Acar, Abbas, 等人. *同态加密方案综述：理论与实现.* ACM 计算机调查 (Csur) 51.4 (2018): 1-35.'
- en: 'Dwork, Cynthia. *Differential privacy: A survey of results.* International
    conference on theory and applications of models of computation. Springer, Berlin,
    Heidelberg, 2008.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwork, Cynthia. *差分隐私：结果综述.* 国际计算模型理论与应用会议。Springer，柏林，海德堡，2008.
- en: Abadi, Martin, et al. *Deep learning with differential privacy.* Proceedings
    of the 2016 ACM SIGSAC conference on computer and communications security. 2016.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi, Martin, 等人. *具有差分隐私的深度学习.* 2016 ACM SIGSAC 计算机与通信安全会议论文集。2016.
- en: 'Yang, Qiang, et al. *Federated machine learning: Concept and applications.*
    ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (2019): 1-19.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨强，等。*联邦机器学习：概念与应用*。ACM智能系统与技术交易（TIST）10.2（2019）：1-19。
- en: Part 2:Improving Machine Learning Models
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：提高机器学习模型
- en: This part will help us transition into the critical aspects of refining and
    understanding machine learning models. We will start with a deep dive into detecting
    performance and efficiency bottlenecks in models, followed by actionable strategies
    to enhance their performance. The narrative then shifts to the subject of interpretability
    and explainability, elucidating the importance of not just building models that
    work, but ones we can understand and trust. We will conclude this part by presenting
    the methods to reduce bias, emphasizing the imperative of fairness in machine
    learning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分将帮助我们过渡到精炼和理解机器学习模型的关键方面。我们将从深入探讨检测模型中的性能和效率瓶颈开始，接着提出可操作的战略来提升它们的性能。随后，叙述转向可解释性和可说明性的主题，阐明不仅构建出能工作的模型，而且我们能够理解和信任的模型的重要性。我们将通过介绍减少偏差的方法来结束这一部分，强调机器学习中公平性的必要性。
- en: 'This part has the following chapters:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 4*](B16369_04.xhtml#_idTextAnchor159), *Detecting Performance and
    Efficiency Issues in Machine Learning Models*'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B16369_04.xhtml#_idTextAnchor159), *检测机器学习模型中的性能和效率问题*'
- en: '[*Chapter 5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance
    of Machine Learning Models*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B16369_05.xhtml#_idTextAnchor183), *提高机器学习模型的性能*'
- en: '[*Chapter 6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability
    in Machine Learning Modeling*'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第六章*](B16369_06.xhtml#_idTextAnchor201), *机器学习建模中的可解释性和可说明性*'
- en: '[*Chapter 7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing Bias and Achieving
    Fairness*'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第七章*](B16369_07.xhtml#_idTextAnchor218), *减少偏差和实现公平性*'
