- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Debugging toward Responsible AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developing successful machine learning models is not solely about achieving
    high performance. We all get excited when we improve the performance of our models.
    We feel responsible for developing a high-performance model. But we are also responsible
    for building fair and secure models. These goals, which are beyond performance
    improvement, are among the objectives of *responsible machine learning*, or more
    broadly, *responsible artificial intelligence*. As part of responsible machine
    learning modeling, we should consider transparency and accountability when training
    and making predictions for our models and consider governance systems for our
    data and modeling processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Impartial modeling fairness in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and privacy in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency in machine learning modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accountable and open to inspection modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data and model governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the need and different concerns
    and challenges in responsible machine learning modeling. You will have also learned
    about different techniques that can help us in responsible modeling and ensuring
    privacy and security while developing machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to understand the components of machine learning life cycles before
    reading this chapter as this will help you better understand the concepts and
    be able to use them in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Impartial modeling fairness in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning models make mistakes. But when a mistake happens, they could
    have biases, such as in the COMPAS example provided in [*Chapter 1*](B16369_01.xhtml#_idTextAnchor015),
    *Beyond Code Debugging*. We need to investigate our models for the existence of
    such biases and revise them to eliminate these biases. Let’s go through more examples
    to clarify the importance of investigating our data and models for the existence
    of such biases.
  prefs: []
  type: TYPE_NORMAL
- en: Recruiting is a challenging process for every company as they must identify
    the most suitable candidates to interview from hundreds of applicants who have
    submitted resumes and cover letters. In 2014, Amazon started to develop a hiring
    tool using machine learning to screen job applicants and select the best ones
    to pursue based on the information provided in their resumes. This was a text
    processing model that used the text in resumes to identify the key information
    and select the top candidates. But eventually, Amazon decided to abandon the system
    as the model was biased in selecting men over women in the hiring process. The
    main reason behind this bias was the data, which contained mainly resumes of men,
    that was fed into the machine learning model. The model learned how to identify
    language and key information in men’s resumes, but it was not effective when it
    came to women’s resumes. Hence, the model couldn’t rank candidates for a job application
    while remaining unbiased in terms of gender.
  prefs: []
  type: TYPE_NORMAL
- en: Some machine learning models are designed to predict the likelihood of hospitalization.
    These models can help reduce individual and population healthcare costs. However,
    such beneficial models can have their own biases. For example, hospitalization
    requires access to and the use of health care services, which is influenced by
    differences in socioeconomic conditions. This means that the datasets that are
    available for building models to predict the likelihood of hospitalization would
    have more positive data on people of high socioeconomic conditions compared to
    poor families. This inequality could cause biases in decision-making by machine
    learning models for hospitalization, which results in limiting the access of low
    socioeconomic people to hospitalization even further.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of biases in machine learning applications in the healthcare
    setting has been in genetic studies. These studies have been criticized for biases
    due to them not properly accounting for diversity in populations, which could
    result in misdiagnosis in the studied diseases.
  prefs: []
  type: TYPE_NORMAL
- en: Two main sources of bias include data, which either originated from the data
    source or was introduced in data processing before model training, and algorithmic
    bias. Let’s review both.
  prefs: []
  type: TYPE_NORMAL
- en: Data bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might have heard of the “garbage in, garbage out” concept in computer science.
    This concept is about the fact that if nonsense data gets into a computer tool,
    such as a machine learning model, the output will be nonsense. The data that gets
    fed to help train machine learning algorithms could have all sorts of issues that
    eventually result in biases, as mentioned previously. For example, the data could
    under-represent a group, similar to women in the hiring data fed into the Amazon
    model. Recall that having this biased data shouldn’t stop us from building models,
    but we have to design our life cycle components, such as data selection and wrangling
    or model training, while considering these biases and testing our models for bias
    detection before bringing a model into production. The following are some of the
    sources of data biases.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data that is collected could contain biases, such as gender bias, as in the
    Amazon applicant sorting example, race bias, as in COMPAS, socioeconomic biases,
    as in hospitalization examples, or other kinds of biases. As another example,
    imagine that a machine learning model for autonomous driving is trained only on
    images of streets, cars, people, and traffic signs taken in the daytime. The model
    will be biased and not reliable in the nighttime. This kind of bias can be removed
    after providing feedback from data exploration or data wrangling steps to data
    collection and selection in the machine learning life cycle. But if it is not
    revised before a model gets trained, tested, and deployed, then the feedback needs
    to be immediately provided from model monitoring, when biases in predictions get
    detected, and used in the life cycle to provide less biased data for modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another source of data bias could be in the process of sampling data points
    or sampling the population in the *data collection* stage of the life cycle. For
    example, when sampling students to fill in a survey, our sampling process could
    be biased toward girls or boys, rich or poor student families, or high versus
    low-grade students. These kinds of biases cannot be easily fixed by adding samples
    of other groups. Sampling processes for filling surveys or designing clinical
    trials for new drug testing on patients are among examples of data collection
    processes where adding data to them is not necessarily allowed. Some of these
    data collection processes need a prior definition of the population that cannot
    be changed in the middle of the process. In such cases, different kinds of possible
    biases need to be determined and considered when designing the data sampling process.
  prefs: []
  type: TYPE_NORMAL
- en: Exclusion bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the process of data cleaning and wrangling, before you start training and
    testing a machine learning model, features could be removed because of statistical
    reasoning, such as low information content or variance across data points or not
    having a desired characteristic. These feature removals can sometimes cause biases
    in our modeling. Although not excluded, some of the features could also cause
    biases in the eventual machine learning model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Measurement or labeling bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Measurement and annotation biases could be caused by issues or differences in
    terms of technologies, experts, or non-expert data annotators, who generated or
    annotated the data that’s used for model training, testing, and prediction in
    production. For example, if one camera type is used to collect the data to train
    a machine learning model for image classification, predictions in production might
    have lower reliability if images in production will be captured by another camera
    that generates images with a different quality.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithmic bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There could be systematic errors associated with the algorithm and training
    process of a machine learning model. For example, instead of the data being biased
    to a specific race or skin color in face recognition tools, the algorithm might
    result in biased predictions regarding a group with a specific skin color or race.
    Keeping the machine learning life cycle in mind, in the modular way it was presented
    in [*Chapter 2*](B16369_02.xhtml#_idTextAnchor076), *Machine Learning Life Cycle*,
    will help you identify the issues in a stage such as model monitoring. Then, the
    feedback can be provided for the relevant step, such as data collection or data
    wrangling, to eliminate the identified biases. There are methodologies to detect
    biases and resolve them that we will go through in future chapters. For example,
    we can use machine learning explainability techniques to identify the contributions
    of features, or their combinations, that could cause biases in predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to eliminating biases in our models, we also need to take into account
    security and privacy concerns while going through a machine learning life cycle,
    which is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Security and privacy in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Security is a concern for all businesses with physical or virtual products
    and services. 60 years ago, each bank had to ensure the security of physical assets,
    such as cash and important documents, in its branches. But after moving to the
    digital world, they had to build new security systems to make sure that the data
    of their clients and their money and assets, which can now be transferred and
    changed digitally, were secure. Machine learning products and technologies are
    no exception and need to have proper security systems. Security concerns in machine
    learning settings could be related to the security of the data, the models themselves,
    or model predictions. In this section, we will introduce three important subjects
    regarding security and privacy in machine learning modeling: **data privacy**,
    **data poisoning**, and **adversarial attacks**.'
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The privacy of the user data in production or the data you have stored and
    used for model training and testing is an important aspect of security system
    design for machine learning technologies. The data needs to be secure for many
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If the data includes confidential information of users, people, or organizations
    the data has been received from
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is licensed from a commercial data provider under legal contracts
    and should not become accessible through your services or technologies with others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the data is generated for you and considered one of the assets of your team
    and organization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these cases, you need to make sure the data is secure. You can use security
    systems for your databases and datasets. You can also design encryption processes
    on top of this if part of the data needs to be transferred digitally between two
    servers, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some attacks are designed to access private and confidential data in your datasets
    and databases, such as patient information in hospitals, customer data in banking
    systems, or the personal information of employees of governmental organizations.
    Three of these attacks are *data reconstruction attacks*, *identity recognition
    attacks*, and *individual tracing attacks*, all of which can be done through **internet
    protocol** (**IP**) tracking, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Data poisoning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Change in the meaning and quality of data is another concern in data security.
    Data could be poisoned and the resulting changes in prediction could have drastic
    consequences financially, legally, and ethically for individuals, teams, and organizations.
    Imagine you designed a machine learning model with your friends for stock market
    prediction and your model uses news feeds and stock prices in previous days as
    input features. This data gets extracted from different resources such as Yahoo
    Finance and different sources of news. If your database gets poisoned, by changing
    the values of some of the features or changes in the collected data, such as the
    price history of a piece of stock, you might go through serious financial losses
    as your model might suggest that you buy stocks that will lose their value by
    more than 50% in a week rather than going up. This is an example that has financial
    consequences. However, data poisoning could have life-threatening consequences
    if, for example, it happens in healthcare or military systems.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, you can fool machine learning models by making very simple changes,
    such as adding small amounts of noise or perturbation to feature values. This
    is the concept behind generating adversarial examples and adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a medical AI system, an image of a benign (that is, not harmful)
    mole could be diagnosed as malignant (that is, harmful and dangerous in general
    terms) by adding adversarial noise in the image that would not be recognizable
    by the human eye or simply rotating the image. Synonymous text substitution such
    as changing “*The patient has a history of back pain and chronic alcohol abuse
    and more recently has been seen in several...*” to “*The patient has a history
    of lumbago and chronic alcohol dependence and more recently has been seen in several...*”
    could change the diagnosis from benign to malignant (Finlayson et al., 2019).
    In other applications of image classification, such as in self-driving cars, simple
    black and white stickers could sometimes fool models into classifying images of
    stop signs or frames of videos of stop signs (Eykholt et al., 2018).
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial examples could mislead your system in inference or training and
    validating whether they get injected into your modeling data and poison it. There
    are three important aspects of knowing your adversary that can help you in protecting
    your systems – that is, the attacker’s goal, knowledge, and capability (*Table
    3.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type of Knowledge about** **the Adversary** | **Aspects of Different Types**
    **of Knowledge** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| The attacker’s goal | Security violation | The attacker tries to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Evade detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compromise system functionalities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get access to private information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Attack specificity | Targeting specific or random data points to generate
    wrong results |'
  prefs: []
  type: TYPE_TB
- en: '| The attacker’s knowledge | Perfect-knowledge white-box attacks | The attacker
    knows everything about the system |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-knowledge black-box attacks | The attacker doesn’t have any knowledge
    of the system itself but collects information through predictions of the model
    in production |'
  prefs: []
  type: TYPE_TB
- en: '| Limited-knowledge gray-box attacks | The attacker has limited knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| The attacker’s capability | Attack influence | **Causative**: Attackers can
    poison train data and manipulate test data**Exploratory**: The attacker can manipulate
    test data only |'
  prefs: []
  type: TYPE_TB
- en: '| Data manipulation constraints | Constraints on data manipulation to eliminate
    data manipulation or make it challenging |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Types of knowledge about adversaries (Biggio et al., 2018)
  prefs: []
  type: TYPE_NORMAL
- en: Output integrity attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This type of attack usually doesn’t affect data processing, model training and
    testing, or even prediction in production. It comes between the output of your
    model and what will be shown to the user. Based on this definition, this attack
    is not specific to machine learning settings. But in our machine learning systems,
    understanding this type of attack solely based on the outputs shown to the users
    might be challenging. For example, if the prediction probabilities or labels of
    your model in classification settings get changed once in a while, the results
    that are shown to the users will be wrong, but the user might accept them if they
    believe in our systems. It is our responsibility to make sure such kinds of attacks
    don’t challenge the integrity of the results of our model in production.
  prefs: []
  type: TYPE_NORMAL
- en: System manipulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your machine learning system could be manipulated by intentionally designed
    synthetic data, which either does not exist or might not have existed in the model
    training and test sets. This manipulation in the prediction level could not only
    have consequences such as time wasted for investigating wrong predictions of the
    model, but it could also poison your models and change the performance of your
    model in testing and production if the data enters your training, evaluation,
    or test data.
  prefs: []
  type: TYPE_NORMAL
- en: Secure and private machine learning techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some techniques help us in developing secure and privacy-preserving processes
    and tools for data storage, transfer, and use in machine learning modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anonymization**: This technique focuses on removing information that helps
    in identifying individual data points, such as individual patients, within a healthcare
    dataset. This information could be very specific, such as health card numbers,
    which could have different names in different countries, or more general information,
    such as gender and age.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pseudonymization**: Instead of removing information, as in anonymization,
    the personally identifiable data could be replaced with synthetic substitutes
    as part of pseudonymization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data and algorithm encryption**: The encryption process transforms the information
    – be it data or an algorithm – into a new (encrypted) form. The encrypted data
    can be decrypted (so that it becomes human-readable or machine understandable)
    if the individual has access to the encryption key (that is, a password-style
    key necessary for the decryption process). In this way, getting access to the
    data and algorithm without the encryption key will be almost impossible or very
    difficult. We will review encryption techniques such as **Advanced Encryption
    Standard** (**AES**) in [*Chapter 16*](B16369_16.xhtml#_idTextAnchor429), *Security
    and Privacy in* *Machine Learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homomorphic encryption**: This is an encryption technique that eliminates
    the need for data decryption at the time of prediction by a machine learning model.
    The model uses the encrypted data for predictions, so the data can be kept encrypted
    through the whole data transfer and usage process in a machine learning pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Federated machine learning**: Federated machine learning relies on the idea
    of decentralizing learning, data analysis, and inference, thus allowing the user
    data to be kept within individual devices or local databases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differential privacy**: Differential privacy tries to ensure that the removal
    or addition of individual data points does not affect the outcome of modeling.
    It attempts to learn from patterns within groups of data points. For example,
    by adding random noise from a normal distribution, it tries to make features of
    individual data points obscure. The effect of noise in learning could be eliminated
    based on the law of large numbers ([https://www.britannica.com/science/law-of-large-numbers](https://www.britannica.com/science/law-of-large-numbers))
    if a large number of data points is accessible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These techniques are not applicable and useful in all settings. For example,
    federated machine learning will not be helpful when you have an internal database
    and need to just be sure about its security. Differential privacy for small data
    sources could also be unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption and decryption processes
  prefs: []
  type: TYPE_NORMAL
- en: Encryption is the process to transform readable data into a human-unreadable
    form. On the other hand, decryption is the process of transforming encrypted data
    back into its original readable format. You can find more information on this
    topic at [https://docs.oracle.com/](https://docs.oracle.com/) and [https://learn.microsoft.com/en-ca/](https://learn.microsoft.com/en-ca/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we talked about privacy and security in machine learning modeling.
    Even if we build a secure system with minimum privacy concerns, we need to consider
    other factors to build trust in our models. Transparency is one of those factors.
    We will introduce this next.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency in machine learning modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transparency helps users of your model trust it by helping them understand
    how it works and how it was built. It also helps you, your team, your collaborators,
    and your organization to collect feedback on different components of your machine
    learning life cycle. It is worth understanding the transparency requirements in
    different stages of a life cycle and the challenges in achieving them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: Transparency in data collection needs to answer two major
    questions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What data are you collecting?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What do you want to use that data for?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when users click on the agreement button for data usage when registering
    for a mobile phone app, they are giving consent for the information they provide
    in the app to be used. But the agreement needs to be clear on the part of the
    user data that is going to be used and for what purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data selection and exploration**: In these stages of the life cycle, your
    process of data selection and how you achieved your exploratory results need to
    be clear. This helps you collect feedback from other collaborators and colleagues
    on your project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data wrangling and modeling data preparation**: Before this step, data is
    almost like the so-called raw data, without any changes in feature definition
    or data being split into train and test sets. If you design these components of
    the life cycle as a black box and it’s not transparent, you might lose both trust
    and the opportunity for feedback from other experts with future access to your
    data and results. For example, imagine you are supposed to not use the genetic
    information of patients in hospitals, and you provide features called Feature1,
    Feature2, and so on after these steps in the life cycle. Without explaining how
    those features were generated and using what original features, people cannot
    be sure if you used patients’ genetic information or not. You also need to be
    transparent about how you designed your testing strategy and separated your training
    data from validation and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training and evaluation**: Transparency in model training helps in
    understanding the decision-making and pattern recognition aspects of models when
    learning from data. Transparency in both training and evaluation builds trust
    for direct users, developers, and auditors to better assess these processes. It
    is true that, for example, >99% of Amazon Prime users never want to understand
    the machine learning modeling behind the scenes. However, our models are sometimes
    used directly by users, such as doctors in hospitals for diagnosis or employees
    of manufacturing facilities. *Explainability*, which we will discuss in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine Learning Modeling*, is an important concept in machine learning modeling
    that has a close relationship with transparency and helps users better understand
    how the model works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing**: Transparency in decision-making in the testing stage of the life
    cycle that helps in better understanding the process of selecting or rejecting
    a model, or planning for improving and debugging the related code, data processing,
    or model training and evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment and monitoring**: In addition to getting feedback from other
    developers and experts in your organization and team, or public experts, you need
    to make the effect of the model in production and what data is collected for monitoring
    clear for users and auditors within or outside of your organization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite the need for transparency, an increase in transparency might make your
    system vulnerable from the perspectives of security and privacy. So, you cannot
    aim for maximum transparency without keeping privacy and security in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness and bias removal, privacy, and transparency are all factors in responsible
    machine learning modeling. But we need to remember that the accountability for
    achieving fair and secure models and systems is on us.
  prefs: []
  type: TYPE_NORMAL
- en: Accountable and open to inspection modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The models we develop as employees of different businesses or incorporations,
    research institutes or universities, or as freelancers could eventually get into
    production in different applications, such as healthcare, finance, manufacturing,
    marketing, retail, transportation, media, and entertainment. Our models could
    use patient data to predict whether they will get cancer or diabetes or whether
    they will respond to therapy. Alternatively, they could use the financial history
    and other information on the clients of a bank to assess their eligibility for
    loans. Another example is that our model can use the history of people’s purchases
    to recommend new products to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed in this chapter, we have to take care of the privacy of data
    and models, provide a fair and impartial model, and make our models as transparent
    as possible. But we have to remember that we are accountable for managing all
    these concerns in developing reliable and responsible machine learning models.
    I asked ChatGPT, the famous machine learning chatbot developed by OpenAI, “*Are
    you accountable for your responses?*” It responded as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“As a machine learning model, I do not have the ability to be held accountable
    for my responses in the same way that a human would. I am designed to generate
    responses based on the input I receive, but I do not have personal beliefs, intentions,
    and motivations. I am not capable of making decisions or choices on my own. My
    responses are a reflection of the training data and algorithms that were used
    to create me, and my creators are responsible for ensuring that I am functioning*
    *as intended.”*'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There are quotes in the next few paragraphs of this chapter from legal sources
    that haven’t been changed to make sure the legal content is not rephrased inappropriately.
    However, this is not a legal book and no content of this book should be considered
    as legal advice.
  prefs: []
  type: TYPE_NORMAL
- en: 'But accountability is not optional. There have been legislations and regulations
    in the last few years to keep the developers and owners of machine learning models
    and products accountable regarding the concerns we talked about in this chapter.
    For example, the **European Union’s** (**EU’s**) **General Data Protection Regulation**
    (**GDPR**) lists the rights of the individuals whose personal data is being processed
    to give them control over their data. It does this through the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for an individual’s clear consent to process their data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easier access for the data subject to their data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rights to rectification, to erasure, and to be forgotten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right to object, including to the use of personal data, for profiling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right to data portability from one service provider to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The EU also established a judicial remedy and compensation system (source:
    [https://www.consilium.europa.eu/en/policies/data-protection/](https://www.consilium.europa.eu/en/policies/data-protection/)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The EU later developed the **Artificial Intelligence** (**AI**) **Act** as
    the first law on AI by a major regulator (source: [https://artificialintelligenceact.eu/](https://artificialintelligenceact.eu/)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'But these regulations are not limited to the EU. For example, the *White House
    Office of Science and Technology Policy* released the following blueprint for
    an AI Bill of Rights to protect the American public in the age of AI (source:
    [https://www.whitehouse.gov/ostp/ai-bill-of-rights/](https://www.whitehouse.gov/ostp/ai-bill-of-rights/)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Canada also later proposed the C-27 AI law, which “creates its baseline obligations
    through a set of primary offenses, protecting citizens from errant AI and a universal
    record-keeping obligation on the use of data" (source: [https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624](https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624)).'
  prefs: []
  type: TYPE_NORMAL
- en: The last topic we want to discuss in this chapter is governance in machine learning
    modeling. In the next section, you will learn how governance can help you and
    your organizations in developing your machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Data and model governance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Governance in machine learning modeling is about the use of tools and procedures
    to help you, your team, and your organization in developing reliable and responsible
    machine learning models. You shouldn’t consider it as any sort of restriction
    on how to conduct your projects but as an opportunity to reduce the risk of undetected
    mistakes. The governance in machine learning is supposed to be designed to help
    you and your organization achieve your objectives in helping humanity and business
    and avoid processes and models that could have ethical, legal, or financial consequences.
    Here are some examples of ways to establish governance systems in a team and organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define guidelines and protocols**: As we want to detect issues in our models
    and improve our models in terms of both performance and responsibility, we need
    to design guidelines and protocols for simplification and consistency. We need
    to define criteria and methods for what are considered issues with models, such
    as from a security perspective, and what is considered an opportunity for model
    improvement that’s worth spending time and effort on. We need to remember that
    machine learning modeling, considering the topics we talked about in this chapter
    and the different steps of the life cycle, is not an easy task and you shouldn’t
    expect that every developer you work with will know all of them like a specialist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and mentorship**: You need to look for mentorship and training programs
    and read books and articles, and then provide these opportunities for your team
    if you are a manager. But you also need to bring what you or your team learn into
    practice. Each concept in machine learning modeling has its challenges. For example,
    if you decide to use defense mechanisms against adversarial attacks, it is not
    as simple as loading a Python library and hoping nothing happens for eternity.
    So, practice what you learn and provide opportunities for your team to bring what
    they learn into practice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define responsibilities and accountabilities**: It is not a one-person job
    to take care of all aspects of the machine learning life cycle to build a technology
    and take care of all the responsibility topics we talked about in this chapter.
    That being said, the responsibilities and accountabilities of individuals within
    teams and organizations need to be clearly defined to reduce the redundancy of
    effort while making sure nothing gets missed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use feedback collection systems**: We need to design simple-to-use and preferably
    automated systems to collect feedback and act upon it throughout the machine learning
    life cycle. This feedback will help developers that are responsible for each step
    of a life cycle and eventually result in a better model being brought up in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use a quality control process**: We need quantitative and predefined methods
    and protocols to assess the quality of machine learning models after training
    or in production or to assess processed data coming out of each stage of a machine
    learning life cycle. Having the quality control processes defined and documented
    help us in attaining a scalable system for a faster and more consistent quality
    assessment. However, these processes can be revised and adapted according to new
    criteria and the risks associated with data and the corresponding machine learning
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the importance of responsible machine learning modeling
    and reviewed important factors and techniques to achieve it, we are ready to move
    on to the next part of this book and get into more technical details concerning
    developing reliable, high-performance, and fair machine learning models and technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about the different elements of responsible AI, such
    as data privacy, security in machine learning systems, the different types of
    attacks and designing defense systems against them, transparency and accountability
    in the machine learning era, and how to use data and model governance to develop
    reliable and responsible models in practice.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter and the two previous chapters, which make up *Part 1* of this book,
    introduced important concepts in machine learning modeling and model debugging.
    *Part 2* includes topics on *h*ow to improve machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about methods for detecting issues in machine
    learning models and opportunities for improving the performance and generalizability
    of such models. We will cover statistical, mathematical, and visualization techniques
    for model debugging with real-life examples to help you quickly start implementing
    these methods so that you can investigate and improve your models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can you explain two types of data biases?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between white-box and black-box adversarial attacks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you explain how data and algorithm encryption can help in securing the privacy
    and security of your systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you explain the difference between differential privacy and federated machine
    learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does transparency help you in increasing the number of users of your machine
    learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zou, James, and Londa Schiebinger. *AI can be sexist and racist – it’s time
    to make it fair*. (2018): 324-326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nushi, Besmira, Ece Kamar, and Eric Horvitz. *Towards accountable ai: Hybrid
    human-machine analyses for characterizing system failure*. Proceedings of the
    AAAI Conference on Human Computation and Crowdsourcing. Vol. 6\. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Busuioc, Madalina. *Accountable artificial intelligence: Holding algorithms
    to account.* Public Administration Review 81.5 (2021): 825-836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unceta, Irene, Jordi Nin, and Oriol Pujol. *Risk mitigation in algorithmic
    accountability: The role of machine learning copies.* Plos one 15.11 (2020): e0241286.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leonelli, Sabina. *Data governance is key to interpretation: Reconceptualizing
    data in data science.* Harvard Data Science Review 1.1 (2019): 10-1162.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sridhar, Vinay, et al. *Model governance: Reducing the anarchy of production
    {ML}.* 2018 USENIX Annual Technical Conference (USENIX ATC 18). 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stilgoe, Jack. *Machine learning, social learning, and the governance of self-driving
    cars.* Social studies of science 48.1 (2018): 25-56.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reddy, Sandeep, et al. *A governance model for the application of AI in health
    care.* Journal of the American Medical Informatics Association 27.3 (2020): 491-497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gervasi, Stephanie S., et al. *The Potential For Bias In Machine Learning And
    Opportunities For Health Insurers To Address It: Article examines the potential
    for bias in machine learning and opportunities for health insurers to address
    it.* Health Affairs 41.2 (2022): 212-218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gianfrancesco, M. A., Tamang, S., Yazdany, J., & Schmajuk, G. (2018). *Potential
    Biases in Machine Learning Algorithms Using Electronic Health Record Data*. JAMA
    internal medicine, 178(11), 1544.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finlayson, Samuel G., et al. *Adversarial attacks on medical machine learning.*
    Science 363.6433 (2019): 1287-1289.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eykholt, Kevin, et al. *Robust physical-world attacks on deep learning visual
    classification.* Proceedings of the IEEE conference on computer vision and pattern
    recognition. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biggio, Battista, and Fabio Roli. *Wild patterns: Ten years after the rise
    of adversarial machine learning.* Pattern Recognition 84 (2018): 317-331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaissis, Georgios A., et al. *Secure, privacy-preserving and federated machine
    learning in medical imaging.* Nature Machine Intelligence 2.6 (2020): 305-311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Acar, Abbas, et al. *A survey on homomorphic encryption schemes: Theory and
    implementation.* ACM Computing Surveys (Csur) 51.4 (2018): 1-35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dwork, Cynthia. *Differential privacy: A survey of results.* International
    conference on theory and applications of models of computation. Springer, Berlin,
    Heidelberg, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abadi, Martin, et al. *Deep learning with differential privacy.* Proceedings
    of the 2016 ACM SIGSAC conference on computer and communications security. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang, Qiang, et al. *Federated machine learning: Concept and applications.*
    ACM Transactions on Intelligent Systems and Technology (TIST) 10.2 (2019): 1-19.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 2:Improving Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will help us transition into the critical aspects of refining and
    understanding machine learning models. We will start with a deep dive into detecting
    performance and efficiency bottlenecks in models, followed by actionable strategies
    to enhance their performance. The narrative then shifts to the subject of interpretability
    and explainability, elucidating the importance of not just building models that
    work, but ones we can understand and trust. We will conclude this part by presenting
    the methods to reduce bias, emphasizing the imperative of fairness in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B16369_04.xhtml#_idTextAnchor159), *Detecting Performance and
    Efficiency Issues in Machine Learning Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16369_05.xhtml#_idTextAnchor183), *Improving the Performance
    of Machine Learning Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability
    in Machine Learning Modeling*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing Bias and Achieving
    Fairness*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
