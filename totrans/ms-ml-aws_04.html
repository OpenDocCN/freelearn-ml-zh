<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Classifying Twitter Feeds with Naive Bayes</h1>
                </header>
            
            <article>
                
<p><strong>Machine learning</strong> (<strong>ML</strong>) plays a major part in analyzing large datasets and extracting actionable insights from the data. ML algorithms perform tasks such as predicting outcomes, clustering data to extract trends, and building recommendation engines. Knowledge of ML algorithms helps data scientists to understand the nature of data they are dealing with and plan what algorithms should be applied to achieve desired outcomes from the data. Although multiple algorithms are available to perform any tasks, it is important for data scientists to know the pros and drawbacks of different ML algorithms. The decision to apply ML algorithms can be based on various factors, such as the size of the dataset, the budget for the clusters used for training and deployment of ML models, and the cost of error rates. Although AWS offers a large number of options in terms of selecting and deploying ML models, a data scientist has to knowledgeable in terms of what algorithms should be used in different situations.</p>
<p>In this part of the book, we present various popular ML algorithms and examples of applications where they can be applied effectively. We will explain the advantages and disadvantages of each algorithm and situations when these algorithms should be selected in AWS. As this book is written with data science students and professionals in mind, we will present a simple example of how the algorithms can be implemented using simple Python libraries, and then deployed on AWS clusters using Spark and AWS SageMaker for larger datasets. These chapters should help data scientists to get familiar with the popular ML algorithms and help them understand the nuances of implementing these algorithms in big data environments on AWS clusters.</p>
<p><a href="9163133d-07bc-43a6-88e6-c79b2187e257.xhtml">Chapter 2</a>, <em>Classifying Twitter Feeds with Naive Bayes</em>, <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>, <a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml">Chapter 4</a>, <em>Predicting User Behavior with Tree-Based Methods</em>, and <a href="ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml">Chapter 5</a>, <em>Customer Segmentation Using Clustering Algorithms</em> present four classification algorithms that can be used to predict an outcome based on a feature set. <a href="c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml">Chapter 6</a>, <em>Analyzing Visitor Patterns to Make Recommendations</em>, explains clustering algorithms and demonstrates how they can be used for applications such as customer segmentation. <a href="c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml">Chapter 7</a>, <em>Implementing Deep Learning Algorithms</em>, presents a recommendation algorithm that can be used to recommend new items to users based on their purchase history.</p>
<p>This chapter will introduce the basics of the Naive Bayes algorithm and present a text classification problem that will be addressed by the use of this algorithm and language models. We'll provide examples on how to apply it on <kbd>scikit-learn</kbd>, Apache Spark, and on SageMaker's BlazingText. Additionally, we'll explore how to further use the ideas behind Bayesian reasoning in more complex scenarios.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Classification algorithms</li>
<li>Naive Bayes classifier</li>
<li>Classifying text with language models</li>
<li>Naive Bayes — pros and cons</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification algorithms</h1>
                </header>
            
            <article>
                
<p>One of the popular subsets of ML algorithms <span><span>are </span></span>the classification algorithms. They are also referred to as supervised learning algorithms. For this approach, we assume that we have a rich dataset of features and events associated with those features. The task of the algorithm is to predict an event given a set of features. The event is referred to as a class variable. For example, consider the following dataset of features related to weather and if it snowed on that day:</p>
<p><strong>Table 1: Sample dataset</strong></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 259px">
<p><strong>Temperature (in °F)</strong></p>
</td>
<td style="width: 209px">
<p><strong>Sky condition</strong></p>
</td>
<td style="width: 178px">
<p><strong>Wind Speed (in MPH)</strong></p>
</td>
<td style="width: 173px">
<p><strong>Snowfall</strong></p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>Less than 20</p>
</td>
<td style="width: 209px">
<p>Sunny</p>
</td>
<td style="width: 178px">
<p>30</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>20-32</p>
</td>
<td style="width: 209px">
<p>Sunny</p>
</td>
<td style="width: 178px">
<p>6</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>32-70</p>
</td>
<td style="width: 209px">
<p>Cloudy</p>
</td>
<td style="width: 178px">
<p>20</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>70 and above</p>
</td>
<td style="width: 209px">
<p>Cloudy</p>
</td>
<td style="width: 178px">
<p>0</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>20-32</p>
</td>
<td style="width: 209px">
<p>Cloudy</p>
</td>
<td style="width: 178px">
<p>10</p>
</td>
<td style="width: 173px">
<p>True</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>32-70</p>
</td>
<td style="width: 209px">
<p>Sunny</p>
</td>
<td style="width: 178px">
<p>15</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>Less than 20</p>
</td>
<td style="width: 209px">
<p>Cloudy</p>
</td>
<td style="width: 178px">
<p>8</p>
</td>
<td style="width: 173px">
<p>True</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>32-70</p>
</td>
<td style="width: 209px">
<p>Sunny</p>
</td>
<td style="width: 178px">
<p>7</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>20-32</p>
</td>
<td style="width: 209px">
<p>Cloudy</p>
</td>
<td style="width: 178px">
<p>11</p>
</td>
<td style="width: 173px">
<p>False</p>
</td>
</tr>
<tr>
<td style="width: 259px">
<p>Less than 20</p>
</td>
<td style="width: 209px">
<p>Sunny</p>
</td>
<td style="width: 178px">
<p>13</p>
</td>
<td style="width: 173px">
<p>True</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the dataset, a weather station has information about the temperature, the sky condition, and the wind speed for the day. They also have records of when they received snowfall. The classification problem they are working on is to predict snowfall based on features such as temperature, sky condition, and wind speed.</p>
<p>Let's discuss some terminology that is used in ML datasets. For the example table, if the classification problem is to predict snowfall, then the snowfall feature is referred to as a <strong>class</strong> or <strong>target</strong> variable. Non-class values are referred to as attribute or feature variables. Each row in this dataset is referred to as an observation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature types</h1>
                </header>
            
            <article>
                
<p>There are three types of features that are available in a classification dataset. The reason why data scientists need to be able to differentiate between different features is that not every ML algorithm supports each type of feature. So, if the type of feature set does not match the desired algorithm, then the features need to be preprocessed to transform the feature that the classification algorithm can process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nominal features</h1>
                </header>
            
            <article>
                
<p><strong>Nominal</strong> or <strong>categorical</strong> features are features that can have a finite set of categorical values, and these values cannot be ordered in any specific order. In the example dataset, the <strong>sky condition</strong> feature is a nominal feature. In the table, the value of the nominal feature is either <strong>Sunny</strong> or <strong>Cloudy</strong>. Other examples of nominal features are gender and color. Nominal features can be converted into continuous variables by using techniques such as one-hot encoding.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ordinal features</h1>
                </header>
            
            <article>
                
<p><strong>Ordinal</strong> features, similar to nominal features, also have a finite set of categorical values. However, unlike nominal features, these categorical values can be put into a specific order. In the previous example, the <strong>Temperature</strong><span> feature </span>is an ordinal feature. The labels in this category can be ordered from coldest to warmest. Ordinal features can be converted into continuous variables by interpolating the range values to a defined scale. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous features</h1>
                </header>
            
            <article>
                
<p><strong>Continuous</strong> features can have infinite possible values. Unlike nominal and ordinal features, which can only have a discrete set of values, continuous variables are numerical variables, and are not compatible with some ML algorithms. However, continuous features can be converted into ordinal features using a technique called <strong>discretization</strong>.</p>
<p class="mce-root"/>
<p>Although we will not discuss techniques to transform features from one form to another here, we will demonstrate how it can be done in our example sections. We have selected example datasets in this book where feature transformation is required. You should not only learn about these various transformation techniques from this book, but also observe how a data scientist analyzes a dataset and uses specific feature transformation techniques based on the application. We have also provided examples to apply these techniques at scale in Python and AWS SageMaker.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes classifier</h1>
                </header>
            
            <article>
                
<p>Naïve Bayes classifier is a ML algorithm based on Bayes' theorem. The algorithm is comparable to how a belief system evolves. Bayes' theorem was initially introduced by an English mathematician, Thomas Bayes, in 1776. This algorithm has various applications, and has been used for many historic tasks for more than two centuries. One of the most famous applications of this algorithm was by Alan Turing during the Second World War, where he used Bayes' theorem to decrypt the German Enigma code. Bayes' theorem has also found an important place in ML for algorithms such as Bayesian Net and Naive Bayes algorithm. Naïve Bayes algorithm is very popular for ML due to its low complexity and transparency in why it makes the prediction. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes' theorem</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will first introduce Bayes' theorem and demonstrate how it is applied in ML.</span></p>
<p>Bayes' theorem calculates the probability of an event given a condition, such that we have prior knowledge about the event, the condition, and the probability of the condition when the event occurs. In our snow prediction example, the event is when snow occurs. A condition would be when the temperature is between 20°F and 32°F. And, based on the data, we can calculate the likelihood of temperature being 20°F and 32°F when it snows. Using this data, we can predict the probability of snow given the temperature being between 20°F and 32°F.</p>
<p>Assume that we have a class variable <em>C</em> and a condition variable <em>x</em>. Bayes' theorem is presented in formula 1. We also present a given simple way to remember different components of the algorithm in formula 2.</p>
<p><strong>Formula 1</strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/761e081a-97d5-4bec-811c-c36b7bc1e555.png" style="width:9.33em;height:2.25em;"/>   </p>
<p><strong>Formula 2 </strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/024ae2dd-62e4-410d-b8d1-f6fdf5c78ee4.png" style="width:12.83em;height:2.08em;"/>   </p>
<p>There are four terms that you need to remember from this formula.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Posterior  </h1>
                </header>
            
            <article>
                
<p>The <strong>posterior</strong> probability is the <span><span>chance </span></span>of an event occurring given the existence of feature variable <em>x</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Likelihood </h1>
                </header>
            
            <article>
                
<p><strong>Likelihood</strong> is the probability of a condition occurring for a given event. In our example, likelihood means what the <span><span>probability is </span></span>of the temperature being between 20°F to 32°F when it snows. Based on the data in the dataset, there is a 66.66% probability that the temperature is 20°F-30°F when it snows. Training data can be used to calculate the probability of each discrete value in the feature set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prior probability </h1>
                </header>
            
            <article>
                
<p>The <strong>prior</strong> probability is the overall probability of the event in the dataset. In our example, this would be the overall probability that it snows in the dataset. Prior probability is important in cases where the datasets are unbalanced, that is, the number of instances of one class variable in the dataset is significantly higher than the other. This leads to bias in the likelihood variable. Prior probabilities are used to renormalize these probabilities by taking the bias in the dataset into account. For example, in our dataset, the prior probability of a snow event is 30% and the prior probability of it not snowing is 70%. The probability of cloudy conditions when it snows is 66%, while the likelihood of cloudy conditions when it does not snow is 42.8%.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>However, by taking the prior probabilities into account, although cloudy conditions are more likely when it snows than when it does not, after multiplying the priors, the posterior probability of snow when it is cloudy is 19% and the probability of not snowing when it is cloudy is 30%. By multiplying the prior probabilities to the likelihood events, we inform our posterior probability that there is a higher probability of it not snowing than snowing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evidence </h1>
                </header>
            
            <article>
                
<p>The <strong>evidence</strong> variable is the probability of a condition in the dataset. In our example, the probability of temperature being 70°F or above is only 10%. Rare events have low evidence probability. Evidence probabilities boost posterior probabilities of rare events. For the purpose of the Naïve Bayes classifier, we do not need to consider the evidence variable, since it is not dependent on the class variable.</p>
<p>So, Bayes' theorem is used to calculate the probability of an event given a single condition. However, when we train ML algorithms, we use one or more features to predict the probability of an event. In the next section, we will explain Naïve Bayes algorithm and how it utilizes posterior probabilities of multiple features variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How the Naive Bayes algorithm works</h1>
                </header>
            
            <article>
                
<p>The Naive Bayes algorithm uses Bayes' theorem to calculate the posterior probability of every condition in the dataset and uses these probabilities to calculate the conditional probability of an event given a set of conditions. The Naive Bayes algorithm assumes that each conditional feature is independent of each other. This is an important assumption that helps simplify how the conditional probability is calculated. The independence assumption is the reason why the algorithm gets the name, Naive Bayes. </p>
<p>In this section, instead of considering one <em>x</em> <span>feature variable,</span> we consider a vector of features,<img class="fm-editor-equation" src="assets/0905d81e-d283-452c-933e-1cc360270d18.png" style="width:9.92em;height:1.33em;"/>, where <em>n</em> is the number of feature variables used to calculate the class probability. We represent the conditional probability of a class variable for the <em>x</em> vector in formula 3:</p>
<p><strong>Formula 3</strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e7156a8e-ff2a-4255-b184-d9ef34deec26.png" style="width:9.83em;height:1.42em;"/> </p>
<p>As we have assumed that each feature variable is independent of each other, the conditional probability of a class variable can be calculated as follows:</p>
<p><strong>Formula 4</strong></p>
<p class="CDPAlignCenter CDPAlign">  <img class="fm-editor-equation" src="assets/4e7880e7-9ad5-4522-98d0-14d8be8c20c2.png" style="width:16.75em;height:3.50em;"/>  </p>
<p>Based on posterior probability calculations shown in the previous sections, this formula can be rewritten as follows:</p>
<p><strong>Formula 5</strong></p>
<p class="CDPAlignCenter CDPAlign"> <img class="fm-editor-equation" src="assets/c6a3509a-c92f-4079-94dc-312c679f4663.png" style="width:15.00em;height:2.67em;"/> </p>
<p>Formula 5 explains how a probability of event <em>C</em> is calculated based on the <img class="fm-editor-equation" src="assets/0905d81e-d283-452c-933e-1cc360270d18.png" style="width:9.25em;height:1.25em;"/><span> feature variables</span>. An interesting thing to note in this formula is how easy it is to calculate each element from the dataset. Also, since the evidence probability from Bayes' theorem is not dependent on the class variable, it is not used in the Naive Bayes formula.</p>
<p>The Naive Bayes algorithm only requires one pass over the dataset during the training phase to calculate the probability of the value of a feature for each event. During the prediction phase, we calculate the probability of each event given the instance of the features and predict the event with the highest probability. Formula 6 shows how the prediction of a Naïve Bayes classifier is calculated when <em>k</em> events are possible. <strong>Argmax</strong> in the formula means that the event with maximum probability is selected as the prediction:</p>
<p><strong> Formula <span>6</span></strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bb33aabc-cbc3-4a34-89f0-702644ccea50.png" style="font-size: 1em;width:25.42em;height:3.75em;"/></p>
<p>Naïve Bayes classifier is a multiclass classifier that can be used to train on a dataset where two or more class variables need to be predicted. In the next chapters, we will present some examples of binary classifiers that only work with two class variables needs to be predicted. However, we will show you the methodologies of applying binary classifiers to multiclass problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying text with language models</h1>
                </header>
            
            <article>
                
<p>Text classification is an application of classification algorithms. However, the text is a combination of words in a specific order. Hence, you can observe that a text document with a class variable is not similar to the dataset that we presented in table 1, in the <em>Classification algorithms</em> section. </p>
<p>A text dataset can be represented as shown in table 2.</p>
<p><strong>Table 2: Example of a Twitter dataset</strong></p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 84.1884%"><strong>Tweet</strong></td>
<td style="width: 15.3275%"><strong>Account</strong></td>
</tr>
<tr>
<td style="width: 84.1884%">The simplest way to protect Americans from gun violence is to actually talk about common-sense gun laws.</td>
<td style="width: 15.3275%">Democrats</td>
</tr>
<tr>
<td style="width: 84.1884%">This cannot be who we are as a country. We need to find out what happened and ensure it never happens again (<a href="https://t.co/RiY7sjMfJK">https://t.co/RiY7sjMfJK)</a>)<br/></td>
<td style="width: 15.3275%"><span>Democrats</span></td>
</tr>
<tr>
<td style="width: 84.1884%">Over the weekend, President Trump visited Arlington National Cemetery to honor fallen soldiers.</td>
<td style="width: 15.3275%">Republicans</td>
</tr>
<tr>
<td style="width: 84.1884%">This President has made it clear that he will secure this country—<kbd>@SecNielsen</kbd>.</td>
<td style="width: 15.3275%"><span>Republicans</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>For this chapter, we have built a dataset based on tweets from two different accounts. We also have provided code in the following sections so that you can create your own datasets to try this example. Our purpose is to build a smart application that is capable of predicting the source of a tweet just by reading the tweet text. We will collect several tweets by the United States Republican Party (<kbd>@GOP</kbd>) and the Democratic Party (<kbd>@TheDemocrats</kbd>) to build a model that can predict which party wrote a given tweet. In order to do this, we will randomly select some tweets from each party and submit them through the model to check whether the prediction actually matched reality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting the tweets</h1>
                </header>
            
            <article>
                
<p>We will start by using the <kbd>Twython</kbd> library to access the Twitter API and collect a series of tweets, labeling them with the originating political party.</p>
<p>The details of the implementation can be found in our GitHub repository in the following Jupyter Notebook: </p>
<p> <kbd>chapter2/collect_tweets.ipynb</kbd> </p>
<p>We need to invoke the following method in the <kbd>Twython</kbd> library to save tweets from <kbd>@GOP</kbd> and <kbd>@TheDemocrats</kbd> onto some text files, <kbd>gop.txt</kbd> and <kbd>dems.txt</kbd> respectively:</p>
<pre>twitter.get_user_timeline(screen_name='GOP', tweet_mode='extended', count=500)</pre>
<p>Each file contains 200 tweets. The following are some excerpts from the <kbd>dems.txt</kbd> file:</p>
<ul>
<li><kbd>This cannot be who we are as a country. We need to find out what happened and ensure it never happens again.</kbd></li>
<li><kbd>RT @AFLCIO: Scott Walker. Forever a national disgrace.</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>Now that we have the source data in text files, we need to convert it to a format that can be used as an input for a ML library. Most general-purpose ML packages, such as <kbd>scikit-learn</kbd> and Apache Spark, only accept a matrix of numbers as input. Hence, feature transformation is required for a text dataset. A common approach is to use language models such as <strong>bag of words</strong> (<strong>BoW</strong>). In this example, we build a BoW for each tweet and construct a matrix in which each row represents a tweet and each column signals the presence of a particular word. We also have a column for the label that can distinguish tweets from <kbd>Republicans</kbd> (<kbd>1</kbd>) or <kbd>Democrats</kbd> (<kbd>0</kbd>), as we can see in the following table:</p>
<p><strong>Table 3: Converting text dataset to structured dataset</strong></p>
<div>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 100px"/>
<td class="CDPAlignCenter CDPAlign" style="width: 127.667px">
<p><strong>Immigration</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99.3333px">
<p><strong>Medicaid</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99px">
<p><strong>Terrorism</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p><strong>Class</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>Tweet 1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 127.667px">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99.3333px">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99px">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>0</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>Tweet 2</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 127.667px">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99.3333px">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99px">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>1</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>Tweet 3</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 127.667px">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99.3333px">
<p>0</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 99px">
<p>1</p>
</td>
<td class="CDPAlignCenter CDPAlign" style="width: 100px">
<p>0</p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>Table 2 represents the matrix that can be derived from tweets. However, there are many points to remember when generating such a matrix. <span>Due to the number of terms in the language lexicon, the number of columns in the matrix can be very high. This poses a problem in ML known as the <strong>curse of dimensionality</strong> (see section <em>X</em>). There are several ways to tackle this problem; however, as our example is fairly small in terms of data, we will only briefly discuss methods to reduce the number of columns.</span></p>
<ul>
<li style="font-weight: 400"><strong>Stopwords</strong>: Certain common words might add no value to our task (for example, the words <strong>the</strong>, <strong>for</strong>, or <strong>as</strong>). We call these words stopwords, and we shall remove these words from <kbd>dems.txt</kbd> and <kbd>gop.txt</kbd>.</li>
<li style="font-weight: 400"><strong>Stemming</strong>: There may be many variants of a word that are used in the text. For example, argue, argued, argues, and arguing all stem from the word <strong>argue</strong>. Techniques such as stemming and lemmatization can be used to find the stem of the word and replace variants of that word with the stem.</li>
<li><strong>Tokenization</strong>: Tokenization can be used to combine various words into phrases so that the number of features can be reduced. <span>For example, <strong>tea party</strong> has a totally different meaning, politically, than the two words alone. We won't consider this for our simple example, but tokenization techniques help in finding such phrases.</span></li>
</ul>
<p>Another issue to consider is that words appearing more than once in a tweet have equal importance on a training row. There are ways to utilize this information by using multinomial or term frequency-<span>inverse document frequency (</span>TFIDF) models. Since tweets are relatively short text, we will not consider this aspect in our implementation.</p>
<p>The table 2 matrix describes the words you would find for each class (that is each political party). However, when we want to predict the source of the tweet, the inverse problem is posed. Given a specific bag of words, we're interested in assessing how likely it is that the terms are used by one party or another. In other words, we know the probability of a bag of words given a particular party, and we are interested in the reverse: the probability of a tweet being written by a party given a bag of words. This is where the Naive Bayes algorithm is applied.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Naive Bayes model through SageMaker notebooks</h1>
                </header>
            
            <article>
                
<p>Let's get started with SageMaker notebooks. This tool will help us run the code that will train our model. SageMaker, among other things, allows us to create notebook instances that host Jupyter Notebooks. Jupyter is a web UI that allows a data scientist or programmer to code interactively by creating paragraphs of code that are executed on demand. It works as an IDE, but with the additional ability to render the output of the code in visually relevant forms (for example, charts, tables, and markdown), and also supports writing paragraphs in different languages within the same notebook. We will use notebooks extensively throughout this book, and we recommend its use as a way to share and present data science findings. It allows users to achieve reproducible research, as the code necessary for a particular research objective can be validated and reproduced by re-running the code paragraphs in the notebook.</p>
<p>You can learn more on SageMaker's AWS console page at <a href="https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard">https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard</a>.</p>
<p class="mce-root"/>
<p>Let's look at what Sagemaker's AWS console page looks in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-711 image-border" src="assets/89acc840-4b2c-4737-9743-852afedb151e.png" style="width:68.58em;height:45.25em;"/></p>
<p>Click on <span class="packt_screen">Add repository</span>, choose your authentication mechanism and add the repository found at <a href="https://github.com/mg-um/mastering-ml-on-aws">https://github.com/mg-um/mastering-ml-on-aws</a><span>:</span><span> </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-712 image-border" src="assets/1e93d7ec-84f0-4db1-9faf-a88ff6164296.png" style="width:69.33em;height:15.50em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Before creating the notebook instance, it is possible that you would want to attach a Git repository so that the notebooks available with this book are attached to the notebook, and so are made available immediately as you will see later:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-713 image-border" src="assets/8af5a0cb-2ad3-4f82-bdd2-8b8a71f5cdcc.png" style="width:37.33em;height:49.17em;"/></p>
<p>We can now proceed to launch a notebook instance. There are several options to configure the hardware, networking, and security of the server that will host the notebook. However, we will not go into much detail for now, and will accept the defaults. The AWS documentation is an excellent resource if you want to limit the access or power-up your machine.</p>
<p>Since we attached the Git repository, once you open Jupyter, you should see the notebooks we created for this book, and you can re-run them, modify them, or improve them:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-714 image-border" src="assets/c2aedb20-c8a9-4ccb-94c6-a5d0d253f08b.png" style="width:33.75em;height:23.83em;"/></p>
<p>In this section, we focus on the <kbd>train_scikit</kbd> Python notebook and go over code snippets to explain how we can build and test a model for out tweet classification problem. We encourage you to run all the paragraphs of this notebook to get an idea of the purpose of this notebook. </p>
<p>The first thing we will do is load the stopwords and the two sets of tweets <span>into variables</span>:</p>
<pre>import pandas as pd<br/>import numpy as np<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from scipy import sparse<br/><br/>SRC_PATH = '/home/ec2-user/SageMaker/mastering-ml-on-aws/chapter2/'<br/>stop_words = [word.strip() for word in open(SRC_PATH + 'stop_words.txt').readlines()]<br/>with open(SRC_PATH + 'dem.txt', 'r') as file:<br/>   dem_text = [line.strip('\n') for line in file]<br/>with open(SRC_PATH + 'gop.txt', 'r') as file:<br/>   gop_text = [line.strip('\n') for line in file]</pre>
<p>We will then proceed to use the utilities in <kbd>scikit-learn</kbd> to construct our matrix. In order to do that, we will use a <kbd>CountVectorizer</kbd> class, which is a class that knows how to allocate the different words into columns while at the same time filtering the stopwords. We will consider both sets of tweets; for our example, we'll just use the first <kbd>1200</kbd> words:</p>
<pre>vectorizer = CountVectorizer(input=dem_text + gop_text,<br/>                             stop_words=stop_words,<br/>                             max_features=1200)</pre>
<p>Through <kbd>vectorizer</kbd> we can now construct two matrices, one for republican party tweets and one for democratic party tweets:</p>
<pre>dem_bow = vectorizer.fit_transform(dem_text)<br/>gop_bow = vectorizer.fit_transform(gop_text)</pre>
<p>These two bag-of-words matrices (<kbd>dem_bow</kbd> and <kbd>gop_bow</kbd>) are represented in a sparse data structure to minimize memory usage, but can be examined by converting them to arrays:</p>
<pre>&gt;&gt;&gt; gop_bow.toarray()<br/><br/>array([[0, 0, 1, ..., 0, 1, 0],<br/>      [0, 0, 0, ..., 0, 0, 1],<br/>      [0, 1, 0, ..., 0, 0, 0],<br/>      ...,<br/>      [0, 0, 0, ..., 0, 0, 0],<br/>      [0, 1, 0, ..., 0, 0, 0],<br/>      [0, 0, 0, ..., 0, 1, 0]], dtype=int64)</pre>
<p>In order to train our model, we need to provide two arrays. The BoWs matrix (for both parties), which we will call <kbd>x</kbd>, and the labels (<span>class variables)</span> for each of the tweets. To construct this, we will vertically stack both matrices (for each party):</p>
<pre>x = sparse.vstack((dem_bow, gop_bow))</pre>
<p>To construct the labels vector, we will just assemble a vector with <kbd>ones</kbd> for <kbd>Democrat</kbd> positions and <kbd>zeros</kbd> for <kbd>Republican</kbd> positions:</p>
<pre>ones = np.ones(200)<br/>zeros = np.zeros(200)<br/>y = np.hstack((ones, zeros))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Before we train our models, we will split the tweets (rows on our <kbd>x</kbd> matrix) randomly, so that some are used to build a model and others are used to check whether the model predicts the correct political party (label):</p>
<pre>from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)</pre>
<p>Now that we have our training and testing datasets, we proceed to train our model using Naive Bayes (a Bernoulli Naive Bayes, since our matrices are ones or zeros):</p>
<pre>from sklearn.naive_bayes import BernoulliNB<br/>naive_bayes = BernoulliNB()<br/>model = naive_bayes.fit(x_train, y_train)</pre>
<p>As you can see in the preceding code, it is very simple to fit a Naive Bayes model. We need to provide the training matrices and the labels. A model is now capable of predicting the label (political party) of arbitrary tweets (as long as we have them as a BoWs matrix representation). Fortunately, we had separated some of the tweets for testing, so we can run these through the model and see how often the model predicts the right label (note that we know the actual party that wrote the tweet for every tweet in the testing dataset).</p>
<p>To get the predictions it's as simple as invoking the <kbd>predict</kbd> method of the model:</p>
<pre>y_predictions = model.predict(x_test)</pre>
<p><span>Now, we can see how many of the predictions match the ground truth:</span></p>
<pre>from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test, y_predictions)</pre>
<p>The output score of the code block is <kbd>0.95</kbd>.</p>
<p>In this example, we are using accuracy as an evaluation metric. Accuracy can be calculated using formula 7:</p>
<p><strong>Formula 7</strong></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0b450b51-a65b-4f29-b4dd-a7447a36b262.png" style="width:23.00em;height:2.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are various evaluation metrics that a data scientist can use to evaluate ML algorithm. We will present evaluation measures such as precision, recall, F1 measure, <strong>root mean squared error</strong> (<strong>RMSE</strong>), and <strong>area under curve</strong> (<strong>AUC</strong>) in our next chapters for different examples. Evaluation metrics should be selected based on the business need of implementing an algorithm, and should indicate whether or not the ML algorithm is performing at the standards required to achieve a task. </p>
<p>Since this is the first example we are working on, we will use the simplest evaluation measure, which is accuracy. As specified in formula 7, accuracy is the ratio of correct predictions to the total number of predictions made by the classifier. It turns out that our Naive Bayes model is very accurate, with an accuracy of 95%. It is possible that some words, such as the names of members of each party, can quickly make the model give a correct prediction. We will explore this using decision trees in <a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml">Chapter 4</a>, <em>Predicting User Behavior with Tree-Based Methods</em>.</p>
<div class="packt_infobox">Note that, during this process, we had to prepare and transform the data in order to fit a model. This process is very common, and both <kbd>scikit-learn</kbd> and Spark support the concept of pipelines, which allow the data scientist to declare the necessary transformations needed to build a model without having to manually obtain intermediary results.</div>
<p>In the following code snippet, we can see an alternative way to produce the same model by creating a pipeline with the following two stages:</p>
<ul>
<li>Count vectorizer</li>
<li>Naive Bayes trainer</li>
</ul>
<pre>from sklearn.pipeline import Pipeline<br/>x_train, x_test, y_train, y_test = train_test_split(dem_text + gop_text, y, test_size=0.25, random_state=5)<br/>pipeline = Pipeline([('vect', vectorizer), ('nb', naive_bayes)])<br/>pipeline_model = pipeline.fit(x_train, y_train)<br/>y_predictions = pipeline_model.predict(x_test)<br/>accuracy_score(y_test, y_predictions)</pre>
<p>This allows our modeling to be a bit more concise and declarative. By calling the <kbd>pipeline.fit()</kbd> method, the library applies any necessary transformations or estimations necessary. Note that, in this case, we split the raw texts (rather than the matrices) as the <kbd>fit()</kbd> method now receives the raw input. As we shall see in the next section, pipelines can contain two kinds of stages, Transformers and Estimators, depending on whether the stage needs to compute a model out of the data, or simply transform the data declaratively.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naïve Bayes model on SageMaker notebooks using Apache Spark</h1>
                </header>
            
            <article>
                
<p>In the previous section <em>Classifying text with language models</em>, we saw how you can train a model with <kbd>scikit-learn</kbd> on a SageMaker notebook instance. This is feasible for examples as small as the ones we collected from Twitter. What if, instead, we had hundreds of terabytes worth of tweet data? For starters, we would not be able to store the data in a single machine. Even if we could, it would probably take too long to train on such large dataset. Apache Spark solves this problem for us by implementing ML algorithms that can read data from distributed datasets (such as AWS S3) and can distribute the computing across many machines. AWS provides a product called <strong>Elastic MapReduce</strong> (<strong>EMR</strong>) that is capable of launching and managing clusters on which we can perform ML at scale.</p>
<p>Many of the ML algorithms require several passes over the data (although this is not the case for Naive Bayes). Apache Spark provides a way to cache the datasets in memory, so that one can efficiently run algorithms that require several passes over the data (such as <strong>logistic regression</strong> or <strong>decision trees</strong>, which we will see in the following chapters). We will show how to launch EMR clusters in<a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml"/> <a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml"/><a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml"/><a href="af506fc8-f482-453e-8162-93a676b2e737.xhtml">Chapter 4</a>, <em>Predicting User Behavior with Tree-Based Methods</em>, however, in this section, we will present how similar it is to work with Apache Spark compared to <kbd>scikit-learn</kbd>. In fact, many of the interfaces in Apache Spark (such as pipelines, Transformers, and Estimators) were inspired by <kbd>scikit-learn</kbd>.</p>
<p>Apache Spark supports four main languages: R, Python, Scala, and Java. In this book we will use the Python flavor, also called PySpark. Even though our spark code will run on a single machine (that is, will run on our SageMaker notebook instance), it could run on multiple machines without any code changes if our data was larger and we had a Spark Cluster (in <a href="https://cdp.packtpub.com/mastering_machine_learning_on_aws/wp-admin/post.php?post=25&amp;action=edit#post_27">Chapter 4</a><span>, </span><em>Predicting User Behavior with Tree-Based Methods</em>, we will dive into creating Spark Clusters with EMR).</p>
<p>In Spark, the first thing we need to do is to create a Spark session. We do this by first creating a Spark context, and then creating a session for SQL-like manipulation of data:</p>
<pre>from pyspark.context import SparkContext<br/>from pyspark.sql import SQLContext<br/><br/>sc = SparkContext('local', 'test')<br/>sql = SQLContext(sc)</pre>
<p>Since we will run Spark locally (on a single machine) we specify <kbd>local</kbd>. However, if we were to run this on a cluster, we would need to specify the master address of the cluster instead. Spark works with abstractions called DataFrames that allow us to manipulate huge tables of data using SQL-like operations.</p>
<p>Our first task will be to define <span>DataFrames </span>for our raw data:</p>
<pre>from pyspark.sql.functions import lit<br/><br/>dems_df = sql.read.text("file://" + SRC_PATH + 'dem.txt')<br/>gop_df = sql.read.text("file://" + SRC_PATH + 'gop.txt')<br/>corpus_df = dems_df.select("value", lit(1).alias("label")).union(gop_df.select("value", lit(0).alias("label")))</pre>
<p>In the first two lines, we create <span>DataFrames </span>out of our raw tweets. We also create <kbd>corpus_df</kbd>, which contains both sources of tweets, and add the label by creating a column with a literal of <kbd>1</kbd> for Democrats and <kbd>0</kbd> for <kbd>Republicans</kbd>:</p>
<pre>&gt;&gt;&gt; corpus_df.select("*").limit(2).show()<br/><br/>+--------------------+-----+<br/>|               value|label|<br/>+--------------------+-----+<br/>|This ruling is th...| 1 . |<br/>|No president shou...| 1 . |<br/>+--------------------+-----+</pre>
<p>Spark works in a lazy fashion, so, even though we defined and unioned the <span>DataFrame</span>, no actual processing will happen until we perform the first operation on the data. In our case, this will be the splitting of the <span>DataFrame </span>into testing and training:</p>
<pre>train_df, test_df = corpus_df.randomSplit([0.75, 0.25])</pre>
<p><span>Now, we are ready to train our model. Spark supports the same concept of pipelines. We will build a pipeline with the necessary transformations for our model. It's very similar to our previous example, except that Spark has two separate stages for tokenization and stop words remover:</span></p>
<pre>from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover<br/>tokenizer = Tokenizer(inputCol="value", outputCol="words")<br/>stop_words_remover = StopWordsRemover(inputCol="words", outputCol="words_cleaned")<br/>vectorizer = CountVectorizer(inputCol="words_cleaned", outputCol="features")<br/>cleaning_pipeline = Pipeline(stages = [tokenizer, stop_words_remover, vectorizer])<br/>cleaning_pipeline_model = cleaning_pipeline.fit(corpus_df)<br/>cleaned_training_df = cleaning_pipeline_model.transform(train_df)<br/>cleaned_testing_df = cleaning_pipeline_model.transform(test_df)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<div class="packt_infobox">A Spark ML pipeline consists of a series of stages. Each stage can be a Transformer or an Estimator. Transformers apply a well-defined transformation on a dataset, while Estimators have the added capability of producing models by traversing the dataset. <kbd>NaiveBayes</kbd> and <kbd>CountVectorizer</kbd> are examples of Estimators, while tokenizer and <kbd>StopWordsRemover</kbd> are examples of Transformers. Models, in turn, are Transformers, because they can provide predictions for all elements in a dataset as a transformation.</div>
<p>As you can see in the preceding code, we defined a pipeline with all the necessary stages to clean the data. Each stage will transform the original <span>DataFrame </span>(which only has two columns value, which are the raw tweet text and label) and add more columns.</p>
<p>In the following code, the relevant columns used at training time are the features (a sparse vector representing the BoWs exactly like our <kbd>scikit-learn</kbd> example) and the label:</p>
<pre>&gt;&gt;&gt; cleaned_training_df.show(n=3)<br/><br/>+-----------+------------------+-------------+--------------------+<br/>| value     |label| . words .  |words_cleaned| features           |<br/>+-----------+------------------+-------------+--------------------+<br/>|#Tuesday...| 1 . |[#tuesday...|[#tuesday... |(3025,[63,1398,18...|<br/>|#WorldAI...| 1 . |[#worlda....|[#worldai... |(3025,[37,75,155,...|<br/>|@Tony4W....| 1 . |[.@tony4w...|[.@tony4w... |(3025,[41,131,160...|<br/>+-----------------+------------+-------------+--------------------+</pre>
<p>By specifying these columns to the <kbd>NaiveBayes</kbd> classifier we can train a model:</p>
<pre>from pyspark.ml.classification import NaiveBayes<br/>naive_bayes = NaiveBayes(featuresCol="features", labelCol="label")</pre>
<p>The model is a transformer that can provide predictions for each row in our training <span>DataFrame</span>:</p>
<pre>naive_bayes_model = naive_bayes.fit(cleaned_training_df)<br/>predictions_df = naive_bayes_model.transform(cleaned_testing_df)<br/><br/>&gt;&gt;&gt; predictions_df.select("features", "label", "prediction").limit(3).show()<br/>+--------------------+-----+----------+<br/>| features           |label|prediction|<br/>+--------------------+-----+----------+<br/>|(3025,[1303,1858,...| 1 . | 1.0      |<br/>|(3025,[1,20,91,13...| 1 . | 1.0      |<br/>|(3025,[16,145,157...| 1 . | 1.0      |<br/>+--------------------+-----+----------+</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Similar to our previous example, we can evaluate the accuracy of our models. By using the <kbd>MulticlassClassificationEvaluator</kbd> class and specifying the actual and predicted labels, we can obtain <kbd>accuracy</kbd>:</p>
<pre>from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>evaluator = MulticlassClassificationEvaluator(<br/>   labelCol="label", predictionCol="prediction", metricName="accuracy")<br/>evaluator.evaluate(predictions_df)</pre>
<p>The output is 0.93, which is similar to the results we had on <kbd>scikit-learn</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using SageMaker's BlazingText built-in ML service</h1>
                </header>
            
            <article>
                
<p>We saw how to perform ML tasks using <kbd>scikit-learn</kbd> and Apache Spark libraries. However, sometimes it's more appropriate to use a ML service<em>. </em>SageMaker provides ways for us to create, tune, and deploy models supporting a variety of built-in ML algorithms by just invoking a service. In a nutshell, you need to place the data in S3 (an Amazon service to store large amounts of data) and call the SageMaker service providing all the necessary details (actual ML algorithm, the location of the data, which kind and how many machines should be used for training). In this section, we go through the process of training our model for predicting tweets through SageMaker's BlazingText ML service. BlazingText is an algorithm that supports text classification using word2vec, which is a way to transform words into vectors in a way that captures precise syntactic and semantic word relationships. We won't dive into the details of SageMaker's architecture yet, but we will present the reader how we would use this AWS service as an alternative to <kbd>scikit-learn</kbd> or Spark.</p>
<p>We will start by importing the SakeMaker libraries, creating a session, and obtaining a role (which is the role that the notebook instance is using (see <a href="https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2">https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2</a>).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Additionally, we specify the S3 bucket we will be using to store all our data and models:</p>
<pre><span class="kn">import</span> <span class="nn">sagemaker</span>
<span class="kn">from</span> <span class="nn">sagemaker</span> <span class="k">import</span> <span class="n">get_execution_role</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">boto3<br/></span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">role</span> <span class="o">=</span> <span class="n">get_execution_role</span><span class="p">()</span>
<span class="n">bucket</span> <span class="o">=</span> <span class="s2">"mastering-ml-aws"</span>
<span class="n">prefix</span> <span class="o">=</span> <span class="s2">"chapter2/blazingtext"</span></pre>
<p class="mce-root"><span>The next step is to put some data in S3 for training. The expected format for BlazingText is to have each line in the <kbd>__label__X TEXT </kbd></span> format. In our case, this means prefixing each tweet by a label representing the originating party:</p>
<pre>__label__1 We are forever g..<br/> __label__0 RT @AFLCIO: Scott Walker.<br/> __label__0 Democrats will hold this<br/> __label__1 Congratulations to hundreds of thousands ...</pre>
<p>To do that, we perform some preprocessing of our tweets and prefix the right label:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3">
<pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SRC_PATH</span> <span class="o">+</span> <span class="s1">'dem.txt'</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">dem_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"__label__0 "</span> <span class="o">+</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">SRC_PATH</span> <span class="o">+</span> <span class="s1">'gop.txt'</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">gop_text</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"__label__1 "</span> <span class="o">+</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">file</span><span class="p">]</span>
    
<span class="n">corpus</span> <span class="o">=</span> <span class="n">dem_text</span> <span class="o">+</span> <span class="n">gop_text</span></pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<p class="prompt input_prompt">We then proceed to create the sets for training and testing as text files:</p>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3">
<pre><span>from sklearn.model_selection import train_test_split<br/>corpus_train, corpus_test = train_test_split(corpus, test_size=0.25, random_state=42) <br/><br/>corpus_train_txt = "\n".join(corpus_train)<br/>corpus_test_txt = "\n".join(corpus_test)<br/><br/>with open('tweets.train', 'w') as file:<br/>    file.write(corpus_train_txt) <br/>with open('tweets.test', 'w') as file:<br/>    file.write(corpus_test_txt)</span></pre></div>
</div>
</div>
</div>
</div>
<p>Once we have our training and validation text files, we upload them into <kbd>S3</kbd>:</p>
<pre>train_path = prefix + '/train'<br/>validation_path = prefix + '/validation'<br/><br/>sess.upload_data(path='tweets.train', bucket=bucket, key_prefix=train_path)<br/>sess.upload_data(path='tweets.test', bucket=bucket, key_prefix=validation_path)<br/><br/>s3_train_data = 's3://{}/{}'.format(bucket, train_path)<br/>s3_validation_data = 's3://{}/{}'.format(bucket, validation_path)</pre>
<p>We then proceed to instantiate <kbd>Estimator</kbd>, by specifying all the necessary details: the type and amount of machines to be used for training, as well as the location of the path in <kbd>S3</kbd> where the models will be stored:</p>
<pre>container = sagemaker.amazon.amazon_estimator.get_image_uri('us-east-1', "blazingtext", "latest")<br/><br/>s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)<br/>bt_model = sagemaker.estimator.Estimator(container,<br/>                                         role, <br/>                                         train_instance_count=1, <br/>                                         train_instance_type='ml.c4.4xlarge',<br/>                                         train_volume_size = 30,<br/>                                         train_max_run = 360000,<br/>                                         input_mode= 'File',<br/>                                         output_path=s3_output_location,<br/>                                         sagemaker_session=sess)</pre>
<p>As we discussed in the previous section <em>Naive Bayes model on SageMaker notebooks using Apache Spark</em> section, an estimator is capable of creating models by processing training data. The next step will be to fit the model providing the training data:</p>
<pre><span class="n">bt_model</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">"supervised"</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">vector_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">word_ngrams</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="n">train_data</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_train_data</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">'FullyReplicated'</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s1">'text/plain'</span><span class="p">,</span> <span class="n">s3_data_type</span><span class="o">=</span><span class="s1">'S3Prefix'</span><span class="p">)</span> <span class="n">validation_data</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">session</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_validation_data</span><span class="p">,</span> <span class="n">distribution</span><span class="o">=</span><span class="s1">'FullyReplicated'</span><span class="p">, </span><span class="n">content_type</span><span class="o">=</span><span class="s1">'text/plain'</span><span class="p">,</span> <span class="n">s3_data_type</span><span class="o">=</span><span class="s1">'S3Prefix'</span><span class="p">)</span> <span class="n">data_channels</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'train'</span><span class="p">:</span> <span class="n">train_data</span><span class="p">,</span> <span class="s1">'validation'</span><span class="p">:</span> <span class="n">validation_data</span><span class="p">}<br/>bt_model.fit(inputs=data_channels, logs=True)</span><span class="p"><br/></span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Before we train the model we need to specify the hyperparameters. We won't go into much detail about this algorithm in this section, but the reader can find the details in <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html">https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html</a>.</p>
<p>This particular algorithm also takes the validation data, as it runs over the data several times (epochs) to improve the error. Once we fit the model, we can deploy the model as a web service so that applications can use it: </p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-ipython3">
<pre><span class="n">predictor</span> <span class="o">=</span> <span class="n">bt_model</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">initial_instance_count</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="n">instance_type</span> <span class="o">=</span> <span class="s1">'ml.m4.xlarge'</span><span class="p">)</span></pre></div>
</div>
</div>
</div>
<p><span>In our case, we will just hit the endpoint to get the predictions and evaluate the accuracy: </span></p>
<pre><span class="n">corpus_test_no_labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">11</span><span class="p">:]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">corpus_test</span><span class="p">]</span>
<span class="n">payload</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"instances"</span> <span class="p">:</span> <span class="n">corpus_test_no_labels</span><span class="p">}</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">payload</span><span class="p">))</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span></pre>
<p>After running the preceding code we get the following output:</p>
<pre><span class="packt_screen">[ { "prob": [ 0.5003 ], "label": [ "__label__0" ] }, { "prob": [ 0.5009 ], "label": [ "__label__1" ] }...</span></pre>
<p>As you can see in the preceding code, each prediction comes along with a probability (which we will ignore for now). Next, we compute how many of these labels matched the original one:</p>
<pre>predicted_labels = [prediction['label'][0] for prediction in predictions]<br/>predicted_labels[:4]</pre>
<p>After running the preceding code we get the following output:</p>
<pre><span class="packt_screen">['__label__0', '__label__1', '__label__0', '__label__0']</span></pre>
<p>Then run the next line of code:</p>
<pre>actual_labels = [x[:10] for x in corpus_test]<br/>actual_labels[:4]</pre>
<p>As you can see in the following output from the previous code block, some of the labels matched the actual while some don't:</p>
<pre><span class="packt_screen">['__label__1', '__label__1', '__label__0', '__label__1']</span></pre>
<p class="mce-root"/>
<p>Next, we run the following code to build a boolean vector containing true or false depending on whether the actual matches the predicted result:</p>
<pre>matches = [(actual_label == predicted_label) for (actual_label, predicted_label) in zip(actual_labels, predicted_labels)]<br/>matches[:4]</pre>
<p>After running the preceding code we get the following output:</p>
<pre><span class="packt_screen">[False, True, True, False]</span></pre>
<p>After we run the preceding output, we will run the following code to calculate the ratio of cases that match out of the total instances:</p>
<pre>matches.count(True) / len(matches)</pre>
<p>The following output from the previous block shows the accuracy score:</p>
<pre>0.61</pre>
<p>We can see that the accuracy is lower than in our previous examples. This is for many reasons. For starters, we did not invest too much in data preparation in this case (for example, no stopwords are used in this case). However, the main reason for the lower accuracy is due to the fact we're using such little data. These models work best on larger datasets. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive Bayes – pros and cons</h1>
                </header>
            
            <article>
                
<p>In this section, we present the advantages and disadvantages in selecting the Naive Bayes algorithm for classification problems:</p>
<p><strong>Pros</strong></p>
<ul>
<li>
<p><strong>Training time</strong>:<strong> </strong>Naive Bayes algorithm only requires one pass on the entire dataset to calculate the posterior probabilities for each value of the feature in the dataset. So, when we are dealing with large datasets or low-budget hardware, Naive Bayes algorithm is a feasible choice for most data scientists.</p>
</li>
<li>
<p><strong>Prediction time</strong>: Since all the probabilities are pre-computed in the Naive Bayes algorithm, the prediction time of this algorithm is very efficient.</p>
</li>
<li>
<p><strong>Transparency</strong>: Since the predictions of Naive Bayes algorithms are based on the posterior probability of each conditional feature, it is easy to understand which features are influencing the predictions. This helps users to understand the predictions.</p>
</li>
</ul>
<p><strong>Cons</strong></p>
<ul>
<li>
<p><strong>Prediction accuracy</strong>: The prediction accuracy of the Naive Bayes algorithm is lower than other algorithms we will discuss in the book. Algorithm prediction accuracy is dataset dependent, a lot of research works have proved that algorithms such as random forest, <strong>support vector machines</strong> (<strong>SVMs</strong>), and <strong>deep neural networks</strong> (<strong>DNNs</strong>) outperform Naive Bayes algorithm in terms of classification accuracy. </p>
</li>
<li>
<p><strong>Assumption of independence</strong>: Since we assume that each feature is independent of each other, this algorithm may lose information for features that are dependent on each other. Other advanced algorithms do use this dependence information when calculating predictions. </p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced you to why ML is a crucial tool in a data scientist's repository. We discussed what a structured ML dataset looks like and how to identify the types of features in the dataset. </p>
<p>We took a deep dive into Naive Bayes classification algorithm, and studied how Bayes' theorem is used in Naive Bayes algorithm. Using Bayes' theorem, we can predict the probability of an event occurring based on the values of each feature, and select the event that has the highest probability.</p>
<p>We also presented an example of a Twitter dataset. We hope that you learned how to think about a text classification problem, and  how to build a Naive Bayes classification model to predict the source of a tweet. We also presented how the algorithm can be implemented in SageMaker, and how it can also be implemented using Apache Spark. This code base should help you tackle any text classification problems in the future. As the implementation is presented using SageMaker services and Spark, it can scale to datasets that can be gigabytes or terabytes in size.</p>
<p>We will look at how to deploy the ML models on actual production clusters in later chapters. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>Bayes' Theorem is not only useful for the Naive Bayes algorithm, but is also used for other purposes. Find two more algorithms where Bayes' theorem is applied, and explain how they are different than the Naive Bayes algorithm.</li>
<li>In this chapter, we presented an example of a binary classifier. Based on our code to download tweets, create a new dataset where you download tweets from five different sources and build a Naive Bayes model that can predict the source of each tweet. </li>
<li>Identify scenarios for when you would use <kbd>scikit-learn</kbd>, Apache Spark, or SageMaker services for a particular problem.</li>
</ol>


            </article>

            
        </section>
    </body></html>