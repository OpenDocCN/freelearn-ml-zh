<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer050">
<h1 class="chapter-number" id="_idParaDest-117"><a id="_idTextAnchor120"/>7</h1>
<h1 id="_idParaDest-118"><a id="_idTextAnchor121"/>Exploring Generative Adversarial Networks</h1>
<p>In this chapter, we will introduce <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>) and discuss the evolution of this data generation<a id="_idIndexMarker318"/> method. You will learn about the typical architecture of a GAN. After this, we will explain its training process and discuss the main challenges. Then, we will highlight various applications of GANs, including generating images and text-to-image translation. Additionally, we will study a practical coding example demonstrating how to use GANs to generate photorealistic images. Finally, we will also discuss variations of GANs, such as conditional GANs, CycleGANs, CTGANs, WGANs, WGAN-GPs, <span class="No-Break">and f-GANs.</span></p>
<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
<ul>
<li>What is <span class="No-Break">a GAN?</span></li>
<li>Training <span class="No-Break">a GAN</span></li>
<li>Utilizing GANs to generate <span class="No-Break">synthetic data</span></li>
<li>Hands-on GANs <span class="No-Break">in practice</span></li>
<li>Variations <span class="No-Break">of GANs</span></li>
</ul>
<h1 id="_idParaDest-119"><a id="_idTextAnchor122"/>Technical requirements</h1>
<p>The code used in this chapter will be available in the corresponding chapter folder in the book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning"><span class="No-Break">https://github.com/PacktPublishing/Synthetic-Data-for-Machine-Learning</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor123"/>What is a GAN?</h1>
<p>In this section, we will introduce GANs<a id="_idIndexMarker319"/> and briefly discuss the evolution and progression of this particular data generation method. Then, we will explain the standard architecture of a typical GAN and how <span class="No-Break">they work.</span></p>
<p>The concept of GANs was introduced in the 2014 paper <em class="italic">Generative Adversarial Networks</em> (<a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a>), by Ian J. Goodfellow and his research<a id="_idIndexMarker320"/> team. In the same year, <strong class="bold">conditional GANs</strong> were introduced, allowing us to generate more customizable synthetic data. Then, <strong class="bold">Deep Convolutional GANs</strong> (<strong class="bold">DCGANs</strong>) were suggested in 2015, which facilitated<a id="_idIndexMarker321"/> the generation of high-resolution images. After that, <strong class="bold">CycleGANs</strong> were proposed in 2017 for unsupervised<a id="_idIndexMarker322"/> image-to-image translation tasks. This opened the door for enormous<a id="_idIndexMarker323"/> applications such as domain adaptation. <strong class="bold">StyleGAN</strong> was introduced in 2019, bringing GANs to new fields such as art <span class="No-Break">and fashion.</span></p>
<p>GANs have also been showing impressive progress in the field of video synthesis. In fact, the recent work by NVIDIA is a testament to their tremendous potential (please check this paper for more details: <em class="italic">One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing</em> <span class="No-Break">at </span><a href="https://arxiv.org/pdf/2011.15126.pdf"><span class="No-Break">https://arxiv.org/pdf/2011.15126.pdf</span></a><span class="No-Break">).</span></p>
<p>This work shows that GANs can now recreate a talking-head video using only a single source image. For the code, dataset, and online demo, refer to the project’s page: <a href="https://nvlabs.github.io/face-vid2vid">https://nvlabs.github.io/face-vid2vid</a>. Next, we delve into the architecture <span class="No-Break">of GANs.</span></p>
<p>Most <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) methods and architectures are designed to predict<a id="_idIndexMarker324"/> something. It could be weather conditions, stock prices, object classes, or something else. However, GANs were proposed to <em class="italic">generate</em> something. It could be images, videos, texts, music, or <span class="No-Break">point clouds.</span></p>
<p>At the heart of this capability lies the essential problem of learning how to generate training samples from a given domain or dataset. GANs are DL methods that can learn complex data distributions and can be leveraged to generate an unlimited number of samples that belong to a specific distribution. These generated synthetic samples have many applications for data augmentation, style transfer, and <span class="No-Break">data privacy.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 7.1 – A typical architecture and training process of GANs" height="410" src="image/Figure_07_01_B18494.jpg" width="1165"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – A typical architecture and training process of GANs</p>
<p>Moving forward, we will learn how to <span class="No-Break">train GANs.</span></p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor124"/>Training a GAN</h1>
<p>In this section, we will learn how<a id="_idIndexMarker325"/> to train a typical GAN. Then, we will discuss the main challenges <span class="No-Break">and difficulties.</span></p>
<p>A GAN is trained using <strong class="bold">unsupervised learning</strong> techniques where both submodels <a id="_idIndexMarker326"/>are trained<a id="_idIndexMarker327"/> simultaneously using a process called <strong class="bold">adversarial training</strong>. A typical GAN consists of two neural networks (usually convolutional neural networks): the <strong class="bold">generator</strong> and the <strong class="bold">discriminator</strong>. The generator takes in a random<a id="_idIndexMarker328"/> noise vector as input and generates<a id="_idIndexMarker329"/> a synthetic (fake) sample. The goal of the generator is to produce synthetic data that is realistic and indistinguishable from real data. The discriminator, on the other hand, is trained to distinguish between real and fake samples. It receives a sample and predicts its data source domain: real or fake. If the discriminator correctly identifies a real data sample, no error is backpropagated. However, if the discriminator fails to identify a synthetic sample, it is penalized, and the generator is rewarded. The generator is penalized if the discriminator is able to correctly identify generated, synthetic data. In this way, both the generator and discriminator are constantly trying to improve their performance, resulting in the generation of increasingly realistic synthetic data. Refer to <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> for a visualization of the training process. Let’s explore more and learn about the training process in <span class="No-Break">more detail.</span></p>
<p class="callout-heading">Disclaimer on hands-on training of GANs</p>
<p class="callout">Please note that we do not provide hands-on elements on how to train GANs because the chapter is committed to the theoretical, conceptual, and design aspects of GANs for synthetic data generation. Thus, hands-on examples are out of the scope of this chapter. However, if you are keen<a id="_idIndexMarker330"/> to train your GAN, please refer to the <em class="italic">Deep Convolutional Generative Adversarial Network </em><span class="No-Break"><em class="italic">Tutorial</em></span><span class="No-Break"> (</span><a href="https://www.tensorflow.org/tutorials/generative/dcgan"><span class="No-Break">https://www.tensorflow.org/tutorials/generative/dcgan</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor125"/>GAN training algorithm</h2>
<p>The training algorithm<a id="_idIndexMarker331"/> is a crucial aspect of enabling GANs to generate useful synthetic data. The following is a step-by-step procedure that can be utilized to <span class="No-Break">train GANs:</span></p>
<ol>
<li>Create <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Space"> </span>by sampling a random noise following a suitable noise distribution such as uniform, Gaussian, Binomial, Poisson, Exponential, Gamma, and <span class="No-Break">Weibull distributions.</span></li>
<li>Feed <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Space"> </span>to the generator to produce a synthetic or fake sample, <span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">f</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">a</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">k</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">e</span></span><span class="No-Break">.</span></li>
<li>Pass both <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable">e</span> and <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span> to a <strong class="source-inline">switch</strong> block, which randomly selects one of its inputs and passes it to <span class="No-Break">the discriminator.</span></li>
<li>The discriminator classifies the given sample as real <span class="No-Break">or fake.</span></li>
<li>Calculate <span class="No-Break">the error.</span></li>
<li>Backpropagate the error to both the generator <span class="No-Break">and discriminator.</span></li>
<li>Update the weights of the generator <span class="No-Break">and discriminator</span></li>
</ol>
<p>Next, we’ll discuss <span class="No-Break">the loss.</span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor126"/>Training loss</h2>
<p>The loss shown next is one of many losses<a id="_idIndexMarker332"/> that can be used to train a GAN. This particular loss<a id="_idIndexMarker333"/> is derived from the <span class="No-Break"><strong class="bold">cross-entropy loss</strong></span><span class="No-Break">:</span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">]</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">G</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">]</span></p>
<p>Let’s break this <span class="No-Break">formula down:</span></p>
<ul>
<li><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span> is the discriminator’s estimate that <span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Space"> </span>is drawn from the <span class="No-Break">real dataset</span></li>
<li><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span> and <span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span> are the expected values over real and generated synthetic (fake) <span class="No-Break">samples, respectively</span></li>
<li><span class="_-----MathTools-_Math_Variable">G</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span> is the output of the generator for a noise <span class="No-Break">vector, </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">z</span></span></li>
<li><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">G</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span> is the discriminator’s estimate that a synthetic sample <span class="No-Break">is real</span></li>
</ul>
<p>As expected, the training process of GANs is complex, but it is a powerful technique for generating realistic data, which motivated researchers to examine new ways to enhance and speed its training<a id="_idIndexMarker334"/> and convergence. Next, let us discuss some of <span class="No-Break">these challenges.</span></p>
<p class="callout-heading">GANs in action</p>
<p class="callout">For an interactive demonstration<a id="_idIndexMarker335"/> of how a GAN is trained, please refer to <em class="italic">Play with Generative Adversarial Networks (GANs)</em> in your browser (<a href="https://poloclub.github.io/ganlab">https://poloclub.github.io/ganlab</a>). For more details, check out the corresponding paper <em class="italic">GAN lab:</em> <em class="italic">Understanding Complex Deep Generative Models using Interactive Visual </em><span class="No-Break"><em class="italic">Experimentation</em></span><span class="No-Break"> (</span><a href="https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf"><span class="No-Break">https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor127"/>Challenges</h2>
<p>Now we will cover some common issues<a id="_idIndexMarker336"/> and challenges encountered when training a GAN. Let’s explore insights and explanations about the cause of <span class="No-Break">such issues:</span></p>
<ul>
<li><strong class="bold">Mode collapse</strong><span class="Strong">:</span> In this scenario, the generator overfits to a limited number of samples and patterns producing the same or similar synthetic samples for different <span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Space"> </span>values. For example, a GAN being trained to generate cat images may keep generating the same cat image again and again with just minor modifications. This is something that we do not want to happen. The point of using GANs is to generate diverse synthetic examples. This problem occurs when the generator learns to produce one or a few excellent synthetic samples that fool the discriminator. Thus, the generator avoids generating other samples and prefers to repeat these excellent synthetic samples. There are various solutions to this problem, such as <em class="italic">unrolled GANs </em>(<a href="https://arxiv.org/pdf/1611.02163.pdf">https://arxiv.org/pdf/1611.02163.pdf</a>) and <em class="italic">Wasserstein </em><span class="No-Break"><em class="italic">loss</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1701.07875"><span class="No-Break">https://arxiv.org/abs/1701.07875</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Discriminator saturations (diminished gradients)</strong><span class="Strong">: </span>As we discussed earlier, the training of the generator and discriminator is done in an adversarial manner. When the discriminator becomes too successful at classifying real from synthetic samples, the error becomes minimal. Thus, the generator can no longer learn <span class="No-Break">useful things.</span></li>
<li><strong class="bold">Hyperparameter sensitivity and tuning</strong><span class="Strong">:</span> Similar to other DL architectures, GANs have many hyperparameters, such as learning rate, batch size, number of layers, activation functions, and others. Finding the optimal hyperparameters is problem- and task-dependent and usually a train-error process. Thus, it is challenging to find the right architecture and hyperparameters to successfully train <span class="No-Break">your GAN.</span></li>
<li><strong class="bold">Instability and non-convergence</strong><span class="Strong">:</span> It is not easy to stabilize the training process of the generator and discriminator. In fact, it is common to observe that one submodel is learning better than another, which causes the GAN to oscillate, giving us unpredictable behavior, and the models may never converge. For more details, please refer to <em class="italic">On Convergence and Stability of </em><span class="No-Break"><em class="italic">GANs</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1705.07215.pdf"><span class="No-Break">https://arxiv.org/pdf/1705.07215.pdf</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Computation complexity</strong><span class="Strong">:</span> GANs have a complex structure, being composed of two DL models. This makes the training process<a id="_idIndexMarker337"/> computationally expensive and time-consuming. However, there are some techniques proposed to speed up the training process, such as <em class="italic">Small-GAN: Speeding up GAN Training using Core-Sets</em> (<a href="http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf">http://proceedings.mlr.press/v119/sinha20b/sinha20b.pdf</a>) and <em class="italic">Projected GANs Converge </em><span class="No-Break"><em class="italic">Faster</em></span><span class="No-Break"> (</span><a href="https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2021/file/9219adc5c42107c4911e249155320648-Paper.pdf</span></a><span class="No-Break">).</span></li>
</ul>
<p>In the next section, we delve into deploying GANs to generate <span class="No-Break">synthetic data.</span></p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor128"/>Utilizing GANs to generate synthetic data</h1>
<p>In this section, we will highlight some interesting applications <span class="No-Break">of GANs.</span></p>
<p>GANs have<a id="_idIndexMarker338"/> enormous applications<a id="_idIndexMarker339"/> because they can be used for data augmentation, style transfer, privacy protection, and generating <span class="No-Break">photo-realistic images.</span></p>
<p>Let’s discuss some of <span class="No-Break">these applications:</span></p>
<ul>
<li><strong class="bold">Generating images</strong><span class="Strong">:</span> GANs can be utilized to generate<a id="_idIndexMarker340"/> photorealistic images. For instance, GANs were utilized to generate handwritten digits, human faces, animals, objects, and scenes. Please check this paper for more details: <em class="italic">Progressive Growing of GANs for Improved Quality, Stability, and </em><span class="No-Break"><em class="italic">Variation</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/pdf/1710.10196.pdf"><span class="No-Break">https://arxiv.org/pdf/1710.10196.pdf</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Generating cartoon and anime characters</strong><span class="Strong">:</span> GANs can be trained to generate appealing and diverse characters. This can be utilized to assess artists, game developers, and designers with anime characters. For more details, please check the paper <em class="italic">Towards the Automatic Anime Characters Creation with Generative Adversarial Networks</em> (<a href="https://arxiv.org/pdf/1708.05509.pdf">https://arxiv.org/pdf/1708.05509.pdf</a>) and the <span class="No-Break">website (</span><a href="https://make.girls.moe"><span class="No-Break">https://make.girls.moe</span></a><span class="No-Break">).</span></li>
<li><strong class="bold">Image-to-image translation</strong>: GANs can be utilized to transform images from one domain to another domain. For example, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>)-based colorizers usually utilize<a id="_idIndexMarker341"/> GANs for turning grayscale images into colored ones. <em class="italic">Image-to-Image Translation with Conditional Adversarial Networks</em> (<a href="https://arxiv.org/abs/1611.07004">https://arxiv.org/abs/1611.07004</a>) and <em class="italic">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</em> (<a href="https://arxiv.org/pdf/1711.09020.pdf">https://arxiv.org/pdf/1711.09020.pdf</a>) are well-known examples of image-to-image <span class="No-Break">GAN-based translators.</span></li>
<li><strong class="bold">Text-to-image translation</strong>: Another interesting application of GANs is to generate appealing images from a given short textual description of scenes and objects. As examples, check <em class="italic">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</em> (<a href="https://arxiv.org/abs/1612.03242">https://arxiv.org/abs/1612.03242</a>) and <span class="No-Break">DALL-E (</span><a href="https://openai.com/research/dall-e"><span class="No-Break">https://openai.com/research/dall-e</span></a><span class="No-Break">).</span></li>
</ul>
<p>In addition to the applications <a id="_idIndexMarker342"/>we have discussed, GANs can be used for the following non-exhaustive list of interesting tasks <span class="No-Break">and applications:</span></p>
<ul>
<li>Semantic <span class="No-Break">image-to-photo translation</span></li>
<li>Generate photographs of <span class="No-Break">human faces</span></li>
<li><span class="No-Break">Face aging</span></li>
<li>Pose guided person <span class="No-Break">image generation</span></li>
<li>Photos <span class="No-Break">to emojis</span></li>
<li><span class="No-Break">Photograph editing</span></li>
<li><span class="No-Break">Image blending</span></li>
<li><span class="No-Break">Image inpainting</span></li>
<li><span class="No-Break">Super-resolution</span></li>
<li><span class="No-Break">Video prediction</span></li>
<li>3D <span class="No-Break">object generation</span></li>
<li><span class="No-Break">Texture synthesis</span></li>
<li><span class="No-Break">Anomaly detection</span></li>
</ul>
<p>Next, we will delve into a hands-on example<a id="_idIndexMarker343"/> that demonstrates the practical<a id="_idIndexMarker344"/> application of GANs for generating photorealistic <span class="No-Break">synthetic images.</span></p>
<h1 id="_idParaDest-126"><a id="_idTextAnchor129"/>Hands-on GANs in practice</h1>
<p>Let’s examine how we can utilize a GAN<a id="_idIndexMarker345"/> to generate some synthetic images in practice. We will examine <em class="italic">Closed-Form Factorization of Latent Semantics in GANs</em> (<a href="https://arxiv.org/abs/2007.06600">https://arxiv.org/abs/2007.06600</a>) to learn how we can simply generate synthetic images for our ML problem. The code for this example was adapted from the paper’s original <span class="No-Break">GitHub (</span><a href="https://github.com/genforce/sefa"><span class="No-Break">https://github.com/genforce/sefa</span></a><span class="No-Break">).</span></p>
<p>We begin by importing the essential libraries <span class="No-Break">as shown:</span></p>
<pre class="source-code">
# import the required libraries
import cv2
import torch
import numpy as np
from utils import to_tensor
from utils import postprocess
from utils import load_generator
from models import parse_gan_type
from utils import factorize_weight
from matplotlib import pyplot as plt</pre> <p>Then, we select the parameters of the generation process such as the number of images to generate, and the noise seed. Please note that the <strong class="source-inline">seed</strong> parameter will help us to get diverse images in <span class="No-Break">this example:</span></p>
<pre class="source-code">
num_samples = 1 # num of image to generate (min:1, max:8)
noise_seed = 186 # noise seed (min:0, max:1000)</pre> <p>Next, we have the latent semantics<a id="_idIndexMarker346"/> parameters of the GAN as proposed by <strong class="bold">SeFa</strong>. Simply, we can change some semantics of the synthesized image by changing these parameters. For example, we can change the painting style, gender, posture, and other semantics<a id="_idIndexMarker347"/> of the generated image. For more details about <strong class="bold">SeFa</strong>, please refer to <em class="italic">Closed-Form Factorization of Latent Semantics in </em><span class="No-Break"><em class="italic">GANs</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2007.06600"><span class="No-Break">https://arxiv.org/abs/2007.06600</span></a><span class="No-Break">):</span></p>
<pre class="source-code">
# params of generation
layer_idx = "0-1"  # ['all', '0-1', '2-5', '6-13']
semantic_1 = -1.4  # min:-3.0, max:3.0
semantic_2 = -2.9  # min:-3.0, max:3.0
semantic_3 = -1.2  # min:-3.0, max:3.0
semantic_4 = 0.2    # min:-3.0, max:3.0
semantic_5 = -1.4  # min:-3.0, max:3.0</pre> <p>Now, we have the <span class="No-Break">following models:</span></p>
<ul>
<li><strong class="source-inline">stylegan_animeface512</strong>: This can be used to generate anime faces with diverse expressions. For more details, please refer to <em class="italic">A Style-Based Generator</em> <em class="italic">Architecture for Generative Adversarial </em><span class="No-Break"><em class="italic">Networks</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1812.04948"><span class="No-Break">https://arxiv.org/abs/1812.04948</span></a><span class="No-Break">).</span></li>
<li><strong class="source-inline">stylegan_car512</strong>: This can be utilized to generate<a id="_idIndexMarker348"/> interesting car models. We will use this model in <span class="No-Break">our example.</span></li>
<li><strong class="source-inline">stylegan_cat256</strong>: We can leverage this model to generate photorealistic <span class="No-Break">cat images.</span></li>
<li><strong class="source-inline">pggan_celebahq1024</strong>: This is a <strong class="bold">Progressive Growing GAN</strong> (<strong class="bold">PGGAN</strong>) that was trained to generate photorealistic<a id="_idIndexMarker349"/> celebrity images. For more details, please refer to <em class="italic">Progressive Growing of GANs for Improved Quality, Stability, and </em><span class="No-Break"><em class="italic">Variation</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1710.10196"><span class="No-Break">https://arxiv.org/abs/1710.10196</span></a><span class="No-Break">).</span></li>
<li><strong class="source-inline">stylegan_bedroom256</strong>: This can be deployed to generate bedroom layout images. For more details, please refer to <em class="italic">Analyzing and Improving the Image Quality of </em><span class="No-Break"><em class="italic">StyleGAN</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1912.04958"><span class="No-Break">https://arxiv.org/abs/1912.04958</span></a><span class="No-Break">).</span></li>
</ul>
<p>We select the model name that we want <span class="No-Break">to test:</span></p>
<pre class="source-code">
# select model name, in this example we use "stylegan_car512"
model_name = 'stylegan_car512'</pre> <p>Next, we need to load the generator of GAN. Please remember that we do not need the discriminator to generate the images. It is only used to help the generator to train on generating the images that <span class="No-Break">we want:</span></p>
<pre class="source-code">
# load the pretrained model
generator = load_generator(model_name)</pre> <p>Now, we send the code to the generator to sample from the latent space. The code is simply random noise. It is the random noise vector, <span class="_-----MathTools-_Math_Variable">z</span>, which we saw in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
<pre class="source-code">
codes = torch.randn(num, generator.z_space_dim).cuda()</pre> <p>Then, we synthetize<a id="_idIndexMarker350"/> the image by sending the noise <span class="No-Break">vector (code):</span></p>
<pre class="source-code">
# generate the synthetic image from the code
images = synthesize(generator, gan_type, codes)</pre> <p>Now, let us visualize the output of the GAN in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 7.2 – Images generated using StyleGAN" height="455" src="image/Figure_07_02_B18494.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Images generated using StyleGAN</p>
<p>After changing the latent<a id="_idIndexMarker351"/> semantic parameters as described by <strong class="bold">SeFa</strong>, we get the outputs shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 7.3 – SeFa approach for controlling generation process by changing latent semantic parameters" height="283" src="image/Figure_07_03_B18494.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – SeFa approach for controlling generation process by changing latent semantic parameters</p>
<p>In the same way, we can generate<a id="_idIndexMarker352"/> images of anime facial expressions, celebrities’ faces, and bedroom layouts using the aforementioned models, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 7.4 – A sample of images generated using different GAN models" height="579" src="image/Figure_07_04_B18494.jpg" width="1101"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – A sample of images generated using different GAN models</p>
<p>As we have seen in this example, we can effortlessly utilize GANs to generate diverse and photorealistic data for training and testing our own ML models. Next, we will explore the variations of GANs<a id="_idIndexMarker353"/> that facilitate many <span class="No-Break">amazing applications.</span></p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor130"/>Variations of GANs</h1>
<p>In this section, we will explore<a id="_idIndexMarker354"/> the main variation of GANs. For an interesting practical application of GANs, please refer to <a href="B18494_12.xhtml#_idTextAnchor203"><span class="No-Break"><em class="italic">Chapter 12</em></span></a> and <em class="italic">Case Study 3 – Predictive Analytics</em> to see how Amazon utilized GANs for fraud transaction<a id="_idIndexMarker355"/> prediction. For more applications, please refer to <em class="italic">Generative Adversarial Networks in the built environment: A comprehensive review of the application of GANs across data types and </em><span class="No-Break"><em class="italic">scales</em></span><span class="No-Break"> (</span><a href="https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089"><span class="No-Break">https://www.sciencedirect.com/science/article/abs/pii/S0360132322007089</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor131"/>Conditional GAN (cGAN)</h2>
<p>A typical GAN generates<a id="_idIndexMarker356"/> images given a random noise vector. However, in many scenarios, we really want to control the attributes and properties of the generated synthetic samples. For example, suppose you are deploying a GAN to generate human faces. The standard GAN architecture has no way to let you specify some attributes of the generated faces such as gender, age, eye color, and hair length. Using cGAN, we can condition the GAN on these attributes in the training process. Thus, we are able to generate synthetic samples with certain attributes. For more details, refer to <em class="italic">Conditional Generative Adversarial Nets</em> <span class="No-Break">at </span><a href="https://arxiv.org/abs/1411.1784"><span class="No-Break">https://arxiv.org/abs/1411.1784</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor132"/>CycleGAN</h2>
<p>In an image-to-image translation task<a id="_idIndexMarker357"/> that aims to transform an image from one domain to another, DL models usually require matching pairs or pairwise correspondences between images from the two domains. This is extremely difficult to achieve. For instance, imagine preparing such a dataset for mapping images from one season (winter) to another (summer). An elegant solution to the problem is using CycleGANs, which can be trained to perform unpaired image-to-image translation between domains given only two sets of images from both domains without the need for any matching pairs. Thus, you only need to provide images taken in winter and summer and there is no need to capture the same scenes in winter and summer to provide matching pairs. For more details, please check <em class="italic">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial </em><span class="No-Break"><em class="italic">Networks</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1703.10593"><span class="No-Break">https://arxiv.org/abs/1703.10593</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-130"><a id="_idTextAnchor133"/>Conditional Tabular GAN (CTGAN)</h2>
<p>CTGANs are a specific variant of GANs<a id="_idIndexMarker358"/> that can generate tabular synthetic data. It is very challenging for other GANs to capture the dependencies between columns or attributes of a given tabular dataset. A CTGAN is a cGAN that can be utilized to model these joint probability distributions between these columns. CTGANs have enormous applications in data augmentation, imputation, and anomaly<a id="_idIndexMarker359"/> detection. For more details, please refer to <em class="italic">Modeling Tabular Data using Conditional </em><span class="No-Break"><em class="italic">GAN</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1907.00503"><span class="No-Break">https://arxiv.org/abs/1907.00503</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor134"/>Wasserstein GAN (WGAN) and Wasserstein GAN with Gradient Penalty (WGAN-GP)</h2>
<p>WGAN and WGAN-GP<a id="_idIndexMarker360"/> are variants of the original GANs. Unlike GANs, which use<a id="_idIndexMarker361"/> a binary cross-entropy loss to classify real and fake samples, this variation utilizes Wasserstein distance to measure the distance between the real and fake data probability distributions. Furthermore, WGAN-GP implements a gradient penalty term to enforce the Lipschitz constraint on the discriminator. These two variants were shown to produce better results and to be more stable. For more details, check <em class="italic">Wasserstein GAN</em> (<a href="https://arxiv.org/abs/1701.07875">https://arxiv.org/abs/1701.07875</a>) and <em class="italic">Improved Training of Wasserstein </em><span class="No-Break"><em class="italic">GANs</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1704.00028"><span class="No-Break">https://arxiv.org/abs/1704.00028</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor135"/>f-GAN</h2>
<p>f-GANs are another family of GANs<a id="_idIndexMarker362"/> that utilize <em class="italic">f</em>-divergences to measure and minimize the divergence between real and fake samples’ probability distributions. This variant of GANs has been widely utilized in image and text generation. For more details, please check <em class="italic">f-GAN: Training Generative Neural Samplers using Variational Divergence </em><span class="No-Break"><em class="italic">Minimization</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1606.00709"><span class="No-Break">https://arxiv.org/abs/1606.00709</span></a><span class="No-Break">).</span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor136"/>DragGAN</h2>
<p>DragGANs are another recent promising<a id="_idIndexMarker363"/> variation of GANs that open the door for many amazing applications, such as point-based image editing. DragGANs allow users to generate photorealistic synthetic images in an interactive and intuitive manner. DragGANs stand out due to their distinctive approach to optimizing the latent space and their unique method<a id="_idIndexMarker364"/> of point tracking. For more information, please refer to <em class="italic">Drag Your GAN: Interactive Point-based Manipulation on the Generative Image </em><span class="No-Break"><em class="italic">Manifold</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2305.10973"><span class="No-Break">https://arxiv.org/abs/2305.10973</span></a><span class="No-Break">).</span></p>
<p>Let’s wrap things up before we <span class="No-Break">move on.</span></p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor137"/>Summary</h1>
<p>In this chapter, we have discussed what GANs are, their architecture, and the training process. At the same time, we explored how GANs were utilized for various applications such as image-to-image translation. Additionally, we covered a coding example demonstrating how to use GANs to generate photorealistic images. In this chapter, we also learned about the main variations of GANs. In the next chapter, we will continue our learning journey by exploring another exciting approach for generating synthetic data by utilizing <span class="No-Break">video games.</span></p>
</div>
</div></body></html>