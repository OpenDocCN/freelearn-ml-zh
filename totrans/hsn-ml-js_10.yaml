- en: Natural Language Processing in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language processing is the science (and art) of parsing, analyzing,
    and reconstructing natural language, such as written or spoken English, French,
    or German. It's not an easy task; **natural language processing** (**NLP**) is
    an entire field of research with a vibrant academic research community and significant
    financial backing from major tech firms. Every time companies such as Google,
    Apple, Amazon, and Microsoft invest in their Google Assistant, Siri, Alexa, and
    Cortana products, the field of NLP gets a little more funding. In short, NLP is
    why you can talk to your phone and your phone can talk back to you.
  prefs: []
  type: TYPE_NORMAL
- en: Siri is a lot more than NLP. We, as consumers, like to criticize our **artificial
    intelligence** (**AI**) assistants when they get something laughably wrong. But
    they are truly marvels of engineering, and the fact that they get *anything *right
    is a wonder!
  prefs: []
  type: TYPE_NORMAL
- en: 'If I look at my phone and say, *Ok Google, give me directions to 7-Eleven*,
    my phone will automatically wake up and respond to me, *Ok, going to 7-Eleven
    on Main Ave, make the next right*. Let''s think about what that takes to accomplish:'
  prefs: []
  type: TYPE_NORMAL
- en: My sleeping phone is monitoring for my pre-trained OK Google catchphrase.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The audio buffer gets an audio hash match on the OK Google soundwaves that I
    trained, and wakes up the phone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The phone starts capturing audio, which is just a time-series vector of numbers
    (representing sound wave intensity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speech audio is decoded to phonemes, or textual representation of phonetic
    sounds. Several candidates for each utterance are generated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The candidate phonemes are combined together to try to make words. The algorithm
    uses a max-likelihood or other estimator to figure out which of the combinations
    is most likely to be a real sentence that would be used in the current context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The resultant sentence must be parsed for meaning, so many types of preprocessing
    are performed, and each word is tagged with its possible **parts of speech** (**POS**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning system (typically an ANN) will try to determine intent given the
    phrase's subject, object, and verb.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actual intent must be carried out by a subroutine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A response to the user must be formulated. In the case where the response can't
    be scripted, it must be generated algorithmically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A text-to-speech algorithm decodes the response into phonemes and must then
    synthesize natural-sounding speech, which then plays over your phone's speakers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations, you're on your way to getting your Slurpee! Your experience
    is powered by several ANNs, many different uses of various NLP tools, giant corpuses
    of data, and millions of engineer-hours of effort to build and maintain. This
    experience also explains the close relationship between NLP and ML—they are not
    the same thing, but they're partnered together at the forefront of technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, there''s more to NLP than the topics we can cover in 25 pages. This
    chapter doesn''t aim to be comprehensive; its aim is to familiarize you with the
    most common tactics you''ll employ when solving ML problems that involve natural
    language. We''ll take a whirlwind tour of seven NLP-related concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring string distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TF-IDF metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phonetics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parts of speech tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embedding with Word2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Don''t worry if those topics look daunting. We''ll introduce each one in turn
    and show lots of examples. There''s a lot of jargon involved in NLP, and many
    edge cases, so the topic seems unapproachable at first glance. But the topic is
    *natural language* after all: we speak it every day! Once we learn the jargon,
    the topic becomes quite intuitive, because we all have a very strong intuitive
    understanding of language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start our discussion with a simple question: how do you measure the
    distance between *quit *and *quote*? We already know that we can measure the distance
    between two points in space, so now let''s take a look at measuring the distance
    between two words.'
  prefs: []
  type: TYPE_NORMAL
- en: String distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is always convenient to be able to measure some form of distance between
    two points. In previous chapters, we used the distance between points to aid in
    clustering and classification. We can do the same for words and passages in NLP.
    The problem, of course, is that words are made up of letters, and distances are
    made up of numbers—so how do we make a number out of two words?
  prefs: []
  type: TYPE_NORMAL
- en: Enter Levenshtein distance*—*a simple metric that measures the number of single-character
    edits it would take to transform one string into the other. The Levenshtein distance
    allows insertions, deletions, and substitutions. A modification of the Levenshtein
    distance, called the **Damerau-Levenshtein distance**, also allows transpositions,
    or the swapping of two neighboring letters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this concept with an example, let''s try transforming the word
    **crate** into the word **plate**:'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the **r** with **l **to get **clate**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the **c **with a **p **to get **plate**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Levenshtein distance between crate and plate is therefore 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distance between **plate** and **laser** is 3:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete the **p** to get **late**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insert an **r** to get **later**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the **t** with an **s** to get **laser**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s confirm these examples in code. Create a new directory called `Ch10-NLP`
    and add the following `package.json` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then issue `yarn install` from the command line to install the dependencies.
    This `package.json` file is a little different from the one in previous chapters,
    because the `wordnet-db` dependency is not compatible with the Browserify bundler.
    We will therefore have to omit some advanced JavaScript features in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory called `src` and add to it an `index.js` file to which you''ll
    add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You'll use these imports for the rest of the chapter, so keep them in the `index.js`
    file. However, the rest of the code we use in this chapter will be fungible; you
    may delete old irrelevant code as you work through the examples in this chapter
    if you wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at Levenshtein distance using the `natural.js` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run `yarn start` from the command line and you''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Try experimenting with a few pairs of words and see if you can calculate the
    distances in your head to get an intuitive feel for it.
  prefs: []
  type: TYPE_NORMAL
- en: The Levenshtein distance has many uses, since it is a metric and not any specific
    tool. Other systems, such as spellcheckers, suggester's, and fuzzy matcher's,
    use Levenshtein, or edit distance metrics in their own algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a more advanced metric: the TF-IDF score, which represents
    how interesting or important a particular word is among a set of documents.'
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency - inverse document frequency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most popular metrics used in search relevance, text mining, and information
    retrieval is the **term frequency-inverse document frequency** (**TF-IDF**) score.
    In essence, TF-IDF measures how significant a word is to a particular document. The
    TF-IDF metric therefore only makes sense in the context of a word in a document
    that's part of a larger corpus of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have a corpus of documents, such as blog posts on varying topics,
    that you want to make searchable. The end user of your application runs a search
    query for *fashion style*. How do you then find matching documents and rank them
    by relevance?
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF score is made of two separate but related components. The first is *term
    frequency*, or the relative frequency of a specific term in a given document.
    If a 100-word blog post contains the word *fashion *four times, then the term
    frequency of the word *fashion* is 4% for that one document.
  prefs: []
  type: TYPE_NORMAL
- en: Note that term frequency only requires a single term and a single document as
    parameters; the full corpus of documents is not required for the term frequency
    component of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency by itself is not sufficient to determine relevance, however.
    Words such as *this* and *the* appear very frequently in most text and will have
    high term frequencies, but those words are not typically relevant to any search.
  prefs: []
  type: TYPE_NORMAL
- en: 'We therefore introduce a second metric to our calculation: inverse document
    frequency. This metric is essentially the inverse of the percentage of documents
    that a given word appears in. If you have 1,000 blog posts, and the word *fashion*
    appears in 50 of them, the (non-inverse) document frequency of that word is 5%.
    The inverse document frequency is an extension of this concept, given by taking
    the log of the inverse document frequency.'
  prefs: []
  type: TYPE_NORMAL
- en: If n[fashion ]is the number of documents containing the word *fashion* and *N*
    is the total number of documents, then the inverse document frequency is given
    by *log(N / n[fashion])*. In our example, the inverse document frequency of the
    word *fashion* is roughly 1.3.
  prefs: []
  type: TYPE_NORMAL
- en: If we now consider the word *the*, which may appear in 90% of documents, we
    find that the inverse document frequency of *the* is 0.0451, much smaller than
    the 1.3 we got for *fashion*. The inverse document frequency therefore measures
    how rare or unique a given word is across a set of documents; higher values mean
    the word is more rare. The parameters required to calculate inverse document frequency
    are the term itself and the corpus of documents (unlike term frequency, which
    only requires one document).
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF score is calculated by multiplying the term frequency and inverse
    document frequency together. The result is a single metric that encapsulates how
    significant or interesting a single term is to a specific document, considered
    across all documents that you've seen. Words such as *the* and *that* may have
    high term frequencies in any one document, but because they are prevalent across
    all documents, their overall TF-IDF score will be very low. Words, such as *fashion*,
    that exist only in a subset of documents will have a higher TF-IDF score. When
    comparing two separate documents that both contain the word *fashion*, the document
    that uses it more often will have a higher TF-IDF score, as the inverse document
    frequency portion will be the same for both documents.
  prefs: []
  type: TYPE_NORMAL
- en: When scoring search results for relevance, the most common approach is to calculate
    the TF-IDF scores for each term in the search query and for each document in the
    corpus. The individual TF-IDF scores for each query term can be added together,
    and the resultant sum can be called the **relevance score** of that particular
    document. Once all matching documents are scored in this manner, you can sort
    by relevance and display them in that order. Most full-text search systems, such
    as Lucene and Elasticsearch, use this sort of approach to relevance scoring.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in practice, using the `natural.js` TF-IDF tool. Add the following
    to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code defines a `fulltextSearch` function that accepts a search query and
    an array of documents to be searched. Each document is added to the TF-IDF database
    object, where it is automatically tokenized by `natural.js`. Run the program with `yarn
    start` and you''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first two documents, which have nothing to do with fashion or style, return
    scores of zero. The term frequency component for both *fashion* and *style* in
    those documents is zero, so the overall score becomes zero. The third document
    also has a score of zero. This document does make a reference to fashion, however,
    the tokenizer was not able to reconcile the word *fashionable* with *fashion*,
    as no stemming has been performed. We'll discuss both tokenization and stemming
    in depth in the later sections of this chapter, but for now it's sufficient to
    know that *stemming *is an operation that reduces a word to its root form.
  prefs: []
  type: TYPE_NORMAL
- en: Documents three and four have non-zero scores. Document three has a higher score
    because it includes both the words *fashion* and *style*, whereas document four
    only includes the word *style*. This simple metric has done a surprisingly good
    job of capturing relevance, which is why it's so widely used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s update our code to add a stemming operation. After applying stemming
    to the text, we would expect document two to also have a non-zero relevance score,
    since *fashionable* should be transformed to *fashion* by the stemmer. Add the
    following code to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We have added a `stemAndTokenize` helper method and applied it both to the
    documents added to the database and to the search query. Run the code with `yarn
    start` and you''ll see the updated output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As expected, document two now has a non-zero score because the stemmer was able
    to transform the word *fashionable* into *fashion*. Documents two and four have
    the same score, but only because this is a very simple example; with a much larger
    corpus we would not expect the inverse document frequencies of the terms *fashion*
    and *style* to be equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Search relevance and ranking is not the only application for TF-IDF. This metric
    is widely used across many use cases and problem domains. One interesting use
    for TF-IDF is article summarization. In article summarization, the goal is to
    reduce a written passage to only a few sentences that effectively summarize the
    passage.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to the article summarization problem is to consider each sentence
    or paragraph in an article to be a separate document. After indexing each sentence
    for TF-IDF, you then evaluate each individual word's TF-IDF score and use that
    to score each sentence as a whole. Pick the top three or five sentences and display
    them in their original order, and you will have a decent summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see this in action, using both `natural.js` and `compromise.js`. Add
    the following code to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding `summarize` method implements the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `compromise.js` to extract sentences from the article
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add each individual sentence to the TF-IDF database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each word in the article, calculate its TF-IDF score for each sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add each word's TF-IDF score to a list of total scores for each sentence (the `scoresMap`
    object)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert `scoresMap` into an array to make sorting easier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sort `scoresArray` by descending relevance score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove all but the top-scoring sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-sort `scoresArray` by the chronological order of sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build the summary by joining the top sentences together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s add a simple article to the code and try out both a three-sentence and
    a five-sentence summary. In this example, I''ll use the first few paragraphs of
    this section, but you can replace the text with anything you like. Add the following
    to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the code with `yarn start`, you''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The quality of these summaries illustrates both the power and flexibility of
    the `tf-idf metric`, while also highlighting the fact that you don't always need
    advanced ML or AI algorithms to accomplish interesting tasks. There are many other
    uses of TF-IDF, so you should consider using this metric any time you need the
    relevance of a word or term as it pertains to a document in a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we made use of tokenizers and stemmers without formally introducing
    them. These are core concepts in NLP, so let's now introduce them formally.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenizing is the act of transforming an input string, such as a sentence, paragraph,
    or even an object such as an email, into individual *tokens*. A very simple tokenizer
    might take a sentence or paragraph and split it by spaces, thus generating tokens
    that are individual words. However, tokens do not necessarily need to be words,
    nor does every word in an input string need to be returned by the tokenizer, nor
    does every token generated by the tokenizer need to be present in the original
    text, nor does a token need to represent only one word. We therefore use the term
    *token* rather than *word* to describe the output of a tokenizer, as tokens are
    not always words.
  prefs: []
  type: TYPE_NORMAL
- en: The manner in which you tokenize text before processing it with an ML algorithm
    has a major effect on the performance of the algorithm. Many NLP and ML applications
    use a *bag-of-words* approach, in which only the words or tokens matter but their
    order does not, as in the Naive Bayes classifier we explored in [Chapter 5](8ba34275-43c8-4d0c-a8d3-8e0dd89dd2f9.xhtml), *Classification
    Algorithms*. However, a tokenizer that generates *bigrams*, or pairs of words
    found next to each other, will actually preserve some of the positional and semantic
    meaning of the original text even when used with a bag-of-words algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to tokenize text. As mentioned, the simplest method is to
    split a sentence by spaces to generate a *token stream *of individual words. There
    are numerous problems with the simple approach, however. For one, the algorithm
    will treat capitalized words as being distinct from their lowercase versions;
    Buffalo and buffalo are considered two separate words or tokens. Sometimes this
    is desirable, other times it's not. Oversimplified tokenization will also treat
    contractions such as *won't* as being distinct and separate from the words *will
    not*, which will get split into two separate tokens, *will and not*.
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, that is, in 80% of applications, the simplest tokenization that
    one should consider is a tokenizer that converts all text to lowercase, removes
    punctuation and newlines, removes formatting and markup such as HTML, and even
    removes *stopwords* or common words such as *this* or *the*. In other cases, more
    advanced tokenization is necessary, and in some cases, simpler tokenization is
    desired.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I've been describing the act of tokenization as a compound
    process, including case transformations, removing non-alphanumeric characters,
    and stopword filtering. However, tokenizer libraries will each have their own
    opinion as to what the roles and responsibilities of the tokenizer are. You may
    need to combine a library's tokenization tool with other tools in order to achieve
    the desired effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s build our own simple tokenizer. This tokenizer will convert a
    string to lowercase, remove non-alphanumeric characters, and also remove words
    that are fewer than three characters in length. Add the following to your `index.js`
    file, either replacing the Levenshtein distance code or adding beneath it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This `simpleTokenizer` will convert the string to lowercase, remove apostrophes
    in the middle of a word (so that *won't* becomes *wont*), and filter out all other
    non-word characters by replacing them with spaces. It then splits the string by
    the space character, returning an array, and finally removes any items that have
    fewer than three characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `yarn start` and you''ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This token stream can then be given to an algorithm, either in an ordered or
    unordered fashion. A classifier, such as Naive Bayes, will ignore the order and
    analyze each word as if it were independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare our simple tokenizer to two tokenizers provided by `natural.js` and `compromise.js`.
    Add the following to your `index.js` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code with `yarn start` will yield the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, short words have been preserved, and contractions, such as *I've*,
    have been split up into separate tokens. Additionally, capitalization has been
    preserved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another one of `natural.js` tokenizers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This tokenizer continues to split on punctuation, however, the punctuation itself
    is preserved. In applications where punctuation is important, this may be desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other tokenizer libraries, such as the one in `compromise.js`, take a more
    intelligent approach and even perform POS tagging in order to parse and understand
    the sentence while tokenizing. Let''s try a number of `compromise.js` tokenizing
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the new code with `yarn start` and you''ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `words()` tokenizer does not split contractions apart like the `natural.js`
    tokenizer did. Additionally, `compromise.js` gives you the capability to extract
    specific entity types from the text. We can separately extract adjectives, nouns,
    verbs, questions, contractions (even with the capability to expand contractions);
    we can also use `compromise.js` to extract dates, hashtags, lists, clauses, and
    numerical values.
  prefs: []
  type: TYPE_NORMAL
- en: It is also not a requirement that your tokens must map directly to words and
    phrases in the input text. For instance, when developing a spam filter for an
    email system, you might find that including some data from the email header in
    the token stream gives you a huge accuracy improvement. Whether the email passes
    SPF and DKIM checks may be a very strong signal to your spam filter. You might
    also find that differentiating body text from the subject line is also advantageous;
    it may also be the case that words that appear as hyperlinks are stronger signals
    than plaintext.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, the simplest way to tokenize this type of semi-structured data is to
    prefix the tokens with a character or set of characters that normally would not
    be allowed by the tokenizer. For example, tokens in the subject line of an email
    may be prefixed by `_SUBJ:` and tokens that appear in hyperlinks may be prefixed
    by `_LINK:`. To illustrate this, here''s an example of what a token stream might
    look like for an email:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Even if a Naive Bayes classifier has never seen references to pharmaceuticals
    before, it may see that most spam emails have failed their DKIM check and still
    flag this message as spam. Or perhaps you work closely with the accounting department
    and they often get emails about payments, but almost never receive a legitimate
    email with the word `pay` in a hyperlink to an external site; the differentiation
    of the *pay* token appearing in plaintext versus the `_LINK:pay` token appearing
    in a hyperlink may make all the difference between an email being classified as
    spam or not.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, one of the earliest spam filtering breakthroughs, developed by Paul
    Graham, of Y Combinator fame, used this approach of annotated email tokens to
    mark a significant improvement in the accuracy of early spam filters.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to tokenization is *n-gram* tokenization, which splits an input
    string into N-sized groups of neighboring tokens. In fact, all tokenization is
    n-gram tokenization, however, in the preceding examples, N is set to 1\. More
    typically, n-gram tokenization refers to schemes where N > 1\. Most commonly,
    you'll encounter *bigram* and *trigram* tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of bigram and trigram tokenization is to preserve some context around
    individual words. An example related to sentiment analysis is an easy visualization.
    The phrase *I did not love the movie* will be tokenized (with a *unigram* tokenizer,
    or n-gram tokenizer where N = 1) to *I*, *did*, *not*, *love*, *the*, *movie*.
    When using a bag-of-words algorithm, such as Naive Bayes, the algorithm will see
    the word *love* and guess that the sentence has a positive sentiment, as bag-of-words
    algorithms do not consider the relationships between words.
  prefs: []
  type: TYPE_NORMAL
- en: A bigram tokenizer, on the other hand, can trick a naive algorithm into considering
    the relationships between words, because every *pair* of words becomes a token.
    The preceding phrase, processed with a bigram tokenizer, will become *I did*,
    *did not*, *not love*, *love the*, *the movie*. Even though each token is composed
    of two individual words, the algorithm operates on tokens and therefore will treat
    *not love* differently from *I love*. A sentiment analyzer will therefore have
    more context around each word and will be able to distinguish negations (*not
    love*) from positive phrases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try out the `natural.js` bigram tokenizer on our earlier example sentence.
    Add the following code to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code with `yarn start` will yield:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The biggest issue with n-gram tokenization is that it dramatically increases
    the entropy of the data domain. When training an algorithm on n-grams, you're
    not just on the hook for making sure the algorithm learns all the significant
    words, but also all the significant *pairs *of words. There are many more pairs
    of words than there are unique words, so n-gram tokenization will only work when
    you have a very large and comprehensive training set.
  prefs: []
  type: TYPE_NORMAL
- en: One clever way around the n-gram entropy issue, particularly for dealing with
    negations in sentiment analysis, is to transform the token immediately following
    the negation in the same way we handled email headers and subject lines. For example,
    the phrase *not love* can be tokenized as *not*, *_NOT:love*, or *not*, *!love*,
    or even just *!love* (discarding *not* as an individual token).
  prefs: []
  type: TYPE_NORMAL
- en: Under this scheme, the phrase *I did not love the movie* will get tokenized
    as *I*, *did*, *not*, *_NOT:love*, *the*, *movie*. The advantage of this approach
    is that the contextual negation still gets preserved, but in general we are still
    using low-entropy unigrams that can be trained with a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to tokenize text, and each has its advantages and disadvantages.
    As always, the approach you choose will depend on the task at hand, the training
    data available, and the problem domain itself.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the topic of tokenization in mind throughout the next few sections, as
    those topics can also be applied to the tokenization process. For example, you
    can stem words after tokenizing to further reduce entropy, or you can filter your
    tokens by their TF-IDF score, therefore only using the most interesting words
    in a document.
  prefs: []
  type: TYPE_NORMAL
- en: In order to continue our discussion about entropy, let's take a moment to discuss *stemming*.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming is a type of transformation that can be applied to a single word, though
    typically the stemming operation occurs right after tokenizing. Stemming after
    tokenizing is so common that `natural.js` offers a `tokenizeAndStem` convenience
    method that can be attached to the `String` class prototype.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, stemming reduces a word to its root form, for instance by transforming
    *running* to *run*. Stemming your text after tokenizing can significantly reduce
    the entropy of your dataset, because it essentially de-duplicates words with similar
    meanings but different tenses or inflections. Your algorithm will not need to
    learn the words *run*, *runs*, *running*, and *runnings* separately, as they will
    all get transformed into *run*.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular stemming algorithm, the *Porter* *stemmer*, is a heuristic
    algorithm that defines a number of staged rules for the transformation. But, in
    essence, it boils down to cutting the standard verb and noun inflections off the
    end of the word and dealing with specific edge cases and common irregular forms
    as they arise.
  prefs: []
  type: TYPE_NORMAL
- en: In one sense, stemming is a sort of compression algorithm that discards information
    about inflections and specific word forms, but retains the conceptual information
    left behind by the word root. Stemming should therefore not be used in cases where
    the inflection or form of the language itself is important.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the same reason, stemming excels in situations where the conceptual information
    is more important than the form. Topic extraction is a good example: it doesn''t
    matter if someone is writing about their own experiences as a runner versus their
    experience watching track races—they''re still writing about running.'
  prefs: []
  type: TYPE_NORMAL
- en: Because stemming reduces data entropy, it is very effectively employed when
    the dataset is small or modest in size. Stemming cannot be applied carelessly,
    however. A very large dataset may incur an accuracy penalty if you use stemming
    unnecessarily. You destroy information when you stem text, and models with very
    large training sets may have been able to use that extra information to generate
    better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, you should never have to guess whether your model will perform
    better with or without stemming: you should try both ways and see which performs
    better. I can''t tell you *when *to use stemming, I can only tell you why it works
    and why it sometimes doesn''t.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try out the `natural.js` Porter stemmer, and we''ll combine it with
    our tokenization from earlier. Add the following to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the code with `yarn start` and you''ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This simple example illustrates how words with different forms get reduced into
    their conceptual meanings. It also illustrates that there is no guarantee that
    the stemmer will create *real *words (you won't find `lucki` in the dictionary),
    only that it will reduce entropy for a set of similarly constructed words.
  prefs: []
  type: TYPE_NORMAL
- en: There are other stemmer algorithms that try to approach the problem more linguistically.
    That type of stemming is called **lemmatization**, and the analog to stems is
    called the **lemma**, or the dictionary form of a word. In essence, a lemmatizer
    is a stemmer that first determines the part of speech of the word (typically requiring
    a dictionary, such as *WordNet*), and then applies in-depth rules for that specific
    part of speech, potentially involving more lookup tables. As an example, the word
    *better* is unchanged by stemming, but it is transformed into the word *good*
    by lemmatization. Lemmatization is not necessary in most everyday tasks, but may
    be useful when your problem requires more precise linguistic rules or drastically
    reduced entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can''t discuss NLP or linguistics without also discussing the most common
    mode of communication: speech. How does a speech-to-text or text-to-speech system
    actually know how to say the hundreds of thousands of defined words in the English
    language, plus an arbitrary amount of names? The answer is *phonetics*.'
  prefs: []
  type: TYPE_NORMAL
- en: Phonetics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speech detection, such as those used in speech-to-text systems, is a surprisingly
    difficult problem. There are so many variations in styles of speaking, pronunciation,
    dialect, and accent, as well as variations in rhythm, tone, speed, and elocution,
    plus the fact that audio is a simple one-dimensional time-domain signal, that
    it's no surprise that even today's state-of-the-art smartphone tech is *good,
    not great*.
  prefs: []
  type: TYPE_NORMAL
- en: While modern speech-to-text goes much deeper than what I'll present here, I
    would like to show you the concept of *phonetic algorithms*. These algorithms
    transform a word into something resembling a phonetic hash, such that it is easy
    to identify words that sound similar to one another.
  prefs: []
  type: TYPE_NORMAL
- en: The *metaphone* algorithm is one such phonetic algorithm. Its aim is to reduce
    a word down to a simplified phonetic form, with the ultimate goal of being able
    to index similar pronunciations. Metaphone uses an alphabet of 16 characters: 0BFHJKLMNPRSTWXY.
    The 0 character represents the *th* sound, *X* represents a *sh* or *ch* sound,
    and the other letters are pronounced as usual. Nearly all vowel information is
    lost in the transformation, though some are preserved if they are the first sound
    in a word.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example illustrates where phonetic algorithms can be useful. Imagine
    that you're in charge of a search engine and people keep searching for *knowledge
    is power, France is bacon*. You, having familiarity with art history, would understand
    that it was actually Francis Bacon who said *knowledge is power*, and that your
    users have simply misheard the quote. You'd like to add a *Did you mean: **Francis
    Bacon ***link to your search results, but don't know how to approach the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at how the Metaphone algorithm would phoneticize the terms
    `France is Bacon` and `Francis Bacon`. Add the following to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run the code with `yarn start`, you''ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Francis has transformed into `FRNSS`, France has transformed into `FRNS`, and
    Bacon has transformed into `BKN`. Intuitively, these strings represent the most
    distinguishable sounds used to pronounced the word.
  prefs: []
  type: TYPE_NORMAL
- en: After phoneticizing, we can use the Levenshtein distance to measure the similarity
    between two words. If you ignore the space, *FRNSS BKN* and *FRNS IS BKN* only
    have a Levenshtein distance of one between them (the addition of the *I*); these
    two phrases therefore sound very similar. You can use this information, combined
    with the rest of the search term and a reverse lookup, to determine that `France
    is Bacon` is a plausible mispronunciation of `Francis Bacon`, and that `Francis
    Bacon` is actually the correct topic to present in your search results. Phonetic
    misspellings and misunderstandings, such as `France is Bacon`, are so common that
    we even use them in some spellchecker tools.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach is used in speech-to-text systems. The recording system does
    its best to capture the specific vowel and consonant sounds you make and uses
    a phonetic index (a reverse lookup of phonetics mapped to various dictionary words)
    to come up with a set of candidate words. Typically, a neural network will then
    determine which is the most likely combination of words considering both the confidence
    of the phonetic form and the semantic meaningfulness or meaninglessness of the
    resultant statements. The set of words that makes the most sense is what is presented
    to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `natural.js` library also provides a convenience method to compare two
    words, returning *true *if they sound alike. Try the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: When run, this will return `true` and then `false`.
  prefs: []
  type: TYPE_NORMAL
- en: You should consider using phonetic algorithms any time your problem involves
    pronunciation or working with similar-sounding words and phrases. This is usually
    restricted to more specialized fields, but speech-to-text and text-to-speech systems
    are becoming very popular, and you may find yourself needing to update your search
    algorithm for phonetic sound-alikes if users start interacting with your service
    by speech in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of speech systems, let's now take a look at POS tagging and how it
    can be used to extract semantic information from phrases—such as commands you
    might issue to your smartphone assistant.
  prefs: []
  type: TYPE_NORMAL
- en: Part of speech tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **part of speech** (**POS**) tagger analyzes a piece of text, such as a sentence,
    and determines each individual word's POS in the context of the sentence. The
    only way to accomplish this is with a dictionary lookup, so it is not an algorithm
    that can be developed from first principles alone.
  prefs: []
  type: TYPE_NORMAL
- en: A great use case for POS tagging is intent extraction from commands. For instance,
    when you say *Siri, please order me a pizza from John's pizzeria*, the AI system
    will tag the command with parts of speech in order to extract the subject, verb,
    object, and any other relevant details from the command.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, POS tagging is often used as a supporting tool for other NLP operations.
    Topic extraction, for instance, makes heavy use of POS tagging in order to separate
    people, places, and topics from verbs and adjectives.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that POS tagging is never perfect, due to the ambiguity of the
    English language in particular. Many words can be used both as a noun and a verb,
    so many POS taggers will return a list of candidate parts of speech for a given
    word. Libraries that perform POS tagging have a wide range of sophistication,
    ranging from simple heuristics, to dictionary lookups, to advanced models that
    attempt to determine the POS based on context.
  prefs: []
  type: TYPE_NORMAL
- en: The `compromise.js` library has a flexible POS tagger and matching/extraction
    system. The `compromise.js` library is unique in that it aims to be *good enough*
    but not comprehensive; it is trained on only the most common words in the English
    language, which is enough to give 80-90% accuracy for most cases while still being
    a fast and small library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the `compromise.js` POS tagging and matching in action. Add the
    following code to `index.js`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `compromise.js` allows us to extract just the verbs, or just the nouns
    (and other parts of speech) from the command. Running the code with `yarn start`
    will yield:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The POS tagger has identified `order` as the sole verb in the sentence; this
    information can then be used to load up the correct subroutine for making orders
    that's built into Siri's AI system. The extracted nouns can then be sent to the
    subroutine in order to determine what type of order to make and from where.
  prefs: []
  type: TYPE_NORMAL
- en: Impressively, the POS tagger has also identified `John's pizzeria` as a single
    noun, rather than considering the words `John's` and `pizzeria` to be separate
    nouns. The tagger has understood that `John's` is a possessive, and therefore
    applies to the word following it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use `compromise.js` to write parsing and extraction rules for common
    commands. Let''s try one out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the code with `yarn start` will yield:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The same matching selector is able to capture both of these commands, ignoring
    the addressee of the command (Siri or Google) through match groups (denoted with `[]`).
    Because both commands follow the verb-noun-noun pattern, both will match the selector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this selector by itself is not enough to build a full AI system
    such as Siri or Google Assistant. This tool would be used near the beginning of
    the AI system process in order to determine the user''s overall intent, based
    on predefined but flexible command formats. You could program a system to respond
    to phrases such as *Open my #Noun*, where the noun can be `calendar` or `email`
    or `Spotify`, or *Write an email to #Noun*, and so on. This tool can be used as
    a first step toward building your own speech or natural language command system,
    as well as for various topic-extraction applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we've discussed the foundational tools used in NLP.
    Many advanced NLP tasks use an ANN as part of the learning process, but for many
    novice practitioners, it's unclear exactly how words and natural language should
    be sent to the input layer of an ANN. In the next section, we will discuss *word
    embedding*, particularly the Word2vec algorithm, which can be used to feed words
    into an ANN and other systems.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding and neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this chapter, we've discussed various NLP techniques, particularly
    with regards to preprocessing text. In many use cases, we will need to interact
    with an ANN to perform the final analysis. The type of analysis is not relevant
    to this section, but imagine you're developing a sentiment analysis ANN. You appropriately
    tokenize and stem your training text, then, as you attempt to train your ANN on
    your preprocessed text, you realize you have no idea how to get words into a neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach is to map each input neuron in the network to an individual
    unique word. When processing a document, you can set the input neuron's value
    to the term frequency (or absolute count) of that word in the document. You'll
    have a network where one input neuron responds to the word *fashion*, another
    neuron responds to *technology*, another neuron responds to *food*, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This approach will work, but it has several drawbacks. The topology of an ANN
    must be defined in advance, so you must know how many unique words are in your
    training set before you start training the network; this will become the size
    of the input layer. This also means that your network is not capable of learning
    new words after it has been trained. To add a new word to the network, you must
    essentially build and train a new network from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, throughout a corpus of documents, you may encounter tens of thousands
    of unique words. This has a huge negative impact on the efficiency of the ANN,
    as you will need an input layer with, say, 10,000 neurons. This will dramatically
    increase the training time required by the network as well as the memory and processing
    requirements of the system.
  prefs: []
  type: TYPE_NORMAL
- en: The approach of one-word-per-neuron also intuitively feels inefficient. While
    your corpus contains 10,000 unique words, most of them will be rare and only appear
    in a few documents. For most documents, only a few hundred input neurons will
    be activated, with the others set to zero. This amounts to what is called a **sparse
    matrix** or **sparse vector**, or a vector where most of the values are zero.
  prefs: []
  type: TYPE_NORMAL
- en: A more evolved approach is therefore required when natural language interacts
    with ANNs. A family of techniques called *word embedding* can analyze a corpus
    of text and transform each word into a fixed-length vector of numerical values.
    The vector is a fixed-length representation of a word in much the same way that
    a hash (such as md5 or sha1) is a fixed-length representation of arbitrary data.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding confers several advantages, particularly when used with ANNs.
    Because the word vectors are fixed in length, the topology of the network can
    be decided beforehand and can also handle the appearance of new words after the
    initial training.
  prefs: []
  type: TYPE_NORMAL
- en: The word vectors are also *dense vectors*, meaning that you don't need 10,000
    input neurons in your network. A good value for the size of a word vector (and
    the size of the input layer) is somewhere between 100-300 items. This factor alone
    significantly reduces the dimensionality of your ANN and will allow for much faster
    training and convergence of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many word embedding algorithms to choose from, but the current state-of-the-art
    choice is the Word2vec algorithm, developed at Google. This particular algorithm
    also has another desirable trait: similar words will be clustered close to one
    another in terms of their vector representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we saw that we can use string distance to measure the
    typographical distance between two words. We can also use the string distance
    between two phonetic representations of words to measure how similar they sound.
    When using Word2vec, you can measure the distance between two word vectors to
    get the *conceptual *distance between two words.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2vec algorithm is itself a shallow neural network that trains itself
    on your corpus of text. The algorithm uses n-grams to develop a sense of the context
    between words. If the words *fashion* and *blogger* often appear next to each
    other in your corpus, Word2vec will assign similar vectors to those words. If
    *fashion* and *mathematics* rarely appear together, their resultant vectors will
    be separated by some distance. The distance between two word vectors therefore
    represents their conceptual and contextual distance, or how alike two words are
    in terms of their semantic content and context.
  prefs: []
  type: TYPE_NORMAL
- en: This trait of the Word2vec algorithm also confers its own efficiency and accuracy
    advantage to the ANN that ultimately processes the data, as the word vectors will
    activate similar input neurons for similar words. The Word2vec algorithm has not
    only reduced the dimensionality of the problem, but it has also added contextual
    information to the word embedding's. This additional contextual information is
    exactly the type of signal that ANNs are highly proficient at picking up on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a common workflow involving both natural language
    and ANNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize and stem all text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove stopwords from text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the appropriate ANN input layer size; use this value both for the
    input layer and the Word2vec dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Word2vec to generate word embedding's for your text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the word embedding's to train the ANN on your task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When evaluating a new document, tokenize, stem, and vectorize the document before
    passing it to the ANN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a word-embedding algorithm such as Word2vec will not only improve the
    speed and memory performance of your model, but it will likely also increase the
    accuracy of your model due to the contextual information that the Word2vec algorithm
    preserves. It should also be noted that Word2vec is, like n-gram tokenization,
    one possible way to trick a naive bag-of-words algorithm into taking word context
    into account, as the Word2vec algorithm itself uses n-grams to develop the embedding's.
  prefs: []
  type: TYPE_NORMAL
- en: While word embedding is used primarily in NLP, the same approach can be used
    in other fields, such as genetics and biochemistry. In those fields, it is sometimes
    advantageous to be able to vectorize sequences of proteins or amino acids such
    that similar structures will have similar vector embedding's.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural language processing is a rich field of study with many advanced techniques
    and wide applications in ML, computational linguistics, and artificial intelligence.
    In this chapter, however, we focused on the specific tools and tactics that are
    most prevalent in everyday ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques presented in this chapter are building blocks that can be mixed
    and matched in order to achieve many different outcomes. Using the information
    in this chapter alone, you can build a simple full-text search engine, an intent
    extractor for spoken or written commands, an article summarizer, and many other
    impressive tools. However, the most impressive applications of NLP arise when
    these techniques are combined with advanced learning models, such as ANNs and
    RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, you learned about word metrics, such as string distance and TF-IDF
    relevance scoring; preprocessing and dimensionality reduction techniques, such
    as tokenization and stemming; phonetic algorithms, such as the Metaphone algorithm;
    part of speech extraction and phrase parsing; and converting words to vectors
    using word embedding algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You have also been introduced, through numerous examples, to two excellent JavaScript
    libraries, `natural.js` and `compromise.js`, which can be used to easily accomplish
    most of the NLP tasks relevant to ML. You were even able to write an article summarizer
    in 20 lines of code!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss how everything you've learned so far can
    be put together in a real-time, user-facing JavaScript application.
  prefs: []
  type: TYPE_NORMAL
