- en: Evaluation and Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to have sustainable, responsible machine learning workflows and develop
    machine learning applications that produce true value, we need to be able to measure
    how well our machine learning models perform. We also need to ensure that our
    machine learning models generalize to data that they will see in production. If
    we don't do these things, we are basically shooting in the dark. We will have
    no understanding of the expected behavior of our models and we won't be able to
    improve them over time.
  prefs: []
  type: TYPE_NORMAL
- en: The process of measuring how a model is performing (with respect to certain
    data) is called **evaluation**. The process of ensuring that our model generalizes
    to data that we might expect to encounter is called **validation**. Both processes
    need to be present in every machine learning workflow and application, and we
    will cover both in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A basic tenet of science is measurement, and the science of machine learning
    is not an exception. We need to be able to measure, or evaluate, how well our
    models are performing, so we can continue to improve on them, compare one model
    to another, and detect when our models are behaving poorly.
  prefs: []
  type: TYPE_NORMAL
- en: There's only one problem. How do we evaluate how our models are performing?
    Should we measure how fast they can be trained or make inferences? Should we measure
    how many times they get the right answer? How do we know what the right answer
    is? Should we measure how far we deviated from the observed values? How do we
    measure that distance?
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there are a lot of decisions to make around how we evaluate
    our models. What really matters is the context. In some cases, efficiency definitely
    matters, but every machine learning context requires us to measure how our predictions,
    inferences, or results match the ideal predictions, inferences, or results. Thus,
    measuring this comparison between computed results and ideal results should always
    take priority over speed optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, there are some types of results that we will need to evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous**: Results such as total sales, stock price, and temperature that
    can take any continuous numerical value ($12102.21, 92 degrees, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical**: Results such as fraud/not fraud, activity, and name that can
    take one of a finite number of categories (fraud, standing, Frank, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these types of results have corresponding evaluation metrics that will
    be covered here. However, remember that your choice of evaluation metric depends
    on what you are trying to achieve with your machine learning model. There is no
    one-size-fits-all metric, and in some cases, you may even need to create your
    own metric.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say that we have a model that is supposed to predict some continuous
    value, like a stock price. Suppose that we have accumulated some predicted values
    that we can compare to actual observed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, how do we measure the performance of this model? Well, the first step
    would be taking the difference between the observed and predicted values to get
    an `error`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The error gives us a general idea of *how far off we were* from the value that
    we were supposed to predict. However, it's not really feasible or practical to
    look at all the error values individually, especially when there is a lot of data.
    There could be a million or more of these error values. Thus, we need a way to
    understand the errors in aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **mean squared error** (**MSE**) and **mean absolute error** (**MAE**)
    provide us with a view on errors in aggregate:'
  prefs: []
  type: TYPE_NORMAL
- en: MSE or **mean squared deviation** (**MSD**) is the average of the squares of
    all the errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAE is the average of the absolute values of all the errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both MSE and MAE give us a good overall picture of how good our predictions
    are, but they do have differences. As the MSE takes the squares of the errors,
    large error values (for example, corresponding to outliers) are emphasized more
    than in the MAE. In other words, MSE is more sensitive to outliers. MAE, on the
    other hand, maintains the same units as the variable that we are trying to predict,
    and is thus directly comparable to these values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this dataset, we can parse the observed and predicted values and calculate
    the MAE and MSE as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For our example data, this results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To judge if these are good values or not, we need to compare them to the values
    in our observed data. In particular, the MAE is `2.55` and the mean of our observed
    values is 14.0, so our MAE is about 20% of our mean value. Not very good, depending
    on the context.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the MSE and MAE, you will likely see **R-squared** (also known as
    **RÂ²** or **R2**), or the **coefficient of determination**, used as an evaluation
    metric for continuous variable models. R-squared also gives us a general idea
    about the deviations of our predictions, but the idea of R-squared is slightly
    different.
  prefs: []
  type: TYPE_NORMAL
- en: R-squared measures the proportion of the variance in the observed values that
    we capture in the predicted values. Remember that the values that we are trying
    to predict have some variability. For example, we might be trying to predict stock
    prices, interest rates, or disease progressions, which, by their very nature,
    aren't all the same. We are attempting to create a model that can predict this
    variability in the observed values, and the percentage of the variation that we
    capture is represented by R-squared.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conveniently, `gonum.org/v1/gonum/stat` has a built-in function to calculate
    R-squared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code for our example dataset results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, is this a good or bad R-squared? Remember that R-squared is a percentage
    and higher percentages are better. Here, we are capturing about 37% of the variance
    in the variable that we are trying to predict. Not very good.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say that we have a model that is supposed to predict some discrete value,
    such as fraud/not fraud, standing/sitting/walking, approved/not approved, and
    so on. Our data might look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The observed values could take any one of a finite number of values (in this
    case 1, 2, or 3). Each of these values represents one of the discrete categories
    in our data (class 1 might correspond to a fraudulent transaction, class 2 might
    correspond to a transaction that is not fraudulent, and class 3 might correspond
    to an invalid transaction, for example). The predicted values could also take
    one of these discrete values. In evaluating our predictions, we want to somehow
    measure how right we were in making those discrete predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Individual evaluation metrics for categorical variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actually, there are a huge number of ways to evaluate discrete predictions with
    metrics, including accuracy, precision, recall, specificity, sensitivity, fallout,
    false omission rate, and many more. As with continuous variables, there is no
    one-size-fits-all metric for evaluation. Each time you approach a problem, you
    need to determine the metric that fits the problem and matches the goals of the
    project. You don't want to optimize for the wrong things and then waste a bunch
    of time reimplementing your model based on other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand these metrics and determine which is appropriate for our use
    case, we need to realize that there are a number of different scenarios that could
    occur when we are making discrete predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive** (**TP**): We predicted a certain category, and the observation
    was actually that category (for example, we predicted fraud and the observation
    was fraud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive** (**FP**): We predicted a certain category, but the observation
    was actually another category (for example, we predicted fraud but the observation
    was not fraud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative** (**TN**): We predicted that the observation wasn''t a certain
    category, and the observation was not that category (for example, we predicted
    not fraud and the observation was not fraud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative** (**FN**): We predicted that the observation wasn''t a certain
    category, but the observation was actually that category (for example, we predicted
    not fraud but the observation was fraud)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see that there are a number of ways we can combine, aggregate, and
    measure these scenarios. In fact, we could even aggregate/measure them in some
    sort of unique way related to our specific problem. However, there are some pretty
    standard ways of aggregating and measuring these scenarios that result in the
    following common metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The percentage of predictions that were right, or *(TP + TN)/(TP
    + TN + FP + FN)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: The percentage of positive predictions that were actually positive,
    or *TP/(TP + FP)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: The percentage of positive predictions that were identified as
    positive, or *TP/(TP + FN)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though I'm going to emphasize these here, you should take a look at other
    common metrics and their implications. A good overview can be found at [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example that parses our data and calculates accuracy. First,
    we read in our `labeled.csv` file, create a CSV reader, and initialize two slices
    that will hold our parsed observed/predicted values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will iterate over the records in the CSV parsing the values, and we
    will compare the observed and predicted values to calculate accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 97%! That's pretty good. That means we were right 97% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can similarly calculate precision and recall. However, you may have noticed
    that there are a couple of ways we can do this when we have more than two categories
    or classes. We could consider class 1 as positive and the other classes as negative,
    class 2 as positive and the other classes as negative, and so on. That is, we
    could calculate a precision or recall for each of our classes, as shown in the
    following code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the precision and recall are slightly different metrics and have
    different implications. If we wanted to get an overall precision or recall, we
    could average the per-class precisions and recalls. In fact, if certain classes
    were more important than other classes, we could take a weighted average of these
    and use that as our evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that a couple of the metrics are 100%. This seems good, but it might
    actually indicate a problem, as we will further discuss.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, such as finance and banking, false positives or other cases may
    be very costly for certain classes. For example, a mislabeling a transaction as
    fraudulent might result in significant losses. On the other hand, certain results
    for other classes might be negligible. These scenario might warrant the use of
    a custom metric or cost function that weights certain classes, certain results,
    or certain combinations of results as more important than others.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrices, AUC, and ROC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to calculating individual numerical metrics for our models, there
    are a variety of techniques to combine various metrics into a form that gives
    you a more complete representation of model performance. These include, but are
    certainly not limited to, **confusion matrices** and **area under the curve**
    (**AUC**)/**Receiver Operating Characteristic** (**ROC**) **curves**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Confusion matrices allow us to visualize the various **TP**, **TN**, **FP**,
    and **FN** values that we predict in a two-dimensional format. A confusion matrix
    has rows corresponding to the categories that you were supposed to predict, and
    columns corresponding to categories that were predicted. Then, the value of each
    element is the corresponding count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56626b65-b9f3-4207-8ddf-f69de05aa45f.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the ideal situation is that your confusion matrix only has entries
    on the diagonal (**TP**, **TN**). The diagonal elements represent predicting a
    certain category and the observation actually being in that category. The off-diagonal
    elements include counts for predictions that were incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: This type of confusion matrix can be especially useful for problems that have
    more than two categories. For example, you may be trying to predict various activities
    based on mobile accelerator and position data. These activities may include more
    than two categories, such as standing, sitting, running, driving, and so on. The
    confusion matrix for this problem with be larger than 2 x 2 and would allow you
    to quickly gauge the overall performance of your model on all categories, and
    identify categories in which your model is performing poorly.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to confusion matrices, ROC curves are commonly used to get an overall
    picture of the performance of binary classifiers (or models that are trained to
    predict one of two categories). ROC curves plot the recall versus false positive
    rate (*FP/(FP + TN)*) for every possible classification threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'The thresholds used in an ROC curve represent various boundaries or rankings
    in which you are separating the two categories of your classification. That is,
    the model that is evaluated by the ROC curve must make a prediction for the two
    classes based on probability, ranking, or score (referred to as a score in the
    following image). In every example mentioned earlier, a score is classified one
    way, and vise versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9847ec0-35b3-40fb-b4bf-f58bc3ba01ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To generate an ROC curve, we plot a point for each score or rank in our testing
    examples (recall, false positive rate). We can then connect these to form a curve.
    In many cases, you will see a straight line plotted down the diagonal of the ROC
    curve plot. This straight line is a reference line for a classifier, with approximately
    random predictive power:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d839efe-add3-4dc7-ab73-e408fc5a7523.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A good ROC curve is one that is in the upper left section of the plot, which
    means that our model has better than random predictive power. The more that the
    ROC curve hugs the upper left hand side of the plot, the better. This means that
    good ROC curves have more AUC; AUC for ROC curves is also used as an evaluation
    metric. Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72073033-e4d7-40f2-96f2-ba2453df4935.png)'
  prefs: []
  type: TYPE_IMG
- en: '`gonum.org/v1/gonum/stat` has some built-in functions and types that help you
    build ROC curves and AUC metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a quick example that calculates the AUC for an ROC curve with gonum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, now we know some ways to measure how well our model is performing. In fact,
    if we wanted to, we could create a super sophisticated, complicated model that
    could predict every observation without error. For example, we could create a
    model that would take the index of the row of the observation and return the exact
    answer for each of those rows. It might be a really big function with a lot of
    parameters, but it would return the correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: So, what's the problem with this? Well, the problem is that it would not generalize
    to new data. Our complicated model would predict really well for the data that
    we would expose it to, but once we try some new input data (that isn't part of
    our training dataset), the model would likely perform poorly.
  prefs: []
  type: TYPE_NORMAL
- en: We call this type of model (that doesn't generalize) a model that has been **overfit**.
    That is, our process of making the model more and more complicated based on the
    data that was available to us was overfitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting can happen when predicting continuous values or discrete/categorical
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b61c8e32-cce8-4f5f-bc80-632475a1f966.png)'
  prefs: []
  type: TYPE_IMG
- en: To prevent overfitting, we need to validate our model. There are multiple ways
    to perform validation, and we will cover a couple of these here.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you are productionizing a model, you need to ensure that you have
    validated your model and understand how it will generalize to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Training and test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first method to help prevent overfitting is to train or fit your model on
    a portion of your dataset and then test or evaluate your model on a different
    portion of your dataset. Training your model generally consists of parameterizing
    one or more functions that make up your model, such that the functions that predict
    what you are trying to predict. Then, you can evaluate this trained model using
    one or more of the evaluation metrics that we discussed previously. The important
    thing here is that you do not want to test/evaluate your model on the same data
    that is used to train your model.
  prefs: []
  type: TYPE_NORMAL
- en: By reserving part of your data for testing, you are simulating the scenario
    in which your model sees new data. That is, the model is making predictions based
    on data that was not used in parameterizing the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d041cb25-7cdc-4f3d-b862-81f27c597e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Many people start by splitting 80% of their data into a training data set and
    20% into a test set (an 80/20 split). However, you will see different people splitting
    their datasets up in different proportions. The proportion of test to training
    data depends a little bit on the type and amount of data that you have and the
    model that you are trying to train. Generally, you want to ensure that both your
    training data and test data are a fairly accurate representation of your data
    on a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you are trying to predict one of a few different categories,
    A, B, and C, you wouldn't want your training data to include observations that
    only correspond to A and B. A model trained on such a dataset would likely only
    be able to predict the A and B categories. Likewise, you wouldn't want your test
    set to include some subset of the categories, or artificially weighted proportions
    of the categories. This could very easily happen, depending on how your data was
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, you want to make sure that you have enough training data to reduce
    the variability in your determined parameters as they are computed over and over.
    If you have too few training data points, or poorly sampled training data points,
    your model training may produce parameters with a lot of variability, or it may
    not even be able to converge numerically. These are indications that your model
    lacks predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, as you increase the complexity of your model, you will be able to
    improve the evaluation metric that you are using for your training data, but at
    some point, the evaluation metric will start getting worse for your test data.
    When the evaluation metric starts to get worse for your test data, you are starting
    to overfit your model. The ideal scenario is when you are able to increase your
    model complexity up to the inflection point, where the test evaluation metric
    starts to degrade. Another way of putting this (which fits very well into our
    general philosophy for model building in this book) is that we want the most interpretable
    model (or simplistic model) that can produce valuable results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82f10fbd-6a15-46dc-bcff-829c780a6f2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One way to quickly split a dataset into training and test sets is with `github.com/kniren/gota/dataframe`.
    Let''s demonstrate this using a dataset, which includes a bunch of anonymized
    information about medical patients and a corresponding indication of the progression
    of disease and diabetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'You can retrieve this data set here: [https://archive.ics.uci.edu/ml/datasets/diabetes](https://archive.ics.uci.edu/ml/datasets/diabetes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To split this data with `github.com/kniren/gota/dataframe`, we can do the following
    (where we save the training and test splits to respective CSV files):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Holdout set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are making progress to ensure that our models generalize using training
    and test sets. However, imagine the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: We develop a first version of our model based on our training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We test this first version of our model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We aren't satisfied with the result on the test set, so we loop back to step
    1 and repeat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process might seem logical, but you are probably already seeing a problem
    that can result from this procedure. We can actually overfit our model on the
    test data by iteratively exposing the model to our test set.
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of ways to deal with this extra level of overfitting. The
    first is by simply creating another split of our data called a **holdout set**
    (also known as a **validation set**). So, now we would have a training set, test
    set, and holdout set. This is sometimes called the three dataset validation, for
    obvious reasons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9421ffa1-cc2d-4dbf-911a-8a474792dc25.png)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that, to truly get an idea about the general performance of your
    model, your holdout set must never be used in training and testing. You should
    reserve this dataset for validation after you have gone through the process of
    training your model, making adjustments to the model, and getting an acceptable
    performance on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering how you can manage this splitting of data over time and
    recover different sets of data used to train or test certain models. This "provenance"
    of data is crucial when trying to maintain integrity in your machine learning
    workflows. This is also exactly what Pachyderm's data versioning (introduced in
    Chapter 1, *Gathering and Organizing Data*) was created to handle. We will see
    exactly how this plays out at scale later in Chapter 9, *Deploying and distributing
    Analyses and Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to reserving a holdout set for validation, cross validation is a
    common technique to validate the generality of a model. In cross validation, or
    k-fold cross validation, you actually perform *k* random splits of your dataset
    into different training and test combinations. Think of these as *k* experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have performed each split, you train your model on the training data
    for that split, and then evaluate it on the test data for that split. This process
    results in an evaluation metric result for each random split of your data. You
    can then average these evaluation metrics to get an overall evaluation metric
    that is a more general representation of model performance than any one of the
    individual evaluation metrics by themselves. You can also look at the variance
    in the evaluation metrics to get an idea about the stability of your various experiments.
    This process is illustrated in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c333632-adaf-4adc-b750-2693dc863aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Some advantages of using cross validation, in comparison to dataset validation,
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: You are making use of your entire dataset, and thus, are actually exposing your
    model to more training examples and more testing examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are some convenience functions and packaging already written for cross
    validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It helps prevent the biases that may result from choosing a single validation
    set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `github.com/sjwhitworth/golearn` is one Go package that provides some convenience
    functions for cross validation. Actually, `github.com/sjwhitworth/golearn` includes
    a bunch of machine learning functionality that we will cover later on in the book,
    but for now, let's just look at what functionality is available for cross validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the `github.com/sjwhitworth/golearn/evaluation` package Godocs,
    you will see the following function that is available for cross validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This function can actually be used with a variety of models, but here is an
    example using a decision tree model (don''t worry about the details of the model
    here):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Essay on overfitting: [http://scott.fortmann-roe.com/docs/MeasuringError.html](http://scott.fortmann-roe.com/docs/MeasuringError.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Essay on bias-variance trade-off: [http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Comparison of categorical evaluation metrics: [https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gonum.org/v1/gonum/stat` docs: [https://godoc.org/gonum.org/v1/gonum/stat](https://godoc.org/gonum.org/v1/gonum/stat)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn/evaluation` docs: [https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`github.com/kniren/gota/dataframe` docs: [https://godoc.org/github.com/kniren/gota/dataframe](https://godoc.org/github.com/kniren/gota/dataframe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`github.com/sjwhitworth/golearn/evaluation` docs: [https://godoc.org/github.com/sjwhitworth/golearn/evaluation](https://godoc.org/github.com/sjwhitworth/golearn/evaluation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing an appropriate evaluation metric and laying out a procedure for evaluation/validation
    are essential parts of any machine learning project. You have learned about a
    variety of relevant evaluation metrics and how to avoid overfitting using holdout
    sets and/or cross validation. In the next chapter, we will start looking at machine
    learning models and we will build our first model using linear regression!
  prefs: []
  type: TYPE_NORMAL
