["```py\ndef ccw(a, b, c):    return (b[1]-a[1])*(c[0]-b[0]) < (b[0]-a[0])*(c[1]-b[1])\n```", "```py\ndef naiveCH(points):    points.sort()\n    p = 0\n    hull = []\n    while True:\n        # Add current point to result\n        hull.append(points[p])\n        # Pick the next point (or go back to the beginning\n        # if p was the last point)\n        q = (p + 1) % n\n        for i in range(len(points)):\n    # If i is more counterclockwise\n    # than current q, then update q\n           if(ccw(points[p], points[i], points[q])):\n                q = i;\n    # Now q is the most counterclockwise with respect to p\n    # Set p as q for next iteration, so that q is added to 'hull'\n          p = q\n    # Terminate when you get back to that start and close the loop\n          if(p == 0):\n              hull.append(points[0])\n              break\n    return hull\n```", "```py\n    def __init__(self, train, args={\"useDF\":True}):        self.readTrainingData(train, args=args)\n        # Make an sklearn SVM\n        self.clsf = sklearn.svm.LinearSVC(max_iter=2000)\n        # Get it to learn from the data\n        self.clsf.fit(self.matrix, self.values)\n```", "```py\n    def applyToTweet(self, tweet):        tweet = tweets.tweet2sparse(tweet, self)\n        # use predict_log_proba\n        p = self.clsf.predict_log_proba(tweet)[0]\n        # compare to previously defined threshold\n        threshold = numpy.log(self.threshold)\n        return [1 if i > threshold else 0 for i in p]\n```", "```py\n    def __init__(self, train, args={}):        self.readTrainingData(train, args=args)\n        # Make an sklearn SVM\n        self.clsf = sklearn.svm.LinearSVC(max_iter=2000)\n        # Get it to learn from the data\n        self.clsf.fit(self.matrix, self.values)\n        # Set its version of predict_proba to be its decision_function\n        self.clsf.predict_proba = self.clsf.decision_function\n        # and set its version of weights to be its coefficients\n        self.weights = self.clsf.coef_\n```", "```py\ndef squeeze(train, i):# Collapse the Gold Standard for each tweet so that we just\n# have two columns, one for emotion[i] in the original and\n# one for the rest.\n    l = []\n    for tweet in train.tweets:\n        # now squeeze the Gold Standard value\n        gs = tweet.GS\n        scores=[gs[i], numpy.sign(sum(gs[:i]+gs[i+1:]))]\n        tweet = tweets.TWEET(id=tweet.id, src=tweet.src,\n                             text=tweet.text, tf=tweet.tf,\n                             scores=scores,\n                             tokens=tweet.tokens,\n                             args=tweet.ARGS)\n        l.append(tweet)\n    emotion = train.emotions[i]\n    emotions = [emotion, \"not %s\"%(emotion)]\n    return tweets.DATASET(emotions, l, train.df,\n                          train.idf, train.ARGS)\n```", "```py\n    def applyToTweet(self, tweet):        k = [0 for i in self.train.emotions]\n        for i in self.classifiers:\n            c = self.classifiers[i]\n            p = c.clsf.predict(tweets.tweet2sparse(tweet, self))[0]\n            \"\"\"\n            if classifier i says that this tweet expresses\n            the classifier's emotion (i.e. if the underlying\n            SVM returns 0) then set the ith column of the\n            main classifier to 1\n            \"\"\"\n            if p == 0:\n                k[i] = 1\n        return k\n```"]