["```py\n    > install.packages(\"adabag\")\n    > library(adabag)\n\n    ```", "```py\n    > set.seed(2)\n    > churn.bagging = bagging(churn ~ ., data=trainset, mfinal=10)\n\n    ```", "```py\n    > churn.bagging$importance\n     international_plan number_customer_service_calls \n     10.4948380                    16.4260510 \n     number_vmail_messages               total_day_calls \n     0.5319143                     0.3774190 \n     total_day_charge             total_day_minutes \n     0.0000000                    28.7545042 \n     total_eve_calls              total_eve_charge \n     0.1463585                     0.0000000 \n     total_eve_minutes              total_intl_calls \n     14.2366754                     8.7733895 \n     total_intl_charge            total_intl_minutes \n     0.0000000                     9.7838256 \n     total_night_calls            total_night_charge \n     0.4349952                     0.0000000 \n     total_night_minutes               voice_mail_plan \n     2.3379622                     7.7020671 \n\n    ```", "```py\n    > churn.predbagging= predict.bagging(churn.bagging, newdata=testset)\n\n    ```", "```py\n    > churn.predbagging$confusion\n     Observed Class\n    Predicted Class yes  no\n     no   35 866\n     yes 106  11\n\n    ```", "```py\n    > churn.predbagging$error\n    [1] 0.0451866\n\n    ```", "```py\n    > install.packages(\"ipred\")\n    > library(ipred)\n\n    ```", "```py\n    > churn.bagging = bagging(churn ~ ., data = trainset, coob = T)\n    > churn.bagging\n\n    Bagging classification trees with 25 bootstrap replications \n\n    Call: bagging.data.frame(formula = churn ~ ., data = trainset, coob = T)\n\n    Out-of-bag estimate of misclassification error:  0.0605 \n\n    ```", "```py\n    > mean(predict(churn.bagging) != trainset$churn)\n    [1] 0.06047516\n\n    ```", "```py\n    > churn.prediction = predict(churn.bagging, newdata=testset, type=\"class\")\n\n    ```", "```py\n    > prediction.table = table(churn.prediction, testset$churn)\n\n    churn.prediction yes  no\n     no   31 869\n     yes 110   8\n\n    ```", "```py\n    > churn.baggingcv = bagging.cv(churn ~ ., v=10, data=trainset, mfinal=10)\n\n    ```", "```py\n    > churn.baggingcv$confusion\n     Observed Class\n    Predicted Class  yes   no\n     no   100 1938\n     yes  242   35\n\n    ```", "```py\n    > churn.baggingcv$error\n    [1] 0.05831533\n\n    ```", "```py\n    > help(bagging.cv)\n\n    ```", "```py\n    > set.seed(2)\n    > churn.boost = boosting(churn ~.,data=trainset,mfinal=10, coeflearn=\"Freund\", boos=FALSE , control=rpart.control(maxdepth=3))\n\n    ```", "```py\n    > churn.boost.pred = predict.boosting(churn.boost,newdata=testset)\n\n    ```", "```py\n    > churn.boost.pred$confusion\n     Observed Class\n    Predicted Class yes  no\n     no   41 858\n     yes 100  19\n\n    ```", "```py\n    > churn.boost.pred$error\n    [1] 0.0589391\n\n    ```", "```py\n    > library(mboost)\n    > install.packages(\"pROC\")\n    > library(pROC)\n\n    ```", "```py\n    > set.seed(2)\n    > ctrl = trainControl(method = \"repeatedcv\", repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary)\n    > ada.train = train(churn ~ ., data = trainset, method = \"ada\", metric = \"ROC\", trControl = ctrl)\n\n    ```", "```py\n    > ada.train$result\n     nu maxdepth iter       ROC      Sens        Spec      ROCSD     SensSD      SpecSD\n    1 0.1        1   50 0.8571988 0.9152941 0.012662155 0.03448418 0.04430519 0.007251045\n    4 0.1        2   50 0.8905514 0.7138655 0.006083679 0.03538445 0.10089887 0.006236741\n    7 0.1        3   50 0.9056456 0.4036134 0.007093780 0.03934631 0.09406015 0.006407402\n    2 0.1        1  100 0.8550789 0.8918487 0.015705276 0.03434382 0.06190546 0.006503191\n    5 0.1        2  100 0.8907720 0.6609244 0.009626724 0.03788941 0.11403364 0.006940001\n    8 0.1        3  100 0.9077750 0.3832773 0.005576065 0.03601187 0.09630026 0.003738978\n    3 0.1        1  150 0.8571743 0.8714286 0.016720505 0.03481526 0.06198773 0.006767313\n    6 0.1        2  150 0.8929524 0.6171429 0.011654617 0.03638272 0.11383803 0.006777465\n    9 0.1        3  150 0.9093921 0.3743697 0.007093780 0.03258220 0.09504202 0.005446136\n\n    ```", "```py\n    > plot(ada.train)\n\n    ```", "```py\n    > ada.predict = predict(ada.train, testset, \"prob\")\n    > ada.predict.result = ifelse(ada.predict[1] > 0.5, \"yes\", \"no\")\n\n    > table(testset$churn, ada.predict.result)\n     ada.predict.result\n     no yes\n     yes  40 101\n     no  872   5\n\n    ```", "```py\n    > churn.boostcv = boosting.cv(churn ~ ., v=10, data=trainset, mfinal=5,control=rpart.control(cp=0.01))\n\n    ```", "```py\n    > churn.boostcv$confusion\n     Observed Class\n    Predicted Class  yes   no\n     no   119 1940\n     yes  223   33\n\n    ```", "```py\n    > churn.boostcv$error\n    [1] 0.06565875\n\n    ```", "```py\n    > help(boosting.cv)\n\n    ```", "```py\n    > install.packages(\"gbm\")\n    > library(gbm)\n\n    ```", "```py\n    > trainset$churn = ifelse(trainset$churn == \"yes\", 1, 0)\n\n    ```", "```py\n    > set.seed(2)\n    > churn.gbm = gbm(formula = churn ~ .,distribution = \"bernoulli\",data = trainset,n.trees = 1000,interaction.depth = 7,shrinkage = 0.01, cv.folds=3)\n\n    ```", "```py\n    > summary(churn.gbm)\n     var    rel.inf\n    total_day_minutes          total_day_minutes 28.1217147\n    total_eve_minutes                total_eve_minutes 16.8097151\n    number_customer_service_calls number_customer_service_calls 12.7894464\n    total_intl_minutes             total_intl_minutes  9.4515822\n    total_intl_calls                   total_intl_calls  8.1379826\n    international_plan               international_plan  8.0703900\n    total_night_minutes             total_night_minutes  4.0805153\n    number_vmail_messages         number_vmail_messages  3.9173515\n    voice_mail_plan                  voice_mail_plan  2.5501480\n    total_night_calls              total_night_calls  2.1357970\n    total_day_calls                     total_day_calls  1.7367888\n    total_eve_calls                     total_eve_calls  1.4398047\n    total_eve_charge                 total_eve_charge  0.5457486\n    total_night_charge              total_night_charge  0.2130152\n    total_day_charge                total_day_charge  0.0000000\n    total_intl_charge                 total_intl_charge  0.0000000\n\n    ```", "```py\n    > churn.iter = gbm.perf(churn.gbm,method=\"cv\")\n\n    ```", "```py\n    > churn.predict = predict(churn.gbm, testset, n.trees = churn.iter)\n    > str(churn.predict)\n     num [1:1018] -3.31 -2.91 -3.16 -3.47 -3.48 ...\n\n    ```", "```py\n    > churn.roc = roc(testset$churn, churn.predict)\n    > plot(churn.roc)\n    Call:\n    roc.default(response = testset$churn, predictor = churn.predict)\n    Data: churn.predict in 141 controls (testset$churn yes) > 877 cases (testset$churn no).\n    Area under the curve: 0.9393\n\n    ```", "```py\n    > coords(churn.roc, \"best\")\n     threshold specificity sensitivity \n     -0.9495258   0.8723404   0.9703535 \n    > churn.predict.class = ifelse(churn.predict > coords(churn.roc, \"best\")[\"threshold\"], \"yes\", \"no\")\n\n    ```", "```py\n    > table( testset$churn,churn.predict.class)\n     churn.predict.class\n     no yes\n     yes  18 123\n     no  851  26\n\n    ```", "```py\n    > install.packages(\"mboost\")\n    > library(mboost)\n\n    ```", "```py\n    > trainset$churn = ifelse(trainset$churn == \"yes\", 1, 0)\n\n    ```", "```py\n    > trainset$voice_mail_plan = NULL\n    > trainset$international_plan = NULL\n\n    ```", "```py\n    > churn.mboost = mboost(churn ~ ., data=trainset,  control = boost_control(mstop = 10))\n\n    ```", "```py\n    > summary(churn.mboost)\n\n     Model-based Boosting\n\n    Call:\n    mboost(formula = churn ~ ., data = trainset, control = boost_control(mstop = 10))\n\n     Squared Error (Regression) \n\n    Loss function: (y - f)^2 \n\n    Number of boosting iterations: mstop = 10 \n    Step size:  0.1 \n    Offset:  1.147732 \n    Number of baselearners:  14 \n\n    Selection frequencies:\n     bbs(total_day_minutes) bbs(number_customer_service_calls) \n     0.6                                0.4 \n\n    ```", "```py\n    > par(mfrow=c(1,2))\n    > plot(churn.mboost)\n\n    ```", "```py\n    > boost.margins = margins(churn.boost, trainset)\n    > boost.pred.margins = margins(churn.boost.pred, testset)\n\n    ```", "```py\n    > plot(sort(boost.margins[[1]]), (1:length(boost.margins[[1]]))/length(boost.margins[[1]]), type=\"l\",xlim=c(-1,1),main=\"Boosting: Margin cumulative distribution graph\", xlab=\"margin\", ylab=\"% observations\", col = \"blue\")\n    > lines(sort(boost.pred.margins[[1]]), (1:length(boost.pred.margins[[1]]))/length(boost.pred.margins[[1]]), type=\"l\", col = \"green\")\n    > abline(v=0, col=\"red\",lty=2)\n\n    ```", "```py\n    > boosting.training.margin = table(boost.margins[[1]] > 0)\n    > boosting.negative.training = as.numeric(boosting.training.margin[1]/boosting.training.margin[2])\n    > boosting.negative.training\n     [1] 0.06387868\n\n    > boosting.testing.margin = table(boost.pred.margins[[1]] > 0)\n    > boosting.negative.testing = as.numeric(boosting.testing.margin[1]/boosting.testing.margin[2])\n    > boosting.negative.testing\n    [1] 0.06263048\n\n    ```", "```py\n    > bagging.margins = margins(churn.bagging, trainset)\n    > bagging.pred.margins = margins(churn.predbagging, testset)\n\n    ```", "```py\n    > plot(sort(bagging.margins[[1]]), (1:length(bagging.margins[[1]]))/length(bagging.margins[[1]]), type=\"l\",xlim=c(-1,1),main=\"Bagging: Margin cumulative distribution graph\", xlab=\"margin\", ylab=\"% observations\", col = \"blue\")\n\n    > lines(sort(bagging.pred.margins[[1]]), (1:length(bagging.pred.margins[[1]]))/length(bagging.pred.margins[[1]]), type=\"l\", col = \"green\")\n    > abline(v=0, col=\"red\",lty=2)\n\n    ```", "```py\n    > bagging.training.margin = table(bagging.margins[[1]] > 0)\n    > bagging.negative.training = as.numeric(bagging.training.margin[1]/bagging.training.margin[2])\n    > bagging.negative.training\n    [1] 0.1733401\n\n    > bagging.testing.margin = table(bagging.pred.margins[[1]] > 0)\n    > bagging.negative.testing = as.numeric(bagging.testing.margin[1]/bagging.testing.margin[2])\n    > bagging.negative.testing\n    [1] 0.04303279\n\n    ```", "```py\n    > boosting.evol.train = errorevol(churn.boost, trainset)\n    > boosting.evol.test = errorevol(churn.boost, testset)\n    > plot(boosting.evol.test$error, type = \"l\", ylim = c(0, 1),\n    +       main = \"Boosting error versus number of trees\", xlab = \"Iterations\",\n    +       ylab = \"Error\", col = \"red\", lwd = 2)\n    > lines(boosting.evol.train$error, cex = .5, col = \"blue\", lty = 2, lwd = 2)\n    > legend(\"topright\", c(\"test\", \"train\"), col = c(\"red\", \"blue\"), lty = 1:2, lwd = 2)\n\n    ```", "```py\n    > bagging.evol.train = errorevol(churn.bagging, trainset)\n    > bagging.evol.test = errorevol(churn.bagging, testset)\n    > plot(bagging.evol.test$error, type = \"l\", ylim = c(0, 1),\n    +       main = \"Bagging error versus number of trees\", xlab = \"Iterations\",\n    +       ylab = \"Error\", col = \"red\", lwd = 2)\n    > lines(bagging.evol.train$error, cex = .5, col = \"blue\", lty = 2, lwd = 2)\n    > legend(\"topright\", c(\"test\", \"train\"), col = c(\"red\", \"blue\"), lty = 1:2, lwd = 2)\n\n    ```", "```py\n    > help(predict.bagging)\n    > help(predict.boosting)\n\n    ```", "```py\n    > install.packages(\"randomForest\")\n    > library(randomForest)\n\n    ```", "```py\n    > churn.rf = randomForest(churn ~ ., data = trainset, importance = T)\n    > churn.rf\n\n    Call:\n     randomForest(formula = churn ~ ., data = trainset, importance = T) \n     Type of random forest: classification\n     Number of trees: 500\n    No. of variables tried at each split: 4\n\n     OOB estimate of  error rate: 4.88%\n    Confusion matrix:\n     yes   no class.error\n    yes 247   95 0.277777778\n    no   18 1955 0.009123163\n\n    ```", "```py\n    > churn.prediction = predict(churn.rf, testset)\n\n    ```", "```py\n    > table(churn.prediction, testset$churn)\n\n    churn.prediction yes  no\n     yes 110   7\n     no   31 870\n\n    ```", "```py\n    > plot(churn.rf)\n\n    ```", "```py\n    > importance(churn.rf)\n     yes         no\n    international_plan            66.55206691 56.5100647\n    voice_mail_plan               19.98337191 15.2354970\n    number_vmail_messages         21.02976166 14.0707195\n    total_day_minutes             28.05190188 27.7570444\n\n    ```", "```py\n    > varImpPlot(churn.rf)\n\n    ```", "```py\n    > margins.rf=margin(churn.rf,trainset)\n    > plot(margins.rf)\n\n    ```", "```py\n    > hist(margins.rf,main=\"Margins of Random Forest for churn dataset\")\n\n    ```", "```py\n    > boxplot(margins.rf~trainset$churn, main=\"Margins of Random Forest for churn dataset by class\")\n\n    ```", "```py\n    > install.packages(\"party\")\n    > library(party)\n\n    ```", "```py\n    > churn.cforest = cforest(churn ~ ., data = trainset, controls=cforest_unbiased(ntree=1000, mtry=5))\n    > churn.cforest\n\n     Random Forest using Conditional Inference Trees\n\n    Number of trees:  1000 \n\n    Response:  churn \n    Inputs:  international_plan, voice_mail_plan, number_vmail_messages, total_day_minutes, total_day_calls, total_day_charge, total_eve_minutes, total_eve_calls, total_eve_charge, total_night_minutes, total_night_calls, total_night_charge, total_intl_minutes, total_intl_calls, total_intl_charge, number_customer_service_calls \n    Number of observations:  2315 \n\n    ```", "```py\n    > churn.cforest.prediction = predict(churn.cforest, testset, OOB=TRUE, type = \"response\")\n\n    ```", "```py\n    > table(churn.cforest.prediction, testset$churn)\n\n    churn.cforest.prediction yes  no\n     yes  91   3\n     no   50 874\n\n    ```", "```py\n    > churn.bagging= errorest(churn ~ ., data = trainset, model = bagging)\n    > churn.bagging\n\n    Call:\n    errorest.data.frame(formula = churn ~ ., data = trainset, model = bagging)\n\n     10-fold cross-validation estimator of misclassification error \n\n    Misclassification error:  0.0583 \n\n    ```", "```py\n    > install.packages(\"ada\")\n    > library(ada)\n    > churn.boosting= errorest(churn ~ ., data = trainset, model = ada)\n    > churn.boosting\n\n    Call:\n    errorest.data.frame(formula = churn ~ ., data = trainset, model = ada)\n\n     10-fold cross-validation estimator of misclassification error \n\n    Misclassification error:  0.0475 \n\n    ```", "```py\n    > churn.rf= errorest(churn ~ ., data = trainset, model = randomForest)\n    > churn.rf\n\n    Call:\n    errorest.data.frame(formula = churn ~ ., data = trainset, model = randomForest)\n\n     10-fold cross-validation estimator of misclassification error \n\n    Misclassification error:  0.051 \n\n    ```", "```py\n    > churn.predict = function(object, newdata) {predict(object, newdata = newdata, type = \"class\")}\n    > churn.tree= errorest(churn ~ ., data = trainset, model = rpart,predict = churn.predict)\n    > churn.tree\n\n    Call:\n    errorest.data.frame(formula = churn ~ ., data = trainset, model = rpart, \n     predict = churn.predict)\n\n     10-fold cross-validation estimator of misclassification error \n\n    Misclassification error:  0.0674 \n\n    ```"]