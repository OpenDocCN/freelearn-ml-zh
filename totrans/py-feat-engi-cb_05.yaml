- en: <st c="0">5</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Working with Outliers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="23">An outlier</st> <st c="34">is a data point that diverges notably
    from other values within a variable.</st> <st c="110">Outliers may stem from the
    inherent variability of the feature itself, manifesting as extreme values that
    occur infrequently within the distribution (typically found in the tails).</st>
    <st c="291">They can be the result of experimental errors or inaccuracies in data
    collection processes, or they can signal important events.</st> <st c="420">For
    instance, an unusually high expense in a card transaction may indicate fraudulent
    activity, warranting flagging and potentially blocking the card to safeguard customers.</st>
    <st c="594">Similarly, unusually distinct tumor morphologies can suggest malignancy,
    prompting</st> <st c="677">further examination.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="697">Outliers can exert a disproportionately large impact on a statistical
    analysis.</st> <st c="778">For example, a small number of outliers can reverse
    the statistical significance of a test in either direction (think A/B testing)
    or directly influence the estimation of the parameters of the statistical model
    (think coefficients).</st> <st c="1011">Some machine learning models are well
    known for being susceptible to outliers, such as linear regression.</st> <st c="1117">Other
    models are known for being robust to outliers, such as decision-tree-based models.</st>
    <st c="1206">AdaBoost is said to be sensitive to outliers in the target variable,
    and in principle, distance-based models, such as PCA and KNN, could also be affected
    by the presence</st> <st c="1376">of outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1388">There isn’t a strict mathematical definition for what qualifies
    as an outlier, and there is also no consensus on how to handle outliers in statistical
    or machine learning models.</st> <st c="1568">If outliers stem from flawed data
    collection, discarding them seems like a safe option.</st> <st c="1656">However,
    in many datasets, pinpointing the exact nature of outliers is challenging.</st>
    <st c="1740">Ultimately, detecting and handling outliers remains a subjective
    exercise, reliant on domain knowledge and an understanding of their potential
    impact</st> <st c="1890">on models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1900">In this chapter, we will begin by discussing methods to identify
    potential outliers, or more precisely, observations that significantly deviate
    from the rest.</st> <st c="2060">Then, we’ll proceed under the assumption that
    these observations are not relevant for the analysis, and show how to either remove
    them or reduce their impact on models</st> <st c="2228">through truncation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2247">This chapter contains the</st> <st c="2274">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2292">Visualizing outliers with boxplots and the inter-quartile</st>
    <st c="2351">proximity rule</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2365">Finding outliers using the mean and</st> <st c="2402">standard
    deviation</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2420">Using the median absolute deviation to</st> <st c="2460">find outliers</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2473">Removing outliers</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2491">Bringing outliers back within</st> <st c="2522">acceptable limits</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2539">Applying winsorization</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="2562">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2585">In this chapter, we will use the Python</st> `<st c="2626">numpy</st>`<st
    c="2631">,</st> `<st c="2633">pandas</st>`<st c="2639">,</st> `<st c="2641">matplotlib</st>`<st
    c="2651">,</st> `<st c="2653">seaborn</st>`<st c="2660">, and</st> `<st c="2666">feature-engine</st>`
    <st c="2680">libraries.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2691">Visualizing outliers with boxplots and the inter-quartile proximity
    rule</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2764">A</st> <st c="2767">common way to visualize outliers is by usin</st><st
    c="2810">g boxplots.</st> <st c="2823">Boxplots</st> <st c="2832">provide a standardized
    display of the variable’s distribution based on quartiles.</st> <st c="2914">The
    box contains the observations within the first and third quartiles, known as the</st>
    **<st c="2999">Inter-Quartile Range</st>**<st c="3019">(</st>**<st c="3021">IQR</st>**<st
    c="3024">).</st> <st c="3028">The</st> <st c="3031">first quartile is the value
    below which 25% of the observations lie (equivalent to the 25th percentile), while
    the third quartile is the value below which 75% of the observations lie (equivalent
    to the 75th percentile).</st> <st c="3252">The IQR is calculated</st> <st c="3274">as
    follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>I</mml:mi><mml:mi>Q</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mo> </mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo> </mml:mo><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:math>](img/21.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="3320">Boxplots also</st> <st c="3334">display whiskers, which are lines
    that protrude from each end of the box toward the minimum and maximum values and
    up to a limit.</st> <st c="3464">These limits are given by the minimum or maximum
    value of the distribution or, in the presence of extreme values, by the</st> <st
    c="3585">following equations:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>u</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>r</mi><mi>l</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mo>=</mo><mn>3</mn><mi>r</mi><mi>d</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>+</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo>×</mo><mn>1.5</mn></mrow></mrow></math>](img/22.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>w</mi><mi>e</mi><mi>r</mi><mi>l</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>t</mi><mo>=</mo><mn>1</mn><mi>s</mi><mi>t</mi><mi>q</mi><mi>u</mi><mi>a</mi><mi>r</mi><mi>t</mi><mi>i</mi><mi>l</mi><mi>e</mi><mo>−</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo>×</mo><mn>1.5</mn></mrow></mrow></math>](img/23.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="3684">According to the</st> **<st c="3701">IQR proximity rule</st>**<st
    c="3719">, we</st> <st c="3723">can consider a value an</st> <st c="3747">outlier
    if it falls beyond the whisker limits determined by the previous equations.</st>
    <st c="3832">In boxplots, outliers are indicated</st> <st c="3868">as dots.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3876">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3881">If the variable has a normal distribution, about 99% of the observations
    will be located within the interval delimited by the whiskers.</st> <st c="4018">Hence,
    we can treat values beyond the whiskers as outliers.</st> <st c="4078">Boxplots
    are, however, non-parametric, which is why we also use them to visualize outliers
    in</st> <st c="4172">skewed variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4189">In this</st> <st c="4198">recipe, we’ll begin by visualizing the
    variable distribution with boxplots, and then we’ll calculate the whisker’s limits
    manually to identify the points beyond which we could consider a value as</st>
    <st c="4394">an o</st><st c="4398">utlier.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4406">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="4422">We will create boxplots utilizing the</st> `<st c="4461">seaborn</st>`
    <st c="4468">library</st><st c="4476">. Let’s begin by importing the Python libraries
    and loading</st> <st c="4536">the dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4548">Let’s import the Python libraries and</st> <st c="4587">the dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4707">Modify the default background from</st> `<st c="4743">seaborn</st>`
    <st c="4750">(it makes prettier plots, but that’s subjective,</st> <st c="4800">of
    course):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4837">Load the California house prices dataset</st> <st c="4879">from
    scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="4962">Make a boxplot of the</st> `<st c="4985">MedInc</st>` <st c="4991">variable
    to visualize</st> <st c="5014">its distribution:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5132">In the following boxplot, we identify the box containing the observations
    within the IQR, that is, the observations between the first and third quartiles.</st>
    <st c="5288">We also see the whiskers.</st> <st c="5314">On</st> <st c="5316">the
    left, the whisker extends to the minimum value of</st> `<st c="5371">MedInc</st>`<st
    c="5377">; on the right, the whisker goes up to the third quartile plus 1.5 times
    the IQR.</st> <st c="5460">Values beyond the right whisker are represented as
    dots and could</st> <st c="5526">constitute out</st><st c="5540">liers:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Boxplot of the MedInc variable highlighting potential outliers
    on the right tail of the distribution](img/B22396_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="5569">Figure 5.1 – Boxplot of the MedInc variable highlighting potential
    outliers on the right tail of the distribution</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5682">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5687">As shown in</st> *<st c="5700">Figure 5</st>**<st c="5708">.1</st>*<st
    c="5710">, the boxplot returns asymmetric boundaries denoted by the varying lengths
    of the left and right whiskers.</st> <st c="5817">This makes boxplots a suitable
    method for identifying outliers in highly skewed distributions.</st> <st c="5912">As
    we’ll see in the coming recipes, alternative methods to identify outliers create
    symmetric boundaries around the center of the distribution, which may not be the
    best option for</st> <st c="6093">asymmetric distributions</st><st c="6117">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6118">Let’s now create a function to</st> <st c="6149">plot a boxplot
    next to</st> <st c="6173">a histogram:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6427">Let’s use</st> <st c="6437">the previous</st> <st c="6450">function
    to create the plots for the</st> `<st c="6487">MedInc</st>` <st c="6493">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6538">In the following figure, we can see the relationship between the
    boxplot and the variable’s distribution shown in the histogram.</st> <st c="6668">Note
    how most of</st> `<st c="6685">MedInc</st>`<st c="6691">’s observations are located
    within the IQR box.</st> `<st c="6740">MedInc</st>`<st c="6746">’s potential outliers
    lie on the right tail, corresponding to people with unusually</st> <st c="6831">high-income
    sa</st><st c="6845">laries:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s
    distribution](img/B22396_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="6920">Figure 5.2 – Boxplot and histogram – two ways of displaying a variable’s
    distribution</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7005">Now that</st> <st c="7015">we’ve seen how we can visualize outliers,
    let’s see how to calculate the limits beyond which we find outliers at each side
    of</st> <st c="7141">the distribution.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7158">Let’s create a function that returns the limits based on the IQR</st>
    <st c="7224">proximity rule:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7452">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7457">Remember that the first and third quartiles are equivalent to the
    25th and 75th percentiles.</st> <st c="7551">That’s why we use pandas’</st> `<st
    c="7577">quantile</st>` <st c="7585">to determine</st> <st c="7599">those values.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7612">With the function from</st> *<st c="7636">step 7</st>*<st c="7642">,
    we’ll calculate the extreme limits</st> <st c="7679">for</st> `<st c="7683">MedInc</st>`<st
    c="7689">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7749">If we now execute</st> `<st c="7768">lower_limit</st>` <st c="7779">and</st>
    `<st c="7784">upper_limit</st>`<st c="7795">, we will see the values</st> `<st
    c="7820">-0.7063</st>` <st c="7827">and</st> `<st c="7832">8.013</st>`<st c="7837">.
    The lower limit is beyond</st> `<st c="7865">MedInc</st>`<st c="7871">’s minimum
    value, hence in</st> <st c="7899">the boxplot, the whisker only goes up to the
    minimum value.</st> <st c="7959">The upper limit, on the other hand, coincides
    with the right</st> <st c="8020">whisker’s limit.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="8036">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8041">Common values to multiply the IQR are</st> `<st c="8080">1.5</st>`<st
    c="8083">, which is the default value in boxplots, or</st> `<st c="8128">3</st>`
    <st c="8130">if we want to be</st> <st c="8147">more conservative.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8165">Let’s display the box plot and histogram for the</st> `<st c="8215">HouseAge</st>`
    <st c="8223">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8270">We can see that this variable does not seem to contain outliers,
    and hence the whiskers in the box plot extend to the minimum and</st> <st c="8401">maximum
    values:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Boxplot and histogram of the HouseAge variable](img/B22396_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="8438">Figure 5.3 – Boxplot and histogram of the HouseAge variable</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8497">Let’s</st> <st c="8503">find the variable’s limits according to
    the IQR</st> <st c="8552">proximity rule:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="8627">If we execute</st> `<st c="8642">lower_limit</st>` <st c="8653">and</st>
    `<st c="8658">upper_limit</st>`<st c="8669">, we will see the values</st> `<st
    c="8694">-10.5</st>` <st c="8699">and</st> `<st c="8704">65.5</st>`<st c="8708">,
    which are beyond the edges of the plots, and hence we don’t see</st> <st c="8774">any</st>
    <st c="8778">outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8787">How it works</st><st c="8800">…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8801">In this recipe, we used the</st> `<st c="8829">boxplot</st>` <st
    c="8836">method from Seaborn to create the boxplots and then we calculated the
    limits beyond which a value could be considered an outlier based on the IQR</st>
    <st c="8983">proximity rule.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8998">In</st> *<st c="9002">Figure 5</st>**<st c="9010">.2</st>*<st c="9012">,
    we saw that the box in the boxplot for</st> `<st c="9053">MedInc</st>` <st c="9059">extended
    from approximately 2 to 5, corresponding to the first and third quantiles (you
    can determine these values precisely by executing</st> `<st c="9198">X[</st>`<st
    c="9200">“</st>`<st c="9202">MedInc</st>`<st c="9208">”</st>`<st c="9210">].quantile(0.25)</st>`
    <st c="9226">and</st> `<st c="9231">X[</st>`<st c="9233">“</st>`<st c="9235">MedInc</st>`<st
    c="9241">”</st>`<st c="9243">].quantile(0.75)</st>`<st c="9259">).</st> <st c="9263">We
    also saw that the whiskers start at</st> `<st c="9302">MedInc</st>`<st c="9308">’s
    minimum on the left and extend up to</st> `<st c="9349">8.013</st>` <st c="9354">on
    the right (we know this value exactly because we calculated it in</st> *<st c="9424">step
    8</st>*<st c="9430">).</st> `<st c="9434">MedInc</st>` <st c="9440">showed values
    greater than</st> `<st c="9468">8.013</st>`<st c="9473">, which were displayed</st>
    <st c="9495">in the boxplot as dots.</st> <st c="9520">Those are the values that
    could be</st> <st c="9555">considered outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9575">In</st> *<st c="9579">Figure 5</st>**<st c="9587">.3</st>*<st c="9589">,
    we displayed the boxplot for the</st> `<st c="9624">HouseAge</st>` <st c="9632">variable.</st>
    <st c="9643">The box included values ranging from approximately 18 to 35 (you
    can determine the precise values by executing</st> `<st c="9754">X[</st>`<st c="9756">“</st>`<st
    c="9758">HouseAge</st>`<st c="9766">”</st>`<st c="9768">].quantile(0.25)</st>`
    <st c="9784">and</st> `<st c="9789">X[</st>`<st c="9791">“</st>`<st c="9793">HouseAge</st>`<st
    c="9801">”</st>`<st c="9803">].quantile(0.75)</st>`<st c="9819">).</st> <st c="9823">The
    whiskers extended to the minimum and maximum values of the distribution.</st>
    <st c="9900">The limits of the whiskers in the plot did not coincide with those
    based on the IQR proximity rule (which we calculated in</st> *<st c="10023">step
    10</st>*<st c="10030">) because these limits were far beyond the value range observed
    for</st> <st c="10099">this</st> <st c="10103">variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10113">Finding outliers using the mean and standard deviation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="10168">I</st><st c="10170">n</st> <st c="10172">normally distributed
    variables, around 99.8% of the observations lie within the interval comprising
    the mean plus and minus</st> <st c="10296">thr</st><st c="10299">ee times the
    standard devi</st><st c="10326">ation.</st> <st c="10334">Thus, values beyond
    those limits can be considered outliers; they</st> <st c="10400">are rare.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10409">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10414">Using the mean and standard deviation to detect outliers has some
    drawbacks.</st> <st c="10492">Firstly, it assumes a normal distribution, including
    outliers.</st> <st c="10555">Secondly, outliers strongly influence the mean and
    standard deviation.</st> <st c="10626">Therefore, a recommended alternative is
    the</st> **<st c="10670">Median Absolute Deviation</st>** <st c="10695">(</st>**<st
    c="10697">MAD</st>**<st c="10700">), which</st> <st c="10709">we’ll discuss in
    the</st> <st c="10731">next recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10743">In this recipe, we will identify outliers as those observations
    that lie outside the interval delimited by the mean plus and minus three times
    the</st> <st c="10891">standa</st><st c="10897">rd deviation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10911">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10927">Let’s begin the</st> <st c="10943">recipe by importing the Python
    libraries and loading</st> <st c="10997">the dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11009">Let’s import the Python libraries</st> <st c="11044">and dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11177">Load the breast cancer dataset</st> <st c="11209">from scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11286">Create</st> <st c="11294">a function to plot a</st> <st c="11315">boxplot
    next to</st> <st c="11331">a histogram:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11584">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11589">We discussed the function from</st> *<st c="11621">step 3</st>*
    <st c="11627">in the previous recipe,</st> *<st c="11652">Visualizing outliers
    with boxplots and the inter-quartile</st>* *<st c="11710">proximity rule</st>*<st
    c="11724">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11725">Let’s plot the distribution of the</st> `<st c="11761">mean</st>`
    `<st c="11766">smoothness</st>` <st c="11776">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11830">In the</st> <st c="11837">following boxplot, we see that the variable’s
    values show a distribution like the</st> <st c="11919">normal distribution, and
    it has six outliers – one on the left and five on the</st> <st c="11999">right
    tail:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Boxplot and histogram of the variable mean smoothness](img/B22396_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12063">Figure 5.4 – Boxplot and histogram of the variable mean smoothness</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12129">Create a function that returns the mean plus and minus</st> `<st
    c="12185">fold</st>` <st c="12189">times the standard deviation, where</st> `<st
    c="12226">fold</st>` <st c="12230">is a parameter to</st> <st c="12249">the function:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12471">Use the</st> <st c="12479">function to identify the extreme limits
    of the</st> `<st c="12527">mean</st>` `<st c="12532">smoothness</st>` <st c="12542">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="12618">If we now execute</st> `<st c="12637">lower_limit</st>` <st c="12648">or</st>
    `<st c="12652">upper_limit</st>`<st c="12663">, we will see the values</st> `<st
    c="12688">0.0541</st>` <st c="12694">and</st> `<st c="12699">0.13</st><st c="12703">855</st>`<st
    c="12707">, correspon</st><st c="12718">ding to the limits beyond which we can
    consider a value</st> <st c="12775">an outlier.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="12786">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12791">The interval between the mean plus and minus three times the standard
    deviation encloses 99.87% of the observations if the variable is normally distributed.</st>
    <st c="12949">For less conservative limits, we could multiply the standard deviation
    by 2 or 2.5, which would produce intervals that enclose 95.4% and 97.6% of the</st>
    <st c="13099">observations, respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13126">Create</st> <st c="13134">a Boolean vector that flags observations
    with values beyond the limits determined in</st> *<st c="13219">step 6</st>*<st
    c="13225">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13339">If we now execute</st> `<st c="13358">outliers.sum()</st>`<st
    c="13372">, we will see the value</st> `<st c="13396">5</st>`<st c="13397">, indicating
    that there are five outliers or observations that are smaller or greater than
    the extreme values found with the mean and the standard deviation.</st> <st c="13555">According
    to these limits, we’d identify one outlier less compared to the</st> <st c="13629">IQR
    rule.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="13638">Let’s add red vertical lines to the histogram from</st> *<st c="13690">step
    3</st>* <st c="13696">to highlight the limits determined</st> <st c="13731">by
    using the mean and the</st> <st c="13758">standard deviation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14128">And</st> <st c="14133">now let’s make</st> <st c="14148">the plots:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14202">In the following plot, we see that the limits observed by the
    IQR proximity rule in the box plot are less conservative than those identified
    by the mean and the standard deviation.</st> <st c="14384">Hence why we observe
    six potential outliers in the boxplot, but only five based on the mean and standard</st>
    <st c="14489">deviation calculations:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Comparison of the limits between the whiskers in the boxplot
    and those determined by using the mean and the standard deviation (vertical lines
    in the histogram)](img/B22396_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="14560">Figure 5.5 – Comparison of the limits between the whiskers in
    the boxplot and those determined by using the mean and the standard deviation
    (vertical lines in the histogram)</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14733">The</st> <st c="14738">boundaries derived from the</st> <st c="14766">mean
    and standard deviation are symmetric.</st> <st c="14809">They extend equidistantly
    from the center of the distribution toward both tails.</st> <st c="14890">As previously
    mentioned, these boundaries are only suitable for normally</st> <st c="14962">distributed
    variables.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14985">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14999">With pandas’</st> `<st c="15013">mean()</st>` <st c="15019">and</st>
    `<st c="15024">std()</st>`<st c="15029">, we captured the mean and standard deviation
    of the variable.</st> <st c="15092">We determined the limits as the mean plus
    and minus three times the standard deviation.</st> <st c="15180">To highlight
    the outliers, we used NumPy’s</st> `<st c="15223">where()</st>`<st c="15230">.
    The</st> `<st c="15236">where()</st>` <st c="15243">function scanned the rows
    of the vari</st><st c="15281">able, and if the value was</st> <st c="15309">greater
    than the upper limit or smaller than the lower limit, it was assigned</st> `<st
    c="15387">True</st>`<st c="15391">, and alternatively</st> `<st c="15411">False</st>`<st
    c="15416">. Finally, we used pandas’</st> `<st c="15443">sum()</st>` <st c="15448">over
    this Boolean vector to calculate the total number</st> <st c="15504">of outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15516">Finally, we compared the boundaries to determine outliers returned
    by the IQR proximity rule, which we discussed in the previous recipe,</st> *<st
    c="15654">Visualizing outliers with boxplots and the inter-quartile proximity
    rule</st>*<st c="15726">, and the mean and the standard deviation.</st> <st c="15769">We
    observed that the limits of the IQR rule are less conservative.</st> <st c="15836">That
    means that with the IQR rule, we’d flag more outliers in this</st> <st c="15903">particular
    variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15923">Using the median absolute deviation to find outliers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="15976">The mean</st> <st c="15986">and the standard deviation are heavily
    impacted by outliers.</st> <st c="16047">Hence, using these parameters to identify
    outliers can defeat the purpose.</st> <st c="16122">A better way to identify outliers
    is</st> <st c="16159">by using MAD.</st> <st c="16173">MAD is the median of the
    absolute deviation between each observation and the median value of</st> <st c="16266">the
    variable:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>M</mi><mi>A</mi><mi>D</mi><mo>=</mo><mi>b</mi><mo>×</mo><mi>M</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mo>(</mo><mfenced
    open="|" close="|"><mrow><mi>x</mi><mi>i</mi><mo>−</mo><mi>M</mi><mi>e</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>n</mi><mfenced
    open="(" close=")"><mi>X</mi></mfenced></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></math>](img/24.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="16314">In the previous equation,</st> `<st c="16340">xi</st>` <st c="16342">is
    each observation in the</st> `<st c="16370">X</st>` <st c="16371">variable.</st>
    <st c="16382">The beauty of MAD is that it uses the median instead of the mean,
    which is robust to outliers.</st> <st c="16477">The</st> `<st c="16481">b</st>`
    <st c="16482">constant is used to estimate the standard deviation from MAD, and
    if we assume normality, then</st> `<st c="16578">b =</st>` `<st c="16582">1.4826</st>`<st
    c="16588">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16589">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16594">If the variable is assumed to have a different distribution,</st>
    `<st c="16656">b</st>` <st c="16657">is then calculated as 1 divided by the 75th
    percentile.</st> <st c="16714">In the case of normality, 1/75th percentile =</st>
    <st c="16760">1.4826.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16767">After computing MAD, we use the median and MAD to establish distribution
    limits, designating values beyond these limits as outliers.</st> <st c="16901">The
    limits are set as the median plus and minus a multiple of MAD, typically ranging
    from 2 to 3.5\.</st> <st c="17001">The multiplication factor we choose reflects
    how stringent we want to be (the higher, the more conservative).</st> <st c="17111">In
    this recipe, we will identify outliers</st> <st c="17153">using MAD.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17163">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="17179">Let’s begin the recipe by importing the Python libraries and loading</st>
    <st c="17249">the dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17261">Let’s import the Python libraries</st> <st c="17296">and dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17429">Load the</st> <st c="17439">breast cancer dataset</st> <st c="17461">from
    scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17538">Create a</st> <st c="17548">function that returns the limits based</st>
    <st c="17587">on MAD:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17830">Let’s use the function to capture the extreme limits of the</st>
    `<st c="17891">mean</st>` `<st c="17896">smoothness</st>` <st c="17906">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17981">If we execute</st> `<st c="17996">lower_limit</st>` <st c="18007">or</st>
    `<st c="18011">upper_limit</st>`<st c="18022">, we will see the values</st> `<st
    c="18047">0.0536</st>` <st c="18053">and</st> `<st c="18058">0.13812</st>`<st
    c="18065">, corresponding to the limits beyond which we can consider a value</st>
    <st c="18132">an outlier.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18143">Let’s create a Boolean vector that flags observations with values
    beyond</st> <st c="18217">the limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="18340">If we</st> <st c="18347">now execute</st> `<st c="18359">outliers.sum()</st>`<st
    c="18373">, we will see the value</st> `<st c="18397">5</st>`<st c="18398">, indicating
    that there are five outliers or observations that are smaller or greater than
    the extreme values found</st> <st c="18514">with MAD.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="18523">Let’s</st> <st c="18530">make a function to plot a boxplot next
    to a histogram of a variable, highlighting in the histogram the limits calculated
    in</st> *<st c="18654">step 4</st>*<st c="18660">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19013">And now let’s make</st> <st c="19033">the plots:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="19087">In the following plot, we see that the limits observed by the
    IQR proximity rule in the box plot are less conservative than those identified
    by using MAD.</st> <st c="19243">MAD returns</st> <st c="19255">symmetric boundaries,
    while the boxplot generates asymmetric boundaries, which are</st> <st c="19338">more
    suitable for highly</st> <st c="19363">skewed distributions:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Comparison of the limits between the whiskers in the boxplot
    and those determined by using MAD](img/B22396_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="19416">Figure 5.6 – Comparison of the limits between the whiskers in
    the boxplot and those determined by using MAD</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19523">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19528">Detecting outliers with MAD requires that the variable has certain
    variability.</st> <st c="19609">If more than 50% of the values in a variable are
    identical, the median coincides with the most frequent value, and MAD=0\.</st>
    <st c="19731">This means that all values different from the median will be flagged
    as outliers.</st> <st c="19813">This constitutes another limitation of using MAD
    in</st> <st c="19865">outlier detection.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19883">That’s it!</st> <st c="19895">You now know how to identify outliers
    using the median</st> <st c="19950">and MAD.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19958">How it works…</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="19972">We</st> <st c="19976">determined the median with pandas’</st>
    `<st c="20011">median()</st>` <st c="20019">and the absolute difference with pandas’</st>
    `<st c="20061">abs()</st>`<st c="20066">. Next, we used the NumPy</st> `<st c="20092">where()</st>`
    <st c="20099">function to create a Boolean vector with</st> `<st c="20141">True</st>`
    <st c="20145">if a value was greater than the upper limit or smaller than the
    lower limit, otherwise</st> `<st c="20233">False</st>`<st c="20238">. Finally,
    we used pandas’</st> `<st c="20265">sum()</st>` <st c="20270">over this Boolean
    vector to calculate the total number</st> <st c="20326">of outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20338">Finally, we</st> <st c="20351">compared the boundaries to determine
    outliers returned by the IQR proximity rule, which we discussed in the</st> *<st
    c="20459">Visualizing outliers with boxplots and the inter-quartile range proximity
    rule</st>* <st c="20537">recipe, and those returned by using MAD.</st> <st c="20579">The
    limits returned by the IQR rule were less conservative.</st> <st c="20639">This
    behavior can be changed by multiplying the IQR by 3 instead of 1.5, which is the
    default value in boxplots.</st> <st c="20752">In addition, we noted that MAD returns
    symmetric boundaries, whereas the boxplot provided asymmetric limits, which could
    be better suited for</st> <st c="20894">asymmetric distributions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20919">See also</st>
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: <st c="20928">For a thorough discussion of the advantages and limitations of
    the different methods to detect outliers, check out the</st> <st c="21048">following
    resources:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21068">Rousseeuw PJ, Croux C.</st> *<st c="21092">Alternatives to the
    Median Absolute deviation</st>*<st c="21137">. Journal of the American Statistical
    Association,</st> <st c="21188">1993\.</st> [<st c="21194">http://www.jstor.org/stable/2291267</st>](https://www.jstor.org/stable/2291267)<st
    c="21229">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '<st c="21230">Leys C, et.</st> <st c="21243">al.</st> *<st c="21247">Detecting
    outliers: Do not use standard deviation around the mean, use absolute deviation
    around the median</st>*<st c="21354">. Journal of Experimental Social Psychology,</st>
    <st c="21399">2013\.</st> <st c="21405">http://dx.doi.org/10.1016/j.jesp.2013.03.013</st><st
    c="21449">.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21450">Thériault R, et.</st> <st c="21468">al.</st> *<st c="21472">Check
    your outliers</st>**<st c="21491">! An introduction to identifying statistical
    outliers in R with easystats</st>*<st c="21564">. Behavior Research Methods,</st>
    <st c="21593">2024\.</st> [<st c="21599">https://doi.</st><st c="21611">org/10.3758/s13428-024-02356-w</st>](https://link.springer.com/article/10.3758/s13428-024-02356-w)<st
    c="21642">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="21643">Removing outliers</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '<st c="21661">Recent</st> <st c="21669">studies distinguish three types of
    outliers: error outliers, interesting outliers, and random outliers.</st> <st
    c="21773">Error outliers are likely due to human or methodological errors and
    should be either corrected or removed from the data analysis.</st> <st c="21903">In
    this recipe, we’ll assume outliers are errors (you don’t want to remove interesting
    or random outliers) a</st><st c="22011">nd remove them from</st> <st c="22032">the
    dataset.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22044">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="22060">We’ll use the IQR proximity rule to find the outliers and then
    remove them from the data using pandas</st> <st c="22163">and Feature-engine:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22182">Let’s import the Python libraries, functions,</st> <st c="22229">and
    classes:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22454">Load the California housing dataset from scikit-learn and separate
    it into train and</st> <st c="22540">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="22705">Let’s create a function to find the limits beyond which we’ll
    consider a data point an outlier using the IQR</st> <st c="22815">proximity rule:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23043">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23048">In</st> *<st c="23052">step 3</st>*<st c="23058">, we use the
    IQR proximity rule to find the limits beyond which data points will be considered
    outliers, which we discussed in the</st> *<st c="23189">Visualizing outliers with
    boxplots and the inter-quartile proximity rule</st>* <st c="23261">recipe.</st>
    <st c="23270">Alternatively, you can identify outliers with the mean and the standard
    deviation or MAD, as we covered in the</st> *<st c="23381">Finding outliers using
    the mean and standard deviation</st>* <st c="23435">and</st> *<st c="23440">Using
    the median absolute deviation to find</st>* *<st c="23484">outliers</st>* <st
    c="23492">recipes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23501">Using the</st> <st c="23511">function from</st> *<st c="23526">step
    3</st>* <st c="23532">, let’s determine the limits of the</st> `<st c="23568">MedInc</st>`
    <st c="23574">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23633">If you execute</st> `<st c="23649">print(lower_limit, upper_limit)</st>`<st
    c="23680">, you’ll see the result of the previous command:</st> `<st c="23729">(-</st>``<st
    c="23731">3.925900000000002, 11.232600000000001)</st>`<st c="23770">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="23771">Let’s retain the observations in the train and test sets whose
    values are greater than or equal to (</st>`<st c="23872">ge</st>`<st c="23875">)
    the</st> <st c="23882">lower limit:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24029">Let’s retain the observations whose values are lower than or equal
    to (</st>`<st c="24101">le</st>`<st c="24104">) the</st> <st c="24111">upper limit:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24258">Go</st> <st c="24262">ahead and execute</st> `<st c="24280">X_train.shape</st>`
    <st c="24293">followed by</st> `<st c="24306">train_t.shape</st>` <st c="24319">to
    corroborate that the transformed DataFrame contains fewer observations than the
    original one after removing</st> <st c="24431">the outliers.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="24444">We can remove outliers across multiple variables simultaneously</st>
    <st c="24509">with</st> `<st c="24514">feature-engine</st>`<st c="24528">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="24529">Set up a transformer to identify outliers in three variables by
    using the</st> <st c="24604">IQR rule:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24736">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="24741">OutlierTrimmer</st>` <st c="24756">can identify boundaries using
    the IQR, as we show in this recipe, as well as by using the mean and standard
    deviation, or MAD.</st> <st c="24884">You need to change</st> `<st c="24903">capping_method</st>`
    <st c="24917">to</st> `<st c="24921">gaussian</st>` <st c="24929">or</st> `<st
    c="24933">mad</st>`<st c="24936">, respectively.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24951">Fit the transformer to the training set so that it learns</st>
    <st c="25010">those limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25044">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="25049">By executing</st> `<st c="25063">trimmer.left_tail_caps_</st>`<st
    c="25086">, we can visualize the lower limits for the three variables:</st> `<st
    c="25147">{''MedInc'': -0.6776500000000012, ''HouseAge'': -10.5, ''Population'':
    -626.0}</st>`<st c="25219">. By executing</st> `<st c="25234">trimmer.right_tail_caps_</st>`<st
    c="25258">, we can see the variables’ upper limits:</st> `<st c="25300">{''MedInc'':
    7.984350000000001, ''HouseAge'': 65.5, ''</st>``<st c="25349">Population'': 3134.0}</st>`<st
    c="25370">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25371">Finally, let’s remove outliers from the train and</st> <st c="25422">test
    sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25509">To finish with the recipe, let’s compare the distribution of a
    variable before and after</st> <st c="25599">removing outliers.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="25617">Let’s create</st> <st c="25630">a function to display a boxplot
    on top of</st> <st c="25673">a histogram:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="25927">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25932">We discussed the code in</st> *<st c="25958">step 10</st>* <st
    c="25965">in the</st> *<st c="25973">Visualizing outliers with boxplots</st>*
    <st c="26007">recipe earlier in</st> <st c="26026">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26039">Let’s plot the distribution of</st> `<st c="26071">MedInc</st>`
    <st c="26077">before removing</st> <st c="26094">the outliers:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26148">In the following plot, we see that</st> `<st c="26184">MedInc</st>`
    <st c="26190">is skewed and observations grea</st><st c="26222">ter than 8 are
    marked</st> <st c="26245">as outliers:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.7– Boxplot and the histogram of MedInc before removing outliers.](img/B22396_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26277">Figure 5.7– Boxplot and the histogram of MedInc before removing
    outliers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26350">Finally, let’s plot</st> <st c="26371">the distribution of</st>
    `<st c="26391">MedInc</st>` <st c="26397">after removing</st> <st c="26413">the
    outliers:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26467">After removing outliers,</st> `<st c="26493">MedInc</st>` <st
    c="26499">seems less skewed and i</st><st c="26523">ts values more</st> <st c="26539">evenly
    distributed:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Boxplot and the histogram of MedInc after removing outliers](img/B22396_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="26577">Figure 5.8 – Boxplot and the histogram of MedInc after removing
    outliers</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26649">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="26654">Using the IQR rule over the transformed variable reveals new
    outliers.</st> <st c="26726">This is not surprising; removing observations at
    the extremes of the distribution alters parameters such as the median and quartile
    values, which in turn determine the length of the whiskers, potentially identifying
    additional observations as outliers.</st> <st c="26979">The tools that we use
    to identify outliers are just that: tools.</st> <st c="27044">To unequivocally
    identify outliers, we need to support these tools with additional</st> <st c="27127">data
    analysis.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27141">If thinking</st> <st c="27153">of removing error outliers from
    the dataset, make sure to compare and report the results with and without outliers,
    to see the ext</st><st c="27284">ent of their impact on</st> <st c="27308">the
    models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27319">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="27335">The</st> `<st c="27340">ge()</st>` <st c="27344">and</st> `<st
    c="27349">le()</st>` <st c="27353">methods</st> <st c="27362">from pandas created
    Boolean vectors identifying observations exceeding or falling below thresholds
    set by the IQR proximity rule.</st> <st c="27492">We used these vectors with pandas</st>
    `<st c="27526">loc</st>` <st c="27529">to retain observations within the interval
    defined by</st> <st c="27584">the IQR.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27592">The</st> `<st c="27597">feature-engine</st>` <st c="27611">library’s</st>
    `<st c="27622">OutlierTrimmer()</st>` <st c="27638">automates the procedure of
    removing outliers for multiple variables.</st> `<st c="27708">OutlierTrimmer()</st>`
    <st c="27724">can identify outliers based on the mean and standard deviation,
    IQR proximity rule, MAD, or quantiles.</st> <st c="27828">We can modify this behavior
    through the</st> `<st c="27868">capping_method</st>` <st c="27882">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27893">The methods to identify outliers can be made more or less conservative
    by changing the factor by which we multiply the IQR, the standard deviation, or
    MAD.</st> <st c="28050">With</st> `<st c="28055">OutlierTrimmer()</st>`<st c="28071">,
    we can control the strength of the methods through the</st> `<st c="28128">fold</st>`
    <st c="28132">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28143">With</st> `<st c="28149">tails</st>` <st c="28154">set to</st>
    `<st c="28162">"both"</st>`<st c="28168">,</st> `<st c="28170">OutlierTrimmer()</st>`
    <st c="28186">found and removed outliers at both ends of the variables’ distribution.</st>
    <st c="28259">To remove outliers just on one of the tails, we can pass</st> `<st
    c="28316">"left"</st>` <st c="28322">or</st> `<st c="28326">"right"</st>` <st
    c="28333">to the</st> `<st c="28341">tails</st>` <st c="28346">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="28357">OutlierTrimmer()</st>` <st c="28374">adopts the scikit-learn
    functionality with the</st> `<st c="28422">fit()</st>` <st c="28427">method, to
    learn parameters, and</st> `<st c="28461">transform()</st>` <st c="28472">to modify
    the dataset.</st> <st c="28496">With</st> `<st c="28501">fit()</st>`<st c="28506">,
    the transformer learned and stored the limits for each variable.</st> <st c="28573">With</st>
    `<st c="28578">transform()</st>`<st c="28589">, it removed the outliers from the
    data, returning</st> `<st c="28640">pandas</st>` <st c="28646">DataFrames.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28658">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '<st c="28667">This is the study that I mentioned earlier that classifies outliers
    into errors; it is interesting and random: Leys C, et.al.</st> <st c="28794">2019\.</st>
    *<st c="28800">How to Classify, Detect, and Manage Univariate and Multivariate
    Outliers, with Emphasis on Pre-Registration</st>*<st c="28907">. International
    Review of Social</st> <st c="28940">Psycholo</st><st c="28948">gy.</st> [<st c="28953">https://doi.org/10.5334/irsp.289</st>](https://rips-irsp.com/articles/10.5334/irsp.289)<st
    c="28985">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28986">Bringing outliers back within acceptable limits</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="29034">Removing</st> <st c="29044">error outliers can be a valid strategy.</st>
    <st c="29084">However, this approach can reduce statistical power, in particular
    when there are outliers across many variables, because we end up removing big
    parts of the dataset.</st> <st c="29251">An alternative way to handle error outliers
    is by bringing outliers back within acceptable limits.</st> <st c="29350">In practice,
    what this means is replacing the value of the outliers with some thresholds identified
    with the IQR proximity rule, the mean and standard deviation, or MAD.</st> <st
    c="29520">In this recipe, we’ll replace outlier va</st><st c="29560">lues using</st>
    `<st c="29572">pandas</st>` <st c="29578">and</st> `<st c="29583">feature-engine</st>`<st
    c="29597">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29598">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="29614">We’ll use</st> <st c="29625">the mean and standard deviation to
    find outliers and then replace their values u</st><st c="29705">sing</st> `<st
    c="29711">pandas</st>` <st c="29717">and</st> `<st c="29722">feature-engine</st>`<st
    c="29736">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29738">Let’s import the required Python libraries</st> <st c="29781">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29943">Load the breast cancer dataset from scikit-learn and separate
    it into train and</st> <st c="30024">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30183">Let’s create a function to find outliers using the mean and</st>
    <st c="30244">standard deviation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30472">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30477">In</st> *<st c="30481">step 3</st>*<st c="30487">, we use the
    mean and standard deviation to find the limits beyond which data points will be
    considered outliers, as discussed in the</st> *<st c="30621">Finding outliers
    using the mean and standard deviation</st>* <st c="30675">recipe.</st> <st c="30684">Alternatively,
    you can identify outliers with the IQR rule or MAD, as we covered in the</st>
    *<st c="30772">Visualizing outliers with boxplots and the inter-quartile proximity
    rule</st>* <st c="30844">and</st> *<st c="30849">Using the median absolute deviation
    to find</st>* *<st c="30893">outliers</st>* <st c="30902">recipes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30910">Using</st> <st c="30916">the function from</st> *<st c="30935">step
    3</st>*<st c="30941">, let’s determine the limits of the</st> `<st c="30977">mean
    smoothness</st>` <st c="30992">variable, which follows approximately a</st> <st
    c="31033">Gaussian distribution:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31137">Let’s make a copy of the</st> <st c="31163">original datasets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31229">Now, replace outliers with the lower or upper limits from</st>
    *<st c="31288">step 4</st>* <st c="31294">in the</st> <st c="31302">new DataFrames:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31459">To corroborate that the outliers were replaced with the values
    determined in</st> *<st c="31537">step 4</st>*<st c="31543">, execute</st> `<st
    c="31553">train_t["worst smoothness"].agg(["min", "max"])</st>` <st c="31600">to
    obtain the new maximum and minimum values.</st> <st c="31647">They should coincide
    with the minimum and maximum values of the variable, or the limits returned in</st>
    *<st c="31747">step 4</st>*<st c="31753">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="31754">We can replace outliers in multiple variables simultaneously by</st>
    <st c="31819">utilizing</st> `<st c="31829">feature-engine</st>`<st c="31843">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="31844">Let’s set up a transformer to replace outliers in two variables,
    using limits determined</st> <st c="31934">with the mean and</st> <st c="31952">standard
    deviation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32091">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="32096">Winsorizer</st>` <st c="32107">can</st> <st c="32111">identify
    boundaries using the mean and standard deviation, as we show in this recipe, as
    well as the IQR proximity rule and MAD.</st> <st c="32241">You need to change</st>
    `<st c="32260">capping_meth</st><st c="32272">od</st>` <st c="32275">to</st> `<st
    c="32279">iqr</st>` <st c="32282">or</st> `<st c="32286">mad</st>`<st c="32289">,
    respectively.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32304">Let’s fit the transformer to the data so that it learns</st> <st
    c="32361">those limits:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '<st c="32394">By executing</st> `<st c="32408">capper.left_tail</st><st c="32424">_caps_</st>`<st
    c="32431">, we can visualize the lower limits for the two variables:</st> `<st
    c="32490">{''worst smoothness'': 0.06364743973736293, ''worst texture'': 7.115307053129349}</st>`<st
    c="32567">. By executing</st> `<st c="32582">capper.right_tail_caps_</st>`<st
    c="32605">, we can see the variables’ upper limits:</st> `<st c="32647">{''worst
    smoothness'': 0.20149734880520967, ''worst</st>` `<st c="32696">texture'': 43.97692158753917}</st>`<st
    c="32724">.</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="32725">Finally, let’s replace the outliers with the limits from</st>
    *<st c="32783">step 8</st>*<st c="32789">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="32861">If we now execute</st> `<st c="32880">train_t[capper.variables_].agg(["min",
    "max"])</st>`<st c="32926">, we’ll see that the maximum and minimum values of
    the transformed DataFrame coincide with either the maximum and minimum values
    of the variables or the identified limits, whatever</st> <st c="33107">comes first:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="33196">If you are</st> <st c="33207">planning to cap variables, make
    sure you compare the performance of your models or the results of your ana</st><st
    c="33314">lysis before and after</st> <st c="33338">replacing outliers.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="33357">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33373">The</st> `<st c="33378">clip()</st>` <st c="33384">function from</st>
    <st c="33398">pandas is used to cap values at lower or upper specified limits.</st>
    <st c="33464">In this recipe, we found those limits using the mean and standard
    deviation, and then clipped the variable so that all observations took values
    within those limits.</st> <st c="33629">The minimum value of the</st> `<st c="33654">worst
    smoothness</st>` <st c="33670">variable was actually greater than the lower limit
    we found in</st> *<st c="33734">step 4</st>*<st c="33740">, so no values were
    replaced at the left of its distribution.</st> <st c="33802">However, there were
    values greater than the upper limit from</st> *<st c="33863">step 4</st>*<st c="33869">,
    and those were replaced with the limit.</st> <st c="33911">This means that the
    minimum value of the transformed variable and the original variable coincide,
    but the maximum values</st> <st c="34032">do not.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34039">We used</st> `<st c="34048">feature-engine</st>` <st c="34062">to
    replace outliers in multiple variables simultaneously.</st> `<st c="34121">Winsorizer()</st>`
    <st c="34133">can identify outliers based on the mean and standard deviation,
    the IQR proximity rule, MAD, or by using percentiles.</st> <st c="34252">We can
    modify this behavior through the</st> `<st c="34292">capping_method</st>` <st
    c="34306">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34317">The methods to identify outliers can be made more or less conservative
    by changing the factor by which we multiply the IQR, the standard deviation, or
    MAD.</st> <st c="34474">With</st> `<st c="34479">Winsorizer()</st>`<st c="34491">,
    we can control the strength of the methods through the</st> `<st c="34548">fold</st>`
    <st c="34552">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34563">With</st> `<st c="34569">tails</st>` <st c="34574">set to</st>
    `<st c="34582">"both"</st>`<st c="34588">,</st> `<st c="34590">Winsorizer()</st>`
    <st c="34602">found and replaced outliers at both ends of the variables’ distribution.</st>
    <st c="34676">To replace outliers at either end, we can pass</st> `<st c="34723">"left"</st>`
    <st c="34729">or</st> `<st c="34733">"right"</st>` <st c="34740">to the</st> `<st
    c="34748">tails</st>` <st c="34753">parameter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="34764">Winsorizer()</st>` <st c="34777">adopts the scikit-learn functionality
    with the</st> `<st c="34825">fit()</st>` <st c="34830">method, to learn parameters,
    and</st> `<st c="34864">transform()</st>` <st c="34875">to modify the dataset.</st>
    <st c="34899">With</st> `<st c="34904">fit()</st>`<st c="34909">, the transformer
    learned and</st> <st c="34939">stored the limits for each variable.</st> <st c="34976">With</st>
    `<st c="34981">transform()</st>`<st c="34992">, it replaced the values of the
    ou</st><st c="35026">tliers, returning</st> <st c="35045">pandas DataFrames.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35063">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="35072">feature-engine</st>` <st c="35087">has</st> `<st c="35092">ArbitraryOutlierCapper()</st>`<st
    c="35116">, which caps variables at arbitrary minimum and maximum</st> <st c="35172">values:</st>
    [<st c="35180">https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html</st>](https://feature-engine.readthedocs.io/en/latest/api_doc/outliers/ArbitraryOutlierCapper.html)<st
    c="35272">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35273">Applying winsorization</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="35296">Winsorizing, or winsorization, consists</st> <st c="35336">of
    replacing extreme, poorly known</st> <st c="35372">observations, that is, outliers,
    with the magnitude of the next largest (or smallest) observation.</st> <st c="35471">It’s
    similar to the procedure described in the previous recipe,</st> *<st c="35535">Bringing
    outliers back within acceptable limits</st>*<st c="35582">, but not exactly the
    same.</st> <st c="35610">Winsorization involves replacing the</st> *<st c="35647">same
    number of outliers</st>* <st c="35670">at both ends of the distribution, which
    makes Winsorization a symmetric process.</st> <st c="35752">This guarantees that
    the</st> **<st c="35777">Winsorized mean</st>**<st c="35792">, that is, the</st>
    <st c="35806">mean estimated after replacing outliers, remains a robust estimator
    of the central tendency of</st> <st c="35902">the variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35915">In practice, to</st> <st c="35931">remove a similar number of
    observations at both tails, we’d use percentiles.</st> <st c="36009">For example,
    the 5th percentile is the value below which 5% of the observations lie and the
    95th percentile is the value beyond which 5% of the observations lie.</st> <st
    c="36171">Using these values as replacements might result in replacing a similar
    number of observations on both tails, but it’s not guaranteed.</st> <st c="36305">If
    the dataset contains repeated values, obtaining reliable percentiles is challenging
    and can lead to an uneven replacement of values at each tail.</st> <st c="36454">If
    this happens, then the winsorized mean is not a good estimator of the central
    tenden</st><st c="36541">cy.</st> <st c="36546">In this recipe, we will</st> <st
    c="36570">apply winsorization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36590">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="36606">We will cap all</st> <st c="36623">variables of the breast cancer
    dataset at their 5th and</st> <st c="36679">95th percentiles:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36696">Let’s import the required Python libraries</st> <st c="36740">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36909">Load the breast cancer dataset</st> <st c="36941">from scikit-learn:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37018">Separate</st> <st c="37027">the data into a train and</st> <st
    c="37054">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37156">Capture the 5th and 95th percentiles of each variable</st> <st
    c="37211">in dictionaries:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37305">Let’s now replace values beyond those percentiles with the respective
    percentiles for all variables</st> <st c="37406">at once:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37502">Let’s display the minimum, maximum, and mean values of one variable</st>
    <st c="37571">before winsorization:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37658">We can see the values in the</st> <st c="37688">following output:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37784">Display the</st> <st c="37796">minimum, maximum, and mean values</st>
    <st c="37831">of the same variable</st> <st c="37852">after winsorization:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37905">In the following output, we can see that the minimum and maximum
    values correspond to the percentiles.</st> <st c="38009">However, the mean is
    quite similar to the original mean of</st> <st c="38068">the variable:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38160">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38165">If you want to use winsorization as part of a scikit-learn pipeline,
    you can use the</st> `<st c="38251">feature-engine</st>` <st c="38265">library’s</st>
    `<st c="38276">Winsorizer()</st>`<st c="38288">, by setting it up</st> <st c="38307">as
    follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="38318">capper =</st>` `<st c="38328">Winsorizer(</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**<st c="38339">capping_method="quantiles",</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="38367">tail="both",</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`**<st c="38380">fold=0.05,</st>**`'
  prefs: []
  type: TYPE_NORMAL
- en: '**`<st c="38391">)</st>`'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38393">After this, proceed with the</st> `<st c="38422">fit()</st>` <st
    c="38427">and</st> `<st c="38432">transform()</st>` <st c="38443">methods as described
    in the</st> *<st c="38472">Bringing outliers back within acceptable</st>* *<st
    c="38513">limits</st>* <st c="38519">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38527">It’s</st> <st c="38533">worth</st> <st c="38539">noting that despite
    employing percentiles, the procedure didn’t precisely replace the same number
    of observations on both sides of the distribution.</st> <st c="38688">If you intend
    to winsorize your variables, compare the out</st><st c="38746">comes of your analyses
    before and</st> <st c="38781">after winsorization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38801">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="38817">We used pandas’</st> `<st c="38834">quantiles()</st>` <st c="38845">to
    obtain the 5th and 95th percentiles of all the variables in the dataset, and combined
    it with</st> `<st c="38943">to_dict()</st>` <st c="38952">to retain those percentiles
    in dictionaries, where the keys were the variables and the values were the percentiles.</st>
    <st c="39069">We then passed these dictionaries to pandas’</st> `<st c="39114">clip()</st>`
    <st c="39120">to replace values smaller or larger than those percentiles by the
    percentiles.</st> <st c="39200">By using dictionaries, we capped multiple variables</st>
    <st c="39252">at once.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39260">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="39269">For more details about how winsorization affects the mean and
    standard deviation in symmetric and asymmetric replacements, check out the</st>
    <st c="39407">original article:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39424">Dixon W.</st> *<st c="39434">Simplified Estimation from Censored
    Normal Samples.</st> <st c="39486">The Annals of Mathematica</st><st c="39511">l
    Statistics</st>*<st c="39524">,</st> <st c="39526">1960\.</st> [<st c="39532">http://www.jstor.org/stable/2237953</st>](https://www.jstor.org/stable/2237953)******
  prefs: []
  type: TYPE_NORMAL
