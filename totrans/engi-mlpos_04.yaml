- en: 'Chapter 3: Code Meets Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll get started with hands-on **MLOps** implementation as
    we learn by solving a business problem using the MLOps workflow discussed in the
    previous chapter. We'll also discuss effective methods of source code management
    for **machine learning** (**ML**), explore data quality characteristics, and analyze
    and shape data for an ML solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin this chapter by categorizing the business problem to curate a best-fit
    MLOps solution for it. Following this, we''ll set up the required resources and
    tools to implement the solution. 10 guiding principles for source code management
    for ML are discussed to apply clean code practices. We will discuss what constitutes
    good-quality data for ML and much more, followed by processing a dataset related
    to the business problem and ingesting and versioning it to the ML workspace. Most
    of the chapter is hands-on and designed to equip you with a good understanding
    of and experience with MLOps. For this, we''re going to cover the following main
    topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Business problem analysis and categorizing the problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up resources and tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 principles of source code management for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good data for machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data registration and versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toward an ML pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without further ado, let's jump into demystifying the business problem and implementing
    the solution using an MLOps approach.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem analysis and categorizing the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked into the following business problem statement.
    In this section, we will demystify the problem statement by categorizing it using
    the principles to curate an implementation roadmap. We will glance at the dataset
    given to us to address the business problem and decide what type of ML model will
    address the business problem efficiently. Lastly, we'll categorize the MLOps approach
    for implementing robust and scalable ML operations and decide on tools for implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the problem statement:'
  prefs: []
  type: TYPE_NORMAL
- en: You work as a data scientist with a small team of data scientists for a cargo
    shipping company based in Finland. 90% of goods are imported into Finland via
    cargo shipping. You are tasked with saving 20% of the costs for cargo operations
    at the port of Turku, Finland. This can be achieved by developing an ML solution
    that predicts weather conditions at the port 4 hours in advance. You need to monitor
    for possible rainy conditions, which can distort operations at the port with human
    resources and transportation, which in turn affects supply chain operations at
    the port. Your ML solution will help port authorities to predict possible rain
    4 hours in advance; this will save 20% of costs and enable smooth supply chain
    operations at the port.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in solving a problem is to simplify and categorize it using an
    appropriate approach. In the previous chapter, we discussed how to categorize
    a business problem to solve it using ML. Let's apply those principles to chart
    a clear roadmap to implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll see what type of model we will train to yield the maximum business
    value. Secondly, we will identify the right approach for our MLOps implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to decide on the type of model to train, we can start by having a
    glance at the dataset available on GitHub: [https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a snapshot of `weather_dataset_raw.csv`, in *Figure 3.1*. The file
    size is 10.7 MB, the number of rows is 96,453, and the file is in CSV format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Dataset snapshot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Dataset snapshot
  prefs: []
  type: TYPE_NORMAL
- en: 'By assessing the data, we can categorize the business problem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Weather condition` column depicts whether an event has recorded rain, snow,
    or clear conditions. This can be framed or relabeled as `rain` or `no rain` and
    used to perform binary classification. Hence, it is straightforward to solve the
    business problem with a supervised learning approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLOps approach**: By observing the problem statement and data, here are the
    facts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(a) Data: The training data is 10.7 MB. The data size is reasonably small (it
    cannot be considered big data).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(b) Operations: We need to train, test, deploy, and monitor an ML model to
    forecast the weather at the port of Turku every hour (4 hours in advance) when
    new data is recorded.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(c) Team size: A small/medium team of data scientists, no DevOps engineers.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Based on the preceding facts, we can categorize the operations into **small
    team ops**; there is no need for big data processing and the team is small and
    agile. Now we will look at some suitable tools to implement the operations needed
    to solve the business problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'For us to get a holistic understanding of MLOps implementation, we will implement
    the business problems using two different tools simultaneously:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure Machine Learning** (Microsoft Azure)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow (**an open source cloud and platform-agnostic tool**)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use these two tools to see how things work from a pure cloud-based approach
    and from an open source / cloud-agnostic approach. All the code and CI/CD operations
    will be managed and orchestrated using Azure DevOps, as shown in *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – MLOps tools for the solution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – MLOps tools for the solution
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will set up the tools and resources needed to implement the solution
    for the business problem. As we will use Python as the primary programming language,
    it is a pre-requisite to have **Python 3** installed within your Mac, Linux, or
    Windows OS.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the resources and tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have these tools already installed and set up on your PC, feel free to
    skip this section; otherwise, follow the detailed instructions to get them up
    and running.
  prefs: []
  type: TYPE_NORMAL
- en: Installing MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We get started by installing MLflow, which is an open source platform for managing
    the ML life cycle, including experimentation, reproducibility, deployment, and
    a central model registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install MLflow, go to your terminal and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After successful installation, test the installation by executing the following
    command to start the `mlflow` tracking UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon running the `mlflow` tracking UI, you will be running a server listening
    at port `5000` on your machine, and it outputs a message like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can access and view the `mlflow` UI at `http://localhost:5000`. When you
    have successfully installed `mlflow` and run the tracking UI, you are ready to
    install the next tool.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Azure Machine Learning provides a cloud-based ML platform for training, deploying,
    and managing ML models. This service is available on Microsoft Azure, so the pre-requisite
    is to have a free subscription to Microsoft Azure. Please create a free account
    with around $170 of credit, which is sufficient to implement the solution, here:
    [https://azure.microsoft.com/](https://azure.microsoft.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: When you have access/a subscription to Azure, move on to the next section to
    get Azure Machine Learning up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a resource group
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A **resource group** is a collection of related resources for an Azure solution.
    It is a container that ties up all the resources related to a service or solution.
    Creating a resource group enables easy access and management of a solution. Let''s
    get started by creating your own resource group:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Azure portal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the portal menu (go to the portal's home page if you are not there by
    default) and hover over the resource group icon in the navigation section. A **Create**
    button will appear; click on it to create a new resource group:![Figure 3.3 –
    Creating a resource group
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16572_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 3.3 – Creating a resource group
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a resource group with the name of your choice (`Learn_MLOps` is recommended),
    as shown in *Figure 3.3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a region close to you to get the optimal performance and pricing. For
    example, in *Figure 3.3* a resource group with the name `Learn MLOps` and region
    **(Europe) North Europe** is ready to be created. After you click the **Review
    + Create** button and Azure validates the request, the final **Create** button
    will appear. The final **Create** button should be pressed to create the new resource
    group.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the resource group is reviewed and created, you can set up and manage all
    the services related to the ML solution in this resource group. The newly created
    resource group will be listed in the resource group list.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Azure Machine Learning workspace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An ML workspace is a central hub for tracking and managing your ML training,
    deploying, and monitoring experiments. To create an Azure Machine Learning workspace,
    go to the Azure portal menu, click on `Machine Learning` and select it. You will
    see the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Creating an Azure Machine Learning workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Creating an Azure Machine Learning workspace
  prefs: []
  type: TYPE_NORMAL
- en: Name the workspace with the name of your choice (for example, we've named it
    **MLOps_WS** in *Figure 3.4*). Select the resource group you created earlier to
    tie this ML service to it (**Learn_MLOps** is selected in *Figure 3.4*). Finally,
    hit the **Review + create** button and you will be taken to a new screen with
    the final **Create** button. Press the final **Create** button to create your
    Azure Machine Learning workspace.
  prefs: []
  type: TYPE_NORMAL
- en: After creating the Azure Machine Learning workspace (`Learn_MLOps`), the Azure
    platform will deploy all the resources this service needs. The resources deployed
    with the Azure Machine Learning instance (`Learn_MLOps`), such as Blob Storage,
    Key Vault, and Application Insights, are provisioned and tied to the workspace.
    These resources will be consumed or used via the workspace and the SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find detailed instructions on creating an Azure Machine Learning instance
    here: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Azure Machine Learning SDK
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Go to the terminal or command line in your PC and install the Azure Machine Learning
    SDK, which will be extensively used in the code to orchestrate the experiment.
    To install it, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find detailed instructions here: [https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py).'
  prefs: []
  type: TYPE_NORMAL
- en: Azure DevOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the source code and CI/CD-related operations will be managed and orchestrated
    using Azure DevOps. The code we manage in the repository in Azure DevOps will
    be used to train, deploy, and monitor ML models enabled by CI/CD pipelines. Let''s
    start by creating an Azure DevOps subscription:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a free account at [dev.azure.com](http://dev.azure.com). A free account
    can be created using a pre-existing Microsoft or GitHub account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a project named `Learn_MLOps` (make it public or private depending on
    your preference).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the **repos** section. In the **Import a repository** section, press the
    **Import** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import a repository from a public GitHub project from this repository: [https://github.com/PacktPublishing/EngineeringMLOps](https://github.com/PacktPublishing/EngineeringMLOps
    ) (as shown in *Figure 3.5*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Import the GitHub repository into the Azure DevOps project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Import the GitHub repository into the Azure DevOps project
  prefs: []
  type: TYPE_NORMAL
- en: After importing the GitHub repository, files from the imported repository will
    be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: JupyterHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lastly, we''ll need an interactive data analysis and visualization tool to
    process data using our code. For this, we use **JupyterHub**. This is a common
    data science tool used widely by data scientists to process data, visualize data,
    and train ML models. To install it, follow two simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install JupyterHub via the command line on your PC:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You may find detailed instructions here: [https://jupyterhub.readthedocs.io/en/stable/quickstart.html](https://jupyterhub.readthedocs.io/en/stable/quickstart.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Install Anaconda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Anaconda is needed as it installs dependencies, setup environments, and services
    to support the JupyterHub. Download Anaconda and install it as per the detailed
    instructions here: [https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we are set up for the hands-on implementation, let's look at what it
    takes to manage good code and data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 10 principles of source code management for ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are 10 principles that can be applied to your code to ensure the quality,
    robustness, and scalability of your code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modularity:** It is better to have modular code than to have one big chunk.
    Modularity encourages reusability and facilitates upgrading by replacing the required
    components. To avoid needless complexity and repetition, follow this golden rule:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two or more ML components should be paired only when one of them uses the other.
    If none of them uses each other, then pairing should be avoided.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: An ML component that is not tightly paired with its environment can be more
    easily modified or replaced than a tightly paired component.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Single task dedicated functions:** Functions are important building blocks
    of pipelines and the system, and they are small sections of code that are used
    to perform particular tasks. The purpose of functions is to avoid repetition of
    commands and enable reusable code. They can easily become a complex set of commands
    to facilitate tasks. For readable and reusable code, it is more efficient to have
    a single function dedicated to a single task instead of multiple tasks. It is
    better to have multiple functions than one long and complex function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Error 300`. Structuring blocks of code and trying to limit the maximum levels
    of indentation for functions and classes can enhance the readability of the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clean code:** If you have to explain the code, it''s not that good. Clean
    code is self-explanatory. It focuses on high readability, optimal modularity,
    reusability, non-repeatability, and optimal performance. Clean code reduces the
    cost of maintaining and upgrading your ML pipelines. It enables a team to perform
    efficiently and can be extended to other developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand this in depth, read *Clean Code: A Handbook of Agile Software
    Craftsmanship* by **Robert C Martin**.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Testing:** It is vital to ensure the robustness of a system, and testing
    plays an important role in this. In general, testing extends to unit testing and
    acceptance testing. Unit testing is a method by which components of source code
    are tested for robustness with coerced data and usage methods to determine whether
    the component is fit for the production system. Acceptance tests are done to test
    the overall system to ensure the system realizes user requirements; end-to-end
    business flows are verified in real-time scenarios. Testing is vital to ensure
    the efficient working of code: "if it isn''t tested, it is broken."'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To learn more about the implementation of unit testing, read this documentation:
    [https://docs.python.org/3/library/unittest.html](https://docs.python.org/3/library/unittest.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Version control (code, data and models):** Git is used for version control
    of code in ML systems. The purpose of version control is to ensure that all the
    team members working on the system have access to up-to-date code and that code
    is not lost when there is a hardware failure. One rule of working with Git should
    be to not break the master (branch). This means when you have working code in
    the repository and you add new features or make improvements, you do this in a
    feature branch, which is merged to the master branch when the code is working
    and reviewed. Branches should be given a short descriptive name, such as feature/label-encoder.
    Branch naming and approval guidelines should be properly communicated and agreed
    upon with the team to avoid any complexity and unnecessary conflicts. Code review
    is done with pull requests to the repository of the code. Usually, it is best
    to review code in small sets, less than 400 lines. In practice, it often means
    one module or a submodule at a time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Versioning of data is essential for ML systems as it helps us to keep track
    of which data was used for a particular version of code to generate a model. Versioning
    data can enable reproducing models and compliance with business needs and law.
    We can always backtrack and see the reason for certain actions taken by the ML
    system. Similarly, versioning of models (artifacts) is important for tracking
    which version of a model has generated certain results or actions for the ML system.
    We can also track or log parameters used for training a certain version of the
    model. This way, we can enable end-to-end traceability for model artifacts, data,
    and code. Version control for code, data, and models can enhance an ML system
    with great transparency and efficiency for the people developing and maintaining
    it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`print` statements are good for testing and debugging but not ideal for production.
    The logger contains information, especially system information, warnings, and
    errors, that are quite useful in the monitoring of production systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling:** Error handling is vital for handling edge cases, especially
    ones that are hard to anticipate. It is recommended to catch and handle exceptions
    even if you think you don''t need to, as prevention is better than cure. Logging
    combined with exception handling can be an effective way of dealing with edge
    cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Readability:** Code readability enables information transfer, code efficiency,
    and code maintainability. It can be achieved by following principles such as following
    industry-standard coding practices such as PEP-8 ([https://www.python.org/dev/peps/pep-0008/](https://www.python.org/dev/peps/pep-0008/))
    or the JavaScript standard style (depending on the language you are using). Readability
    is also increased by using docstrings. A docstring is a text that is written at
    the beginning of, for example, a function, describing what it does and possibly
    what it takes as input. In some cases, it is enough to have a one-liner explanation,
    such as this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A longer docstring is needed for a more complex function. Explaining the arguments
    and returns is a good idea:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Commenting and documenting:** Commenting and documentation are vital for
    maintaining sustainable code. It is not always possible to explain the code clearly.
    Comments can be useful in such cases to prevent confusion and explain the code. Comments
    can convey information such as copyright info, intent, clarification of code,
    possible warnings, and elaboration of code. Elaborate documentation of the system
    and modules can enable a team to perform efficiently, and the code and assets
    can be extended to other developers. For documentation, open source tools are
    available for documenting APIs such as Swagger ([https://swagger.io](https://swagger.io))
    and Read the Docs ([https://readthedocs.org](https://readthedocs.org)). Using
    the right tools for documentation can enable efficiency and standardize knowledge
    for developers.'
  prefs: []
  type: TYPE_NORMAL
- en: What is good data for ML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good ML models are a result of training on good-quality data. Before proceeding
    to ML training, a pre-requisite is to have good-quality data. Therefore, we need
    to process the data to increase its quality. So, determining the quality of data
    is essential. Five characteristics will enable us to discern the quality of data,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: Accuracy is a crucial characteristic of data quality, as having
    inaccurate data can lead to poor ML model performance and consequences in real
    life. To check the accuracy of the data, confirm whether the information represents
    a real-life situation or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: In most cases, incomplete information is unusable and can
    lead to incorrect outcomes if an ML model is trained on it. It is vital to check
    the comprehensiveness of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: Contradictions or duplications in data can lead to the unreliability
    of the data. Reliability is a vital characteristic; trusting the data is essential,
    primarily when it is used to make real-life decisions using ML. To some degree,
    we can assess the reliability of data by examining bias and distribution. In case
    of any extremities, the data might not be reliable for ML training or might carry
    bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance**: The relevance of data plays an essential role in contextualizing
    and determining if irrelevant information is being gathered. Having relevant data
    can enable appropriate decisions in real-life contexts using ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timeliness**: Obsolete or out-of-date information costs businesses time and
    money; having up-to-date information is vital in some cases and can improve the
    quality of data. Decisions enabled by ML using untimely data can be costly and
    can lead to wrong decisions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When these five characteristics are maximized, it ensures the highest data quality.
    With these principles in mind, let's delve into the implementation, where code
    meets data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, let''s assess the data and process it to get it ready for ML training.
    To get started, clone the repository you imported to your Azure DevOps project
    (from GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open your terminal and access the folder of the cloned repository and
    spin up the JupyterLab server for data processing. To do so, type the following
    command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will automatically open a window in your browser at `http://localhost:8888`
    where you can code and execute the code on the JupyterLab interface. In the `Code_meets_data_c3`
    folder, there is a Python script (`dataprocessing.py`) and a `.ipynb` notebook
    (`dataprocessing.ipynb`); feel free to run any of these files or create a new
    notebook and follow the upcoming steps.
  prefs: []
  type: TYPE_NORMAL
- en: We will perform computing for tasks as described in *Figure 3.6*. Data processing
    will be done locally on your PC, followed by ML training, deploying, and monitoring
    on compute targets in the cloud. This is to acquire experience of implementing
    models in various setups. In the rest of this chapter, we will do data processing
    (locally) to get the data to the best quality in order to do ML training (in the
    cloud, which is described in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Computation locations for data and ML tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Computation locations for data and ML tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'To process raw data and get it ready for ML, you will do the compute and data
    processing on your local PC. We start by installing and importing the required
    packages and importing the raw dataset (as shown in the `dataprocessing.ipynb`
    and `.py` scripts). Python instructions in the notebooks must be executed in the
    existing notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this, you have imported the dataset into a pandas DataFrame, `df`, for
    further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Raw data cannot be directly passed to the ML model for training purposes. We
    have to refine or preprocess the data before training the ML model. To further
    analyze the imported data, we will perform a series of steps to preprocess the
    data into a suitable shape for the ML training. We start by assessing the quality
    of the data to check for accuracy, completeness, reliability, relevance, and timeliness. After
    this, we calibrate the required data and encode text into numerical data, which
    is ideal for ML training. Lastly, we will analyze the correlations and time series,
    and filter out irrelevant data for training ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality assessment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To assess the quality of the data, we look for accuracy, completeness, reliability,
    relevance, and timeliness. Firstly, let''s check if the data is complete and reliable
    by assessing the formats, cumulative statistics, and anomalies such as missing
    data. We use pandas functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'By using the `describe` function, we can observe descriptive statistics in
    the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Descriptive statistics of the DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Descriptive statistics of the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Some observations can be made to conclude the data is coherent, and relevant
    as it depicts real-life statistics such as a mean temperature of ~11 C and a wind
    speed of ~10 kmph. Minimum temperatures in Finland tend to reach around ~-21 C,
    and there is an average visibility of 10 km. Facts like these depict the relevance
    and data origin conditions. Now, let''s observe the column formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the formats of each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '`S_No`                                           `int64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Timestamp`                                `object`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Location`                                     `object`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Temperature_C`                         `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Apparent_Temperature_C`      `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Humidity`                                    `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_speed_kmph`                  `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_bearing_degrees`           `int64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Visibility_km`                              `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pressure_millibars`                    `float64`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Weather_conditions`                `object`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype:`                                         `object`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most of the columns are numerical (`float` and `int`), as expected. The `Timestamp`
    column is in `object` format, which needs to be changed to `DateTime` format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Using pandas'' `to_datetime` function, we convert `Timestamp` to `DateTime`
    format. Next, let''s see if there are any null values. We use pandas'' `isnull`
    function to check this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Upon checking for any null values, if null values are discovered, as a next
    step the calibration of missing data is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Calibrating missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is not ideal to have missing values in the data as it is a sign of poor
    data quality. Missing data or values can be replaced using various techniques
    without compromising the correctness and reliability of data. After inspecting
    the data we have been working on, some missing values are observed. We use the
    `Forward fill` method to handle missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`NaN` or null values have only been observed in the `Weather_conditions` column.
    We replace the `NaN` values by using the `fillna()` method from pandas and the
    forward fill (`ffill`) method. As weather is progressive, it is likely to replicate
    the previous event in the data. Hence, we use the forward fill method, which replicates
    the last observed non-null value until another non-null value is encountered.'
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the machines do not understand human language or text, all the text has
    to be converted into numbers. Before that, let''s process the text. We have a
    `Weather_conditons` column in text with values or labels such as `rain`, `snow`,
    and `clear`. These values are found using pandas'' `value_counts()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '`Weather_conditions` can be simplified by categorizing the column label into
    two labels, `rain` or `no_rain`. Forecasting in these two categories will enable
    us to solve the business problem for the cargo company:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will replace both `snow` and `clear` values with `no_rain` as both conditions
    imply no rain conditions at the port. Now that labels are processed, we can convert
    the `Weather_conditions` column into a machine-readable form or numbers using
    `rain` and `no_rain`, label encoding can be efficient as it converts these values
    to 0 and 1\. If there are more than two values, **one-hot encoding** is a good
    choice because assigning incremental numbers to categorical variables can give
    the variables higher priority or numerical bias during training. One-hot encoding
    prevents bias or higher preference for any variable, ensuring neutral privileges
    to each value of categorical variables. In our case, as we have only two categorical
    variables, we perform label encoding using scikit-learn as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we import the `LabelEncoder()` function, which will encode the `Weather_conditions`
    column into 0s and 1s using the `fit_transform()` method. We can do this by replacing
    the previous textual column with a label encoded or machine-readable form to column
    `Weather_condition` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, we concatenate our new label-encoded or machine-readable `Weather_condition`
    column to the DataFrame and drop the previous non-machine readable or textual
    `Weather_conditions` column. Data is now in machine-readable form and ready for
    further processing. You can check the transformed data by executing `df.head()`
    in the notebook (optional).
  prefs: []
  type: TYPE_NORMAL
- en: New feature – Future_weather_condition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we are tasked with forecasting weather conditions 4 hours in the future,
    we create a new feature named `Future_weather_condition` by shifting `Current_weather_condition`
    by four rows, as each row is recorded with a time gap of an hour. `Future_weather_condition`
    is the label of future weather conditions 4 hours ahead. We will use this new
    feature as a dependent variable to forecast using ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will use pandas' `dropna()` function on the DataFrame to discard or drop
    null values, because some rows will have null values due to shifting to a new
    column.
  prefs: []
  type: TYPE_NORMAL
- en: Data correlations and filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the data is fully machine readable, we can observe the correlations
    using the **Pearson correlation coefficient** to observe how every single column
    is related to the other columns. Data and feature correlation is a vital step
    before feature selection for ML model training, especially when the features are
    continuous, like in our case. The Pearson correlation coefficient is a statistical
    linear correlation between each variable (*X* and *y*) that produces a value between
    *+1* and *-1*. A value of *+1* is a positive linear correlation, *-1* is a negative
    linear correlation, and *0* is no linear correlation. It can be used to understand
    the relationship between continuous variables, though it is worth noting that
    Pearson correlation does not mean causation. We can observe Pearson correlation
    coefficients for our data using pandas as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the heatmap of the `Pearson` correlation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Heatmap of correlation scores'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Heatmap of correlation scores
  prefs: []
  type: TYPE_NORMAL
- en: 'From the heatmap in *Figure 3.8*, we can see that the `Temperature` and `Apparent_Temperature_C`
    coefficient is `0.99`. `S_No` (Serial number) is a continuous value, which is
    more or less like an incremental index for a DataFrame and can be discarded or
    filtered out as it does not provide great value. Hence both `Apparent_Temperature`
    and `S_No` are dropped or filtered. Now let''s observe our dependent variable,
    `Future_weather_condition`, and its correlation with other independent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Pearson correlation for Future_weather_condition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.9 – Pearson correlation for Future_weather_condition
  prefs: []
  type: TYPE_NORMAL
- en: Anything between 0.5 and 1.0 has a positive correlation and anything between
    -0.5 and -1.0 has a negative correlation. Judging from the graph, there is a positive
    correlation with `Current_weather_condition`, and `Temperature_C` is also positively
    correlated with `Future_weather_c`.
  prefs: []
  type: TYPE_NORMAL
- en: Time series analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the temperature is a continuous variable, it is worth observing its progression
    over time. We can visualize a time series plot using matplotlib as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the resulting plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 3.10 – Time series progression of Temperature in C'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.10 – Time series progression of Temperature in C
  prefs: []
  type: TYPE_NORMAL
- en: After assessing the time series progression of temperature in *Figure 3.10*,
    we can see that it depicts a stationary pattern since the mean, variance, and
    covariance are observed to be stationary over time. Stationary behaviors can be
    trends, cycles, random walks, or a combination of the three. It makes sense, as
    temperature changes over seasons and follows seasonal patterns. This brings us
    to the end of data analysis and processing; we are now ready to register the processed
    data in the workspace before proceeding to train the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Data registration and versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is vital to register and version the data in the workspace before starting
    ML training as it enables us to backtrack our experiments or ML models to the
    source of data used for training the models. The purpose of versioning the data
    is to backtrack at any point, to replicate a model''s training, or to explain
    the workings of the model as per the inference or testing data for explaining
    the ML model. For these reasons, we will register the processed data and version
    it to use it for our ML pipeline. We will register and version the processed data
    to the Azure Machine Learning workspace using the Azure Machine Learning SDK as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Fetch your `subscription ID`, `resource_group` and `workspace_name` from the
    Azure Machine Learning portal, as shown in *Figure 3.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and
    Workspace name)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Workspace credentials (Resource group, Subscription ID, and Workspace
    name)
  prefs: []
  type: TYPE_NORMAL
- en: 'By requesting the workspace credentials, a workspace object is obtained. When
    running the `Workspace()` function, your notebook will be connected to the Azure
    platform. You will be prompted to click on an authentication link and provide
    a random code and the Azure account details. After that, the script will confirm
    the authentication. Using the workspace object, we access the default data store
    and upload the required data files to the data store on Azure Blob Storage connected
    to the workspace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`Tabular.from_delimited_files()` may cause a failure in Linux or MacOS machines
    that do not have .NET Core 2.1 installed. For correct installation of this dependency,
    follow these instructions: [https://docs.microsoft.com/en-us/dotnet/core/install/linux](https://docs.microsoft.com/en-us/dotnet/core/install/linux).
    After successfully executing the preceding commands, you will upload the data
    file to the data store and see the result shown in *Figure 3.12*. You can preview
    the dataset from the datastore as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'When the data is uploaded to the data store, then we will register the dataset
    to the workspace and version it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `register(...)` function registers the dataset to the workspace, as shown
    in *Figure 3.12*. For detailed documentation, visit https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets#register-datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Processed dataset registered in the ML workspace'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Processed dataset registered in the ML workspace
  prefs: []
  type: TYPE_NORMAL
- en: Toward the ML Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have processed the data by working on irregularities such as missing
    data, selected features by observing correlations, created new features, and finally
    ingested and versioned the processed data to the Machine learning workspace. There
    are two ways to fuel the data ingestion for ML model training in the ML pipeline.
    One way is from the central storage (where all your raw data is stored) and the
    second way is using a feature store. As knowledge is power, Let's get to know
    the use of the feature store before we move to the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feature store compliments the central storage by storing important features
    and make them available for training or inference. A feature store is a store
    where you transform raw data into useful features that ML models can use directly
    to train and infer to make predictions. Raw Data typically comes from various
    data sources, which are structured, unstructured, streaming, batch, and real-time.
    It all needs to get pulled, transformed (using a feature pipeline), and stored
    somewhere, and that somewhere can be the feature store. The feature store then
    takes the data and makes it available for consumption. Data scientists tend to
    duplicate work (especially data processing). It can be avoided if we have a centralized
    feature store. Feature store allows data scientists to efficiently share and reuse
    features with other teams and thereby increase their productivity as they don't
    have to pre-process features from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Feature store workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16572_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Feature store workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in *Figure 3.13*, a **Feature Store** is using a **Feature Pipeline**
    connected to a **Central Storage** (which stores data from multiple sources) to
    transform and store raw data into useful features for ML training. The features
    stored in the feature store can be retrieved for training, serving, or discovering
    insights or trends. Here are some benefits of using a feature Store:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient **Feature Engineering** for **Training Data**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid unnecessary data pre-processing before training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid repetitive feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features available for quick inferencing (testing)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: System support for serving of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploratory Data Analysis by feature Store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opportunity to reuse models features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick Queries on features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducibility for training data sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring feature drift in production (we will learn about feature drift in
    [*Chapter 12*](B16572_12_Final_JM_ePub.xhtml#_idTextAnchor222), *Model Serving
    and Monitoring*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features available for data drift monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is good to know the advantages of a feature store as it can be useful to
    fuel the ML pipeline (especially the data ingestion step), however not suitable
    for all cases. It depends on your use case. For our use case implementation, we
    will not use feature store but proceed to liaisoning data directly from central
    storage where we have preprocessed and registered the datasets we need for training
    and testing. With ingested and versioned data, you are set to proceed towards
    building your ML Pipeline. The ML pipeline will enable further feature engineering,
    feature scaling, curating training, and testing datasets that will be used to
    train ML models and tune hyperparameters for machine learning training. The ML
    pipeline and functionalities will be performed over cloud computing resources,
    unlike locally on your computer as we did in this chapter. It will be purely cloud-based.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to identify a suitable ML solution to a
    business problem and categorize operations to implement suitable MLOps. We set
    up our tools, resources, and development environment. 10 principles of source
    code management were discussed, followed by data quality characteristics. Congrats!
    So far, you have implemented a critical building block of the MLOps workflow –
    data processing and registering processed data to the workspace. Lastly, we had
    a glimpse into the essentials of the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you will do the most exciting part of MLOps: building
    the ML pipeline. Let''s press on!'
  prefs: []
  type: TYPE_NORMAL
