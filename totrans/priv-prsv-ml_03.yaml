- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Phases and Privacy Threats/Attacks in Each Phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will provide a quick refresher on the different types of
    **machine learning** (**ML**): supervised, unsupervised, and reinforcement learning.
    We will also review the essential phases or pipelines of ML. You may already be
    familiar with these; if not, this chapter will serve as a foundational introduction.'
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we will delve into the crucial topic of privacy preservation within
    each phase of the ML process. Specifically, we will explore the importance of
    maintaining privacy in training data, input data, model storage, and inference/output
    data. Additionally, we will examine various privacy attacks that can occur in
    each phase, such as training data extraction attacks, model inversion attacks,
    and model inference attacks. Through detailed examples, we will gain an understanding
    of how these attacks function and discuss strategies to safeguard against them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: ML types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of ML phases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy threats/attacks in the ML phases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some of you may already be familiar with the different types of ML, namely supervised
    ML, unsupervised ML, and reinforcement learning. In the next sections, we will
    provide a quick refresher on these ML types, summarizing what you may have already
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised ML models involve the development of a mathematical model using a
    set of input data and corresponding actual output. The input data is known as
    the training data, while the output is referred to as the predicted output. These
    models employ mathematical functions to learn from the training data and aim to
    minimize the errors between the predicted output and the expected output using
    an optimal function. The training data, which consists of input examples, is typically
    represented in formats such as arrays, vectors, matrices, or tensors. This data
    is often referred to as feature data or feature vectors, where each attribute
    within the data is considered a feature.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Example** | **Details** |'
  prefs: []
  type: TYPE_TB
- en: '| Scalar | 1 | A scalar is a single number. |'
  prefs: []
  type: TYPE_TB
- en: '| Vector | [ 1 2 3 4 ] | A vector is an array of numbers or objects with different
    data types. |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix | [1¬†2¬†3¬†4¬†5¬†6¬†7¬†8¬†9] | A matrix is an array of numbers arranged in
    rows and columns. In order to access the matrix, we need two indexes: column number
    and row number. |'
  prefs: []
  type: TYPE_TB
- en: '| Tensor | [[ 1 2] [ 3 4] [ 5 6][ 7 8 ] [9 0] [ 0 1]] | A tensor is an *n*-dimensional
    array with *n>2*. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 ‚Äì Examples of scalar, vector, matrix, and tensor
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, supervised learning in ML can be represented by a model,
    with parameters, *ùúÉ*. This model acts as a mapping function between input *x*
    and output *y*, denoted as *y =* ùëì *(x,* ùúÉ*).* In this context, *x* represents
    a vector of attributes or features with a dimensionality of *ùëõ*. The output or
    label *y* can vary in dimension depending on the specific learning task.
  prefs: []
  type: TYPE_NORMAL
- en: To train the model, a training set T, is utilized, which consists of data points
    in the form of T = {(x , yùëñ)}, where *ùëñ* ranges from 1 to n, representing the
    number of input‚Äìoutput pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised ML algorithms typically fall into two categories: regression and
    classification. These algorithms aim to learn patterns and relationships within
    the training data to make predictions or assign labels to new, unseen data.'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A target variable in ML is the variable that we aim to predict or forecast.
    It is also often termed as the dependent variable. It is what the ML model is
    trained to predict using independent or feature variables. For example, in a house
    price prediction model, the house price would be the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Regression is a fundamental concept in ML that focuses on predicting continuous
    numerical values based on input variables. It is a supervised learning technique
    that involves analyzing the relationship between the input features and the target
    variable. The goal of regression is to build a mathematical model that can accurately
    estimate or approximate the value of the target variable when provided with new
    input data.
  prefs: []
  type: TYPE_NORMAL
- en: In regression, the target variable, also known as the dependent variable, is
    a continuous value. The input variables, also called independent variables or
    features, can be numerical or categorical. The regression model seeks to understand
    the relationship between these input variables and the target variable, enabling
    predictions to be made for unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: The performance of a regression model is typically measured by evaluating the
    closeness of its predictions to the actual target values. Various regression algorithms
    exist, such as linear regression, polynomial regression, and more complex techniques
    such as support vector regression and random forest regression. These algorithms
    use mathematical optimization methods to fit a regression function that minimizes
    the difference between predicted values and the true values of the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis finds applications in numerous fields, including finance,
    economics, healthcare, and social sciences, where it is used for tasks such as
    price prediction, demand forecasting, risk assessment, and trend analysis. By
    leveraging regression algorithms, valuable insights can be gained from data, allowing
    for informed decision-making and accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Regression model example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here‚Äôs a straightforward example of predicting a target variable using two input
    features. In this scenario, a model is trained on a set of historical data, typically
    representing the data from the last X days.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose is to forecast or predict the target variable based on the provided
    input data. By analyzing patterns and relationships in the historical dataset,
    the trained model can make predictions about the target variable when presented
    with new input data. This process allows for forecasting future values or understanding
    potential outcomes based on the given inputs. The model‚Äôs accuracy and performance
    are evaluated based on how well it can predict the target variable compared to
    the actual values. By leveraging historical data and utilizing ML algorithms,
    valuable insights can be gained, enabling accurate predictions and informed decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature 1 value | Feature 2 value | Target variable value |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 10 | 130 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 20 | 180 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 20 | 210 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 30 | 260 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 50 | ?? |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 ‚Äì regression model example
  prefs: []
  type: TYPE_NORMAL
- en: This example was implemented using the scikit-learn library. I have used Python
    version 3.8 and scikit-learn version 1.2.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps followed in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Jupyter Notebook (**pip** **install notebook**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open a Python Jupyter notebook (**jupyter notebook**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install sci-kit learn libraries (**pip install -****U scikit-learn**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type the following code and run the model code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can directly execute the code using the Jupyter notebook provided in the
    GitHub location of this book‚Äî`LinearRegression.ipynb`‚Äîlocated at [https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%202/LinearRegression.ipynb](https://github.com/PacktPublishing/Privacy-Preserving-Machine-Learning/blob/main/Chapter%202/LinearRegression.ipynb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, target variable y is linearly dependent on input variables
    X0 and X1 with the linear equation y= 3X0+ 5X1+50 (50 is the intercept of the
    line).
  prefs: []
  type: TYPE_NORMAL
- en: The model uses the root mean square value as an optimal function and predicts
    the target variable. In this case, it predicts 100% accuracy because of the strong
    linear relationship between the features and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: Model persistence and retrieving the persisted model for inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After developing and testing the model using training data and validating it
    with test data, the next step is to persist the model. This allows for easy sharing
    with other developers or engineers without revealing the training data and intricate
    model details. Additionally, if the model demonstrates sufficient accuracy during
    training, it can be deployed in production environments.
  prefs: []
  type: TYPE_NORMAL
- en: To persist the model, various formats are supported to store it in the disk
    or file system. The specific format utilized depends on the framework used to
    develop the ML or **deep learning** (**DL**) model. By employing these formats,
    the model can be stored and accessed efficiently, facilitating seamless integration
    into production systems or collaborations with other team members.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting the model enables reproducibility and scalability, as it can be shared,
    reused, and deployed in different environments without the need to retrain it
    from scratch. It also helps protect proprietary information and intellectual property
    associated with the model, allowing organizations to safeguard their valuable
    research and development efforts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 ‚Äì Model persistence and retrieval](img/B16573_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 ‚Äì Model persistence and retrieval
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows some formats that are widely used and accepted in
    the community:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Framework | Model persistence format | Details |'
  prefs: []
  type: TYPE_TB
- en: '| scikit-learn[https://scikit-learn.org/](https://scikit-learn.org/) | JoblibPickle
    | The Joblib and pickle formats don‚Äôt require any code changes.The pickle format
    has security issues, so most frameworks don‚Äôt advise using the pickle format for
    model persistence because arbitrary code can be executed during unpickling. |'
  prefs: []
  type: TYPE_TB
- en: '| TensorFlow/Keras[https://www.tensorflow.org/](https://www.tensorflow.org/)
    | JSONYAMLHDF5 | This is model data stored in JSON format or YAML format, and
    these formats are text-based formats, so they are language-agnostic.Weights are
    saved in HDF5 format. |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch[https://pytorch.org/](https://pytorch.org/) | state_dictPickle |
    For neural network models to store weights and biases. |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX | Onnx | Models need to be converted to ONNX format and exported and
    loaded/executed using ONNX Runtime so that they can be run either on CPU- or GPU-based
    servers. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.3 ‚Äì Examples of scalar, vector, matrix, and tensor
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to store and retrieve a model in the Joblib format
    using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Joblib?
  prefs: []
  type: TYPE_NORMAL
- en: Joblib ([https://joblib.readthedocs.io/en/latest/](https://joblib.readthedocs.io/en/latest/))
    is a set of tools to provide lightweight pipelining in Python to persist (or serialize)
    the Python objects. Joblib version 1.2.0 is used in this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Jupyter notebook for this example is `LinearRegression_SaveModel.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is persisted in a file system or a file, then the file can be
    shared with other developers or engineers without sharing any of the training
    data or model details used in the code. Other developers/engineers can then load
    this file and use it for further predictions or deploy it in production for production
    usage. This is explained in *Figure 2**.1*. This model is saved in the current
    directory and has the name `sample_model.sav`; you can make use of any extension
    as it doesn‚Äôt matter which extension is used. The source code is in `Linear_Regression_Load_Model.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A classification model employs different algorithms to predict an output or
    dependent variable based on the relationship between the input variables. Classification
    algorithms are specifically designed to predict discrete values, such as spam/not
    spam, male/female, yes/no, and so on. Each of these predicted values is referred
    to as a label or class.
  prefs: []
  type: TYPE_NORMAL
- en: In binary classification scenarios, there are only two possible class labels,
    e.g., determining whether an email is spam or not spam. On the other hand, multi-label
    classification involves predicting multiple class labels simultaneously. An example
    could be classifying images into various categories such as cat, dog, and bird.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models are trained using historical data that contains both the
    input variables and their corresponding class labels. The algorithms learn from
    this labeled data and establish patterns and relationships to make accurate predictions
    on new, unseen data. The performance of a classification model is evaluated based
    on metrics such as accuracy, precision, recall, and F1 score. These metrics assess
    how well the model can correctly assign the appropriate class labels to new instances
    based on their input features.
  prefs: []
  type: TYPE_NORMAL
- en: Classification models find extensive applications in various domains, including
    spam filtering, sentiment analysis, customer churn prediction, fraud detection,
    and medical diagnosis. By leveraging different classification algorithms, valuable
    insights can be gained from data, enabling informed decision-making and efficient
    problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Classification type** | **Details** | **Examples** | **Algorithms** |'
  prefs: []
  type: TYPE_TB
- en: '| Binary | Predicts one of two classes based on the training data | Yes/noSpam/not
    spamPass/failCancer/no cancer | Logistic regressionK-nearest neighborsDecision
    treesSupport vector machineNaive Bayes |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-class | Predicts one of more than two classes | Based on symptoms,
    e.g., cold, flu, or COVID-19 | K-nearest neighborsDecision treesNaive BayesRandom
    forestGradient boosting |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-label | Has two or more class labels | Prediction of the topic based
    on the content: finance, politics, science, language, or all of them | Multi-label
    decision treesMulti-label random forestsMulti-label gradient boosting |'
  prefs: []
  type: TYPE_TB
- en: '| Extreme | Classification task in which the number of candidate labels is
    huge | Amazon 3M dataset, where the number of labels is 2,812,281 | DL algorithmsMore
    algorithms: [http://manikvarma.org/downloads/XC/XMLRepository.html](http://manikvarma.org/downloads/XC/XMLRepository.html)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 2.4 ‚Äì Classification types and associated algorithms
  prefs: []
  type: TYPE_NORMAL
- en: Classification example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this example, we will utilize the decision tree classification algorithm
    to determine the likelihood of a patient‚Äôs survival based on two features: age
    and whether they have a pre-existing cancer condition.'
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree classification algorithm is a widely used technique in ML
    that constructs a tree-like model of decisions. It analyzes the provided data
    to create a structure that represents the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: In our scenario, the age of the patient and their cancer status will be used
    as input features for classification. By examining a labeled dataset consisting
    of patient information, including age, cancer status, and survival outcome, the
    algorithm learns patterns and establishes decision rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, it becomes capable of predicting the survival outcome
    for new patients who have not been previously encountered. By considering the
    age and cancer status of these patients, the model traverses the decision tree
    until reaching a leaf node that signifies the predicted outcome: whether the patient
    is expected to survive or not.'
  prefs: []
  type: TYPE_NORMAL
- en: By employing the decision tree classification algorithm in this example, we
    aim to classify patients‚Äô survival probabilities based on their age and cancer
    status. This valuable insight can aid medical professionals in assessing patient
    prognosis and informing treatment decisions.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age (years)** | **Has/had cancer (1 = yes, 0 =** **no)** | **Survived (1
    = yes,** **0 =no)** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 1 | ?? (predict) |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | 1 | ?? (predict) |'
  prefs: []
  type: TYPE_TB
- en: Table 2.5 ‚Äì Toy dataset for classification example
  prefs: []
  type: TYPE_NORMAL
- en: In this toy dataset, the model needs to predict whether the last two patients
    survive or not (classification with two labels) based on the trained historical
    data of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The source code is in `Classification_Example.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn provides various Python classes for classification algorithms.
    Since we have chosen the decision tree algorithm for this example, import the
    necessary classes and prepare the data in a format that the model accepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the model predicted that the 35-year-old patient would survive
    but the 78-year-old patient would not survive based on the training data provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand more about decision trees and how the trees are split, let‚Äôs
    look at the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will plot the tree based on the input features and how the tree is split.
    This is useful when more features are in the training data and we need to know
    which feature has more importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 2.2 - Visualizing tree splitting](img/B16573_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 - Visualizing tree splitting
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained and tested, it can be persisted in a file system or
    directly used for production.
  prefs: []
  type: TYPE_NORMAL
- en: In the last example, we persisted the model in the Joblib format.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now try to persist the model with the ONNX format to learn more about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Model persistence using the ONNX format and executing the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**ONNX**, short for **Open Neural Network Exchange**, is an open source format
    designed for ML and DL models. Its purpose is to facilitate the interoperability
    of models across different frameworks. It accomplishes this by providing an extensible
    computation graph model and defining a set of built-in operators and standard
    data types.'
  prefs: []
  type: TYPE_NORMAL
- en: With ONNX, ML/DL models can be easily converted to the ONNX format, allowing
    for seamless deployment, export, loading, and execution using ONNX Runtime. ONNX
    Runtime is a powerful tool that enables high-performance execution of ML models
    on either CPU or GPU. Importantly, it does not rely on dependencies on the specific
    training framework used to develop the models. By leveraging ONNX and ONNX Runtime,
    developers can ensure that their models are portable across various frameworks
    and can be efficiently executed. More details about ONNX can be found at [https://github.com/onnx/onnx](https://github.com/onnx/onnx).
  prefs: []
  type: TYPE_NORMAL
- en: Converting the sklearn sample model to the ONNX format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Converting the model to ONNX format requires the frameworks `onnx`, `onnxruntime`,
    and `skl2onnx` for scikit-learn. Install the frameworks in the following maner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the frameworks are installed, execute the following code to convert the
    model to ONNX format (the source code is in `Model_Persistence_Load_ONNX_Format.ipynb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, to convert the ML model that was developed using `sklearn` to
    ONNX format, first, the data types used in the training need to be provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Later, use the methods provided to convert the model to ONNX format and specify
    the classifier that is used in `sklearn`. In our examples, we have used decision
    trees and named the model `clf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is converted to ONNX format, store it in the disk and name the
    model file (in our example, `survive.onnx`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now the model is stored in ONNX format and it can be loaded and executed on
    any framework that supports ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the ML model using ONNX format and executing the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following lines of code show how to load the model stored in ONNX format
    and how to use the model for inference. ONNX version 1.14.1 is used in this code
    (the source code is in `Model_Persistence_Load_ONNX_Format.ipynb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Unsupervised ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In unsupervised ML, the model is trained using unlabeled training data, which
    means there are no target labels or classes provided. Instead, unsupervised ML
    models focus on understanding the inherent patterns and structures within the
    data. Unlike supervised learning, where the model learns from labeled examples,
    unsupervised machine learning models uncover hidden patterns and relationships
    within the data without any predefined class labels. This allows for the discovery
    of previously unknown insights and patterns that may not be readily apparent.
    By leveraging unsupervised ML techniques, analysts and data scientists can gain
    valuable insights from unlabeled data, uncover hidden structures, and make data-driven
    decisions based on the inherent patterns discovered in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Clustering is a primary technique used in unsupervised ML. It involves grouping
    similar data points together based on their intrinsic characteristics. By examining
    the data and identifying similarities, unsupervised models create clusters, which
    represent distinct groups or patterns within the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms, such as k-means clustering, hierarchical clustering,
    or density-based clustering, are commonly employed in unsupervised ML to organize
    data into meaningful groups. These clusters can help in data exploration, anomaly
    detection, customer segmentation, and other data-driven tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let‚Äôs consider a scenario where a company aims to offer transportation services
    to its employees and wants to cluster them based on their residential locations.
    To achieve this, the company can utilize a clustering model that takes the longitude
    and latitude coordinates of each employee‚Äôs residence as input data. The ML model
    will cluster the employees based on the specified cluster size, which can be equal
    to the number of vehicles available for transportation. By analyzing the spatial
    data of employees‚Äô locations, the clustering model will group individuals who
    live in close proximity to one another. This grouping enables the company to efficiently
    allocate vehicles to each cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Once the clustering model is trained and established, it can predict the appropriate
    cluster for new employees based on their residential coordinates. This allows
    the company to easily determine which cluster the new employee should join, facilitating
    seamless transportation arrangements.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing ML clustering techniques in this scenario, the company can effectively
    organize its transportation services and optimize resource allocation based on
    employees‚Äô residential locations.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Employee number** | **Latitude (****o N)** | **Longitude (****o E)** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.93 | 77.4472 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 12.32 | 77.4472 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 12.51 | 77.4472 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12.62 | 77.4472 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 12.73 | 77.4472 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 12.84 | 76.4158 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 12.91 | 76.4158 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 12.41 | 76.4158 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 12.92 | 76.4158 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 12.55 | 76.4158 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.6 ‚Äì Toy dataset for clustering example
  prefs: []
  type: TYPE_NORMAL
- en: The `sklearn` framework supports various clustering algorithms, and we will
    use the K-means clustering algorithm in this example to cluster the given data.
  prefs: []
  type: TYPE_NORMAL
- en: The K-means algorithm is a centroid-based algorithm, where each cluster is associated
    with a centroid. The main aim of this algorithm is to minimize the sum of distances
    between the input data point and their corresponding cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The source code is in `Clustering_Example.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `sklearn` K-means clustering classes and prepare the training data
    as a `numpy` array format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the first five employees are assigned to cluster `1` and the
    remaining are assigned to cluster `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Employee number | Latitude (o N) | Longitude (o E) | Assigned cluster |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 12.93 | 77.4472 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 12.32 | 77.4472 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 12.51 | 77.4472 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 12.62 | 77.4472 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 12.73 | 77.4472 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 12.84 | 76.4158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 12.91 | 76.4158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 12.41 | 76.4158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 12.92 | 76.4158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 12.55 | 76.4158 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.7 - Assigned cluster
  prefs: []
  type: TYPE_NORMAL
- en: The cluster model learned based on the input and formed two clusters. K-means
    is a clustering algorithm that finds the center of the cluster, divides the data
    into clusters, and predicts the new data based on the nearest cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of clustering algorithms supported by the `sklearn`
    framework:'
  prefs: []
  type: TYPE_NORMAL
- en: Affinity propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agglomerative clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BIRCH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mini-batch K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean shift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OPTICS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixture of Gaussians
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforced ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is a type of ML technique that enables
    an agent to learn in an interactive environment by trial and error using feedback
    from its own actions and experiences. The agent learns a series of actions that
    lead to the final goal, maximizing its total rewards. RL differs from supervised
    learning in that the model learns from taking actions and observing the results,
    not from explicit teaching.'
  prefs: []
  type: TYPE_NORMAL
- en: One classic use case of RL is in gaming, such as teaching a model to play and
    excel at chess. The model starts with no knowledge of the game but learns by making
    moves and seeing the outcome of the game it plays, with the aim of maximizing
    the reward (i.e., winning the game).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of RL, exploration and exploitation are two strategies that
    an agent can use to navigate through the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploration**: This is when the agent seeks to learn more about its environment.
    It means trying out different actions and gathering more information to learn
    about each possible action‚Äôs reward. The agent aims to balance out the reward
    it gets from known information with the possibility of receiving an even higher
    reward from unknown areas. However, exploration might involve the risk of the
    agent making non-optimal choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation**: Here, the agent uses the information it has already learned
    to make the best action that will maximize its reward. It means using known information
    to maximize success instead of further exploring. The benefit of exploitation
    is that it allows for more assured, immediate rewards, but excessive exploitation
    can lead to suboptimal results as it may neglect even better options. The challenge
    lies in finding the right balance, as focusing too much on exploration might mean
    the agent will lose out on immediate rewards while focusing extensively on exploitation
    might prevent the agent from exploring options that could lead to larger rewards
    in the future. This trade-off is often referred to as the exploration‚Äìexploitation
    dilemma.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example problem using RL‚Äîthe multi-armed bandit problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The multi-armed bandit problem is a classic problem in the field of RL that
    captures the fundamental trade-off between exploration and exploitation. The name
    is derived from a hypothetical experiment in which you face several slot machines
    (also known as ‚Äúone-armed bandits‚Äù) with different fixed payouts. Because of these
    differing payouts, your goal is to maximize your total payout over a certain number
    of attempts by figuring out which machines to play, how many times to play each
    machine, and in what sequence‚Äîhence, the ‚Äúmulti-armed bandit problem.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: The primary challenge in the multi-armed bandit problem is balancing the immediate
    rewards from exploitative actions (playing the machine that you believe currently
    has the highest expected payout) with the possible benefits from exploration (trying
    out others that might have higher expected payouts but you‚Äôre less certain about).
    This tension between exploration and exploitation is at the core of many reinforcement
    learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is example code for reinforcement learning‚Äîthe source code is
    in `BandIt_RL_Example.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the epsilon-greedy strategy is used, where epsilon is `1`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a very simplistic example and real-world RL problems require much more
    sophisticated algorithms (e.g., Q-learning, policy gradient, etc.) and are therefore
    implemented using specialized libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have covered the different types of ML and provided examples
    of how to save and load models for inference and prediction. Moving forward, the
    next section will delve into the various phases of ML, providing a detailed exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of ML phases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML encompasses a variety of techniques and approaches, and it involves several
    distinct phases or stages in the process of developing and deploying ML models.
    These phases help guide engineers through the iterative and cyclical nature of
    ML projects, allowing them to build effective and accurate models.
  prefs: []
  type: TYPE_NORMAL
- en: The ML process typically consists of several key phases, each serving a specific
    purpose and contributing to the overall success of the project. These phases are
    not always strictly linear, and iterations may occur between them to refine and
    improve the models. The specific steps and terminology used may vary depending
    on the ML methodology employed, but the core phases remain consistent.
  prefs: []
  type: TYPE_NORMAL
- en: The ML phases provide a systematic framework for developing and deploying ML
    models, guiding practitioners through the complexities and challenges inherent
    in building effective solutions. By following these phases, practitioners can
    maximize their chances of success and create ML models that deliver valuable insights
    and predictions in a wide range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: The main phases of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the main phases of ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: This phase involves gathering relevant data from various
    sources, such as databases, APIs, or manual collection. The data should be representative
    of the problem domain and cover a wide range of scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preparation**: In this phase, the collected data is preprocessed and
    transformed into a suitable format for analysis. This may include tasks such as
    cleaning the data, handling missing values, removing outliers, and normalizing
    or scaling the features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Feature engineering involves selecting and creating
    relevant features from the available data that will enhance the model‚Äôs predictive
    power. This phase requires domain knowledge and creativity to extract meaningful
    insights from the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model development**: In this phase, a suitable ML algorithm or model is selected
    based on the problem at hand. The model is trained on the prepared data to learn
    patterns and relationships within the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation**: The trained model is evaluated using appropriate evaluation
    metrics to assess its performance. This helps in understanding how well the model
    generalizes to unseen data and whether it meets the desired criteria for accuracy
    and reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model optimization**: If the model‚Äôs performance is not satisfactory, this
    phase involves fine-tuning the model by adjusting hyperparameters or trying different
    algorithms to improve its performance. The optimization process aims to achieve
    the best possible results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Deployment**: Once the model is trained and optimized, it is deployed
    in a production environment where it can make predictions on new, unseen data.
    This phase involves integrating the model into existing systems or creating an
    interface for users to interact with the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring and maintenance**: After deployment, the model needs to
    be monitored to ensure it continues to perform well over time. Monitoring involves
    tracking performance metrics, identifying drift in data distribution, and updating
    the model if necessary. Regular maintenance is essential to keeping the model
    up to date and accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These phases provide a systematic approach to building and deploying ML models,
    enabling organizations to leverage the power of data and make informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Sub-phases in the ML process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following diagram shows the phases of the ML process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 ‚Äì ML phases](img/B16573_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 ‚Äì ML phases
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the main phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model phase (design and development)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML operations phase
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let‚Äôs look at these in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation phase
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data preparation phase deals with data collection, extraction, and manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Phase** | **Sub-phases** | **Details** |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation | Data collection | Identify the data that needs to be analyzed
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data extraction | Extract the data from the data source |'
  prefs: []
  type: TYPE_TB
- en: '| Data manipulation | Data transformation, missing data, duplicate data, noise,
    and data preprocessing |'
  prefs: []
  type: TYPE_TB
- en: '| **Exploratory data** **analysis** (**EDA**) | EDA and handling data |'
  prefs: []
  type: TYPE_TB
- en: Table 2.8 - Data preparation phase
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the data preparation sub-phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation Phase
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Figure 2.4 ‚Äì ML data preparation sub-phases](img/B16573_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 ‚Äì ML data preparation sub-phases
  prefs: []
  type: TYPE_NORMAL
- en: ML model phase
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This phase is subdivided into several phases to deal with feature engineering,
    actual model identification, the training and testing of models, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Phase** | **Sub-phases** | **Details** |'
  prefs: []
  type: TYPE_TB
- en: '| ML model | Model identification | This involves classification, clustering,
    re-enforcement, time series analysis, and so on. |'
  prefs: []
  type: TYPE_TB
- en: '| Feature engineering | In this phase, features are selected from the data.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Input data preparation for the model | This involves data processed and data
    prepared in the format the model expects. |'
  prefs: []
  type: TYPE_TB
- en: '| Split the data (train, test, and validate) | Split the entire data into three
    parts‚Äîtraining data, test data, and validation data‚Äîto train, test, and validate
    the models. |'
  prefs: []
  type: TYPE_TB
- en: '| Train the model with the training dataset | In this phase, the model is trained
    with the training data. |'
  prefs: []
  type: TYPE_TB
- en: '| Test the model with the testing dataset | The ML model is tested with the
    test data to find out the accuracy of predictions. |'
  prefs: []
  type: TYPE_TB
- en: '| Version of data, model, model parameters, and results | Version control is
    applied to datasets used, as well as to the model and its parameters, along with
    the results of each experiment. |'
  prefs: []
  type: TYPE_TB
- en: '| Validate the dataset with the trained model | This is similar to test data
    but the samples are from the validation dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| Predict results with new data (inference) | For inference, use the new data
    and find out the results. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.9 - ML model phase
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the ML model sub-phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 ‚Äì ML model sub-phases](img/B16573_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 ‚Äì ML model sub-phases
  prefs: []
  type: TYPE_NORMAL
- en: ML operations phase
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This phase is mainly focused on the operations of the models in production.
  prefs: []
  type: TYPE_NORMAL
- en: '| Phase | Sub-phases | Details |'
  prefs: []
  type: TYPE_TB
- en: '| ML operations(MLOps) | Package model artifacts | Persist the model (store
    the weights and biases) in ONNX format or other formats. |'
  prefs: []
  type: TYPE_TB
- en: '| Deploy model | This involves the production deployment of the model.(A/B
    testing, canary deployment, shadow models, etc.) |'
  prefs: []
  type: TYPE_TB
- en: '| Validate the inference results |  |'
  prefs: []
  type: TYPE_TB
- en: '| Monitor model performance | Monitor the performance model, i.e, whether the
    accuracy stays constant or degrades over a period. |'
  prefs: []
  type: TYPE_TB
- en: '| Retrain the model and repeat the ML model life cycle | Retrain the model
    if the model performance degrades and handle model drift and data drift accordingly.
    |'
  prefs: []
  type: TYPE_TB
- en: Table 2.10 - ML operations
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the ML operations sub-phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 ‚Äì ML operations sub-phases](img/B16573_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 ‚Äì ML operations sub-phases
  prefs: []
  type: TYPE_NORMAL
- en: We have thoroughly covered the development of ML models and the various phases
    involved in the process. In the upcoming section, our focus will shift to exploring
    the privacy threats and attacks that can occur in each phase of ML. We will delve
    into understanding these threats and discuss effective mitigation strategies to
    safeguard the privacy of the data and models involved. By addressing these privacy
    concerns at each stage, we can ensure the responsible and secure implementation
    of ML techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy threats/attacks in ML phases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML projects are developed in collaboration with data engineers, ML engineers,
    and software engineers, and each one plays a different role in order to develop
    end-to-end systems to predict and provide insights.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative roles in ML projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML projects are collaborative efforts involving various roles such as data
    engineers, ML engineers, and software engineers. Each role contributes in different
    ways to develop end-to-end systems that can predict outcomes and provide valuable
    insights. Let‚Äôs explore the roles and their responsibilities in the ML project
    life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data engineer**: The data engineer primarily focuses on the data preparation
    phase. They are responsible for extracting data from one or multiple sources and
    ensuring its quality and suitability for the ML project. Data engineers work on
    tasks such as data cleaning, transformation, and feature selection to prepare
    the data for ML modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML engineer**: ML engineers play a crucial role in designing and developing
    ML models. They leverage the data provided by the data engineer to train and test
    the models. ML engineers are responsible for selecting appropriate algorithms
    or model architectures, tuning hyperparameters, and optimizing the models for
    accuracy and efficiency. They validate the model against validation test data
    and provide APIs for inference or export/deployment of the model into production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model consumer**: The model consumer can be an individual or another application
    that interacts with the ML model. They make API calls and provide input to the
    model for prediction or inference. Model consumers utilize the insights generated
    by the ML model to make informed decisions or take appropriate actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privacy threats/attacks in ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of ML, an adversary refers to an entity or system that actively
    tries to undermine or exploit the machine learning model or system. The goal of
    an adversary is typically to manipulate the model‚Äôs behavior, gain unauthorized
    access to sensitive information, or deceive the system by exploiting vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Adversaries can take various forms and have different motives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adversarial examples**: In this case, the adversary aims to create input
    samples (e.g., images or text) that are intentionally crafted to mislead or deceive
    the ML model. Adversarial examples are designed to exploit vulnerabilities in
    the model‚Äôs decision-making process, leading to incorrect predictions or misclassifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data poisoning**: An adversary may try to inject malicious or misleading
    data into the training dataset. By inserting carefully crafted samples, the adversary
    aims to manipulate the model‚Äôs training process, leading to biased or compromised
    results. This can be particularly problematic in scenarios where the training
    data is collected from untrusted or unreliable sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model inversion**: An adversary might attempt to extract sensitive information
    from a trained model. By providing specific input and observing the model‚Äôs output,
    the adversary aims to infer confidential or private data that was used to train
    the model, such as **personally identifiable information** (**PII**) or proprietary
    knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evasion attacks**: Adversaries can also launch evasion attacks, also known
    as adversarial attacks, during the deployment phase. In these attacks, the adversary
    tries to bypass or manipulate the model‚Äôs defenses by carefully modifying input
    samples. For example, in the case of a spam email classifier, an adversary may
    add specific patterns or keywords to trick the model into classifying a malicious
    email as legitimate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To mitigate the impact of adversaries, researchers and practitioners develop
    robust ML models and techniques, such as adversarial training, defensive distillation,
    and input sanitization. These approaches aim to enhance the resilience of ML systems
    against adversarial attacks and maintain their performance and reliability in
    the presence of potential threats.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the ML life cycle, privacy threats or attacks can occur, posing risks
    to the confidentiality of sensitive information. In the context of ML, adversaries
    attempt to gain unauthorized access to confidential data used in ML, the core
    ML model, or specific features of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two primary types of attacks, white-box and black-box:'
  prefs: []
  type: TYPE_NORMAL
- en: '**White-box attack**: A white-box attack assumes that the adversary has full
    knowledge and access to the ML model, including its architecture, input, output,
    and weights. The attacker exploits this information to extract confidential details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Black-box attack**: In contrast, a black-box attack assumes that the attacker
    only has access to the input and output of the ML model. They have no knowledge
    of the underlying architecture or weights used in the ML/DL model. Despite the
    limited information, they aim to infer sensitive information from the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Privacy threats/attacks classification:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following are the privacy attacks on the ML models for classification
  prefs: []
  type: TYPE_NORMAL
- en: '**Membership inference attack**: This attack aims to determine whether a particular
    data point was part of the training dataset used to train the ML model. The adversary
    tries to infer membership information by exploiting the model‚Äôs responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model extraction attack**: In this attack, the adversary attempts to extract
    the entire or partial ML model architecture, weights, or parameters. This attack
    allows the attacker to replicate the ML model for their own purposes, potentially
    leading to intellectual property theft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reconstruction attack**: This attack focuses on reconstructing sensitive
    information from the ML model‚Äôs outputs. The attacker aims to infer private data
    or specific features that were used to generate the model‚Äôs predictions. By understanding
    and addressing these privacy threats, ML practitioners can take appropriate measures
    to safeguard sensitive data and ensure the security of ML models throughout their
    life cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We‚Äôll look at these in more detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Membership inference attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume a classification model is developed with certain input training data,
    X { x1, x2, x3, ‚Ä¶. Xn} to predict certain labels, y, using a function, F.
  prefs: []
  type: TYPE_NORMAL
- en: A membership inference attack tries to determine whether an input sample, x,
    was used as part of the training dataset, X, or not. Basically, an adversary (attacker)
    needs to find out whether the data point at hand belongs to the original dataset
    that is used for training the ML model or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 ‚Äì Membership inference attack - Example](img/B16573_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 ‚Äì Membership inference attack - Example
  prefs: []
  type: TYPE_NORMAL
- en: '(Image source: Suha Hussain, PrivacyRaven, [https://blog.openmined.org/privacyraven-comprehensive-privacy-testing-for-deep-learning/](https://blog.openmined.org/privacyraven-comprehensive-privacy-testing-for-deep-learning/))'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs look at an example. Suppose an adversary wants to determine whether a
    particular person‚Äôs data was used in the training data or not without the knowledge
    of that person. Later, this data is used to derive insights on whether to approve
    that person‚Äôs insurance policy or not.
  prefs: []
  type: TYPE_NORMAL
- en: This is the most popular category of attacks and was first introduced by Shokri
    et al.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the reference to the full paper: *Reza Shokri, Marco Stronati, Congzheng
    Song, and Vitaly Shmatikov. 2017\. Membership inference attacks against machine
    learning models. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, San
    Francisco, CA,* *USA, 3‚Äì18.*'
  prefs: []
  type: TYPE_NORMAL
- en: This is a kind of black-box testing attack because the adversary doesn‚Äôt have
    the details of the actual ML model; all they have is the set of input data and
    inference results from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the paper says about this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúThe attacker queries the target model with a data record and obtains the model‚Äôs
    prediction on that record. The prediction is a vector of probabilities, one per
    class, that the record belongs to a certain class. This prediction vector, along
    with the label of the target record, is passed to the attack model, which determines
    whether the record was in or out of the target model‚Äôs training dataset.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure also comes from the aforementioned paper ([https://arxiv.org/pdf/1610.05820.pdf](https://arxiv.org/pdf/1610.05820.pdf)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 ‚Äì Membership inference attack - Example](img/B16573_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 ‚Äì Membership inference attack - Example
  prefs: []
  type: TYPE_NORMAL
- en: Membership inference attack‚Äîbasic example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a membership inference attack, an adversary attempts to determine whether
    a specific data point was used in the training set of an ML model, as stated earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example using a simple decision tree classifier (the source code
    can be found in `Membership_Inference_basic_example.ipynb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we load the `Iris` dataset from scikit-learn and split it into
    a training set and a test set. We then train a decision tree classifier on the
    training set. Next, we select a data point (or set of points) from the test set
    and attempt to determine whether it was present in the training set by predicting
    its class. If the predicted class matches the actual class from the test set,
    we infer that the target data point was in the training set, indicating a successful
    attack. Remember, conducting membership inference attacks without proper authorization
    is unethical and often illegal.
  prefs: []
  type: TYPE_NORMAL
- en: Membership inference attack‚Äîadvanced example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let‚Äôs consider a scenario where an adversary seeks to determine whether a specific
    individual‚Äôs data is present in the training data without any prior knowledge
    of that person. This scenario involves discovering whether a person‚Äôs name exists
    within a hospital‚Äôs sensitive clinical data. The adversary intends to exploit
    this information to make decisions, such as granting or denying an insurance policy,
    based on the insights gained.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this scenario, let‚Äôs use a sample dataset (for illustrative purposes
    only) similar to the classification example we previously discussed. The dataset
    focuses on predicting whether a patient will live for the next 5 to 10 years or
    not based on factors such as age and existing diseases.
  prefs: []
  type: TYPE_NORMAL
- en: In this context, the adversary‚Äôs goal is to identify whether the data related
    to a specific person, whose identity they are unaware of, is present in the training
    data. By discovering this information, the adversary can potentially manipulate
    decisions related to insurance policies based on the insights gained from the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this example serves to highlight a potential privacy
    threat and does not aim to validate its accuracy or real-world applicability.
    The objective is to raise awareness about the importance of safeguarding sensitive
    data and implementing robust privacy measures to prevent unauthorized access and
    misuse.
  prefs: []
  type: TYPE_NORMAL
- en: '| Age (years) | Has/had cancer (1 = yes, 0 = no) | Survived ( 1 = yes, 0 =
    no) |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.11 - Training data
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for this ML model can be found in `Membership_Inference_advanced_example.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Inference results with sample test data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The adversary creates synthetic test data with diverse inputs to evaluate the
    model‚Äôs performance. Their objective is to determine whether the given patient
    data exists in the training dataset or not using the model‚Äôs predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are in the table format; 1 means the person has cancer and the
    predicted probability column shows the percentage of predicted probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Age | Cancer | Class predicted | Predicted probability |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 0 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 1 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 0 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 0 | 1 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 0 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 0 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 0 | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.12 - Predicted probability
  prefs: []
  type: TYPE_NORMAL
- en: Next, the adversary proceeds to develop shadow models and a final attack model.
    These models are designed to predict whether a given data record was used in the
    training dataset. By utilizing each record and its predicted class, along with
    the corresponding predicted probabilities, the adversary infers whether the data
    record was part of the training set or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 ‚Äì Shadow models](img/B16573_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 ‚Äì Shadow models
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure was sourced from the paper at [https://arxiv.org/pdf/1610.05820.pdf](https://arxiv.org/pdf/1610.05820.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The attacker model makes use of the class label 'In' for the given input record
    used in the training, while 'out' means it is not used.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the training data for the final attack model:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Age | Cancer | Class predicted | Predicted probability | Record used in the
    training set (in = 1, out = 0) |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 1 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 0 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 1 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 45 | 0 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 0 | 1 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 0 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 0 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 30 | 1 | 0 | 100 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 60 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 75 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 90 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 78 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.13 - Training data for final attack model
  prefs: []
  type: TYPE_NORMAL
- en: Membership inference attacks can be easily executed with remarkable accuracy
    in simple linear models, as shown in the preceding example. ML models hosted in
    the cloud are susceptible to such inference attacks, as researchers have successfully
    demonstrated membership attack models with accuracies exceeding 90%.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to mitigate membership inference attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Membership inference attacks can be a concern in ML models that involve sensitive
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some techniques to mitigate membership inference attacks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit access to sensitive information**: One of the simplest ways to mitigate
    membership inference attacks is to limit access to sensitive information. By minimizing
    the amount of sensitive data that is exposed, you reduce the potential for attackers
    to perform membership inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differential privacy**: Differential privacy is a technique that adds noise
    to the training data or the model‚Äôs output, making it harder for an attacker to
    determine whether a specific record was part of the training set. Applying differential
    privacy mechanisms can help protect against membership inference attacks. We will
    learn about differential privacy in the next chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training set augmentation**: By augmenting the training set with additional
    synthetic or generated data, you can make it more difficult for attackers to distinguish
    between genuine training instances and potential members. Augmentation techniques
    such as data generation, perturbation, or adding noise can help to increase the
    privacy of the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization and dropout**: Applying regularization techniques such as
    L1 or L2 regularization and incorporating dropout layers in neural networks can
    improve model robustness and reduce overfitting. Regularization can help in reducing
    the memorization of training instances, making it harder for attackers to infer
    membership.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model compression**: When sharing models or making predictions, consider
    using model compression techniques to reduce the amount of information leaked
    about the training data. Techniques such as quantization, pruning, or knowledge
    distillation can help reduce the model‚Äôs sensitivity to the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Training an ensemble of multiple models with different
    architectures or using different algorithms can make it more difficult for attackers
    to perform accurate membership inference. Ensemble methods make it harder for
    an attacker to learn the specific patterns in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure aggregation**: If the model is trained using a distributed setting,
    secure aggregation protocols can be employed to ensure that individual contributions
    from different parties are protected and the membership information is not exposed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Randomized response**: Randomized response techniques can be used to introduce
    noise into the model‚Äôs outputs during inference, making it harder for an attacker
    to determine membership status. Randomized response mechanisms ensure plausible
    deniability for individual records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control and authorization**: Implementing access control measures
    and strong authorization mechanisms can help restrict access to sensitive models
    and data, limiting the exposure to potential attackers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Continuously monitoring the model‚Äôs behavior for any
    unusual patterns or unexpected outputs can help detect potential membership inference
    attacks. Monitoring can involve techniques such as outlier detection, adversarial
    robustness checks, or statistical analysis of model outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It‚Äôs important to note that no single technique can provide complete protection
    against membership inference attacks. A combination of multiple techniques and
    a comprehensive approach to privacy and security is usually required to effectively
    mitigate these attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Model extraction attack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model extraction attack is a type of black-box attack in which the adversary
    aims to extract information, and possibly recreate a model, by creating a substitute
    model (denoted as *ùëì*‚Äô) that closely emulates the behavior of the original model
    being targeted (denoted as *ùëì*).
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs consider a scenario where we have developed an ML model specifically designed
    to predict whether a given post/tweet pertains to a disaster or not. We provide
    APIs to consumers, enabling them to access these prediction capabilities, and
    charge a fee for each API request made.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 - Securing ML Model Integrity](img/B16573_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 - Securing ML Model Integrity
  prefs: []
  type: TYPE_NORMAL
- en: The adversary takes advantage of the API provided and systematically submits
    thousands of input tweets in order to obtain their respective predictions. Subsequently,
    the adversary proceeds to construct a new ML model using these tweets, which were
    obtained by querying the API exposed by the original author. The predicted results
    obtained from the API serve as the class labels for this new model, indicating
    whether the tweets are classified as disaster-related or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 ‚Äì Model extraction attack‚Äîadversary ML model](img/B16573_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 ‚Äì Model extraction attack‚Äîadversary ML model
  prefs: []
  type: TYPE_NORMAL
- en: In certain instances, the ML model developed by the adversary may exhibit superior
    accuracy compared to the original author‚Äôs ML model. As a consequence, this can
    significantly affect the revenue of the original author‚Äôs company. The adversary
    may exploit this advantage by exposing similar inference APIs, charging substantially
    lower fees than the original author, and potentially engaging in the theft of
    intellectual property. Furthermore, the model extraction attack enables the adversary
    to gain access to private information associated with the ML model, further exacerbating
    the potential damages caused.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a model extraction attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The source code for this example can be found in `Model` `Extraction_Attack
    Example.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we start by creating a sample dataset that consists of two
    features (*X*) and their corresponding labels (*y*). Subsequently, we train a
    logistic regression model on this dataset using the `LogisticRegression` class
    from scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: The attacker‚Äôs code aims to execute a model extraction attack by training a
    new `LogisticRegression` model (`extracted_model`) on the same dataset. The attacker‚Äôs
    objective is to replicate the original model‚Äôs behavior without having direct
    access to its internal workings.
  prefs: []
  type: TYPE_NORMAL
- en: Once the extracted model is successfully generated, it can be utilized for unauthorized
    purposes, such as making predictions on new data (`new_data`) without requiring
    access to the original model. This unauthorized usage raises concerns regarding
    the security and integrity of the original model‚Äôs functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the source code for the model extraction attack (`Model` `Extraction_Attack
    Example.ipynb`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Techniques to mitigate model extraction attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mitigating model extraction attacks, where an adversary tries to extract the
    underlying model‚Äôs architecture, parameters, or functionality, is crucial for
    protecting intellectual property and maintaining the security of sensitive models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the techniques to mitigate model extraction attacks are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model watermarking**: Embedding a unique watermark into the model‚Äôs parameters
    or architecture can help identify the origin of the model and deter unauthorized
    extraction. Watermarking techniques can be designed to be resilient against removal
    attempts or modifications while remaining imperceptible to normal model operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model obfuscation**: Applying obfuscation techniques to the model‚Äôs code
    or architecture can make it harder for attackers to understand the internal workings
    of the model. Obfuscation can involve techniques such as code obfuscation, function
    renaming, control flow diversification, or encryption to protect the model‚Äôs implementation
    details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure model sharing**: When sharing models with authorized users or collaborators,
    it‚Äôs important to employ secure sharing mechanisms. This can involve encryption
    during transit and at rest, strong access control measures, and secure authentication
    and authorization protocols to prevent unauthorized access to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model compression**: Using model compression techniques such as quantization,
    pruning, or knowledge distillation can make the model more compact and reduce
    the amount of information that can be extracted. Compressed models often have
    fewer parameters and structural details, making them more resistant to model extraction
    attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained access control**: Implementing fine-grained access control mechanisms
    can limit the exposure of sensitive models. This can involve providing access
    to only the necessary components or functionalities of the model based on user
    roles and permissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure execution environment**: Running the model in a secure execution environment
    can help protect against extraction attacks. Techniques such as secure enclaves
    (e.g., Intel SGX or AMD SEV), **trusted execution environments** (**TEEs**), and
    **secure multiparty computation** (**MPC**) can provide isolation and integrity
    guarantees for executing models, preventing unauthorized access to the model‚Äôs
    internals. We will learn more about TEE in [*Chapter 9*](B16573_09.xhtml#_idTextAnchor204).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model metadata protection**: Protecting the metadata associated with the
    model, such as the training data, hyperparameters, or training process details,
    can make it harder for attackers to extract meaningful information about the model.
    Techniques such as differential privacy or data perturbation can help preserve
    privacy in model metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring for abnormal model usage**: Implementing model monitoring and
    anomaly detection mechanisms can help identify suspicious activities, such as
    repeated queries or excessive model interactions, which could indicate unauthorized
    extraction attempts. Monitoring can trigger alerts or initiate defensive actions
    when potential attacks are detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal and licensing measures**: Implementing legal protections, such as copyright,
    patent, or licensing agreements, can provide additional legal recourse and deter
    unauthorized model extraction and usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we discussed with the membership inference attack, it‚Äôs important to note
    that no single technique can provide complete protection against model extraction
    attacks; a combination of multiple techniques is usually required. The choice
    of mitigation techniques depends on the specific threat model, the sensitivity
    of the model, and the desired level of protection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have learned about membership inference attacks and model extraction attacks
    on ML models. Let‚Äôs now explore the third type of privacy attack on ML models:
    the reconstruction attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction attacks‚Äîmodel inversion attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reconstruction attacks try to recreate one or more instances of training data
    and/or their respective class labels. The reconstruction may be partial or full,
    depending on the strength of the original model. A fully successful attack can
    generate more realistic training data and various samples to match exact class
    label predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Model inversion or attribute inference are kinds of reconstruction attacks.
    They come under the black-box attack category because the attacker doesn‚Äôt need
    to know the details of the model‚Äôs structure or internal workings. They only need
    access to the model‚Äôs output based on some input data. Using that, they can infer
    details about the data used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step example of creating a model inversion attack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this example, we first create a simple dataset with two input features (`X`)
    and binary labels (`y`). We train a logistic regression model using this dataset.
    The `model_inversion_attack` function attempts to invert the model by finding
    an input that produces the desired output probability.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is a basic example to illustrate the concept of model
    inversion attacks. In real-world scenarios, model inversion attacks can be more
    complex and require sophisticated techniques to handle larger and more complex
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code can be found in `Model_Inversion_LR_Sample.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs explore a more complex example to understand model inversion attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Model inversion attacks in neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are a class of ML models inspired by the structure and functioning
    of the human brain. They are designed to recognize complex patterns and relationships
    in data. Neural networks consist of interconnected layers of artificial neurons,
    known as nodes or units, which collectively form a network.
  prefs: []
  type: TYPE_NORMAL
- en: Each neuron receives input signals, applies a mathematical operation to them,
    and produces an output signal. These signals are passed through the network, with
    weights assigned to the connections between neurons determining the strength of
    the signal. Neural networks are trained using a process called backpropagation,
    which adjusts the weights based on the errors between predicted and actual outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layers of a neural network enable it to learn and represent intricate
    nonlinear relationships in the data, making it capable of solving highly complex
    tasks such as image and speech recognition, natural language processing, and even
    playing games. Popular neural network architectures include feedforward neural
    networks, **convolutional neural networks** (**CNNs**), and **recurrent neural**
    **networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks have achieved remarkable success in various fields, demonstrating
    state-of-the-art performance in many domains. They have become a fundamental tool
    in machine learning and continue to advance the boundaries of artificial intelligence
    by enabling sophisticated decision-making and pattern recognition capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We will not delve into the intricacies of neural networks, as this exceeds the
    scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will demonstrate how an adversary can generate input data
    using the output of a neural network model. The adversary‚Äôs goal is to reconstruct
    the original input that led to a specific output prediction by leveraging the
    characteristics of the model‚Äôs behavior. By reverse-engineering the relationship
    between the model‚Äôs output and the corresponding input data, the adversary can
    gain insights into the original data points used for training the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 ‚Äì Neural network models of the original author and the adversary](img/B16573_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 ‚Äì Neural network models of the original author and the adversary
  prefs: []
  type: TYPE_NORMAL
- en: Input data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this example, we utilize the **Modified National Institute of Standards and
    Technology** (**MNIST**) dataset to train a neural network model. The MNIST dataset
    comprises 60,000 grayscale images of handwritten single digits between 0 and 9\.
    Each image is a small square with dimensions of 28 x 28 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Original authors model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset from Github ([https://github.com/pytorch/tutorials/blob/main/_static/mnist.pkl.gz](https://github.com/pytorch/tutorials/blob/main/_static/mnist.pkl.gz))
    and keep it in the **data/mnist** directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code can be found in `Model_Inversion_Attack_Example.ipynb`.
    We are using PyTorch version 1.13.1 here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the data, you can visualize one sample image using the Matplotlib
    library and obtain its shape. Here‚Äôs the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16573_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`(50000, 784) 5`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, convert the training samples into the tensor format in order to use the
    same in the neural network model as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs build a simple sequential neural network model with linear layers using
    **rectified linear unit** (**ReLU**) as an activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`AuthorsNN` class extends the `nn.Module` class from PyTorch. This class represents
    a neural network model designed for a classification task. Here‚Äôs a breakdown
    of the preceding code and its functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The AuthorsNN class**: This class represents the neural network model designed
    by the author(s). It inherits from the **nn.Module** class, which is the base
    class for all neural network modules in PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The __init__ method**: This method is the constructor of the **AuthorsNN**
    class and is called when an instance of the class is created. Inside this method,
    the architecture of the model is defined:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**self.first_sec** is a sequential module consisting of two layers:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.Linear(784, 450)** represents a linear layer with 784 input features and
    450 output features.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.ReLU()** applies the ReLU activation function to introduce non-linearity.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**self.second_sec** is another sequential module consisting of three layers:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.Linear(450, 450)** represents a linear layer with 450 input features and
    450 output features.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.ReLU()** applies the ReLU activation function.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.Linear(450, 10)** represents a linear layer with 450 input features and
    10 output features, corresponding to the number of classes.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nn.Softmax(dim=-1)** applies the softmax activation function to convert the
    raw output scores into probabilities, ensuring they sum to **1** across classes.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The forward method**: This method defines the forward pass of the model,
    specifying how input data flows through the network. The input, **x**, is passed
    through **self.first_sec**, followed by **self.second_sec**, and the resulting
    output is returned.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code creates an instance of the `AuthorsNN` class named `auth_nn`.
    This instance represents the initialized neural network model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Printing `auth_nn` will display information about the model, such as its architecture
    and the number of trainable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a loss function in order to measure the error between the actual
    data versus the predicted data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: To enhance the network, let‚Äôs add the Adam optimizer function. This is an optimization
    algorithm that replaces **stochastic gradient descent** (**SGD**) for training
    DL models.
  prefs: []
  type: TYPE_NORMAL
- en: 'It combines the desirable aspects of the `AdaGrad` and `RMSProp` algorithms,
    making it suitable for handling sparse gradients in noisy problem scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Here, we import the `optim` module from PyTorch. After instantiating the `AuthorsNN`
    class, we define the Adam optimizer using the `optim.Adam()` function. The optimizer
    is initialized with the model‚Äôs parameters (`auth_nn.parameters()`), enabling
    it to optimize the model during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, print `optimizer` to provide details about the model‚Äôs optimizer‚Äôs configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, train the neural network model with the MNIST training dataset that we
    loaded earlier and wrap it in a Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs break this code down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The** **train** **function**: This function trains the neural network model
    (**ann**) for a specified number of epochs (**num_epochs**). The function assumes
    the presence of input data (**x_train**) and corresponding target labels (**y_train**)
    used for training the model. Here, **ann.train()** is called to set the model
    in training mode, enabling functionalities such as dropout and batch normalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The training loop**: For each epoch in the range of **num_epochs**, the following
    steps are executed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output = ann(x_train)**: Forward passes through the model, obtaining the
    output predictions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**loss = loss_func(output, y_train)**: Computes the loss between the predicted
    output and the ground truth labels.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimizer.zero_grad()**: Clears the gradients accumulated from the previous
    iteration.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**loss.backward()**: Performs backpropagation to compute the gradients of the
    model‚Äôs parameters with respect to the loss.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**optimizer.step()**: Updates the model‚Äôs parameters by applying the computed
    gradients using the chosen optimizer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**print(epoch, loss.item()**): Prints the current epoch number and the **loss**
    value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **pass** statement: A placeholder that does nothing in this context and
    can be removed if not needed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, train the neural network model with the MNIST training dataset that we
    loaded earlier with the author‚Äôs neural network model, which was built in the
    previous step with `100` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is trained, it can be used for further predictions. Now, we will
    construct the adversary attacker model with the objective of recreating the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Adversary model to get the trained input data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Considering that the author‚Äôs model has been trained on the MNIST dataset and
    we have access to the size `450` vector output from the model‚Äôs first section
    (`first_sec`), we can utilize this information for our attack. Next, we will develop
    our adversary model. This model takes a size `450` vector as input, which corresponds
    to the output of the target‚Äôs first section. The adversary model‚Äôs objective is
    to generate a size `784` vector, matching the size of the original input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Based on the information available, the authors‚Äô original model was trained
    on a dataset consisting of handwritten images. This knowledge provides us with
    an understanding of the model‚Äôs training data source.
  prefs: []
  type: TYPE_NORMAL
- en: To train our adversary model, we can utilize the MNIST test data. Specifically,
    we will use the first 1,000 rows of the MNIST test data to train our adversary
    model. After training, we can evaluate the accuracy of the adversary model using
    the MNIST test data ranging from the 1,000th row to the 2,000th row.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs train the adversary model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To assess the similarity between the recreated data and the original trained
    images from the training dataset, we can utilize the Matplotlib library to visualize
    the images. By plotting the recreated image, we can determine the level of resemblance
    it holds with the original trained images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16573_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It is evident that the images generated using model inversion attacks closely
    resemble the training data. This example demonstrates the successful recreation
    of training data without requiring complete knowledge of the model details, thereby
    achieving a model inversion attack.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to mitigate model inversion attacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mitigating model inversion attacks, where an adversary tries to infer sensitive
    training data from a trained model‚Äôs outputs, is crucial for preserving privacy
    and protecting sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some techniques to mitigate model inversion attacks include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Differential privacy**: Applying differential privacy mechanisms during the
    training process can help protect against model inversion attacks. Differential
    privacy adds controlled noise to the training data or model‚Äôs outputs, making
    it harder for an attacker to extract specific sensitive information from the model‚Äôs
    predictions. We will learn more about differential privacy in the next two chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limit access to sensitive output**: Restricting access to sensitive model
    output or predictions can help mitigate model inversion attacks. By carefully
    controlling who has access to the output and under what circumstances, you can
    reduce the risk of an adversary inferring sensitive training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preprocessing and postprocessing**: Applying preprocessing and postprocessing
    techniques to the data and model‚Äôs output can help protect against model inversion
    attacks. For example, data anonymization, aggregation, or transformation techniques
    can be applied to remove or obfuscate sensitive information from the input or
    output. We will learn more about data anonymization and aggregation in the subsequent
    chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: Incorporating regularization techniques such as L1 or L2
    regularization during the model training process can help improve privacy by reducing
    the model‚Äôs reliance on specific sensitive features. Regularization can help prevent
    overfitting and limit the leakage of sensitive information through the model‚Äôs
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**): Using generative models such
    as GANs can help protect against model inversion attacks. By generating synthetic
    data that preserves the statistical properties of the original data, GANs can
    provide alternative output for the attacker without revealing specific sensitive
    training instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure multi-party computation** (**MPC**): Leveraging secure MPC protocols
    can enable multiple parties to collaboratively train a model while keeping their
    individual training data private. Secure MPC ensures that no party can access
    the sensitive data of others, thereby mitigating model inversion attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secure aggregation**: In scenarios where models are trained using a distributed
    setting, secure aggregation protocols can be employed to prevent sensitive information
    leakage during the aggregation of model updates. This protects against model inversion
    attacks during the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access control and authorization**: Implementing access control measures
    and strong authorization mechanisms can help restrict access to sensitive model
    output, limiting the exposure to potential attackers. Only authorized entities
    should have access to sensitive predictions or output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data generation**: Instead of training models directly on sensitive
    data, using synthetic data generated from the original data can help mitigate
    model inversion attacks. Synthetic data retains the statistical characteristics
    of the original data but does not expose sensitive information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Continuously monitoring the model‚Äôs behavior for any
    unusual patterns or unexpected output can help detect potential model inversion
    attacks. Monitoring can involve techniques such as outlier detection, adversarial
    robustness checks, or statistical analysis of model predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like the previous two attacks, It‚Äôs important to note that choosing mitigation
    techniques depends on the specific context, the sensitivity of the data, and the
    desired level of privacy protection. Multiple techniques can be combined to achieve
    stronger privacy guarantees against model inversion attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, we have covered different types of ML (supervised and unsupervised)
    and explored how to save and execute models in various formats. Additionally,
    we delved into the different phases of ML (data extraction, data preparation,
    model development, model deployment, and inferencing) and discussed the privacy
    threats and attacks associated with each phase in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deeper into privacy-preserving data analysis
    and focus on understanding the concept of differential privacy. This will allow
    us to explore techniques and methodologies that ensure privacy while conducting
    data analysis tasks. By gaining a thorough understanding of differential privacy,
    we can better safeguard sensitive information and mitigate privacy risks in the
    context of ML.
  prefs: []
  type: TYPE_NORMAL
