<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer097">
<h1 class="chapter-number" id="_idParaDest-101"><a id="_idTextAnchor144"/>7</h1>
<h1 id="_idParaDest-102"><a id="_idTextAnchor145"/>Support Vector Machines</h1>
<p>In <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Naive Bayes</em>, we looked at using Bayes’ Theorem to find the emotions that are associated with individual tweets. The conclusion there was that the standard Naive Bayes algorithm worked well with some datasets and less well with others. In the following chapters, we will look at several other algorithms to see whether we can get any improvements, starting in this<a id="_idIndexMarker603"/> chapter with the well-known <strong class="bold">support vector machine</strong> (<strong class="bold">SVM</strong>)<a id="_idTextAnchor146"/> (Boser et al., <span class="No-Break">1992) approach.</span></p>
<p>We will start this chapter by giving a brief introduction to SVMs. This introduction will take a geometric approach that may be easier for you than the standard presentation.<a id="_idTextAnchor147"/> Bennett and Bredensteiner (see the <em class="italic">References</em> section) give detailed formal proof that the two approaches are equivalent – the discussion in this chapter is intended simply to provide an intuitive grasp of the issues. We will then show you how to use the <strong class="source-inline">sklearn.svm.LinearSVC</strong> implementation for our current task. As with the previous approaches, we will start with a simple application of the approach that will work well for some examples but less so for others; we will then introduce two ways of refining the approach to work with multi-label datasets, and finally, we will reflect on the results we <span class="No-Break">have obtained.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>The basic ideas <span class="No-Break">behind SVMs</span></li>
<li>Application of simple SVMs for <span class="No-Break">standard datasets</span></li>
<li>Ways of extending SVMs to cover <span class="No-Break">multi-label datasets</span></li>
</ul>
<h1 id="_idParaDest-103"><a id="_idTextAnchor148"/>A geometric introduction to SVMs</h1>
<p>Suppose we have <a id="_idIndexMarker604"/>two groups of entities, called B and R, where each entity is described by a pair of numerical coordinates. B includes objects with coordinates (6.38, -10.62), (4.29, -8.99), (8.68, -4.54), and so on<a id="_idIndexMarker605"/> and R contains objects with coordinates (6.50, -3.82), (7.39, -3.13), (7.64, -10.02), and so on (the example used in this discussion has been taken from <a href="https://scikit-learn.org/stable/modules/svm.xhtml#classification">https://scikit-learn.org/stable/modules/svm.xhtml#classification</a>). Plotting these points on a graph gives us <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 7.1 – Plot of the R and B points" height="219" src="image/B18714_07_01.jpg" width="291"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Plot of the R and B points</p>
<p>It looks as <a id="_idIndexMarker606"/>though you should be able to draw a straight line to separate the two groups, and if you could, then you could use it to decide whether some new point was an instance of R <span class="No-Break">or B.</span></p>
<p>There are numerous ways of finding such a line for a simple case like this. One approach would be to find <a id="_idIndexMarker607"/>the <strong class="bold">convex hu<a id="_idTextAnchor149"/>lls</strong> (Graham, 1972) for the two groups – that is, the polygons that include them. The easiest way to visualize this involves taking the leftmost point in the set as a starting point. Then, you should pick the most clockwise point from there and set that as the next point on the list, and then do the same again with that one until you get back to <span class="No-Break">the original.</span></p>
<p>To see how to pick the most clockwise point from a given starting point, consider the two diagrams <span class="No-Break">shown here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 7.2 – Turning counter-clockwise and clockwise" height="459" src="image/B18714_07_02.jpg" width="1023"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Turning counter-clockwise and clockwise</p>
<p>In the<a id="_idIndexMarker608"/> left-hand diagram, the slope from A to B is less steep than the slope from B to C, which means that you have to turn counter-clockwise when you get to B if you want to go from A to B to C, which, in turn, means that C is further counter-clockwise from A than B is. In the right-hand diagram, the slope from A’ to B’ is steeper than the slope from B’ to C’, which means that C’ is less counter-clockwise from A’ than B’ is. Thus, to see whether C is more or less counter-clockwise from A than B is, we need to calculate the slopes of the lines joining them and see which is steeper: the slope of the line from A to C is (C[1]-A[1])/(C[0]-A[0]), and likewise for the line joining A and B, so C is further counter-clockwise from A than B if (C[1]-A[1])/(C[0]-A[0]) &gt; (B[1]-A[1])/(B[0]-A[0]). Rearranging this gives us <strong class="source-inline">ccw</strong>, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
def ccw(a, b, c):    return (b[1]-a[1])*(c[0]-b[0]) &lt; (b[0]-a[0])*(c[1]-b[1])</pre>
<p>We can then use this to find the <a id="_idIndexMarker609"/>convex hull. We sort the points by their <em class="italic">Y</em> and <em class="italic">X</em> coordinates, which lets us find the lowest point, <strong class="source-inline">p</strong> (picking the leftmost of these if there is a tie). This point must lie on the hull, so we add it to the hull. We then pick the next item, <strong class="source-inline">q</strong>, in the list of points (or go back to the beginning if <strong class="source-inline">p</strong> was the last point – <strong class="source-inline">(p+1)%n</strong> will be 0 if <strong class="source-inline">p</strong> is <strong class="source-inline">n</strong> and <strong class="source-inline">p+1</strong> otherwise). We now go through the entire list of points starting at <strong class="source-inline">q</strong> using <strong class="source-inline">ccw</strong> to see whether going from <strong class="source-inline">p</strong> to <strong class="source-inline">i</strong> to <strong class="source-inline">q</strong> satisfies the constraint given previously: if it does, then <strong class="source-inline">i</strong> is further counter-clockwise from <strong class="source-inline">p</strong> than <strong class="source-inline">q</strong> is, so we replace <strong class="source-inline">q</strong> with it. At the end of this, we know that <strong class="source-inline">q</strong> is the furthest counter-clockwise point from <strong class="source-inline">p</strong>, so we add it to the hull <span class="No-Break">and continue.</span></p>
<p>The<a id="_idIndexMarker610"/> complexity of this algorithm is <em class="italic">o(H*N)</em>, where <em class="italic">H</em> is the size of the hull and <em class="italic">N</em> is the total number of points – <em class="italic">H</em> because the main loop terminates after the hull has been constructed by adding one item for each iteration, <em class="italic">N</em> because on each pass through the main loop, we have to look at every point to find the most counter-clockwise one. There are more complicated algorithms that are more efficient than this under certain circumstances, but the one given here is efficient enough for <span class="No-Break">our purposes:</span></p>
<pre class="source-code">
def naiveCH(points):    points.sort()
    p = 0
    hull = []
    while True:
        # Add current point to result
        hull.append(points[p])
        # Pick the next point (or go back to the beginning
        # if p was the last point)
        q = (p + 1) % n
        for i in range(len(points)):
    # If i is more counterclockwise
    # than current q, then update q
           if(ccw(points[p], points[i], points[q])):
                q = i;
    # Now q is the most counterclockwise with respect to p
    # Set p as q for next iteration, so that q is added to 'hull'
          p = q
    # Terminate when you get back to that start and close the loop
          if(p == 0):
              hull.append(points[0])
              break
    return hull</pre>
<p>The following figure<a id="_idIndexMarker611"/> illustrates how this algorithm progresses around the set of B points in <span class="No-Break">our example:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 7.3 – Growing the convex hull for B" height="222" src="image/B18714_07_03.jpg" width="1190"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Growing the convex hull for B</p>
<p>There are more <a id="_idIndexMarker612"/>efficient algorithms for growing the hull (see <strong class="source-inline">scipy.spatial.ConvexHull</strong>: <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml">https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml</a>), but this one is simple to understand. We use it to calculate the convex hulls of R <span class="No-Break">and B:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 7.4 – Convex hulls of B and R" height="219" src="image/B18714_07_04.jpg" width="292"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Convex hulls of B and R</p>
<p>If any lines <a id="_idIndexMarker613"/>separate R and B (if they are <strong class="bold">linearly separable</strong>), then at least one of the segments of the convex hull must be one. If we pick the edge from the hull of B that is nearest to some edge from the hull of R, we can see that it is a separator – all the B items are above or on the dashed orange line and all the R ones are below it, and likewise, all the R items are on or below the dotted green line and all the B ones are <span class="No-Break">above it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 7.5 – Hull segments as candidate separators for R and B" height="218" src="image/B18714_07_05.jpg" width="290"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Hull segments as candidate separators for R and B</p>
<p>But they <a id="_idIndexMarker614"/>are not very good separators. All items that fell just below the dashed orange line would be classified as R, even if they were only just below this line and hence were much nearer to the Bs than to the Rs; and all items that appeared just above the dotted green line would be classified as B, even if they were only just <span class="No-Break">above it.</span></p>
<p>So, we want some way of finding a separator that will deal with cases that fall between these two extreme lines appropriately. We can do this by finding the line from the closest point on one of the segments to the other (the dotted gray line) and then drawing our separator through the middle of, and perpendicular to, this line (the solid <span class="No-Break">black line):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 7.6 – Optimal separator for R and B" height="218" src="image/B18714_07_06.jpg" width="290"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Optimal separator for R and B</p>
<p>The solid black line is an optimal separator in that it makes the separation between the two groups as great as possible: the distance from the nearest point in each group to the line is as great as possible, so any unseen point that falls above it will be assigned to B, which is the best place for it to go, and any point that falls below it will be assigned <span class="No-Break">to R.</span></p>
<p>This is what we want. Unfortunately, it will all go wrong if some points are outliers – that is, if some R points fall within the main body of B points or some B points fall within the main body of R points. In the following example, we have switched two so that there is a B near the top left of the Rs and an R near the bottom of <span class="No-Break">the Bs:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 7.7 – One R point and one B point switched" height="216" src="image/B18714_07_07.jpg" width="289"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – One R point and one B point switched</p>
<p>The convex <a id="_idIndexMarker615"/>hulls of the two groups now overlap, and cannot sensibly be used for <span class="No-Break">finding separators:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 7.8 – Convex hulls with outliers" height="217" src="image/B18714_07_08.jpg" width="291"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Convex hulls with outliers</p>
<p>We can try to identify outliers and exclude them from their groups, for example, by finding the center of mass of the entire group, marked as black ovals, and removing a point from it if it is nearer to the center of mass of the <span class="No-Break">other group:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 7.9 – Centers of mass of the two groups" height="218" src="image/B18714_07_09.jpg" width="291"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Centers of mass of the two groups</p>
<p>It is clear<a id="_idIndexMarker616"/> that the outliers are nearer to the centers of mass of the “wrong” groups, and hence can be identified and removed from consideration when trying to find the separators. If we remove them both, we get non-overlapping hulls. The separator does not put the outliers on the “right” sides, but then no straight line could do that – any straight line that included the outlying R with the other Rs would have to include the outlying B and <span class="No-Break">vice versa:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two outliers (brown Xs are ignored)" height="221" src="image/B18714_07_10.jpg" width="592"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two outliers (brown Xs are ignored)</p>
<p>However, we can also<a id="_idIndexMarker617"/> get non-overlapping hulls by ignoring just one <span class="No-Break">of them:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying R (left) and B (right) points" height="219" src="image/B18714_07_11.jpg" width="593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying R (left) and B (right) points</p>
<p>This time, we are<a id="_idIndexMarker618"/> ignoring just one of the outlying points when calculating the hulls. In each case, we get two non-overlapping hulls and a separator, but this time, we have used more of the original data points, with just one of them lying on the wrong side of <span class="No-Break">the line.</span></p>
<p>Which is better? To use more of the data points when trying to find the separator, to have more of the data points lying on the right-hand side of the line, or to minimize the total distance between the line and the points in the two sets? If we do decide to ignore a point, which one should we choose (is the separator at the top of <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.11</em> better than the one at the bottom)? Is the bottom left B point a normal member of B or is it an outlier? If we did not have the outlying R point then there would never have been any reason to doubt that this one was indeed <span class="No-Break">a B.</span></p>
<p>We need to have a <a id="_idIndexMarker619"/>view of how important each of these issues is, and then we have to optimize our choice of points to get the best outcome. That turns this into an optimization algorithm, where we make successive changes to the sets of points to try to optimize the criteria given previously – how many points are included, how close the separator is to the nearest points, and in the most general case what the equation that defines the separator should be (there are tricks to allow for circular or curved separators or to allow the separator to be a bit bendy). If the separating line is straight – that is, the classes are linearly separable – then for the 2-dimensional cases, the line will have an equation such as <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="38" src="image/15.png" style="vertical-align:-0.257em;height:0.921em;width:8.464em" width="353"/>. When we move to three dimensions, the separator becomes a plane with an equation such as <img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;C&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="38" src="image/16.png" style="vertical-align:-0.257em;height:0.921em;width:11.140em" width="464"/>. When we move to even higher dimensions, it becomes a <strong class="bold">hyperplane</strong>, with an<a id="_idIndexMarker620"/> equation such <span class="No-Break">as </span><span class="No-Break"><img alt="&lt;mml:math  &gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;A&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;B&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;/mml:math&gt;" height="28" src="image/17.png" style="vertical-align:-0.012em;height:0.676em;width:15.699em" width="654"/></span></p>
<p>The simple programs that we used for illustration purposes are not powerful enough to deal with all these issues. We are going to want to work with very high-dimensional spaces where, in most cases, most of the dimensions are zero. Fortunately, there are plenty of efficient implementations that we can use. We will use the Python <strong class="source-inline">LinearSVC</strong> implementation from <strong class="source-inline">sklearn.svm</strong> – there are plenty of other implementations in Python, but <strong class="source-inline">sklearn</strong> packages tend to be stable and well-integrated with other parts of <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">LinearSVC</strong> is known to be particularly efficient for large sparse <span class="No-Break">linear tasks.</span></p>
<h1 id="_idParaDest-104"><a id="_idTextAnchor150"/>Using SVMs for sentiment mining</h1>
<p>We have now seen how<a id="_idIndexMarker621"/> SVMs provide classifiers by finding hyperplanes that separate the data into classes and have seen a graphical explanation of how such hyperplanes can be found, even when the data is not linearly separable. Now, we’ll look at how SVMs can be applied to our datasets to find the boundaries between sentiments, with an analysis of their behavior on single-label and multi-label datasets and a preliminary investigation into how their performance on multi-label datasets might <span class="No-Break">be improved.</span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor151"/>Applying our SVMs</h2>
<p>As with the previous<a id="_idIndexMarker622"/> classifiers, we can define the <strong class="source-inline">SVMCLASSIFIERs</strong> class as a subclass of <strong class="source-inline">SKLEARNCLASSIFIER</strong> by using the following initialization code (<strong class="source-inline">useDF</strong> is a flag to decide whether to use the TF-IDF algorithm from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> when building the training set; <strong class="source-inline">max_iter</strong> sets an upper bound on the number of iterations the SVM algorithm should carry out – for our examples, the scores tend to converge by 2,000 steps, so we generally use that as <span class="No-Break">the limit):</span></p>
<pre class="source-code">
    def __init__(self, train, args={"useDF":True}):        self.readTrainingData(train, args=args)
        # Make an sklearn SVM
        self.clsf = sklearn.svm.LinearSVC(max_iter=2000)
        # Get it to learn from the data
        self.clsf.fit(self.matrix, self.values)</pre>
<p>This is exactly like the constructor for <strong class="source-inline">NBCLASSIFIERS</strong> – just use <strong class="source-inline">readTrainingData</strong> to get the data into the right format and then use the <strong class="source-inline">sklearn</strong> implementation to construct <span class="No-Break">the SVM.</span></p>
<p>As usual, we start by applying this to our <span class="No-Break">standard datasets:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-7">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">Precision</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Recall</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">micro F1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">macro F1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Jaccard</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SEM4-EN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.916</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.916</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.916</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.916</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.845</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.620</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.260</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.366</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.372</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.224</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">WASSA-EN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.870</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.870</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.870</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.870</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.770</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.870</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.870</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.870</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.870</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.770</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.848</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.848</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.848</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.848</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.736</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SEM4-AR</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.679</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.679</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.679</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.679</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.514</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.586</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.255</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.356</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.367</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.216</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.781</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.767</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.774</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.778</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.631</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.592</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.574</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.583</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.494</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.412</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.493</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.295</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.369</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.372</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.226</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – SVM applied to the standard datasets</p>
<p>The basic<a id="_idIndexMarker623"/> SVM gets us the best scores we have seen so far for the WASSA and two of the SEM4 datasets, with scores for most of the other datasets that are close to the best we have obtained so far. As with the previous algorithms, if we use it with the standard settings, it does very poorly on the multi-label problems, simply because an algorithm that returns exactly one label will fail to cope with datasets where items can have zero or more than <span class="No-Break">one label.</span></p>
<p>Training an SVM takes significantly longer than training any of the classifiers we have looked at so far. Therefore, it is worth looking briefly at how accuracy and training time for CARER vary as we vary the size of the training data – if we find that having more data has little effect on accuracy but makes training take much longer, we may decide that getting more data is not worth <span class="No-Break">the bother.</span></p>
<p>When we plot accuracy (because there is no empty class, the values of recall and macro- and micro-F measure are all the same) and Jaccard score for CARER against the size of the training set, we see that we do not need the entire dataset – these two measures converge fairly rapidly, and if anything, performance starts to go down after <span class="No-Break">a while:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 7.13 – Accuracy versus training data size for SVMs" height="899" src="image/B18714_07_13.jpg" width="1629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Accuracy versus training data size for SVMs</p>
<p>The decrease in <a id="_idIndexMarker624"/>performance after about 30K tweets could just be noise or it could be the result of over-training – as machine learning algorithms see more and more data, they can start to pick up on things that are specific to the training data and are not present in the test data. Either way, the performance is substantially better than anything we saw in <em class="italic">Chapters 5</em> and <em class="italic">6</em> and appears to have leveled out at around 0.89 accuracy and <span class="No-Break">0.80 Jaccard.</span></p>
<p>We also plotted training time against data size to see whether it would be feasible to run it with more data if we could get it, and the time went up more or less linearly with the data size (this is commonly reported for this kind of problem). However, since the accuracy has already leveled off by about 40K, it seems unlikely that adding more data would make any <span class="No-Break">difference anyway:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 7.14 – Training time versus data size for SVMs" height="924" src="image/B18714_07_14.jpg" width="1647"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Training time versus data size for SVMs</p>
<p class="callout-heading">Note</p>
<p class="callout">The accuracy that’s obtained by any classifier with any amount of data must level off before reaching 1. Most of the curves that can easily be fitted to plots such as the one shown here (for example, polynomial curves) continue increasing as the <em class="italic">X</em>-value increases, so they can only be approximations to the real curve, but they do let you get an impression of whether increasing the amount of training data <span class="No-Break">is worthwhile.</span></p>
<h3>Experiments with datasets with tweets with no labels and tweets with multiple labels</h3>
<p>Why is the <a id="_idIndexMarker625"/>behavior<a id="_idIndexMarker626"/> of the standard SVM so much worse for the multi-label datasets than for most of <span class="No-Break">the others?</span></p>
<p>SVMs, like any other standard classifier, are designed for assigning each item to a single class. Each point in the training set is given a set of features and a label and the learning algorithm works out how the features and labels are connected. It is fairly straightforward to adapt this to include data where some points have no label, simply by saying that there is an extra label, called something such as neutral, or none of the above, or something like that, to be used when a point has not been given a label. This is a slightly artificial way to proceed because it means that the classifier finds words that are associated with having no emotion, whereas the real situation is that such points simply don’t have any words that carry emotional weight. However, it usually works reasonably well and can be assimilated into the standard SVM training algorithm (the definition of <strong class="source-inline">onehot2value</strong> shown in <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Naive Bayes</em>, allows for exactly this kind <span class="No-Break">of situation).</span></p>
<p>The<a id="_idIndexMarker627"/> SEM11 and KWT.M-AR examples belong to a harder, and arguably more realistic, class of problem, where<a id="_idIndexMarker628"/> a single tweet may express zero <em class="italic">or more</em> emotions. The second tweet in the test set for SEM11-EN, <em class="italic">I’m doing all this to make sure you smiling down on me bro</em>, expresses all three of joy, love, and optimism, and the second to last, <em class="italic"># ThingsIveLearned The wise # shepherd never trusts his flock to a # smiling wolf. # TeamFollowBack # fact # wisewords</em>, has no emotion linked <span class="No-Break">to it.</span></p>
<p>It is easy enough to encode these as vectors, given the possible set of emotions [‘anger’, ‘anticipation’, ‘disgust’, ‘fear’, ‘joy’, ‘love’, ‘optimism’, ‘pessimism’, ‘sadness’, ‘surprise’, ‘trust’]: we use [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] for joy, love, and optimism and [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0] for no emotion. <em class="italic">But these are not one-hot encodings</em>, and <strong class="source-inline">onehot2value</strong> will not deal with them properly. In particular, it will interpret [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] as joy since that is the first non-zero item <span class="No-Break">it encounters.</span></p>
<p>There is no simple way around this – SEM11 data has multiple emotions, and SVMs expect single emotions. This has two consequences. During training, only one of the emotions associated with a tweet will be used – if the preceding example had occurred during training, it would have led to the words <em class="italic">smiling</em> and <em class="italic">bro</em> (which are the only low document frequency words in this tweet) being associated with joy but not with love and optimism, which could lead to lower precision and lower recall; and if it occurred during testing, then it would inevitably have led to a loss of recall because only one of the three could be returned. This is borne out by the results shown previously, where SEM11-EN in particular has quite good precision but very poor recall, in contrast to the best of the results in <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Naive Bayes</em> where it has good recall but <span class="No-Break">poor precision.</span></p>
<p>The illustration earlier in this chapter of how SVMs work showed a two-class problem – Rs and Bs. There are two obvious ways of coping with the extension to multiclass problems (such as the CARER and WASSA datasets). Suppose we had Rs, Bs, and Gs. We could train three two-class classifiers, one for R versus B, one for R versus G, and one for B versus G (<strong class="bold">one versus one</strong>), and combine the results; alternatively, we could train a different set of two-class classifiers, one for R versus (B or G), one for B versus (R or G), and one for G versus (R or B) (<strong class="bold">one versus many</strong>). Both give similar results, but when there are a lot of classes, you have to train N*(N+1)/2 classifiers (N for the first class versus each of the rest + N-1 for the second class versus each of the rest + ...) for one versus one but only N for one versus many. For CARER, for instance, where there are six classes, we would have to train 21 classifiers and combine their results for one versus one, whereas for one versus many, we would only have to train six classifiers and combine <span class="No-Break">their results.</span></p>
<p>We need to follow one of these strategies for all our datasets since they all have several possible outcomes. Fortunately, <strong class="source-inline">sklearn.svm.LinearSVC</strong> does this automatically (using one versus many) for problems where there are a range of possible labels. This by itself, however, will not solve the problem of multi-label datasets – there is a difference between having an outcome consisting of one label drawn from several options and an outcome having an unknown number of labels drawn from several options. The standard multiple-classifiers, which are combined as one versus one or one versus many, will solve the first of these problems but not <span class="No-Break">the second.</span></p>
<p>There are two <a id="_idIndexMarker629"/>ways we can adapt our SVM classifier to deal with <span class="No-Break">this problem:</span></p>
<ul>
<li>We can follow<a id="_idIndexMarker630"/> the same strategy as with the Naive Bayes classifier of taking the real-valued scores for each emotion and using a threshold to determine whether or not a tweet satisfies <span class="No-Break">each emotion.</span></li>
<li>We can train several classifiers, one versus many style, and simply accept the results from each of them. In the preceding example, if the R versus (B or G) classifier says R, then we accept R as one of the labels for a test case; if B versus (R or G) says B, then we accept that <span class="No-Break"><em class="italic">as well</em></span><span class="No-Break">.</span></li>
</ul>
<p>We will look at each of these in turn in the next <span class="No-Break">two sections.</span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor152"/>Using a standard SVM with a threshold</h2>
<p>To use Naive Bayes <a id="_idIndexMarker631"/>with multi-label datasets, we changed <strong class="source-inline">applyToTweet</strong> <span class="No-Break">like so:</span></p>
<pre class="source-code">
    def applyToTweet(self, tweet):        tweet = tweets.tweet2sparse(tweet, self)
        # use predict_log_proba
        p = self.clsf.predict_log_proba(tweet)[0]
        # compare to previously defined threshold
        threshold = numpy.log(self.threshold)
        return [1 if i &gt; threshold else 0 for i in p]</pre>
<p>The preceding code used the fact that <strong class="source-inline">predict_log_proba</strong> returns a value for every label. In the standard version of Naive Bayes, we just pick the highest scoring label for each case, but using a threshold allows us to pick any number of labels, starting <span class="No-Break">from 0.</span></p>
<p>This won’t quite work for SVMs because they do not have a method called <strong class="source-inline">predict_log_proba</strong>. What they do have is a method called <strong class="source-inline">decision_function</strong>, which produces a score for each label. Rather than changing the definition of <strong class="source-inline">applyToTweet</strong> to use <strong class="source-inline">decision_function</strong> instead of <strong class="source-inline">predict_log_proba</strong>, we simply set the value of <strong class="source-inline">predict_log_proba</strong> to be <strong class="source-inline">decision_function</strong> in the constructor for SVMs, and then use <strong class="source-inline">applyToTweet</strong>, as we did previously. So, we must adapt the constructor for SVMs, as <span class="No-Break">shown here:</span></p>
<pre class="source-code">
    def __init__(self, train, args={}):        self.readTrainingData(train, args=args)
        # Make an sklearn SVM
        self.clsf = sklearn.svm.LinearSVC(max_iter=2000)
        # Get it to learn from the data
        self.clsf.fit(self.matrix, self.values)
<strong class="bold">        # Set its version of predict_proba to be its decision_function</strong>
<strong class="bold">        self.clsf.predict_proba = self.clsf.decision_function</strong>
<strong class="bold">        # and set its version of weights to be its coefficients</strong>
<strong class="bold">        self.weights = self.clsf.coef_</strong></pre>
<p>In other <a id="_idIndexMarker632"/>words, once we have made the underlying SVM, we must set a couple of standard properties that we will find useful and that do not have the same names in all the <strong class="source-inline">sklearn</strong> classifiers. The results of this for the multi-label cases are <span class="No-Break">as follows:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-4">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.511</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.328</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.399</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.387</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.249</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.521</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.290</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.373</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.361</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.229</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.135</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.694</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.227</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.131</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.128</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.434</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.338</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.380</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.361</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.235</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – SVMs using thresholds to deal with multi-label problems</p>
<p>The SEM11 cases are better than those for the simple SVM that we looked at earlier but are not better than the scores we obtained using the earlier algorithms, and the scores for KWT.M-AR are worse than with the simple SVMs. Just using the values that the decision function for the SVM assigns to each label does not solve the problem of multi-label datasets. We will refer to SVMs that use the set of values for each label plus a threshold as SVM (<span class="No-Break">multi) classifiers.</span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor153"/>Making multiple SVMs</h2>
<p>The second option is to make a set <a id="_idIndexMarker633"/>of one versus rest classifiers and accept every label for which the relevant classifier succeeds. The key is to take each label in turn and <strong class="source-inline">squeeze</strong> the <em class="italic">N</em> labels in the training data into two – one for the target label and one for all the others. Consider a tweet labeled as joy. The representation of this as a vector would be [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0] – that is, with 1 in the column for joy. If we squeeze this to be joy versus the rest, then it will come out as [1, 0] – that is, with a 1 in the new column for joy and a 0 in the column for not-joy. If we squeeze it to be angry versus not-angry, then it would be [0, 1], with a 0 in the new column for anger and a 1 in not-angry. If it had been labeled as joy and love, then the vector would have been [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0] and the squeezed version would have been [1, 1]: 1 in the first column because it does express joy and 1 in the second because it <em class="italic">also</em> expresses <span class="No-Break">something else.</span></p>
<p>Suppose <a id="_idIndexMarker634"/>we have a vector, <strong class="source-inline">gs</strong>, that represents the emotions for a multi-label tweet and we want to squeeze it on the <strong class="source-inline">I</strong> column. The first column is easy – we just set it to <strong class="source-inline">gs[i]</strong>. To get the second, which represents whether some column other than <strong class="source-inline">I</strong> is non-zero, we use <strong class="source-inline">numpy.sign(sum(gs[:i]+gs[i+1:])</strong>: <strong class="source-inline">gs[:i]</strong> and <strong class="source-inline">gs[i+1:]</strong> are the other columns. Taking their sum will be greater than 0 if at least one of them is non-zero while taking the sign of that will be 0 if the sum was 0 and 1 if it was greater than zero. Note that it is possible for both <strong class="source-inline">gs[i]</strong> and <strong class="source-inline">numpy.sign(sum(gs[:i]+gs[i+1:])</strong> to be <strong class="source-inline">0</strong> and for both of them to <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
def squeeze(train, i):# Collapse the Gold Standard for each tweet so that we just
# have two columns, one for emotion[i] in the original and
# one for the rest.
    l = []
    for tweet in train.tweets:
        # now squeeze the Gold Standard value
        gs = tweet.GS
        scores=[gs[i], numpy.sign(sum(gs[:i]+gs[i+1:]))]
        tweet = tweets.TWEET(id=tweet.id, src=tweet.src,
                             text=tweet.text, tf=tweet.tf,
                             scores=scores,
                             tokens=tweet.tokens,
                             args=tweet.ARGS)
        l.append(tweet)
    emotion = train.emotions[i]
    emotions = [emotion, "not %s"%(emotion)]
    return tweets.DATASET(emotions, l, train.df,
                          train.idf, train.ARGS)</pre>
<p>The constructor for <strong class="source-inline">MULTISVMCLASSIFIER</strong> is straightforward – just make one standard <strong class="source-inline">SVMCLASSIFIER</strong> for each emotion. To apply one to a tweet, we must apply each of the standard ones and gather the positive results. So, if the classifier that has been trained on joy versus not-joy says some tweet expresses joy, then we mark the tweet as satisfying joy, but <a id="_idIndexMarker635"/>we ignore what it says about not-joy since a positive score on not-joy simply tells us that the tweet also expresses some other emotion, and we are allowing tweets to express more than <span class="No-Break">one emotion:</span></p>
<pre class="source-code">
    def applyToTweet(self, tweet):        k = [0 for i in self.train.emotions]
        for i in self.classifiers:
            c = self.classifiers[i]
            p = c.clsf.predict(tweets.tweet2sparse(tweet, self))[0]
            """
            if classifier i says that this tweet expresses
            the classifier's emotion (i.e. if the underlying
            SVM returns 0) then set the ith column of the
            main classifier to 1
            """
            if p == 0:
                k[i] = 1
        return k</pre>
<p>This is <a id="_idIndexMarker636"/>pretty much the standard one versus many approach to training an SVM with multiple labels. The key difference is in the way that the results of the individual <em class="italic">X</em> versus not-<em class="italic">X</em> classifiers are combined – we accept <em class="italic">all</em> the positive results, whereas the standard approach just <span class="No-Break">accepts one.</span></p>
<p>The following table is a repeat of the table for the multi-label problems using SVM (multi) <span class="No-Break">for comparison:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table003-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.511</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.328</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.399</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.387</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.249</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.521</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.290</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.373</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.361</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.229</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.135</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.694</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.227</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.131</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.128</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.434</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.338</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.380</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.361</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.235</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Multi-label datasets, SVM (multi)</p>
<p>When we use multiple SVMs, one per label, we get an improvement in each case, with the score for SEM11-EN being the best <span class="No-Break">so far:</span></p>
<table class="No-Table-Style" id="table004-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Precision</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Recall</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">micro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">macro F1</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Jaccard</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">SEM11-EN</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.580</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.535</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.556</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.529</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">0.385</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.531</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.485</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.507</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.478</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.340</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.648</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.419</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.509</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.340</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.341</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.498</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.368</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.423</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.378</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.268</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Multi-label datasets, multiple SVMs</p>
<p>This is better than the results for the SVM (multi) case and is the best so far for SEM11-EN. The improvement over the SVM for the SEM11 datasets comes from the huge improvement in recall. Remember that the standard SVM can only return one result per datapoint, so its recall <em class="italic">must</em> be poor in cases where the Gold Standard contains more than one emotion – if a tweet has three emotions associated with it and the classifier reports just one, then the recall for that tweet is 1/3. The improvement for KWT.M-AR comes from the improvement in precision – if a tweet has zero emotions associated with it, as is common in this dataset, then the standard SVM must produce a false positive <span class="No-Break">for it.</span></p>
<p>Numerous <a id="_idIndexMarker637"/>tweaks can be applied to <strong class="source-inline">sklearn.svm.LinearSVC</strong>, and we can also try the tweaks from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> – using IDF to get the feature values, for instance, produces a small improvement across the board. These are worth trying once you have reasonable results with the default values, but it is easy to get carried away trying variations to try to gain a few percentage points on a given dataset. For now, we will simply note that even the default values provide good results in cases where the dataset has exactly one emotion per tweet, with the multi-SVM providing the best results yet for some of the more <span class="No-Break">difficult cases.</span></p>
<p>Using an SVM can easily be seen as yet another way of extracting a lexicon with weights from a corpus. The dimensions of the SVMs that were used in this chapter are just the words in the lexicon, and we can play the same games with that as in <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> – using different tokenizers, stemming, and eliminating uncommon words. We will not repeat these variations here: we know from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Sentiment Lexicons and Vector Space Models</em> that different combinations suit different datasets, and simply running through all the variations will not tell us anything new. It is, however, worth reflecting on exactly how SVMs use the weights that they assign to <span class="No-Break">individual words.</span></p>
<p>The SVM for the CARER dataset, for instance, has an array of six rows by 74,902 columns as its coefficients: six rows because there are six emotions in this dataset, and 75K columns because there are 75K distinct words. If we pick several words more or less at random, some of which are associated with some emotion and some that have very little emotional significance, we will see that their weights for the various emotions reflect <span class="No-Break">our intuition:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table005-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">anger</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">fear</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">joy</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">love</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">sadness</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">surprise</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">sorrow</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.033</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.233</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.014</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.026</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.119</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.068</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">scared</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.508</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">1.392</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">1.039</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.474</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.701</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.290</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">disgust</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">1.115</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.293</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.973</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.185</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.855</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.121</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">happy</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.239</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.267</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.546</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.210</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.432</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.080</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">adores</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.000</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.000</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.412</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.060</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.059</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.000</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">and</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.027</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.008</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.001</span></p>
</td>
<td class="No-Table-Style">
<p> -<span class="No-Break">0.008</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.020</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.004</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">the</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.001</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.012</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.004</span></p>
</td>
<td class="No-Table-Style">
<p> <span class="No-Break">0.001</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.002</span></p>
</td>
<td class="No-Table-Style">
<p>-<span class="No-Break">0.002</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Associations between words and emotions, SVM as the classifier, with the CARER dataset</p>
<p><em class="italic">sorrow</em> is strongly linked to <strong class="bold">sadness</strong>, <em class="italic">scared</em> is strongly linked to <strong class="bold">fear</strong>, <em class="italic">disgust</em> is strongly linked to <strong class="bold">anger</strong>, <em class="italic">happy</em> is strongly linked to <strong class="bold">joy</strong>, and <em class="italic">adores</em> is strongly linked to <strong class="bold">joy</strong> (but not, interestingly, to <strong class="bold">love</strong>: words always throw up surprises); and neutral words are not strongly linked to any particular emotion. The main thing that is different from the lexicons in <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Naive Bayes</em> is that some words also vote very strongly <em class="italic">against</em> some emotions – if you are <em class="italic">scared</em>, then you are not joyous, and if you are <em class="italic">happy</em>, then you are not angry, fearful, <span class="No-Break">or sad.</span></p>
<p>The way that <a id="_idIndexMarker638"/>SVMs use these weights for classification is the same as in <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Naive Bayes</em> – if you are given a vector of values, V = [v0, v1, ..., vn], and a set of coefficients, C = [w0, w1, ..., wn], then checking whether <strong class="source-inline">V.dot(C)</strong> is greater than some threshold is exactly what we did with the weights in <a href="B18714_06.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Naive Bayes</em> (given that V and C are sparse arrays in <strong class="source-inline">sklearn.svm.LinearSVC</strong>, this may be a fairly fast way to do this sum, but it is the same sum). The only differences lie in the way that SVMs obtain the weights and the fact that an SVM can assign negative weights to words. We will return to ways of handling multi-label datasets in <a href="B18714_10.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Multiclassifiers</em>. For now, we will just note that SVMs and the simple lexicon-based approaches end up using the same decision function on the same features, but that the way that SVMs arrive at the weights for those features is generally better, and in some cases <span class="No-Break">much better.</span></p>
<h1 id="_idParaDest-108"><a id="_idTextAnchor154"/>Summary</h1>
<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.17</em> shows the best classifiers that we have seen so far, with Jaccard scores, for each of <span class="No-Break">the datasets:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table006-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">LEX</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">CP (unstemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">CP (</strong><span class="No-Break"><strong class="bold">stemmed)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">NB (</strong><span class="No-Break"><strong class="bold">single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">NB (multi)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">SVM (</strong><span class="No-Break"><strong class="bold">single)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">SVM (</strong><span class="No-Break"><strong class="bold">multi)</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">MULTI-SVM</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.497</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.593</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.593</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.775</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.778</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.845*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.836</span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.348</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.352</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.353</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.227</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.267</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.224</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.249</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.385*</strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">WASSA-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.437</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.512</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.505</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.709</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.707</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.770*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.749</span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">CARER-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.350</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.414</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.395</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.776</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.774</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.770</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.796*</strong></span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">IMDB-EN</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.667</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.721</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.722</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.738</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.740*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.736</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.736</span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.509</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.493</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.513</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.531</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.532*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.514</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.494</span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-AR</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.386*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.370</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.382</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.236</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.274</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.216</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.229</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.340</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">KWT.M-AR</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.663</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.684*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.666</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.494</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.507</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.631</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.128</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.341</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM4-ES</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.420*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.191</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.177</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.360</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.331</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.412</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.336</span></p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">SEM11-ES</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.271</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.276</span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">*</strong><span class="No-Break"><strong class="bold">0.278*</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.230</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.255</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.226</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.235</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0.268</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Best classifier for each dataset</p>
<p>As we can see, different classifiers work well with different datasets. The major lesson here is that you should not just accept that there is a single best classification algorithm: do experiments, try out variations, and see for yourself what works best with your data. It is also worth noting that the multi-label datasets (SEM11-EN, SEM11-AR, SEM11-ES, and KWT.M-AR) score very poorly with simple SVMs, and the only one where the multi-SVM wins is SEM11-EN, with simple algorithms from <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Sentiment Lexicons and Vector-Space Models</em>, still producing the best scores for the <span class="No-Break">other cases.</span></p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor155"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li><a id="_idTextAnchor156"/>Bennett, K. P., &amp; Bredensteiner, E. J. (2000). <em class="italic">Duality and Geometry in SVM Classifiers</em>. Proceedings of the Seventeenth International Conference on Machine <span class="No-Break">Learning, 57–64.</span></li>
<li>Boser, B. E., Guyon, I. M., &amp; Vapnik, V. N. (1992). <em class="italic">A Training Algorithm for Optimal Margin Classifiers</em>. Proceedings of the Fifth Annual Workshop on Computational Learning Theory, <span class="No-Break">144–152. </span><a href="https://doi.org/10.1145/130385.130401"><span class="No-Break">https://doi.org/10.1145/130385.130401</span></a><span class="No-Break">.</span></li>
<li>Graham, R. L. (1972). <em class="italic">An efficient algorithm for determining the convex hull of a finite planar set</em>. Information Processing Letters, 1(4), <span class="No-Break">132–133. </span><a href="https://doi.org/10.1016/0020-0190(72)90045-2"><span class="No-Break">https://doi.org/10.1016/0020-0190(72)90045-2</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div></body></html>