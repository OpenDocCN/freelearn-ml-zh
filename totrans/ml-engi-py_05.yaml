- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Deployment Patterns and Tools
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署模式和工具
- en: In this chapter, we will dive into some important concepts around the deployment
    of your **machine learning** (**ML**) solution. We will begin to close the circle
    of the ML development lifecycle and lay the groundwork for getting your solutions
    out into the world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨围绕部署您的**机器学习**（**ML**）解决方案的一些重要概念。我们将开始闭合机器学习开发生命周期的闭环，并为将您的解决方案推向世界打下基础。
- en: The act of deploying software, of taking it from a demo you can show off to
    a few stakeholders to a service that will ultimately impact customers or colleagues,
    is a very exhilarating but often challenging exercise. It also remains one of
    the most difficult aspects of any ML project and getting it right can ultimately
    make the difference between generating value or just hype.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将软件部署的过程，即从您可以向少数利益相关者展示的演示版本到最终影响客户或同事的服务，是一项非常令人兴奋但往往具有挑战性的练习。这也一直是任何机器学习项目中最困难的部分之一，而且做对它最终可能是在创造价值或仅仅炒作之间产生差异的关键。
- en: We are going to explore some of the main concepts that will help your ML engineering
    team cross the chasm between a fun proof-of-concept to solutions that can run
    on scalable infrastructure in an automated way. This will require us to first
    cover questions of how to design and architect your ML systems, particularly if
    you want to develop solutions that can be scaled and extended seamlessly. We will
    then discuss the concept of containerization and how this allows your application
    code to be abstracted from the specific infrastructure it is being built or run
    on, allowing for portability in many different cases. We will then move on to
    a concrete example of using these ideas to deploy an ML microservice on AWS. The
    rest of the chapter will then return to the question of how to build effective
    and robust pipelines for your end-to-end ML solution, which was introduced in
    *Chapter 4*, *Packaging Up*. We will introduce and explore **Apache Airflow**
    for building and orchestrating any generic Python process, including your data
    preparation and ML pipelines. Then we will finish up with a similar deep dive
    on **ZenML** and **Kubeflow**, two open-source advanced ML pipelining tools that
    are now extensively used in industry. This collection of tools means that you
    should finish this chapter feeling very confident that you can deploy and orchestrate
    quite complex ML solutions using a variety of software.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨一些主要概念，这些概念将帮助您的机器学习（ML）工程团队能够跨越从有趣的证明概念到可以在可扩展基础设施上以自动化方式运行的解决方案之间的鸿沟。这要求我们首先讨论如何设计和架构您的机器学习（ML）系统，尤其是如果您想开发可以无缝扩展和扩展的解决方案。然后我们将讨论容器化的概念以及它如何允许您的应用程序代码从它正在构建或运行的特定基础设施中抽象出来，从而在许多不同情况下实现可移植性。然后我们将继续到一个具体的例子，使用这些想法在AWS上部署机器学习（ML）微服务。本章的其余部分将回到如何构建有效且健壮的管道以用于您的端到端机器学习（ML）解决方案的问题，这在*第4章*，*打包*中已介绍。我们将介绍并探索**Apache
    Airflow**，用于构建和编排任何通用的Python过程，包括您的数据准备和机器学习（ML）管道。然后我们将以类似的方式深入研究**ZenML**和**Kubeflow**，这两个开源的高级机器学习（ML）管道工具现在在工业界得到了广泛的应用。这些工具的集合意味着您应该在本章结束时非常有信心，可以使用各种软件部署和编排相当复杂的机器学习（ML）解决方案。
- en: 'This will all be broken down into the following sections:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都将分为以下几部分：
- en: Architecting systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统架构
- en: Exploring some standard ML patterns
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索一些标准的机器学习（ML）模式
- en: Containerizing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器化
- en: Hosting your own microservice on **Amazon Web Services** (**AWS**)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**亚马逊网络服务**（**AWS**）上托管您的微服务
- en: Building general pipelines with Airflow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Airflow构建通用管道
- en: Building advanced ML pipelines
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建高级机器学习（ML）管道
- en: The next section will kick things off with a discussion of how we can architect
    and design our ML systems with deployment in mind. Let’s go!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将开始讨论如何考虑部署来架构和设计我们的机器学习（ML）系统。让我们开始吧！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As with the other chapters, you can set up your Python development environment
    to be able to run the examples in this chapter by using the supplied Conda environment
    `yml` file or the `requirements.txt` files from the book repository, under *Chapter05*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节一样，您可以通过使用提供的Conda环境`yml`文件或从书库中的*Chapter05*目录下的`requirements.txt`文件来设置您的Python开发环境，以便能够运行本章中的示例。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will also require some non-Python tools to be installed to follow the examples
    from end to end. Please see the respective documentation for each tool:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要安装一些非Python工具才能从头到尾跟随示例。请参阅每个工具的相关文档：
- en: AWS CLI v2
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI v2
- en: Postman
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Postman
- en: Docker
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Architecting systems
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统架构
- en: No matter how you are working to build your software, it is always important
    to have a design in mind. This section will highlight the key considerations we
    must bear in mind when architecting ML systems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你如何努力构建你的软件，始终有一个设计在心中是非常重要的。本节将强调我们在设计机器学习系统时必须牢记的关键考虑因素。
- en: Consider a scenario where you are contracted to organize the building of a house.
    We would not simply go out and hire a team of builders, buy all the supplies,
    hire all the equipment, and just tell everyone to *start building*. We would also
    not assume we knew exactly what the client who hired us wants without first speaking
    to them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个场景，你被雇佣来组织建造房屋。我们不会简单地出去雇佣一支建筑队，购买所有材料，雇佣所有设备，然后告诉每个人开始建造。我们也不会在没有先与他们交谈的情况下就假设我们确切地知道雇佣我们的客户想要什么。
- en: Instead, we would likely try to understand what the client wanted in detail,
    and then try to design the solution that would fit their requirements. We would
    potentially iterate this plan a few times with them and with appropriate experts
    who knew the details of pieces that fed into the overall design. Although we are
    not interested in building houses (or maybe you are, but there will not be any
    in this book!), we can still see the analogy with software. Before building anything,
    we should create an effective and clear design. This design provides the direction
    of travel for the solution and helps the build team know exactly what components
    they will work on. This means that we will be confident that what we build will
    solve the end user’s problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可能会试图详细了解客户的需求，然后尝试设计符合他们要求的解决方案。我们可能会与他们以及了解整体设计细节的适当专家一起迭代这个计划几次。尽管我们可能不感兴趣建造房屋（或者也许你感兴趣，但本书中不会有任何房屋的例子！），但我们仍然可以将其与软件进行类比。在建造任何东西之前，我们应该创建一个有效且清晰的设计。这个设计为解决方案提供了方向，并帮助构建团队确切地知道他们将工作的组件。这意味着我们将有信心，我们所建造的东西将解决最终用户的问题。
- en: This, in a nutshell, is what software architecture is all about.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是软件架构的全部内容。
- en: If we did the equivalent of the above example for our ML solution, some of the
    following things may happen. We could end up with a very confusing code base,
    with some ML engineers in our team building elements and functionality that are
    already covered by the work that other engineers have done. We may also build
    something that fundamentally cannot work later in the project; for example, if
    we have selected a tool that has specific environmental requirements we cannot
    meet due to another component. We may also struggle to anticipate what infrastructure
    we need to be provisioned ahead of time, leading to a disorganized scramble within
    the project to get the correct resource. We may also underestimate the amount
    of work required and miss our deadline. All of these are outcomes we wish to avoid
    and can be avoided if we are following a good design.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对我们的机器学习解决方案做了上述类似的事情，以下一些事情可能会发生。我们可能会得到一个非常混乱的代码库，我们团队中的某些机器学习工程师可能会构建已经被其他工程师的工作所覆盖的元素和功能。我们也可能会构建在项目后期基本无法工作的东西；例如，如果我们选择了一个具有特定环境要求，而我们无法满足这些要求的工具。我们也可能难以预测需要提前配置的基础设施，导致项目内部混乱地争夺正确的资源。我们也可能低估所需的工作量，错过截止日期。所有这些都是我们希望避免的结果，如果我们遵循良好的设计，这些结果是可以避免的。
- en: 'In order to be effective, the architecture of a piece of software should provide
    at least the following things to the team working on building the solution:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效，软件的架构应该至少为构建解决方案的团队提供以下东西：
- en: It should define the functional components required to solve the problem in
    totality.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该定义解决整个问题的所需功能组件。
- en: It should define how these functional components will interact, usually through
    the exchange of some form of data.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该定义这些功能组件如何交互，通常是通过交换某种形式的数据。
- en: It should show how the solution can be extended in the future to include further
    functionality the client may require.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该展示解决方案如何在未来扩展，以包括客户可能需要的进一步功能。
- en: It should provide guidance on which tools should be selected to implement each
    of the components outlined in the architecture.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该提供关于应选择哪些工具来实现架构中概述的每个组件的指导。
- en: It should stipulate the process flow for the solution, as well as the data flow.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它应该规定解决方案的过程流程以及数据流程。
- en: This is what a piece of good architecture should do, but what does this actually
    mean in practice?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一件好的架构应该做的事情，但实际上这意味着什么呢？
- en: There is no strict definition of how an architecture has to be compiled. The
    key point is that it acts as a design against which building can progress. So,
    for example, this might take the form of a nice diagram with boxes, lines, and
    some text, or it could be a several-page document. It might be compiled using
    a formal modeling language such as **Unified Modeling Language** (**UML**), or
    not. This often depends on the business context in which you operate and what
    requirements are placed on the people writing the architecture. The key is that
    it checks off the points above and gives the engineers clear guidance on what
    to build and how it will all stick together.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如何构建架构并没有严格的定义。关键点在于它作为一个设计，建筑可以在此基础上进行。例如，这可能是一个带有框、线和一些文本的漂亮图表，或者可能是一份多页文档。它可能使用形式化的建模语言，如**统一建模语言**（**UML**），也可能不使用。这通常取决于你所在的企业环境以及对你编写架构的人提出的要求。关键是它检查了上述要点，并为工程师提供了关于要构建什么以及如何将这些部分组合在一起的明确指导。
- en: Architecture is a vast and fascinating subject in itself, so we will not go
    much further into the details of this here, but we will now focus on what architecture
    means in an ML engineering context.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 架构本身是一个广泛而迷人的主题，所以我们不会在这里深入探讨其细节，但我们将现在关注在ML工程背景下的架构含义。
- en: Building with principles
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建筑原则
- en: The field of architecture is vast but no matter where you look, like any mature
    discipline, there are always consistent principles that are presented. The good
    news is that some of them are actually the same as some of the principles we met
    when discussing good Python programming in *Chapter 4*, *Packaging Up*. In this
    section, we will discuss some of these and how they can be used for architecting
    ML systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 架构领域非常广泛，但无论你往哪里看，就像任何成熟的学科一样，总有始终如一的原则被提出。好消息是，其中一些原则实际上与我们讨论第4章“打包”时遇到的原则相同。在本节中，我们将讨论这些原则以及它们如何用于架构ML系统。
- en: '**Separation of Concerns** has already been mentioned in this book as a good
    way to ensure that software components inside your applications are not unnecessarily
    complex and that your solutions are extensible and can be easily interfaced with.
    This principle holds true of systems in their entirety and as such is a good architecture
    principle to bear in mind. In practice, this often manifests in the idea of separate
    “layers” within your applications that have distinct responsibilities. For example,
    let’s look at the architecture shown in *Figure 5.1*. This shows how to use tools
    to create an automated deployment and orchestration process for ML pipelines and
    is taken from the AWS Solutions Library, [https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/](https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/).
    We can see that there are distinct “areas” within the architecture corresponding
    to provisioning, pipeline deployment, and pipeline serving. These blocks show
    that there are distinct pieces of the solution that have specific functionalities
    and that the interaction between these different pieces is handled by an interface.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**关注点分离**已经在本书中提到，是确保应用程序内部软件组件不必要复杂以及解决方案可扩展且易于接口的一种好方法。这个原则对整个系统都适用，因此是一个值得记住的良好架构原则。在实践中，这通常表现为应用程序内部有不同职责的“层”的概念。例如，让我们看看图5.1所示的架构。这展示了如何使用工具创建自动化部署和编排过程，它来自AWS解决方案库，[https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/](https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/)。我们可以看到，架构中有对应于资源分配、管道部署和管道服务的“区域”。这些块表明解决方案中有具有特定功能的独立部分，而这些不同部分之间的交互由接口处理。'
- en: '![](img/B19525_05_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B19525_05_01.png)'
- en: 'Figure 5.1: An ML workload orchestrator architecture from the AWS Solutions
    Library MLOps Workload Orchestrator.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：AWS解决方案库中的ML工作负载编排器架构。
- en: The **Principle of Least Surprise** is a rule of thumb that essentially captures
    the fact that the first time any reasonably knowledgeable person in your domain,
    such as a developer, tester, or data scientist, encounters your architecture,
    it should not have anything within it that should stand out as unorthodox or surprising.
    This may not always be possible, but it is a good principle to keep in mind as
    it forces you to consider what those who are likely to be working with your architecture
    already know and how you can leverage that to both make a good design and have
    it followed. Using *Figure 5.1* as an example again, the architecture embodies
    the principle very nicely, as the design has clear logic building blocks for provisioning,
    promoting, and running the ML pipelines. At a lower level in the architecture,
    we can see that data is consistently being sourced from S3 buckets, that Lambdas
    are interacting with API gateways, and so on and so forth. This means that ML
    engineers, data scientists, and cloud platform engineers will both understand
    and leverage this architecture well when implementing it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小惊讶原则**是一个经验法则，它本质上捕捉了这样一个事实：当你领域中的任何合理有知识的人，比如开发者、测试员或数据科学家，第一次遇到你的架构时，其中不应该有任何不寻常或令人惊讶的东西。这可能并不总是可能的，但这是一个值得记住的好原则，因为它迫使你考虑那些可能与你架构一起工作的人已经知道什么，以及你如何利用这一点来做出良好的设计并使其得到遵循。再次以*图5.1*为例，架构很好地体现了这一原则，因为设计有清晰的逻辑构建块，用于提供、提升和运行机器学习管道。在架构的较低层面，我们可以看到数据始终来自S3存储桶，Lambda与API网关进行交互，等等。这意味着机器学习工程师、数据科学家和云平台工程师在实施时都会很好地理解和利用这个架构。'
- en: The **Principle of Least Effort** is a bit more subtle than the previous one,
    in that it captures the idea that developers, being human, will follow the path
    of least resistance and not create more work unless necessary. I interpret this
    principle as emphasizing the importance of taking the time to consider your architecture
    thoughtfully and building it with care, as it could be used for a long time after
    it has been developed, by engineer after engineer!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小努力原则**比前一个原则更微妙，因为它捕捉了这样一个想法：开发者作为人类，会遵循阻力最小的路径，除非必要，否则不会创造更多的工作。我解读这个原则为强调花时间深思熟虑地考虑你的架构并小心构建它的重要性，因为它在开发后可能会被工程师长时间使用！'
- en: So far, we have only discussed high-level architecture principles. Now we will
    look at some design principles that – while they can still be used at the system
    design level – are also very powerful when used at the level of your code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了高级架构原则。现在我们将探讨一些设计原则，虽然它们在系统设计层面仍然可以使用，但在你的代码层面使用时也非常强大。
- en: 'The **SOLID** principles (**Single Responsibility**, **Open/Closed**, **Liskov
    Substitution**, **Interface Segregation**, **Dependency Inversion**) are a set
    that is often applied to the code base but can also be extrapolated up to system
    design and architecture quite nicely. Once we adapt these principles to the architecture
    level, they can be explained in the following way:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**SOLID原则**（**单一职责**、**开放/封闭**、**里氏替换**、**接口隔离**、**依赖倒置**）是一组通常应用于代码库的原则，但也可以很好地扩展到系统设计和架构。一旦我们将这些原则应用到架构层面，它们可以这样解释：'
- en: '**Single Responsibility**: This is very similar, perhaps identical, to the
    idea of separation of concerns. Specifically, this states that if a module only
    has one reason to change at any one time, or it only has one job to do, then this
    makes it more resilient and easier to maintain. If you have one box in your architecture
    diagram that is going to have to do ten different things, then you have violated
    this principle and it means that whenever any one of those processes or interfaces
    has to change, you have to go into that box and poke around, potentially creating
    more issues or drastically increasing the likelihood of downtime.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单一职责**：这与关注点分离的概念非常相似，也许可以说是相同的。具体来说，这表明如果一个模块在任何时候只有一个改变的理由，或者只有一个任务要做，那么这会使它更加健壮且更容易维护。如果你在架构图中有一个盒子需要做十件事，那么你就违反了这个原则，这意味着每当这些流程或接口中的任何一个需要改变时，你都必须进入那个盒子四处摸索，可能会产生更多问题或极大地增加停机时间。'
- en: '**Open/Closed**: This refers to the fact that it is a really good idea to architect
    in a way that components are “open for extension but closed for modification.”
    This also works at the level of the entire design. If you design your system so
    that new functionality can be tagged on and does not require going back and rewiring
    the core, then you will likely build something that will stand the test of time.
    A great example from ML would be that if we try and build our system so that if
    we want to add in new processing pipelines we can just do that, and we don’t have
    to go back into some obscure section of the code and severely modify things.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放/封闭原则**：这指的是以组件“对扩展开放但对修改封闭”的方式进行架构设计是一个非常不错的想法。这在整个设计层面也是适用的。如果你设计你的系统，使得新的功能可以添加而不需要回过头去重新布线核心部分，那么你很可能会构建出能够经受时间考验的东西。在机器学习领域的一个很好的例子是，如果我们尝试构建我们的系统，以便我们想要添加新的处理管道时，我们可以直接这样做，而不必回到代码的某个晦涩部分进行严重修改。'
- en: '**Liskov Substitution**: When the SOLID principles were written, they originally
    referred to object-oriented programming in languages like Java. This principle
    then stated that objects should be able to be replaced by their subtypes and still
    maintain application behavior. At the system level, this now basically states
    that if two components are supposed to have the same interface and contract with
    other components, you can swap them for one another.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**里氏替换原则**：当SOLID原则被编写时，它们最初是指Java等语言中的面向对象编程。这个原则当时指出，对象应该能够被它们的子类型替换，同时仍然保持应用程序的行为。在系统层面，这现在基本上表示，如果两个组件应该具有相同的接口并与其他组件进行交互，你可以互相替换它们。'
- en: '**Interface Segregation**: I interpret this one as “don’t have multiple ways
    for components to talk to one another.” So, in your application, try and ensure
    that the ways of handing off between different pieces of the solution are pretty
    narrow. Another way of phrasing this is that making your interfaces as client
    specific as possible is a good idea.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接口隔离原则**：我理解这个原则为“不要让组件之间有多个交流方式。”所以，在你的应用程序中，尽量确保不同解决方案部分之间的交接方式非常狭窄。另一种表述方式是，尽可能使你的接口针对客户端是好的想法。'
- en: '**Dependency Inversion**: This is very similar to the Liskov Substitution principle
    but is a bit more general. The idea here is that the communications between modules
    or parts of your solution should be taken care of by abstractions and not by a
    concrete, specific implementation. A good example would be that instead of calling
    an ML microservice directly from another process, you instead place the requisite
    job data in a queue, for example, AWS Simple Queue Service, and then the microservice
    picks up the work from the queue. This ensures that the client and the serving
    microservice do not need to know details about each other’s interface, and also
    that the downstream application can be extended with more services reading from
    the queue. This would then also embody the Open/Closed principle, and can be seen
    in the architecture in *Figure 5.1* through the use of the Lambda function calling
    into AWS CloudFormation.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖倒置原则**：这与里氏替换原则非常相似，但更为通用。这里的想法是，模块或你解决方案的部分之间的通信应该由抽象来处理，而不是由具体的、特定的实现来处理。一个很好的例子是，与其直接从另一个进程调用机器学习微服务，你不如将必要的作业数据放入队列中，例如AWS简单队列服务，然后微服务从队列中提取工作。这确保了客户端和提供服务的微服务不需要了解彼此的接口细节，同时也确保下游应用程序可以通过读取队列来扩展更多的服务。这也就体现了开放/封闭原则，并且可以在*图5.1*的架构中通过Lambda函数调用AWS
    CloudFormation来看到这一点。'
- en: A final favorite of mine is the concept of **Bounded Contexts**, where we have
    to seek to ensure that data models, or other important data or metadata, are aligned
    within specific conceptual models and are not a “free-for-all.” This applies particularly
    well to Domain-Driven Design and applies very well to large, complex solutions.
    A great example would be if you have a large organization with multiple business
    units and they want a series of very similar services that run ML on business
    data stored in a database. It would be better for there to be several databases
    hosting the information, one for each business unit, rather than having a shared
    data layer across multiple applications. More concretely, your data model shouldn’t
    contain information specific to the sales and marketing function and the engineering
    function and the human resources function, and so on. Instead, each should have
    their own database with their own models, and there should be explicit contracts
    for joining any information between them later if needed. I believe that this
    idea can still be applied to Data Lakes, which are discussed later in this chapter.
    In this case, the bounded contexts could apply to specific folders within the
    lake, or they could actually refer to the context of entire lakes, each segregated
    into different domains. This is very much the idea behind the so-called Data Mesh.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人的一个最喜欢的概念是**边界上下文**，我们必须努力确保数据模型或其他重要的数据或元数据与特定的概念模型保持一致，而不是“自由放任”。这一点特别适用于领域驱动设计，并且非常适合大型、复杂的解决方案。一个很好的例子是，如果你有一个大型组织，拥有多个业务单元，并且他们希望运行在存储在数据库中的业务数据上的非常相似的服务。最好是有几个数据库来托管信息，每个业务单元一个，而不是在多个应用程序之间共享数据层。更具体地说，你的数据模型不应该包含特定于销售和营销职能、工程职能和人力资源职能等信息。相反，每个职能都应该有自己的数据库和自己的模型，如果需要，应该有明确的合同来连接它们之间的任何信息。我相信这个想法仍然可以应用于后面章节中讨论的数据湖。在这种情况下，边界上下文可以应用于湖中的特定文件夹，或者它们实际上可以指整个湖的上下文，每个都被隔离在不同的领域。这正是所谓的数据网格背后的理念。
- en: We have just mentioned some of the most used ML patterns, so let’s now move
    on to explore this concept in a bit more detail as we look to apply the principles
    we have been discussing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚提到了一些最常用的机器学习模式，现在让我们更详细地探讨这个概念，因为我们正在寻找应用我们一直在讨论的原则。
- en: Exploring some standard ML patterns
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索一些标准的机器学习模式
- en: In this book, we have already mentioned a few times that we should not attempt
    to *reinvent* the wheel and we should reuse, repeat, and recycle what works according
    to the wider software and ML community. This is also true about your deployment
    architectures.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们已经多次提到，我们不应该试图*重新发明轮子*，而应该重用、重复和回收更广泛的软件和机器学习社区中行之有效的方法。这同样适用于你的部署架构。
- en: When we discuss architectures that can be reused for a variety of different
    use cases with similar characteristics, we often refer to these as *patterns*.
    Using standard (or at least well-known) patterns can really help you speed up
    the time to value of your project and help you engineer your ML solution in a
    way that is robust and extensible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论可以用于具有相似特性的各种不同用例的架构时，我们通常将这些称为*模式*。使用标准（或者至少是众所周知的）模式确实可以帮助你加快项目的价值实现时间，并帮助你以稳健和可扩展的方式构建机器学习解决方案。
- en: Given this, we will spend the next few sections summarizing some of the most
    important architectural patterns that have become increasingly successful in the
    ML space over the past few years.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，在接下来的几节中，我们将总结一些在过去几年中在机器学习领域越来越成功的最重要的架构模式。
- en: Swimming in data lakes
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据湖中游泳
- en: The single most important asset for anyone trying to use ML is, of course, the
    data that we can analyze and train our models on. The era of **big data** meant
    that the sheer size and variability in the format of this data became an increasing
    challenge. If you are a large organization (or even not so large), it is not viable
    to store all of the data you will want to use for ML applications in a structured
    relational database. Just the complexity of modeling the data for storage in such
    a format would be very high. So, what can you do?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何试图使用机器学习（ML）的人来说，最重要的资产当然是我们可以分析和训练模型的数据。大数据时代意味着数据的规模和格式多样性成为了一个日益增长的挑战。如果你是一个大型组织（或者甚至不是那么大），将所有你希望用于ML应用的数据存储在结构化关系数据库中是不可行的。仅仅是为了存储这种格式中的数据而建模的复杂性就非常高。那么，你能做什么呢？
- en: Well, this problem was initially tackled with the introduction of **data warehouses**,
    which let you bring all of your relational data storage into one solution and
    create a single point of access. This helps alleviate, to some extent, the problem
    of data volumes, as each database can store relatively small amounts of data even
    if the total is large. These warehouses were designed with the integration of
    multiple data sources in mind. However, they are still relatively restrictive
    as they usually bundle together the infrastructure for compute and storage. This
    means they can’t be scaled very well, and they can be expensive investments that
    create vendor lock-in. Most importantly for ML, data warehouses cannot store raw
    and semi-structured or unstructured data (for example, images). This automatically
    rules out a lot of good ML use cases if warehouses are used as your main data
    store. Now, with tools such as **Apache Spark**, which we’ve already used extensively
    throughout this book, if we have the clusters available, we can feasibly analyze
    and model any size or structure of data. The question then becomes, how should
    we store it?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个问题最初是通过引入**数据仓库**来解决的，它允许你将所有关系型数据存储整合到一个解决方案中，并创建一个单一的访问点。这在一定程度上有助于缓解数据量的问题，因为即使总量很大，每个数据库也能存储相对较小的数据量。这些仓库在设计时考虑了多个数据源的集成。然而，它们仍然相对受限，因为它们通常将计算和存储的基础设施捆绑在一起。这意味着它们很难进行扩展，并且可能成为昂贵的投资，导致供应商锁定。最重要的是，对于机器学习来说，数据仓库无法存储原始的、半结构化或非结构化数据（例如，图像）。如果使用仓库作为主要数据存储，这会自动排除许多好的机器学习用例。现在，有了像**Apache
    Spark**这样的工具，我们在整本书中已经广泛使用，如果我们有可用的集群，我们实际上可以分析和建模任何大小或结构的数据。那么问题就变成了，我们应该如何存储它？
- en: '**Data lakes** are technologies that allow you to store any type of data at
    any scale you feasibly need. There are a variety of providers of data lake solutions,
    including the main public cloud providers, such as **Microsoft Azure**, **Google
    Cloud Platform** (**GCP**), and AWS. Since we have met AWS before, let’s focus
    on that.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据湖**是允许你在任何可操作的规模下存储任何类型数据的技术。有各种各样的数据湖解决方案提供商，包括主要的公共云提供商，如**Microsoft
    Azure**、**Google Cloud Platform**（**GCP**）和AWS。由于我们之前已经接触过AWS，让我们专注于它。'
- en: The main storage solution in AWS is called the **Simple Storage Service**, or
    **S3**. Like all of the core data lake technologies, you can effectively load
    anything into it since it is based on the concept of *object storage*. This means
    that every instance of data you load is treated as its own object with a unique
    identifier and associated metadata.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: AWS中的主要存储解决方案被称为**简单存储服务**，或**S3**。像所有核心数据湖技术一样，你可以有效地将其中的任何内容加载进去，因为它基于**对象存储**的概念。这意味着你加载的每个数据实例都被视为一个具有唯一标识符和相关元数据的独立对象。
- en: It allows your S3 bucket to simultaneously contain photographs, JSON files,
    `.txt` files, Parquet files, and any other number of data formats.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许你的S3存储桶同时包含照片、JSON文件、`.txt`文件、Parquet文件以及其他多种数据格式。
- en: If you work in an organization that does not have a data lake, this does not
    automatically exclude you from doing ML, but it can definitely make it a more
    difficult journey since with a lake you always know how you can store the data
    you need for your problem, no matter the format.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个没有数据湖的组织工作，这并不意味着你无法进行机器学习，但确实可能会使这个过程变得更加困难，因为有了数据湖，你总是知道如何存储你为解决问题所需的数据，无论其格式如何。
- en: Microservices
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务
- en: Your ML project’s code base will start small – just a few lines at first. But
    as your team expends more and more effort in building the solution required, this
    will quickly grow. If your solution has to have a few different capabilities and
    perform some quite distinct actions and you keep all of this in the same code
    base, your solution can become incredibly complex. In fact, software in which
    the components are all tightly coupled and non-separable like this is called **monolithic**,
    as it is akin to single big blocks that can exist independently of other applications.
    This sort of approach may fit the bill for your use case, but as the complexity
    of solutions continues to increase, a much more resilient and extensible design
    pattern is often required.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习项目的代码库最初会很小——最初只有几行。但随着你的团队在构建所需解决方案上投入越来越多的努力，这会迅速增长。如果你的解决方案需要具备几种不同的能力并执行一些相当不同的操作，而你又把所有这些都放在同一个代码库中，你的解决方案可能会变得极其复杂。实际上，这种所有组件都紧密耦合且不可分离的软件被称为**单体**，因为它类似于可以独立于其他应用程序存在的单个大块。这种方法可能适合你的用例，但随着解决方案复杂性的持续增加，通常需要一个更具弹性和可扩展的设计模式。
- en: 'Microservice architectures are those in which the functional components of
    your solution are cleanly separated, potentially in completely different code
    bases or running on completely different infrastructure. For example, if we are
    building a user-facing web application that allows users to browse, select, and
    purchase products, we may have a variety of ML capabilities we wish to deploy
    in quick succession. We may want to recommend new products based on what they
    have just been looking at, we may want to retrieve forecasts of when their recently
    ordered items will arrive, and we may want to highlight some discounts we think
    they will benefit from (based on our analysis of their historic account behavior).
    This would be a very tall order, maybe even impossible, for a monolithic application.
    However, it is something that quite naturally falls into microservice architecture
    like that in *Figure 5.2*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构是指你的解决方案的功能组件被干净地分离，可能完全在不同的代码库中或运行在不同的基础设施上。例如，如果我们正在构建一个面向用户的Web应用程序，允许用户浏览、选择和购买产品，我们可能希望快速连续部署各种机器学习能力。我们可能希望根据他们刚刚查看的内容推荐新产品，我们可能希望检索他们最近订购的项目何时到达的预测，我们可能还希望突出一些我们认为他们将从中获得好处的折扣（基于我们对他们历史账户行为的分析）。这对于一个单体应用程序来说可能是一个非常高的要求，甚至可能是不可能的。然而，这恰好是像*图5.2*中那样的微服务架构所自然适应的：
- en: '![Figure 5.1 – An example of some ML microservices ](img/B19525_05_02.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 一些机器学习微服务的示例](img/B19525_05_02.png)'
- en: 'Figure 5.2: An example of some ML microservices.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：一些机器学习微服务的示例。
- en: The implementation of a microservice architecture can be accomplished using
    a few tools, some of which we will cover in the *Hosting your own microservice
    on AWS* section. The main idea is that you always separate out the elements of
    your solution into their own services that are not tightly coupled together.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构的实现可以使用一些工具来完成，其中一些我们将在*在AWS上托管自己的微服务*部分中介绍。主要思想是始终将你的解决方案的元素分离成它们自己的服务，这些服务不是紧密耦合在一起的。
- en: 'Microservice architectures are particularly good at allowing our development
    teams to achieve the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构特别擅长让我们的开发团队能够实现以下目标：
- en: Independently debug, patch, or deploy individual services rather than tearing
    down the whole system.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立调试、修补或部署单个服务，而不是整个系统。
- en: Avoid a single point of failure.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免单点故障。
- en: Increase maintainability.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高可维护性。
- en: Allow separate services to be owned by distinct teams with clearer responsibilities.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许不同的服务由不同的团队拥有，并有更清晰的责任。
- en: Accelerate the development of complex products.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速复杂产品的开发。
- en: Like every architecture pattern or design style, it is, of course, not a silver
    bullet, but we would do well to remember the microservice architecture when designing
    our next solution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 就像每个架构模式或设计风格一样，它当然不是万能的银弹，但当我们设计下一个解决方案时，记住微服务架构会大有裨益。
- en: Next, we will discuss event-based designs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论基于事件的设计。
- en: Event-based designs
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于事件的设计
- en: You do not always want to operate in scheduled batches. As we have seen, even
    just in the previous section, *Microservices*, not all use cases align with running
    a large batch prediction from a model on a set schedule, storing the results,
    and then retrieving them later. What happens if the data volumes you need are
    not there for a training run? What if no new data to run predictions on has arrived?
    What if other systems could make use of a prediction based on individual data
    points at the earliest time they become available rather than at a specific time
    every day?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您并不总是想以预定批次的方式运行。正如我们之前所看到的，即使是上一个部分，*微服务*，并不是所有用例都与在预定时间表上从模型运行大型批次预测、存储结果然后稍后检索它们相匹配。如果所需的训练运行数据量不存在怎么办？如果没有新的数据用于运行预测怎么办？如果其他系统可以在数据点最早可用时而不是每天特定时间基于单个数据点进行预测，它们能利用预测怎么办？
- en: In an event-based architecture, individual actions produce results that then
    trigger other individual actions in the system, and so on and so forth. This means
    that processes can happen as early as they can and no earlier. It also allows
    for a more dynamic or stochastic data flow, which can be beneficial if other systems
    are not running on scheduled batches either.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件驱动的架构中，单个操作产生结果，然后触发系统中其他单个操作，如此类推。这意味着过程可以在尽可能早的时候发生，而不是更早。这也允许有更动态或随机的数据流，如果其他系统不是在预定批次上运行，这可能是有益的。
- en: Event-based patterns could be mixed with others, for example, microservices
    or batch processing. The benefits still stand, and, in fact, event-based components
    allow for more sophisticated orchestration and management of your solution.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动模式可以与其他模式混合，例如微服务或批量处理。这些好处仍然存在，实际上，事件驱动组件允许更复杂的解决方案编排和管理。
- en: 'There are two types of event-based patterns:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种基于事件的模式：
- en: '**Pub/sub**: In this case, event data is published to a message broker or event
    bus to be consumed by other applications. In one variant of the pub/sub pattern,
    the broker or buses used are organized by some appropriate classification and
    are designated as **topics**. An example of a tool that does this is **Apache
    Kafka**.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布/订阅**：在这种情况下，事件数据被发布到消息代理或事件总线，以便由其他应用程序消费。在发布/订阅模式的一个变体中，使用的代理或总线根据某些适当的分类组织，并指定为**主题**。执行此操作的示例工具是**Apache
    Kafka**。'
- en: '**Event streaming**: Streaming use cases are ones where we want to process
    a continuous flow of data in something very close to real time. We can think of
    this as working with data as it *moves through* the system. This means it is not
    persisted *at rest* in a database but processed as it is created or received by
    the streaming solution. An example tool to use for event streaming applications
    is **Apache Storm**.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件流**：流用例是我们希望在非常接近实时的情况下处理连续数据流的情况。我们可以将其视为在数据*通过*系统时处理数据。这意味着数据不是在数据库中静态持久化，而是在创建或接收时由流解决方案处理。用于事件流应用的示例工具是**Apache
    Storm**。'
- en: '*Figure 5.3* shows an example event-based architecture applied to the case
    of **IoT** and mobile devices that have their data passed into classification
    and anomaly detection algorithms:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.3* 展示了一个应用于**物联网**和移动设备的事件驱动架构示例，这些设备的数据被传递到分类和异常检测算法中：'
- en: '![Figure 5.2 – A basic event-based architecture where a stream of data is accessed
    by different services via a broker ](img/B19525_05_03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 一个基本的事件驱动架构，其中数据流通过代理被不同的服务访问](img/B19525_05_03.png)'
- en: 'Figure 5.3: A basic event-based high-level design where a stream of data is
    accessed by different services via a broker.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：一个基本的事件驱动高级设计，其中数据流通过代理被不同的服务访问。
- en: The next section will touch on designs where we do the opposite of processing
    one data point at a time and instead work with large chunks or batches at any
    one time.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将涉及设计，其中我们做的是一次处理一个数据点，而不是同时处理大量数据或批次。
- en: Batching
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量处理
- en: Batches of work may not sound like the most sophisticated concept, but it is
    one of the most common pattern flavors out there in the world of ML.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 工作批次可能听起来不是最复杂的概念，但在机器学习的世界中，它是最常见的模式之一。
- en: If the data you require for prediction comes in at regular time intervals in
    batches, it can be efficient to schedule your prediction runs with a similar cadence.
    This type of pattern can also be useful if you do not have to create a low-latency
    solution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您用于预测的数据以固定的时间间隔以批次形式到来，那么安排您的预测运行以类似的节奏可能是高效的。如果不需要创建低延迟解决方案，这种模式也可能很有用。
- en: 'This concept can also be made to run quite efficiently for a few reasons:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于几个原因，这个概念也可以运行得相当高效：
- en: Running in scheduled batches means that we know exactly when we will need compute
    resources, so we can plan accordingly. For example, we may be able to shut down
    our clusters for most of the day or repurpose them for other activities.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预定批次中运行意味着我们知道何时需要计算资源，因此我们可以相应地计划。例如，我们可能能够关闭我们的集群大部分时间，或者将它们用于其他活动。
- en: Batches allow for the use of larger numbers of data points at runtime, so you
    can run things such as anomaly detection or clustering at the batch level if desired.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次允许在运行时使用更多的数据点，因此如果你希望的话，可以在批次级别运行异常检测或聚类等操作。
- en: The size of your batches of data can often be chosen to optimize some criterion.
    For example, using large batches and running parallelized logic and algorithms
    on it could be more efficient.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的数据批次的大小通常可以选择以优化某些标准。例如，使用大型批次并在其上运行并行化的逻辑和算法可能更有效。
- en: Software solutions where ML algorithms are run in batches often look very similar
    to classic **Extract**, **Transform**, **Load** (**ETL**) systems. These are systems
    where data is extracted from a source or sources, before being processed on route
    to a target system where it is then uploaded. In the case of an ML solution, the
    processing is not standard data transformation such as joins and filters but is
    instead the application of feature engineering and ML algorithm pipelines. This
    is why, in this book, we will term these designs **Extract, Transform, Machine
    Learning** (**ETML**) patterns. ETML will be discussed more in *Chapter 9*, *Building
    an Extract, Transform, Machine Learning Use Case*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在批次中运行ML算法的软件解决方案通常看起来与经典的**提取**、**转换**、**加载**（**ETL**）系统非常相似。这些系统是从源或多个源提取数据，然后在路由到目标系统之前进行处理，然后上传。在ML解决方案的情况下，处理不是标准的数据转换，如连接和筛选，而是应用特征工程和ML算法管道。这就是为什么在这本书中，我们将这些设计称为**提取、转换、机器学习**（**ETML**）模式。ETML将在第9章*构建提取、转换、机器学习用例*中进一步讨论。
- en: We will now discuss a key piece of technology that is critical to making modern
    architectures applicable to a wide range of platforms – containers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论一项关键技术，这对于使现代架构适用于广泛的平台至关重要——容器。
- en: Containerizing
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器化
- en: If you develop software that you want to deploy somewhere, which is the core
    aim of an ML engineer, then you have to be very aware of the environmental requirements
    of your code, and how different environments might affect the ability of your
    solution to run. This is particularly important for Python, which does not have
    a core capability for exporting programs as standalone executables (although there
    are options for doing this). This means that Python code needs a Python interpreter
    to run and needs to exist in a general Python environment where the relevant libraries
    and supporting packages have been installed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你开发软件并将其部署到某个地方，这是ML工程师的核心目标，那么你必须非常了解你的代码的环境要求，以及不同的环境可能会如何影响你的解决方案的运行能力。这对于Python尤其重要，Python没有将程序作为独立可执行文件导出的核心功能（尽管有选项可以这样做）。这意味着Python代码需要Python解释器来运行，并且需要存在于一个通用的Python环境中，其中已安装相关的库和支持包。
- en: 'A great way to avoid headaches from this point of view is to ask the question:
    *Why can’t I just put everything I need into something that is relatively isolated
    from the host environment, which I can ship and then run as a standalone application
    or program?* The answer to this question is that you can and that you do this
    through **containerization**. This is a process whereby an application and its
    dependencies can be packaged together in a standalone unit that can effectively
    run on any computing platform.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 避免从这个角度来看头痛的一个好方法是问自己：*为什么我不能把所有需要的东西都放入一个相对隔离主机环境的东西中，然后我可以将其发送并作为一个独立的应用程序或程序运行？*
    这个问题的答案是你可以，而且你通过**容器化**来实现这一点。这是一个过程，其中应用程序及其依赖项可以打包在一个独立的单元中，该单元可以在任何计算平台上有效地运行。
- en: 'The most popular container technology is **Docker**, which is open-source and
    very easy to use. Let’s learn about it by using it to containerize a simple **Flask**
    web application that could act as an interface to a forecasting model like that
    created in the *Example 2*: *Forecasting API* section in *Chapter 1*, *Introduction
    to ML Engineering*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '最受欢迎的容器技术是**Docker**，它是开源的，非常易于使用。让我们通过使用它来容器化一个简单的**Flask**网络应用程序来了解它，这个应用程序可以作为类似在*示例2*：*第1章，机器学习工程简介*中的*预测API*部分的预测模型的接口。 '
- en: The next few sections will use a similar simple Flask application that has a
    forecast serving endpoint. As a proxy for a full ML model, we will first work
    with a skeleton application that simply returns a short list of random numbers
    when requested for a forecast. The detailed code for the application can be found
    in this book’s GitHub repo at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将使用一个类似的简单Flask应用程序，它有一个提供预测的端点。作为一个完整ML模型的代理，我们首先将与一个简单的骨架应用程序一起工作，该应用程序在请求预测时简单地返回一个随机数字的短列表。应用程序的详细代码可以在本书的GitHub仓库中找到，地址为[https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service)。
- en: The web application creates a basic app where you can supply a store ID and
    forecast a start date for the system and it will return the dummy forecast. To
    get this, you hit the `/forecast` endpoint.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 网络应用程序创建了一个基本的应用程序，你可以提供存储ID并预测系统的开始日期，然后它会返回虚拟预测。要获取这个，你需要点击`/forecast`端点。
- en: 'An example is shown in *Figure 5.4*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子在*图5.4*中展示：
- en: '![](img/B19525_05_04.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_05_04.png)'
- en: 'Figure 5.4: The result of querying our skeleton ML microservice.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：查询我们的骨骼ML微服务的结果。
- en: 'Now, we’ll move on to discuss how to containerize this application. First,
    you need to install Docker on your platform by using the documentation at [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论如何容器化这个应用程序。首先，你需要通过使用[https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/)上的文档在你的平台上安装Docker：
- en: 'Once you have Docker installed, you need to tell it how to build the container
    image, which you do by creating a `Dockerfile` in your project. The `Dockerfile`
    specifies all of the build steps in text so that the process of building the image
    is automated and easily configurable. We will now walk through building a simple
    example `Dockerfile`, which will be built on in the next section, *Hosting your
    own microservice on AWS*. First, we need to specify the base image we are working
    from. It usually makes sense to use one of the official Docker images as a base,
    so here we will use the `python:3.10-slim` environment to keep things lean and
    mean. This base image will be used in all commands following the `FROM` keyword,
    which signifies we are entering a build stage. We can actually name this stage
    for later use, calling it `builder` using the `FROM … as` syntax:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你安装了Docker，你需要告诉它如何构建容器镜像，这通过在你的项目中创建一个`Dockerfile`来完成。`Dockerfile`以文本形式指定所有构建步骤，以便构建镜像的过程自动化且易于配置。现在，我们将通过构建一个简单的示例`Dockerfile`来演示，这个示例将在下一节中继续，即*在AWS上托管自己的微服务*。首先，我们需要指定我们工作的基本镜像。通常使用官方Docker镜像作为基础是有意义的，所以我们在这里将使用`python:3.10-slim`环境来保持事情简洁。这个基本镜像将在所有跟随`FROM`关键字的命令中使用，这表示我们正在进入构建阶段。我们实际上可以用`FROM
    … as`语法命名这个阶段，以便以后使用，将其命名为`builder`：
- en: '[PRE1]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we copy all the files we need from the current directory to a directory
    labeled `src` in the build stage and install all of our requirements using our
    `requirements.txt` file (if you want to run this step without specifying any requirements,
    you can just use an empty `requirements.txt` file):'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将从当前目录复制所有需要的文件到构建阶段的`src`目录，并使用我们的`requirements.txt`文件安装所有需求（如果你想在未指定任何需求的情况下运行此步骤，你可以只使用一个空的`requirements.txt`文件）：
- en: '[PRE2]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next stage involves similar steps but is aliased to the word `app` since
    we are now creating our application. Notice the reference to the `builder` stage
    from steps *1* and *2* here:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个阶段涉及类似的步骤，但被别名为单词`app`，因为我们现在正在创建我们的应用程序。注意这里对步骤*1*和*2*中的`builder`阶段的引用：
- en: '[PRE3]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can define or add to environment variables as we are used to in a bash environment:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像在bash环境中一样定义或添加环境变量：
- en: '[PRE4]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since in this example we are going to be running a simple Flask web application,
    we need to tell the system which port to expose:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于在这个例子中我们将运行一个简单的 Flask Web 应用程序，我们需要告诉系统要公开哪个端口：
- en: '[PRE5]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can execute commands during the Docker build using the `CMD` keyword. Here,
    we use this to run `app.py`, which is the main entry point to the Flask app, and
    will start the service we will call via REST API to get ML results later:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在 Docker 构建过程中使用 `CMD` 关键字来执行命令。在这里，我们使用它来运行 `app.py`，这是 Flask 应用程序的主入口点，并且将启动我们稍后通过
    REST API 调用来获取 ML 结果的服务：
- en: '[PRE6]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we can build the image with the `docker build` command. Here, we create
    an image named `basic-ml-microservice` and tag it with the `latest` label:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `docker build` 命令来构建镜像。在这里，我们创建一个名为 `basic-ml-microservice` 的镜像，并使用
    `latest` 标签对其进行标记：
- en: '[PRE7]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To check the build was successful, run the following command in the Terminal:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要检查构建是否成功，请在终端中运行以下命令：
- en: '[PRE8]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see an output like that in *Figure 5.5*:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该会在 *图 5.5* 中看到类似的输出：
- en: '![](img/B19525_05_05.png)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B19525_05_05.png)'
- en: 'Figure 5.5: Output from the docker images command.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.5：`docker images` 命令的输出。
- en: 'Finally, you can run your Docker image with the following command in your Terminal:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以在终端中使用以下命令运行您的 Docker 镜像：
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that you have containerized some basic applications and can run your Docker
    image, we need to answer the question of how we can use this to build an ML solution
    hosted on an appropriate platform. The next section covers how we can do this
    on AWS.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经容器化了一些基本的应用程序并且可以运行您的 Docker 镜像，我们需要回答如何使用它来构建一个托管在适当平台上的 ML 解决方案的问题。下一节将介绍我们如何在
    AWS 上做到这一点。
- en: Hosting your own microservice on AWS
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AWS 上托管自己的微服务
- en: A classic way to surface your ML models is via a lightweight web service hosted
    on a server. This can be a very flexible pattern of deployment.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个将您的 ML 模型公开的经典方式是通过在服务器上托管一个轻量级的 Web 服务。这可以是一个非常灵活的部署模式。
- en: You can run a web service on any server with access to the internet (roughly)
    and, if designed well, it is often easy to add further functionality to your web
    service and expose it via new endpoints.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在任何可以访问互联网（大致上）的服务器上运行 Web 服务，并且如果设计得当，通常很容易向您的 Web 服务添加更多功能，并通过新的端点公开。
- en: In Python, the two most used web frameworks have always been **Django** and
    **Flask**. In this section, we will focus on Flask as it is the simpler of the
    two and has been written about extensively for ML deployments on the web, so you
    will be able to find plenty of material to build on what you learn here.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，最常用的两个 Web 框架一直是 **Django** 和 **Flask**。在本节中，我们将重点关注 Flask，因为它比 Django
    更简单，并且已经广泛地讨论了其在 Web 上的 ML 部署，因此您将能够找到大量材料来构建您在这里学到的内容。
- en: On AWS, one of the simplest ways you can host your Flask web solution is as
    a containerized application on an appropriate platform. We will go through the
    basics of doing this here, but we will not spend time on the detailed aspects
    of maintaining good web security for your service. To fully discuss this may require
    an entire book in itself, and there are excellent, more focused resources elsewhere.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 上，您可以托管 Flask Web 解决方案的最简单方法之一是将它作为一个容器化应用程序在适当平台上运行。我们将在本节中介绍如何做到这一点的基础知识，但我们将不会花费时间在维护良好
    Web 安全性的详细方面。这可能需要一本完整的书来充分讨论，并且在其他地方有很好的、更专注的资源。
- en: We will assume that you have your AWS account set up from *Chapter 2*, *The
    Machine Learning Development Process*. If you do not, then go back and refresh
    yourself on what you need to do.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设您已经从 *第 2 章*，*机器学习开发过程* 中设置了您的 AWS 账户。如果没有，请返回并复习您需要做什么。
- en: We will need the AWS **Command Line Interface** (**CLI**). You can find the
    appropriate commands for installing and configuring the AWS CLI, as well as a
    lot of other useful information, on the AWS CLI documentation pages at [https://docs.aws.amazon.com/cli/index.xhtml](https://docs.aws.amazon.com/cli/index.xhtml).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要 AWS **命令行界面**（**CLI**）。您可以在 AWS CLI 文档页面 [https://docs.aws.amazon.com/cli/index.xhtml](https://docs.aws.amazon.com/cli/index.xhtml)
    上找到安装和配置 AWS CLI 的适当命令，以及大量其他有用的信息。
- en: 'Specifically, configure your Amazon CLI by following the steps in this tutorial:
    [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，请按照本教程中的步骤配置您的 Amazon CLI：[https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml)。
- en: The documentation specifies how to install the CLI for a variety of different
    computer architectures, so follow along for your given platform and then you will
    be ready to have fun with the AWS examples used in the rest of the book!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 文档指定了如何为各种不同的计算机架构安装CLI，所以按照你的平台进行操作，然后你就可以准备好享受本书中使用的AWS示例了！
- en: In the following example, we will use Amazon **Elastic Container Registry**
    (**ECR**) and **Elastic Container Service** (**ECS**) to host a skeleton containerized
    web application. In *Chapter 8*, *Building an Example ML Microservice*, we will
    discuss how to build and scale an ML microservice in more detail and using a lower-level
    implementation based on Kubernetes. These two approaches complement each other
    nicely and will help you widen your ML engineering toolkit.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用Amazon **弹性容器注册库**（**ECR**）和**弹性容器服务**（**ECS**）来托管一个基本的容器化Web应用程序。在*第8章*，*构建示例ML微服务*中，我们将更详细地讨论如何构建和扩展ML微服务，并使用基于Kubernetes的低级别实现。这两种方法相辅相成，将帮助你扩展ML工程工具箱。
- en: 'Deploying our service on ECS will require a few different components, which
    we will walk through in the next few sections:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在ECS上部署我们的服务需要几个不同的组件，我们将在接下来的几节中介绍：
- en: Our container hosted inside a repository on ECR
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在ECR仓库内部托管的容器
- en: A cluster and service created on ECS
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ECS上创建的集群和服务
- en: An application load balancer created via the **Elastic Compute Cloud** (**EC2**)
    service
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**弹性计算云**（**EC2**）服务创建的应用程序负载均衡器
- en: First, let’s tackle pushing the container to ECR.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们解决将容器推送到ECR的问题。
- en: Pushing to ECR
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推送到ECR
- en: 'Let’s look at the following steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下步骤：
- en: 'We have the following Dockerfile defined within the project directory from
    the *Containerizing* section:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在*容器化*部分的项目目录中定义了以下Dockerfile：
- en: '[PRE10]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can then use the AWS `CLI` to create an ECR repository for hosting our container.
    We will call the repository `basic-ml-microservice` and will set the region as
    `eu-west-1`, but this should be changed to what region seems most appropriate
    for your account. The command below will return some metadata about your ECR repository;
    keep this for later steps:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用AWS `CLI`创建一个ECR仓库来托管我们的容器。我们将把这个仓库命名为`basic-ml-microservice`，并将区域设置为`eu-west-1`，但这个应该根据你的账户最合适的区域来更改。下面的命令将返回一些关于你的ECR仓库的元数据；保留这些信息以供后续步骤使用：
- en: '[PRE11]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can then log in to the container registry with the following command in
    the Terminal. Note that the repository URI will have been in the metadata provided
    after running step *2*. You can also retrieve this by running `aws ecr describe-repositories
    --region eu-west-1`:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以在终端中使用以下命令登录到容器注册库。注意，仓库URI将在运行步骤*2*后提供的元数据中。你也可以通过运行`aws ecr describe-repositories
    --region eu-west-1`来检索这个信息：
- en: '[PRE12]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, if we navigate to the directory containing the `Dockerfile` (`app`),
    we can run the following command to build the container:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，如果我们导航到包含`Dockerfile`（`app`）的目录，我们可以运行以下命令来构建容器：
- en: '[PRE13]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The next step tags the image:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步为镜像打标签：
- en: '[PRE14]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then deploy the Docker image we have just built to the container registry
    with the following command:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令将我们刚刚构建的Docker镜像部署到容器注册库：
- en: '[PRE15]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If successful, this last command will have pushed the locally built Docker image
    to your remotely hosted ECR repository. You can confirm this by navigating to
    the AWS management console, going to the ECR service, and selecting the basic-ml-microservice
    repository. You should then see something like what is shown in *Figure 5.6*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，这个最后的命令将把本地构建的Docker镜像推送到你的远程托管ECR仓库。你可以通过导航到AWS管理控制台，进入ECR服务，并选择basic-ml-microservice仓库来确认这一点。你应该会看到类似于*图5.6*中所示的内容。
- en: '![](img/B19525_05_06.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_05_06.png)'
- en: 'Figure 5.6: Succesful push of the locally built Docker image to the ECR repository.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：本地构建的Docker镜像成功推送到ECR仓库。
- en: The steps we have just gone through are actually quite powerful in general,
    as you are now able to build cross-platform Docker images and share them in a
    central repository under your AWS account. You can share Docker containers and
    images via DockerHub as well, [https://hub.docker.com/](https://hub.docker.com/),
    but this gives you more control if you want to do this inside your own organization.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚走过的步骤在一般情况下是非常强大的，因为你现在能够构建跨平台的Docker镜像，并在你的AWS账户下的中央仓库中共享它们。你也可以通过DockerHub共享Docker容器和镜像，[https://hub.docker.com/](https://hub.docker.com/)，但如果你想在你的组织内部做这件事，这会给你更多的控制权。
- en: Now that we have built the container that hosts the Flask app, we will now look
    to deploy this on scalable infrastructure. To do this, in the next section, we
    will set up our cluster on ECS.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了托管 Flask 应用的容器，接下来我们将考虑将其部署到可伸缩的基础设施上。为了做到这一点，在下一节中，我们将在 ECS 上设置我们的集群。
- en: Hosting on ECS
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 ECS 中托管
- en: 'Now, let’s start with the setup! At the time of writing in mid-2023, AWS has
    recently introduced a revamped ECS console that allows for a far smoother setup
    than previously. So, if you read the first edition of this book, you will find
    this a far smoother experience:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始设置！截至 2023 年中旬，AWS 最近推出了一款全新的 ECS 控制台，它允许比之前更平滑的设置。因此，如果您阅读的是这本书的第一版，您会发现这是一个更加流畅的体验：
- en: First, navigate to **ECS** on the AWS Management Console and click **Create
    Cluster**. You will be provided with a form that asks for details about networking,
    infrastucture, monitoring, and the provision of any tags on the resources we are
    about to create. This should look like *Figure 5.7*.![](img/B19525_05_07.png)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到 AWS 管理控制台中的**ECS**，然后点击**创建集群**。您将看到一个表单，要求您提供有关网络、基础设施、监控以及我们即将创建的资源上的任何标签的详细信息。这应该看起来像*图
    5.7*。![](img/B19525_05_07.png)
- en: 'Figure 5.7: Creating a cluster in Elastic Container Service.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.7：在弹性容器服务中创建集群。
- en: First, we can name the cluster `mlewp2-ecs-cluster`, or indeed whatever you
    want! Then when you expand the **Networking** section, you should see that many
    of the **VPC** and subnet details are auto-populated with defaults based on your
    AWS account setup. If you need to set these up, the form points to the relevant
    documentation. See *Figure 5.8* for an example.![](img/B19525_05_08.png)
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们可以将集群命名为 `mlewp2-ecs-cluster`，或者您想要的任何名称！然后当您展开**网络**部分时，您应该会看到许多**VPC**和子网细节都是基于您的
    AWS 账户设置自动填充的默认值。如果您需要设置这些，表单会指向相关的文档。请参见*图 5.8*以获取示例。![](img/B19525_05_08.png)
- en: 'Figure 5.8: Networking configuration for our cluster in AWS ECS.'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8：我们集群在 AWS ECS 中的网络配置。
- en: The **Infrastructure** section contains three options, with the use of **AWS
    Fargate** being the pre-selected default option. We do not need to know the details
    of how Fargate works but suffice it to say that this provides a very high-level
    abstraction for managing container workloads across multiple servers. The introduction
    of Fargate has meant that you do not need to worry about details of the provisioning
    and running of clusters of virtual machines to run your container workloads. According
    to the AWS documentation, the Fargate service is ideal for dynamic bursts of work
    or large workloads with low operational overhead. If you know you are going to
    be running large jobs that have to be price optimized, you can then look to the
    other infra options provided, for example, **EC2 instances**. We will not need
    these for the purposes of this example. *Figure 5.9* shows the **Infrastructure**
    section for reference.![](img/B19525_05_09.png)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基础设施**部分包含三个选项，其中使用**AWS Fargate**是预选的默认选项。我们不需要了解 Fargate 的工作细节，但可以说这为跨多台服务器管理容器工作负载提供了一个非常高级的抽象层。Fargate
    的引入意味着您不需要担心为运行容器工作负载的虚拟机集群的配置和运行细节。根据 AWS 文档，Fargate 服务非常适合动态工作突增或具有低运营开销的大规模工作负载。如果您知道您将要运行需要价格优化的大型作业，那么您可以查看提供的其他基础设施选项，例如**EC2
    实例**。在这个示例中我们不需要这些。*图 5.9*显示了**基础设施**部分以供参考。![](img/B19525_05_09.png)'
- en: 'Figure 5.9: Configuring the infrastructure options in the ECS service.'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.9：在 ECS 服务中配置基础设施选项。
- en: The **Monitoring** and **Tags** sections are relatively self-explanatory and
    allow you to toggle on **container insights** and provide your own string tags
    for the ECS resources that will be created. Let’s leave these as the default for
    now and click **Create** at the bottom of the page. You should then see that the
    cluster was successfully created after a few minutes, as shown in *Figure 5.10*.![](img/B19525_05_10.png)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控**和**标签**部分相对容易理解，允许您开启**容器洞察**并为即将创建的 ECS 资源提供自己的字符串标签。现在我们先保持这些默认设置，然后点击页面底部的**创建**按钮。然后您应该会看到集群在几分钟内成功创建，如图
    5.10 所示。![](img/B19525_05_10.png)'
- en: 'Figure 5.10: The successful creation of the ECS cluster.'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.10：ECS 集群成功创建。
- en: The previous steps were all about setting up the ECS cluster, the infrastructure
    on which our containerized application can run. To actually tell ECS how to run
    the solution, we need to define **tasks**, which are simply processes we wish
    to be executed on the cluster. There is a related concept of **Services** in ECS,
    which refers to a process for managing your tasks, for example, by ensuring a
    certain number of tasks are always running on the cluster. This is useful if you
    have certain uptime requirements for your solution, such as, if it needs to be
    available for requests 24/7\. We can create the task definition in the cluster
    by first navigating to the cluster review page in the AWS management console,
    and then selecting **Task Definitions** on the left-hand side. We will then click
    on **Create New Task Definition**. Follow the steps below to create this task
    definition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的步骤都是关于设置 ECS 集群，这是我们的容器化应用程序可以运行的基础设施。要实际告诉 ECS 如何运行解决方案，我们需要定义**任务**，这些任务简单来说就是希望在集群上执行的过程。在
    ECS 中有一个相关的概念叫做**服务**，它指的是管理你的任务的过程，例如，确保集群上始终运行一定数量的任务。如果你对解决方案有特定的正常运行时间要求，例如，如果它需要全天候可用，那么这很有用。我们可以通过首先在
    AWS 管理控制台中导航到集群审查页面，然后在左侧选择**任务定义**来在集群中创建任务定义。然后我们将点击**创建新任务定义**。按照以下步骤创建此任务定义。
- en: We have to name the task definition family, which is just the collection of
    versions of the task definition. Let’s call ours `basic-ml-microservice-tasks`
    for simplicity. We then need to provide some container details such as the URI
    for the image we want to use. This is the URI for the image we pushed to the ECR
    repository previously, which is formatted something like `<YOUR_AWS_ID>.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest`.
    You can give the container a new name. Here, I have called it **mlmicro**. Finally,
    you need to supply appropriate port mappings to allow the container and the application
    it contains to be accessible to external traffic. I have mapped `port 5000`, which
    you may recall is the port we exposed in the original Dockerfile using the TCP
    protocol. This is all shown in *Figure 5.11*. You can leave the rest of the optional
    settings for this first section as the default just now and click **Next** to
    move on to the next page of settings.![](img/B19525_05_11.png)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须命名任务定义家族，这仅仅是任务定义的版本集合。为了简单起见，让我们将其命名为 `basic-ml-microservice-tasks`。然后我们需要提供一些容器细节，例如我们想要使用的镜像的
    URI。这是我们之前推送到 ECR 仓库的镜像的 URI，格式类似于 `<YOUR_AWS_ID>.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest`。你可以给容器起一个新的名字。在这里，我将其命名为**mlmicro**。最后，你需要提供适当的端口映射，以便容器及其包含的应用程序能够被外部流量访问。我已经映射了`端口
    5000`，你可能记得这是我们在原始 Dockerfile 中使用 TCP 协议暴露的端口。所有这些都在*图 5.11*中显示。你现在可以保留此第一部分的其余可选设置为默认值，然后点击**下一步**进入下一页设置。![img/B19525_05_11.png](img/B19525_05_11.png)
- en: 'Figure 5.11: Defining the container image to use for the task definition in
    ECS.'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.11：定义用于 ECS 任务定义的容器镜像。
- en: The next page in the console asks for information about the environment and
    infrastructure you will be running the solution on. Based on the settings we used
    for the ECS cluster, we will be using Fargate as the infrastructure option, running
    on a **Linux x86_64** environment. The tasks we are running are very small in
    this case (we’re just returning some numbers for demo purposes) so we can keep
    the default options of **1 vCPU** with **3 GB** memory. You can also add container-level
    memory and CPU requirements if necessary, but we can leave this blank for now.
    This is particularly useful if you have a computationally heavy service, or it
    contains an application that is pre-loaded with some large model or configuration
    data. You can see this in *Figure 5.12*.![](img/B19525_05_12.png)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台中的下一页会要求你提供关于你将在其上运行解决方案的环境和基础设施的信息。根据我们为 ECS 集群使用的设置，我们将使用 Fargate 作为基础设施选项，在**Linux
    x86_64**环境中运行。在这种情况下，我们运行的任务非常小（我们只是为了演示目的返回一些数字），因此我们可以保留默认的**1 vCPU**和**3 GB**内存选项。如果你需要，也可以添加容器级别的内存和
    CPU 要求，但现在我们可以留空。这特别有用，如果你有一个计算密集型的服务，或者它包含一个预先加载了一些大型模型或配置数据的应用程序。你可以在*图 5.12*中看到这一点。![img/B19525_05_12.png](img/B19525_05_12.png)
- en: 'Figure 5.12: Configuring our application environment for the AWS ECS task definition
    used for our ML microservice.'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.12：配置用于我们的 ML 微服务 AWS ECS 任务定义的应用程序环境。
- en: Next, IAM roles need to be configured. We will not be calling other AWS services
    from our application, so at this point, we do not need an IAM task role, but you
    can create one if you need this at a later point, for example, if you wish to
    call another data or ML service. For executing the tasks we need an execution
    role, which by default is created for you, so let’s use that. The IAM configuration
    section is shown in *Figure 5.13*.![](img/B19525_05_13.png)
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，需要配置IAM角色。我们不会从我们的应用程序中调用其他AWS服务，因此在此阶段，我们不需要IAM任务角色，但如果您稍后需要此功能，例如，如果您希望调用其他数据或ML服务，您可以创建一个。执行任务我们需要一个执行角色，默认情况下为您创建，所以让我们使用它。IAM配置部分在*图5.13*中显示。![img/B19525_05_13.png](img/B19525_05_13.png)
- en: 'Figure 5.13: The IAM roles defined for use by the AWS ECS task definition.'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.13：为AWS ECS任务定义定义的IAM角色。
- en: The rest of this section contains optional sections for storage, monitoring,
    and tagging. The storage subsection refers to ephemeral storage used to decompress
    and host your Docker container. Again, for larger containers, you may need to
    consider increasing this size from the default 21 GiB. Monitoring can be enabled
    using **Amazon CloudWatch**, which is useful when you need infrastructure monitoring
    as part of your solution, but we will not cover that here and focus more on the
    core deployment. Keep these sections as is for now and click **Next** at the bottom
    of the page.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本节剩余部分包含存储、监控和标记的可选部分。存储子部分指的是用于解压缩和托管您的Docker容器的临时存储。再次提醒，对于更大的容器，您可能需要考虑将此大小从默认的21
    GiB增加。监控可以使用**Amazon CloudWatch**启用，这在您需要将基础设施监控作为解决方案的一部分时很有用，但在此处我们将不涉及此内容，而是更多地关注核心部署。目前请保持这些部分不变，并在页面底部点击**下一步**。
- en: We are almost there. Now we’ll review and create the task definition. If you
    are happy with the selections upon reviewing, then create the task definition
    and you will be taken to a summary page like that shown in *Figure 5.14*.![](img/B19525_05_14.png)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们几乎完成了。现在，我们将审查并创建任务定义。如果您在审查后对选择满意，那么创建任务定义，您将被带到类似于*图5.14*所示的摘要页面。![img/B19525_05_14.png](img/B19525_05_14.png)
- en: 'Figure 5.14: Successful creation of the ML microservice task definition.'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.14：成功创建的ML微服务任务定义。
- en: 'Now, the final step of setting up our ECS-hosted solution is the creation of
    a service. We will now walk through how to do this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，设置我们的ECS托管解决方案的最后一步是创建一个服务。我们将现在说明如何进行此操作：
- en: First, navigate to the task definition we have just created in the previous
    steps and select the **Deploy** button. This will provide a dropdown where you
    can select **Create service**. *Figure 5.15* shows you what this looks like as
    it may be easy to miss.![](img/B19525_05_15.png)
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到之前步骤中创建的任务定义，并选择**部署**按钮。这将提供一个下拉菜单，您可以选择**创建服务**。*图5.15*显示了此操作的外观，因为它可能很容易错过。![img/B19525_05_15.png](img/B19525_05_15.png)
- en: 'Figure 5.15: Selecting the Create service option for the task definition we
    have just created in the previous steps.'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.15：选择创建服务选项，用于之前步骤中创建的任务定义。
- en: You will then be taken to another page where we need to fill in the details
    of the service we wish to create. For **Existing cluster**, select the ECS cluster
    we defined before, which for this example was called **mlewp2-ecs-cluster**. For
    **Compute configuration**, we will just use the **Launch type** option, which
    means we can just allow Fargate to manage the infrastructure requirements. If
    you have multiple infrastructure options that you want to blend together, then
    you can use the **Capacity provider strategy** option. Note that this is more
    advanced and so I encourage you to read more in the AWS documentation about your
    options here if you need to use this route. For reference, my selections are shown
    in *Figure 5.16*.![](img/B19525_05_16.png)
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您将被带到另一个页面，我们需要填写我们希望创建的服务详情。对于**现有集群**，选择之前定义的ECS集群，在这个例子中被称为**mlewp2-ecs-cluster**。对于**计算配置**，我们将仅使用**启动类型**选项，这意味着我们可以仅允许Fargate管理基础设施需求。如果您想将多个基础设施选项混合在一起，则可以使用**容量提供者策略**选项。请注意，这更高级，所以我鼓励您在需要使用此路径时，在AWS文档中了解更多关于您选项的信息。为了参考，我的选择在*图5.16*中显示。![img/B19525_05_16.png](img/B19525_05_16.png)
- en: 'Figure 5.16: AWS ECS selections for the environment that we will run our ECS
    service on. This service will enable the task definition we defined before, and
    therefore our application, to run continuously.'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.16：在我们要运行ECS服务的环境中选择的AWS ECS选项。此服务将启用我们之前定义的任务定义，因此我们的应用程序可以持续运行。
- en: Next is the deployment configuration, which refers to how the service runs in
    terms of the number of replicas and what actions to take upon failures of the
    solution. I have defined the service name simply as **basic-ml-microservice-service**,
    and have used the **Replica** service type, which specifies how many tasks should
    be maintained across the cluster. We can leave this as **1** for now as we only
    have one task in our task definition. This is shown in *Figure 5.17*.![](img/B19525_05_17.png)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是部署配置，它指的是服务在副本数量和解决方案故障时采取的操作方面如何运行。我已经简单地定义服务名称为**basic-ml-microservice-service**，并使用了**Replica**服务类型，该类型指定了应在集群中维护多少个任务。现在我们可以将其保留为**1**，因为我们只有一个任务在我们的任务定义中。这如图5.17所示。![](img/B19525_05_17.png)
- en: 'Figure 5.17: Configuring the AWS ECS service name and type.'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.17：配置AWS ECS服务名称和类型。
- en: The **Deployment options** and **Deployment failure detection** subsections
    will be auto-populated with some defaults. A rolling deployment type refers to
    the replacement of the container with the latest version when that is available.
    The failure detection options ensure that deployments that run into errors fail
    to proceed and that rollbacks to previous versions are enabled. We do not need
    to enable **CloudWatch alarms** at this stage as we have not configured CloudWatch,
    but this could be added in future iterations of your project. See *Figure 5.18*
    for reference.![](img/B19525_05_18.png)
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署选项**和**部署故障检测**子部分将自动填充一些默认值。滚动部署类型指的是当有最新版本可用时，用最新版本替换容器。故障检测选项确保遇到错误的部署无法继续进行，并且可以回滚到之前的版本。在此阶段，我们不需要启用**CloudWatch警报**，因为我们尚未配置CloudWatch，但可以在项目的未来迭代中添加。参见*图5.18*以供参考.![](img/B19525_05_18.png)'
- en: 'Figure 5.18: Deployment and failure detection options for the AWS ECS service
    we are about to deploy.'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.18：即将部署的AWS ECS服务的部署和故障检测选项。
- en: As in the other examples, there is a **Networking** section that should be prepopulated
    with the VPC and subnet information appropriate for your account. As before, you
    can switch these out for specific VPCs and subnets according to your requirements.
    *Figure 5.19* shows what this looks like for reference.![](img/B19525_05_19.png)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如其他示例所示，有一个**网络**部分，应该预先填充适合您账户的VPC和子网信息。与之前一样，您可以根据需要将这些信息切换为特定的VPC和子网。*图5.19*显示了参考示例。![](img/B19525_05_19.png)
- en: 'Figure 5.19: The networking section for the AWS ECS service that we are defining
    for hosting the ML microservice.'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.19：为我们定义的托管ML微服务的AWS ECS服务的网络部分。
- en: The remaining sections are optional and contain configuration elements for load
    balancing, auto-scaling, and tagging. Although we do not necessarily need it for
    such a simple application, we will use this section to create an application load
    balancer, which is one of the options available. An application load balancer
    routes HTTP and HTTPS requests and supports useful capabilities like path-based
    routing and dynamic host port mapping, which allows for multiple tasks from a
    single service to run on the same container. We can name the load balancer `basic-ml-microservice-lb`
    and configure the **listener** for this load balancer to listen on `port 80` with
    the HTTP protocol, as shown in *Figure 5.20*. This listener checks for connection
    requests at the given port and uses the specified protocol so that requests can
    then be routed by the load balancer to the downstream system.![](img/B19525_05_20.png)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余部分是可选的，包含用于负载均衡、自动扩展和标记的配置元素。尽管对于如此简单的应用程序我们可能不需要它，但我们将使用此部分创建一个应用程序负载均衡器，这是可用的选项之一。应用程序负载均衡器路由HTTP和HTTPS请求，并支持诸如基于路径的路由和动态主机端口映射等有用功能，这允许单个服务中的多个任务在同一个容器上运行。我们可以将负载均衡器命名为`basic-ml-microservice-lb`，并配置此负载均衡器的**监听器**以监听`端口80`并使用HTTP协议，如图5.20所示。此监听器检查给定端口的连接请求，并使用指定的协议，以便请求可以被负载均衡器路由到下游系统。![](img/B19525_05_20.png)
- en: 'Figure 5.20: Defining the load balancer name and listener details for the AWS
    ECS service.'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.20：定义AWS ECS服务的负载均衡器名称和监听器详细信息。
- en: Finally, we must specify a target group for the load balancer, which as the
    name suggests is basically the collection of target endpoints for the tasks in
    your service. AWS ECS ensures that this updates as task definitions are updated
    through the lifetime of your service. *Figure 5.21* shows the configurations for
    the target group, which just specifies the HTTP protocol and home path for health
    checks.![](img/B19525_05_21.png)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们必须为负载均衡器指定一个目标组，正如其名称所暗示的，这基本上是您服务中任务的目标端点集合。AWS ECS确保在您服务的整个生命周期中，随着任务定义的更新，此更新也会进行。*图5.21*显示了目标组的配置，它仅指定了用于健康检查的HTTP协议和主页路径。![图5.21](img/B19525_05_21.png)
- en: 'Figure 5.21: Target group definition for the application load balancer.'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.21：应用程序负载均衡器的目标组定义。
- en: After filling in these details, hit the **Create** button. This will then deploy
    your service. If all has gone well, then you should be able to see the service
    in your cluster details on the AWS ECS console page. You can navigate to this
    service and find the load balancer. This will have a **Domain Name System** (**DNS**)
    address that will be the root of the target URL for sending requests. *Figure
    5.22* shows what this page with the DNS looks like. Copy or save this DNS value.![](img/B19525_05_22.png)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在填写这些详细信息后，点击**创建**按钮。然后，您的服务将被部署。如果一切顺利，您应该能够在AWS ECS控制台页面的集群详细信息中看到该服务。您可以导航到该服务并找到负载均衡器。这将有一个**域名系统**（**DNS**）地址，这将作为发送请求的目标URL的根。*图5.22*显示了带有DNS的此页面的外观。复制或保存此DNS值。![图5.22](img/B19525_05_22.png)
- en: 'Figure 5.22: The deployed load balancer for our service with the DNS name in
    the bottom right-hand corner.'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.22：我们服务的已部署负载均衡器，DNS名称位于右下角。
- en: Finally, to test the service, we can run the same request we had for local testing
    in Postman, but now update the URL to contain the load balancer DNS name and the
    port we have specified that the load balancer will receive oncoming traffic with
    . For us, this is port 80\. This is shown with the application response in *Figure
    5.23*.![](img/B19525_05_23.png)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了测试服务，我们可以在Postman中运行与本地测试相同的请求，但现在更新URL以包含负载均衡器DNS名称和我们指定的负载均衡器将接收的端口。对我们来说，这是端口80。这在与应用程序响应的*图5.23*中显示。![图5.23](img/B19525_05_23.png)
- en: 'Figure 5.23: A valid result is returned by our simple forecasting service from
    the hosted application AWS ECS.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.23：我们的简单预测服务从托管应用程序AWS ECS返回有效结果。
- en: And that’s it! We have now successfully built and deployed a simplified forecasting
    service using Flask, Docker, AWS Elastic Container Registry, AWS Elastic Container
    Service, and an application load balancer. These components can all be adapted
    for deploying your future ML microservices.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经成功使用Flask、Docker、AWS弹性容器注册库、AWS弹性容器服务和应用程序负载均衡器构建并部署了一个简化的预测服务。所有这些组件都可以适应部署您未来的ML微服务。
- en: The first half of this chapter has been about architectural and design principles
    that apply at the system and code level, as well as showing you how some of this
    comes together in one mode of deployment that is very common for ML systems, that
    of the ML microservice. Now that we have done this, we will move on to discuss
    some tools and techniques that allow us to build, deploy, and host complex ML
    workflows as pipelines, a concept we briefly introduced earlier in the book. The
    tools and concepts we will cover in the second half of this chapter are crucial
    for any modern ML engineer to have a strong grasp of, as they are starting to
    form the backbone of so many deployed ML systems.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前半部分主要介绍了适用于系统和代码级别的架构和设计原则，以及向您展示如何在一种非常常见的ML系统部署模式中实现这些原则，即ML微服务。现在我们已经完成了这个，我们将继续讨论一些工具和技术，它们允许我们以管道的形式构建、部署和托管复杂的ML工作流程，这是我们之前在书中简要介绍过的概念。本章后半部分我们将涵盖的工具和概念对于任何现代ML工程师来说都是至关重要的，因为它们正在成为许多已部署ML系统的骨架。
- en: The next section will start this discussion with an exploration of how we can
    use Airflow to create and orchestrate flexible, general-purpose, production-ready
    pipelines, before we move on to some tools aimed specifically at advanced ML pipelining
    and orchestration.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将首先通过探讨如何使用Airflow创建和编排灵活、通用、生产就绪的管道来开始这次讨论，然后我们将转向一些专门针对高级ML管道编排的工具。
- en: Building general pipelines with Airflow
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Airflow构建通用管道
- en: In *Chapter 4*, *Packaging Up*, we discussed the benefits of writing our ML
    code as pipelines. We discussed how to implement some basic ML pipelines using
    tools such as `sklearn` and **Spark ML**. The pipelines we were concerned with
    there were very nice ways of streamlining your code and making several processes
    available to use within a single object to simplify an application. However, everything
    we discussed then was very much focused on one Python file and not necessarily
    something we could extend very flexibly outside the confines of the package we
    were using. With the techniques we discussed, for example, it would be very difficult
    to create pipelines where each step was using a different package or even where
    they were entirely different programs. They did not allow us to build much sophistication
    into our data flows or application logic either, as if one of the steps failed,
    the pipeline failed, and that was that.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The tools we are about to discuss take these concepts to the next level. They
    allow you to manage the workflows of your ML solutions so that you can organize,
    coordinate, and orchestrate elements with the appropriate level of complexity
    to get the job done.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Airflow
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Apache Airflow** is the workflow management tool that was initially developed
    by **Airbnb** in the 2010s and has been open-source since its inception. It gives
    data scientists, data engineers, and ML engineers the capability of programmatically
    creating complex pipelines through Python scripts. Airflow’s task management is
    based on the definition and then execution of a **Directed Acyclic Graph** (**DAG**)
    with nodes as the tasks to be run. DAGs are also used in **TensorFlow** and **Spark**,
    so you may have heard of them before.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Airflow contains a variety of default operators to allow you to define DAGs
    that can call and use multiple components as tasks, without caring about the specific
    details of a task. It also provides functionality for scheduling your pipelines.
    As an example, let’s build an Apache Airflow pipeline that will get data, perform
    some feature engineering, train a model, and then persist the model. We won’t
    cover the detailed implementation of each command, but simply show you how your
    ML processes hang together in an Airflow DAG. In *Chapter 9*, *Building an Extract,
    Transform, Machine Learning Use Case*, we will build out a detailed end-to-end
    example discussing these lower-level details. This first example is more concerned
    with understanding the high level of how to write, deploy, and manage your DAGs
    in the cloud:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in a file called `classification_pipeline_dag.py`, we can import the
    relevant Airflow packages and any utility packages we need:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, Airflow allows you to define default arguments that can be referenced
    by all of the following tasks, with the option to overwrite at the same level:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We then have to instantiate our DAG and provide the relevant metadata, including
    our scheduling interval:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, all that is required is to define your tasks within the `DAG` definition.
    First, we define an initial task that gets our dataset. This next piece of code
    assumes there is a Python executable, for example, a function or class method,
    called `get_data` that we can pass to the task. This could have been imported
    from any submodule or package we want. Note that *steps* *3*-*5* assume we are
    inside the code block of the DAG instantiation, so we assume another indent that
    we don’t show here to save space:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then perform a task that takes this data and performs our model training
    steps. This task could, for example, encapsulate one of the pipeline types we
    covered in *Chapter 3*, *From Model to Model Factory*; for example, a Spark ML
    pipeline, **Scikit-Learn** pipeline, or any other ML training pipeline we looked
    at. Again, we assume there is a Python executable called `train_model` that can
    be used in this step:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The final step of this process is a placeholder for taking the resultant trained
    model and persisting it to our storage layer. This means that other services or
    pipelines can use this model for prediction:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we define the running order of the task nodes that we have defined
    in the DAG using the `>>` operator. The tasks above could have been defined in
    any order, but the following syntax stipulates how they must run:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the next sections, we will briefly cover how to set up an Airflow pipeline
    on AWS using the **Managed Workflows for Apache Airflow** (**MWAA**) service.
    The section after will then show how you can use **CI/CD** principles to continuously
    develop and update your Airflow solutions. This will bring together some of the
    setup and work we have been doing in previous chapters of the book.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Airflow on AWS
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS provides a cloud-hosted service called **Managed Workflows for Apache Airflow**
    (**MWAA**) that allows you to deploy and host your Airflow pipelines easily and
    robustly. Here, we will briefly cover how to do this.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Complete the following steps:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Select **Create an environment** on the MWAA landing page. You can find this
    by searching for MWAA in the AWS Management Console.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will then be provided with a screen asking for the details of your new Airflow
    environment. *Figure 5.24* shows the high-level steps that the website takes you
    through:![Figure 5.29 – The high-level steps for setting up an MWAA environment
    and associated managed Airflow runs ](img/B19525_05_24.png)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.24: The high-level steps for setting up an MWAA environment and associated
    managed Airflow runs.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Environment details**, as shown in *Figure 5.25*, is where we specify our
    environment name. Here, we have called it **mlewp2-airflow-dev-env**:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_05_25.png)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.25: Naming your MWAA environment.'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For MWAA to run, it needs to be able to access code defining the DAG and any
    associated requirements or plugin files. The system then asks for an AWS S3 bucket
    where these pieces of code and configuration reside. In this example, we create
    a bucket called `mlewp2-ch5-airflow-example` that will contain these pieces. *Figure
    5.26* shows the creation of the bucket:![](img/B19525_05_26.png)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.26: The successful creation of our AWS S3 bucket for storing our Airflow
    code and supporting configuration elements.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 5.27* shows how we point MWAA to the correct bucket, folders, and plugins
    or requirement files if we have them too:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_05_27.png)'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.27: We reference the bucket we created in the previous step in the
    configuration of the MWAA instance.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We then have to define the configuration of the network that the managed instance
    of Airflow will use, similar to the other AWS examples in this chapter. This can
    get a bit confusing if you are new to networking, so it might be good to read
    around the topics of subnets, IP addresses, and VPCs. Creating a new MWAA VPC
    is the easiest approach for getting started in terms of networking here, but your
    organization will have networking specialists who can help you use the appropriate
    settings for your situation. We will go with this simplest route and click **Create
    MWAA VPC**, which opens a new window where we can quickly spin up a new VPC and
    network setup based on a standard stack definition provided by AWS. You will be
    asked for a stack name. I have called mine `MLEWP-2-MWAA-VPC`. The networking
    information will be populated with something like that shown in *Figure 5.28*:![](img/B19525_05_28.png)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.28: An example stack template for creating your new VPC.'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are then taken to a page where we are asked for more details on networking.
    We can select **Public network (No additional setup)** for this example as we
    will not be too concerned with creating an organizationally aligned security model.
    For deployments in an organization, work with your security team to understand
    what additional security you need to put in place. We can also select **Create
    new security group**. This is shown in *Figure 5.29*.![](img/B19525_05_29.png)
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.29: Finalizing the networking for our MWAA setup.'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we have to define the **Environment class** that we want to spin up. Currently,
    there are three options. Here, we’ll use the smallest, but you can choose the
    environment that best suits your needs (always ask the billpayer’s permission!).
    *Figure 5.30* shows that we can select the **mw1.small** environment class with
    a min to max worker count of 1-10\. MWAA does allow you to change the environment
    class after instantiating if you need to, so it can often be better to start small
    and scale up as needed from a cost point of view. You will also be asked about
    the number of schedulers you want for the environment. Let’s leave this as the
    default, **2**, for now, but you can go up to 5.![](img/B19525_05_30.png)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.30: Selecting an environment class and worker sizes.'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, if desired, we confirm some optional configuration parameters (or leave
    these blank, as done here) and confirm that we are happy for AWS to create and
    use a new execution role. We can also just proceed with the default monitoring
    settings. *Figure 5.31* shows an example of this (and don’t worry, the security
    group will have long been deleted by the time you are reading this page!):![](img/B19525_05_31.png)
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.31: The creation of the execution role used by AWS for the MWAA environment.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next page will supply you with a final summary before allowing you to create
    your MWAA environment. Once you do this, you will be able to see your newly created
    environment in the MWAA service, as in *Figure 5.32*. This process can take some
    time, and for this example it took around 30 minutes:![](img/B19525_05_32.png)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.32: Our newly minted MWAA environment.'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have this MWAA environment and you have supplied your DAG to the
    S3 bucket that it points to, you can open the Airflow UI and see the scheduled
    jobs defined by your DAG. You have now deployed a basic running service that we
    can build upon in later work.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will want to see the DAGs in the Airflow UI so that we can orchestrate
    and monitor the jobs. To do this, you may need to configure access for your own
    account to the MWAA UI using the details outlined on the AWS documentation pages.
    As a quick summary, you need to go to the IAM service on AWS. You will need to
    be logged in as a root user, and then create a new policy title, **AmazonMWAAWebServerAccess**.
    Give this policy the following JSON body:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: For this definition, the Airflow role refers to one of the five roles of **Admin**,
    **Op**, **Viewer**, **User**, or **Public**, as defined in the Airflow documentation
    at [https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml).
    I have used the Admin role for this example. If you add this policy to the permissions
    of your account, you should be able to access the Airflow UI by clicking the **Open
    Airflow UI** button in the MWAA service. You will then be directed to the Airflow
    UI, as shown in *Figure 5.33*.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_33.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: The Airflow UI accessed via the AWS MWAA service. This view shows
    the classification DAG that we wrote earlier in the example.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: The Airflow UI allows you to trigger DAG runs, manage the jobs that you have
    scheduled, and monitor and troubleshoot your pipelines. As an example, upon a
    successful run, you can see summary information for the runs, as shown in *Figure
    5.34*, and can use the different views to understand the time taken for each of
    the pipeline steps and diagnose where any issues have arisen if there are errors
    raised.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_34.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: Example run summary for our simple classification DAG in the Airflow
    UI.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline we have built and run in this example is obviously very simple,
    with only core Python functionality being used. If you want to leverage other
    AWS services, for example, by submitting a Spark job to an EMR cluster, then you
    will need to configure further access policies like the one we did above for the
    UI access. This is covered in the MWAA documentation.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Once you have created this MWAA environment, you cannot pause it, as it costs
    a small amount to run per hour (around 0.5 USD per hour for the environment configuration
    above). MWAA does not currently contain a feature for pausing and resuming an
    environment, so you will have to delete the environment and re-instantiate a new
    one with the same configuration when required. This can be automated using tools
    such as **Terraform** or **AWS** **CloudFormation**, which we will not cover here.
    So, a word of warning – *DO NOT ACCIDENTALLY LEAVE YOUR ENVIRONMENT RUNNING*.
    For example, definitely do not leave it running for a week, like I may or may
    not have done.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting CI/CD for Airflow
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced the basics of CI/CD in *Chapter 2*, *The Machine Learning Development
    Process*, and discussed how this can be achieved by using **GitHub Actions**.
    We will now take this a step further and start to set up CI/CD pipelines that
    deploy code to the cloud.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: First, we will start with an important example where we will push some code
    to an AWS S3 bucket. This can be done by creating a `.yml` file in your GitHub
    repo under your `.github./workflows` directory called `aws-s3-deploy.yml`. This
    will be the nucleus around which we will form our CI/CD pipeline.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'The .`yml` file, in our case, will upload the Airflow DAG and contain the following
    pieces:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'We name the process using the syntax for `name` and express that we want the
    deployment process to be triggered on a push to the main branch or a pull request
    to the main branch:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We then define the jobs we want to occur during the deployment process. In
    this case, we want to upload our DAG files to an S3 bucket we have already created,
    and we want to use the appropriate AWS credentials we have configured in our GitHub
    secrets store:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, as part of the job, we run the step that copies the relevant files to
    our specified AWS S3 bucket. In this case, we are also specifying some details
    about how to make the copy using the AWS CLI. Specifically, here we want to copy
    over all the Python files to the `dags` folder of the repo:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once we perform a `git push` command with updated code, this will then execute
    the action and push the `dag` Python code to the specified S3 bucket. In the GitHub
    UI, you will be able to see something like *Figure 5.35* on a successful run:![Figure
    5.38 – A successful CI/CD process run via GitHub Actions and using the AWS CLI
    ](img/B19525_05_35.png)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.35: A successful CI/CD process run via GitHub Actions and using the
    AWS CLI.'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This process then allows you to successfully push new updates to your Airflow
    service into AWS to be run by your MWAA instance. This is real CI/CD and allows
    you to continually update the service you are providing without downtime.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Building advanced ML pipelines
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed in this chapter how **SciKit-learn** and **Spark ML**
    provide mechanisms for creating ML pipelines. You can think of these as the basic
    way to do this and to get started. There are a series of tools now available,
    both open-source and enterprise, that take this concept to the next level.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: For awareness, the three main public cloud providers have tools in this area
    you may want to be aware of and try out. **Amazon SageMaker** is one of the giants
    of this space and contains within it a large ecosystem of tools and capabilities
    to help take your ML models into production. This book could have been entirely
    about Amazon SageMaker, but since that was done elsewhere, in *Learn Amazon SageMaker*,
    [https://tinyurl.com/mr48rsxp](https://tinyurl.com/mr48rsxp), we will leave the
    details for the reader to discover. The key thing you need to know is that this
    is AWS’s managed service for building up ML pipelines, as well as monitoring,
    model registry, and a series of other capabilities in a way that lets you develop
    and promote your models all the way through the ML lifecycle. **Google Vertex
    AI** is the Google Cloud Platform ML pipelining, development, and deployment tool.
    It brings tons of functionality under one UI and API, like Sagemaker, but seems
    to have less flexibility on the types of models you can train. **Azure ML** is
    the Microsoft cloud provider’s offering.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: These are all enterprise-grade solutions that you can try for free, but you
    should be prepared to have your credit card ready when things scale up. The solutions
    above are also naturally tied into specific cloud providers and therefore can
    create “vendor lock-in,” where it becomes difficult to switch later. Thankfully,
    there are solutions that help with this and allow ML engineers to work with a
    less complex setup and then migrate to more complex infrastructure and environments
    later. The first one of these that we will discuss is **ZenML**.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Finding your ZenML
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ZenML** is a completely open-source framework that helps you write ML pipelines
    in a way that is totally abstracted from the underlying infrastructure. This means
    that your local development environment and your eventual production environment
    can be very different, and can be changed through changes in configuration without
    altering the core of your pipelines. This is a very powerful idea and is one of
    ZenML’s key strengths.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'ZenML has some core concepts that you need to understand in order to get the
    best out of the tool:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: As you might expect given the discussion in the rest of this
    chapter, these are the definitions of the steps in the ML workflow. Pipelines
    consist of “steps” chained together in a specified order.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacks**: Configurations specifying the environment and infrastructure that
    the pipeline is to run on.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orchestrator**: Within the stack definition, there are two key components,
    the first of which is an orchestrator. Its job is to coordinate the steps in the
    pipeline that are executed on the infrastructure. This could be the default orchestrator
    that comes with the distribution or it could be something like Airflow or the
    Kubeflow orchestrator. Airflow is described in the *Building general pipelines
    with Airflow* section in this chapter and Kubeflow is covered in the *Going with
    the Kubeflow* section.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact store**: This is the stack component responsible for data and metadata
    storage. ZenML has a series of different compatible artifact stores out of the
    box, specifically AWS S3, Azure Blob Storage, and Google Cloud Storage. The assumption
    here is that the artifact store is really just a storage layer, and nothing too
    complex on top of that.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, so straightforward. Let’s get on and start setting ZenML up. You can
    install it with:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will also want to use the React dashboard that comes with ZenML, but to
    run this locally you also need to install a different repository:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'ZenML also comes with a series of existing templates you can leverage, which
    you can install with:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can then start working with a template by running:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This will then start a terminal-based wizard to help you generate the ZenML
    template. See *Figure 5.36*.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_36.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.36: The ZenML template wizard.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Hit *Enter*; then you will be asked a series of questions to help configure
    the template. Some are shown with their answers in *Figure 5.37*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_37.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.37: Providing responses for the ZenML template definition.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The next series of questions start to get very interesting as we are asked about
    the details of the information we wish to be logged and made visible in the CLI,
    as well as selecting the dataset and model type. Here we will work with the `Wine`
    dataset, again using a `RandomForestClassifier`, as can be seen in *Figure 5.38*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_38.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.38: Selecting a model for the ZenML template instantiation.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: ZenML will then start initializing the template for you. You can see that this
    process generates a lot of new files to use, as shown in *Figure 5.39*.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_39.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: File and folder structure generated after using the ZenML template
    generation wizard.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start to explore some of these elements for the ZenML solution. First,
    let’s look at `pipelines/model_training.py`. This is a short script that is there
    to give you a starting point. Omitting the comments in the file, we have the following
    code present:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can already start to appreciate some of the features that are available in
    ZenML and how it works. First, we see that the use of the `@pipeline` decorator
    signals that the function following will contain the main pipeline logic. We can
    also see that the pipeline is actually written in pure Python syntax; all you
    need is the decorator to make it “Zen.” This is a very powerful feature of ZenML
    as it provides you the flexibility to work as you want but still leverage the
    downstream abstraction we will see soon for deployment targets. The steps inside
    the pipeline are just dummy function calls created when the template was initialized
    to help guide you in what you should develop.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the pipeline steps, which have been defined in the `steps/data_loaders.py`
    and `steps/model_trainers.py` files. In our discussions of these modules, we will
    not discuss the helper classes and utility functions used; these are left for
    the reader to play around with. Instead, we will focus on the pieces that show
    the most important ZenML functionality. Before we do that, let us briefly discuss
    some important ZenML modules that are imports at the top of the module:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first import brings in `StrEnum` from the `enums` module of ZenML. This
    is a collection of Python enumerations that have been defined to help with specific
    elements of building ZenML workflows.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that a Python enumeration (or `enum`) is a collection of members with
    unique values that can be iterated over to return the values in their order of
    definition. You can think of these as somewhere between a class and a dictionary.
    First, in the `data_loaders.py` module, we can see that the first step wraps simple
    logic for pulling in different datasets from `scikit-learn`, depending on the
    parameters supplied. This is a very basic example but can be updated to incorporate
    much more sophisticated behavior like calling out to databases or pulling from
    cloud-hosted object storage. The method looks like the following:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note that the output of this function is a pandas DataFrame, and in the language
    of ZenML this is an artifact. The next important step given is data processing.
    The example given in the template looks like the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can see that, here, the processing is relatively standard and will drop `NULL`
    values in the dataset, remove columns we have labeled in the `DataProcessingStepParameters`
    classes (not shown here), and apply some normalization using standard scaling
    – the steps given are in fact identical to applying the `sklearn.preprocessing.StandardScaler`
    method.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'The final method in the data loaders module performs train/test splitting of
    the data, using methods we have already seen in this book:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, moving back into the `steps` folder, we can see that there is also a module
    entitled `model_trainers.py`. At the top of this folder are some more important
    imports we should understand before we proceed:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In particular, we can see that ZenML provides a wrapper to the Python logging
    library and that there are two modules being used here, called `artifacts` and
    `materializers`. These are defined within the template repo and show how you can
    create custom code to work with the artifact store. Specifically, in the `artifacts/model_metadata.py`
    module, there is a class that allows you to store model metadata in a format of
    your choosing for later serialization and deserialization. Once again, all docstrings
    and most imports are omitted for brevity:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In ZenML, materializers are the objects that contain the logic for the serialization
    and deserialization of the artifacts. They define how your pipelines interact
    with the artifact store. When defining materializers, you can create custom code
    but you have to inherit from the `BaseMaterializer` class in order to ensure that
    ZenML knows how to persist and read in data between steps and at the beginning
    and end of pipelines. This is shown below in important code from `materializers/model_metadata.py`:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now that we have discussed all the key pieces of the ZenML template, we want
    to run the pipeline. This is done via runner `run.py` at the utmost level of the
    repository. You can then run the pipeline with:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After the pipeline successfully runs (you will see a series of outputs in the
    terminal), you can run the following command to spin up a locally hosted ZenML
    dashboard:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, if you navigate to the URL that is returned as output, usually something
    like `http://127.0.0.1:8237/login`, you will see a home screen like that shown
    in *Figure 5.40*.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_40.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.40: The ZenML UI login page.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: In the output that gave you the URL is also a default username and password,
    conveniently **default** and a blank. Fill these in and you will see the home
    page shown in *Figure 5.41*.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_41.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.41: The ZenML UI home page.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: If you then click through into the **Pipelines** section on the left and then
    click the pipeline created by your first run, you will be able to see all of the
    times that it has been run since then. This view is shown in *Figure 5.42*.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_42.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.42: The pipelines view in the ZenML UI.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: You can then also get really detailed information about the specifics of each
    run by clicking through. This gives you information like a graphical representation
    of the pipeline as a DAG at the time of the run. See *Figure 5.43*.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_43.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.43: An example DAG for a ZenML pipeline shown in the UI.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click through on the pipeline name in any of these views, you can also
    retrieve the configuration of the run at the time of its execution in YAML format,
    which you can download and then use in subsequent pipeline runs:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_44.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.44: An example YAML configuration for a ZenML pipeline run, shown
    in the ZenML UI.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: This has only begun to scratch the surface of what is possible with ZenML, but
    hopefully, you can already see how it is a very flexible way to define and execute
    your ML pipelines. This becomes even more powerful when you leverage its ability
    to deploy the same pipeline across different stacks and different artifact stores.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss another pipelining tool that focuses on
    creating cross-platform compatibility and standardization for your ML pipelines,
    **Kubeflow**.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Going with the Kubeflow
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kubeflow** is an open-source solution aimed at providing portable methods
    for building end-to-end ML systems. This tool has a particular focus on giving
    developers the ability to quickly create pipelines for data processing, ML model
    training, prediction, and monitoring that are platform agnostic. It does all this
    by leveraging Kubernetes, allowing you to develop your solution on very different
    environments from where you eventually deploy. Kubeflow is agnostic about the
    particular programming and ML frameworks you use, so you can leverage everything
    you like out there in the open-source community but still stitch it together in
    a way you can trust.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: The Kubeflow documentation provides a great wealth of detail on the architecture
    and design principles behind the tool at [https://www.kubeflow.org/docs/](https://www.kubeflow.org/docs/).
    We will focus instead on understanding the most salient points and getting started
    with some practical examples. This will allow you to compare to the other tools
    we have discussed in this chapter and make your own decisions around which to
    take forward in future projects.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubeflow is a platform that consists of multiple modular components, each one
    playing a role in the ML development lifecycle. Specifically, there are:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook web app and controller for exploratory data analysis and
    initial modeling.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training operators like PyTorch, TFJob, and XGBoost operators, among others,
    to build a variety of models.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning and neural network architecture search capabilities using
    Katib.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark operators for data transformation, including an option for AWS EMR clusters.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashboard for interfacing with your Kubernetes cluster and for managing your
    Kubeflow workloads.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubeflow Pipelines: its own platform for building, running, and managing end-to-end
    ML workflows. This includes an orchestration engine for workflows with multiple
    steps and an SDK for working with your pipelines. You can install Kubeflow Pipelines
    as a standalone platform.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The installation steps for getting Kubeflow up and running can be quite involved
    and so it is best to look at the official documentation and run the appropriate
    steps for your platform and needs. We will proceed via the following steps:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Kind, a tool that facilitates easy building and running of local Kubernetes
    clusters. On Linux, this is done with:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And on MacOS this is done by:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Install the Kubernetes command-line tool `kubectl`, which allows you to interact
    with your cluster. On Linux, this is done with:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Or on MacOS:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To check this has worked, you can run the following command in the terminal:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'And you should receive an output like this:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Use Kind to create your local cluster. As the default, the name of the cluster
    will be `kind`, but you can provide your own name as a flag:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You will then see output that is something like this:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You then have to deploy Kubeflow pipelines to the cluster. The commands for
    doing this have been brought into a script called `deploy_kubeflow_pipelines.zsh`
    in the book’s GitHub repository and it contains the following code (the `PIPELINE_VERSION`
    number can be updated as needed to match your installation):'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After running these commands, you can verify that the installation was a success
    through port forwarding and opening the Kubeflow Pipelines UI at `http://localhost:8080/:`
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This should then give you a landing page like the one shown in *Figure 5.45*.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_45.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.45: The Kubeflow UI landing page.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have initiated port-forwarding with the previous command, you
    will use this to allow the Kubeflow Pipelines SDK to talk to the cluster via the
    following Python code (note that you cannot do this until you have installed the
    Kubeflow Pipelines SDK, which is covered in the next step):'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To install the Kubeflow Pipelines SDK, run:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To check that everything is in order, you can run this command:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Which gives output that should be similar to:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: And that’s it! We are now ready to start building some Kubeflow pipelines. Let’s
    get started with a basic example.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start building out some basic pipelines using the SDK and then we
    can deploy them to our cluster. Let’s assume for the next few steps we are working
    in a file called `pipeline_basic.py`:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import what is known as the KFP **Domain-Specific Language** (**DSL**),
    which is a set of Python packages with various utilities for defining KFP steps.
    We also import the client package for interacting with the cluster. We’ll also
    import several DSL sub-modules that we will use later. An important point to note
    here is that some functionality we will leverage is in fact contained in the `V2`
    of the Kubeflow pipelines SDK and so we will need to import some of those specific
    modules as well:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The next step is to define the steps in the pipeline. These are called “components”
    and are functions wrapped with `dsl` decorators. In this first step, we retrieve
    the Iris dataset and write it to CSV. In the first line, we will use the `dsl`
    decorator and also define what packages need to be installed in the container
    that will run that step:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now that we have retrieved a dataset, and remembering what we learned in *Chapter
    3*, *From Model to Model Factory*, we want to feature engineer this data. So,
    we will normalize the data in another component. Most of the code should be self-explanatory,
    but note that we have had to add the `scikit-learn` dependency in the `packages_to_install`
    keyword argument and that we have again had to write the result of the component
    out to a CSV file:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We will now train a K-nearest neighbors classifier on the data. Instead of
    outputting a dataset in this component, we will output the trained model artifact,
    a `.pkl` file:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We now have all the components for the work we want to do, so now we can finally
    bring it together into a Kubeflow pipeline. To do this, we use the `@dsl.pipeline`
    decorator and as an argument to that decorator, we provide the name of the pipeline:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The final stage is to submit the pipeline to run. This is done by instantiating
    a Kubeflow Pipelines client class and feeding in the appropriate arguments. `<KFP_UI_URL`>
    is the URL for the host of your instance of Kubeflow Pipelines – in this case,
    the one that we got from performing port-forwarding earlier. It is also important
    to note that since we are using several features from the `V2` Kubeflow Pipelines
    API, we should pass in the `kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE` flag
    for the mode argument:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'To build and deploy this pipeline and run it, you can then execute:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'After running this last step, you will see the URL of the run is printed to
    the terminal, and should look something like this:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If you navigate to that link and the pipeline has successfully run, you should
    see a view in the Kubeflow dashboard showing the steps of the pipeline, with a
    sidebar that allows you to navigate through a series of metadata about your pipeline
    and its run. An example from running the above code is shown in *Figure 5.46*.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_46.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.46: The Kubeflow UI showing the successful run of the training pipeline
    defined in the main text.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it, you have now built and run your first Kubeflow pipeline!
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also compile your Kubeflow pipelines to serialized YAML, which can
    then be read by the Kubeflow backend. You would do this by running a command like
    the following, where `pipeline` is the same pipeline object used in the previous
    example:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: One reason to do this is it is then super easy to run the pipeline. You can
    just upload it to the Kubeflow Pipelines UI, or you can send the YAML to the cluster
    programmatically.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: As in the *Finding your ZenML* section, we have only begun to scratch the surface
    of this tool and have focused initially on getting to know the basics in a local
    environment. The beauty of Kubeflow being based on Kubernetes is that platform
    agnosticism is very much at its core and so these pipelines can be effectively
    run anywhere that supports containers.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'That although I have presented ZenML and Kubeflow as two different pipelining
    tools, they can actually be viewed as complementary, so much so that ZenML provides
    the ability to deploy Kubeflow pipelines through the use of the ZenML Kubeflow
    orchestrator. This means you can leverage the higher-level abstractions provided
    by ZenML but still get the scaling behavior and robustness of Kubeflow as a deployment
    target. We will not cover the details here but the ZenML documentation provides
    an excellent guide: [https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow).'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: The next section will finish the chapter with a brief note on some different
    deployment strategies that you should be aware of when you aim to put all of these
    tools and techniques into practice with real solutions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Selecting your deployment strategy
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed many of the technical details of ways to take ML solutions
    into production in this chapter. The missing piece, however, is that we have not
    defined how you deal with existing infrastructure and how you introduce your solution
    to real traffic and requests. This is what is defined by your deployment strategy,
    and selecting an appropriate one is an important part of being an ML engineer.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Most deployment strategies are, like many of the concepts in this book, inherited
    from the world of **software engineering** and **DevOps**. Two of the most important
    to know about are listed below with some discussion about when they can be particularly
    useful in an ML context.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '**Blue/green deployments** are deployments where the new version of your software
    runs alongside your existing solution until some predefined criteria are met.
    After this point, you then switch all incoming traffic/requests to the new system
    before decommissioning the old one or leave it there for use as a potential rollback
    solution. The method was originally developed by two developers, Daniel North
    and Jez Humble, who were working on an e-commerce site in 2005\.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: The origin of the name is described in this GitHub Gist, [https://gitlab.com/-/snippets/1846041](https://gitlab.com/-/snippets/1846041),
    but essentially boils down to the fact that any other naming convention they could
    come up with always implied one of the candidate solutions or environments was
    “better” or “worse” than the other, for example with “A and B” or “Green and Red.”
    The strategy has since become a classic.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: In an ML engineering context, this is particularly useful in scenarios where
    you want to gather model and solution performance data over a known period of
    time before trusting full deployment. It also helps with giving stakeholders evidence
    that the ML solution will perform as expected “in the wild.” It also plays particularly
    well with batch jobs, as you are just effectively running another batch at the
    same time. This may have some cost implications for you to consider if the job
    is big or complex, or if your production environment is costly to maintain.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: The next strategy is known as **canary deployments** and involves a similar
    setup to the blue/green method but involves a more gradual switching of traffic
    between the two solutions. Here the idea is that the new system is deployed and
    receives some percentage of the traffic initially, say 5% or 10%, before stability
    and performance are confirmed, and then the next increment of traffic is added.
    The total always remains at 100% so as the new system gains more traffic, the
    old system receives less. The name originates from the old coal mining technique
    of using canaries as a test of toxicity in the atmosphere in mines. Release the
    canaries and if they survive, all is well. Thankfully, no birds are harmed in
    the usage of this deployment technique. This strategy makes a lot of sense when
    you are able to divide the data you need to score and still get the information
    you need for progression to the next stage. As an example, an ML microservice
    that is called in the backend of a website would fit the bill nicely, as you can
    just gradually change the routing to the new service on your load balancer. This
    may make less sense for large batch jobs, as there may be no natural way to split
    your data into different increments, whereas with web traffic there definitely
    is.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Building an Example ML Microservice*, will show you how to use
    these strategies when building a custom ML endpoint using Kubernetes.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: No matter what deployment strategy you use, always remember that the key is
    to strike the balance between cost-effectiveness, the uptime of your solution,
    and trust in the outputs it produces. If you can do all of these, then you will
    have deployed a winning combination.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed some of the most important concepts when
    it comes to deploying your ML solutions. In particular, we focused on the concepts
    of architecture and what tools we could potentially use when deploying solutions
    to the cloud. We covered some of the most important patterns used in modern ML
    engineering and how these can be implemented with tools such as containers and
    AWS Elastic Container Registry and Elastic Container Service, as well as how to
    create scheduled pipelines in AWS using Managed Workflows for Apache Airflow.
    We also explored how to hook up the MWAA example with GitHub Actions, so that
    changes to your code can directly trigger updates of running services, providing
    a template to use in future CI/CD processes.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to a discussion of more advanced pipelining tools to build
    on the discussion in *Chapter 4*, *Packaging Up*. This focused on how to use Apache
    Airflow to build and orchestrate your generic pipelines for running your data
    engineering, ML, and MLOps pipelines. We then moved on to a detailed introduction
    to ZenML and Kubeflow, two powerful tools for developing and deploying ML and
    MLOps pipelines at scale.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the question of other ways to scale up
    our solutions so that we can deal with large volumes of data and high-throughput
    calculations.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
