- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployment Patterns and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will dive into some important concepts around the deployment
    of your **machine learning** (**ML**) solution. We will begin to close the circle
    of the ML development lifecycle and lay the groundwork for getting your solutions
    out into the world.
  prefs: []
  type: TYPE_NORMAL
- en: The act of deploying software, of taking it from a demo you can show off to
    a few stakeholders to a service that will ultimately impact customers or colleagues,
    is a very exhilarating but often challenging exercise. It also remains one of
    the most difficult aspects of any ML project and getting it right can ultimately
    make the difference between generating value or just hype.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to explore some of the main concepts that will help your ML engineering
    team cross the chasm between a fun proof-of-concept to solutions that can run
    on scalable infrastructure in an automated way. This will require us to first
    cover questions of how to design and architect your ML systems, particularly if
    you want to develop solutions that can be scaled and extended seamlessly. We will
    then discuss the concept of containerization and how this allows your application
    code to be abstracted from the specific infrastructure it is being built or run
    on, allowing for portability in many different cases. We will then move on to
    a concrete example of using these ideas to deploy an ML microservice on AWS. The
    rest of the chapter will then return to the question of how to build effective
    and robust pipelines for your end-to-end ML solution, which was introduced in
    *Chapter 4*, *Packaging Up*. We will introduce and explore **Apache Airflow**
    for building and orchestrating any generic Python process, including your data
    preparation and ML pipelines. Then we will finish up with a similar deep dive
    on **ZenML** and **Kubeflow**, two open-source advanced ML pipelining tools that
    are now extensively used in industry. This collection of tools means that you
    should finish this chapter feeling very confident that you can deploy and orchestrate
    quite complex ML solutions using a variety of software.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will all be broken down into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecting systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring some standard ML patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosting your own microservice on **Amazon Web Services** (**AWS**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building general pipelines with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building advanced ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next section will kick things off with a discussion of how we can architect
    and design our ML systems with deployment in mind. Let’s go!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with the other chapters, you can set up your Python development environment
    to be able to run the examples in this chapter by using the supplied Conda environment
    `yml` file or the `requirements.txt` files from the book repository, under *Chapter05*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also require some non-Python tools to be installed to follow the examples
    from end to end. Please see the respective documentation for each tool:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS CLI v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how you are working to build your software, it is always important
    to have a design in mind. This section will highlight the key considerations we
    must bear in mind when architecting ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where you are contracted to organize the building of a house.
    We would not simply go out and hire a team of builders, buy all the supplies,
    hire all the equipment, and just tell everyone to *start building*. We would also
    not assume we knew exactly what the client who hired us wants without first speaking
    to them.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we would likely try to understand what the client wanted in detail,
    and then try to design the solution that would fit their requirements. We would
    potentially iterate this plan a few times with them and with appropriate experts
    who knew the details of pieces that fed into the overall design. Although we are
    not interested in building houses (or maybe you are, but there will not be any
    in this book!), we can still see the analogy with software. Before building anything,
    we should create an effective and clear design. This design provides the direction
    of travel for the solution and helps the build team know exactly what components
    they will work on. This means that we will be confident that what we build will
    solve the end user’s problem.
  prefs: []
  type: TYPE_NORMAL
- en: This, in a nutshell, is what software architecture is all about.
  prefs: []
  type: TYPE_NORMAL
- en: If we did the equivalent of the above example for our ML solution, some of the
    following things may happen. We could end up with a very confusing code base,
    with some ML engineers in our team building elements and functionality that are
    already covered by the work that other engineers have done. We may also build
    something that fundamentally cannot work later in the project; for example, if
    we have selected a tool that has specific environmental requirements we cannot
    meet due to another component. We may also struggle to anticipate what infrastructure
    we need to be provisioned ahead of time, leading to a disorganized scramble within
    the project to get the correct resource. We may also underestimate the amount
    of work required and miss our deadline. All of these are outcomes we wish to avoid
    and can be avoided if we are following a good design.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to be effective, the architecture of a piece of software should provide
    at least the following things to the team working on building the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: It should define the functional components required to solve the problem in
    totality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should define how these functional components will interact, usually through
    the exchange of some form of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should show how the solution can be extended in the future to include further
    functionality the client may require.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should provide guidance on which tools should be selected to implement each
    of the components outlined in the architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It should stipulate the process flow for the solution, as well as the data flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is what a piece of good architecture should do, but what does this actually
    mean in practice?
  prefs: []
  type: TYPE_NORMAL
- en: There is no strict definition of how an architecture has to be compiled. The
    key point is that it acts as a design against which building can progress. So,
    for example, this might take the form of a nice diagram with boxes, lines, and
    some text, or it could be a several-page document. It might be compiled using
    a formal modeling language such as **Unified Modeling Language** (**UML**), or
    not. This often depends on the business context in which you operate and what
    requirements are placed on the people writing the architecture. The key is that
    it checks off the points above and gives the engineers clear guidance on what
    to build and how it will all stick together.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture is a vast and fascinating subject in itself, so we will not go
    much further into the details of this here, but we will now focus on what architecture
    means in an ML engineering context.
  prefs: []
  type: TYPE_NORMAL
- en: Building with principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of architecture is vast but no matter where you look, like any mature
    discipline, there are always consistent principles that are presented. The good
    news is that some of them are actually the same as some of the principles we met
    when discussing good Python programming in *Chapter 4*, *Packaging Up*. In this
    section, we will discuss some of these and how they can be used for architecting
    ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Separation of Concerns** has already been mentioned in this book as a good
    way to ensure that software components inside your applications are not unnecessarily
    complex and that your solutions are extensible and can be easily interfaced with.
    This principle holds true of systems in their entirety and as such is a good architecture
    principle to bear in mind. In practice, this often manifests in the idea of separate
    “layers” within your applications that have distinct responsibilities. For example,
    let’s look at the architecture shown in *Figure 5.1*. This shows how to use tools
    to create an automated deployment and orchestration process for ML pipelines and
    is taken from the AWS Solutions Library, [https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/](https://aws.amazon.com/solutions/implementations/mlops-workload-orchestrator/).
    We can see that there are distinct “areas” within the architecture corresponding
    to provisioning, pipeline deployment, and pipeline serving. These blocks show
    that there are distinct pieces of the solution that have specific functionalities
    and that the interaction between these different pieces is handled by an interface.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: An ML workload orchestrator architecture from the AWS Solutions
    Library MLOps Workload Orchestrator.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Principle of Least Surprise** is a rule of thumb that essentially captures
    the fact that the first time any reasonably knowledgeable person in your domain,
    such as a developer, tester, or data scientist, encounters your architecture,
    it should not have anything within it that should stand out as unorthodox or surprising.
    This may not always be possible, but it is a good principle to keep in mind as
    it forces you to consider what those who are likely to be working with your architecture
    already know and how you can leverage that to both make a good design and have
    it followed. Using *Figure 5.1* as an example again, the architecture embodies
    the principle very nicely, as the design has clear logic building blocks for provisioning,
    promoting, and running the ML pipelines. At a lower level in the architecture,
    we can see that data is consistently being sourced from S3 buckets, that Lambdas
    are interacting with API gateways, and so on and so forth. This means that ML
    engineers, data scientists, and cloud platform engineers will both understand
    and leverage this architecture well when implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: The **Principle of Least Effort** is a bit more subtle than the previous one,
    in that it captures the idea that developers, being human, will follow the path
    of least resistance and not create more work unless necessary. I interpret this
    principle as emphasizing the importance of taking the time to consider your architecture
    thoughtfully and building it with care, as it could be used for a long time after
    it has been developed, by engineer after engineer!
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have only discussed high-level architecture principles. Now we will
    look at some design principles that – while they can still be used at the system
    design level – are also very powerful when used at the level of your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **SOLID** principles (**Single Responsibility**, **Open/Closed**, **Liskov
    Substitution**, **Interface Segregation**, **Dependency Inversion**) are a set
    that is often applied to the code base but can also be extrapolated up to system
    design and architecture quite nicely. Once we adapt these principles to the architecture
    level, they can be explained in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single Responsibility**: This is very similar, perhaps identical, to the
    idea of separation of concerns. Specifically, this states that if a module only
    has one reason to change at any one time, or it only has one job to do, then this
    makes it more resilient and easier to maintain. If you have one box in your architecture
    diagram that is going to have to do ten different things, then you have violated
    this principle and it means that whenever any one of those processes or interfaces
    has to change, you have to go into that box and poke around, potentially creating
    more issues or drastically increasing the likelihood of downtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open/Closed**: This refers to the fact that it is a really good idea to architect
    in a way that components are “open for extension but closed for modification.”
    This also works at the level of the entire design. If you design your system so
    that new functionality can be tagged on and does not require going back and rewiring
    the core, then you will likely build something that will stand the test of time.
    A great example from ML would be that if we try and build our system so that if
    we want to add in new processing pipelines we can just do that, and we don’t have
    to go back into some obscure section of the code and severely modify things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Liskov Substitution**: When the SOLID principles were written, they originally
    referred to object-oriented programming in languages like Java. This principle
    then stated that objects should be able to be replaced by their subtypes and still
    maintain application behavior. At the system level, this now basically states
    that if two components are supposed to have the same interface and contract with
    other components, you can swap them for one another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interface Segregation**: I interpret this one as “don’t have multiple ways
    for components to talk to one another.” So, in your application, try and ensure
    that the ways of handing off between different pieces of the solution are pretty
    narrow. Another way of phrasing this is that making your interfaces as client
    specific as possible is a good idea.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency Inversion**: This is very similar to the Liskov Substitution principle
    but is a bit more general. The idea here is that the communications between modules
    or parts of your solution should be taken care of by abstractions and not by a
    concrete, specific implementation. A good example would be that instead of calling
    an ML microservice directly from another process, you instead place the requisite
    job data in a queue, for example, AWS Simple Queue Service, and then the microservice
    picks up the work from the queue. This ensures that the client and the serving
    microservice do not need to know details about each other’s interface, and also
    that the downstream application can be extended with more services reading from
    the queue. This would then also embody the Open/Closed principle, and can be seen
    in the architecture in *Figure 5.1* through the use of the Lambda function calling
    into AWS CloudFormation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final favorite of mine is the concept of **Bounded Contexts**, where we have
    to seek to ensure that data models, or other important data or metadata, are aligned
    within specific conceptual models and are not a “free-for-all.” This applies particularly
    well to Domain-Driven Design and applies very well to large, complex solutions.
    A great example would be if you have a large organization with multiple business
    units and they want a series of very similar services that run ML on business
    data stored in a database. It would be better for there to be several databases
    hosting the information, one for each business unit, rather than having a shared
    data layer across multiple applications. More concretely, your data model shouldn’t
    contain information specific to the sales and marketing function and the engineering
    function and the human resources function, and so on. Instead, each should have
    their own database with their own models, and there should be explicit contracts
    for joining any information between them later if needed. I believe that this
    idea can still be applied to Data Lakes, which are discussed later in this chapter.
    In this case, the bounded contexts could apply to specific folders within the
    lake, or they could actually refer to the context of entire lakes, each segregated
    into different domains. This is very much the idea behind the so-called Data Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: We have just mentioned some of the most used ML patterns, so let’s now move
    on to explore this concept in a bit more detail as we look to apply the principles
    we have been discussing.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring some standard ML patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we have already mentioned a few times that we should not attempt
    to *reinvent* the wheel and we should reuse, repeat, and recycle what works according
    to the wider software and ML community. This is also true about your deployment
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: When we discuss architectures that can be reused for a variety of different
    use cases with similar characteristics, we often refer to these as *patterns*.
    Using standard (or at least well-known) patterns can really help you speed up
    the time to value of your project and help you engineer your ML solution in a
    way that is robust and extensible.
  prefs: []
  type: TYPE_NORMAL
- en: Given this, we will spend the next few sections summarizing some of the most
    important architectural patterns that have become increasingly successful in the
    ML space over the past few years.
  prefs: []
  type: TYPE_NORMAL
- en: Swimming in data lakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The single most important asset for anyone trying to use ML is, of course, the
    data that we can analyze and train our models on. The era of **big data** meant
    that the sheer size and variability in the format of this data became an increasing
    challenge. If you are a large organization (or even not so large), it is not viable
    to store all of the data you will want to use for ML applications in a structured
    relational database. Just the complexity of modeling the data for storage in such
    a format would be very high. So, what can you do?
  prefs: []
  type: TYPE_NORMAL
- en: Well, this problem was initially tackled with the introduction of **data warehouses**,
    which let you bring all of your relational data storage into one solution and
    create a single point of access. This helps alleviate, to some extent, the problem
    of data volumes, as each database can store relatively small amounts of data even
    if the total is large. These warehouses were designed with the integration of
    multiple data sources in mind. However, they are still relatively restrictive
    as they usually bundle together the infrastructure for compute and storage. This
    means they can’t be scaled very well, and they can be expensive investments that
    create vendor lock-in. Most importantly for ML, data warehouses cannot store raw
    and semi-structured or unstructured data (for example, images). This automatically
    rules out a lot of good ML use cases if warehouses are used as your main data
    store. Now, with tools such as **Apache Spark**, which we’ve already used extensively
    throughout this book, if we have the clusters available, we can feasibly analyze
    and model any size or structure of data. The question then becomes, how should
    we store it?
  prefs: []
  type: TYPE_NORMAL
- en: '**Data lakes** are technologies that allow you to store any type of data at
    any scale you feasibly need. There are a variety of providers of data lake solutions,
    including the main public cloud providers, such as **Microsoft Azure**, **Google
    Cloud Platform** (**GCP**), and AWS. Since we have met AWS before, let’s focus
    on that.'
  prefs: []
  type: TYPE_NORMAL
- en: The main storage solution in AWS is called the **Simple Storage Service**, or
    **S3**. Like all of the core data lake technologies, you can effectively load
    anything into it since it is based on the concept of *object storage*. This means
    that every instance of data you load is treated as its own object with a unique
    identifier and associated metadata.
  prefs: []
  type: TYPE_NORMAL
- en: It allows your S3 bucket to simultaneously contain photographs, JSON files,
    `.txt` files, Parquet files, and any other number of data formats.
  prefs: []
  type: TYPE_NORMAL
- en: If you work in an organization that does not have a data lake, this does not
    automatically exclude you from doing ML, but it can definitely make it a more
    difficult journey since with a lake you always know how you can store the data
    you need for your problem, no matter the format.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your ML project’s code base will start small – just a few lines at first. But
    as your team expends more and more effort in building the solution required, this
    will quickly grow. If your solution has to have a few different capabilities and
    perform some quite distinct actions and you keep all of this in the same code
    base, your solution can become incredibly complex. In fact, software in which
    the components are all tightly coupled and non-separable like this is called **monolithic**,
    as it is akin to single big blocks that can exist independently of other applications.
    This sort of approach may fit the bill for your use case, but as the complexity
    of solutions continues to increase, a much more resilient and extensible design
    pattern is often required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservice architectures are those in which the functional components of
    your solution are cleanly separated, potentially in completely different code
    bases or running on completely different infrastructure. For example, if we are
    building a user-facing web application that allows users to browse, select, and
    purchase products, we may have a variety of ML capabilities we wish to deploy
    in quick succession. We may want to recommend new products based on what they
    have just been looking at, we may want to retrieve forecasts of when their recently
    ordered items will arrive, and we may want to highlight some discounts we think
    they will benefit from (based on our analysis of their historic account behavior).
    This would be a very tall order, maybe even impossible, for a monolithic application.
    However, it is something that quite naturally falls into microservice architecture
    like that in *Figure 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – An example of some ML microservices ](img/B19525_05_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: An example of some ML microservices.'
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of a microservice architecture can be accomplished using
    a few tools, some of which we will cover in the *Hosting your own microservice
    on AWS* section. The main idea is that you always separate out the elements of
    your solution into their own services that are not tightly coupled together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microservice architectures are particularly good at allowing our development
    teams to achieve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Independently debug, patch, or deploy individual services rather than tearing
    down the whole system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid a single point of failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase maintainability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allow separate services to be owned by distinct teams with clearer responsibilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accelerate the development of complex products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like every architecture pattern or design style, it is, of course, not a silver
    bullet, but we would do well to remember the microservice architecture when designing
    our next solution.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss event-based designs.
  prefs: []
  type: TYPE_NORMAL
- en: Event-based designs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You do not always want to operate in scheduled batches. As we have seen, even
    just in the previous section, *Microservices*, not all use cases align with running
    a large batch prediction from a model on a set schedule, storing the results,
    and then retrieving them later. What happens if the data volumes you need are
    not there for a training run? What if no new data to run predictions on has arrived?
    What if other systems could make use of a prediction based on individual data
    points at the earliest time they become available rather than at a specific time
    every day?
  prefs: []
  type: TYPE_NORMAL
- en: In an event-based architecture, individual actions produce results that then
    trigger other individual actions in the system, and so on and so forth. This means
    that processes can happen as early as they can and no earlier. It also allows
    for a more dynamic or stochastic data flow, which can be beneficial if other systems
    are not running on scheduled batches either.
  prefs: []
  type: TYPE_NORMAL
- en: Event-based patterns could be mixed with others, for example, microservices
    or batch processing. The benefits still stand, and, in fact, event-based components
    allow for more sophisticated orchestration and management of your solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of event-based patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pub/sub**: In this case, event data is published to a message broker or event
    bus to be consumed by other applications. In one variant of the pub/sub pattern,
    the broker or buses used are organized by some appropriate classification and
    are designated as **topics**. An example of a tool that does this is **Apache
    Kafka**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Event streaming**: Streaming use cases are ones where we want to process
    a continuous flow of data in something very close to real time. We can think of
    this as working with data as it *moves through* the system. This means it is not
    persisted *at rest* in a database but processed as it is created or received by
    the streaming solution. An example tool to use for event streaming applications
    is **Apache Storm**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5.3* shows an example event-based architecture applied to the case
    of **IoT** and mobile devices that have their data passed into classification
    and anomaly detection algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – A basic event-based architecture where a stream of data is accessed
    by different services via a broker ](img/B19525_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: A basic event-based high-level design where a stream of data is
    accessed by different services via a broker.'
  prefs: []
  type: TYPE_NORMAL
- en: The next section will touch on designs where we do the opposite of processing
    one data point at a time and instead work with large chunks or batches at any
    one time.
  prefs: []
  type: TYPE_NORMAL
- en: Batching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batches of work may not sound like the most sophisticated concept, but it is
    one of the most common pattern flavors out there in the world of ML.
  prefs: []
  type: TYPE_NORMAL
- en: If the data you require for prediction comes in at regular time intervals in
    batches, it can be efficient to schedule your prediction runs with a similar cadence.
    This type of pattern can also be useful if you do not have to create a low-latency
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept can also be made to run quite efficiently for a few reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Running in scheduled batches means that we know exactly when we will need compute
    resources, so we can plan accordingly. For example, we may be able to shut down
    our clusters for most of the day or repurpose them for other activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batches allow for the use of larger numbers of data points at runtime, so you
    can run things such as anomaly detection or clustering at the batch level if desired.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of your batches of data can often be chosen to optimize some criterion.
    For example, using large batches and running parallelized logic and algorithms
    on it could be more efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software solutions where ML algorithms are run in batches often look very similar
    to classic **Extract**, **Transform**, **Load** (**ETL**) systems. These are systems
    where data is extracted from a source or sources, before being processed on route
    to a target system where it is then uploaded. In the case of an ML solution, the
    processing is not standard data transformation such as joins and filters but is
    instead the application of feature engineering and ML algorithm pipelines. This
    is why, in this book, we will term these designs **Extract, Transform, Machine
    Learning** (**ETML**) patterns. ETML will be discussed more in *Chapter 9*, *Building
    an Extract, Transform, Machine Learning Use Case*.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss a key piece of technology that is critical to making modern
    architectures applicable to a wide range of platforms – containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you develop software that you want to deploy somewhere, which is the core
    aim of an ML engineer, then you have to be very aware of the environmental requirements
    of your code, and how different environments might affect the ability of your
    solution to run. This is particularly important for Python, which does not have
    a core capability for exporting programs as standalone executables (although there
    are options for doing this). This means that Python code needs a Python interpreter
    to run and needs to exist in a general Python environment where the relevant libraries
    and supporting packages have been installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'A great way to avoid headaches from this point of view is to ask the question:
    *Why can’t I just put everything I need into something that is relatively isolated
    from the host environment, which I can ship and then run as a standalone application
    or program?* The answer to this question is that you can and that you do this
    through **containerization**. This is a process whereby an application and its
    dependencies can be packaged together in a standalone unit that can effectively
    run on any computing platform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular container technology is **Docker**, which is open-source and
    very easy to use. Let’s learn about it by using it to containerize a simple **Flask**
    web application that could act as an interface to a forecasting model like that
    created in the *Example 2*: *Forecasting API* section in *Chapter 1*, *Introduction
    to ML Engineering*.'
  prefs: []
  type: TYPE_NORMAL
- en: The next few sections will use a similar simple Flask application that has a
    forecast serving endpoint. As a proxy for a full ML model, we will first work
    with a skeleton application that simply returns a short list of random numbers
    when requested for a forecast. The detailed code for the application can be found
    in this book’s GitHub repo at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/tree/main/Chapter05/microservices/mlewp2-web-service).
  prefs: []
  type: TYPE_NORMAL
- en: The web application creates a basic app where you can supply a store ID and
    forecast a start date for the system and it will return the dummy forecast. To
    get this, you hit the `/forecast` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example is shown in *Figure 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: The result of querying our skeleton ML microservice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’ll move on to discuss how to containerize this application. First,
    you need to install Docker on your platform by using the documentation at [https://docs.docker.com/engine/install/](https://docs.docker.com/engine/install/):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have Docker installed, you need to tell it how to build the container
    image, which you do by creating a `Dockerfile` in your project. The `Dockerfile`
    specifies all of the build steps in text so that the process of building the image
    is automated and easily configurable. We will now walk through building a simple
    example `Dockerfile`, which will be built on in the next section, *Hosting your
    own microservice on AWS*. First, we need to specify the base image we are working
    from. It usually makes sense to use one of the official Docker images as a base,
    so here we will use the `python:3.10-slim` environment to keep things lean and
    mean. This base image will be used in all commands following the `FROM` keyword,
    which signifies we are entering a build stage. We can actually name this stage
    for later use, calling it `builder` using the `FROM … as` syntax:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we copy all the files we need from the current directory to a directory
    labeled `src` in the build stage and install all of our requirements using our
    `requirements.txt` file (if you want to run this step without specifying any requirements,
    you can just use an empty `requirements.txt` file):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next stage involves similar steps but is aliased to the word `app` since
    we are now creating our application. Notice the reference to the `builder` stage
    from steps *1* and *2* here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can define or add to environment variables as we are used to in a bash environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since in this example we are going to be running a simple Flask web application,
    we need to tell the system which port to expose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can execute commands during the Docker build using the `CMD` keyword. Here,
    we use this to run `app.py`, which is the main entry point to the Flask app, and
    will start the service we will call via REST API to get ML results later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we can build the image with the `docker build` command. Here, we create
    an image named `basic-ml-microservice` and tag it with the `latest` label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check the build was successful, run the following command in the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see an output like that in *Figure 5.5*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_05_05.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.5: Output from the docker images command.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, you can run your Docker image with the following command in your Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you have containerized some basic applications and can run your Docker
    image, we need to answer the question of how we can use this to build an ML solution
    hosted on an appropriate platform. The next section covers how we can do this
    on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting your own microservice on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classic way to surface your ML models is via a lightweight web service hosted
    on a server. This can be a very flexible pattern of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: You can run a web service on any server with access to the internet (roughly)
    and, if designed well, it is often easy to add further functionality to your web
    service and expose it via new endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, the two most used web frameworks have always been **Django** and
    **Flask**. In this section, we will focus on Flask as it is the simpler of the
    two and has been written about extensively for ML deployments on the web, so you
    will be able to find plenty of material to build on what you learn here.
  prefs: []
  type: TYPE_NORMAL
- en: On AWS, one of the simplest ways you can host your Flask web solution is as
    a containerized application on an appropriate platform. We will go through the
    basics of doing this here, but we will not spend time on the detailed aspects
    of maintaining good web security for your service. To fully discuss this may require
    an entire book in itself, and there are excellent, more focused resources elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: We will assume that you have your AWS account set up from *Chapter 2*, *The
    Machine Learning Development Process*. If you do not, then go back and refresh
    yourself on what you need to do.
  prefs: []
  type: TYPE_NORMAL
- en: We will need the AWS **Command Line Interface** (**CLI**). You can find the
    appropriate commands for installing and configuring the AWS CLI, as well as a
    lot of other useful information, on the AWS CLI documentation pages at [https://docs.aws.amazon.com/cli/index.xhtml](https://docs.aws.amazon.com/cli/index.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, configure your Amazon CLI by following the steps in this tutorial:
    [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: The documentation specifies how to install the CLI for a variety of different
    computer architectures, so follow along for your given platform and then you will
    be ready to have fun with the AWS examples used in the rest of the book!
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will use Amazon **Elastic Container Registry**
    (**ECR**) and **Elastic Container Service** (**ECS**) to host a skeleton containerized
    web application. In *Chapter 8*, *Building an Example ML Microservice*, we will
    discuss how to build and scale an ML microservice in more detail and using a lower-level
    implementation based on Kubernetes. These two approaches complement each other
    nicely and will help you widen your ML engineering toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying our service on ECS will require a few different components, which
    we will walk through in the next few sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Our container hosted inside a repository on ECR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster and service created on ECS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An application load balancer created via the **Elastic Compute Cloud** (**EC2**)
    service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let’s tackle pushing the container to ECR.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing to ECR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following Dockerfile defined within the project directory from
    the *Containerizing* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then use the AWS `CLI` to create an ECR repository for hosting our container.
    We will call the repository `basic-ml-microservice` and will set the region as
    `eu-west-1`, but this should be changed to what region seems most appropriate
    for your account. The command below will return some metadata about your ECR repository;
    keep this for later steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then log in to the container registry with the following command in
    the Terminal. Note that the repository URI will have been in the metadata provided
    after running step *2*. You can also retrieve this by running `aws ecr describe-repositories
    --region eu-west-1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, if we navigate to the directory containing the `Dockerfile` (`app`),
    we can run the following command to build the container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step tags the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then deploy the Docker image we have just built to the container registry
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If successful, this last command will have pushed the locally built Docker image
    to your remotely hosted ECR repository. You can confirm this by navigating to
    the AWS management console, going to the ECR service, and selecting the basic-ml-microservice
    repository. You should then see something like what is shown in *Figure 5.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Succesful push of the locally built Docker image to the ECR repository.'
  prefs: []
  type: TYPE_NORMAL
- en: The steps we have just gone through are actually quite powerful in general,
    as you are now able to build cross-platform Docker images and share them in a
    central repository under your AWS account. You can share Docker containers and
    images via DockerHub as well, [https://hub.docker.com/](https://hub.docker.com/),
    but this gives you more control if you want to do this inside your own organization.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have built the container that hosts the Flask app, we will now look
    to deploy this on scalable infrastructure. To do this, in the next section, we
    will set up our cluster on ECS.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting on ECS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s start with the setup! At the time of writing in mid-2023, AWS has
    recently introduced a revamped ECS console that allows for a far smoother setup
    than previously. So, if you read the first edition of this book, you will find
    this a far smoother experience:'
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to **ECS** on the AWS Management Console and click **Create
    Cluster**. You will be provided with a form that asks for details about networking,
    infrastucture, monitoring, and the provision of any tags on the resources we are
    about to create. This should look like *Figure 5.7*.![](img/B19525_05_07.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.7: Creating a cluster in Elastic Container Service.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, we can name the cluster `mlewp2-ecs-cluster`, or indeed whatever you
    want! Then when you expand the **Networking** section, you should see that many
    of the **VPC** and subnet details are auto-populated with defaults based on your
    AWS account setup. If you need to set these up, the form points to the relevant
    documentation. See *Figure 5.8* for an example.![](img/B19525_05_08.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.8: Networking configuration for our cluster in AWS ECS.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **Infrastructure** section contains three options, with the use of **AWS
    Fargate** being the pre-selected default option. We do not need to know the details
    of how Fargate works but suffice it to say that this provides a very high-level
    abstraction for managing container workloads across multiple servers. The introduction
    of Fargate has meant that you do not need to worry about details of the provisioning
    and running of clusters of virtual machines to run your container workloads. According
    to the AWS documentation, the Fargate service is ideal for dynamic bursts of work
    or large workloads with low operational overhead. If you know you are going to
    be running large jobs that have to be price optimized, you can then look to the
    other infra options provided, for example, **EC2 instances**. We will not need
    these for the purposes of this example. *Figure 5.9* shows the **Infrastructure**
    section for reference.![](img/B19525_05_09.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.9: Configuring the infrastructure options in the ECS service.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **Monitoring** and **Tags** sections are relatively self-explanatory and
    allow you to toggle on **container insights** and provide your own string tags
    for the ECS resources that will be created. Let’s leave these as the default for
    now and click **Create** at the bottom of the page. You should then see that the
    cluster was successfully created after a few minutes, as shown in *Figure 5.10*.![](img/B19525_05_10.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.10: The successful creation of the ECS cluster.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The previous steps were all about setting up the ECS cluster, the infrastructure
    on which our containerized application can run. To actually tell ECS how to run
    the solution, we need to define **tasks**, which are simply processes we wish
    to be executed on the cluster. There is a related concept of **Services** in ECS,
    which refers to a process for managing your tasks, for example, by ensuring a
    certain number of tasks are always running on the cluster. This is useful if you
    have certain uptime requirements for your solution, such as, if it needs to be
    available for requests 24/7\. We can create the task definition in the cluster
    by first navigating to the cluster review page in the AWS management console,
    and then selecting **Task Definitions** on the left-hand side. We will then click
    on **Create New Task Definition**. Follow the steps below to create this task
    definition.
  prefs: []
  type: TYPE_NORMAL
- en: We have to name the task definition family, which is just the collection of
    versions of the task definition. Let’s call ours `basic-ml-microservice-tasks`
    for simplicity. We then need to provide some container details such as the URI
    for the image we want to use. This is the URI for the image we pushed to the ECR
    repository previously, which is formatted something like `<YOUR_AWS_ID>.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest`.
    You can give the container a new name. Here, I have called it **mlmicro**. Finally,
    you need to supply appropriate port mappings to allow the container and the application
    it contains to be accessible to external traffic. I have mapped `port 5000`, which
    you may recall is the port we exposed in the original Dockerfile using the TCP
    protocol. This is all shown in *Figure 5.11*. You can leave the rest of the optional
    settings for this first section as the default just now and click **Next** to
    move on to the next page of settings.![](img/B19525_05_11.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.11: Defining the container image to use for the task definition in
    ECS.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next page in the console asks for information about the environment and
    infrastructure you will be running the solution on. Based on the settings we used
    for the ECS cluster, we will be using Fargate as the infrastructure option, running
    on a **Linux x86_64** environment. The tasks we are running are very small in
    this case (we’re just returning some numbers for demo purposes) so we can keep
    the default options of **1 vCPU** with **3 GB** memory. You can also add container-level
    memory and CPU requirements if necessary, but we can leave this blank for now.
    This is particularly useful if you have a computationally heavy service, or it
    contains an application that is pre-loaded with some large model or configuration
    data. You can see this in *Figure 5.12*.![](img/B19525_05_12.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.12: Configuring our application environment for the AWS ECS task definition
    used for our ML microservice.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, IAM roles need to be configured. We will not be calling other AWS services
    from our application, so at this point, we do not need an IAM task role, but you
    can create one if you need this at a later point, for example, if you wish to
    call another data or ML service. For executing the tasks we need an execution
    role, which by default is created for you, so let’s use that. The IAM configuration
    section is shown in *Figure 5.13*.![](img/B19525_05_13.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.13: The IAM roles defined for use by the AWS ECS task definition.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of this section contains optional sections for storage, monitoring,
    and tagging. The storage subsection refers to ephemeral storage used to decompress
    and host your Docker container. Again, for larger containers, you may need to
    consider increasing this size from the default 21 GiB. Monitoring can be enabled
    using **Amazon CloudWatch**, which is useful when you need infrastructure monitoring
    as part of your solution, but we will not cover that here and focus more on the
    core deployment. Keep these sections as is for now and click **Next** at the bottom
    of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are almost there. Now we’ll review and create the task definition. If you
    are happy with the selections upon reviewing, then create the task definition
    and you will be taken to a summary page like that shown in *Figure 5.14*.![](img/B19525_05_14.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.14: Successful creation of the ML microservice task definition.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the final step of setting up our ECS-hosted solution is the creation of
    a service. We will now walk through how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to the task definition we have just created in the previous
    steps and select the **Deploy** button. This will provide a dropdown where you
    can select **Create service**. *Figure 5.15* shows you what this looks like as
    it may be easy to miss.![](img/B19525_05_15.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.15: Selecting the Create service option for the task definition we
    have just created in the previous steps.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will then be taken to another page where we need to fill in the details
    of the service we wish to create. For **Existing cluster**, select the ECS cluster
    we defined before, which for this example was called **mlewp2-ecs-cluster**. For
    **Compute configuration**, we will just use the **Launch type** option, which
    means we can just allow Fargate to manage the infrastructure requirements. If
    you have multiple infrastructure options that you want to blend together, then
    you can use the **Capacity provider strategy** option. Note that this is more
    advanced and so I encourage you to read more in the AWS documentation about your
    options here if you need to use this route. For reference, my selections are shown
    in *Figure 5.16*.![](img/B19525_05_16.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.16: AWS ECS selections for the environment that we will run our ECS
    service on. This service will enable the task definition we defined before, and
    therefore our application, to run continuously.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next is the deployment configuration, which refers to how the service runs in
    terms of the number of replicas and what actions to take upon failures of the
    solution. I have defined the service name simply as **basic-ml-microservice-service**,
    and have used the **Replica** service type, which specifies how many tasks should
    be maintained across the cluster. We can leave this as **1** for now as we only
    have one task in our task definition. This is shown in *Figure 5.17*.![](img/B19525_05_17.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.17: Configuring the AWS ECS service name and type.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The **Deployment options** and **Deployment failure detection** subsections
    will be auto-populated with some defaults. A rolling deployment type refers to
    the replacement of the container with the latest version when that is available.
    The failure detection options ensure that deployments that run into errors fail
    to proceed and that rollbacks to previous versions are enabled. We do not need
    to enable **CloudWatch alarms** at this stage as we have not configured CloudWatch,
    but this could be added in future iterations of your project. See *Figure 5.18*
    for reference.![](img/B19525_05_18.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.18: Deployment and failure detection options for the AWS ECS service
    we are about to deploy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As in the other examples, there is a **Networking** section that should be prepopulated
    with the VPC and subnet information appropriate for your account. As before, you
    can switch these out for specific VPCs and subnets according to your requirements.
    *Figure 5.19* shows what this looks like for reference.![](img/B19525_05_19.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.19: The networking section for the AWS ECS service that we are defining
    for hosting the ML microservice.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The remaining sections are optional and contain configuration elements for load
    balancing, auto-scaling, and tagging. Although we do not necessarily need it for
    such a simple application, we will use this section to create an application load
    balancer, which is one of the options available. An application load balancer
    routes HTTP and HTTPS requests and supports useful capabilities like path-based
    routing and dynamic host port mapping, which allows for multiple tasks from a
    single service to run on the same container. We can name the load balancer `basic-ml-microservice-lb`
    and configure the **listener** for this load balancer to listen on `port 80` with
    the HTTP protocol, as shown in *Figure 5.20*. This listener checks for connection
    requests at the given port and uses the specified protocol so that requests can
    then be routed by the load balancer to the downstream system.![](img/B19525_05_20.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.20: Defining the load balancer name and listener details for the AWS
    ECS service.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we must specify a target group for the load balancer, which as the
    name suggests is basically the collection of target endpoints for the tasks in
    your service. AWS ECS ensures that this updates as task definitions are updated
    through the lifetime of your service. *Figure 5.21* shows the configurations for
    the target group, which just specifies the HTTP protocol and home path for health
    checks.![](img/B19525_05_21.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.21: Target group definition for the application load balancer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After filling in these details, hit the **Create** button. This will then deploy
    your service. If all has gone well, then you should be able to see the service
    in your cluster details on the AWS ECS console page. You can navigate to this
    service and find the load balancer. This will have a **Domain Name System** (**DNS**)
    address that will be the root of the target URL for sending requests. *Figure
    5.22* shows what this page with the DNS looks like. Copy or save this DNS value.![](img/B19525_05_22.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.22: The deployed load balancer for our service with the DNS name in
    the bottom right-hand corner.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, to test the service, we can run the same request we had for local testing
    in Postman, but now update the URL to contain the load balancer DNS name and the
    port we have specified that the load balancer will receive oncoming traffic with
    . For us, this is port 80\. This is shown with the application response in *Figure
    5.23*.![](img/B19525_05_23.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.23: A valid result is returned by our simple forecasting service from
    the hosted application AWS ECS.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: And that’s it! We have now successfully built and deployed a simplified forecasting
    service using Flask, Docker, AWS Elastic Container Registry, AWS Elastic Container
    Service, and an application load balancer. These components can all be adapted
    for deploying your future ML microservices.
  prefs: []
  type: TYPE_NORMAL
- en: The first half of this chapter has been about architectural and design principles
    that apply at the system and code level, as well as showing you how some of this
    comes together in one mode of deployment that is very common for ML systems, that
    of the ML microservice. Now that we have done this, we will move on to discuss
    some tools and techniques that allow us to build, deploy, and host complex ML
    workflows as pipelines, a concept we briefly introduced earlier in the book. The
    tools and concepts we will cover in the second half of this chapter are crucial
    for any modern ML engineer to have a strong grasp of, as they are starting to
    form the backbone of so many deployed ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will start this discussion with an exploration of how we can
    use Airflow to create and orchestrate flexible, general-purpose, production-ready
    pipelines, before we move on to some tools aimed specifically at advanced ML pipelining
    and orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Building general pipelines with Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 4*, *Packaging Up*, we discussed the benefits of writing our ML
    code as pipelines. We discussed how to implement some basic ML pipelines using
    tools such as `sklearn` and **Spark ML**. The pipelines we were concerned with
    there were very nice ways of streamlining your code and making several processes
    available to use within a single object to simplify an application. However, everything
    we discussed then was very much focused on one Python file and not necessarily
    something we could extend very flexibly outside the confines of the package we
    were using. With the techniques we discussed, for example, it would be very difficult
    to create pipelines where each step was using a different package or even where
    they were entirely different programs. They did not allow us to build much sophistication
    into our data flows or application logic either, as if one of the steps failed,
    the pipeline failed, and that was that.
  prefs: []
  type: TYPE_NORMAL
- en: The tools we are about to discuss take these concepts to the next level. They
    allow you to manage the workflows of your ML solutions so that you can organize,
    coordinate, and orchestrate elements with the appropriate level of complexity
    to get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Apache Airflow** is the workflow management tool that was initially developed
    by **Airbnb** in the 2010s and has been open-source since its inception. It gives
    data scientists, data engineers, and ML engineers the capability of programmatically
    creating complex pipelines through Python scripts. Airflow’s task management is
    based on the definition and then execution of a **Directed Acyclic Graph** (**DAG**)
    with nodes as the tasks to be run. DAGs are also used in **TensorFlow** and **Spark**,
    so you may have heard of them before.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Airflow contains a variety of default operators to allow you to define DAGs
    that can call and use multiple components as tasks, without caring about the specific
    details of a task. It also provides functionality for scheduling your pipelines.
    As an example, let’s build an Apache Airflow pipeline that will get data, perform
    some feature engineering, train a model, and then persist the model. We won’t
    cover the detailed implementation of each command, but simply show you how your
    ML processes hang together in an Airflow DAG. In *Chapter 9*, *Building an Extract,
    Transform, Machine Learning Use Case*, we will build out a detailed end-to-end
    example discussing these lower-level details. This first example is more concerned
    with understanding the high level of how to write, deploy, and manage your DAGs
    in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, in a file called `classification_pipeline_dag.py`, we can import the
    relevant Airflow packages and any utility packages we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, Airflow allows you to define default arguments that can be referenced
    by all of the following tasks, with the option to overwrite at the same level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then have to instantiate our DAG and provide the relevant metadata, including
    our scheduling interval:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, all that is required is to define your tasks within the `DAG` definition.
    First, we define an initial task that gets our dataset. This next piece of code
    assumes there is a Python executable, for example, a function or class method,
    called `get_data` that we can pass to the task. This could have been imported
    from any submodule or package we want. Note that *steps* *3*-*5* assume we are
    inside the code block of the DAG instantiation, so we assume another indent that
    we don’t show here to save space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then perform a task that takes this data and performs our model training
    steps. This task could, for example, encapsulate one of the pipeline types we
    covered in *Chapter 3*, *From Model to Model Factory*; for example, a Spark ML
    pipeline, **Scikit-Learn** pipeline, or any other ML training pipeline we looked
    at. Again, we assume there is a Python executable called `train_model` that can
    be used in this step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final step of this process is a placeholder for taking the resultant trained
    model and persisting it to our storage layer. This means that other services or
    pipelines can use this model for prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we define the running order of the task nodes that we have defined
    in the DAG using the `>>` operator. The tasks above could have been defined in
    any order, but the following syntax stipulates how they must run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next sections, we will briefly cover how to set up an Airflow pipeline
    on AWS using the **Managed Workflows for Apache Airflow** (**MWAA**) service.
    The section after will then show how you can use **CI/CD** principles to continuously
    develop and update your Airflow solutions. This will bring together some of the
    setup and work we have been doing in previous chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AWS provides a cloud-hosted service called **Managed Workflows for Apache Airflow**
    (**MWAA**) that allows you to deploy and host your Airflow pipelines easily and
    robustly. Here, we will briefly cover how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Create an environment** on the MWAA landing page. You can find this
    by searching for MWAA in the AWS Management Console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will then be provided with a screen asking for the details of your new Airflow
    environment. *Figure 5.24* shows the high-level steps that the website takes you
    through:![Figure 5.29 – The high-level steps for setting up an MWAA environment
    and associated managed Airflow runs ](img/B19525_05_24.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.24: The high-level steps for setting up an MWAA environment and associated
    managed Airflow runs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Environment details**, as shown in *Figure 5.25*, is where we specify our
    environment name. Here, we have called it **mlewp2-airflow-dev-env**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_05_25.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.25: Naming your MWAA environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For MWAA to run, it needs to be able to access code defining the DAG and any
    associated requirements or plugin files. The system then asks for an AWS S3 bucket
    where these pieces of code and configuration reside. In this example, we create
    a bucket called `mlewp2-ch5-airflow-example` that will contain these pieces. *Figure
    5.26* shows the creation of the bucket:![](img/B19525_05_26.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.26: The successful creation of our AWS S3 bucket for storing our Airflow
    code and supporting configuration elements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Figure 5.27* shows how we point MWAA to the correct bucket, folders, and plugins
    or requirement files if we have them too:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B19525_05_27.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5.27: We reference the bucket we created in the previous step in the
    configuration of the MWAA instance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We then have to define the configuration of the network that the managed instance
    of Airflow will use, similar to the other AWS examples in this chapter. This can
    get a bit confusing if you are new to networking, so it might be good to read
    around the topics of subnets, IP addresses, and VPCs. Creating a new MWAA VPC
    is the easiest approach for getting started in terms of networking here, but your
    organization will have networking specialists who can help you use the appropriate
    settings for your situation. We will go with this simplest route and click **Create
    MWAA VPC**, which opens a new window where we can quickly spin up a new VPC and
    network setup based on a standard stack definition provided by AWS. You will be
    asked for a stack name. I have called mine `MLEWP-2-MWAA-VPC`. The networking
    information will be populated with something like that shown in *Figure 5.28*:![](img/B19525_05_28.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.28: An example stack template for creating your new VPC.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are then taken to a page where we are asked for more details on networking.
    We can select **Public network (No additional setup)** for this example as we
    will not be too concerned with creating an organizationally aligned security model.
    For deployments in an organization, work with your security team to understand
    what additional security you need to put in place. We can also select **Create
    new security group**. This is shown in *Figure 5.29*.![](img/B19525_05_29.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.29: Finalizing the networking for our MWAA setup.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we have to define the **Environment class** that we want to spin up. Currently,
    there are three options. Here, we’ll use the smallest, but you can choose the
    environment that best suits your needs (always ask the billpayer’s permission!).
    *Figure 5.30* shows that we can select the **mw1.small** environment class with
    a min to max worker count of 1-10\. MWAA does allow you to change the environment
    class after instantiating if you need to, so it can often be better to start small
    and scale up as needed from a cost point of view. You will also be asked about
    the number of schedulers you want for the environment. Let’s leave this as the
    default, **2**, for now, but you can go up to 5.![](img/B19525_05_30.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.30: Selecting an environment class and worker sizes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, if desired, we confirm some optional configuration parameters (or leave
    these blank, as done here) and confirm that we are happy for AWS to create and
    use a new execution role. We can also just proceed with the default monitoring
    settings. *Figure 5.31* shows an example of this (and don’t worry, the security
    group will have long been deleted by the time you are reading this page!):![](img/B19525_05_31.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.31: The creation of the execution role used by AWS for the MWAA environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next page will supply you with a final summary before allowing you to create
    your MWAA environment. Once you do this, you will be able to see your newly created
    environment in the MWAA service, as in *Figure 5.32*. This process can take some
    time, and for this example it took around 30 minutes:![](img/B19525_05_32.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.32: Our newly minted MWAA environment.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have this MWAA environment and you have supplied your DAG to the
    S3 bucket that it points to, you can open the Airflow UI and see the scheduled
    jobs defined by your DAG. You have now deployed a basic running service that we
    can build upon in later work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will want to see the DAGs in the Airflow UI so that we can orchestrate
    and monitor the jobs. To do this, you may need to configure access for your own
    account to the MWAA UI using the details outlined on the AWS documentation pages.
    As a quick summary, you need to go to the IAM service on AWS. You will need to
    be logged in as a root user, and then create a new policy title, **AmazonMWAAWebServerAccess**.
    Give this policy the following JSON body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: For this definition, the Airflow role refers to one of the five roles of **Admin**,
    **Op**, **Viewer**, **User**, or **Public**, as defined in the Airflow documentation
    at [https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/security/access-control.xhtml).
    I have used the Admin role for this example. If you add this policy to the permissions
    of your account, you should be able to access the Airflow UI by clicking the **Open
    Airflow UI** button in the MWAA service. You will then be directed to the Airflow
    UI, as shown in *Figure 5.33*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.33: The Airflow UI accessed via the AWS MWAA service. This view shows
    the classification DAG that we wrote earlier in the example.'
  prefs: []
  type: TYPE_NORMAL
- en: The Airflow UI allows you to trigger DAG runs, manage the jobs that you have
    scheduled, and monitor and troubleshoot your pipelines. As an example, upon a
    successful run, you can see summary information for the runs, as shown in *Figure
    5.34*, and can use the different views to understand the time taken for each of
    the pipeline steps and diagnose where any issues have arisen if there are errors
    raised.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.34: Example run summary for our simple classification DAG in the Airflow
    UI.'
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline we have built and run in this example is obviously very simple,
    with only core Python functionality being used. If you want to leverage other
    AWS services, for example, by submitting a Spark job to an EMR cluster, then you
    will need to configure further access policies like the one we did above for the
    UI access. This is covered in the MWAA documentation.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: Once you have created this MWAA environment, you cannot pause it, as it costs
    a small amount to run per hour (around 0.5 USD per hour for the environment configuration
    above). MWAA does not currently contain a feature for pausing and resuming an
    environment, so you will have to delete the environment and re-instantiate a new
    one with the same configuration when required. This can be automated using tools
    such as **Terraform** or **AWS** **CloudFormation**, which we will not cover here.
    So, a word of warning – *DO NOT ACCIDENTALLY LEAVE YOUR ENVIRONMENT RUNNING*.
    For example, definitely do not leave it running for a week, like I may or may
    not have done.
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting CI/CD for Airflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced the basics of CI/CD in *Chapter 2*, *The Machine Learning Development
    Process*, and discussed how this can be achieved by using **GitHub Actions**.
    We will now take this a step further and start to set up CI/CD pipelines that
    deploy code to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will start with an important example where we will push some code
    to an AWS S3 bucket. This can be done by creating a `.yml` file in your GitHub
    repo under your `.github./workflows` directory called `aws-s3-deploy.yml`. This
    will be the nucleus around which we will form our CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'The .`yml` file, in our case, will upload the Airflow DAG and contain the following
    pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We name the process using the syntax for `name` and express that we want the
    deployment process to be triggered on a push to the main branch or a pull request
    to the main branch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then define the jobs we want to occur during the deployment process. In
    this case, we want to upload our DAG files to an S3 bucket we have already created,
    and we want to use the appropriate AWS credentials we have configured in our GitHub
    secrets store:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, as part of the job, we run the step that copies the relevant files to
    our specified AWS S3 bucket. In this case, we are also specifying some details
    about how to make the copy using the AWS CLI. Specifically, here we want to copy
    over all the Python files to the `dags` folder of the repo:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we perform a `git push` command with updated code, this will then execute
    the action and push the `dag` Python code to the specified S3 bucket. In the GitHub
    UI, you will be able to see something like *Figure 5.35* on a successful run:![Figure
    5.38 – A successful CI/CD process run via GitHub Actions and using the AWS CLI
    ](img/B19525_05_35.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 5.35: A successful CI/CD process run via GitHub Actions and using the
    AWS CLI.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This process then allows you to successfully push new updates to your Airflow
    service into AWS to be run by your MWAA instance. This is real CI/CD and allows
    you to continually update the service you are providing without downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Building advanced ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already discussed in this chapter how **SciKit-learn** and **Spark ML**
    provide mechanisms for creating ML pipelines. You can think of these as the basic
    way to do this and to get started. There are a series of tools now available,
    both open-source and enterprise, that take this concept to the next level.
  prefs: []
  type: TYPE_NORMAL
- en: For awareness, the three main public cloud providers have tools in this area
    you may want to be aware of and try out. **Amazon SageMaker** is one of the giants
    of this space and contains within it a large ecosystem of tools and capabilities
    to help take your ML models into production. This book could have been entirely
    about Amazon SageMaker, but since that was done elsewhere, in *Learn Amazon SageMaker*,
    [https://tinyurl.com/mr48rsxp](https://tinyurl.com/mr48rsxp), we will leave the
    details for the reader to discover. The key thing you need to know is that this
    is AWS’s managed service for building up ML pipelines, as well as monitoring,
    model registry, and a series of other capabilities in a way that lets you develop
    and promote your models all the way through the ML lifecycle. **Google Vertex
    AI** is the Google Cloud Platform ML pipelining, development, and deployment tool.
    It brings tons of functionality under one UI and API, like Sagemaker, but seems
    to have less flexibility on the types of models you can train. **Azure ML** is
    the Microsoft cloud provider’s offering.
  prefs: []
  type: TYPE_NORMAL
- en: These are all enterprise-grade solutions that you can try for free, but you
    should be prepared to have your credit card ready when things scale up. The solutions
    above are also naturally tied into specific cloud providers and therefore can
    create “vendor lock-in,” where it becomes difficult to switch later. Thankfully,
    there are solutions that help with this and allow ML engineers to work with a
    less complex setup and then migrate to more complex infrastructure and environments
    later. The first one of these that we will discuss is **ZenML**.
  prefs: []
  type: TYPE_NORMAL
- en: Finding your ZenML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**ZenML** is a completely open-source framework that helps you write ML pipelines
    in a way that is totally abstracted from the underlying infrastructure. This means
    that your local development environment and your eventual production environment
    can be very different, and can be changed through changes in configuration without
    altering the core of your pipelines. This is a very powerful idea and is one of
    ZenML’s key strengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ZenML has some core concepts that you need to understand in order to get the
    best out of the tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: As you might expect given the discussion in the rest of this
    chapter, these are the definitions of the steps in the ML workflow. Pipelines
    consist of “steps” chained together in a specified order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacks**: Configurations specifying the environment and infrastructure that
    the pipeline is to run on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orchestrator**: Within the stack definition, there are two key components,
    the first of which is an orchestrator. Its job is to coordinate the steps in the
    pipeline that are executed on the infrastructure. This could be the default orchestrator
    that comes with the distribution or it could be something like Airflow or the
    Kubeflow orchestrator. Airflow is described in the *Building general pipelines
    with Airflow* section in this chapter and Kubeflow is covered in the *Going with
    the Kubeflow* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Artifact store**: This is the stack component responsible for data and metadata
    storage. ZenML has a series of different compatible artifact stores out of the
    box, specifically AWS S3, Azure Blob Storage, and Google Cloud Storage. The assumption
    here is that the artifact store is really just a storage layer, and nothing too
    complex on top of that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So far, so straightforward. Let’s get on and start setting ZenML up. You can
    install it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also want to use the React dashboard that comes with ZenML, but to
    run this locally you also need to install a different repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'ZenML also comes with a series of existing templates you can leverage, which
    you can install with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then start working with a template by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This will then start a terminal-based wizard to help you generate the ZenML
    template. See *Figure 5.36*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.36: The ZenML template wizard.'
  prefs: []
  type: TYPE_NORMAL
- en: Hit *Enter*; then you will be asked a series of questions to help configure
    the template. Some are shown with their answers in *Figure 5.37*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.37: Providing responses for the ZenML template definition.'
  prefs: []
  type: TYPE_NORMAL
- en: The next series of questions start to get very interesting as we are asked about
    the details of the information we wish to be logged and made visible in the CLI,
    as well as selecting the dataset and model type. Here we will work with the `Wine`
    dataset, again using a `RandomForestClassifier`, as can be seen in *Figure 5.38*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.38: Selecting a model for the ZenML template instantiation.'
  prefs: []
  type: TYPE_NORMAL
- en: ZenML will then start initializing the template for you. You can see that this
    process generates a lot of new files to use, as shown in *Figure 5.39*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.39: File and folder structure generated after using the ZenML template
    generation wizard.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start to explore some of these elements for the ZenML solution. First,
    let’s look at `pipelines/model_training.py`. This is a short script that is there
    to give you a starting point. Omitting the comments in the file, we have the following
    code present:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We can already start to appreciate some of the features that are available in
    ZenML and how it works. First, we see that the use of the `@pipeline` decorator
    signals that the function following will contain the main pipeline logic. We can
    also see that the pipeline is actually written in pure Python syntax; all you
    need is the decorator to make it “Zen.” This is a very powerful feature of ZenML
    as it provides you the flexibility to work as you want but still leverage the
    downstream abstraction we will see soon for deployment targets. The steps inside
    the pipeline are just dummy function calls created when the template was initialized
    to help guide you in what you should develop.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will look at the pipeline steps, which have been defined in the `steps/data_loaders.py`
    and `steps/model_trainers.py` files. In our discussions of these modules, we will
    not discuss the helper classes and utility functions used; these are left for
    the reader to play around with. Instead, we will focus on the pieces that show
    the most important ZenML functionality. Before we do that, let us briefly discuss
    some important ZenML modules that are imports at the top of the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first import brings in `StrEnum` from the `enums` module of ZenML. This
    is a collection of Python enumerations that have been defined to help with specific
    elements of building ZenML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that a Python enumeration (or `enum`) is a collection of members with
    unique values that can be iterated over to return the values in their order of
    definition. You can think of these as somewhere between a class and a dictionary.
    First, in the `data_loaders.py` module, we can see that the first step wraps simple
    logic for pulling in different datasets from `scikit-learn`, depending on the
    parameters supplied. This is a very basic example but can be updated to incorporate
    much more sophisticated behavior like calling out to databases or pulling from
    cloud-hosted object storage. The method looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the output of this function is a pandas DataFrame, and in the language
    of ZenML this is an artifact. The next important step given is data processing.
    The example given in the template looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can see that, here, the processing is relatively standard and will drop `NULL`
    values in the dataset, remove columns we have labeled in the `DataProcessingStepParameters`
    classes (not shown here), and apply some normalization using standard scaling
    – the steps given are in fact identical to applying the `sklearn.preprocessing.StandardScaler`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final method in the data loaders module performs train/test splitting of
    the data, using methods we have already seen in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, moving back into the `steps` folder, we can see that there is also a module
    entitled `model_trainers.py`. At the top of this folder are some more important
    imports we should understand before we proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, we can see that ZenML provides a wrapper to the Python logging
    library and that there are two modules being used here, called `artifacts` and
    `materializers`. These are defined within the template repo and show how you can
    create custom code to work with the artifact store. Specifically, in the `artifacts/model_metadata.py`
    module, there is a class that allows you to store model metadata in a format of
    your choosing for later serialization and deserialization. Once again, all docstrings
    and most imports are omitted for brevity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'In ZenML, materializers are the objects that contain the logic for the serialization
    and deserialization of the artifacts. They define how your pipelines interact
    with the artifact store. When defining materializers, you can create custom code
    but you have to inherit from the `BaseMaterializer` class in order to ensure that
    ZenML knows how to persist and read in data between steps and at the beginning
    and end of pipelines. This is shown below in important code from `materializers/model_metadata.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have discussed all the key pieces of the ZenML template, we want
    to run the pipeline. This is done via runner `run.py` at the utmost level of the
    repository. You can then run the pipeline with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After the pipeline successfully runs (you will see a series of outputs in the
    terminal), you can run the following command to spin up a locally hosted ZenML
    dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you navigate to the URL that is returned as output, usually something
    like `http://127.0.0.1:8237/login`, you will see a home screen like that shown
    in *Figure 5.40*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.40: The ZenML UI login page.'
  prefs: []
  type: TYPE_NORMAL
- en: In the output that gave you the URL is also a default username and password,
    conveniently **default** and a blank. Fill these in and you will see the home
    page shown in *Figure 5.41*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.41: The ZenML UI home page.'
  prefs: []
  type: TYPE_NORMAL
- en: If you then click through into the **Pipelines** section on the left and then
    click the pipeline created by your first run, you will be able to see all of the
    times that it has been run since then. This view is shown in *Figure 5.42*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.42: The pipelines view in the ZenML UI.'
  prefs: []
  type: TYPE_NORMAL
- en: You can then also get really detailed information about the specifics of each
    run by clicking through. This gives you information like a graphical representation
    of the pipeline as a DAG at the time of the run. See *Figure 5.43*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.43: An example DAG for a ZenML pipeline shown in the UI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click through on the pipeline name in any of these views, you can also
    retrieve the configuration of the run at the time of its execution in YAML format,
    which you can download and then use in subsequent pipeline runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.44: An example YAML configuration for a ZenML pipeline run, shown
    in the ZenML UI.'
  prefs: []
  type: TYPE_NORMAL
- en: This has only begun to scratch the surface of what is possible with ZenML, but
    hopefully, you can already see how it is a very flexible way to define and execute
    your ML pipelines. This becomes even more powerful when you leverage its ability
    to deploy the same pipeline across different stacks and different artifact stores.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss another pipelining tool that focuses on
    creating cross-platform compatibility and standardization for your ML pipelines,
    **Kubeflow**.
  prefs: []
  type: TYPE_NORMAL
- en: Going with the Kubeflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kubeflow** is an open-source solution aimed at providing portable methods
    for building end-to-end ML systems. This tool has a particular focus on giving
    developers the ability to quickly create pipelines for data processing, ML model
    training, prediction, and monitoring that are platform agnostic. It does all this
    by leveraging Kubernetes, allowing you to develop your solution on very different
    environments from where you eventually deploy. Kubeflow is agnostic about the
    particular programming and ML frameworks you use, so you can leverage everything
    you like out there in the open-source community but still stitch it together in
    a way you can trust.'
  prefs: []
  type: TYPE_NORMAL
- en: The Kubeflow documentation provides a great wealth of detail on the architecture
    and design principles behind the tool at [https://www.kubeflow.org/docs/](https://www.kubeflow.org/docs/).
    We will focus instead on understanding the most salient points and getting started
    with some practical examples. This will allow you to compare to the other tools
    we have discussed in this chapter and make your own decisions around which to
    take forward in future projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubeflow is a platform that consists of multiple modular components, each one
    playing a role in the ML development lifecycle. Specifically, there are:'
  prefs: []
  type: TYPE_NORMAL
- en: The Jupyter Notebook web app and controller for exploratory data analysis and
    initial modeling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training operators like PyTorch, TFJob, and XGBoost operators, among others,
    to build a variety of models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning and neural network architecture search capabilities using
    Katib.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark operators for data transformation, including an option for AWS EMR clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dashboard for interfacing with your Kubernetes cluster and for managing your
    Kubeflow workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubeflow Pipelines: its own platform for building, running, and managing end-to-end
    ML workflows. This includes an orchestration engine for workflows with multiple
    steps and an SDK for working with your pipelines. You can install Kubeflow Pipelines
    as a standalone platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The installation steps for getting Kubeflow up and running can be quite involved
    and so it is best to look at the official documentation and run the appropriate
    steps for your platform and needs. We will proceed via the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install Kind, a tool that facilitates easy building and running of local Kubernetes
    clusters. On Linux, this is done with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And on MacOS this is done by:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the Kubernetes command-line tool `kubectl`, which allows you to interact
    with your cluster. On Linux, this is done with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or on MacOS:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check this has worked, you can run the following command in the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And you should receive an output like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use Kind to create your local cluster. As the default, the name of the cluster
    will be `kind`, but you can provide your own name as a flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will then see output that is something like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You then have to deploy Kubeflow pipelines to the cluster. The commands for
    doing this have been brought into a script called `deploy_kubeflow_pipelines.zsh`
    in the book’s GitHub repository and it contains the following code (the `PIPELINE_VERSION`
    number can be updated as needed to match your installation):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After running these commands, you can verify that the installation was a success
    through port forwarding and opening the Kubeflow Pipelines UI at `http://localhost:8080/:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This should then give you a landing page like the one shown in *Figure 5.45*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.45: The Kubeflow UI landing page.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have initiated port-forwarding with the previous command, you
    will use this to allow the Kubeflow Pipelines SDK to talk to the cluster via the
    following Python code (note that you cannot do this until you have installed the
    Kubeflow Pipelines SDK, which is covered in the next step):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the Kubeflow Pipelines SDK, run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'To check that everything is in order, you can run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Which gives output that should be similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We are now ready to start building some Kubeflow pipelines. Let’s
    get started with a basic example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now start building out some basic pipelines using the SDK and then we
    can deploy them to our cluster. Let’s assume for the next few steps we are working
    in a file called `pipeline_basic.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import what is known as the KFP **Domain-Specific Language** (**DSL**),
    which is a set of Python packages with various utilities for defining KFP steps.
    We also import the client package for interacting with the cluster. We’ll also
    import several DSL sub-modules that we will use later. An important point to note
    here is that some functionality we will leverage is in fact contained in the `V2`
    of the Kubeflow pipelines SDK and so we will need to import some of those specific
    modules as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to define the steps in the pipeline. These are called “components”
    and are functions wrapped with `dsl` decorators. In this first step, we retrieve
    the Iris dataset and write it to CSV. In the first line, we will use the `dsl`
    decorator and also define what packages need to be installed in the container
    that will run that step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have retrieved a dataset, and remembering what we learned in *Chapter
    3*, *From Model to Model Factory*, we want to feature engineer this data. So,
    we will normalize the data in another component. Most of the code should be self-explanatory,
    but note that we have had to add the `scikit-learn` dependency in the `packages_to_install`
    keyword argument and that we have again had to write the result of the component
    out to a CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now train a K-nearest neighbors classifier on the data. Instead of
    outputting a dataset in this component, we will output the trained model artifact,
    a `.pkl` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now have all the components for the work we want to do, so now we can finally
    bring it together into a Kubeflow pipeline. To do this, we use the `@dsl.pipeline`
    decorator and as an argument to that decorator, we provide the name of the pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final stage is to submit the pipeline to run. This is done by instantiating
    a Kubeflow Pipelines client class and feeding in the appropriate arguments. `<KFP_UI_URL`>
    is the URL for the host of your instance of Kubeflow Pipelines – in this case,
    the one that we got from performing port-forwarding earlier. It is also important
    to note that since we are using several features from the `V2` Kubeflow Pipelines
    API, we should pass in the `kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE` flag
    for the mode argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To build and deploy this pipeline and run it, you can then execute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this last step, you will see the URL of the run is printed to
    the terminal, and should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: If you navigate to that link and the pipeline has successfully run, you should
    see a view in the Kubeflow dashboard showing the steps of the pipeline, with a
    sidebar that allows you to navigate through a series of metadata about your pipeline
    and its run. An example from running the above code is shown in *Figure 5.46*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_05_46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.46: The Kubeflow UI showing the successful run of the training pipeline
    defined in the main text.'
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it, you have now built and run your first Kubeflow pipeline!
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also compile your Kubeflow pipelines to serialized YAML, which can
    then be read by the Kubeflow backend. You would do this by running a command like
    the following, where `pipeline` is the same pipeline object used in the previous
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: One reason to do this is it is then super easy to run the pipeline. You can
    just upload it to the Kubeflow Pipelines UI, or you can send the YAML to the cluster
    programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: As in the *Finding your ZenML* section, we have only begun to scratch the surface
    of this tool and have focused initially on getting to know the basics in a local
    environment. The beauty of Kubeflow being based on Kubernetes is that platform
    agnosticism is very much at its core and so these pipelines can be effectively
    run anywhere that supports containers.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'That although I have presented ZenML and Kubeflow as two different pipelining
    tools, they can actually be viewed as complementary, so much so that ZenML provides
    the ability to deploy Kubeflow pipelines through the use of the ZenML Kubeflow
    orchestrator. This means you can leverage the higher-level abstractions provided
    by ZenML but still get the scaling behavior and robustness of Kubeflow as a deployment
    target. We will not cover the details here but the ZenML documentation provides
    an excellent guide: [https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow](https://docs.zenml.io/stacks-and-components/component-guide/orchestrators/kubeflow).'
  prefs: []
  type: TYPE_NORMAL
- en: The next section will finish the chapter with a brief note on some different
    deployment strategies that you should be aware of when you aim to put all of these
    tools and techniques into practice with real solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting your deployment strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed many of the technical details of ways to take ML solutions
    into production in this chapter. The missing piece, however, is that we have not
    defined how you deal with existing infrastructure and how you introduce your solution
    to real traffic and requests. This is what is defined by your deployment strategy,
    and selecting an appropriate one is an important part of being an ML engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Most deployment strategies are, like many of the concepts in this book, inherited
    from the world of **software engineering** and **DevOps**. Two of the most important
    to know about are listed below with some discussion about when they can be particularly
    useful in an ML context.
  prefs: []
  type: TYPE_NORMAL
- en: '**Blue/green deployments** are deployments where the new version of your software
    runs alongside your existing solution until some predefined criteria are met.
    After this point, you then switch all incoming traffic/requests to the new system
    before decommissioning the old one or leave it there for use as a potential rollback
    solution. The method was originally developed by two developers, Daniel North
    and Jez Humble, who were working on an e-commerce site in 2005\.'
  prefs: []
  type: TYPE_NORMAL
- en: The origin of the name is described in this GitHub Gist, [https://gitlab.com/-/snippets/1846041](https://gitlab.com/-/snippets/1846041),
    but essentially boils down to the fact that any other naming convention they could
    come up with always implied one of the candidate solutions or environments was
    “better” or “worse” than the other, for example with “A and B” or “Green and Red.”
    The strategy has since become a classic.
  prefs: []
  type: TYPE_NORMAL
- en: In an ML engineering context, this is particularly useful in scenarios where
    you want to gather model and solution performance data over a known period of
    time before trusting full deployment. It also helps with giving stakeholders evidence
    that the ML solution will perform as expected “in the wild.” It also plays particularly
    well with batch jobs, as you are just effectively running another batch at the
    same time. This may have some cost implications for you to consider if the job
    is big or complex, or if your production environment is costly to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: The next strategy is known as **canary deployments** and involves a similar
    setup to the blue/green method but involves a more gradual switching of traffic
    between the two solutions. Here the idea is that the new system is deployed and
    receives some percentage of the traffic initially, say 5% or 10%, before stability
    and performance are confirmed, and then the next increment of traffic is added.
    The total always remains at 100% so as the new system gains more traffic, the
    old system receives less. The name originates from the old coal mining technique
    of using canaries as a test of toxicity in the atmosphere in mines. Release the
    canaries and if they survive, all is well. Thankfully, no birds are harmed in
    the usage of this deployment technique. This strategy makes a lot of sense when
    you are able to divide the data you need to score and still get the information
    you need for progression to the next stage. As an example, an ML microservice
    that is called in the backend of a website would fit the bill nicely, as you can
    just gradually change the routing to the new service on your load balancer. This
    may make less sense for large batch jobs, as there may be no natural way to split
    your data into different increments, whereas with web traffic there definitely
    is.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Building an Example ML Microservice*, will show you how to use
    these strategies when building a custom ML endpoint using Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: No matter what deployment strategy you use, always remember that the key is
    to strike the balance between cost-effectiveness, the uptime of your solution,
    and trust in the outputs it produces. If you can do all of these, then you will
    have deployed a winning combination.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed some of the most important concepts when
    it comes to deploying your ML solutions. In particular, we focused on the concepts
    of architecture and what tools we could potentially use when deploying solutions
    to the cloud. We covered some of the most important patterns used in modern ML
    engineering and how these can be implemented with tools such as containers and
    AWS Elastic Container Registry and Elastic Container Service, as well as how to
    create scheduled pipelines in AWS using Managed Workflows for Apache Airflow.
    We also explored how to hook up the MWAA example with GitHub Actions, so that
    changes to your code can directly trigger updates of running services, providing
    a template to use in future CI/CD processes.
  prefs: []
  type: TYPE_NORMAL
- en: We then moved on to a discussion of more advanced pipelining tools to build
    on the discussion in *Chapter 4*, *Packaging Up*. This focused on how to use Apache
    Airflow to build and orchestrate your generic pipelines for running your data
    engineering, ML, and MLOps pipelines. We then moved on to a detailed introduction
    to ZenML and Kubeflow, two powerful tools for developing and deploying ML and
    MLOps pipelines at scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at the question of other ways to scale up
    our solutions so that we can deal with large volumes of data and high-throughput
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  prefs: []
  type: TYPE_IMG
