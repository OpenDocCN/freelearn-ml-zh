<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Object Detection – Features and Descriptors</h1>
                
            
            
                
<p class="calibre2">In the previous chapter, we learned about processing videos and how to perform the operations and algorithms from all of the previous chapters on frames read from cameras or video files. We learned that each video frame can be treated as an individual image, so we can easily use algorithms, such as filtering, on videos in almost the same way as we did with images. After learning how to process videos using algorithms that work on single individual frames, we moved on to learn about video processing algorithms that require a set of consecutive video frames to perform object detection, tracking, and so on. We learned about how to use the magic of the Kalman filter to improve object-tracking results, and ended the chapter by learning about background and foreground extraction.</p>
<p class="calibre2">The object detection (and tracking) algorithms that we learned about in the previous chapter rely heavily on the color of an object, which has proven not to be too reliable, especially if the object and the environment we are working with are not controlled in terms of lighting. We all know that the brightness and color of an object can easily (and sometimes extremely) change under sunlight and moonlight, or if a light of a different color is near the object, such as a red traffic light. These difficulties are the reason why the detection of objects is more reliable when their physical shape and features are used as a basis for object detection algorithms. Obviously, the shape of an image is independent of its color. A circular object will remain circular during the day or night, so an algorithm that is capable of extracting the shape of such an object would be more reliable to be used for detecting that object.</p>
<p class="calibre2">In this chapter, we're going to learn about computer vision algorithms, functions, and classes that can be used to detect and recognize objects using their features. We'll learn about a number of algorithms that can be used for shape extraction and analysis, and then we'll proceed to learning about key-point detection and descriptor-extraction algorithms. We'll also learn how to match descriptors from two images to detect objects of known shapes in an image. In addition to the topics that we just mentioned, this chapter will also include the required functions for proper visualization of key points and matching results.</p>
<p class="calibre2">In this chapter, you'll learn about the following:</p>
<ul class="calibre10">
<li class="calibre11">Template matching for object detection</li>
<li class="calibre11">Detecting contours and using them for shape analysis</li>
<li class="calibre11">Calculating and analyzing contours</li>
<li class="calibre11">Extracting lines and circles using the Hough transformation</li>
<li class="calibre11">Detecting, descripting, and matching features</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Technical requirements</h1>
                
            
            
                
<ul class="calibre10">
<li class="calibre11">An IDE to develop C++ or Python applications</li>
<li class="calibre11">The OpenCV library</li>
</ul>
<p class="calibre2">Refer to <a href="part0030.html#SJGS0-15c05657f8254d318ea883ef10fc67f4" class="calibre9">Chapter 2</a>, <em class="calibre7">Getting Started with OpenCV</em>, for more information about how to set up a personal computer and make it ready for developing computer vision applications using the OpenCV library.</p>
<p class="calibre2">You can use the following URL to download the source code and examples for this chapter:</p>
<p class="calibre2"><a href="https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter07" class="calibre9">https://github.com/PacktPublishing/Hands-On-Algorithms-for-Computer-Vision/tree/master/Chapter07</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Template matching for object detection</h1>
                
            
            
                
<p class="calibre2">Before we start with the shape-analysis and feature-analysis algorithms, we are going to learn about an easy-to-use, extremely powerful method of object detection called <strong class="calibre4">template matching</strong>. Strictly speaking, this algorithm does not fall into the category of algorithms that use any knowledge about the shape of an object, but it uses a previously acquired template image of an object that can be used to extract a template-matching result and consequently objects of known look, size, and orientation. You can use the <kbd class="calibre13">matchTemplate</kbd> function in OpenCV to perform a templating-matching operation. Here's an example that demonstrates the complete usage of the <kbd class="calibre13">matchTemplate</kbd> function:</p>
<pre class="calibre15">Mat object = imread("Object.png"); 
Mat objectGr; 
cvtColor(object, objectGr, COLOR_BGR2GRAY); 
Mat scene = imread("Scene.png"); 
Mat sceneGr; 
cvtColor(scene, sceneGr, COLOR_BGR2GRAY); 
 
TemplateMatchModes method = TM_CCOEFF_NORMED; 
 
Mat result; 
matchTemplate(sceneGr, objectGr, result, method); </pre>
<p class="calibre2"><kbd class="calibre13">method</kbd> must be an entry from the <kbd class="calibre13">TemplateMatchModes</kbd> enum, which can be any of the following values:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">TM_SQDIFF</kbd></li>
<li class="calibre11"><kbd class="calibre13">TM_SQDIFF_NORMED</kbd></li>
<li class="calibre11"><kbd class="calibre13">TM_CCORR</kbd></li>
<li class="calibre11"><kbd class="calibre13">TM_CCORR_NORMED</kbd></li>
<li class="calibre11"><kbd class="calibre13">TM_CCOEFF</kbd></li>
<li class="calibre11"><kbd class="calibre13">TM_CCOEFF_NORMED</kbd></li>
</ul>
<p class="calibre2"/>
<p class="calibre2">For detailed information about each template-matching method, you can refer to the OpenCV documentation. For our practical examples, and to learn how the <kbd class="calibre13">matchTemplate</kbd> function is used in practice, it is important to note that each method will result in a different type of result, and consequently a different interpretation of the result is required, which we'll learn about in this section. In the preceding example, we are trying to detect an object in a scene by using an object image and a scene image. Let's assume the following images are the object (left-hand side) and the scene (right-hand side) that we'll be using:</p>
<div><img src="img/00080.jpeg" class="calibre91"/></div>
<p class="calibre2">The very simple idea in template matching is that we are searching for a point in the scene image on the right-hand side that has the highest possibility of containing the image on the left-hand side, or in other words, the template image. The <kbd class="calibre13">matchTemplate</kbd> function, depending on the method that is used, will provide a probability distribution. Let's visualize the result of the <kbd class="calibre13">matchTemplate</kbd> function to better understand this concept. Another important thing to note is that we can only properly visualize the result of the <kbd class="calibre13">matchTemplate</kbd> function if we use any of the methods ending with <kbd class="calibre13">_NORMED</kbd>, which means they contain a normalized result, otherwise we have to use the normalize method to create a result that contains values in the displayable range of the OpenCV <kbd class="calibre13">imshow</kbd> function. Here is how it can be done:</p>
<pre class="calibre15">normalize(result, result, 0.0, 1.0, NORM_MINMAX, -1); </pre>
<p class="calibre2">This function call will translate all the values in <kbd class="calibre13">result</kbd> to the range of <kbd class="calibre13">0.0</kbd> and <kbd class="calibre13">1.0</kbd>, which can then be properly displayed. Here is how the resulting image will look if it is displayed using the <kbd class="calibre13">imshow</kbd> function:</p>
<div><img src="img/00081.jpeg" class="calibre92"/></div>
<p class="calibre2">As mentioned previously, the result of the <kbd class="calibre13">matchTemplate</kbd> function and how it should be interpreted depends completely on the template matching method that is used. In the case that we use the <kbd class="calibre13">TM_SQDIFF</kbd> or <kbd class="calibre13">TM_SQDIFF_NORMED</kbd> methods for template matching, we need to look for the global minimum point in the result (it is shown using an arrow in the preceding image), which has the highest possibility of containing the template image. Here's how we can find the global minimum point (along with global maximum, and so on) in the template matching result:</p>
<pre class="calibre15">double minVal, maxVal; 
Point minLoc, maxLoc; 
minMaxLoc(result, &amp;minVal, &amp;maxVal, &amp;minLoc, &amp;maxLoc); </pre>
<p class="calibre2">Since the template-matching algorithm works only with objects of a fixed size and orientation, we can assume that a rectangle that has an upper-left point that is equal to the <kbd class="calibre13">minLoc</kbd> point and has a size that equals the template image is the best possible bounding rectangle for our object. We can draw the result on the scene image, for better comparison, using the following sample code:</p>
<pre class="calibre15">Rect rect(minLoc.x, 
          minLoc.y, 
          object.cols, 
          object.rows); 
 
Scalar color(0, 0, 255); 
int thickness = 2; 
rectangle(scene, 
          rect, 
          color, 
          thickness);</pre>
<p class="calibre2">The following image depicts the result of the object detection operation that was performed using the <kbd class="calibre13">matchTemplate</kbd> function:</p>
<div><img src="img/00082.jpeg" class="calibre93"/></div>
<p class="calibre2">If we use <kbd class="calibre13">TM_CCORR</kbd>, <kbd class="calibre13">TM_CCOEFF</kbd>, or their normalized versions, we must use the global maximum point as the point with the highest possibility of containing our template image. The following image depicts the result of the <kbd class="calibre13">TM_CCOEFF_NORMED</kbd> method used with the <kbd class="calibre13">matchTemplate</kbd> function:</p>
<div><img src="img/00083.jpeg" class="calibre94"/></div>
<p class="calibre2">As you can see, the brightest point in the resultant image corresponds to the upper-left point of the template image in the scene image.</p>
<p class="calibre2">Before ending our template matching lesson, let's also note that the width and height of the template matching resultant image is smaller than the scene image. This is because the template matching resultant image can only contain the upper-left point of the template image, so the template image width and height are subtracted from the scene image's width and height to determine the resultant image's width and height in the template-matching algorithm.</p>
<p class="calibre2"/>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Detecting corners and edges</h1>
                
            
            
                
<p class="calibre2">It is not always possible to just compare images pixel-wise and decide whether an object is present in an image or not, or whether an object has the expected shape or not, and many more similar scenarios that we can't even begin to list here. That is why the smarter way of interpreting the contents of an image is to look for meaningful features in it, and then base our interpretation on the properties of those features. In computer vision, a feature is synonymous with a keypoint, so don't be surprised if we use them interchangeably in this book. In fact, the word keypoint is better suited to describe the concept, since the most commonly used features in an image are usually <em class="calibre7">key points</em> in that image where there is a sudden change in color intensity, which can happen in corners and edges of shapes and objects in an image.</p>
<p class="calibre2">In this section, we'll learn about some of the most important and widely used keypoint-detection algorithms, namely the corner- and edge-detection algorithms that are the basis of almost all of the feature-based object detection algorithms we'll be learning about in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Learning the Harris corner-detection algorithm</h1>
                
            
            
                
<p class="calibre2">One of the most well-known corner- and edge-detection algorithms is the Harris corner-detection algorithm, which is implemented in the <kbd class="calibre13">cornerHarris</kbd> function in OpenCV. Here is how this function is used:</p>
<pre class="calibre15">Mat image = imread("Test.png"); 
cvtColor(image, image, COLOR_BGR2GRAY); 
 
Mat result; 
int blockSize = 2; 
int ksize = 3; 
double k = 1.0; 
cornerHarris(image, 
             result, 
             blockSize, 
             ksize, 
             k);</pre>
<p class="calibre2"/>
<p class="calibre2"><kbd class="calibre13">blockSize</kbd> determines the width and height of the square block over which the Harris corner-detection algorithm will calculate a 2 x 2 gradient-covariance matrix. <kbd class="calibre13">ksize</kbd> is the kernel size of the Sobel operator internally used by the Harris algorithm. The preceding example demonstrates one of the most commonly used sets of Harris algorithm parameters, but for more detailed information about the Harris corner-detection algorithm and its internals mathematics, you can refer to the OpenCV documentation. It's important to note that the <kbd class="calibre13">result</kbd> object from the preceding example code is not displayable unless it is normalized using the following example code:</p>
<pre class="calibre15">normalize(result, result, 0.0, 1.0, NORM_MINMAX, -1); </pre>
<p class="calibre2">Here is the result of the Harris corner-detection algorithm from the preceding example, when normalized and displayed using the OpenCV <kbd class="calibre13">imshow</kbd> function:</p>
<div><img src="img/00084.jpeg" class="calibre95"/></div>
<p class="calibre2">The OpenCV library includes another famous corner-detection algorithm, called <strong class="calibre4">Good Features to Track</strong> (<strong class="calibre4">GFTT</strong>). You can use the <kbd class="calibre13">goodFeaturesToTrack</kbd> function in OpenCV to use the GFTT algorithm to detect corners, as seen in the following example:</p>
<pre class="calibre15">Mat image = imread("Test.png"); 
Mat imgGray; 
cvtColor(image, imgGray, COLOR_BGR2GRAY); 
 
vector&lt;Point2f&gt; corners; 
int maxCorners = 500; 
double qualityLevel = 0.01; 
double minDistance = 10; 
Mat mask; 
int blockSize = 3; 
int gradientSize = 3; 
bool useHarrisDetector = false; 
double k = 0.04; 
goodFeaturesToTrack(imgGray, 
                    corners, 
                    maxCorners, 
                    qualityLevel, 
                    minDistance, 
                    mask, 
                    blockSize, 
                    gradientSize, 
                    useHarrisDetector, 
                    k); </pre>
<p class="calibre2">As you can see, this function requires a single-channel image, so, before doing anything else, we have converted our BGR image to grayscale. Also, this function uses the <kbd class="calibre13">maxCorners</kbd> value to limit the number of detected corners based on how strong they are as candidates, and setting <kbd class="calibre13">maxCorners</kbd> to a negative value or to zero means all detected corners should be returned, which is not a good idea if you are looking for the best corners in an image, so make sure you set a reasonable value for this based on the environment in which you'll be using it. <kbd class="calibre13">qualityLevel</kbd> is the internal threshold value for accepting detected corners. <kbd class="calibre13">minDistance</kbd> is the minimum allowed distance between returned corners. This is another parameter that is completely dependent on the environment this algorithm will be used in. You have already seen  the remaining parameters in the previous algorithms from this chapter and the preceding one. It's important to note that this function also incorporates the Harris corner-detection algorithm, so, by setting <kbd class="calibre13">useHarrisDetector</kbd> to <kbd class="calibre13">true</kbd>, the resultant features will be calculated using the Harris corner-detection algorithm.</p>
<p class="calibre2">You might have already noticed that the <kbd class="calibre13">goodFeaturesToTrack</kbd> function returns a set of <kbd class="calibre13">Point</kbd> objects (<kbd class="calibre13">Point2f</kbd> to be precise) instead of a <kbd class="calibre13">Mat</kbd> object. The returned <kbd class="calibre13">corners</kbd> vector simply contains the best possible corners detected in the image using the GFTT algorithm, so we can use the <kbd class="calibre13">drawMarker</kbd> function to visualize the results properly, as seen in the following example:</p>
<pre class="calibre15">Scalar color(0, 0, 255); 
MarkerTypes markerType = MARKER_TILTED_CROSS; 
int markerSize = 8; 
int thickness = 2; 
for(int i=0; i&lt;corners.size(); i++) 
{ 
    drawMarker(image, 
               corners[i], 
               color, 
               markerType, 
               markerSize, 
               thickness); 
}</pre>
<p class="calibre2">Here is the result of the preceding example and detecting corners using the <kbd class="calibre13">goodFeaturesToTrack</kbd> function:</p>
<div><img src="img/00085.jpeg" class="calibre96"/></div>
<p class="calibre2">You can also use the <kbd class="calibre13">GFTTDetector</kbd> class to detect corners in a similar way as you did with the <kbd class="calibre13">goodFeaturesToTrack</kbd> function. The difference here is that the returned type is a vector of <kbd class="calibre13">KeyPoint</kbd> objects. Many OpenCV functions and classes use the <kbd class="calibre13">KeyPoint</kbd> class to return various properties of detected keypoints, instead of just a <kbd class="calibre13">Point</kbd> object that corresponds to the location of the keypoint. Let's see what this means with the following:</p>
<pre class="calibre15">Ptr&lt;GFTTDetector&gt; detector =  
    GFTTDetector::create(maxCorners, 
                         qualityLevel, 
                         minDistance, 
                         blockSize, 
                         gradientSize, 
                         useHarrisDetector, 
                         k); 
 
vector&lt;KeyPoint&gt; keypoints; 
detector-&gt;detect(image, keypoints); </pre>
<p class="calibre2">The parameters passed to the <kbd class="calibre13">GFTTDetector::create</kbd> function are no different from the parameters we used with the <kbd class="calibre13">goodFeaturesToTrack</kbd> function. You can also omit all of the given parameters and simply write the following to use the default and optimal values for all parameters:</p>
<pre class="calibre15">Ptr&lt;GFTTDetector&gt; detector = GFTTDetector::create();</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">But let's get back to the <kbd class="calibre13">KeyPoint</kbd> class and the result of the <kbd class="calibre13">detect</kbd> function from the previous example. Recall that we used a loop to go through all of the detected points and draw them on the image. There is no need for this if we use the <kbd class="calibre13">GFTTDetector</kbd> class, since we can use an existing OpenCV function called <kbd class="calibre13">drawKeypoints</kbd> to properly visualize all of the detected keypoints. Here's how this function is used:</p>
<pre class="calibre15">Mat outImg; 
drawKeypoints(image, 
              keypoints, 
              outImg); </pre>
<p class="calibre2">The <kbd class="calibre13">drawKeypoints</kbd> function goes through all <kbd class="calibre13">KeyPoint</kbd> objects in the <kbd class="calibre13">keypoints</kbd> vector and draws them using random colors on <kbd class="calibre13">image</kbd> and saves the result in the <kbd class="calibre13">outImg</kbd> object, which we can then display by calling the <kbd class="calibre13">imshow</kbd> function. The following image is the result of the <kbd class="calibre13">drawKeypoints</kbd> function when it is called using the preceding example code:</p>
<div><img src="img/00086.jpeg" class="calibre97"/></div>
<p class="calibre2">The <kbd class="calibre13">drawKeypoints</kbd> function can be provided with an additional (optional) color parameter in case we want to use a specific color instead of random colors. In addition, we can also provide a flag parameter that can be used to further enhance the visualized result of the detected keypoints. For instance, if the flag is set to <kbd class="calibre13">DRAW_RICH_KEYPOINTS</kbd>, the <kbd class="calibre13">drawKeypoints</kbd> function will also use the size and orientation values in each detected keypoint to visualize more properties of keypoints.</p>
<p>Each <kbd class="calibre29">KeyPoint</kbd> object may contain the following properties, depending on the algorithm used for calculating it:<br class="calibre41"/>
- <kbd class="calibre29">pt</kbd>: A <kbd class="calibre29">Point2f</kbd> object containing the coordinates of the keypoint.<br class="calibre41"/>
- <kbd class="calibre29">size</kbd>: The diameter of the meaningful keypoint neighborhood.<br class="calibre41"/>
- <kbd class="calibre29">angle</kbd>: The orientation of the keypoint in degrees, or -1 if not applicable.<br class="calibre41"/>
- <kbd class="calibre29">response</kbd>: The strength of the keypoint determined by the algorithm.<br class="calibre41"/>
- <kbd class="calibre29">octave</kbd>: The octave or pyramid layer from which the keypoint was extracted. Using octaves allows us to deal with keypoints detected from the same image but in different scales. Algorithms that set this value usually require an input octave parameter, which is used to define the number of octaves (or scales) of an image that is used to extract keypoints.<br class="calibre41"/>
- <kbd class="calibre29">class_id</kbd>: This integer parameter can be used to group keypoints, for instance, when keypoints belong to a single object, they can have the same optional <kbd class="calibre29">class_id</kbd> value.</p>
<p class="calibre2">In addition to Harris and GFTT algorithms, you can also use the FAST corner-detection algorithm using the <kbd class="calibre13">FastFeatureDetector</kbd> class, and the AGAST corner-detection algorithm (<strong class="calibre4">Adaptive and Generic Corner Detection Based on the Accelerated Segment Test</strong>) using the <kbd class="calibre13">AgastFeatureDetector</kbd> class, quite similar to how we used the <kbd class="calibre13">GFTTDetector</kbd> class. It's important to note that all of these classes belong to the <kbd class="calibre13">features2d</kbd> module in the OpenCV library and they are all subclasses of the <kbd class="calibre13">Feature2D</kbd> class, therefore all of them contain a static <kbd class="calibre13">create</kbd> function that creates an instance of their corresponding classes and a <kbd class="calibre13">detect</kbd> function that can be used to extract the keypoints from an image.</p>
<p class="calibre2">Here is an example code demonstrating the usage of <kbd class="calibre13">FastFeatureDetector</kbd> using all of its default parameters:</p>
<pre class="calibre15">int threshold = 10; 
bool nonmaxSuppr = true; 
int type = FastFeatureDetector::TYPE_9_16; 
Ptr&lt;FastFeatureDetector&gt; fast = 
        FastFeatureDetector::create(threshold, 
                                    nonmaxSuppr, 
                                    type); 
 
vector&lt;KeyPoint&gt; keypoints; 
fast-&gt;detect(image, keypoints);</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Try increasing the <kbd class="calibre13">threshold</kbd> value if too many corners are detected. Also, make sure to check out the OpenCV documentation for more information about the <kbd class="calibre13">type</kbd> parameter used in the <kbd class="calibre13">FastFeatureDetector</kbd> class. As mentioned previously, you can simply omit all of the parameters in the preceding example code to use the default values for all parameters.</p>
<p class="calibre2">Using the <kbd class="calibre13">AgastFeatureDetector</kbd> class is extremely similar to using <kbd class="calibre13">FastFeatureDetector</kbd>. Here is an example:</p>
<pre class="calibre15">int threshold = 10; 
bool nonmaxSuppr = true; 
int type = AgastFeatureDetector::OAST_9_16; 
Ptr&lt;AgastFeatureDetector&gt; agast = 
        AgastFeatureDetector::create(threshold, 
                                     nonmaxSuppr, 
                                     type); 
 
vector&lt;KeyPoint&gt; keypoints; 
agast-&gt;detect(image, keypoints); </pre>
<p class="calibre2">Before moving on to edge-detection algorithms, it's worth noting that OpenCV also contains the <kbd class="calibre13">AGAST</kbd> and <kbd class="calibre13">FAST</kbd> functions, which can be employed to directly use their corresponding algorithms and avoid dealing with creating an instance to use them; however, using the class implementation of these algorithms has the huge advantage of switching between algorithms using polymorphism. Here's a simple example that demonstrates how we can use polymorphism to benefit from the class implementations of corner-detection algorithms:</p>
<pre class="calibre15">Ptr&lt;Feature2D&gt; detector; 
switch (algorithm) 
{ 
 
case 1: 
    detector = GFTTDetector::create(); 
    break; 
 
case 2: 
    detector = FastFeatureDetector::create(); 
    break; 
 
case 3: 
    detector = AgastFeatureDetector::create(); 
    break; 
 
default: 
    cout &lt;&lt; "Wrong algorithm!" &lt;&lt; endl; 
    return 0; 
 
} 
 
vector&lt;KeyPoint&gt; keypoints; 
detector-&gt;detect(image, keypoints); </pre>
<p class="calibre2"><kbd class="calibre13">algorithm</kbd>, in the preceding example, is an integer value that can be set at run-time and will change the type of the corner-detection algorithm assigned to the <kbd class="calibre13">detector</kbd> object, which has the <kbd class="calibre13">Feature2D</kbd> type, or in other words, the base class of all corner-detection algorithms.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Edge-detection algorithms</h1>
                
            
            
                
<p class="calibre2">Now that we've gone through corner-detection algorithms, let's take a look at edge-detection algorithms, which are crucial when it comes to shape analysis in computer vision. OpenCV contains a number of algorithms that can be used to extract edges from images. The first edge-detection algorithm that we're going to learn about is called the <strong class="calibre4">line-segment-detection algorithm</strong>, and it can be performed by using the <kbd class="calibre13">LineSegmentDetector</kbd> class, as seen in the following example:</p>
<pre class="calibre15">Mat image = imread("Test.png"); 
Mat imgGray; 
cvtColor(image, imgGray, COLOR_BGR2GRAY); 
 
Ptr&lt;LineSegmentDetector&gt; detector = createLineSegmentDetector(); 
 
vector&lt;Vec4f&gt; lines; 
detector-&gt;detect(imgGray, 
                 lines); </pre>
<p class="calibre2">As you can see, the <kbd class="calibre13">LineSegmentDetector</kbd> class requires a single-channel image as the input and produces a <kbd class="calibre13">vector</kbd> of lines. Each line in the result is <kbd class="calibre13">Vec4f</kbd>, or four floating-point values that represent <em class="calibre7">x1</em>, <em class="calibre7">y1</em>, <em class="calibre7">x2</em>, and <em class="calibre7">y2</em> values, or in other words, the coordinates of the two points that form each line. You can use the <kbd class="calibre13">drawSegments</kbd> function to visualize the result of the <kbd class="calibre13">detect</kbd> function of the <kbd class="calibre13">LineSegmentDetector</kbd> class, as seen in the following example:</p>
<pre class="calibre15">Mat result(image.size(), 
           CV_8UC3, 
           Scalar(0, 0, 0)); 
 
detector-&gt;drawSegments(result, 
                       lines); </pre>
<p class="calibre2">To have more control over how the resultant lines are visualized, you might want to manually draw the lines vector, as seen in the following example:</p>
<pre class="calibre15">Mat result(image.size(), 
           CV_8UC3, 
           Scalar(0, 0, 0)); 
 
Scalar color(0,0,255); 
int thickness = 2; 
for(int i=0; i&lt;lines.size(); i++) 
{ 
    line(result, 
         Point(lines.at(i)[0], 
            lines.at(i)[1]), 
         Point(lines.at(i)[2], 
            lines.at(i)[3]), 
         color, 
         thickness); 
} </pre>
<p class="calibre2">The following image demonstrates the result of the line-segment-detection algorithm that was used in the preceding example codes:</p>
<div><img src="img/00087.jpeg" class="calibre98"/></div>
<p class="calibre2">For more details about how to customize the behavior of the <kbd class="calibre13">LineSegmentDetector</kbd> class, make sure to view the documentation of <kbd class="calibre13">createLineSegmentDetector</kbd> and its parameters. In our example, we simply omitted all of its input parameters and used the <kbd class="calibre13">LineSegmentDetector</kbd> class with the default values set for its parameters.</p>
<p class="calibre2">Another function of the <kbd class="calibre13">LineSegmentDetector</kbd> class is comparing two sets of lines to find the number of non-overlapping pixels, and at the same time drawing the result of the comparison on an output image for visual comparison. Here's an example:</p>
<pre class="calibre15">vector&lt;Vec4f&gt; lines1, lines2; 
detector-&gt;detect(imgGray1, 
                 lines1); 
 
detector-&gt;detect(imgGray2, 
                 lines2); 
 
Mat resultImg(imageSize, CV_8UC3, Scalar::all(0)); 
int result = detector-&gt;compareSegments(imageSize, 
                                       lines1, 
                                       lines2, 
                                       resultImg); </pre>
<p class="calibre2">In the preceding code, <kbd class="calibre13">imageSize</kbd> is a <kbd class="calibre13">Size</kbd> object that contains the size of the input image where the lines were extracted from. The result is an integer value that contains the result of the comparison function, or the <kbd class="calibre13">compareSegments</kbd> function, which will be zero in the case of the complete overlapping of pixels.</p>
<p class="calibre2">The next edge-detection algorithm is probably one of the most widely used and cited edge-detection algorithms in computer vision, called the <strong class="calibre4">Canny algorithm</strong>, which has a function of the same name in OpenCV. The biggest advantage of the <kbd class="calibre13">Canny</kbd> function is the simplicity of its input parameters. Let's first see an example of how it's used, and then walk through its details:</p>
<pre class="calibre15">Mat image = imread("Test.png"); 
 
double threshold1 = 100.0; 
double threshold2 = 200.0; 
int apertureSize = 3; 
bool L2gradient = false; 
Mat edges; 
Canny(image, 
      edges, 
      threshold1, 
      threshold2, 
      apertureSize, 
      L2gradient); </pre>
<p class="calibre2">The threshold values (<kbd class="calibre13">threshold1</kbd> and <kbd class="calibre13">threshold2</kbd>) are the lower and higher bound values for thresholding the input image. <kbd class="calibre13">apertureSize</kbd> is the internal Sobel operator aperture size, and <kbd class="calibre13">L2gradient</kbd> is used to enable or disable the more accurate L2 norm when calculating the gradient image. The result of the <kbd class="calibre13">Canny</kbd> function is a grayscale image that contains white pixels where edges are detected and black pixels for the rest of the pixels. This makes the result of the <kbd class="calibre13">Canny</kbd> function a suitable mask wherever such a mask is needed, or, as you'll see later on, a suitable set of points to extract contours from.</p>
<p class="calibre2">The following image depicts the result of the <kbd class="calibre13">Canny</kbd> function used in the previous example:</p>
<div><img src="img/00088.jpeg" class="calibre99"/></div>
<p class="calibre2">As we mentioned before, the result of the <kbd class="calibre13">Canny</kbd> function is suitable to use as the input to algorithms that require a binary image, or in other words, a grayscale image containing only absolute black and absolute white pixel values. The next algorithm that we'll learn about is one where the result of a previous <kbd class="calibre13">Canny</kbd> function must be used as the input, and it is called the <strong class="calibre4">Hough transformation</strong>. The Hough transformation can be used to extract lines from an image, and it is implemented in a function called <kbd class="calibre13">HoughLines</kbd> in the OpenCV library.</p>
<p class="calibre2">Here is a complete example that demonstrates how the <kbd class="calibre13">HoughLines</kbd> function is used in practice:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Call the <kbd class="calibre13">Canny</kbd> function to detect edges in the input image, as seen here:</li>
</ol>
<pre class="calibre30">Mat image = imread("Test.png"); 
 
double threshold1 = 100.0; 
double threshold2 = 200.0; 
int apertureSize = 3; 
bool L2gradient = false; 
Mat edges; 
Canny(image, 
      edges, 
      threshold1, 
      threshold2, 
      apertureSize, 
      L2gradient); </pre>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">Call the <kbd class="calibre13">HoughLines</kbd> function to extract lines from the detected edges:</li>
</ol>
<pre class="calibre30">vector&lt;Vec2f&gt; lines; 
double rho = 1.0; // 1 pixel, r resolution 
double theta = CV_PI / 180.0; // 1 degree, theta resolution 
int threshold = 100; // minimum number of intersections to "detect" a line 
HoughLines(edges, 
           lines, 
           rho, 
           theta, 
           threshold); </pre>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">Use the following code to extract points in the standard coordinate system, and draw them over the input image:</li>
</ol>
<pre class="calibre30">Scalar color(0,0,255); 
int thickness = 2; 
for(int i=0; i&lt;lines.size(); i++) 
{ 
    float rho = lines.at(i)[0]; 
    float theta = lines.at(i)[1]; 
    Point pt1, pt2; 
    double a = cos(theta); 
    double b = sin(theta); 
    double x0 = a*rho; 
    double y0 = b*rho; 
    pt1.x = int(x0 + 1000*(-b)); 
    pt1.y = int(y0 + 1000*(a)); 
    pt2.x = int(x0 - 1000*(-b)); 
    pt2.y = int(y0 - 1000*(a)); 
    line( image, pt1, pt2, color, thickness); 
} </pre>
<p class="calibre2">The following images depict the result of the preceding example from left to right, starting with the original image, edges detected using the <kbd class="calibre13">Canny</kbd> function, lines detected using the <kbd class="calibre13">HoughLines</kbd> function, and finally the output image:</p>
<div><img src="img/00089.jpeg" class="calibre100"/></div>
<p class="calibre2">To avoid having to deal with the coordinate-system change, you can use the <kbd class="calibre13">HoughLinesP</kbd> function to directly extract the points forming each detected line. Here's an example:</p>
<pre class="calibre15">vector&lt;Vec4f&gt; lines; 
double rho = 1.0; // 1 pixel, r resolution 
double theta = CV_PI / 180.0; // 1 degree, theta resolution 
int threshold = 100; // minimum number of intersections to "detect" a line 
HoughLinesP(edges, 
            lines, 
            rho, 
            theta, 
            threshold); 
 
Scalar color(0,0,255); 
int thickness = 2; 
for(int i=0; i&lt;lines.size(); i++) 
{ 
    line(image, 
         Point(lines.at(i)[0], 
            lines.at(i)[1]), 
         Point(lines.at(i)[2], 
            lines.at(i)[3]), 
         color, 
         thickness); 
} </pre>
<p class="calibre2">The Hough transformation is extremely powerful, and OpenCV contains more variations of the Hough transformation algorithm that we'll leave for you to discover using the OpenCV documentation and online resources. Note that using the Canny algorithm is a prerequisite of the Hough transformation, and, as you'll see in the next section, a prerequisite of many algorithms that deal with the shape of objects in an image.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Contour calculation and analysis</h1>
                
            
            
                
<p class="calibre2">Contours of shapes and objects in an image are an important visual property that can be used to describe and analyze them. Computer vision is no exception, so there are quite a few algorithms in computer vision that can be used to calculate the contours of objects in an image or calculate their area and so on.</p>
<p class="calibre2">The following image depicts two contours that are extracted from two 3D objects:</p>
<div><img src="img/00090.jpeg" class="calibre101"/></div>
<p class="calibre2">OpenCV includes a function, called <kbd class="calibre13">findContours</kbd>, that can be used to extract contours from an image. This function must be provided with a proper binary image that contains the best candidate pixels for contours; for instance, the result of the <kbd class="calibre13">Canny</kbd> function is a good choice. The following example demonstrates the steps required to calculate the contours of an image:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Find the edges using the <kbd class="calibre13">Canny</kbd> function, as seen here:</li>
</ol>
<pre class="calibre30">Mat image = imread("Test.png"); 
Mat imgGray; 
cvtColor(image, imgGray, COLOR_BGR2GRAY); 
 
double threshold1 = 100.0; 
double threshold2 = 200.0; 
int apertureSize = 3; 
bool L2gradient = false; 
Mat edges; 
Canny(image, 
      edges, 
      threshold1, 
      threshold2, 
      apertureSize, 
      L2gradient); </pre>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">Use the <kbd class="calibre13">findContours</kbd> function to calculate the contours using the detected edges. It's worth noting that each contour is a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">Point</kbd> objects, making all contours a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">Point</kbd> objects, as seen here:</li>
</ol>
<pre class="calibre30">vector&lt;vector&lt;Point&gt; &gt; contours; 
int mode = CV_RETR_TREE; 
int method = CV_CHAIN_APPROX_TC89_KCOS; 
findContours(edges, 
             contours, 
             mode, 
             method);</pre>
<p class="calibre2"/>
<p class="calibre31">In the preceding example, the contour-retrieval mode is set to <kbd class="calibre13">CV_RETR_TREE</kbd> and the contour-approximation method is set to <kbd class="calibre13">CV_CHAIN_APPROX_TC89_KCOS</kbd>. Make sure to go through the list of possible modes and methods by yourself, and compare the results to find the best parameters for your use case.</p>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">A common method of visualizing detected contours is by using the <kbd class="calibre13">RNG</kbd> class, or the Random Number Generator class, to generate random colors for each detected contour. The following example demonstrates how you can use the <kbd class="calibre13">RNG</kbd> class in combination with the <kbd class="calibre13">drawContours</kbd> function to properly visualize the result of the <kbd class="calibre13">findContours</kbd> function:</li>
</ol>
<pre class="calibre30">RNG rng(12345); // any random number 
 
Mat result(edges.size(), CV_8UC3, Scalar(0)); 
int thickness = 2; 
for( int i = 0; i&lt; contours.size(); i++ ) 
{ 
    Scalar color = Scalar(rng.uniform(0, 255), 
                          rng.uniform(0,255), 
                          rng.uniform(0,255) ); 
 
    drawContours(result, 
                 contours, 
                 i, 
                 color, 
                 thickness); 
} </pre>
<p class="calibre2">The following image demonstrates the result of the <kbd class="calibre13">Canny</kbd> and <kbd class="calibre13">findContours</kbd> functions:</p>
<div><img src="img/00091.jpeg" class="calibre102"/></div>
<p class="calibre2">Note the different colors in the image on the right-hand side, which correspond to one complete contour detected by using the <kbd class="calibre13">findContours</kbd> function.</p>
<p class="calibre2"/>
<p class="calibre2">After calculating the contours, we can use contour-analysis functions to further modify them or analyze the shape of the object in an image. Let's start with the <kbd class="calibre13">contourArea</kbd> function, which can be used to calculate the area of a given contour. Here is how this function is used:</p>
<pre class="calibre15">double area = contourArea(contour); </pre>
<p class="calibre2">You can use area as a threshold for ignoring the detected contours that do not pass certain criteria. For example, in the preceding example code where we used the <kbd class="calibre13">drawContours</kbd> function, we could get rid of contours with smaller areas than some predefined threshold value. Here's an example:</p>
<pre class="calibre15">for( int i = 0; i&lt; contours.size(); i++ ) 
{ 
    if(contourArea(contours[i]) &gt; thresholdArea) 
    { 
        drawContours(result, 
                     contours, 
                     i, 
                     color, 
                     thickness); 
    } 
} </pre>
<p class="calibre2">Setting the second parameter of the <kbd class="calibre13">contourArea</kbd> function (which is a Boolean parameter) to <kbd class="calibre13">true</kbd> would result in the orientation being considered in the contour area, which means you can get positive or negative values of the area depending on the orientation of the contour.</p>
<p class="calibre2">Another contour-analysis function that can be quite handy is the <kbd class="calibre13">pointPolygonTest</kbd> function. As you can guess from its name, this function is used to perform a point-in-polygon test, or in other words, a point-in-contour test. Here is how this function is used:</p>
<pre class="calibre15">Point pt(x, y); 
double result = pointPolygonTest(contours[i], Point(x,y), true); </pre>
<p class="calibre2">If the result is zero, it means the test point is right on the edge of the contour. A negative result would mean the test point is outside, and a positive result would mean the test point is inside the contour. The value itself is the distance between the test point and the nearest contour edge.</p>
<p class="calibre2">To be able to check whether a contour is convex or not, you can use the <kbd class="calibre13">isContourConvex</kbd> function, as seen in the following example:</p>
<pre class="calibre15">bool isIt = isContourConvex(contour);</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Being able to compare two contours with each other is probably the most essential algorithm that you'll need when dealing with contour and shape analysis. You can use the <kbd class="calibre13">matchShapes</kbd> function in OpenCV to compare and try to match two contours. Here is how this function is used:</p>
<pre class="calibre15">ShapeMatchModes method = CONTOURS_MATCH_I1; 
double result = matchShapes(cntr1, cntr2, method, 0); </pre>
<p class="calibre2"><kbd class="calibre13">method</kbd> can take any of the following values, while the last parameter must always be set to zero, unless specified by the method used:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">CONTOURS_MATCH_I1</kbd></li>
<li class="calibre11"><kbd class="calibre13">CONTOURS_MATCH_I2</kbd></li>
<li class="calibre11"><kbd class="calibre13">CONTOURS_MATCH_I3</kbd></li>
</ul>
<p class="calibre2">For details about the mathematical difference between the preceding list of contour-matching methods, you can refer to the OpenCV documentation.</p>
<p class="calibre2">Being able to find the boundaries of a contour is the same as being able to correctly localize it, for instance to find a region that can be used for tracking or performing any other computer vision algorithm. Let's assume we have the following image and its single contour detected using the <kbd class="calibre13">findContours</kbd> function:</p>
<div><img src="img/00092.jpeg" class="calibre103"/></div>
<p class="calibre2">Having this contour, we can perform any of the contour- and shape-analysis algorithms that we've learned about. In addition, we can use a number of OpenCV functions to localize the extracted contour. Let's start with the <kbd class="calibre13">boundingRect</kbd> function, which is used to find the minimal upright rectangle (<kbd class="calibre13">Rect</kbd> object) that contains a given point set or contour. Here's how this function is used:</p>
<pre class="calibre15">Rect br = boundingRect(contour);</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The following is the result of drawing the upright rectangle acquired by using <kbd class="calibre13">boundingRect</kbd> in the preceding sample code:</p>
<div><img src="img/00093.jpeg" class="calibre104"/></div>
<p class="calibre2">Similarly, you can use the <kbd class="calibre13">minAreaRect</kbd> function to find the minimal rotated rectangle that contains a given set of points or a contour. Here's an example:</p>
<pre class="calibre15">RotatedRect br = minAreaRect(contour); </pre>
<p class="calibre2">You can use the following code to visualize the resultant rotated rectangle:</p>
<pre class="calibre15">Point2f points[4]; 
br.points(points); 
for (int i=0; i&lt;4; i++) 
    line(image, 
         points[i], 
         points[(i+1)%4], 
         Scalar(0,0,255), 
         2); </pre>
<p class="calibre2">You can draw an ellipse instead, using the <kbd class="calibre13">ellipse</kbd> function, or you can do both, which would result in something similar to the following:</p>
<div><img src="img/00094.jpeg" class="calibre105"/></div>
<p class="calibre2">In addition to algorithms for finding the minimal upright and rotated bounding rectangles of contours, you can also use the <kbd class="calibre13">minEnclosingCircle</kbd> and <kbd class="calibre13">minEnclosingTriangle</kbd> functions to find the minimal bounding circle and rectangle of a given set of points or a contour. Here's an example of how these functions can be used:</p>
<pre class="calibre15">// to detect the minimal bounding circle 
Point2f center; 
float radius; 
minEnclosingCircle(contour, center, radius); 
 
// to detect the minimal bounding triangle 
vector&lt;Point2f&gt; triangle; 
minEnclosingTriangle(contour, triangle); </pre>
<p class="calibre2">There is no end to the list of possible use cases of contours, but we will name just a few of them before moving on to the next section. You can try using contour-detection and shape-analysis algorithms in conjunction with thresholding algorithms or back-projection images, for instance, to make sure your tracking algorithm uses the shape information in addition to the color and intensity values of pixels. You can also use contours to count and analyze shapes of objects on a production line, where the background and the visual environment is more controlled.</p>
<p class="calibre2">The final section of this chapter will teach you how to use feature detection, descriptor extraction, and descriptor-matching algorithms to detect known objects, but with rotation, scale, and even perspective invariance.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Detecting, descripting, and matching features</h1>
                
            
            
                
<p class="calibre2">As we learned earlier in this chapter, features or keypoints can be extracted from images using various feature-extraction (detection) algorithms, most of which rely on detecting points with a significant change in intensity, such as corners. Detecting the right keypoints is the same as being able to correctly determine which parts of an image are helpful in identifying it. But just a keypoint, or in other words the location of a significant point in an image, by itself is not useful. One might argue that the collection of keypoint locations in an image is enough, but even then, another object with a totally different look can have keypoints in the exact same locations in an image, say, by chance.</p>
<p class="calibre2"/>
<p class="calibre2">This is where feature descriptors, or simply descriptors, come into play. A descriptor, as you can guess from the name, is an algorithm-dependent method of describing a feature, for instance, by using its neighboring pixel values, gradients, and so on. There are many different descriptor-extraction algorithms, each one with its own advantages and disadvantages, and going through all of them would not be a fruitful endeavor, especially for a hands-on book, but it's worth noting that most of them simply take a list of keypoints and produce a vector of descriptors. After a set of descriptors are extracted from sets of keypoints, we can use descriptor-matching algorithms to find the matching features from two different images, for instance, an image of an object and a scene where that object exists.</p>
<p class="calibre2">OpenCV contains a large number of feature detectors, descriptor extractors, and descriptor matchers. All feature-detector and descriptor-extractor algorithms in OpenCV are subclasses of the <kbd class="calibre13">Feature2D</kbd> class, and they are located in either the <kbd class="calibre13">features2d</kbd> module, which is included by default in OpenCV packages, or the <kbd class="calibre13">xfeatures2d</kbd> (extra module) module. You should use these algorithms with care and always refer to the OpenCV documentation, since some of them are actually patented and require permission from their owners to be used in commercial projects. The following is a list of some of the main feature-detector and descriptor-extractor algorithms that are included in OpenCV by default:</p>
<ul class="calibre10">
<li class="calibre11"><strong class="calibre1">BRISK</strong> (<strong class="calibre1">Binary Robust Invariant Scalable Keypoints</strong>)</li>
<li class="calibre11"><strong class="calibre1">KAZE</strong></li>
<li class="calibre11"><strong class="calibre1">AKAZE</strong> (<strong class="calibre1">Accelerated KAZE</strong>)</li>
<li class="calibre11"><strong class="calibre1">ORB</strong>, or Oriented <strong class="calibre1">BRIEF</strong> (<strong class="calibre1">Binary Robust Independent Elementary Features</strong>)</li>
</ul>
<p class="calibre2">All of these algorithms are implemented in classes of exactly the same title in OpenCV, and to repeat once more, they are all subclasses of the <kbd class="calibre13">Feature2D</kbd> class. They are extremely simple to use, especially when no parameters are modified. In all of them, you can simply use the static <kbd class="calibre13">create</kbd> method to create an instance of them, call the <kbd class="calibre13">detect</kbd> method to detect the keypoints, and finally call <kbd class="calibre13">computer</kbd> to extract descriptors of the detected keypoints.</p>
<p class="calibre2">As for descriptor-matcher algorithms, OpenCV contains the following matching algorithms by default:</p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre13">FLANNBASED</kbd></li>
<li class="calibre11"><kbd class="calibre13">BRUTEFORCE</kbd></li>
<li class="calibre11"><kbd class="calibre13">BRUTEFORCE_L1</kbd></li>
<li class="calibre11"><kbd class="calibre13">BRUTEFORCE_HAMMING</kbd></li>
<li class="calibre11"><kbd class="calibre13">BRUTEFORCE_HAMMINGLUT</kbd></li>
<li class="calibre11"><kbd class="calibre13">BRUTEFORCE_SL2</kbd></li>
</ul>
<p class="calibre2">You can use the <kbd class="calibre13">DescriptorMatcher</kbd> class, or its subclasses, namely <kbd class="calibre13">BFMatcher</kbd> and <kbd class="calibre13">FlannBasedMatcher</kbd>, to perform various matching algorithms. You simply need to use the static <kbd class="calibre13">create</kbd> method of these classes to create an instance of them, and then use the <kbd class="calibre13">match</kbd> method to match two sets of descriptors.</p>
<p class="calibre2">Let's walk through all of what we've discussed in this section with a complete example, since breaking apart the feature detection, descriptor extraction, and matching is impossible, and they are all parts of a chain of processes that lead to the detection of an object in a scene using its features:</p>
<ol class="calibre14">
<li value="1" class="calibre11">Read the image of the object, and the scene that will be searched for the object, using the following code:</li>
</ol>
<pre class="calibre30">Mat object = imread("object.png"); 
Mat scene = imread("Scene.png"); </pre>
<p class="calibre31">In the following picture, let's assume the image on the left is the object we are looking for, and the image on the right is the scene that contains the object:</p>
<div><img src="img/00095.jpeg" class="calibre106"/></div>
<ol start="2" class="calibre14">
<li value="2" class="calibre11">Extract the keypoints from both of these images, which are now stored in <kbd class="calibre13">object</kbd> and <kbd class="calibre13">scene</kbd>. We can use any of the aforementioned algorithms for feature detection, but let's assume we're using KAZE for our example, as seen here:</li>
</ol>
<pre class="calibre30">Ptr&lt;KAZE&gt; detector = KAZE::create(); 
vector&lt;KeyPoint&gt; objKPs, scnKPs; 
detector-&gt;detect(object, objKPs); 
detector-&gt;detect(scene, scnKPs); </pre>
<ol start="3" class="calibre14">
<li value="3" class="calibre11">We have the keypoints of both the object image and the scene image. We can go ahead and view them using the <kbd class="calibre13">drawKeypoints</kbd> function, as we learned previously in this chapter. Try that on your own, and then use the same <kbd class="calibre13">KAZE</kbd> class to extract descriptors from the keypoints. Here's how it's done:</li>
</ol>
<pre class="calibre30">Mat objDesc, scnDesc; 
detector-&gt;compute(object, objKPs, objDesc); 
detector-&gt;compute(scene, scnKPs, scnDesc); </pre>
<ol start="4" class="calibre14">
<li value="4" class="calibre11"><kbd class="calibre13">objDesc</kbd> and <kbd class="calibre13">scnDesc</kbd> correspond to the descriptors of the keypoints extracted from the object and scene images. As mentioned previously, descriptors are algorithm-dependent, and interpreting the exact values in them requires in-detail knowledge about the specific algorithm that was used to extract them. Make sure to refer to the OpenCV documentation to gain more knowledge about them, however, in this step, we're going to simply use a brute-force matcher algorithm to match the descriptors extracted from both images. Here's how:</li>
</ol>
<pre class="calibre30">Ptr&lt;BFMatcher&gt; matcher = BFMatcher::create(); 
vector&lt;DMatch&gt; matches; 
matcher-&gt;match(objDesc, scnDesc, matches); </pre>
<p class="calibre31">The <kbd class="calibre13">BFMatcher</kbd> class, which is a subclass of the <kbd class="calibre13">DescriptorMatcher</kbd> class, implements the brute-force matching algorithm. The result of descriptor matching is stored in a <kbd class="calibre13">vector</kbd> of <kbd class="calibre13">DMatch</kbd> objects. Each <kbd class="calibre13">DMatch</kbd> object contains all the necessary information for matched features, from the object descriptors to the scene descriptors.</p>
<ol start="5" class="calibre14">
<li value="5" class="calibre11">You can now try to visualize the result of matching by using the <kbd class="calibre13">drawMatches</kbd> function, as seen here:</li>
</ol>
<pre class="calibre30">Mat result; 
drawMatches(object, 
            objKPs, 
            scene, 
            scnKPs, 
            matches, 
            result, 
            Scalar(0, 255, 0), // green for matched 
            Scalar::all(-1), // unmatched color (not used) 
            vector&lt;char&gt;(), // empty mask 
            DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS); </pre>
<p class="calibre31">As you can see, some of the matched features are obviously incorrect, some at the top of the scene image and a few at the bottom:</p>
<div><img src="img/00096.jpeg" class="calibre107"/></div>
<ol start="6" class="calibre14">
<li value="6" class="calibre11">The bad matches can be filtered out by using a threshold on the <kbd class="calibre13">distance</kbd> value of the <kbd class="calibre13">DMatch</kbd> objects. The threshold value depends on the algorithm and the type of image content, but in our example case, and with the KAZE algorithm, a value of <kbd class="calibre13">0.1</kbd> seems to be enough for us. Here's how the thresholding is done to get good matches out of all the matches:</li>
</ol>
<pre class="calibre30">vector&lt;DMatch&gt; goodMatches; 
double thresh = 0.1; 
for(int i=0; i&lt;objDesc.rows; i++) 
{ 
    if(matches[i].distance &lt; thresh) 
        goodMatches.push_back(matches[i]); 
} 
 
if(goodMatches.size() &gt; 0) 
{ 
    cout &lt;&lt; "Found " &lt;&lt; goodMatches.size() &lt;&lt; " good matches."; 
} 
else 
{ 
    cout &lt;&lt; "Didn't find a single good match. Quitting!"; 
    return -1; 
} </pre>
<p class="calibre31">The following image depicts the result of the <kbd class="calibre13">drawMatches</kbd> function on the <kbd class="calibre13">goodMatches</kbd> vector:</p>
<div><img src="img/00097.jpeg" class="calibre108"/></div>
<ol start="7" class="calibre14">
<li value="7" class="calibre11">Obviously, the result of the filtered matches is much better now. We can use the <kbd class="calibre13">findHomography</kbd> function to find the transformation between good matched keypoints from the object image to the scene image. Here's how:</li>
</ol>
<pre class="calibre30">vector&lt;Point2f&gt; goodP1, goodP2; 
for(int i=0; i&lt;goodMatches.size(); i++) 
{ 
    goodP1.push_back(objKPs[goodMatches[i].queryIdx].pt); 
    goodP2.push_back(scnKPs[goodMatches[i].trainIdx].pt); 
} 
Mat homoChange = findHomography(goodP1, goodP2); </pre>
<ol start="8" class="calibre14">
<li value="8" class="calibre11">As we've already seen in the preceding chapters, the result of the <kbd class="calibre13">findHomography</kbd> function can be used to transform a set of points. We can abuse this fact to create four points using the four corners of the object image, and then transform those points using the <kbd class="calibre13">perspectiveTransform</kbd> function to get the location of those points in the scene image. Here is an example:</li>
</ol>
<pre class="calibre30">vector&lt;Point2f&gt; corners1(4), corners2(4); 
corners1[0] = Point2f(0,0); 
corners1[1] = Point2f(object.cols-1, 0); 
corners1[2] = Point2f(object.cols-1, object.rows-1); 
corners1[3] = Point2f(0, object.rows-1); 
perspectiveTransform(corners1, corners2, homoChange);</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="9" class="calibre14">
<li value="9" class="calibre11">The transformed points can be used to draw four lines that localize the detected object in the scene image, as seen here:</li>
</ol>
<pre class="calibre30">line(result, corners2[0], corners2[1], Scalar::all(255), 2); 
line(result, corners2[1], corners2[2], Scalar::all(255), 2); 
line(result, corners2[2], corners2[3], Scalar::all(255), 2); 
line(result, corners2[3], corners2[0], Scalar::all(255), 2); </pre>
<p class="calibre2">It's important to also change the <em class="calibre7">x</em> values of the resultant points to account for the width of the object image, if you are going to draw the four lines that localize the object, over the <kbd class="calibre13">drawMatches</kbd> image result. Here's an example:</p>
<pre class="calibre15">for(int i=0; i&lt;4; i++) 
    corners2[i].x += object.cols; </pre>
<p class="calibre2">The following image depicts the final result of our detection operation:</p>
<div><img src="img/00098.jpeg" class="calibre109"/></div>
<p class="calibre2">Make sure to try the rest of the feature-detection, descriptor-extraction, and matching algorithms by yourself and compare their results. Also, try to measure the time of the calculations for each one. For instance, you might notice that AKAZE is much faster than KAZE, or that BRISK is better suited to some images, while KAZE or ORB is better with others. As mentioned before, the feature-based methods of object detection are much more reliable with scale, rotation, and even perspective change. Try different views of the same objects to figure out the best parameters and algorithms for your own project and use case. For instance, here's another example that demonstrates the rotation and scale invariance of the AKAZE algorithm and brute-force matching:</p>
<div><img src="img/00099.jpeg" class="calibre110"/></div>
<p class="calibre2">Note that the source code used for producing the preceding output is created by using exactly the same set of instructions we went through in this section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="calibre2">We started this chapter by learning about a template-matching algorithm and an object detection algorithm that, despite its popularity, lacks some of the most essential aspects of a proper object detection algorithm, such as scale and rotation invariance; moreover, it's a pure pixel-based object detection algorithm. Building upon that, we learned how to use global maximum- and minimum-detection algorithms to interpret the template-matching algorithm result. Then, we learned about corner- and edge-detection algorithms, or in other words, algorithms that detect points and areas of significance in images. We learned how to visualize them, and then moved on to learn about contour-detection and shape-analysis algorithms. The final section of this chapter included a complete tutorial on how to detect keypoints in an image, extract descriptors from those keypoints, and use matcher algorithms to detect an object in a scene. We're now familiar with a huge set of algorithms that can be used to analyze images based not only on their pixel colors and intensity values, but also their content and existing keypoints.</p>
<p class="calibre2">The final chapter of this book will take us through computer vision and machine learning algorithms in OpenCV and how they are employed to detect objects using a previously existing set of their images, among many other interesting artificial-intelligence-related topics.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Questions</h1>
                
            
            
                
<ol class="calibre14">
<li value="1" class="calibre11">The template-matching algorithm is not scale- and rotation-invariant by itself. How can we make it so for a) double the scale of the template image, and b) a 90-degrees-rotated version of the template image?</li>
<li value="2" class="calibre11">Use the <kbd class="calibre13">GFTTDetector</kbd> class to detect keypoints with the Harris corner-detection algorithm. You can set any values for the corner-detection algorithm.</li>
<li value="3" class="calibre11">The Hough transformation can also be used to detect circles in an image, using the <kbd class="calibre13">HoughCircles</kbd> function. Search for it in the OpenCV documentation and write a program to detect circles in an image.</li>
<li value="4" class="calibre11">Detect and draw the convex contours in an image.</li>
<li value="5" class="calibre11">Use the <kbd class="calibre13">ORB</kbd> class to detect keypoints in two images, extract their descriptors, and match them.</li>
<li value="6" class="calibre11">Which feature-descriptor-matching algorithm is incompatible with the ORB algorithm, and why?</li>
<li value="7" class="calibre11">You can use the following OpenCV functions and the given sample to calculate the time required to run any number of lines of code. Use it to calculate the time it takes for the matching algorithms on your computer:</li>
</ol>
<pre class="calibre30">double freq = getTickFrequency(); 
double countBefore = getTickCount(); 
 
// your code goes here .. 
 
double countAfter = getTickCount(); 
cout &lt;&lt; "Duration: " &lt;&lt; 
          (countAfter - countBefore) / freq &lt;&lt; " seconds"; </pre>


            

            
        
    </body></html>