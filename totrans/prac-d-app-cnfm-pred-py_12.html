<html><head></head><body>
<div id="_idContainer070">
<h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.1.1">12</span></h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.2.1">Multi-Class Conformal Prediction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Welcome to the last chapter of this book, where we delve into the fascinating world of multi-class </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Conformal Prediction</span></strong><span class="koboSpan" id="kobo.5.1">. </span><span class="koboSpan" id="kobo.5.2">This chapter introduces you to various conformal prediction methods that can be effectively applied to multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">classification problems.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will explore the concept of multi-class classification, a common scenario in </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">machine learning</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">ML</span></strong><span class="koboSpan" id="kobo.11.1">), where an instance can belong to one of many classes. </span><span class="koboSpan" id="kobo.11.2">Understanding this problem is the first step toward applying conformal prediction </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">techniques effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">Next, we will investigate the metrics used to evaluate multi-class classification problems. </span><span class="koboSpan" id="kobo.13.2">These metrics provide a quantitative measure of the performance of our models, and understanding them is crucial for effective model evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">and selection.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">Finally, we will learn how to apply conformal prediction to multi-class classification problems. </span><span class="koboSpan" id="kobo.15.2">This section will provide practical insights and techniques to apply directly to your </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">industrial applications.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">By the end of this chapter, you will have gained valuable skills and knowledge in multi-class classification and learned how conformal prediction can be effectively applied to these problems. </span><span class="koboSpan" id="kobo.17.2">So, let’s dive in and start our journey into multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">conformal prediction!</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.21.1">Multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">classification problems</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Metrics for multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">classification problems</span></span></li>
<li><span class="koboSpan" id="kobo.25.1">How conformal prediction can be applied to multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">classification problems</span></span></li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.27.1">Multi-class classification problems</span></h1>
<p><span class="koboSpan" id="kobo.28.1">In ML, classification </span><a id="_idIndexMarker605"/><span class="koboSpan" id="kobo.29.1">problems are ubiquitous. </span><span class="koboSpan" id="kobo.29.2">They involve predicting a discrete class label output for an instance. </span><span class="koboSpan" id="kobo.29.3">While binary classification – predicting one of two possible outcomes – is a common scenario, many real-world problems require predicting more than two classes. </span><span class="koboSpan" id="kobo.29.4">This is where multi-class classification comes </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">into play.</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">Multi-class classification is a problem where an instance can belong to one of many classes. </span><span class="koboSpan" id="kobo.31.2">For example, consider an ML model designed to categorize news articles into topics. </span><span class="koboSpan" id="kobo.31.3">The articles could be classified into categories such as </span><em class="italic"><span class="koboSpan" id="kobo.32.1">Sports</span></em><span class="koboSpan" id="kobo.33.1">, </span><em class="italic"><span class="koboSpan" id="kobo.34.1">Politics</span></em><span class="koboSpan" id="kobo.35.1">, </span><em class="italic"><span class="koboSpan" id="kobo.36.1">Technology</span></em><span class="koboSpan" id="kobo.37.1">, </span><em class="italic"><span class="koboSpan" id="kobo.38.1">Health</span></em><span class="koboSpan" id="kobo.39.1">, and so on. </span><span class="koboSpan" id="kobo.39.2">Each of these categories represents a class, and since there are more than two classes, this is a multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">classification problem.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">It’s important to note that each instance belongs to exactly one class in multi-class classification. </span><span class="koboSpan" id="kobo.41.2">If each instance could belong to multiple classes, it would be a multi-label classification problem, which is a different kind </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">of problem.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Multi-class classification </span><a id="_idIndexMarker606"/><span class="koboSpan" id="kobo.44.1">problems are a staple in ML, and they require a slightly different approach than binary classification problems. </span><span class="koboSpan" id="kobo.44.2">Let’s dive deeper into the intricacies of </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">multi-class classification.</span></span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.46.1">Algorithms for multi-class classification</span></h2>
<p><span class="koboSpan" id="kobo.47.1">Several ML algorithms can handle </span><a id="_idIndexMarker607"/><span class="koboSpan" id="kobo.48.1">multi-class classification problems directly. </span><span class="koboSpan" id="kobo.48.2">These include, but are not limited to, </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.50.1">Decision trees</span></strong><span class="koboSpan" id="kobo.51.1">: Decision tree</span><a id="_idIndexMarker608"/><span class="koboSpan" id="kobo.52.1"> algorithms such as </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">Classification and Regression Trees</span></strong><span class="koboSpan" id="kobo.54.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.55.1">CARTs</span></strong><span class="koboSpan" id="kobo.56.1">) can </span><a id="_idIndexMarker609"/><span class="koboSpan" id="kobo.57.1">naturally handle </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">multi-class classification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Naive Bayes</span></strong><span class="koboSpan" id="kobo.60.1">: Naive Bayes treats </span><a id="_idIndexMarker610"/><span class="koboSpan" id="kobo.61.1">each class as a separate one-versus-all binary classification problem and picks the outcome with the </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">highest probability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.63.1">Neural networks (NNs)</span></strong><span class="koboSpan" id="kobo.64.1">: NNs can be</span><a id="_idIndexMarker611"/><span class="koboSpan" id="kobo.65.1"> designed with an output layer of multiple nodes representing a class. </span><span class="koboSpan" id="kobo.65.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.66.1">softmax</span></strong><span class="koboSpan" id="kobo.67.1"> activation function can then be used to calculate the probability distribution of </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">each class.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.69.1">Many ML algorithms are inherently designed for binary classification between two classes. </span><span class="koboSpan" id="kobo.69.2">Special strategies must be employed to extend these models to multi-class problems with more than two categories. </span><span class="koboSpan" id="kobo.69.3">Two common approaches are one-vs-all </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">and one-vs-one.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">Next, we will explore these one-vs-all and one-vs-one strategies in more detail, including how binary classification outcomes get aggregated to make final multi-class predictions. </span><span class="koboSpan" id="kobo.71.2">We will also discuss evaluating multi-class classifiers using specialized performance metrics suited for problems with more than </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">two categories.</span></span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.73.1">One-vs-all and one-vs-one strategies</span></h2>
<p><span class="koboSpan" id="kobo.74.1">For algorithms that do not natively support multi-class classification, strategies such as one-vs-all (also known as one-vs-rest) and one-vs-one are used </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.76.1">One-vs-all strategy</span></strong><span class="koboSpan" id="kobo.77.1">: For a problem with </span><em class="italic"><span class="koboSpan" id="kobo.78.1">n</span></em><span class="koboSpan" id="kobo.79.1"> classes, </span><em class="italic"><span class="koboSpan" id="kobo.80.1">n</span></em><span class="koboSpan" id="kobo.81.1"> separate binary classification models are trained. </span><span class="koboSpan" id="kobo.81.2">Each model is trained to distinguish instances of one class from instances </span><a id="_idIndexMarker612"/><span class="koboSpan" id="kobo.82.1">of all other classes. </span><span class="koboSpan" id="kobo.82.2">All </span><em class="italic"><span class="koboSpan" id="kobo.83.1">n</span></em><span class="koboSpan" id="kobo.84.1"> models</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.85.1"> are applied for a new instance, and the model that gives the highest confidence score determines the </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">instance’s class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">One-vs-one strategy</span></strong><span class="koboSpan" id="kobo.88.1">: A binary</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.89.1"> classification model is trained for every pair of classes in this strategy. </span><span class="koboSpan" id="kobo.89.2">For </span><em class="italic"><span class="koboSpan" id="kobo.90.1">n</span></em><span class="koboSpan" id="kobo.91.1"> classes, this</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.92.1"> results in </span><em class="italic"><span class="koboSpan" id="kobo.93.1">n(n-1)/2</span></em><span class="koboSpan" id="kobo.94.1"> models. </span><span class="koboSpan" id="kobo.94.2">Each model’s decision contributes to a voting scheme, and the class with the most votes is chosen as the final class of the instance. </span><span class="koboSpan" id="kobo.94.3">For example, for a 4-class problem, 4*3/2 = 6 binary models would be built, one for each pair of classes. </span><span class="koboSpan" id="kobo.94.4">Each model casts a vote for its predicted class, and the class with the most votes across all models is chosen as the final prediction. </span><span class="koboSpan" id="kobo.94.5">So, with four classes, if three models predicted class A, two predicted class B, and one predicted class C, class A would be selected since it received the most votes. </span><span class="koboSpan" id="kobo.94.6">In this way, each model’s decision contributes to a voting scheme to determine the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">predicted class.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.96.1">The following section will discuss the metrics used for evaluating multi-class classification problems. </span><span class="koboSpan" id="kobo.96.2">Understanding these metrics is crucial for assessing the performance of our models and making informed decisions about model selection </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">and optimization.</span></span></p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.98.1">Metrics for multi-class classification problems</span></h1>
<p><span class="koboSpan" id="kobo.99.1">In the multi-class classification field, evaluating models’ performance is as crucial as developing them. </span><span class="koboSpan" id="kobo.99.2">Effective evaluation hinges upon utilizing the right metrics that can accurately measure the </span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.100.1">performance of the multi-class classification models and provide insights for improvement. </span><span class="koboSpan" id="kobo.100.2">This section demystifies the various metrics essential for assessing the performance of multi-class classification models, providing a solid foundation for selecting and employing the right metric for your specific </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">use case.</span></span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.102.1">Confusion matrix</span></h2>
<p><span class="koboSpan" id="kobo.103.1">One of the fundamental metrics for evaluating multi-class classification models is the </span><strong class="bold"><span class="koboSpan" id="kobo.104.1">confusion matrix.</span></strong><span class="koboSpan" id="kobo.105.1"> It provides a visualization of the performance of an algorithm, typically a </span><strong class="bold"><span class="koboSpan" id="kobo.106.1">supervised learning</span></strong><span class="koboSpan" id="kobo.107.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.108.1">SL</span></strong><span class="koboSpan" id="kobo.109.1">) one. </span><span class="koboSpan" id="kobo.109.2">Each </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.110.1">row of the confusion matrix represents the instances of an actual class, while each </span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.111.1">column represents the instances of a predicted </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.112.1">class. </span><span class="koboSpan" id="kobo.112.2">It’s an essential tool for understanding the model’s performance beyond overall accuracy, offering insight into </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">classification errors.</span></span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.114.1">Precision</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.115.1">Precision</span></strong><span class="koboSpan" id="kobo.116.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.117.1">positive predictive value</span></strong><span class="koboSpan" id="kobo.118.1">; </span><strong class="bold"><span class="koboSpan" id="kobo.119.1">PPV</span></strong><span class="koboSpan" id="kobo.120.1"> in short) is a measure that examines the number of true positive</span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.121.1"> predictions among the total positive predictions made by the model. </span><span class="koboSpan" id="kobo.121.2">High precision indicates</span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.122.1"> that the false positive rate is low. </span><span class="koboSpan" id="kobo.122.2">For multi-class classification problems, precision</span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.123.1"> is calculated for each class separately and can be averaged to understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">overall performance.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.125.1">Recall</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.126.1">Recall</span></strong><span class="koboSpan" id="kobo.127.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">sensitivity</span></strong><span class="koboSpan" id="kobo.129.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.130.1">true positive rate</span></strong><span class="koboSpan" id="kobo.131.1">; </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">TPR</span></strong><span class="koboSpan" id="kobo.133.1"> in short) gauges the number of true positive predictions among</span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.134.1"> the actual positives. </span><span class="koboSpan" id="kobo.134.2">It is </span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.135.1">a crucial metric for problems where identifying all actual </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.136.1">positives is essential. </span><span class="koboSpan" id="kobo.136.2">As with precision, recall is calculated for each class and can be averaged for overall </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">performance assessment.</span></span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.138.1">F1 score</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.139.1">F1 score</span></strong><span class="koboSpan" id="kobo.140.1"> is the harmonic mean of </span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.141.1">precision and recall, balancing the two metrics. </span><span class="koboSpan" id="kobo.141.2">It is particularly useful when dealing with imbalanced </span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.142.1">datasets, offering a more holistic view of the model’s performance </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">beyond accuracy.</span></span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.144.1">Macro- and micro-averaged metrics</span></h2>
<p><span class="koboSpan" id="kobo.145.1">In multi-class classification problems, averaging metrics such as precision, recall, and F1 score, commonly known as macro- and micro-averaging, can be done in </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">multiple ways:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.147.1">Macro-averaging</span></strong><span class="koboSpan" id="kobo.148.1"> computes the </span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.149.1">metric independently for each class and then takes the average, treating all </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">classes </span></span><span class="No-Break"><a id="_idIndexMarker629"/></span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">equally</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.152.1">Micro-averaging</span></strong><span class="koboSpan" id="kobo.153.1"> aggregates the </span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.154.1">contributions of all</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.155.1"> classes to compute the </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">average metric</span></span></li>
</ul>
<h2 id="_idParaDest-172"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.157.1">Area Under Curve (AUC-ROC)</span></h2>
<p><span class="koboSpan" id="kobo.158.1">Another important metric is the </span><strong class="bold"><span class="koboSpan" id="kobo.159.1">Area Under the Receiver Operating Characteristic Curve</span></strong><span class="koboSpan" id="kobo.160.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.161.1">AUC-ROC</span></strong><span class="koboSpan" id="kobo.162.1">). </span><span class="koboSpan" id="kobo.162.2">While it is</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.163.1"> primarily used for binary classification problems, it can be extended to</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.164.1"> multi-class classification by considering each class against </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">the rest.</span></span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.166.1">Log loss and its application in measuring calibration of multi-class models</span></h2>
<p><span class="koboSpan" id="kobo.167.1">Log loss, also </span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.168.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">logistic loss</span></strong><span class="koboSpan" id="kobo.170.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.171.1">cross-entropy loss</span></strong><span class="koboSpan" id="kobo.172.1">, is a commonly used loss function for classification </span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.173.1">problems, including</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.174.1"> multi-class classification. </span><span class="koboSpan" id="kobo.174.2">It quantifies the performance of a classification model by measuring the uncertainty in the predictions. </span><span class="koboSpan" id="kobo.174.3">Log loss assigns a penalty for incorrect classifications; the penalty is higher for confidently </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">wrong predictions.</span></span></p>
<h3><span class="koboSpan" id="kobo.176.1">Mathematical representation</span></h3>
<p><span class="koboSpan" id="kobo.177.1">Mathematically, log loss for </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.178.1">multi-class classification can be represented </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.180.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.181.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.182.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.183.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.184.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.185.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.186.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.187.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.188.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.189.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.190.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.191.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.192.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.193.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.194.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.195.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.196.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.197.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.198.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.199.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.200.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.201.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.202.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.203.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.204.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.205.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.206.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.207.1">j</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Function_v-normal"><span class="koboSpan" id="kobo.208.1">log</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.209.1">(</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.210.1">p</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.211.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.212.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.213.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.214.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.215.1">Here, the </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">following apply:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.217.1">N</span></em><span class="koboSpan" id="kobo.218.1"> is the number </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">of observations</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.220.1">M</span></em><span class="koboSpan" id="kobo.221.1"> is the number </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">of classes.</span></span></li>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.223.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.224.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.225.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.226.1">j</span></span><span class="koboSpan" id="kobo.227.1"> is a binary indicator for whether class </span><em class="italic"><span class="koboSpan" id="kobo.228.1">j</span></em><span class="koboSpan" id="kobo.229.1"> is the correct classification for </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">observation i.</span></span></li>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.231.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.232.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.233.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.234.1">j</span></span><span class="koboSpan" id="kobo.235.1"> is the predicted probability that observation </span><em class="italic"><span class="koboSpan" id="kobo.236.1">i</span></em><span class="koboSpan" id="kobo.237.1"> is of </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">class </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.239.1">j</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.241.1">Using log loss to measure calibration</span></h3>
<p><span class="koboSpan" id="kobo.242.1">A calibrated model is one whose predicted probabilities reliably reflect the true likelihood of the predicted outcomes. </span><span class="koboSpan" id="kobo.242.2">Calibration in multi-class classification means that if a model predicts a class with a probability </span><em class="italic"><span class="koboSpan" id="kobo.243.1">p</span></em><span class="koboSpan" id="kobo.244.1">, then that class should occur about </span><em class="italic"><span class="koboSpan" id="kobo.245.1">p</span></em><span class="koboSpan" id="kobo.246.1"> percent of the time among all instances where that class is predicted with </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">probability </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.248.1">p</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">.</span></span></p>
<h4><span class="koboSpan" id="kobo.250.1">Log loss and calibration</span></h4>
<p><span class="koboSpan" id="kobo.251.1">Log loss is an appropriate metric to </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.252.1">assess the calibration of a multi-class classification model because it directly compares the predicted probabilities (the confidence of the predictions) with the actual classes. </span><span class="koboSpan" id="kobo.252.2">A well-calibrated model will have a lower log loss as the predicted probabilities for the actual classes will </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">be higher.</span></span></p>
<h4><span class="koboSpan" id="kobo.254.1">How to evaluate calibration using Log loss</span></h4>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.255.1">Predict the probabilities</span></strong><span class="koboSpan" id="kobo.256.1">: Use your multi-class classification model to predict the probabilities for each class for each observation in a </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">validation dataset</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Compute log loss</span></strong><span class="koboSpan" id="kobo.259.1">: Calculate the log loss using the formula </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">previously shown</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Interpret the result</span></strong><span class="koboSpan" id="kobo.262.1">: A lower log loss value indicates better calibration as it shows that the predicted probabilities are closer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">actual classes</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.264.1">Evaluating the log loss of</span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.265.1"> your multi-class classification model gives you insights into the calibration of the model. </span><span class="koboSpan" id="kobo.265.2">A model with a lower log loss is more calibrated, providing more reliable prediction probability estimates. </span><span class="koboSpan" id="kobo.265.3">Understanding </span><a id="_idIndexMarker640"/><span class="koboSpan" id="kobo.266.1">and using log loss as a metric to measure the calibration is vital to ensure that your multi-class classification model performs optimally in </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">real-world applications.</span></span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.268.1">Brier score and its application in measuring the calibration of multi-class models</span></h2>
<p><span class="koboSpan" id="kobo.269.1">The Brier score, or quadratic loss, is another</span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.270.1"> popular metric used to evaluate the performance of classification models, including multi-class classification problems. </span><span class="koboSpan" id="kobo.270.2">It quantifies the difference between the predicted </span><a id="_idIndexMarker642"/><span class="koboSpan" id="kobo.271.1">probabilities and the actual classes, assigning a lower score to </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">better-calibrated models.</span></span></p>
<h3><span class="koboSpan" id="kobo.273.1">Mathematical representation</span></h3>
<p><span class="koboSpan" id="kobo.274.1">For multi-class classification, the</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.275.1"> Brier score is calculated </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.277.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.278.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.279.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.280.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.281.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.282.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.283.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.284.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.285.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.286.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.287.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.288.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.289.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.290.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.291.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.292.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.293.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.294.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.295.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.296.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.297.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.298.1">M</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.299.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.300.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.301.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.302.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.303.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.304.1">j</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.305.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.306.1">o</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.307.1"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.308.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.309.1">j</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.310.1">)</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.311.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.312.1">2</span></span></span></p>
<p><span class="koboSpan" id="kobo.313.1">Here, the </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">following apply:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.315.1">N</span></em><span class="koboSpan" id="kobo.316.1"> is the number </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">of observations</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.318.1">M</span></em><span class="koboSpan" id="kobo.319.1"> is the number </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">of classes.</span></span></li>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.321.1">y</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.322.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.323.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.324.1">j</span></span><span class="koboSpan" id="kobo.325.1"> is a binary indicator for whether class j is the correct classification for </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">observation i.</span></span></li>
<li><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.327.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.328.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.329.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.330.1">j</span></span><span class="koboSpan" id="kobo.331.1"> is the</span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.332.1"> predicted probability that observation i is of </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">class j.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.334.1">Using the Brier score to measure calibration</span></h3>
<p><span class="koboSpan" id="kobo.335.1">The Brier score is an effective metric for assessing the calibration of multi-class models as it penalizes the model </span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.336.1">more when there is a larger difference between the predicted probabilities and the actual outcomes. </span><span class="koboSpan" id="kobo.336.2">A well-calibrated model will have a lower Brier score as its predicted probabilities will be closer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">actual outcomes.</span></span></p>
<h3><span class="koboSpan" id="kobo.338.1">How to evaluate calibration using Brier Score</span></h3>
<p><span class="koboSpan" id="kobo.339.1">The Brier score provides a way to quantitatively assess how well calibrated a multi-class classifier’s predicted probabilities are. </span><span class="koboSpan" id="kobo.339.2">Evaluating calibration is key for ensuring reliability in </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">real-world deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">To utilize the Brier score, there are</span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.342.1"> three </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">main steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.344.1">Predict the probabilities</span></strong><span class="koboSpan" id="kobo.345.1">: Use your multi-class classification model to estimate the probabilities for each class for every observation in a </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">validation dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Compute the Brier score</span></strong><span class="koboSpan" id="kobo.348.1">: Calculate the Brier score using the </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">provided formula.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.350.1">Interpret the result</span></strong><span class="koboSpan" id="kobo.351.1">: A lower Brier score indicates better calibration. </span><span class="koboSpan" id="kobo.351.2">It signifies that the model’s predicted probabilities are more aligned with the actual outcomes, thus making the model </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">more reliable.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.353.1">In essence, employing the Brier score to evaluate the calibration of your multi-class model helps ensure the reliability of your model’s probability estimates. </span><span class="koboSpan" id="kobo.353.2">A lower Brier score, reflecting smaller differences between predicted and actual probabilities, indicates a well-calibrated model, enhancing the model’s trustworthiness in real-world applications. </span><span class="koboSpan" id="kobo.353.3">Understanding and utilizing the Brier score as a calibration metric is essential for optimizing the performance of your multi-class classification model in </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">practical scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">Understanding and employing the appropriate metrics is pivotal for evaluating and improving multi-class</span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.356.1"> classification models. </span><span class="koboSpan" id="kobo.356.2">A thorough grasp of these metrics allows for a more nuanced analysis, paving the way for developing robust and efficient multi-class classification models and ensuring their successful deployment in </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">real-world scenarios.</span></span></p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.358.1">How conformal prediction can be applied to multi-class classification problems</span></h1>
<p><span class="koboSpan" id="kobo.359.1">conformal prediction is a powerful framework that can be</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.360.1"> applied to multi-class classification problems. </span><span class="koboSpan" id="kobo.360.2">It provides a way to make predictions with a measure of certainty, which is particularly useful when dealing with</span><a id="_idIndexMarker649"/> <span class="No-Break"><span class="koboSpan" id="kobo.361.1">multiple classes.</span></span></p>
<p><span class="koboSpan" id="kobo.362.1">In the previous chapters, we have already looked at how conformal prediction assigns a </span><em class="italic"><span class="koboSpan" id="kobo.363.1">p</span></em><span class="koboSpan" id="kobo.364.1">-value to each class for a given instance in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">multi-class classification.</span></span></p>
<p><span class="koboSpan" id="kobo.366.1">The p-value represents the confidence level of the prediction for that class. </span><span class="koboSpan" id="kobo.366.2">The higher the p-value, the more confident the model is that the instance belongs to </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">that class.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">The procedure for applying conformal prediction to multi-class classification is </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.370.1">Calibration</span></strong><span class="koboSpan" id="kobo.371.1">: A portion of the training data, known as the calibration set, is set aside. </span><span class="koboSpan" id="kobo.371.2">The model is trained on the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">remaining data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Prediction</span></strong><span class="koboSpan" id="kobo.374.1">: For each class, the model predicts class scores. </span><span class="koboSpan" id="kobo.374.2">The conformity score, which measures how well the prediction conforms to the actual outcomes in the calibration set, </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">is calculated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.376.1">P-value calculation</span></strong><span class="koboSpan" id="kobo.377.1">: For a new instance, the model calculates a nonconformity score for each class. </span><span class="koboSpan" id="kobo.377.2">The p-value for each class is then calculated as the proportion of instances in the calibration set with a higher </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">nonconformity</span></span><span class="No-Break"><span class="koboSpan" id="kobo.379.1"> score.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.380.1">Output</span></strong><span class="koboSpan" id="kobo.381.1">: The model outputs the predicted class labels along with their p-values. </span><span class="koboSpan" id="kobo.381.2">The classes are ranked by their p-values, providing a measure of confidence for </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">each prediction.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.383.1">Applying conformal prediction to multi-class classification problems offers </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">several benefits:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Confidence measures</span></strong><span class="koboSpan" id="kobo.386.1">: conformal prediction provides a measure of confidence (the p-value) for each prediction, which can be very useful in </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">decision-making processes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.388.1">Validity</span></strong><span class="koboSpan" id="kobo.389.1">: conformal prediction offers a theoretical guarantee of validity, meaning that the error rate of the predictions will be close to the significance level set by </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">the user</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.391.1">Efficiency</span></strong><span class="koboSpan" id="kobo.392.1">: conformal prediction is computationally efficient and can be applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">large datasets</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Versatility</span></strong><span class="koboSpan" id="kobo.395.1">: conformal prediction can</span><a id="_idIndexMarker650"/><span class="koboSpan" id="kobo.396.1"> be used with any ML algorithm, making it a versatile tool for multi-class </span><a id="_idIndexMarker651"/><span class="No-Break"><span class="koboSpan" id="kobo.397.1">classification problems</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.398.1">The next section will examine how Venn-ABERS predictors can be applied to multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">classification problems.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.400.1">Multi-class probabilistic classification using inductive and cross-Venn-ABERS predictors</span></h2>
<p><span class="koboSpan" id="kobo.401.1">Venn-ABERS is a conformal prediction method developed by Vladimir Vovk, Ivan Petej, and Valentina Fedorova (</span><em class="italic"><span class="koboSpan" id="kobo.402.1">Large-scale probabilistic predictors with and without guarantees of validity</span></em><span class="koboSpan" id="kobo.403.1">, </span><a href="https://papers.nips.cc/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html"><span class="koboSpan" id="kobo.404.1">https://papers.nips.cc/paper/2015/hash/a9a1d5317a33ae8cef33961c34144f84-Abstract.html</span></a><span class="koboSpan" id="kobo.405.1">) to address the limitations of classical calibrators such as Platt scaling and isotonic regression. </span><span class="koboSpan" id="kobo.405.2">It guarantees</span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.406.1"> mathematical validity, regardless of the data distribution, dataset size, or underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">classification model.</span></span></p>
<p><span class="koboSpan" id="kobo.408.1">Venn-ABERS predictors work by fitting</span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.409.1"> isotonic regression twice, assuming that each test object can have both the label </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">0</span></strong><span class="koboSpan" id="kobo.411.1"> and the label </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">1</span></strong><span class="koboSpan" id="kobo.413.1">. </span><span class="koboSpan" id="kobo.413.2">This results in two probabilities, </span><em class="italic"><span class="koboSpan" id="kobo.414.1">p0</span></em><span class="koboSpan" id="kobo.415.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.416.1">p1</span></em><span class="koboSpan" id="kobo.417.1">, for each test object, representing </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.418.1">probabilities of the object belonging to class 1. </span><span class="koboSpan" id="kobo.418.2">These probabilities create a prediction interval for the probability of class 1, with mathematical guarantees that the actual probability</span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.419.1"> falls within </span><span class="No-Break"><span class="koboSpan" id="kobo.420.1">this interval.</span></span></p>
<p><span class="koboSpan" id="kobo.421.1">The Venn-ABERS prediction is a multi-predictor, and the width of the interval (p0, p1) contains valuable information about the classification confidence. </span><span class="koboSpan" id="kobo.421.2">In critical situations, the Venn-ABERS predictor outputs accurate and well-calibrated probabilities and issues an “alert” by widening the (p0, p1) interval. </span><span class="koboSpan" id="kobo.421.3">This alert indicates that the decision-making process should consider the </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">increased uncertainty.</span></span></p>
<p><span class="koboSpan" id="kobo.423.1">The probabilities can be combined into a single value using </span><em class="italic"><span class="koboSpan" id="kobo.424.1">p = p1 / (1 - p0 + p1)</span></em><span class="koboSpan" id="kobo.425.1"> for practical decision-making purposes. </span><span class="koboSpan" id="kobo.425.2">This combined probability of class 1, </span><em class="italic"><span class="koboSpan" id="kobo.426.1">p</span></em><span class="koboSpan" id="kobo.427.1">, can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">decision-making tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.429.1">The research paper by Valery Manokhin, </span><em class="italic"><span class="koboSpan" id="kobo.430.1">Multi-class probabilistic classification using inductive and cross Venn–Abers predictors</span></em><span class="koboSpan" id="kobo.431.1">, introduces a method for adapting Venn-ABERS predictors for multi-class classification. </span><span class="koboSpan" id="kobo.431.2">You can access the paper </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">here: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">http://proceedings.mlr.press/v60/manokhin17a/manokhin17a.pdf</span></span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.435.1">Experimental results demonstrate</span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.436.1"> that the proposed multi-class predictors outperform uncalibrated and existing</span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.437.1"> classical calibration methods in terms of accuracy, indicating potential substantial </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.438.1">advancements in multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">probabilistic classification.</span></span></p>
<p><span class="koboSpan" id="kobo.440.1">For practitioners and </span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.441.1">researchers eager to employ this technique, a Python implementation of the fast Venn-ABERS predictor for binary classification is accessible on GitHub (</span><em class="italic"><span class="koboSpan" id="kobo.442.1">Multi-class probabilistic classification using inductive and cross Venn–Abers predictors</span></em><span class="koboSpan" id="kobo.443.1">: </span><a href="https://github.com/valeman/Multi-class-probabilistic-classification"><span class="koboSpan" id="kobo.444.1">https://github.com/valeman/Multi-class-probabilistic-classification</span></a><span class="koboSpan" id="kobo.445.1">). </span><span class="koboSpan" id="kobo.445.2">This educational resource offers a hands-on opportunity to explore the details of implementation and practical advantages of utilizing Venn-ABERS predictors in real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">ML scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.447.1">The proposed approach to multi-class probability estimation using </span><strong class="bold"><span class="koboSpan" id="kobo.448.1">inductive</span></strong><span class="koboSpan" id="kobo.449.1"> and</span><strong class="bold"><span class="koboSpan" id="kobo.450.1"> cross-Venn-ABERS predictors</span></strong><span class="koboSpan" id="kobo.451.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.452.1">IVAPs</span></strong><span class="koboSpan" id="kobo.453.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.454.1">CVAPs</span></strong><span class="koboSpan" id="kobo.455.1">) is based on transforming multi-class classifiers into binary classifiers. </span><span class="koboSpan" id="kobo.455.2">In this approach, the binary classifiers are trained to distinguish between each class and all other </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">classes combined.</span></span></p>
<p><span class="koboSpan" id="kobo.457.1">For example, in a three-class problem with classes A, B, and C, three binary classifiers are trained: one to distinguish between A and (B or C), one to distinguish between B and (A or C), and one to distinguish between C and (A or B). </span><span class="koboSpan" id="kobo.457.2">The IVAP is then used to estimate the probability of each class for a given test instance. </span><span class="koboSpan" id="kobo.457.3">The IVAP computes the probability of a class as the fraction of binary classifiers that classify the instance as belonging to </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">that class.</span></span></p>
<p><span class="koboSpan" id="kobo.459.1">The formula for converting pairwise classification scores and pairwise class probabilities uses a method introduced in the paper </span><em class="italic"><span class="koboSpan" id="kobo.460.1">Pairwise Neural Network Classifiers with Probabilistic </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.461.1">Outputs </span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">(</span></span><a href="https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.463.1">https://proceedings.neurips.cc/paper_files/paper/1994/file/210f760a89db30aa72ca258a3483cc7f-Paper.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.464.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.465.1">Specifically, if </span><em class="italic"><span class="koboSpan" id="kobo.466.1">rij</span></em><span class="koboSpan" id="kobo.467.1"> is the class score </span><em class="italic"><span class="koboSpan" id="kobo.468.1">i</span></em><span class="koboSpan" id="kobo.469.1"> over class </span><em class="italic"><span class="koboSpan" id="kobo.470.1">j</span></em><span class="koboSpan" id="kobo.471.1"> from the respective binary model, the estimated probability for class </span><em class="italic"><span class="koboSpan" id="kobo.472.1">i</span></em><span class="koboSpan" id="kobo.473.1"> is calculated </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">as follows.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">The key idea is to first compute </span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.476.1">pairwise probabilities between each pair of classes using the dedicated binary </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.477.1">classifiers. </span><span class="koboSpan" id="kobo.477.2">Then, these pairwise probabilities can be combined to estimate a normalized probability distribution over </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">all classes:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.479.1">p</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.480.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.481.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.482.1"> </span></span><span class="_-----MathTools-_Math_Text"><span class="koboSpan" id="kobo.483.1">PKPD</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.484.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.485.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.486.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.487.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.488.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.489.1">__________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.490.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.491.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.492.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.493.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.494.1">:</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.495.1">j</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.496.1">≠</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.497.1">i</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.498.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.499.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.500.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.501.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.502.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.503.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.504.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.505.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.506.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.507.1">r</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.508.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.509.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.510.1">j</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.511.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.512.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.513.1">k</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.514.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.515.1">2</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.516.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.517.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.518.1">This provides a </span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.519.1">principled approach to converting pairwise binary classification outcomes into</span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.520.1"> class probability estimates suitable for multi-class evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">and calibration.</span></span></p>
<p><span class="koboSpan" id="kobo.522.1">After calculating the probabilities (this technique is dubbed the </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">PKPD</span></strong><span class="koboSpan" id="kobo.524.1"> method), normalizing these values is crucial to ensure their total sum is 1. </span><span class="koboSpan" id="kobo.524.2">These pairwise probabilities can be obtained by applying IVAPs and CVAPs to the pairwise classification scores/probabilities, which are used to calibrate classification scores produced by underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">classification models.</span></span></p>
<p><span class="koboSpan" id="kobo.526.1">In simpler terms, the PKPD method helps convert probabilities from binary comparisons (pairwise probabilities) into multi-class probabilities. </span><span class="koboSpan" id="kobo.526.2">The calculated multi-class probabilities are then used to classify test objects into one of the </span><em class="italic"><span class="koboSpan" id="kobo.527.1">k</span></em><span class="koboSpan" id="kobo.528.1"> possible classes. </span><span class="koboSpan" id="kobo.528.2">This classification enables the computation of metrics, which can be compared across various calibration algorithms to evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">their performance.</span></span></p>
<p><span class="koboSpan" id="kobo.530.1">Now, let’s explore a real-world example to understand how to apply conformal prediction to multi-class classification problems. </span><span class="koboSpan" id="kobo.530.2">The following is a code walk-through that demonstrates </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">this application.</span></span></p>
<p><span class="koboSpan" id="kobo.532.1">Let’s analyze the code from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">Chapter_12.ipynb</span></strong><span class="koboSpan" id="kobo.534.1"> notebook (the code can be found in the book’s GitHub repo at </span><a href="https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_12.ipynb"><span class="koboSpan" id="kobo.535.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_12.ipynb</span></a><span class="koboSpan" id="kobo.536.1">). </span><span class="koboSpan" id="kobo.536.2">The notebook demonstrates the application of conformal prediction to a multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">classification problem.</span></span></p>
<p><span class="koboSpan" id="kobo.538.1">The general steps of multi-class classification are </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">as follows:</span></span></p>
<ol>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.540.1">Data preparation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.542.1">Load </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">the dataset.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.544.1">Divide the dataset into features (</span><em class="italic"><span class="koboSpan" id="kobo.545.1">X</span></em><span class="koboSpan" id="kobo.546.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">target (</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.548.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">).</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.550.1">Split the data into training </span><a id="_idIndexMarker664"/><span class="koboSpan" id="kobo.551.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">test sets.</span></span></li></ol></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.553.1">Model training</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.555.1">Train a</span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.556.1"> classification model on the </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">training data.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.558.1">Use the trained model to make predictions on the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">test data.</span></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.560.1">Applying </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.561.1">conformal prediction</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.563.1">Apply conformal prediction to the </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">trained model.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.565.1">Obtain confidence and credibility measures for </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">the predictions.</span></span></li></ol></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.567.1">Evaluation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.569.1">Evaluate the model using </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">appropriate metrics.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.571.1">Compare the performance of the calibrated model with the </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">original model.</span></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.573.1">Model </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.574.1">evaluation function</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.576.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.577.1">evaluate_model_performance</span></strong><span class="koboSpan" id="kobo.578.1"> function is used to evaluate the performance of</span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.579.1"> different models and </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">calibration methods.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.581.1">It trains the</span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.582.1"> model, makes predictions, and evaluates the performance using various metrics such as accuracy, log loss, and </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">Brier loss.</span></span></li></ol></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.584.1">Calibration</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.586.1">Different calibration methods, such as Platt, isotonic, and others, are applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">the model.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.588.1">The predictions of the calibrated</span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.589.1"> models are evaluated to analyze the </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">performance improvement.</span></span></li></ol></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.591.1">Model </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.592.1">processing loop</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">:</span></span><ol><li class="upper-roman"><span class="koboSpan" id="kobo.594.1">A loop </span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.595.1">processes different models and applies the </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">evaluate_model_performance</span></strong><span class="koboSpan" id="kobo.597.1"> function to </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">each model.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.599.1">The results for </span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.600.1">each model and calibration method are stored and can be analyzed to </span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.601.1">determine the best-performing model and </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">calibration method.</span></span></li></ol></li>
</ol>
<p><span class="koboSpan" id="kobo.603.1"> Let’s summarize the </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">chapter next.</span></span></p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.605.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.606.1">In this book’s final chapter, we explored the intriguing domain of multi-class conformal prediction. </span><span class="koboSpan" id="kobo.606.2">We began by understanding the concept of multi-class classification, a prevalent scenario in ML where an instance can belong to one of many classes. </span><span class="koboSpan" id="kobo.606.3">This understanding is crucial for effectively applying conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">prediction techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">We then delved into the metrics used for evaluating multi-class classification problems. </span><span class="koboSpan" id="kobo.608.2">These metrics quantitatively measure our model’s performance and are vital for effective model evaluation </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">and selection.</span></span></p>
<p><span class="koboSpan" id="kobo.610.1">Finally, we learned how to apply conformal prediction to multi-class classification problems. </span><span class="koboSpan" id="kobo.610.2">This section provided practical insights and techniques to apply to your industrial </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">applications directly.</span></span></p>
<p><span class="koboSpan" id="kobo.612.1">By the end of this chapter, you should have gained valuable skills and knowledge in multi-class classification and how conformal prediction can be effectively applied to these problems. </span><span class="koboSpan" id="kobo.612.2">This knowledge will prove invaluable in your journey as a data scientist, ML engineer, </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">or researcher.</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">We covered different main topics, including multi-class classification problems, where we explored multi-class classification and its importance in ML. </span><span class="koboSpan" id="kobo.614.2">We also discussed the difference between multi-class and multi-label classification problems. </span><span class="koboSpan" id="kobo.614.3">We then investigated the metrics used to evaluate multi-class classification problems. </span><span class="koboSpan" id="kobo.614.4">Understanding these metrics is crucial for assessing the performance of our models and making informed decisions about model selection and optimization. </span><span class="koboSpan" id="kobo.614.5">Finally, we learned how to apply conformal prediction to multi-class classification problems. </span><span class="koboSpan" id="kobo.614.6">This section provided practical insights and techniques to apply directly to your </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">industrial applications.</span></span></p>
<p><span class="koboSpan" id="kobo.616.1">This chapter marks the end of our journey into conformal prediction. </span><span class="koboSpan" id="kobo.616.2">We hope that the knowledge and skills you’ve gained will serve you well in your future endeavors in ML. </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">Happy learning!</span></span></p>
</div>
</body></html>