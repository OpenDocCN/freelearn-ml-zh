<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer170">
<h1 class="chapter-number" id="_idParaDest-81"><a id="_idTextAnchor109"/>5</h1>
<h1 id="_idParaDest-82"><a id="_idTextAnchor110"/>Understanding AutoML Algorithms</h1>
<p>All ML algorithms have a foundation in <strong class="bold">computational statistics</strong>. Computational statistics is the combination<a id="_idIndexMarker438"/> of statistics and computer science where computers are used to compute complex mathematics. This computation is the ML algorithm and the results that we get from it are the predictions. As engineers and scientists working in the field of ML, we are often expected to know the basic logic of ML algorithms. There are plenty of ML algorithms in the AI domain. All of them aim to solve different types of prediction problems. All of them also have their own set of pros and cons. Thus, it became the job of engineers and scientists to find the best ML algorithms that can solve a given prediction problem within the required constraints. This job, however, was eased with the invention of AutoML.</p>
<p>Despite AutoML taking over this huge responsibility of finding the best ML algorithm, it is still our job as engineers and scientists to verify and justify the selection of these algorithms. And to do that, a basic understanding of the ML algorithms is a must. </p>
<p>In this chapter, we shall explore and understand the various ML algorithms that H2O AutoML uses to train models. As mentioned previously, all ML algorithms have a heavy foundation in statistics. Statistics itself is a huge branch of mathematics and is too large to cover in a single chapter. Hence, for the sake of having a basic understanding of how ML algorithms work, we shall explore their inner workings conceptually with basic statistics rather than diving deep into the math. </p>
<p class="callout-heading">Tip</p>
<p class="callout">If you are interested in gaining more knowledge in the field of statistics, then the following link should be a good place to start: <a href="https://online.stanford.edu/courses/xfds110-introduction-statistics">https://online.stanford.edu/courses/xfds110-introduction-statistics</a>.</p>
<p>First, we shall understand what the different types of ML algorithms are and then learn about the workings of these algorithms. We will do this by breaking them down into individual concepts, understanding them, and then building the algorithm back up to understand the big picture.</p>
<p>In this chapter, we are going to cover the following topics:<a id="_idTextAnchor111"/></p>
<ul>
<li>Understanding the different types of ML algorithms<a id="_idTextAnchor112"/></li>
<li>Understanding the Generalized Linear Model algorithm<a id="_idTextAnchor113"/></li>
<li>Understanding the Distributed Random Forest algorithm</li>
<li>Understanding the Gradient Boosting Machine algorithm</li>
<li>Understanding what is Deep Learning</li>
</ul>
<p>So, let’s begin our journey by understanding the different types of ML algorithms.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor114"/>Understanding the different types of ML algorithms</h1>
<p>ML algorithms are designed to solve a specific prediction problem. These prediction problems<a id="_idIndexMarker439"/> can be anything that can provide value if predicted accurately. The differentiating factor between various prediction problems is what value is to be predicted. Is it a simple yes or no value, a range of numbers, or a specific value from a list of potential values, probabilities, or semantics of a text? The field of ML is vast enough to cover the majority, if not all, of such problems in a wide variety of ways.</p>
<p>So, let’s start with understanding the different categories of prediction problems. They are as follows:</p>
<ul>
<li><strong class="bold">Regression</strong>: Regression analysis is a statistical process<a id="_idIndexMarker440"/> that aims to find the<a id="_idIndexMarker441"/> relationship between independent variables, also called features, and<a id="_idIndexMarker442"/> dependent variables, also<a id="_idIndexMarker443"/> called label or response variables, and use that <a id="_idIndexMarker444"/>relationship<a id="_idIndexMarker445"/> to predict future values.</li>
</ul>
<p>Regression problems<a id="_idIndexMarker446"/> are problems that aim to predict<a id="_idIndexMarker447"/> certain continuous numerical values – for example, predicting the price of a car given the car’s brand name, engine size, economy, and electronic features. In such a scenario, the car’s brand name, engine size, economy, and electronic features are the independent variables as their presence is independent of other values, while the car price is the dependent variable whose value is dependent on the other features. Also, the price of the car is a continuous value as it can numerically range anywhere from 0 to 100 million in dollars or any other currency.</p>
<ul>
<li><strong class="bold">Classification</strong>: Classification is a statistical<a id="_idIndexMarker448"/> process that aims to categorize<a id="_idIndexMarker449"/> the label values depending on their relationship to the features into certain classes or categories.</li>
</ul>
<p>Classification problems are problems that aim to predict a certain set of values – for example, predicting if a person is likely to face heart disease, depending on their cholesterol level, weight, exercise levels, heart rate, and family history. Another example would be predicting the rating of a restaurant on Google reviews that ranges from 1-5 stars, depending on the location, food, ambience, and price. </p>
<p>As you can see from these examples, classification problems can be either a <em class="italic">yes</em> or <em class="italic">no</em>, <em class="italic">true</em> or <em class="italic">false</em>, or <em class="italic">1</em> or <em class="italic">0</em> type of classification, or a specific set of classification values, such as those in the Google Review example, where the values can either be 1, 2, 3, 4, or 5. Thus, classification problems can be further divided, as follows:</p>
<ul>
<li><strong class="bold">Binary Classification</strong>: In this type of classification<a id="_idIndexMarker450"/> problem, the predicted<a id="_idIndexMarker451"/> values are binary, meaning they have only two values – that is, <em class="italic">yes</em> or <em class="italic">no</em>, <em class="italic">true</em> or <em class="italic">false</em>, <em class="italic">1</em> or <em class="italic">0</em>.</li>
<li><strong class="bold">Multiclass/Polynomial Classification</strong>: In this type of classification problem, the predicted<a id="_idIndexMarker452"/> values are non-binary, also called polynomial<a id="_idIndexMarker453"/>, in nature, meaning they <a id="_idIndexMarker454"/>have more than two sets of values. For example, classification by age, which involves whole numbers from 1 to 100, or classification by primary colors, which can be red, yellow, or blue.</li>
</ul>
<ul>
<li><strong class="bold">Clustering</strong>: Clustering is a statistical process<a id="_idIndexMarker455"/> that aims to group or divide <a id="_idIndexMarker456"/>certain data points in such a way that data points in a single group have similar characteristics that are different from data points in other groups.</li>
</ul>
<p>Clustering problems are problems that aim to understand similarities within a set of values.</p>
<p>For example, given a set of people who play video games with certain details, such as the hardware they use, the different games they play, and time spent playing those video games, you can categorize people by their favorite game genre. Clustering can be further divided as follows:</p>
<ul>
<li><strong class="bold">Hard Clustering</strong>: In this type of clustering, all<a id="_idIndexMarker457"/> data points either<a id="_idIndexMarker458"/> belong to one or another cluster; they are mutually exclusive.</li>
<li><strong class="bold">Soft Clustering</strong>: In this type of clustering, rather<a id="_idIndexMarker459"/> than assigning a data point to a cluster, the probability<a id="_idIndexMarker460"/> that a data point might belong to a certain cluster is calculated. This opens the likelihood that a data point might belong to multiple clusters at the same time.</li>
</ul>
<ul>
<li><strong class="bold">Association</strong>: Association is a statistical process<a id="_idIndexMarker461"/> that aims to find the probability<a id="_idIndexMarker462"/> that if event A happened, what is the likelihood that event B will happen too? Association problems are based on association rules, which are if-then statements that show the probability of a relationship between different data points.</li>
</ul>
<p>The most common example<a id="_idIndexMarker463"/> of the association problem is <strong class="bold">Market Basket Analysis</strong>. Market Basket Analysis is a prediction problem where, given a user buys a certain product A from the market, what is the probability of the user buying product B, which is related to product A?</p>
<ul>
<li><strong class="bold">Optimization/Control</strong>: <strong class="bold">Control Theory</strong>, <strong class="bold">Optimal Control Theory</strong>, or <strong class="bold">Optimization Problems</strong> is a branch of mathematics<a id="_idIndexMarker464"/> that deals with finding<a id="_idIndexMarker465"/> a certain combination<a id="_idIndexMarker466"/> of values that collectively<a id="_idIndexMarker467"/> optimize a dynamic system. <strong class="bold">Machine Learning Control</strong> (<strong class="bold">MLC</strong>) is a subfield in ML that aims to solve the Optimization Problem<a id="_idIndexMarker468"/> using ML. A good example of MLC is the implementation of ML to optimize traffic on roads using automated cars.</li>
</ul>
<p>Now that we understand the different types of prediction problems, let’s dive into understanding the different types of ML algorithms. The different types of ML algorithms are categorized as follows:</p>
<ul>
<li><strong class="bold">Supervised Learning</strong>: Supervised learning is the ML task<a id="_idIndexMarker469"/> of mapping the relationship between the independent<a id="_idIndexMarker470"/> variables and dependent variables based on previously existing values that are labeled. Labeled data is data that contains information about which of its features are dependent and which features are independent. In supervised learning, we know which feature we want to predict and tag that feature as a label. The ML algorithm will use this information to map the relationships. Using this mapping, we predict the output for new input values. Another way of understanding this problem is that the previously existing values supervise the ML algorithm's learning task.</li>
</ul>
<p>Supervised learning<a id="_idIndexMarker471"/> algorithms are often used to solve regression and classification problems.</p>
<p>Some<a id="_idIndexMarker472"/> examples<a id="_idIndexMarker473"/> of supervised learning<a id="_idIndexMarker474"/> algorithms are <strong class="bold">decision trees</strong>, <strong class="bold">linear regression</strong>, and <strong class="bold">neural networks</strong>.</p>
<ul>
<li><strong class="bold">Unsupervised learning</strong>: As mentioned previously, supervised learning is the ML task of finding patterns<a id="_idIndexMarker475"/> and behaviors from data that is not tagged. In this case, we don’t know<a id="_idIndexMarker476"/> which feature we want to predict, or the feature we want to predict may not even be a part of the dataset. Unsupervised learning helps us predict potential repeating patterns and categorize the set of data using those patterns. Another way of understanding this problem is that there are no labeled values to supervise the ML algorithm learning task; the algorithm learns the patterns and behaviors on its own.</li>
</ul>
<p>Unsupervised learning<a id="_idIndexMarker477"/> algorithms are often used to solve clustering and association problems.</p>
<p>Some examples<a id="_idIndexMarker478"/> of unsupervised<a id="_idIndexMarker479"/> learning algorithms are <strong class="bold">K-means clustering</strong> and <strong class="bold">association rule learning</strong>.</p>
<ul>
<li><strong class="bold">Semi-supervised learning</strong>: Semi-supervised learning falls between supervised learning<a id="_idIndexMarker480"/> and unsupervised learning. It is the ML<a id="_idIndexMarker481"/> task of performing learning on a dataset that is partially labeled. It is used in scenarios where you have a small dataset that is labeled along with a large unlabeled dataset. In real-world<a id="_idIndexMarker482"/> scenarios, labeling large amounts of data is an expensive task as it requires a lot of experimentation and contextual information that is manually interpreted, while unlabeled data is relatively cheap to acquire. Semi-supervised learning often proves efficient in this case as it is good at assuming expected label values from unlabeled datasets while working as efficiently as any supervised learning algorithm.</li>
</ul>
<p>Unsupervised learning algorithms are often used to solve clustering and classification problems.</p>
<p>Some examples<a id="_idIndexMarker483"/> of semi-supervised<a id="_idIndexMarker484"/> learning algorithms are <strong class="bold">generative models</strong> and <strong class="bold">Laplacian regularization</strong>.</p>
<ul>
<li><strong class="bold">Reinforcement learning</strong>: Reinforcement learning is an ML task<a id="_idIndexMarker485"/> that aims to identify the<a id="_idIndexMarker486"/> next correct logical action to take in a given environment to maximize the cumulative reward. In this type of learning, the accuracy of the prediction is calculated after the prediction is made using positive and/or negative reinforcement, which is again fed to the algorithm. This continuous learning of the environment eventually helps the algorithm find the best sequence of steps to take to maximize the reward, thus making the most accurate decision.</li>
</ul>
<p>Reinforcement learning<a id="_idIndexMarker487"/> is often used to solve a mix<a id="_idIndexMarker488"/> of regression, classification, and optimization problems.</p>
<p>Some<a id="_idIndexMarker489"/> examples<a id="_idIndexMarker490"/> of reinforcement<a id="_idIndexMarker491"/> learning algorithms are <strong class="bold">Monte Carlo Methods</strong>, <strong class="bold">Q-Learning</strong>, and <strong class="bold">Deep Q Network</strong>.</p>
<p>The AutoML technology, despite being mature enough to be used commercially, is still in its infancy compared to the vast developments in the field of ML. AutoML may be able to train the best predictive models in the shortest time using little to no human intervention, but its potential is currently limited to only supervised learning. The following diagram summarizes the various types of ML algorithms categorized under the different ML tasks:</p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 5.1 – Types of ML problems and algorithms " height="925" src="image/B17298_05_001.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Types of ML problems and algorithms</p>
<p>Similarly, H2O’s AutoML also focuses on supervised learning and as such, you are often expected to have labeled data that you can feed to it. </p>
<p>ML algorithms <a id="_idIndexMarker492"/>that perform unsupervised learning are often quite sophisticated compared to supervised learning algorithms as there is no ground truth to measure the performance of the model. This goes against the very nature of AutoML, which is very reliant on model performance measurements to automate training and hyperparameter tuning. </p>
<p>So, accordingly, H2O AutoML falls in the domain of supervised ML algorithms, where it trains several supervised ML algorithms to solve regression and classification problems and ranks them based on their performance. In this chapter, we shall focus on these ML algorithms and understand their functionality so that we are well equipped to understand, select, and justify the different models that H2O AutoML trains for a given prediction problem. </p>
<p>With this understanding, let’s start with the first ML algorithm: Generalized Linear Model. </p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor115"/>Understanding the Generalized Linear Model algorithm</h1>
<p><strong class="bold">Generalized Linear Model</strong> (<strong class="bold">GLM</strong>), as its name suggests, is a flexible<a id="_idIndexMarker493"/> way of generalizing linear models. It was formulated by <em class="italic">John Nelder</em> and <em class="italic">Robert Wedderburn</em> as a way of combining various regression models into a single analysis with considerations given to different probability distributions. You can find their detailed paper (Nelder, J.A. and Wedderburn, R.W., 1972. <em class="italic">Generalized linear models. Journal of the Royal Statistical Society: Series A (General), 135(3), pp.370-384</em>.) at <a href="https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614">https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614</a>.</p>
<p>Now, you may be wondering what linear models are. Why do we need to generalize them? What benefit does it provide? These are relevant questions indeed and they are pretty easy to understand without diving too deep into the mathematics. Once we break down the logic, you will notice that the concept of GLM is pretty easy to understand.</p>
<p>So, let’s start by understanding the basics of linear regression.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor116"/>Introduction to linear regression</h2>
<p><strong class="bold">Linear regression</strong> is probably one of the oldest<a id="_idIndexMarker494"/> statistical models, dating back to 200 years ago. It is an approach that maps the relationship between the dependent and independent variables linearly on a graph. What that means is that the relationship between the two variables can be completely explained by a straight line.</p>
<p>Consider the following example:</p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 5.2 – Linear regression " height="577" src="image/B17298_05_002.jpg" width="1160"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Linear regression</p>
<p>This example demonstrates<a id="_idIndexMarker495"/> the relationship between two variables. The height of a person, H, is an independent variable, while the weight of a person, W, is a dependent variable. The relationship between these two variables can easily be explained by a straight red line. The taller a person is, the more likely he or she will weigh more. Easy enough to understand.</p>
<p>Statistically, the general equation<a id="_idIndexMarker496"/> for any straight line, also called the <strong class="bold">linear equation</strong>, is as follows:</p>
<p class="IMG---Figure"><img alt="" height="106" src="image/Formula_B17298__05_001.png" width="334"/> </p>
<p>Here, we have the following: </p>
<ul>
<li><em class="italic">y</em> is a point on the <em class="italic">Y</em>-axis and indicates the dependent variable.</li>
<li><em class="italic">x</em> is a point on the <em class="italic">X</em>-axis and indicates the independent variable.</li>
<li><em class="italic">b</em><em class="italic">1</em> is the slope of the line, also called the gradient, and indicates how steep the line is. The bigger<a id="_idIndexMarker497"/> the gradient, the steeper the line.</li>
<li><em class="italic">b</em><em class="italic">0</em> is a constant that indicates the point at which the line crosses the <em class="italic">Y</em>-axis.</li>
</ul>
<p>During linear regression, the machine<a id="_idIndexMarker498"/> will map all the data points of the two variables on the graph and randomly place the line on the graph. Then, it will calculate the values of <em class="italic">y</em> by inserting the value of <em class="italic">x</em> from the data points in the graph into the linear equation and comparing the result with the respective <em class="italic">y</em> values from the data points. After that, it will calculate the magnitude of the error between the <em class="italic">y</em> value it calculated and the actual <em class="italic">y</em> value. This difference<a id="_idIndexMarker499"/> in values is what we call a <strong class="bold">residual</strong>.</p>
<p>The following diagram should help you understand what residuals are:</p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 5.3 – Residuals in linear regression " height="523" src="image/B17298_05_003.jpg" width="962"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Residuals in linear regression</p>
<p>The machine will do this for all the data points and make a note of all the errors. It will then try to tweak the line by changing the values of b<span class="subscript">1</span> and b<span class="subscript">0</span>, meaning changing the angle and position of the line on the graph, and repeating the process. It will do this until it minimizes the error.</p>
<p>The values of b<span class="subscript">1</span> and b<span class="subscript">0</span> that generate the least amount of error are the most accurate linear relationship between the two variables. The equation with these values for b1 and b<span class="subscript">0</span> is the linear model.</p>
<p>Now, say you want to predict<a id="_idIndexMarker500"/> how much a person would weigh if they were 180 cm tall. Then, you use this same linear model equation with the b<span class="subscript">1</span> and b<span class="subscript">0</span> values, set <em class="italic">x</em> to 180, and calculate <em class="italic">y</em>, which will be the expected weight. </p>
<p>Congratulations, you just performed ML in your mind without any computers and made predictions too! Actual ML works the same way, albeit with added complexities from complex algorithms. Linear regression doesn’t need to be restricted to just two variables – it can also work on multiple variables where there’s more than one independent<a id="_idIndexMarker501"/> variable. Such linear regression is called multiple or curvilinear regression. The equation of such a linear regression expands as follows:</p>
<p class="IMG---Figure"><img alt="" height="83" src="image/Formula_B17298__05_002.png" width="679"/></p>
<p>In this equation, the additional variables – x<span class="subscript">1</span>, x<span class="subscript">2</span>, x<span class="subscript">3</span>, and so on – are added with their own coefficients – b<span class="subscript">1</span>, b<span class="subscript">2</span>, and b<span class="subscript">3</span>, respectively.</p>
<p>Feel free to explore these algorithms and the mathematics behind them if you are interested in the inner workings of linear regression.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor117"/>Understanding the assumptions of linear regression</h2>
<p>Linear regression, when training<a id="_idIndexMarker502"/> a model on a given dataset, works on certain assumptions<a id="_idIndexMarker503"/> about the data. One of these assumptions is the <strong class="bold">normality of errors</strong>. </p>
<p>Before we understand what the normality<a id="_idIndexMarker504"/> of errors is, let’s quickly understand the concept of the <strong class="bold">probability density function</strong>. This is a mathematical expression that defines the probability distribution of discrete values – in other words, it is a mathematical expression that shows the probability of a sample value occurring from a given sample space. To understand this, refer to the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 5.4 – Probability distribution of values for two dice " height="597" src="image/B17298_05_004.jpg" width="1367"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Probability distribution of values for two dice</p>
<p>The preceding diagram<a id="_idIndexMarker505"/> shows the distribution of probabilities of all the values that can occur when a pair of six-sided dice are thrown fairly and independently. There are different kinds of distributions. Some examples of commonly occurring distributions are as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 5.5 – Different types of distribution " height="930" src="image/B17298_05_005.jpg" width="1210"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Different types of distribution</p>
<p>The normality of errors<a id="_idIndexMarker506"/> states that the residuals<a id="_idIndexMarker507"/> of the data must be normally distributed. A <strong class="bold">normal distribution</strong>, also called <strong class="bold">Gaussian distribution</strong>, is a probability density function that is symmetric<a id="_idIndexMarker508"/>al about the mean, where the values closest to the mean occur frequently, while those far from the mean rarely occur. The following diagram shows a normal distribution:</p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 5.6 – Normal distribution  " height="756" src="image/B17298_05_006.jpg" width="1531"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Normal distribution </p>
<p>Linear regression expects<a id="_idIndexMarker509"/> the residuals that get calculated to fall within a normal distribution. In our previous example of the expected weight for a height, there is bound to be some error between the predicted weight and the actual weight of a person with a certain height. However, the residuals or errors from the prediction will most likely fall within a normal distribution as there cannot be too many occurrences of people with an extreme difference between the expected weight and the predicted weight.</p>
<p>Consider a scenario of people claiming health insurance payouts. The following diagram shows a sample of the linear regression graph for that dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 5.7 – Health insurance payout " height="547" src="image/B17298_05_007.jpg" width="1424"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Health insurance payout</p>
<p>In the preceding diagram, you<a id="_idIndexMarker510"/> can see that the majority of people from various age groups did not claim health insurance. Some of them did and the cost of claims varied a lot. Some had minor issues costing <em class="italic">little</em>, while some had serious injuries and had to go through expensive surgeries.</p>
<p>If you plot a linear regression line through this dataset, it will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 5.8 – Linear regression on health insurance payouts " height="546" src="image/B17298_05_008.jpg" width="1257"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Linear regression on health insurance payouts</p>
<p>But now, if you calculate<a id="_idIndexMarker511"/> the residual errors from the expected and predicted value for all the data points, then the probability distribution of these residuals will not fall into a normal distribution. It will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 5.9 – Residual distribution of health insurance payouts " height="526" src="image/B17298_05_009.jpg" width="1155"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Residual distribution of health insurance payouts</p>
<p>This is an inaccurate model as the expected value and the predicted values are not even close enough to round off or correct. So, what do you do for such a scenario, where the normality of errors assumption fails for the dataset? What if the distribution of the residuals is, say, Poisson instead of normal? How will the machine correct that?</p>
<p>Well, the answer to this is that<a id="_idIndexMarker512"/> the distribution of residuals depends on the distribution of the dataset itself. If the values of the dependent variables are normally distributed, then the distribution of the residuals will also be normal. So, once we have identified which probability density function fits the dataset, we can use that function to train our linear model.</p>
<p>Depending on this function, there are specialized linear regression methods for every<a id="_idIndexMarker513"/> probability<a id="_idIndexMarker514"/> density function. If your distribution is <strong class="bold">Poisson</strong>, then you can use <strong class="bold">Poisson regression</strong>. If your data <a id="_idIndexMarker515"/>distribution is negative <strong class="bold">binomial</strong>, then you can use <strong class="bold">negative binomial regression</strong>.</p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor118"/>Working with a Generalized Linear Model</h2>
<p>Now that we have<a id="_idIndexMarker516"/> covered the basics, let’s focus on understanding what GLM is. GLM is a way of pointing to all the regression methods that are specific to the type of probability distribution of the data. Technically, all the regression models are GLM, including our ordinary simple linear model. GLM just encapsulates them together and trains the appropriate regression model based on the probability distribution function.</p>
<p>The way GLM works is by using something called a link function in conjunction with a systematic component and the random variable. </p>
<p>These are three components of GLM:</p>
<ul>
<li><strong class="bold">Systematic component</strong>: Going back to the multi-variate<a id="_idIndexMarker517"/> linear equation, we have<a id="_idIndexMarker518"/> the following:</li>
</ul>
<p class="IMG---Figure"><img alt="" height="127" src="image/Formula_B17298__05_003.png" width="889"/></p>
<p>Here, b<span class="subscript">1</span>x<span class="subscript">1</span>+ b<span class="subscript">2</span>x<span class="subscript">2</span> + b<span class="subscript">3</span>x<span class="subscript">3</span> + ……. + b<span class="subscript">0</span> is the systematic<a id="_idIndexMarker519"/> component. This is the function<a id="_idIndexMarker520"/> that links our data, also called predictors, with our <a id="_idIndexMarker521"/>predictions.</p>
<ul>
<li><strong class="bold">Random Component</strong>: This component refers to the<a id="_idIndexMarker522"/> probability distribution<a id="_idIndexMarker523"/> of the response variable. This will be whether the response variable is normally distributed or binomially distributed or any other form of distribution.</li>
<li><strong class="bold">Link function</strong>: A link function <a id="_idIndexMarker524"/>is a function that maps the non-linear relationship of data<a id="_idIndexMarker525"/> to a linear one. In other words, it bends the line of linear regression to represent the relationship of non-linear data more accurately. It is a link between the random and the systematic components. We can explain the equation with a link function mathematically as <em class="italic">Y = f</em><span class="subscript">n</span><em class="italic">( b</em><span class="subscript">1</span><em class="italic">x</em><span class="subscript">1</span><em class="italic">+ b</em><span class="subscript">2</span><em class="italic">x</em><span class="subscript">2</span><em class="italic"> + b</em><span class="subscript">3</span><em class="italic">x</em><span class="subscript">3</span><em class="italic"> + ……. + b</em><span class="subscript">0</span><em class="italic"> )</em>, where <em class="italic">f</em><span class="subscript">n</span> is the link function that changes as per the distribution of the response variable.</li>
</ul>
<p>The link function is different<a id="_idIndexMarker526"/> for different distributions. The following table shows the different link functions for different distributions:</p>
<table class="No-Table-Style" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Distribution Type</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Link Function</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Name of Algorithm</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Normal</p>
</td>
<td class="No-Table-Style">
<p><img alt="" height="88" src="image/Formula_B17298__05_004.png" width="217"/></p>
</td>
<td class="No-Table-Style">
<p>Linear model</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Binomial</p>
</td>
<td class="No-Table-Style">
<p><img alt="" height="138" src="image/Formula_B17298__05_005.png" width="335"/></p>
</td>
<td class="No-Table-Style">
<p>Logistic regression</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Poisson</p>
</td>
<td class="No-Table-Style">
<p><img alt="" height="63" src="image/Formula_B17298__05_006.png" width="198"/></p>
</td>
<td class="No-Table-Style">
<p>Poisson regression</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Gamma</p>
</td>
<td class="No-Table-Style">
<p><img alt="" height="111" src="image/Formula_B17298__05_007.png" width="253"/></p>
</td>
<td class="No-Table-Style">
<p>Gamma regression</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Link functions for different distribution types</p>
<p>When training GLM models, you have the option of selecting the value for the family hyperparameter. The family option specifies the probability distribution of your response column and the GLM training algorithm uses the appropriate link function during training.</p>
<p>The values for the family hyperparameter <a id="_idIndexMarker527"/>are as follows:</p>
<ul>
<li><strong class="bold">gaussian</strong>: You should select this option if the response is a real integer number.</li>
<li><strong class="bold">binomial</strong>: You should select this<a id="_idIndexMarker528"/> option if the response is categorical with two classes or binaries that could be either enums or integers.</li>
<li><strong class="bold">fractionalbinomial</strong>: You should select this option if the response is numeric between 0 and 1.</li>
<li><strong class="bold">ordinal</strong>: You should select this option if the response is a categorical response with three or more classes. </li>
<li><strong class="bold">quasibinomial</strong>: You should select this option if the response is numeric.</li>
<li><strong class="bold">multinomial</strong>: You should select this option if the response is a categorical response with three or more classes that are of enum types.</li>
<li><strong class="bold">poisson</strong>: You should select this option if the response is numeric and contains non-negative integers.</li>
<li><strong class="bold">gamma</strong>: You should select this option if the response is numeric and continuous and contains positive real integers.</li>
<li><strong class="bold">tweedie</strong>: You should select this option if the response is numeric and contains continuous real values and non-negative values.</li>
<li><strong class="bold">negativebinomial</strong>: You should select this option if the response is numeric and contains a non-negative integer.</li>
<li><strong class="bold">AUTO</strong>: This determines the family<a id="_idIndexMarker529"/> automatically for the user.</li>
</ul>
<p>As you may have guessed, H2O’s AutoML selects AUTO as the family type when training GLM models. The AutoML process handles this case of selecting the correct distribution family by understanding the distribution of the response variable in the dataset and applying the correct link function to train the GLM model.</p>
<p>Congratulations, we have just<a id="_idIndexMarker530"/> looked into how the GLM algorithm works! GLM is a very powerful and flexible algorithm and H2O AutoML expertly configures its training so that it trains the most accurate and high-performance GLM model. </p>
<p>Now, let’s move on to the next ML algorithm that H2O trains: <strong class="bold">Distributed Random Forest</strong> (<strong class="bold">DRF</strong>).</p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor119"/>Understanding the Distributed Random Forest algorithm</h1>
<p><strong class="bold">DRF</strong>, simply called <strong class="bold">Random Forest</strong>, is a very powerful supervised<a id="_idIndexMarker531"/> learning technique often<a id="_idIndexMarker532"/> used for classification and regression. The foundation<a id="_idIndexMarker533"/> of the DRF learning technique is based on <strong class="bold">decision trees</strong>, where a large number of decision trees are randomly created and used for predictions and their results are combined to get the final output. This randomness is used to minimize the bias and variance of all the individual decision trees. All the decision trees are collectively combined and called a forest, hence the name Random Forest. </p>
<p>To get a deeper conceptual understanding of DRF, we need to understand the basic building block of DRF – that is, a decision tree.</p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor120"/>Introduction to decision trees</h2>
<p>In very simple terms, a decision tree<a id="_idIndexMarker534"/> is just a set of <em class="italic">IF</em> conditions that either<a id="_idIndexMarker535"/> return a yes or a no answer based on data passed to it. The following diagram shows a simple example of a decision tree:</p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 5.11 – Simple decision tree " height="339" src="image/B17298_05_010.jpg" width="928"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Simple decision tree</p>
<p>The preceding diagram<a id="_idIndexMarker536"/> shows a basic decision<a id="_idIndexMarker537"/> tree. A decision tree consists of the following components:</p>
<ul>
<li><strong class="bold">Nodes</strong>: Nodes are basically <em class="italic">IF</em> conditions that split<a id="_idIndexMarker538"/> the decision tree based on whether the condition was met or not.</li>
<li><strong class="bold">Root Node</strong>: The node on the top of the decision<a id="_idIndexMarker539"/> tree is called the root node.</li>
<li><strong class="bold">Leaf Node</strong>: The nodes of the decision tree that do not branch out further are called leaf nodes, or simply leaves. The<a id="_idIndexMarker540"/> condition, in this case, is if the value of the data that’s passed to it is numeric, then the answer is the data is a number; if the data that’s passed to it is not numeric, then the answer will be the data is non-numeric. This is simple enough to understand.</li>
</ul>
<p>As seen in <em class="italic">Figure 5.11</em>, the decision tree is based on a simple true or false question. Decision trees<a id="_idIndexMarker541"/> can also be based <a id="_idIndexMarker542"/>on mathematical conditions on numeric data. The following example shows a decision tree on numeric conditions:</p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="Figure 5.12 – Numerical decision tree " height="336" src="image/B17298_05_011.jpg" width="1032"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – Numerical decision tree</p>
<p>In this example, the root node computes whether the IQ number is greater than 300 and decides if it is artificial intelligence or human intelligence.</p>
<p>Decision trees can be combined as well. They can form a complex set of decision-making conditions that rely on the results of previous decisions. Refer to the following example for a complex decision tree:</p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="Figure 5.13 – Complex decision tree " height="687" src="image/B17298_05_012.jpg" width="1270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – Complex decision tree</p>
<p>In the preceding example, we are<a id="_idIndexMarker543"/> trying to calculate if <em class="italic">you can go play outside</em> or <em class="italic">finish your ML studies</em>. This decision tree<a id="_idIndexMarker544"/> combines numeric data as well as the classification of data. When making predictions, the decision tree will start at the top and work its way down, making decisions on whether the data satisfies the conditions or not. The leaf nodes are the final potential results of the decision tree.</p>
<p>With this knowledge in mind, let’s create a decision tree on a sample dataset. Refer to the following table for the sample dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 5.14 – Sample dataset for creating a decision tree " height="262" src="image/B17298_05_013.jpg" width="762"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Sample dataset for creating a decision tree</p>
<p>The content<a id="_idIndexMarker545"/> of the aforementioned<a id="_idIndexMarker546"/> dataset is as follows:</p>
<ul>
<li><strong class="bold">Chest Pain</strong>: This column indicates if a patient suffers from chest pain.</li>
<li><strong class="bold">Good Blood Circulation</strong>: This column indicates if a patient has good blood circulation.</li>
<li><strong class="bold">Blocked Arteries</strong>: This column indicates if a patient has any blocked arteries.</li>
<li><strong class="bold">Heart Disease</strong>: This column indicates if the patient suffers from heart disease.</li>
</ul>
<p>For this scenario, we want to create a decision tree that uses Chest Pain, Good Blood Circulation, and Blocked Arteries features to predict whether a patient has heart disease. Now, when forming a decision tree, the first thing that we need to do is find the root node. So, what feature should we place at the top of the decision tree?</p>
<p>We start by looking at how the Chest Pain feature alone fairs when predicting heart disease. We shall go through all the values in the dataset and map them to this decision tree while comparing the values in the Chest Pain column with those of heart disease. We shall<a id="_idIndexMarker547"/> keep track of these relationships in the decision tree. The decision tree for Chest Pain as the root node will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 5.15 – Decision tree for the Chest Pain feature " height="475" src="image/B17298_05_014.jpg" width="927"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – Decision tree for the Chest Pain feature</p>
<p>Now, we do this for all the other<a id="_idIndexMarker548"/> features in the dataset. We create<a id="_idIndexMarker549"/> a decision tree for Good Blood Circulation and see how it fairs alone when making predictions for Heart Disease and keep a track of the comparison, repeating the same process for the Blocked Arteries status as well. If there are any missing values in the dataset, then we skip them. Ideally, you should not work with datasets that have missing values. We can use the techniques we learned about in <a href="B17298_03.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Understanding Data Processing</em>, where we impute and handle missing dataset values. </p>
<p>Refer to the following diagram, which shows the two decision trees that were created – one for <strong class="bold">Patient Has Blocked Arteries</strong> and another for <strong class="bold">Patient Has Good Blood Circulation</strong>:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation features " height="619" src="image/B17298_05_015.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation features</p>
<p>Now that we have created<a id="_idIndexMarker550"/> a decision tree for all the features<a id="_idIndexMarker551"/> in the dataset, we can compare their results to find the pure feature. In the context of decision trees, a feature is said to be 100% impure when a node is split evenly, 50/50, and 100% pure when all of its data belongs to a single class. In our scenario, we don’t have any feature that is 100% pure. All of our features are impure to some degree. So, we need to find some way of finding the feature that is the purest. For that, we need a metric that can measure the purity of a decision tree.</p>
<p>There are plenty of ways by which data scientists and engineers measure purity. The most common<a id="_idIndexMarker552"/> metric to measure impurity in decision trees is <strong class="bold">Gini Impurity</strong>. Gini Impurity is the measure of the likelihood that a new random instance of data will be incorrectly classified during classification.</p>
<p>Gini Impurity is calcul<a id="_idTextAnchor121"/>ated as follows:</p>
<p class="IMG---Figure"><img alt="" height="91" src="image/Formula_B17298__05_008.png" width="1181"/></p>
<p>Here, p<span class="subscript">1</span>, p<span class="subscript">2</span> , p<span class="subscript">3</span> , p<span class="subscript">4</span> … are the probabilities of the various classifications for Heart Disease. In our scenario, we only have two classifications – either a yes or a no. Thus, for our scenario, the measure of impurity is as follows:</p>
<p class="IMG---Figure"><img alt="" height="88" src="image/Formula_B17298__05_009.png" width="1550"/></p>
<p>So, let’s calculate<a id="_idIndexMarker553"/> the Gini Impurity<a id="_idIndexMarker554"/> of all the decision<a id="_idIndexMarker555"/> trees we just created so that we can find the feature that is the purest. Gini Impurity for a decision tree with multiple leaf nodes is calculated by calculating the Gini Impurity of individual leaf nodes and then calculating the weighted average of all the impurity values to get the Gini Impurity of the decision tree as a whole. So, let’s start by calculating the Gini Impurity of the left leaf node of the Chest Pain decision tree and repeat this for the right leaf node:</p>
<p class="IMG---Figure"><img alt="" height="132" src="image/Formula_B17298__05_010.png" width="1650"/></p>
<p class="IMG---Figure"><img alt="" height="132" src="image/Formula_B17298__05_011.png" width="1650"/></p>
<p>The reason why we calculate the weighted average of the Gini Impurities is because the representation of the data is not equally divided between the two branches of the decision tree. The weighted average helps us offset this unequal distribution of the data values. Thus, we can calculate the Gini Impurity of the whole Chest Pain decision tree as follows:</p>
<p><em class="italic">Gini Impurity (Chest Pain) = weighted average of the Gini Impurities of the leaf nodes</em></p>
<p><em class="italic">Gini Impurity (Chest Pain) = </em></p>
<p><em class="italic">(Total number of data inputs in the left leaf node / total number of rows) x Gini Impurity of the left leaf node</em></p>
<p><em class="italic">+</em></p>
<p><em class="italic">(Total number of data inputs in the right leaf node / total number of rows) x Gini Impurity of the right leaf node</em></p>
<p><em class="italic">= (144 / (144 + 159)) x 0.395 + (159 / (144 + 159)) x 0.364</em></p>
<p><em class="italic">= 0.364</em></p>
<p>The Gini Impurity<a id="_idIndexMarker556"/> of the Chest Pain decision tree is 0.364.</p>
<p>We repeat this process for all the other feature decision trees as well. We should get the following results:</p>
<ul>
<li>The Gini Impurity of the Chest Pain decision tree is 0.364</li>
<li>The Gini Impurity of the Good Blood Circulation tree is 0.360</li>
<li>The Gini Impurity of the Blocked Arteries tree is 0.381</li>
</ul>
<p>Comparing these values, we can infer<a id="_idIndexMarker557"/> that the Gini Impurity<a id="_idIndexMarker558"/> of the Good Blood Circulation feature has the lowest Gini Impurity, making it the purest feature in the dataset. So, we will use it as the root of our decision tree.</p>
<p>Referring to <em class="italic">Figures 5.12</em> and <em class="italic">5.13</em>, when we divided the patients by using the Good Blood Circulation feature, we were left with an impure distribution of the results on the left and right leaf nodes. So, each leaf node had a mix of results that showed with and without Heart Disease. Now, we need to figure out a way to separate the mix of results from the Good Blood Circulation feature using the remaining features – Chest Pain and Blocked Arteries.</p>
<p>So, just as how we did previously, we shall use these mixed results and separate them using the other features and calculate the Gini Impurity value of those features. We shall choose the feature that is the purest and replace it at the given node for further classification. </p>
<p>We shall repeat this process for the right branch as well. So, to simplify the selection of the decision tree nodes, we must do the following:</p>
<ul>
<li>Calculate the Gini Impurity score of all the remaining features for that node using the mixed results.</li>
<li>Choose the one with the lowest impurity and replace it with the node.</li>
<li>Repeat the same process further down the decision tree with the remaining features.</li>
<li>Continue replacing the nodes, so long as the classification lowers the Gini Impurity; otherwise, leave it as a leaf node.</li>
</ul>
<p>So, your final<a id="_idIndexMarker559"/> decision tree<a id="_idIndexMarker560"/> will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="Figure 5.17 – The final decision tree " height="723" src="image/B17298_05_016.jpg" width="1300"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – The final decision tree</p>
<p>This decision tree is good for classification with true or false values. What if you had numerical data instead?</p>
<p>Creating decision trees with numerical data is very easy and has almost the same steps as we do for true/false data. Consider Weight as a new feature; the data for the <strong class="bold">Weight</strong> column is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 5.18 – Dataset with a new feature, Weight, in kilograms " height="266" src="image/B17298_05_017.jpg" width="761"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Dataset with a new feature, Weight, in kilograms</p>
<p>For this scenario, we must <a id="_idIndexMarker561"/>follow these <a id="_idIndexMarker562"/>steps:</p>
<ol>
<li>Sort the data in ascending order. In our scenario, we shall sort the rows of the dataset with the Weight column from highest to lowest.</li>
<li>Calculate the average weights for all the adjacent rows:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="Figure 5.19 – Calculating the average of the subsequent row values " height="336" src="image/B17298_05_018.jpg" width="809"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – Calculating the average of the subsequent row values</p>
<ol>
<li value="3">Calculate the Gini Impurity of all the averages we calculated:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="Figure 5.20 – Calculating the Gini Impurity of all the averages " height="612" src="image/B17298_05_019.jpg" width="1248"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – Calculating the Gini Impurity of all the averages</p>
<ol>
<li value="4">Identify and select the average feature value that gives us the least Gini Impurity value.</li>
<li>Use the selected feature value as a decision node in the decision tree.</li>
</ol>
<p>Making predictions using decision trees<a id="_idIndexMarker563"/> is very easy. You will have data <a id="_idIndexMarker564"/>with values for chest pain, blocked arteries, good circulation, and weight and you will feed it to the decision tree model. The model will filter the values down the decision tree while calculating the node conditions and eventually arriving at the leaf node with the prediction value.</p>
<p>Congratulations – you have just understood the concept of decision trees! Despite decision trees being easy to understand and implement, they are not that good at solving real-life ML problems. </p>
<p>There are certain drawbacks to using decision trees:</p>
<ul>
<li>Decision trees are very unstable. Any minor changes in the dataset can drastically alter the performance of the model and prediction results.</li>
<li>They are inaccurate. </li>
<li>They can get very complex for large datasets with a large number of features. Imagine a dataset with 1,000 features – the decision tree for this dataset will have a tree whose depth will be very large and its computation will be very resource-intensive.</li>
</ul>
<p>To mitigate all these<a id="_idIndexMarker565"/> drawbacks, the Random Forest<a id="_idIndexMarker566"/> algorithm was developed, which builds on top of decision trees. With this knowledge, let’s move on to the next concept: Random Forest.</p>
<h2 id="_idParaDest-90"><a id="_idTextAnchor122"/>Introduction to Random Forest</h2>
<p><strong class="bold">Random Forest</strong>, also called <strong class="bold">Random Decision Forest</strong>, is an ML method that builds a large number of decision<a id="_idIndexMarker567"/> trees during learning<a id="_idIndexMarker568"/> and groups, or ensembles, the results<a id="_idIndexMarker569"/> of the individual decision trees to make predictions. Random Forest is used to solve both classification and regression problems. For classification problems, the class value predicted by the majority of the decision trees is the predicted value. For regression problems, the mean or average prediction of the individual trees is calculated and returned as the prediction value.</p>
<p>The Random Forest algorithm follows these steps for learning during training:</p>
<ol>
<li value="1">Create a bootstrapped dataset from the original dataset.</li>
<li>Randomly select a subset of the data features.</li>
<li>Start creating a decision tree using the selected subset of features, where the feature that splits the data the best is chosen as the root node.</li>
<li>Select a random subset of the other remaining features to further split the decision tree.</li>
</ol>
<p>Let’s understand this concept of Random Forest by creating one. </p>
<p>We shall use the same dataset<a id="_idIndexMarker570"/> that we used to make our complex decision<a id="_idIndexMarker571"/> tree in <em class="italic">Figure 5.17</em>. The dataset is the same one we used to make our decision trees. To create a Random Forest, we need to create a bootstrapped version of the dataset.</p>
<p>A bootstrapped dataset is a dataset that is created from the original dataset by randomly selecting rows from the dataset. The bootstrapped dataset is the same size as the original dataset and can also contain duplicate rows from the original dataset. There are plenty of inbuilt functions for creating a bootstrapped dataset and you can use any of them to create one.</p>
<p>Consider the following bootstrapped dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 5.21 – Bootstrapping dataset " height="367" src="image/B17298_05_020.jpg" width="1097"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Bootstrapping dataset</p>
<p>The next step is to create a decision tree from the bootstrapped dataset but using only a subset of the feature columns at each step. So, selecting all the features to be considered for the decision tree only lets you go with Good Blood Circulation and Blocked Arteries as features for the decision tree.</p>
<p>We shall follow the same purity identification criteria to determine the root of the node. Let’s assume that for<a id="_idIndexMarker572"/> our experiment, Good Blood Circulation<a id="_idIndexMarker573"/> is the purest. Setting that as the root node, we shall now consider the remaining features to fill the next level of decision nodes. Just like we did previously, we shall randomly select two features from the remaining features and decide which feature should fit in the next decision node. We will build the tree as usual while considering the random subset of remaining variables at each step.</p>
<p>Here is the tree we just made:</p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 5.22 – First decision tree from the bootstrapped dataset " height="707" src="image/B17298_05_021.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – First decision tree from the bootstrapped dataset</p>
<p>Now, we repeat the same process while creating multiple decision trees and bootstrapping and selecting features from random trees. An ideal Random Forest will create hundreds of decision trees, as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 5.23 – Multiple decision trees from different bootstrapped datasets " height="680" src="image/B17298_05_022.jpg" width="1262"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – Multiple decision trees from different bootstrapped datasets</p>
<p>This large variety of decision<a id="_idIndexMarker574"/> trees that were created with randomized implementation<a id="_idIndexMarker575"/> is what makes Random Forest more effective than a single decision tree.</p>
<p>Now that we have created our Random Forest, let’s see how we can use it to make predictions. To make predictions, you will have a row that contains data values for the different features and you want to predict whether that person has heart disease.</p>
<p>You will pass this data down an individual decision tree in the Random Forest:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 5.24 – Predictions from the first decision tree in the Random Forest " height="729" src="image/B17298_05_023.jpg" width="1309"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – Predictions from the first decision tree in the Random Forest</p>
<p>The decision tree will predict<a id="_idIndexMarker576"/> the results based on its structure. We shall keep<a id="_idIndexMarker577"/> a track of the prediction made by this tree and continue passing the data down to the other trees, noting their predictions as well:</p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 5.25 – Predictions from the other individual trees in the Random Forest " height="670" src="image/B17298_05_024.jpg" width="1420"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – Predictions from the other individual trees in the Random Forest</p>
<p>Once we get predictions<a id="_idIndexMarker578"/> from all the individual trees, we can find out which <a id="_idIndexMarker579"/>value got the most votes from all the decision trees. The prediction value with the most votes concludes the prediction for the Random Forest.</p>
<p>Bootstrapping the dataset and aggregating the prediction values of all the decision<a id="_idIndexMarker580"/> trees to make a decision is called <strong class="bold">bagging</strong>.</p>
<p>Congratulations – you have just understood the concept of Random Forest! Random Forest, despite being a very good ML algorithm with low bias and variance, still suffers from high computation requirements. Hence, H2O AutoML, instead of training Random Forest, trains an alternate version of Random Forest called <strong class="bold">Extremely Randomized Trees</strong> (<strong class="bold">XRT</strong>). </p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor123"/>Understanding Extremely Randomized Trees</h2>
<p>The <strong class="bold">XRT</strong> algorithm, also called <strong class="bold">ExtraTrees</strong>, is just like the ordinary<a id="_idIndexMarker581"/> Random Forest algorithm. However, there<a id="_idIndexMarker582"/> are two key differences between Random Forest and XRT, as follows:</p>
<ul>
<li>In Random Forest, we use a bootstrapped dataset to train the individual decision trees. In XRT, we use the whole dataset to train the individual decision trees.</li>
<li>In Random Forest, the decision nodes are split based on certain selection criteria such as the impurity metric or error rate when building the individual decision tree. In XRT, this process is completely randomized and the one with the best results is chosen.</li>
</ul>
<p>Let’s consider the same example we used to understand Random Forest to understand XRT. We have a dataset, as shown in <em class="italic">Figure 5.17</em>. Instead of bootstrapping the data, as we did in <em class="italic">Figure 5.20</em>, we shall use the dataset as-is.</p>
<p>Then, we start creating<a id="_idIndexMarker583"/> our decision trees by randomly<a id="_idIndexMarker584"/> selecting a subset of the features. In Random Forest, we used the purity criteria to decide which feature should be set as the root node of the decision tree. However, for XRT, we shall set the root node as well as the decision nodes of the decision tree randomly. Similarly, we shall create multiple decision trees like these with all the features randomly selected. This added randomness allows the algorithm to further reduce the variance of the model, at the expense of a slight increase in bias.</p>
<p>Congratulations! We have just investigated how the XRT algorithm uses an extremely randomized forest of decision trees to make accurate regressions and classification predictions. Now, let’s understand how the GBM algorithm trains a classification model to classify data.</p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor124"/>Understanding the Gradient Boosting Machine algorithm</h1>
<p><strong class="bold">Gradient Boosting Machine</strong> (<strong class="bold">GBM</strong>) is a forward learning ensemble ML algorithm that works on both classification<a id="_idIndexMarker585"/> as well as regression. The GBM model is an ensemble model just like the DRF algorithm in the sense that the GBM model, as a whole, is a combination of multiple weak learner models whose results are aggregated and presented as a GBM prediction. GBM works similarly to DRF in that it consists of multiple decision trees that are built in a sequence that sequentially minimizes the error.</p>
<p>GBM can be used to predict continuous numerical values, as well as to classify data. If GBM is used to predict continuous numerical values, we say that we are using GBM for regression. If we are using GBM to classify data, then we say we are using GBM for classification.</p>
<p>The GBM algorithm has a foundation on decision trees, just like DRF. However, how the decision trees are built is different compared to DRF.</p>
<p>Let’s try to understand how the GBM algorithm works for regression.</p>
<h2 id="_idParaDest-93"><a id="_idTextAnchor125"/>Building a Gradient Boosting Machine</h2>
<p>We shall use the following sample<a id="_idIndexMarker586"/> dataset and understand how GBM works as we conceptually build the model. The following table contains a sample of the dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 5.26 – Sample dataset for GBM " height="390" src="image/B17298_05_025.jpg" width="561"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.26 – Sample dataset for GBM</p>
<p>This is an arbitrary dataset<a id="_idIndexMarker587"/> that we are using just for the sake of understanding how GBM will build its ML model. The contents of the dataset are as follows:</p>
<ul>
<li><strong class="bold">Height</strong>: This column indicates the height of the person in centimeters.</li>
<li><strong class="bold">Gender</strong>: This column indicates the gender of the person.</li>
<li><strong class="bold">Age</strong>: This column indicates the age of the person.</li>
<li><strong class="bold">Weight</strong>: This column indicates the weight of the person.</li>
</ul>
<p>GBM, unlike DRF, starts creating its weak learner decision trees from leaf nodes instead of root nodes. The very first leaf node that it will create will be the average of all the values of the response variable. So, accordingly, the GBM algorithm will create the leaf node, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 5.27 – Calculating the leaf node using the column average " height="320" src="image/B17298_05_026.jpg" width="877"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.27 – Calculating the leaf node using the column average</p>
<p>This leaf node alone can also be<a id="_idIndexMarker588"/> considered a decision tree. It acts like a prediction model that only predicts a constant value for any kind of input data. In this case, it’s the average value that we get from the response column. This is, as we expect, an incorrect way of making any predictions, but it is just the first step for GBM.</p>
<p>The next thing GBM will do is create another decision tree based on the errors it observed from its initial leaf node predictions on the dataset. An error, as we discussed previously, is nothing but the difference between the observed weight and the predicted<a id="_idIndexMarker589"/> weight and is also called the residual. However, these residuals are different from the actual residuals that we will get from the complete GBM model. The residuals<a id="_idIndexMarker590"/> that we get from the weak learner decision trees of GBM are called <strong class="bold">pseudo-residuals</strong>, while those of the GBM model are the actual residuals.</p>
<p>So, as mentioned previously, the GBM algorithm will calculate the pseudo-residuals of the first leaf node for all the data values in the dataset and create a special column that keeps track of these pseudo-residual values. </p>
<p>Refer to the following diagram for a better understanding:</p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="Figure 5.28 – Dataset with pseudo-residuals " height="416" src="image/B17298_05_027.jpg" width="701"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.28 – Dataset with pseudo-residuals</p>
<p>Using these pseudo-residual<a id="_idIndexMarker591"/> values, the GBM algorithm then builds a decision tree using all the remaining features – that is, Height, Favorite Color, and Gender. The decision tree will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 5.29 – Decision tree using pseudo-residual values " height="483" src="image/B17298_05_028.jpg" width="1116"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.29 – Decision tree using pseudo-residual values</p>
<p>As you can see, this decision tree only has four leaf nodes, while the pseudo-residual values in the algorithm generated from the first tree are way more than four. This is because the GBM <a id="_idIndexMarker592"/>algorithm restricts the size of the decision trees it makes. For this scenario, we are only using four leaf nodes. Data scientists can control the size of the trees by passing the right hyperparameters when configuring the GBM algorithm. Ideally, for large datasets, you often use 8 to 32 leaf nodes.</p>
<p>Due to the restriction of the leaf nodes in the decision trees, the decision tree ends up with multiple pseudo-residual values in the same leaf nodes. So, the GBM algorithm replaces them with their average to get one concrete number for a single leaf node. Accordingly, after calculating the averages, we will end up with a decision tree that looks as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 5.30 – Decision tree using averaged pseudo-residual values " height="480" src="image/B17298_05_029.jpg" width="1173"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.30 – Decision tree using averaged pseudo-residual values</p>
<p>Now, the algorithm combines the original leaf node with this new decision tree to make predictions on it. So, now, we have a value of 71.2 from the initial leaf node prediction. Then, after running the data down the decision tree, we get 16.8. So, the predicted weight is the summation of both the predictions, which is 88. This is also the observed weight.</p>
<p>This is not correct<a id="_idIndexMarker593"/> as this is a case of overfitting. <strong class="bold">Overfitting</strong> is a modeling error where the model function is too fine-tuned to predict only the data values available in the dataset<a id="_idIndexMarker594"/> and not any other values outside the dataset. As a result, the model becomes useless for predicting any values that fall outside of the dataset.</p>
<p>So, to correct this, the GBM algorithm assigns a learning rate to all the weak learner<a id="_idIndexMarker595"/> decision trees that it trains. The <strong class="bold">learning rate</strong> is a hyperparameter that tunes the rate at which the model learns new information that can override the old information. The value of the learning rate ranges from 0 to 1. By adding this learning rate to the predictions from the decision trees, the algorithm controls the influence of the decision tree’s predictions and slowly moves toward minimizing the error step by step. For our example, let’s assume that the learning rate is 0.1. So, accordingly, the predicted weight can be calculated as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="Figure 5.31 – Calculating the predicted weight " height="417" src="image/B17298_05_030.jpg" width="1232"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.31 – Calculating the predicted weight</p>
<p>Thus, the algorithm will plug in the learning rate for the predictions made by the decision tree and then calculate the predicted weight. Now, the predicted weight will be <em class="italic">62.1 + (0.1 x -14.2) = 60.68</em>.</p>
<p>60.68 is not a very good prediction but it is still a better prediction than 62.68, which is what the initial leaf node predicted. The incremental steps to minimize the errors are the right way to maintain low variance in predictions. A correct balance of learning rate is also important as too high a learning rate will offshoot the correction in the opposite direction, while too low a learning rate will lead to long computation time as the algorithm will take very small correction steps to reach the minimum error.</p>
<p>To further correct the prediction<a id="_idIndexMarker596"/> value and minimize the error, the GBM algorithm will create another decision tree. For this, it will calculate the new pseudo-residual values from predictions made with the leaf node and the first decision tree and use these values to build the second decision tree.</p>
<p>The following diagram shows how new pseudo-residual values are calculated: </p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="Figure 5.32 – Calculating new pseudo-residual values " height="345" src="image/B17298_05_031.jpg" width="1252"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.32 – Calculating new pseudo-residual values</p>
<p>You will notice that the new pseudo-residual values that were generated are a lot closer to the actual values compared to the first pseudo-residual values. This indicates that the GBM model is slowly minimizing errors and improving its accuracy.</p>
<p>Moving on with the second decision tree, the algorithm uses the new pseudo residual values to create the second decision tree. Once created, it aggregates the tree, along with the learning rate, to the already existing leaf node and the first decision tree.</p>
<p>The decision trees can be different each time the GBM algorithm creates one. However, the learning rate stays common for all the trees. So, now, the prediction values will be the summation of the three components – the initial leaf node prediction value, the scaled value of the first decision tree prediction, and the scaled value of the second decision tree prediction. So, the prediction values will be as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="Figure 5.33 – GBM model with a second boosted decision tree " height="405" src="image/B17298_05_032.jpg" width="1057"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.33 – GBM model with a second boosted decision tree</p>
<p>The GBM algorithm<a id="_idIndexMarker597"/> will repeat the same process, creating decision trees up to the specified number of trees or until adding decision trees stops improving the predictions. So, eventually, the GBM model will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="Figure 5.34 – Complete GBM model " height="615" src="image/B17298_05_033.jpg" width="1408"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.34 – Complete GBM model</p>
<p>Congratulations! we have<a id="_idIndexMarker598"/> just explored how the GBM algorithm uses an ensemble of weak decision tree learners to make accurate regressions predictions. </p>
<p>Another algorithm that H2O AutoML uses, which builds on top of GBM, is the XGBoost algorithm. XGBoost<a id="_idIndexMarker599"/> stands for Extreme Gradient Boosting and implements <a id="_idIndexMarker600"/>a process called boosting that sometimes helps in training better-performing models. It is one of the most widely used ML algorithms in Kaggle competitions and has proven to be an amazing ML algorithm that can be used for both classification and regression. The mathematics behind how XGBoost works can be slightly difficult for users not well versed with statistics. However, it is highly recommended that you take the time and learn more about this algorithm. You can find more information about how H2O<a id="_idIndexMarker601"/> performs XGBoost training at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml</a>.</p>
<p class="callout-heading">Tip</p>
<p class="callout">Ensemble ML<a id="_idIndexMarker602"/> is a method of combining multiple ML models to obtain better prediction results compared to the performance of the models individually – just like how a combination of decision trees creates the Random Forest algorithm using bagging and how the GBM algorithm uses a combination of weak learners to minimize errors. Ensemble models take things one step further by finding the best combinations of prediction algorithms and using their combined performance to train a meta-learner<a id="_idIndexMarker603"/> that provides improved performance. This is done using a process called stacking. You can find more information about how H2O trains<a id="_idIndexMarker604"/> these stacked ensemble models at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles</a>.</p>
<p>Now, let’s learn how deep learning works and understand neural networks. </p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor126"/>Understanding what is Deep Learning </h1>
<p><strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) is a branch of ML that develops<a id="_idIndexMarker605"/> prediction models using <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>). ANNs, simply called <strong class="bold">Neural Networks</strong> (<strong class="bold">NNs</strong>), are computations that are loosely<a id="_idIndexMarker606"/> based on how human brains<a id="_idIndexMarker607"/> with neurons work to process information. ANNs consist of neurons, which are types of nodes that are interconnected with other neurons. These neurons transmit information among themselves; this gets processed down the NN to eventually arrive at a result. </p>
<p>DL is one of the most powerful ML techniques and is used to train models that are highly configurable and can support predictions for large and complicated datasets. DL models can be supervised, semi-supervised, or unsupervised, depending on their configuration.</p>
<p>There are various types of ANNs:</p>
<ul>
<li><strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>): RNN is a type of NN<a id="_idIndexMarker608"/> where the connections<a id="_idIndexMarker609"/> between the various neurons of the NN can form a directed or undirected graph. This type of network is cyclic since the outputs of the network are fed back to the start of the network and contribute to the next cycle of predictions. The following diagram shows an example of an RNN:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<img alt="Figure 5.35 – RNN " height="1013" src="image/B17298_05_034.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.35 – RNN</p>
<p>As you can see, the values from the last nodes in the NN are fed to the starting nodes of the network as inputs.</p>
<ul>
<li><strong class="bold">Feedforward NN</strong>: A feedforward neural network is similar to an RNN, with<a id="_idIndexMarker610"/> the only difference being that the network of nodes does not form a cycle. The following diagram shows an example of a feedforward neural network:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<img alt="Figure 5.36 – Feedforward neural network " height="992" src="image/B17298_05_035.jpg" width="1617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.36 – Feedforward neural network</p>
<p>As you can see, this type of NN <a id="_idIndexMarker611"/>is unidirectional. This is the simplest type of ANN. A feedforward<a id="_idIndexMarker612"/> neural network is also called <strong class="bold">Deep Neural Network</strong> (<strong class="bold">DNN</strong>).</p>
<p>H2O’s DL is based on a multi-layer<a id="_idIndexMarker613"/> free forward ANN. It is trained on <strong class="bold">stochastic gradient descent</strong> using <strong class="bold">backpropagation</strong>. There are plenty of different types<a id="_idIndexMarker614"/> of DNNs that H2O can train. They are as follows:</p>
<ul>
<li><strong class="bold">Multi-Layer Perceptron</strong> (<strong class="bold">MLP</strong>): These types of DNNs<a id="_idIndexMarker615"/> are best suited for tabular data.</li>
<li><strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>): These types of DNNs<a id="_idIndexMarker616"/> are best suited for image data.</li>
<li><strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>): These types of DNNs<a id="_idIndexMarker617"/> are best suited for sequential data such as voice data or time series data.</li>
</ul>
<p>It is recommended to use the default DNNs that H2O provides out of the box as configuring a DNN can be very difficult for non-experts. H2O has already preconfigured its implementation of DL to use the best type of DNNs for the given cases.</p>
<p>With these basics in mind, let’s dive deeper into understanding how DL works.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor127"/>Understanding neural networks</h2>
<p><strong class="bold">NNs</strong> form the basis of DL. The workings<a id="_idIndexMarker618"/> of a NN are easy to understand:</p>
<ol>
<li value="1">You feed the data into the input layer of the NN.</li>
<li>The nodes in the NN train themselves to recognize patterns and behaviors from the input data.</li>
<li>The NN then makes predictions based on the patterns and behaviors it learns during training.</li>
</ol>
<p>The structure of an NN looks as follows:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 5.37 – Structure of an NN " height="1013" src="image/B17298_05_036.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.37 – Structure of an NN</p>
<p>There are three essential components<a id="_idIndexMarker619"/> of an NN:</p>
<ul>
<li><strong class="bold">The input layer</strong>: The input layer consists of multiple sets of neurons. These neurons are connected to the next layer of neurons, which reside in the hidden layer.</li>
<li><strong class="bold">The hidden layer</strong>: Within the hidden layer, there can be multiple layers of neurons, all of which are interconnected layer by layer. </li>
<li><strong class="bold">The output layer</strong>: The output layer is the final layer in the NN that makes the final calculations to compute the final prediction values in terms of probability.</li>
</ul>
<p>The learning process of the NN can be broken down into two components:</p>
<ul>
<li><strong class="bold">Forward propagation</strong>: As the name suggests, forward propagation<a id="_idIndexMarker620"/> is where<a id="_idIndexMarker621"/> the information flows from the input layer to the output layer through the middle layer. The following diagram shows forward propagation in action:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="Figure 5.38 – Forward propagation in an NN " height="981" src="image/B17298_05_037.jpg" width="1599"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.38 – Forward propagation in an NN</p>
<p>The neurons within the middle layer are connected via <strong class="bold">channels</strong>. These channels are assigned numerical values called <strong class="bold">weights</strong>. Weights determine how important <a id="_idIndexMarker622"/>the neuron is in terms of its value contributing to the overall prediction. The higher the value of the weight, the more important that node is when making predictions.</p>
<p>The input values from the input layer are multiplied by these weights as they pass through the channels and their sum is sent as inputs to the neurons in the hidden layer. Each neuron in the hidden layer is associated with a numerical value called a <strong class="bold">bias,</strong> which is added<a id="_idIndexMarker623"/> to the input sum.</p>
<p>This weighted value is then passed to a non-linear function called the activation function. The activation function<a id="_idIndexMarker624"/> is a function that decides if the particular neuron can pass its calculated weight value onto the next layer of the neuron or not, depending on the equation of the non-linear function. The bias is a scalar value that shifts the activation function either to the left or right of the graph for corrections.</p>
<p>This flow of information continues to the next layer of neurons in the hidden layer, following the same process of multiplying the weight of the channels and passing the input to the next activation function of the node.</p>
<p>Finally, in the output<a id="_idIndexMarker625"/> layer, the neuron with the highest value determines what the prediction value is, which is a form of probability.</p>
<ul>
<li><strong class="bold">Backpropagation</strong>: Backpropagation works the same way<a id="_idIndexMarker626"/> as forward propagation except<a id="_idIndexMarker627"/> that it works in the reverse direction. Information is passed from the output layer to the input layer through the hidden layer in a reverse manner. The following diagram will give you a better understanding of this:</li>
</ul>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="Figure 5.39 – Backpropagation in NN " height="992" src="image/B17298_05_038.jpg" width="1617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.39 – Backpropagation in NN</p>
<p>It may be counterintuitive<a id="_idIndexMarker628"/> to understand how backpropagation<a id="_idIndexMarker629"/> can work since it works in a reverse manner from the output to the input, but it is a concept that makes DL so powerful for ML. It is through backpropagation that NNs can learn by themselves.</p>
<p>The way it does this is pretty simple. In backpropagation learning, the NN will calculate the magnitude of error between the expected value and the predicted value and evaluate its performance by mapping it onto a <strong class="bold">loss function</strong>. The loss function <a id="_idIndexMarker630"/>is a function that calculates the deviance between the predicted and expected values. This deviation value is the information that helps the NN adjust its biases and weights in the hidden layer to improve its performance<a id="_idIndexMarker631"/> and make better predictions.</p>
<p>Congratulations – we have just gotten a basic glimpse of how DL trains ML models! </p>
<p class="callout-heading">Tip</p>
<p class="callout">DL is one of the most sophisticated<a id="_idIndexMarker632"/> fields in ML, as well as AI as a whole. It’s a vast area of ML specialization where data scientists spend a lot of time researching and understanding the problem statement and the data they are working with so that they can correctly tune their DL NN. The mathematics behind it is also very complex and as such deserves a dedicated book. So, if you are interested in mathematics and want to excel in the art of DL, feel free to explore the ML algorithm in depth as every step in understanding it will make you that much more of an expert ML engineer. You can find more information about how<a id="_idIndexMarker633"/> H2O performs DL training at <a href="https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml">https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml</a>. </p>
<h1 id="_idParaDest-96"><a id="_idTextAnchor128"/>Summary</h1>
<p>In this chapter, we understood the different types of prediction problems and how various algorithms aim to solve them. Then, we understood how the different ML algorithms are categorized into supervised, unsupervised, semi-supervised, and reinforcement based on their method of learning from data. Once we had an understanding of the overall problem domain of ML, we understood that H2O AutoML trains only supervised learning ML algorithms and can solve prediction problems in this domain specifically.</p>
<p>Then, we understood which algorithms H2O AutoML trains starting with GLM. To understand GLM, we understood what linear regression is and how it works and what assumptions about the normal distribution of data it has to make to be effective. With these basics in mind, we understood how GLM is generalized to be effective, even if these assumptions of linear regression are met, which is a common case in real life.</p>
<p>Then, we learned about DRF. To understand DRF, we understood what decision trees are – that is, the basic building blocks of DRF. Then, we learned that multiple decision trees with their ensembled learnings are better ML models than a normal decision tree – that is how Random Forest works. Building on top of this, we learned how DRF adds more randomization in the form of XRT to make the algorithm all the more effective with low variance and bias.</p>
<p>After that, we learned about GBM. We learned how GBM is similar to DRFs but that it has a slightly different way of learning. We understood how GBM sequentially builds decision trees and slowly minimizes error by learning from its residuals from previous decision tree prediction aggregates.</p>
<p>Finally, we learned what DL is. We understood how NNs are the building blocks of DL and their different types. We also understood how NNs perform backpropagation learning from its results and self-learn and improve the model by adjusting the weights and biases of the neurons in the middle layer.</p>
<p>This chapter gave you a brief conceptual understanding of how the various ML algorithms are trained by H2O AutoML without diving too deep into the mathematics. However, ML enthusiasts who want to become experts in the field of ML and wish to work on complex ML problems are strongly encouraged to understand the math behind the wonderful world of ML algorithms. It is the culmination of years of research and effort by scientists and enthusiasts such as yourselves that we have the capability today to potentially predict the future with the help of machines.</p>
<p>In the next chapter, we shall dive deep into understanding how you can understand if an ML model is performing optimally or not using different statistical measurements and other metrics that explain more about ML model performance.</p>
</div>
</div></body></html>