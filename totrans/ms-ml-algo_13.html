<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Belief Networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to present two probabilistic generative models that employ a set of latent variables to represent a specific data generation process. <strong>Restricted Boltzmann Machines</strong> (<strong>RBMs</strong>), proposed in 1986, are the building blocks of a more complex model, called a <strong>Deep Belief Network</strong> (<strong>DBN</strong>), which is capable of capturing complex relationships among features at different levels (in a way not dissimilar to a deep convolutional network). Both models can be used in unsupervised and supervised scenarios as preprocessors or, as is usual with DBN, fine-tuning the parameters using a standard backpropagation algorithm.</p>
<p>In particular, we will discuss:</p>
<ul>
<li><strong>Markov random fields</strong> (<strong>MRF</strong>)</li>
<li>RBM</li>
<li><strong>Contrastive Divergence</strong> (<strong>CD-k</strong>) algorithm</li>
<li>DBN with supervised and unsupervised examples</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MRF</h1>
                </header>
            
            <article>
                
<p>Let's consider a set of random variables, <em>x<sub>i</sub></em>, organized in an undirected graph, <em>G=(V, E)</em>, as shown in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/78d1aaaf-ec2e-4308-af39-97b91d0f6f98.png" style="width:26.83em;height:20.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Example of a probabilistic undirected graph</div>
<p>Two random variables, <em>a</em> and <em>b</em>, are conditionally independent given the random variable, <em>c</em> if:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d40802c-56e7-4a0f-aef4-c9a4a5758b21.png" style="width:13.25em;height:1.50em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">Now, consider the graph again; if all generic couples of subsets of variables <em>S<sub>i</sub></em> and <em>S<sub>j</sub></em> are conditionally independent given a separating subset, <em>S<sub>k</sub></em> (so that all connections between variables belonging to <em>S<sub>i</sub></em> to variables belonging to <em>S<sub>j</sub></em> pass through <em>S<sub>k</sub></em>), the graph is called a <strong>Markov random field</strong> (<strong>MRF</strong>). </p>
<p>Given <em>G=(V, E)</em>, a subset containing vertices such that every couple is adjacent is called a <strong>clique</strong> (the set of all cliques is often denoted as <em>cl(G))</em>. For example, consider the graph shown previously; <em>(x<sub>0</sub>, x<sub>1</sub>)</em> is a clique and if <em>x<sub>0</sub></em> and <em>x<sub>5</sub></em> were connected, <em>(x<sub>0</sub>, x<sub>1</sub>, x<sub>5</sub>)</em> would be a clique. A <strong>maximal clique</strong> is a clique that cannot be expanded by adding new vertices. A particular family of MRF is made up of all those graphs whose joint probability distribution can be factorized as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3a6781e8-0fcd-45e0-8fc7-3fc62a442155.png" style="width:12.42em;height:3.58em;"/></div>
<p class="mce-root">In this case, <em>α</em> is the normalizing constant and the product is extended to the set of all maximal cliques. According to the <strong>Hammersley–Clifford</strong> theorem (for further information, please refer to <em>Proof of Hammersley-Clifford Theorem</em>, <em>Cheung S., University of Kentucky, 2008</em>), if the joint probability density function is strictly positive, the MRF can be factorized and all the <em>ρ<sub>i</sub></em> functions are strictly positive too. Hence <em>p(x)</em>, after some straightforward manipulations based on the properties of logarithms, can be rewritten as a <strong>Gibbs</strong> (or <strong>Boltzmann</strong>) distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/caff2327-6603-415c-9a9b-805e948b04af.png" style="width:33.92em;height:3.17em;"/></div>
<p>The term <em>E(x)</em> is called energy, as it derives from the first application of such a distribution in statistical physics. <em>1/Z</em> is now the normalizing constant employing the standard notation. In our scenarios, we always consider graphs containing observed <em>(x<sub>i</sub>)</em> and latent variables <em>(h<sub>j</sub>)</em>. Therefore, it's useful to express the joint probability as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0fd98304-a2bb-4dc8-a801-9e354591b72a.png" style="width:12.25em;height:3.08em;"/></div>
<p>Whenever it's necessary to marginalize to obtain <em>p(x)</em>, we can simply sum over <em>h<sub>j</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7aa19f63-e8ff-46ac-906c-acbaa30e0007.png" style="width:12.58em;height:3.75em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RBMs</h1>
                </header>
            
            <article>
                
<p>A RBM (originally called <strong>Harmonium</strong>) is a neural model proposed by Smolensky (in <em>I</em><em>nformation processing in dynamical systems: Foundations of harmony theory, </em><em>Smolensky P., Parallel Distributed Processing, Vol 1, The MIT Press</em>) that is made up of a layer of input (observable) neurons and a layer of hidden (latent) neurons. A generic structure is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-medium wp-image-1362 image-border" src="assets/4e23e3da-c002-4238-bd80-aa05e33d7ba7.png" style="width:22.67em;height:10.42em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Structure of Restricted Boltzmann Machine</div>
<p>As the undirected graph is bipartite (there are no connections between neurons belonging to the same layer), the underlying probabilistic structure is MRF. In the original model (even if this is not a restriction), all the neurons are assumed to be Bernoulli-distributed <em>(x<sub>i</sub>, h<sub>i</sub> = {0, 1})</em>, with a bias, <em>b<sub>i</sub></em> (for the observed units) and <em>c<sub>j</sub></em> (for the latent neurons). The resulting energy function is:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/204d3b4b-ad7e-4053-8488-26cd2fb2586e.png" style="width:27.75em;height:3.17em;"/></div>
<p class="mce-root">A RBM is a probabilistic generative model that can learn a data-generating process, <em>p<sub>data</sub></em>, which is represented by the observed units but exploits the presence of the latent variables in order to model all the internal relationships. If we summarized all the parameters in a single vector, <em>θ = {w<sub>ij</sub>, b<sub>i</sub>, c<sub>j</sub>}</em>, the Gibbs distribution becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1683c289-da77-49c0-9c58-4e473d85ddbe.png" style="width:27.58em;height:4.42em;"/></div>
<p>The training goal of a RBM is to maximize the log-likelihood with respect to an input distribution. Hence, the first step is determining <em>L(θ; x)</em> after the marginalization of the previous expression:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0caa878b-b9a4-48c6-b8ac-8ef0e40261e4.png" style="width:53.00em;height:4.00em;"/></div>
<p class="mce-root">As we need to maximize the log-likelihood, it's useful to compute the gradient with respect to <em>θ</em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/773cb0ac-76e3-4105-bc66-f69e1f97022c.png" style="width:35.92em;height:3.83em;"/></div>
<p class="mce-root">Applying the chain rule of derivatives, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/da3493f6-5cad-4768-b4cb-8571af2f0bac.png" style="width:58.00em;height:4.83em;"/></div>
<p>Using the conditional and joint probability equalities, the previous expression becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/86e3b4e3-2e5d-45d8-9d21-f2ebd696ddcb.png" style="width:42.75em;height:3.33em;"/></div>
<p class="mce-root">Considering the full joint probability, after some tedious manipulations (which we omit), it's possible to derive the following expressions (<em>σ(•)</em> is the sigmoid function):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ed282e75-cbb0-4076-b3b6-1ac21c45924e.png" style="width:19.33em;height:4.42em;"/></div>
<p class="mce-root">At this point, we can compute the gradient of the log-likelihood with respect to each single parameter, <em><span>w</span><sub>ij</sub></em><span>, <em>b</em></span><em><sub>i</sub></em><span>, and <em>c</em></span><em><sub>j</sub></em>. Starting with <em><span>w</span><sub>ij</sub></em>, and considering that <em>∇<sub>wij</sub> E(x, h; θ) = -x<sub>i</sub>h<sub>j</sub></em>, we get:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8f7b51aa-8366-498c-b37a-ca9042c5d5ae.png" style="width:30.92em;height:3.17em;"/></div>
<p>The expression can be rewritten as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/28b42155-e075-4942-83c0-555285d9340e.png" style="width:35.75em;height:3.25em;"/></div>
<p>Now, considering that all the units are Bernoulli-distributed, and isolating only the <em>j<sup>th</sup></em> hidden unit, it's possible to apply the simplification:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5097a2ee-27cf-4eba-8a95-2fd06a5b75f5.png" style="width:58.58em;height:7.83em;"/></div>
<p class="mce-root">Therefore, the gradient becomes:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d12c1d4-c022-4040-a344-d6993cc18acb.png" style="width:31.50em;height:3.00em;"/></div>
<p>Analogously, we can derive the gradient of <em>L</em> with respect to <em>b</em><sub><em>i</em> </sub>and <em>c<sub>j</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6265c30f-5188-4089-a8b9-4ba746d8a035.png" style="width:28.75em;height:4.00em;"/></div>
<p class="mce-root">Hence, the first term of every gradient is very easy to compute, while the second one requires summing over all observed values. As this operation is impracticable, the only feasible alternative is an approximation based on sampling, using a method such as Gibbs sampling (for further information, see <a href="0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml" target="_blank">Chapter 4</a>, <em>Bayesian Networks and Hidden Markov Models</em>). However, as this algorithm samples from the conditionals <em>p(x|h)</em> and <em>p(h|x)</em>, rather than from the full joint distribution <em>p(x, h)</em>, it requires the associated Markov chain to reach its stationary distribution, <em>π</em>, in order to provide valid samples. As we don't know how many sampling steps are required to reach <span><em>π</em>, Gibbs sampling can also be an unfeasible solution because of its potentially high computational cost.</span></p>
<p class="mce-root"><span>In order to solve this problem, Hinton proposed (in <em>A Practical Guide to Training Restricted Boltzmann Machines, </em><em>Hinton G., Dept. Computer Science, University of Toronto</em>) an alternative algorithm called</span> <strong>CD-k</strong>. The idea is very simple but extremely effective: instead of waiting for the Markov chain to reach the stationary distribution, we sample a fixed number of times starting from a training sample at <em>t=0 x<sup>(0)</sup></em> and computing <em>h<sup>(1)</sup></em> by sampling from <em>p(<span>h</span><sup>(1)</sup>|<span>x</span><sup>(0)</sup>)</em>. Then, the hidden vector is employed to sample the reconstruction, <em>x<sup>(2)</sup></em>, from <em>p(<span>x</span><sup>(2)</sup>|<span>h</span><sup>(1)</sup>)</em>. This procedure can be repeated any number of times, but in practice, a single sampling step is normally enough to ensure quite good accuracy. At this point, the gradient of the log-likelihood is approximated as (considering <em>t</em> steps):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/965a0a11-806a-41f2-bd50-f53c1bc01e98.png" style="width:58.08em;height:3.92em;"/></div>
<p class="mce-root">The single gradients with respect to <em><span>w</span><sub>ij</sub></em><span>, <em>b</em></span><em><sub>i</sub></em><span>, and <em>c</em></span><em><sub>j</sub></em> can be easily obtained considering the preceding procedure. The term <em>contrastive</em> derives from the approximation of the gradient of <em>L</em> computed at <em><span>x</span></em><sup><em>(0)</em> </sup>with a weighted difference between a term called the <em>positive gradient</em> and another defined as the <em>negative gradient</em>. This approach is analogous to the approximation of a derivative with this incremental ratio:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f8d9d1c3-6a85-42b1-a79d-55f7ea07d8af.png" style="width:23.33em;height:2.67em;"/></div>
<p class="mce-root">The complete RBM training algorithm, based on a single-step CD-k is (assuming that there are <em>M</em> training samples):</p>
<ol>
<li>Set the number, <em>N<sub>h</sub></em>, of hidden units</li>
<li>Set a number of epochs, <em>N<sub>e</sub></em></li>
<li>Set a <kbd>learning_rate</kbd><em> η</em> (for example, <span><em>η = 0.01</em>)</span></li>
<li>For <em>e=1</em> to <em>N<sub>e</sub></em>:
<ol>
<li>Set <span><em>Δw = 0</em>, <em>Δb = 0</em>, and <em>Δc = 0</em></span></li>
<li>For <em>i=1</em> to <em>M</em>:
<ol>
<li>Sample <em>h<sup>(i)</sup></em> from <em>p(h|x<sup>(i)</sup>)</em></li>
<li>Sample a reconstruction <em>x<sup>(i+1)</sup></em> from <em><span>p(x</span><sup>(i+1)</sup>|<span>h</span><sup>(i)</sup><span>)</span></em> </li>
<li>Accumulate the updates for weights and biases:
<ol>
<li><em>Δw += p(h = 1|<span>x</span><sup>(i)</sup></em><span><em>)x<sup>(i)</sup> - p(h = 1|x<sup>(i+1)</sup>)x<sup>(i+1)</sup></em> (as outer product)</span></li>
<li><em><span>Δb += x<sup>(i)</sup> - x<sup>(i+1)</sup></span></em></li>
<li><em><span>Δc<span> </span>+= p(h = 1|x<sup>(i)</sup>) - p(h = 1|x<sup>(i+1)</sup>)</span></em></li>
</ol>
</li>
</ol>
</li>
<li>Update weights and biases:
<ol>
<li><em>w += η<span>Δw</span></em></li>
<li><em>b += η<span>Δb</span></em></li>
<li><em>c += η<span>Δc</span></em></li>
</ol>
</li>
</ol>
</li>
</ol>
<p class="mce-root">The outer product between two vectors is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c9249259-6881-455f-8b01-1091e0686677.png" style="width:31.83em;height:6.00em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign CDPAlignCenter"><span>If vector <em>a</em> has an <em>(n, 1)</em> shape and <em>b</em> has an <em>(m, 1)</em> shape, the result is a matrix with a <em>(n, m)</em> shape.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DBNs</h1>
                </header>
            
            <article>
                
<p>A Belief or Bayesian network is a concept already explored in <a href="0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml" target="_blank">Chapter 4</a>, <em>Bayesian Networks and Hidden Markov Models</em>. In this particular case, we are going to consider Belief Networks where there are visible and latent variables, organized into homogeneous layers. The first layer always contains the input (visible) units, while all the remaining ones are latent. Hence, a DBN can be structured as a stack of RBMs, where each hidden layer is also the visible one of the subsequent RBM, as shown in the following diagram (the number of units can be different for each layer):</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-medium wp-image-1363 image-border" src="assets/48c9ddc4-bedc-4d1b-8287-bc07533adaf2.png" style="width:19.75em;height:23.25em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Structure of a generic Deep Belief Network</div>
<p class="mce-root">The learning procedure is usually greedy and step-wise (as proposed in <em>A fast learning algorithm for deep belief nets</em>,<em> </em><em>Hinton G. E.</em>, <em>Osindero S.</em>, <em>Teh Y. W.</em>, <em>Neural Computation, 18/7</em>). The first RBM is trained with the dataset and optimized to reconstruct the original distribution using the CD-k algorithm. At this point, the internal (hidden) representations are employed as input for the next RBM, and so on until all the blocks are fully trained. In this way, the DBN is forced to create subsequent internal representations of the dataset that can be used for different purposes. Of course, when the model is trained, it's possible to infer from the recognition (inverse) model sampling from the hidden layers and compute the activation probability as (<em>x</em> represents a generic cause):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/08b8c50d-ce2e-42a8-a5e2-7569624636ff.png" style="width:19.08em;height:4.17em;"/></div>
<p class="mce-root">As a DBN is always a generative process, in an unsupervised scenario, it can perform a component analysis/dimensionality reduction with an approach that is based on the idea of creating a chain of sub-processes, which are able to rebuild an internal representation. While a single RBM focuses on a single hidden layer and hence cannot learn sub-features, a DBN greedily learns how to represent each sub-feature vector using a refined hidden distribution. The concept behind this process is not very different from a cascade of convolutional layers, with the main difference that in this case, the learning procedure is greedy. Another distinction with methods such as PCA is that we don't know exactly how the internal representation is built. As the latent variables are optimized by maximizing the log-likelihood, there are possibly many optimal points but we cannot easily impose constraints on them. However, DBNs show very powerful properties in different scenarios, even if their computational cost is normally considerably higher than other methods. One of the main problems (common to the majority of deep learning methods) concerns the right choice of hidden units in every layer. As they represent latent variables, their number is a crucial factor for the success of a training procedure. The right choice is not immediate, because it's necessary to know the complexity of the data-generating process, however, as a rule of thumb, I suggest starting with a couple of layers containing 32/64 units and proceeding to increase the number of hidden neurons and the layers until the desired accuracy is reached (in the same way, I suggest starting with a small learning rate, for example, 0.01 -, increasing it if necessary).</p>
<p class="mce-root">As the first RBM is responsible for reconstructing the original dataset, it's very useful to monitor the log-likelihood (or the error) after each epoch in order to understand whether the process is learning correctly (decreasing error) or it's saturating the capacity. It's clear that an initial bad reconstruction leads to subsequently worse representations. As the learning process is greedy, in an unsupervised task there's no way to improve the performance of lower layers when the previous training steps are finished therefore, I always suggest tuning up the parameters so that the first reconstruction is very accurate. Of course, all the considerations about overfitting are still valid, so, it's also important to monitor the generalization ability with validation samples. However, in a component analysis, we assume we're working with a distribution that is representative of the underlying data-generating process, so the risk of finding before-seen features should be minimal.</p>
<p>In a supervised scenario, there are generally two options whose first step is always a greedy training of the DBN. However, the first approach performs a subsequent refinement using a standard algorithm, such as backpropagation (considering the whole architecture as a single deep network), while the second one uses the last internal representation as the input of a separate classifier. It goes without saying that the first method has many more degrees of freedom because it works with a pre-trained network whose weights can be adjusted until the validation accuracy reaches its maximum value. In this case, the first greedy step works with the same assumption that has been empirically confirmed by observing the internal behavior of deep models (similar to convolutional networks). The first layers learn how to detect low-level features, while all the subsequent ones increase the details. Therefore, the backpropagation step presumably starts from a point that is already quite close to the optimum and can converge more quickly. Conversely, the second approach is analogous to applying the kernel trick to a standard <strong>Support Vector Machine</strong> (<strong>SVM</strong>). In fact, the external classifier is generally a very simple one (such as a logistic regression or an SVM) and the increased accuracy is normally due to an improved linear separability obtained by projecting the original samples onto a sub-space (often higher-dimensional) where they can be easily classified. In general, this method yields worse performance than the first one because there's no way to tune up the parameters once the DBN is trained. Therefore, when the final projections are not suitable for a linear classification, it's necessary to employ more complex models and the resulting computational cost can be very high without a proportional performance gain. As deep learning is generally based on the concept of end-to-end learning, training the whole network can be useful to implicitly include the pre-processing steps in the complete structure, which becomes a black box that associates input samples with specific outcomes. On the other hand, whenever an explicit pipeline is requested, greedy-training the DBN and employing a separate classifier could be a more suitable solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of unsupervised DBN in Python</h1>
                </header>
            
            <article>
                
<p>In this example, we are going to use a Python library freely available on GitHub (<a href="https://github.com/albertbup/deep-belief-network">https://github.com/albertbup/deep-belief-network</a>) that allows working with supervised and unsupervised DBN using NumPy (CPU-only) or Tensorflow (CPU or GPU support) with the standard Scikit-Learn interface. Our goal is to create a lower-dimensional representation of a subset of the <kbd>mnist</kbd> dataset (as the training process can be quite slow, we'll limit it to <kbd>400</kbd> samples). The first step is loading (using the Keras helper function), shuffling, and normalizing the dataset:</p>
<pre>import numpy as np<br/><br/>from keras.datasets import mnist<br/>from sklearn.utils import shuffle<br/><br/>(X_train, Y_train), (_, _) = mnist.load_data()<br/>X_train, Y_train = shuffle(X_train, Y_train, random_state=1000)<br/><br/>nb_samples = 400<br/><br/>width = X_train.shape[1]<br/>height = X_train.shape[2]<br/><br/>X = X_train[0:nb_samples].reshape((nb_samples, width * height)).astype(np.float32) / 255.0<br/>Y = Y_train[0:nb_samples]</pre>
<p>At this point, we can create an instance of the the <kbd>UnsupervisedDBN</kbd> class, setting three layers with respectively <kbd>512</kbd>, <kbd>256</kbd>, and <kbd>64</kbd> sigmoid units (as we want to bind the values between <kbd>0</kbd> and <kbd>1</kbd>). The learning rate, <em>η</em> (<kbd>learning_rate_rbm</kbd>), is set equal to <kbd>0.05</kbd>, the batch size (<kbd>batch_size</kbd>) to <kbd>64</kbd>, and the number of epochs for each RBM (<kbd>n_epochs_rbm</kbd>) to <kbd>100</kbd>. The default value for the number of CD-k steps is <kbd>1</kbd>, but it's possible to change it using the <kbd>contrastive_divergence_iter</kbd> parameter:</p>
<pre>from dbn.tensorflow import UnsupervisedDBN<br/><br/>unsupervised_dbn = UnsupervisedDBN(hidden_layers_structure=[512, 256, 64],<br/>                                   learning_rate_rbm=0.05,<br/>                                   n_epochs_rbm=100,<br/>                                   batch_size=64,<br/>                                   activation_function='sigmoid')<br/><br/>X_dbn = unsupervised_dbn.fit_transform(X)<br/><br/>[START] Pre-training step:
&gt;&gt; Epoch 1 finished       RBM Reconstruction error 55.562027
&gt;&gt; Epoch 2 finished       RBM Reconstruction error 53.663380<br/><br/>...<br/><br/>&gt;&gt; Epoch 99 finished       RBM Reconstruction error 5.169244
&gt;&gt; Epoch 100 finished     RBM Reconstruction error 5.130809
[END] Pre-training step</pre>
<p>Once the training process is complete, the <kbd>X_dbn</kbd> array contains the values sampled from the last hidden layer. Unfortunately, this library doesn't implement an inverse transformation method, but we can use the t-SNE algorithm to project the distribution onto a bidimensional space:</p>
<pre>from sklearn.manifold import TSNE<br/><br/>tsne = TSNE(n_components=2, perplexity=20, random_state=1000)<br/>X_tsne = tsne.fit_transform(X_dbn)</pre>
<p>The corresponding plot is shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1368 image-border" src="assets/2709e21a-b1ee-4023-95ea-211d043f6c88.png" style="width:76.08em;height:42.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"> t-SNE plot of the last DBN hidden layer distribution (64-dimensional)</div>
<p>As you can see, even if there are still a few anomalies, the hidden low-dimensional representation is globally coherent with the original dataset because the group containing the same digits is organized in compact clusters that preserve some geometrical properties. For example, the group containing the digits representing a <strong>1</strong> is very close to the one containing the images of 7s, as well as the groups of 3s and 8s. This result confirms that a DBN can be successfully employed as a preprocessing layer for classification purposes, but in this case, rather than reducing the dimensionality, it's often preferable to increase it, in order to exploit the redundancy to use a simpler linear classifier (to better understand this concept, think about augmenting a dataset with polynomial features). I invite you to test this ability by preprocessing the whole MNIST dataset and then classifying it using a logistic regression, comparing the results with a direct approach.</p>
<div class="packt_infobox">The library can be installed using the <kbd>pip install git+git://github.com/albertbup/deep-belief-network.git</kbd> command (NumPy or Tensorflow CPU) or <kbd>pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu</kbd> (Tensorflow GPU). In both cases, the commands will also install Tensorflow and other dependencies that are often present in common Python distributions (such as Anaconda); therefore, in order to limit the installation only to the DBN component, it's necessary to add the <kbd>--no-deps</kbd> attribute to the <kbd>pip</kbd> command. For further information, please refer to the GitHub page.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Supervised DBN with Python</h1>
                </header>
            
            <article>
                
<p>In this example, we are going to employ the KDD Cup '99 dataset (provided by Scikit-Learn), which contains the logs generated by an intrusion detection system exposed to normal and dangerous network activities. We are focusing only on the <kbd>smtp</kbd> sub-dataset, which is the smallest one, because, as explained before, the training process can be very long. This dataset is not extremely complex and it can be successfully classified with simpler methods; however, the example has only a didactic purpose and can be useful for understanding how to work with this kind of data.</p>
<p>The first step is to load the dataset, encode the labels (which are strings), and standardize the values:</p>
<pre>from sklearn.datasets import fetch_kddcup99<br/>from sklearn.preprocessing import LabelEncoder, StandardScaler<br/><br/>kddcup = fetch_kddcup99(subset='smtp', shuffle=True, random_state=1000)<br/><br/>ss = StandardScaler()<br/>X = ss.fit_transform(kddcup['data']).astype(np.float32)<br/><br/>le = LabelEncoder()<br/>Y = le.fit_transform(kddcup['target']).astype(np.float32)</pre>
<p>At this point, we can create train and test sets:</p>
<pre>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=1000)</pre>
<p>The model is based on an instance of the <kbd>SupervisedDBNClassification</kbd> class, which implements the backpropagation method. The parameters are very similar to the unsupervised case, but now we can also specify the <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) learning rate (<kbd>learning_rate</kbd>), the number of backpropagation epochs (<kbd>n_iter_backprop</kbd>), and an optional dropout (<kbd>dropout_p</kbd>). The algorithm performs an initial greedy training (whose computational cost is normally higher than the SGD phase), followed by a fine-tuning:</p>
<pre>from dbn.tensorflow import SupervisedDBNClassification<br/><br/>classifier = SupervisedDBNClassification(hidden_layers_structure=[64, 64],<br/>                                         learning_rate_rbm=0.001,<br/>                                         learning_rate=0.01,<br/>                                         n_epochs_rbm=20,<br/>                                         n_iter_backprop=150,<br/>                                         batch_size=256,<br/>                                         activation_function='relu',<br/>                                         dropout_p=0.25)<br/><br/>classifier.fit(X_train, Y_train)<br/><br/>[START] Pre-training step:
&gt;&gt; Epoch 1 finished       RBM Reconstruction error 2.478997
&gt;&gt; Epoch 2 finished       RBM Reconstruction error 2.459004<br/><br/>...<br/><br/>&gt;&gt; Epoch 147 finished       ANN training loss 0.006651
&gt;&gt; Epoch 148 finished     ANN training loss 0.006631
&gt;&gt; Epoch 149 finished     ANN training loss 0.006612
[END] Fine tuning step<br/><br/>SupervisedDBNClassification(batch_size=256, dropout_p=0.25,
              idx_to_label_map={0: 1.0, 1: 0.0, 2: 2.0},
              l2_regularization=1.0,
              label_to_idx_map={0.0: 1, 1.0: 0, 2.0: 2},
              learning_rate=0.01, n_iter_backprop=150, verbose=True)</pre>
<p>Once the training process is finished, we can evaluate performance on the test set:</p>
<pre>from sklearn.metrics.classification import accuracy_score<br/><br/>Y_pred = classifier.predict(X_test)<br/>print(accuracy_score(Y_test, Y_pred))<br/>1.0</pre>
<p>The validation accuracy is <kbd>1.0</kbd> (there are no misclassifications), but this is really a simple dataset that needs only a few minutes of training. I invite you to test the performance of a DBN in the classification of the MNIST/Fashion MNIST dataset, comparing the results with the one obtained using a deep convolutional network. In this case, it's important to monitor the reconstruction error of each RBM, trying to minimize it before running the backpropagation phase. At the end of this exercise, you should be able to answer this question: which is preferable, an end-to-end or a preprocessing-based approach?</p>
<div class="packt_tip">When running these experiments, where there's an intensive use of sampling, I always suggest setting the random seed (and keeping it constant) in NumPy in order to guarantee reproducibility (using the <kbd>np.random.seed(...)</kbd> command). As this library also works with Tensorflow, it's necessary to repeat the operation using the <kbd>tf.set_random_seed(...)</kbd> command.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we presented the MRF as the underlying structure of an RBM. An MRF is represented as an undirected graph whose vertices are random variables. In particular, for our purposes, we considered MRFs whose joint probability can be expressed as a product of the positive functions of each random variable. The most common distribution, based on an exponential, is called the Gibbs (or Boltzmann) distribution and it is particularly suitable for our problems because the logarithm cancels the exponential, yielding simpler expressions.</p>
<p>An RBM is a simple bipartite, undirected graph, made up of visible and latent variables, with connections only between different groups. The goal of this model is to learn a probability distribution, thanks to the presence of hidden units that can model the unknown relationships. Unfortunately, the log-likelihood, although very simple, cannot be easily optimized because the normalization term requires summing over all the input values. For this reason, Hinton proposed an alternative algorithm, called CD-k, which outputs an approximation of the gradient of the log-likelihood based on a fixed number (normally 1) of Gibbs sampling steps.</p>
<p>Stacking multiple RBMs allows modeling DBNs, where the hidden layer of each block is also the visible layer of the following one. DBN can be trained using a greedy approach, maximizing the log-likelihood of each RBM in sequence. In an unsupervised scenario, a DBN is able to extract the features of a data-generating process in a hierarchical way, and therefore the application includes component analysis and dimensionality reduction. In a supervised scenario, a DBN can be greedily pre-trained and fine-tuned using the backpropagation algorithm (considering the whole network) or sometimes using a preprocessing step in a pipeline where the classifier is generally a very simple model (such as a logistic regression).</p>
<p> In the next chapter, <a href="51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml" target="_blank">Chapter 14</a>, <em>Introduction to Reinforcement Learning</em>, we are going to introduce the concept of reinforcement learning, discussing the most important elements of systems that can autonomously learn to play a game or allow a robot to walk, jump, and perform tasks that are extremely difficult to model and control using classic methods.</p>


            </article>

            
        </section>
    </body></html>