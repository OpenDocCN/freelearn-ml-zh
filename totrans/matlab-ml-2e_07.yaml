- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Natural Language Processing Using MATLAB
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MATLAB进行自然语言处理
- en: '**Natural language processing** (**NLP**) automatically processes information
    conveyed through spoken or written language. This task is fraught with difficulty
    and complexity, largely due to the innate ambiguity of human language. To enable
    **machine learning** (**ML**) and interaction with the world in ways typical of
    humans, it is essential not only to store data but also to teach machines how
    to translate this data simultaneously into meaningful concepts. As natural language
    interacts with the environment, it generates predictive knowledge. In this chapter,
    we will learn the basic concepts of NLP and how to build a model to label sentences.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）自动处理通过口语或书面语言传达的信息。这项任务充满困难和复杂性，很大程度上是由于人类语言的固有歧义。为了使**机器学习**（**ML**）和以人类典型方式与世界互动，不仅需要存储数据，还需要教会机器如何将数据同时转换为有意义的概念。随着自然语言与环境的互动，它会产生预测性知识。在本章中，我们将学习NLP的基本概念以及如何构建一个用于标记句子的模型。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主要主题：
- en: Explaining NLP
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释NLP
- en: Exploring corpora and word and sentence tokenize
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索语料库和单词及句子分词
- en: Implementing a MATLAB model to label sentences
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个用于标记句子的MATLAB模型
- en: Understanding gradient boosting techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度提升技术
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will introduce basic ML concepts. To understand these topics,
    a basic knowledge of algebra and mathematical modeling is needed. You will also
    require working knowledge of the MATLAB environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍基本的机器学习概念。为了理解这些主题，需要具备代数和数学建模的基本知识。你还需要熟悉MATLAB环境。
- en: 'To work with the MATLAB code in this chapter, you’ll need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用本章中的MATLAB代码，你需要以下文件（可在GitHub上找到：[https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)）：
- en: '`IMDBSentimentClassification.m`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IMDBSentimentClassification.m`'
- en: '`ImdbDataset.xlsx`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImdbDataset.xlsx`'
- en: Explaining NLP
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释NLP
- en: 'NLP is a field that’s dedicated to the development of technology that enables
    computers to interact with, understand, and generate human language in a way that
    mimics natural human communication. This involves various techniques and approaches
    aimed at processing and analyzing the complexities of natural languages, such
    as English, Chinese, Arabic, and more. The goal is to bridge the gap between human
    language and computer language, allowing computers to comprehend and generate
    text as if they were engaging in a conversation with a human interlocutor (*Figure
    7**.1*):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一个致力于开发使计算机能够以模仿自然人类交流的方式与人类语言进行交互、理解和生成技术的领域。这涉及到各种技术和方法，旨在处理和分析自然语言的复杂性，如英语、中文、阿拉伯语等。目标是弥合人类语言和计算机语言之间的差距，使计算机能够理解并生成文本，就像它们在与人类对话者进行对话一样（**图7.1**.1*）：
- en: '![Figure 7.1 – NLP tasks](img/B21156_07_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – NLP任务](img/B21156_07_01.jpg)'
- en: Figure 7.1 – NLP tasks
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – NLP任务
- en: NLP strives to develop information technology tools for analyzing, comprehending,
    and creating texts in a manner that resonates with human understanding, mimicking
    interactions with another human rather than a machine. Natural language, both
    spoken and written, represents the most instinctive and widespread mode of communication.
    In contrast to formal languages, it holds a greater level of intricacy, often
    carrying connotations and uncertainties, which renders its processing quite challenging.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NLP致力于开发分析、理解和创建文本的信息技术工具，这些工具与人类理解产生共鸣，模仿与另一人类的互动，而不是与机器的互动。自然语言，无论是口语还是书面语，都是最本能和最普遍的交流方式。与正式语言相比，它具有更高的复杂性，经常携带隐含意义和不确定性，这使得其处理相当具有挑战性。
- en: 'NLP encompasses a range of tasks, including but not limited to the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: NLP包括一系列任务，包括但不限于以下内容：
- en: '**Text understanding**: This involves extracting meaning and information from
    text. It includes tasks such as sentiment analysis (determining the emotional
    tone of a text), named entity recognition (identifying names of people, places,
    organizations, and so on), and text classification (categorizing text into predefined
    classes).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本理解**：这涉及到从文本中提取意义和信息。它包括诸如情感分析（确定文本的情感基调）、命名实体识别（识别人名、地点、组织等名称）和文本分类（将文本分类到预定义的类别）等任务。'
- en: '**Language generation**: This aspect focuses on generating coherent and contextually
    appropriate human-like text. It includes tasks such as machine translation (translating
    text from one language to another), text summarization (creating concise summaries
    of longer texts), and dialogue generation (constructing natural-sounding conversational
    responses).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言生成**：这一方面专注于生成连贯且符合上下文的类似人类的文本。它包括诸如机器翻译（将文本从一种语言翻译成另一种语言）、文本摘要（创建较长的文本的简洁摘要）和对话生成（构建自然的对话回应）等任务。'
- en: '**Speech recognition**: NLP also extends to the realm of spoken language. Speech
    recognition technology converts spoken language into text, enabling applications
    such as voice assistants and transcription services.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：NLP也扩展到口语领域。语音识别技术将口语转换为文本，使得语音助手和转录服务等应用成为可能。'
- en: '**Language models**: Recent advances in NLP have led to the development of
    large language models such as GPT-3, which can generate remarkably human-like
    text based on the input it receives. These models are trained on massive amounts
    of text data and can be fine-tuned for various NLP tasks.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言模型**：NLP的最近进展导致了大型语言模型如GPT-3的开发，这些模型可以根据接收到的输入生成非常类似人类的文本。这些模型在大量文本数据上训练，并且可以根据各种NLP任务进行微调。'
- en: '**Chatbots and virtual assistants**: NLP is the foundation of chatbots and
    virtual assistants, which can engage in text-based or voice-based conversations
    with users, providing information, assistance, and responses.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人和虚拟助手**：NLP是聊天机器人和虚拟助手的基础，它们可以与用户进行基于文本或语音的对话，提供信息、协助和回应。'
- en: 'The field of NLP is interdisciplinary, drawing from computer science, linguistics,
    cognitive psychology, and more. It involves working with linguistic structures,
    statistical models, ML algorithms, and **deep learning** (**DL**) techniques to
    process, understand, and generate human language. While significant progress has
    been made, NLP still faces challenges, such as handling context, understanding
    nuances, and truly comprehending the intricacies of human communication:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: NLP领域是跨学科的，从计算机科学、语言学、认知心理学等领域汲取知识。它涉及到处理语言结构、统计模型、机器学习算法以及**深度学习**（**DL**）技术来处理、理解和生成人类语言。虽然已经取得了重大进展，但NLP仍然面临挑战，如处理上下文、理解细微差别以及真正理解人类交流的复杂性：
- en: "![Figure 7.2 – Text analysis and text generation in \uFEFFNLP](img/B21156_07_02.jpg)"
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – NLP中的文本分析和文本生成](img/B21156_07_02.jpg)'
- en: Figure 7.2 – Text analysis and text generation in NLP
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – NLP中的文本分析和文本生成
- en: 'The pursuits in this domain encompass two main objectives: text analysis and
    text generation. These principles give rise to the subsequent disciplines, known
    as **natural language analysis** (**NLA**) and **natural language generation**
    (**NLG**). We’ll delve deep into both.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域的追求包括两个主要目标：文本分析和文本生成。这些原则导致了后续学科的产生，即**自然语言分析**（**NLA**）和**自然语言生成**（**NLG**）。我们将深入探讨这两个领域。
- en: NLA
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLA
- en: 'This field centers on enhancing machines’ grasp of natural language. It involves
    transforming a natural language text into a structured and unequivocal representation.
    NLA involves the process of enabling machines to comprehend and interpret human
    language. Its primary goal is to bridge the gap between the unstructured nature
    of natural language and the structured representation that computers can work
    with. NLA encompasses several tasks:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个领域集中在增强机器对自然语言的理解能力。它涉及到将自然语言文本转换成结构化和明确的表现形式。NLA包括使机器能够理解和解释人类语言的过程。其首要目标是弥合自然语言的无结构性质与计算机可以处理的结构化表示之间的差距。NLA包括几个任务：
- en: '**Syntax analysis**: This involves parsing the grammatical structure of sentences
    to understand the relationships between words and their roles (subject, object,
    verb, and so on). It helps in identifying how words are organized to convey meaning.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句法分析**：这涉及到解析句子的语法结构，以理解词语之间的关系及其角色（主语、宾语、动词等）。它有助于识别词语是如何组织起来传达意义的。'
- en: '**Semantic understanding**: NLA seeks to comprehend the meaning of words and
    phrases in context. It involves extracting the underlying concepts and intentions
    behind the words used in a text.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义理解**：NLA 旨在理解词语和短语在上下文中的含义。它涉及从文本中使用的词语中提取潜在的概念和意图。'
- en: '**Named entity recognition** (**NER**): NER is the process of identifying and
    classifying named entities, such as the names of people, organizations, locations,
    dates, and more, within a text.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（NER）：NER 是识别和分类文本中命名实体的过程，例如人名、组织、地点、日期等。'
- en: '**Sentiment analysis**: Sentiment analysis aims to determine the emotional
    tone expressed in a piece of text, classifying it as positive, negative, neutral,
    or even more nuanced emotions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：情感分析旨在确定文本中表达的情感基调，将其分类为正面、负面、中性，甚至更细微的情感。'
- en: '**Coreference resolution**: Coreference refers to cases where different words
    or phrases in a text refer to the same entity. Coreference resolution identifies
    and connects these references to provide a coherent understanding of the text.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指代消解**：指代是指文本中不同词语或短语指代同一实体的情况。指代消解识别并连接这些引用，以提供对文本的连贯理解。'
- en: '**Text classification**: Text classification involves categorizing text into
    predefined classes or categories based on its content. This is used for tasks
    such as spam detection, topic classification, and content tagging.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本分类**：文本分类是指根据文本内容将其归类到预定义的类别或类别中。这用于诸如垃圾邮件检测、主题分类和内容标记等任务。'
- en: '**Information extraction**: This involves extracting specific information or
    structured data from unstructured text. An example of this is extracting dates,
    events, relationships, or numerical data from news articles.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息提取**：这涉及到从非结构化文本中提取特定信息或结构化数据。一个例子是从新闻文章中提取日期、事件、关系或数值数据。'
- en: '**Dependency parsing**: Dependency parsing analyzes the grammatical relationships
    between words in a sentence, often represented as a tree structure that shows
    how words depend on each other.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依存句法分析**：依存句法分析分析句子中词语之间的语法关系，通常表示为树状结构，显示词语如何相互依赖。'
- en: '**Language modeling**: Language models, often trained using ML techniques,
    are used to predict the likelihood of words or phrases occurring based on the
    context of the surrounding text. These models play a crucial role in understanding
    the probabilities and patterns of language.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言模型**：语言模型通常使用机器学习技术训练，用于根据周围文本的上下文预测词语或短语出现的可能性。这些模型在理解语言的概率和模式方面发挥着关键作用。'
- en: '**Parsing ambiguity**: Languages often contain ambiguous structures that can
    be interpreted in multiple ways. NLA aims to resolve such ambiguities to arrive
    at the intended meaning of a sentence.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解析歧义**：语言往往包含可以以多种方式解释的模糊结构。NLA 旨在解决这些歧义，以达到句子的预期含义。'
- en: NLA draws from linguistic theories, computational linguistics, and ML. It’s
    a complex field that continues to evolve, driven by advances in technology and
    the increasing demand for machines to understand and process human language in
    diverse applications, including search engines, chatbots, sentiment analysis tools,
    and more.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NLA 从语言学理论、计算语言学和机器学习（ML）中汲取灵感。它是一个复杂的领域，随着技术的进步和对机器在多样化应用中理解和处理人类语言需求的增加而不断发展，包括搜索引擎、聊天机器人、情感分析工具等。
- en: NLG
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLG
- en: 'Here, the emphasis is on enabling machines to construct sentences in natural
    language. This involves developing applications that can produce accurate sentences
    in a specific language. NLG is a pivotal aspect of NLP that revolves around the
    creation of human-like text by computers. The main objective of NLG is to enable
    machines to generate coherent, contextually appropriate, and linguistically accurate
    textual content, mimicking the way humans communicate. NLG encompasses various
    techniques and processes to transform structured data or information into readable
    and understandable language. Here are some key points regarding NLG:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，重点是使机器能够用自然语言构造句子。这涉及到开发能够产生特定语言中准确句子的应用程序。NLG 是 NLP 的一个关键方面，围绕计算机创建类似人类的文本。NLG
    的主要目标是使机器能够生成连贯、上下文适当和语言准确的文本内容，模仿人类的交流方式。NLG 包含各种技术和过程，将结构化数据或信息转换为可读和可理解的语言。以下是关于
    NLG 的几个要点：
- en: '**Machine translation**: One of the earliest forms of NLG, machine translation
    involves translating text from one language into another while retaining its meaning
    and context. Modern machine translation systems often utilize neural network-based
    approaches for improved accuracy.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：机器翻译是NLG的最早形式之一，涉及将文本从一种语言翻译成另一种语言，同时保留其意义和上下文。现代机器翻译系统通常利用基于神经网络的途径以提高准确性。'
- en: '**Text summarization**: NLG is employed in summarizing longer texts, such as
    articles or documents, by condensing the main ideas and relevant information into
    a concise and coherent summary.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**：NLG用于总结较长的文本，如文章或文档，通过将主要思想和相关信息压缩成一个简洁且连贯的摘要。'
- en: '**Dialogue generation**: NLG powers chatbots, virtual assistants, and other
    conversational agents. It involves constructing natural-sounding responses in
    conversations, considering the ongoing context and user inputs.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话生成**：NLG为聊天机器人、虚拟助手和其他对话代理提供动力。它涉及在对话中构建自然响亮的回应，考虑持续的上下文和用户输入。'
- en: '**Data-to-text generation**: NLG is used to convert structured data, such as
    statistical figures or database entries, into human-readable narratives. An example
    of this is generating weather reports from weather data.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据到文本生成**：NLG用于将结构化数据，如统计数据或数据库条目，转换为人类可读的叙述。例如，从天气数据生成天气预报。'
- en: '**Content creation**: NLG systems can create articles, reports, and other content
    based on predefined templates or prompts. This can be particularly useful for
    generating routine reports or news updates.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容创作**：NLG系统可以根据预定义的模板或提示创建文章、报告和其他内容。这对于生成常规报告或新闻更新特别有用。'
- en: '**Personalized messaging**: NLG can be employed to generate personalized messages
    or recommendations tailored to individual users – for instance, crafting product
    recommendations based on a user’s browsing history.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化信息**：NLG可以用于生成针对个人用户的个性化信息或推荐，例如，根据用户的浏览历史创建产品推荐。'
- en: '**Storytelling and creative writing**: Some NLG systems can generate fictional
    stories, poetry, or creative pieces of writing. While these might not be as advanced
    as human-generated content, they showcase the potential of NLG in creative domains.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故事讲述和创意写作**：一些自然语言生成（NLG）系统可以生成虚构故事、诗歌或创意写作作品。虽然这些可能不如人类生成的内容先进，但它们展示了NLG在创意领域的潜力。'
- en: '**Automatic code generation**: In programming, NLG can be used to generate
    code snippets or documentation based on high-level descriptions or specifications.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动代码生成**：在编程中，NLG可以用于根据高级描述或规范生成代码片段或文档。'
- en: '**Medical reports**: NLG can assist in generating medical reports or patient
    summaries based on electronic health records and medical data.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗报告**：NLG可以根据电子健康记录和医疗数据协助生成医疗报告或患者摘要。'
- en: NLG techniques can range from rule-based approaches to more advanced ML methods,
    such as **recurrent neural networks** (**RNNs**) and transformer models such as
    **generative pre-trained transformer** (**GPT**). These models learn patterns
    and structures from vast amounts of text data to produce coherent and contextually
    relevant language.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: NLG技术可以从基于规则的途径到更高级的机器学习（ML）方法，如**循环神经网络**（**RNNs**）和**生成预训练转换器**（**GPT**）等转换器模型。这些模型从大量文本数据中学习模式和结构，以产生连贯且上下文相关的语言。
- en: NLG is integral in applications where conveying information in a human-readable
    format is essential, making technology more accessible and user-friendly. While
    NLG systems have made significant progress, challenges still exist, including
    maintaining coherence over longer texts, ensuring accuracy, and avoiding biases
    in generated content.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要以人类可读格式传达信息的应用中，NLG是必不可少的，这使得技术更加易于访问和用户友好。尽管NLG系统已经取得了重大进展，但仍然存在挑战，包括在较长的文本中保持连贯性、确保准确性和避免生成内容中的偏见。
- en: Analyzing NLP tasks
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析NLP任务
- en: 'NLP encompasses a series of tasks within text analysis, resulting in a layered
    structure. The foundational strata upon which sentence analysis relies are outlined
    as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）包含一系列文本分析任务，形成一个分层结构。以下概述了句子分析所依赖的基础层：
- en: '**Morphology analysis**: The objective of this stage is to break down input
    language strings into sets of tokens, which correspond to discrete words, sub-words,
    and punctuation elements. Through the process of tokenization, the text is fragmented,
    resulting in a sequence of tokens, with each token representing a word from the
    text. Within this stage, two concepts hold significance: the stem and the lemma.
    The stem is the foundational form of a word and is obtained by removing inflections
    (such as verb conjugations or noun plurals) from its altered version. Conversely,
    the lemma represents the standardized form of the word, chosen conventionally
    to encompass all its inflected variations. This phase involves identifying both
    the stem and lemma for each word, which can be accomplished through two distinct
    operations: stemming and lemmatization, respectively. This information becomes
    instrumental in subsequent analysis stages. The rationale behind this approach
    lies in the efficient utilization of memory – maintaining rules based on word
    components and their combinations that form specific inflected forms is much more
    resource-effective than managing each word as an individual element within an
    extensive inventory.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形态分析**：这一阶段的目标是将输入的语言字符串分解成一系列标记，这些标记对应于离散的单词、子单词和标点符号元素。通过标记化过程，文本被分割，形成一系列标记，每个标记代表文本中的一个单词。在这一阶段，有两个概念具有重要意义：词干和词元。词干是单词的基础形式，通过从其变化形式中去除屈折（如动词变形或名词复数）来获得。相反，词元代表单词的标准形式，通常选择以包含其所有屈折变化。这一阶段涉及识别每个单词的词干和词元，这可以通过两种不同的操作来完成：分别进行词干提取和词元还原。这些信息在后续分析阶段变得非常有用。这种方法的合理性在于对内存的高效利用——基于词成分及其组合形成的特定屈折形式的规则比管理大量库存中的每个单词作为独立元素要资源有效得多。'
- en: '**Syntax analysis**: To grasp the meaning of a sentence, it’s insufficient
    to merely comprehend the definitions of its constituent words; it’s equally crucial
    to discern the relationships between these words. This stage addresses the syntactic
    examination of the provided text. All components of speech are recognized and
    encompass verbs, nouns, adjectives, adverbs, prepositions, and pronouns. The process
    that annotates each word with its respective part of speech is termed **part-of-speech
    tagging**. This process is divided into two sub-processes: firstly, shallow parsing
    generates a binary tree in which elementary segments – namely, the **nominal part**
    (**NP**) and the **verbal part** (**VP**) – are identified. Secondly, full parsing
    produces a syntactic tree that designates the syntactic role of each word within
    the sentence.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语法分析**：要理解一个句子的含义，仅仅理解其构成单词的定义是不够的；同样重要的是要辨别这些单词之间的关系。这一阶段涉及对提供的文本进行句法检查。所有语言成分都被识别，包括动词、名词、形容词、副词、介词和代词。将每个单词标注为其相应词性的过程被称为**词性标注**。这个过程分为两个子过程：首先，浅层解析生成一个二叉树，其中识别出基本段——即**名词部分**（**NP**）和**动词部分**（**VP**）。其次，完全解析生成一个句法树，指定句子中每个单词的句法角色。'
- en: '**Semantic analysis**: The semantic analysis phase leverages insights garnered
    from preceding analytical stages, encompassing the meanings of individual words
    and their interconnected relationships, to interpret the overall significance
    of the sentence. The process of named entity recognition is employed to detect
    and classify groups of words that collectively form an entity, which might include
    personal names, countries, events, and the like. Semantic processing endeavors
    to deduce the potential interpretation of a sentence, with a particular focus
    on the interplay between the meanings of the words within the sentence being examined.
    This level of processing may encompass semantic disambiguation for words that
    possess multiple meanings, akin to how disambiguation is performed for words that
    can adopt multiple syntactic roles. Diverse methodologies can be employed for
    disambiguation, some of which involve assessing the frequency of each meaning
    within a specific corpus, while others consider contextual cues. Additionally,
    certain methods tap into pragmatic knowledge within the document’s domain to aid
    in disambiguation.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分析**：语义分析阶段利用先前分析阶段获得的见解，包括单个词语的意义及其相互关系，以解释句子的整体意义。该过程使用命名实体识别来检测和分类由多个词语组成的实体组，这些实体可能包括人名、国家、事件等。语义处理旨在推断句子的潜在解释，特别关注句子中被检查的词语之间的意义互动。这种处理水平可能包括对具有多种意义的词语进行语义消歧，类似于对可以采用多种句法角色的词语进行的消歧。可以采用多种方法进行消歧，其中一些方法涉及评估特定语料库中每种意义的频率，而另一些方法则考虑上下文线索。此外，某些方法利用文档领域的语用知识来帮助消歧。'
- en: '**Pragmatic analysis**: This phase revolves around identifying the contextual
    environment in which the text is situated and the subsequent utilization of that
    context. Specifically, pragmatics delves into how the context influences the interpretation
    of meanings. In this context, *context* refers to the surrounding circumstances,
    including a range of non-linguistic factors (such as social dynamics, environmental
    cues, and psychological factors) that impact linguistic expressions. Indeed, human
    language is not solely rooted in its morphological, syntactic, and semantic attributes;
    it is also deeply intertwined with external knowledge linked to the situation
    in which a sentence is embedded. Within pragmatic analysis, a distinction is often
    drawn between the literal meaning of an utterance and the intended meaning of
    the speaker. The literal meaning pertains to the direct interpretation of the
    expression, while the speaker’s intention refers to the underlying concept the
    speaker aims to convey. For accurate interpretation of communication, several
    factors may be essential. These include the following:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语用分析**：这一阶段围绕识别文本所处的语境及其后续利用。具体来说，语用学深入探讨语境如何影响意义的解释。在这个语境中，“语境”指的是周围的环境，包括一系列非语言因素（如社会动态、环境线索和心理因素），这些因素影响语言表达。确实，人类语言不仅仅根植于其形态学、句法和语义属性；它还与句子嵌入的情境相关的外部知识紧密相连。在语用分析中，通常区分话语的字面意义和说话者的意图意义。字面意义是指对表达的直接解释，而说话者的意图是指说话者旨在传达的潜在概念。为了准确解释交流，可能需要几个因素。以下包括以下内容：'
- en: Understanding the roles and statuses of the conversational participants
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解对话参与者的角色和地位
- en: Recognizing the spatiotemporal context of the situation
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别情境的时空背景
- en: Possessing knowledge about the subject matter being discussed
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解所讨论主题的知识
- en: The ability to comprehend implied meanings from another speaker is termed **pragmatic
    competence**. Despite its significance, this type of analysis is still not extensively
    detailed in the literature, primarily due to the considerable challenges it poses.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从另一个人说话中理解隐含意义的本领被称为**语用能力**。尽管这种分析很重要，但在文献中仍未得到充分详细地阐述，这主要是因为它所提出的巨大挑战。
- en: Introducing automatic processing
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入自动处理
- en: 'From an automated processing perspective, the syntactic level presents fewer
    challenges. Complex sentences can be analyzed, their grammatical accuracy identified,
    and their syntactic structures reconstructed seamlessly. The semantic level, on
    the other hand, proves significantly more intricate. In simpler scenarios, the
    following approach is feasible:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从自动处理的角度来看，句法层面呈现的挑战较少。复杂的句子可以被分析，其语法准确性可以被识别，其句法结构可以无缝重建。然而，语义层面则证明要复杂得多。在简单的情况下，以下方法可行：
- en: Sentence meaning is deduced from the meanings of individual words once the sentence’s
    syntactic structure has been discerned
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦句子的句法结构被识别，句子意义就可以从单个词的意义中推断出来。
- en: The meanings of individual words are derived from a readily accessible dictionary
    through automated means
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个词的意义可以通过自动化手段从易于获取的词典中得出。
- en: 'However, numerous issues arise, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出现了许多问题，如下所述：
- en: Firstly, the same word can bear distinct meanings in diverse contexts.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，同一个词在不同的语境中可以有不同的含义。
- en: Secondly, the syntactic structure of a sentence can be ambiguous, occasionally
    attributing different structures to the same sequence of words.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，句子的句法结构可能是模糊的，有时会赋予同一序列的词不同的结构。
- en: Lastly, certain idiomatic phrases possess meanings separate from the literal
    interpretations derived from composing the meanings of their constituent words.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，某些习语短语具有与从其构成词的意义组合中得出的字面解释不同的含义。
- en: Artificial intelligence endeavors to tackle these challenges by leveraging a
    suitable knowledge base that the language processing program can access.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能通过利用语言处理程序可以访问的适当知识库来应对这些挑战。
- en: The pragmatic level introduces an even more intricate terrain. The primary challenge
    stems from our capacity, during a conversation, to anticipate the mental states
    of our conversational partners. In essence, effective communication necessitates
    the representation of interlocutors’ intentions, which are only partially evident
    in their spoken words.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 话语层面引入了一个更加复杂的领域。主要的挑战来自于我们在对话中预测对话者心理状态的能力。本质上，有效的沟通需要表达对话者的意图，而这些意图只在他们的话语中部分显现。
- en: Now that we’ve introduced the basic concepts of NLP, let’s analyze the specific
    contents that allow us to approach the problem from a practical point of view.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了NLP的基本概念，让我们分析一下那些能让我们从实际角度接近问题的具体内容。
- en: Exploring corpora and word and sentence tokenizers
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索语料库和单词及句子分词器
- en: The analysis of corpora, words, and sentence tokenization forms the basis for
    comprehensive language understanding. Corpora provides real-world language data
    for analysis, words constitute the elements of expression, and sentence tokenization
    structures the text into meaningful units for further investigation. This trio
    of concepts plays a central role in advancing linguistic research and enhancing
    NLP capabilities.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库、单词和句子分词的分析构成了全面语言理解的基础。语料库提供了用于分析的现实世界语言数据，单词构成了表达元素，句子分词将文本结构化为有意义的单元以供进一步研究。这三个概念在推进语言研究和增强NLP能力方面发挥着核心作用。
- en: Corpora
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语料库
- en: In linguistics and NLP, corpora refer to extensive collections of written or
    spoken texts that serve as valuable sources of data for linguistic analysis and
    language-related studies. Corpora provides a diverse range of language samples,
    enabling researchers to examine patterns, trends, and variations in language usage,
    syntax, and semantics across different contexts and genres.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言学和NLP中，语料库指的是大量书面或口语文本的集合，这些文本是语言学分析和语言相关研究的有价值的数据来源。语料库提供了多样化的语言样本，使研究人员能够检查不同语境和体裁中语言使用、句法和语义的规律、趋势和变化。
- en: 'Linguistic corpora represent sizable collections of spoken or written texts,
    often originating from authentic communication contexts (including speeches or
    newspaper articles). These collections are digitized and frequently accompanied
    by computerized tools for convenient access:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学语料库代表大量口语或书面文本的集合，通常源自真实的交流语境（包括演讲或报纸文章）。这些集合被数字化，并经常伴随着计算机化工具以便于访问：
- en: '![Figure 7.3 – Word cloud of corpora basic key concepts. The words are displayed
    in different sizes, with the most frequent words being the largest. Word clouds
    are often used to identify the key concepts in a corpus](img/B21156_07_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 语料库基本关键概念的词云。单词以不同的大小显示，最频繁的单词最大。词云通常用于识别语料库中的关键概念](img/B21156_07_03.jpg)'
- en: Figure 7.3 – Word cloud of corpora basic key concepts. The words are displayed
    in different sizes, with the most frequent words being the largest. Word clouds
    are often used to identify the key concepts in a corpus
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 语料库基本关键概念的词云。单词以不同的大小显示，最频繁的单词最大。词云通常用于识别语料库中的关键概念
- en: Corpora serve the purpose of examining how language is practically utilized
    and validating overarching trends through statistical analysis. They hold a pivotal
    role in contemporary lexicography, aiding in tasks such as selecting lemmas based
    on their usage frequency, identifying typical linguistic structures involving
    specific words, and grasping subtle nuances of meaning within various contexts.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库的作用是检查语言的实际应用情况，并通过统计分析验证总体趋势。它们在当代词典学中扮演着关键角色，有助于诸如根据使用频率选择词元、识别涉及特定单词的典型语言结构以及理解不同语境中意义的细微差别等任务。
- en: Furthermore, corpora wield significance in the advancement of language technologies
    such as automatic translation and speech recognition. In these applications, corpora
    are employed to construct statistical language models. They also find utility
    in language education, supporting the creation of instructional resources. Particularly
    for advanced learners, corpora enable the deduction of word properties and linguistic
    structures by observing their contextual applications.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，语料库在语言技术（如自动翻译和语音识别）的发展中发挥着重要作用。在这些应用中，语料库被用来构建统计语言模型。它们在语言教育中也很有用，支持教学资源的创建。特别是对于高级学习者来说，通过观察其语境应用，语料库能够帮助他们推导出词的性质和语言结构。
- en: 'There are various examples of corpora; the most used are as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库的例子有很多；以下是最常用的几个：
- en: '**The Brown Corpus**: One of the first and most well-known corpora in English,
    it contains over a million words of text from various sources'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《布朗语料库》**：英语中最早和最著名的语料库之一，它包含来自各种来源的超过一百万个单词的文本'
- en: '**The New York Times Corpus**: A large corpus of news articles from the New
    York Times, spanning from 1987 to 2007'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《纽约时报语料库》**：一个包含从1987年到2007年纽约时报新闻文章的大型语料库'
- en: '**The Project Gutenberg Corpus**: A collection of over 60,000 free electronic
    books in the public domain'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《古腾堡语料库》**：一个包含60,000多本公共领域免费电子书的集合'
- en: '**The Linguistic Data Consortium** (**LDC**) **Corpora**: A repository of a
    wide variety of corpora, including those for different languages and domains'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《语言数据联盟》（**LDC**）语料库**：一个包含各种语料库的仓库，包括不同语言和领域的语料库'
- en: 'MATLAB provides a variety of tools for accessing and processing corpora. Some
    of the most commonly used tools are as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: MATLAB提供了各种工具来访问和处理语料库。以下是一些最常用的工具：
- en: '**The Text Analysis Toolbox**: A comprehensive toolbox for text processing,
    including functions for tokenization, stemming, lemmatization, and sentiment analysis'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《文本分析工具箱》**：一个全面的文本处理工具箱，包括分词、词干提取、词形还原和情感分析等功能'
- en: '**The Natural Language Processing Toolbox**: A toolbox for NLP tasks, such
    as named entity recognition, part-of-speech tagging, and dependency parsing'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《自然语言处理工具箱》**：一个用于自然语言处理任务的工具箱，如命名实体识别、词性标注和依存句法分析'
- en: '**The Web Services Toolbox**: A toolbox for accessing and interacting with
    web services, including those that provide access to corpora'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**《网络服务工具箱》**：一个用于访问和交互网络服务的工具箱，包括那些提供访问语料库的服务'
- en: Words
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词
- en: Words are the foundational units of language that carry meaning and convey ideas.
    The analysis of words involves investigating their forms, meanings, relationships,
    and usage patterns. This examination can encompass aspects such as morphological
    structure, part-of-speech categorization, frequency distribution, and semantic
    associations. Studying words allows linguists to understand how language is structured
    and how meanings are conveyed through vocabulary.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 词是语言的基础单位，承载着意义并传达思想。对词的分析包括研究它们的形态、意义、关系和用法模式。这种研究可以包括诸如形态结构、词性分类、频率分布和语义关联等方面。研究词可以帮助语言学家理解语言的结构以及如何通过词汇传达意义。
- en: In ML and DL tasks, words are represented as vectors of numbers. This is done
    because ML algorithms can only work with numerical data. The process of converting
    words into vectors is called word embedding.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）和深度学习（DL）任务中，单词被表示为数字向量。这样做是因为机器学习算法只能处理数值数据。将单词转换为向量的过程称为词嵌入。
- en: 'There are two main types of word embedding:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种主要的词嵌入类型：
- en: '**Bag of words** (**BoW**): In **BoW**, each unique word in a corpus is assigned
    a unique index. A document is then represented as a vector of the counts of each
    word in the document. Let’s assume vocab size = 4; here, a sample document that
    states “I love cats” is represented as [ 1 1 1 0]. In BoW, we simply represent
    the document by the frequency of each word. For example, if we have a vocabulary
    of 1,000 words, then the whole document is represented by a 1,000-dimensional
    vector, where the *ith* entry of the vector represents the frequency of the *ith*
    vocabulary word in the document.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词袋模型**（**BoW**）：在**BoW**中，语料库中的每个唯一单词被分配一个唯一的索引。然后，文档被表示为文档中每个单词计数的向量。假设词汇量大小为4；在这里，一个表示“我喜欢猫”的样本文档被表示为[1
    1 1 0]。在BoW中，我们简单地通过每个单词的频率来表示文档。例如，如果我们有一个包含1,000个单词的词汇表，那么整个文档由一个1,000维向量表示，其中向量的第*i*个条目表示文档中第*i*个词汇词的频率。'
- en: '**Word2vec**: Word2vec is a more sophisticated approach to word embedding that
    takes into account the context in which words appear. Word2vec learns two types
    of embeddings: word embeddings and context embeddings. Word embeddings are vectors
    that represent individual words, while context embeddings are vectors that represent
    the words that appear around a given word. For example, the word embedding for
    “cat” might be [0.1, 0.2, 0.3], while the context embedding for “cat” might be
    [0.4, 0.5, 0.6].'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2vec**：Word2vec是一种更复杂的词嵌入方法，它考虑了单词出现的环境。Word2vec学习两种类型的嵌入：词嵌入和上下文嵌入。词嵌入是表示单个单词的向量，而上下文嵌入是表示给定单词周围单词的向量。例如，“猫”的词嵌入可能是[0.1,
    0.2, 0.3]，而“猫”的上下文嵌入可能是[0.4, 0.5, 0.6]。'
- en: Sentence tokenize
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子分词
- en: 'During the **sentence tokenization** process, we identify atomic elements known
    as tokens within each sentence. These tokens serve as the basis for analyzing
    and evaluating the sentence itself. Consequently, during the tokenization phase,
    we not only recognize and assess these elements but also occasionally convert
    negative constructs. This results in the text being divided into tokens, which
    can be thought of as *indivisible* units. This task is relatively straightforward
    for languages that use spaces to separate words but becomes considerably more
    intricate for languages with continuous spelling systems. Focusing on languages
    such as English, which fall into the former category, a token can be defined as
    any sequence of characters enclosed by spaces. However, it’s worth noting that
    this definition allows for numerous exceptions. In languages where word boundaries
    aren’t explicitly indicated in written text, this process is referred to as word
    segmentation:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在**句子分词**过程中，我们在每个句子中识别称为标记的原子元素。这些标记是分析评估句子本身的基础。因此，在分词阶段，我们不仅识别评估这些元素，而且偶尔还会转换否定结构。这导致文本被分割成标记，这些标记可以被视为*不可分割*的单位。对于使用空格分隔单词的语言来说，这项任务相对简单，但对于具有连续拼写系统的语言来说，它变得相当复杂。专注于像英语这样的语言，它属于前者，一个标记可以定义为任何由空格包围的字符序列。然而，值得注意的是，这个定义允许有多个例外。在单词边界在书面文本中未明确表示的语言中，这个过程被称为词分割：
- en: '![Figure 7.4 – Word tokenization process](img/B21156_07_04.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 单词分词过程](img/B21156_07_04.jpg)'
- en: Figure 7.4 – Word tokenization process
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 单词分词过程
- en: 'In the broader context of tokenization, several challenges need to be addressed:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词的更广泛背景下，需要解决几个挑战：
- en: Lack of spaces separating words from preceding or succeeding punctuation
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏分隔单词与前后标点符号的空间
- en: Instances where sequences of characters are not separated by spaces should be
    treated as two separate tokens
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符序列未用空格分隔的实例应被视为两个单独的标记
- en: Occasions when sequences of characters are separated by spaces should be considered
    a single token
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符序列用空格分隔的场合应被视为一个单独的标记
- en: Managing uppercase and lowercase variations
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理大小写变化
- en: Navigating variations in spelling conventions
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航拼写习惯的变化
- en: Ultimately, at the end of this procedure, we identify a string with an assigned
    meaning, which we refer to as a **token**. Each token is structured as a pair
    comprising a token name and an optional token value.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在这个过程的最后，我们识别出一个具有指定意义的字符串，我们称之为**标记**。每个标记都由一个标记名称和一个可选的标记值组成的对构成。
- en: Now that we have examined the basics of NLP in detail, we can proceed to analyze
    the implementation of a sentence classifier in MATLAB.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细研究了NLP的基础知识，我们可以继续分析在MATLAB中实现句子分类器的实现。
- en: Implementing a MATLAB model to label sentences
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现MATLAB模型以标记句子
- en: In this section, we will discuss a very interesting topic that is very popular
    in today’s society. I am referring to the importance of reviews in influencing
    a customer’s interest in making the right decision.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个在当今社会非常受欢迎的非常有趣的话题。我指的是评论在影响顾客做出正确决策的兴趣方面的重要性。
- en: Introducing sentiment analysis
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍情感分析
- en: Sentiment analysis, a technique that utilizes NLP, extracts and analyzes subjective
    information from text. Analyzing vast datasets reveals collective opinions that
    impact various domains. While manual sentiment analysis is challenging, automated
    methods have emerged. However, automating language modeling is complex and costly
    due to the nuances of human language. Additionally, the methodology varies across
    languages, increasing complexity.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析，一种利用NLP的技术，从文本中提取和分析主观信息。分析大量数据集揭示了影响各个领域的集体意见。虽然手动情感分析具有挑战性，但自动方法已经出现。然而，由于人类语言的细微差别，自动化语言建模既复杂又昂贵。此外，方法论在不同语言之间有所不同，增加了复杂性。
- en: 'A major challenge lies in determining the polarity of opinions. Polarity classification
    is subjective, with one sentence perceived differently by individuals based on
    their value systems. The rise of social media has heightened interest in sentiment
    analysis. As online expressions proliferate, this field has become valuable for
    businesses to promote products, identify opportunities, and protect their reputation.
    Challenges arise from the use of simplistic terms in sentiment analysis algorithms
    to convey opinions. Cultural influences, linguistic disparities, and contextual
    factors make converting written text into a binary positive or negative sentiment
    highly intricate:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主要挑战在于确定意见的极性。极性分类是主观的，因为不同的人根据他们的价值观对同一句话有不同的理解。社交媒体的兴起提高了对情感分析的兴趣。随着在线表达的增多，这个领域对商业来说已经成为推广产品、识别机会和保护声誉的有价值工具。情感分析算法中使用简单术语来传达意见引发了挑战。文化影响、语言差异和上下文因素使得将书面文本转换为二元积极或消极情感变得非常复杂：
- en: '![Figure 7.5 – Sentiment analysis polarity](img/B21156_07_05.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 情感分析极性](img/B21156_07_05.jpg)'
- en: Figure 7.5 – Sentiment analysis polarity
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 情感分析极性
- en: Advanced sentiment analysis endeavors to pinpoint specific moods, such as happiness,
    sadness, or anger. For instance, it can involve tasks such as categorizing a song
    review not just as positive or negative, but also predicting a numerical score,
    conducting a comprehensive analysis of hotel reviews, and providing ratings for
    various aspects such as comfort, noise levels, and design.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 高级情感分析旨在确定特定的情绪，如快乐、悲伤或愤怒。例如，它可以涉及将歌曲评论不仅分类为正面或负面，还可以预测数值评分，对酒店评论进行全面分析，并为舒适度、噪音水平和设计等方面提供评分。
- en: 'Furthermore, it’s feasible to categorize a given text into one of two classes:
    objective or subjective. A text may contain objective information, as seen in
    a news article, or it can be subjective, such as the expression of political opinions
    in an interview. The subjectivity of sentences often hinges on their context,
    and even an objective document may feature subjective phrases, such as quotations.
    Discerning subjectivity versus objectivity can be more challenging than classifying
    polarity, as it relies heavily on the surrounding textual context.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将给定的文本分类为两类之一：客观或主观是可行的。文本可能包含客观信息，如新闻文章中所示，或者它可以是主观的，例如采访中表达的政治观点。句子的主观性通常取决于其上下文，甚至一份客观文件也可能包含主观短语，如引语。区分主观性和客观性可能比分类极性更具挑战性，因为它在很大程度上依赖于周围的文本上下文。
- en: 'Sentiment analysis offers several approaches, with the most commonly employed
    methods falling into four broad categories:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析提供了几种方法，其中最常用的方法可以分为四个广泛的类别：
- en: '**Lexicon-based methods**: These techniques identify emotional keywords within
    the text and assign them a predetermined affinity to represent specific emotions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于词典的算法**：这些技术识别文本中的情感关键词，并赋予它们预定的亲和力以代表特定的情感。'
- en: '**Rule-based methods**: This approach classifies text based on explicit emotional
    categories, relying on the presence of unambiguous emotional words such as *happy*,
    *sad*, or *bored*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于规则的算法**：这种方法根据明确的情感类别对文本进行分类，依赖于诸如*快乐*、*悲伤*或*无聊*等明确的情感词的存在。'
- en: '**Statistical methods**: In this category, the aim is to ascertain the source
    of the sentiment (the subject) and the target (the object being evaluated). To
    gauge opinions in context and identify the assessed attribute, grammatical relations
    among the words are examined through an extensive analysis of the text.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计方法**：在这个类别中，目标是确定情感来源（主题）和目标（被评估的对象）。为了在上下文中衡量意见并识别被评估的属性，通过广泛分析文本中的词语之间的语法关系。'
- en: '**ML methods**: These methods employ various learning algorithms to deduce
    sentiment, often utilizing a dataset with predefined classifications (supervised
    methods). The learning process is iterative, requiring the construction of models
    that associate polarity with different types of comments and, when necessary,
    topic analysis for a more comprehensive understanding.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习算法**：这些方法使用各种学习算法来推断情感，通常利用具有预定义分类的数据集（监督方法）。学习过程是迭代的，需要构建将极性与不同类型的评论关联起来的模型，并在必要时进行主题分析以获得更全面的理解。'
- en: The utilization of emojis and emoticons is on the rise. Instead of relying solely
    on words, people often express their emotions using tiny round faces and other
    symbols. These visual elements prove valuable for swiftly and pleasantly conveying
    one’s message. Using DL architectures, it is possible to learn the relationship
    between emoticons and sentiments. For example, a DL method might be trained on
    a large corpus of text and emoticons to learn the features of emoticons that are
    most predictive of sentiment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表情符号和表情符号的使用正在增加。人们不再仅仅依赖文字，而是经常使用小小的圆形面孔和其他符号来表达他们的情感。这些视觉元素被证明是迅速而愉快地传达信息的宝贵工具。使用DL架构，可以学习表情符号和情感之间的关系。例如，一种DL方法可能在一个包含大量文本和表情符号的语料库上训练，以学习表情符号的特征，这些特征最能够预测情感。
- en: Let’s learn how to apply the tools made available by DL to tackle a practical
    case of sentiment analysis.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何应用DL提供的工具来解决情感分析的实际案例。
- en: Movie review sentiment analysis
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电影评论情感分析
- en: Reviews have become an essential element for businesses to thrive, providing
    valuable feedback from customers and driving purchasing decisions. They serve
    as a reliable source of information for prospective buyers and offer sellers insights
    to enhance their products and services. Moreover, they foster user engagement
    and amplify the power of word-of-mouth marketing, significantly impacting a company’s
    online reputation. Securing positive reviews is crucial for businesses of all
    sizes to maintain profitability.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 评论已成为企业繁荣发展的关键要素，它们提供了来自客户的宝贵反馈，并推动购买决策。评论作为潜在买家可靠的信息来源，为卖家提供改进产品和服务的洞察。此外，评论促进了用户参与度，放大了口碑营销的力量，对公司的在线声誉产生重大影响。对于所有规模的企业来说，确保获得正面评论对于维持盈利至关重要。
- en: The rise of social media and the internet has dramatically transformed consumer
    buying behaviors. Potential buyers now actively seek online reviews and information
    before making purchasing decisions, relying less on traditional word-of-mouth
    recommendations. They have become more independent and discerning, carefully considering
    their choices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体和互联网的兴起极大地改变了消费者的购买行为。潜在买家现在在做出购买决策之前会积极寻找在线评论和信息，对传统口碑推荐的依赖减少。他们变得更加独立和挑剔，仔细考虑他们的选择。
- en: In the past, film critics played a dominant role in shaping consumer preferences,
    influencing movie choices and fostering critical thinking. However, the advent
    of consumer-generated reviews has shifted the paradigm, with consumer opinions
    now holding greater weight in influencing purchasing decisions. This democratization
    of opinion has empowered consumers to share their experiences and shape the cultural
    landscape.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，影评人在塑造消费者偏好方面扮演着主导角色，影响电影选择并培养批判性思维。然而，随着消费者生成评论的出现，这一范式发生了转变，消费者意见现在在影响购买决策方面具有更大的影响力。这种意见民主化赋予了消费者分享他们的经验和塑造文化景观的权力。
- en: 'For the task of classifying movie reviews as either positive or negative sentiment,
    we will employ a short version of the IMDb Movie Reviews dataset. This dataset
    is specifically designed for binary sentiment classification and contains 748
    reviews opportunely labeled. The dataset was adequately revisited in this example
    using only two features: review and class.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将电影评论分类为正面或负面情感的任务，我们将使用IMDb电影评论数据集的简短版本。该数据集专门设计用于二元情感分类，并包含748条恰当地标记的评论。在这个例子中，数据集仅使用两个特征：评论和类别进行了充分回顾。
- en: Let’s learn how to build a model for classifying sentences using an RNN.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何使用RNN构建用于分类句子的模型。
- en: Using an LSTM model for label sentences
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LSTM模型进行句子标记
- en: An LSTM model is an advanced form of **recurrent neural network** (**RNN**)
    that can learn long-term dependencies within sequences, making it a powerful tool
    for processing sequential data, such as text, speech, and time series data. This
    makes it ideal for tasks such as label sentence classification, where the goal
    is to assign a label to a sentence based on its overall sentiment. LSTM has been
    successfully applied to a variety of tasks, including sentiment analysis, machine
    translation, speech recognition, and natural language generation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型是**循环神经网络**（RNN）的一种高级形式，能够在序列中学习长期依赖关系，使其成为处理序列数据（如文本、语音和时间序列数据）的强大工具。这使得它在诸如标签句子分类等任务中非常理想，这些任务的目的是根据句子的整体情感为其分配标签。LSTM已经在包括情感分析、机器翻译、语音识别和自然语言生成在内的各种任务中得到了成功应用。
- en: To learn how to implement a model to label sentences in MATLAB, we will use
    movie reviews that have been made by a lot of online user:.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习如何在MATLAB中实现用于标记句子的模型，我们将使用大量在线用户制作的电影评论：。
- en: 'To begin, we need to import the dataset into the MATLAB workspace. You can
    find the dataset in `.xlsx` format on the project’s GitHub page (`ImdbDataset.xlsx`).
    To simplify this task, the dataset has been appropriately trimmed. Once you’ve
    downloaded the file, all you need to do is specify the root folder’s path. Then,
    you can load the data into the MATLAB workspace like so:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据集导入MATLAB工作空间。您可以在项目的GitHub页面（`ImdbDataset.xlsx`）上找到`.xlsx`格式的数据集。为了简化此任务，数据集已被适当裁剪。一旦您下载了文件，您只需指定根文件夹的路径。然后，您可以像这样将数据加载到MATLAB工作空间中：
- en: '[PRE0]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After loading the data, we need to visualize the distribution of the classes
    in the dataset by creating a histogram. This will help us assess whether the data
    is evenly distributed:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在加载数据后，我们需要通过创建直方图来可视化数据集中类别的分布。这将帮助我们评估数据是否均匀分布：
- en: '[PRE1]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The diagram shown in *Figure 7**.6* will be drawn:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将绘制如图*图7**.6*所示的图表：
- en: '![Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive](img/B21156_07_06.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 电影评论的类别分布：0表示负面，1表示正面](img/B21156_07_06.jpg)'
- en: 'Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 电影评论的类别分布：0表示负面，1表示正面
- en: 'The subsequent step involves dividing the data into training and validation
    sets. Partition the data into a training set and a separate held-out set for validation
    and testing purposes:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的步骤是将数据分为训练集和验证集。将数据分为训练集和用于验证和测试的单独保留集：
- en: '[PRE2]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `cvpartition()` function establishes a random dataset partition. You can
    employ this partition to create training and testing sets for assessing a statistical
    model through cross-validation. Set the holdout percentage to 30%; this means
    that 70% of the data will be allocated for training, while the remaining 30% will
    be reserved for validation.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cvpartition()`函数建立了一个随机数据集分区。您可以使用此分区创建用于通过交叉验证评估统计模型的训练集和测试集。将保留百分比设置为30%；这意味着70%的数据将用于训练，而剩余的30%将保留用于验证。'
- en: 'Now, we will utilize the training option to obtain the training indices and
    the test option to obtain the testing indices for cross-validation:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们将使用训练选项来获取交叉验证的训练索引，以及测试选项来获取测试索引：
- en: '[PRE3]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Following this, we will retrieve the review data and corresponding classes
    from the partitioned tables:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们将从分割的表中检索评论数据和相应的类别：
- en: '[PRE4]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, our data has been prepared for use, and we can start preprocessing
    the review data. We will employ a function designed for tokenization and text
    preprocessing called `preprocessText`. This function carries out the following
    tasks:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们的数据已经准备好使用，我们可以开始预处理评论数据。我们将使用一个名为`preprocessText`的函数，该函数用于标记化和文本预处理。此函数执行以下任务：
- en: Tokenizes the text using `tokenizedDocument`.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tokenizedDocument` 对文本进行标记化。
- en: Converts the text into lowercase using `lower`.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `lower` 将文本转换为小写。
- en: Removes punctuation using `erasePunctuation`.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `erasePunctuation` 移除标点符号。
- en: 'We will apply the `preprocessText` function to both the training and validation
    data for preprocessing:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将应用 `preprocessText` 函数对训练数据和验证数据进行预处理：
- en: '[PRE5]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After that, we have to apply the word encoding technique. This method involves
    transforming words from a human language, such as English, into a numerical or
    symbolic format that’s suitable for a wide range of computational tasks, with
    a particular focus on applications in NLP and ML.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之后，我们必须应用单词编码技术。这种方法涉及将人类语言（如英语）中的单词转换成适合广泛计算任务的数值或符号格式，特别关注在自然语言处理（NLP）和机器学习（ML）中的应用。
- en: 'To transform the documents into sequences of numeric indices, we will employ
    the `wordEncoding()` function, as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将文档转换为数值索引序列，我们将使用 `wordEncoding()` 函数，如下所示：
- en: '[PRE6]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function associates words within a vocabulary with numeric indices.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此函数将词汇表中的单词与数值索引关联起来。
- en: To use the reviews as input for an LSTM, all inputs must have the same length.
    However, the reviews in the dataset vary in length. One solution is to truncate
    longer reviews and pad shorter ones to ensure they are all the same length. To
    truncate and pad the reviews, start by choosing a target length. Then, truncate
    longer documents and pad shorter ones. To achieve optimal results, the target
    length should be short enough that you can avoid discarding a significant amount
    of data.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要将评论作为LSTM的输入，所有输入必须具有相同的长度。然而，数据集中的评论长度各不相同。一种解决方案是截断较长的评论并填充较短的评论，以确保它们的长度相同。为了截断和填充评论，首先选择一个目标长度。然后，截断较长的文档并填充较短的文档。为了达到最佳效果，目标长度应该足够短，以避免丢弃大量数据。
- en: 'To find an appropriate target length, we can visualize a histogram of the lengths
    of the training documents:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了找到一个合适的目标长度，我们可以可视化训练文档长度的直方图：
- en: '[PRE7]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To start, we counted the length of each review. Then, we drew a histogram of
    the distribution. The following chart was printed:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们计算了每条评论的长度。然后，我们绘制了分布的直方图。以下图表被打印出来：
- en: '![Figure 7.7 – Tokens distribution of the movie reviews](img/B21156_07_07.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 电影评论的标记分布](img/B21156_07_07.jpg)'
- en: Figure 7.7 – Tokens distribution of the movie reviews
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 电影评论的标记分布
- en: 'By analyzing the tokens distribution of the movie reviews, we’ll notice that
    most reviews have several tokens between 0 and 30\. So, we can choose this number
    as the target length:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析电影评论的标记分布，我们会注意到大多数评论在0到30之间有几个标记。因此，我们可以选择这个数字作为目标长度：
- en: '[PRE8]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to convert the reviews into numeric indices sequences:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将评论转换为数值索引序列：
- en: '[PRE9]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To do this, we used the `doc2sequence()` function, which transforms documents
    into sequences that are suitable for DL. This function provides a cell array containing
    the numeric indices of words in the documents, as determined by the word encoding
    provided. Each element within the sequences array is a vector consisting of indices
    representing the words within the respective document.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用了 `doc2sequence()` 函数，该函数将文档转换为适合深度学习的序列。此函数提供了一个包含文档中单词的数值索引的单元格数组，这些索引由提供的单词编码确定。序列数组中的每个元素都是一个向量，由表示相应文档中单词的索引组成。
- en: 'Now, we will proceed with defining the architecture of the LSTM network:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将继续定义LSTM网络的架构：
- en: '[PRE10]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To feed sequence data into the network, incorporate a sequence input layer with
    an input size of 1\. A sequence input with 1 dimension means each numerically
    encoded token is represented by a 1x1 scalar. Following that, introduce a word
    embedding layer with a dimension of 150, which should match the number of words
    specified in the word encoding. Word embedding layers learn to map words to vectors
    in a way that captures the meaning of the words. Subsequently, include an LSTM
    layer with 50 hidden units. For a sequence-to-label classification task using
    the LSTM layer, configure `OutputMode` last. Finally, add a fully connected layer
    that’s the same size as the number of classes, followed by a softmax layer and
    a classification layer.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要将序列数据输入网络，包含一个输入大小为1的序列输入层。一个具有1维的序列输入意味着每个数值编码的标记由一个1x1标量表示。随后，引入一个维度为150的词嵌入层，这应该与单词编码中指定的单词数量相匹配。词嵌入层学会将单词映射到向量中，以捕捉单词的意义。随后，包含一个具有50个隐藏单元的LSTM层。对于使用LSTM层的序列到标签分类任务，最后配置`OutputMode`。最后，添加一个与类别数量相同的全连接层，然后是softmax层和分类层。
- en: 'The architecture of the LSTM that’s been defined is shown here:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已定义的LSTM架构如下所示：
- en: '[PRE11]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we have to specify the training options:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须指定训练选项：
- en: '[PRE12]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following options were set:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置了以下选项：
- en: '`adam solver`: Adam, derived from *Adaptive Moment Estimation*, is a widely
    embraced optimization algorithm in the realms of ML and DL. Its primary role is
    to adjust the parameters, while encompassing weights and biases, within a model
    as it undergoes training. Adam is especially renowned for its effectiveness in
    training neural networks.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adam solver`: Adam，源于*自适应矩估计*，是机器学习和深度学习领域广泛采用的优化算法。其主要作用是在模型训练过程中调整参数，同时包含权重和偏差。Adam因其训练神经网络的有效性而特别闻名。'
- en: '`MiniBatchSize`: This is a hyperparameter that dictates the number of examples,
    which are individual data points or samples, that are employed in each training
    iteration during the execution of Adam or a similar optimization algorithm.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MiniBatchSize`: 这是一个超参数，决定了在执行Adam或类似优化算法的每个训练迭代中使用的示例数量，这些示例是单个数据点或样本。'
- en: '`GradientThreshold`: This specifies a threshold value for gradients during
    the training process. It is used to make training models more stable and efficient,
    particularly when dealing with RNNs or deep architectures.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientThreshold`: 这指定了训练过程中的梯度阈值。它用于使训练模型更加稳定和高效，尤其是在处理RNN或深度架构时。'
- en: '`Shuffle`: This option shuffles the data in each epoch to increase the degree
    of randomness in the algorithm.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Shuffle`: 此选项在每个epoch中对数据进行洗牌，以增加算法中的随机程度。'
- en: '`ValidationData`: This option specifies the data used for the validation process.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidationData`: 此选项指定用于验证过程的数据。'
- en: '`Plots`: With this option, the training process will be checked through a plot
    that upgrades the evolution of the process.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Plots`: 使用此选项，训练过程将通过一个升级过程进化的图表进行检查。'
- en: '`Verbose`: The output of the training process will be hidden.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Verbose`: 训练过程的输出将被隐藏。'
- en: 'Finally, we can start the training process:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以开始训练过程：
- en: '[PRE13]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following plot will be printed:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将打印以下图表：
- en: '![Figure 7.8 – Training process of the LSTM](img/B21156_07_08.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – LSTM的训练过程](img/B21156_07_08.jpg)'
- en: Figure 7.8 – Training process of the LSTM
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – LSTM的训练过程
- en: The training process processed the data by building a network capable of returning
    a validation accuracy of 75.89%. Given the small amount of data available, this
    result can be considered satisfactory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程通过构建一个能够返回75.89%验证准确率的网络来处理数据。鉴于可用数据量较小，这个结果可以被认为是令人满意的。
- en: To improve the performance of the model, we can try the simulation with a slightly
    large number of LSTM units (100 instead of 50). However, changing the embedding
    dimension (from 150 to 100) could increase the validation accuracy to at least
    85%.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高模型的性能，我们可以尝试使用稍大的LSTM单元数量（100而不是50）进行模拟。然而，将嵌入维度（从150变为100）可以提高验证准确率至至少85%。
- en: Now that we’ve analyzed how to build an LSTM to classify movie reviews, let’s
    learn how to improve the accuracy of predictions using ensemble methods.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分析了如何构建用于分类电影评论的LSTM，让我们学习如何使用集成方法提高预测的准确性。
- en: Understanding gradient boosting techniques
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度提升技术
- en: To improve the performance of an algorithm, we can perform a series of steps
    and use different techniques, depending on the type of algorithm and the specific
    problems being addressed. The first approach involves a thorough analysis of the
    data to identify possible inaccuracies or shortcomings. In addition, many algorithms
    have parameters that can be adjusted to achieve better performance – not to mention
    techniques such as feature scaling or feature selection. A popular technique is
    to combine the capabilities offered by different algorithms to achieve better
    overall performance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高算法的性能，我们可以执行一系列步骤并使用不同的技术，具体取决于算法的类型和要解决的问题。第一种方法涉及对数据进行彻底分析，以识别可能的不准确或不足之处。此外，许多算法都有可以调整以实现更好性能的参数——更不用说特征缩放或特征选择等技术。一种流行的技术是结合不同算法提供的功能，以实现更好的整体性能。
- en: Approaching ensemble learning
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接近集成学习方法
- en: The concept of **ensemble learning** involves the use of multiple models combined
    in a way that maximizes performance by exploiting their strengths and mitigating
    their relative weaknesses. These ensemble learning methods are based on weak learning
    models that do not achieve high levels of accuracy on their own, but when combined
    can produce robust predictions. In the context of ensemble learning, a **weak
    learner** is a model that can produce slightly better results than randomly, while
    a **strong learner** approaches an ideal model and can overcome problems typical
    of traditional models, such as overfitting.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的概念涉及使用多个模型以最大化性能的方式组合，通过利用它们的优点并减轻它们的相对弱点。这些集成学习方法基于弱学习模型，这些模型本身不能达到高水平的准确性，但结合在一起可以产生稳健的预测。在集成学习的背景下，**弱学习器**是一个可以产生略好于随机结果的模型，而**强学习器**则接近理想模型，可以克服传统模型典型的问题，如过拟合。
- en: 'Each model that’s used returns predictions that may or may not be correct.
    The prediction error, in a supervised learning problem, is computed as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每个使用的模型都会返回可能正确或错误的预测。在监督学习问题中，预测误差的计算如下：
- en: error = bias 2 + variance
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 误差 = 偏差² + 方差
- en: 'The two fundamental components of error are precision and variance. The goal
    of models that use ensemble approaches is to obtain reliable predictions by trying
    to reduce both **variance** and **bias**. Variance is the variability of a model’s
    prediction for a given data point. A model with high variance is said to be overfitting,
    which means that it is learning the noise in the training data rather than the
    underlying patterns. This can also lead to poor generalization performance. Bias
    is the difference between the average prediction of a model and the correct value.
    A model with high bias is said to be underfitting, which means that it is not
    learning the underlying patterns in the data. This can lead to poor generalization
    performance, where the model does not perform well on unseen data. There is often
    a trade-off between these two components: classifiers with low accuracy tend to
    have high variance and vice versa. The goal of ML is to find a classifier that
    has a good balance of bias and variance. This is known as the bias-variance trade-off.
    In general, it is difficult to find a classifier that has both low bias and low
    variance. However, several techniques can be used to reduce bias and variance,
    such as feature selection, regularization, and cross-validation.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的两个基本组成部分是精确度和方差。使用集成方法的目标是通过尝试减少**方差**和**偏差**来获得可靠的预测。方差是模型对给定数据点的预测的变异性。具有高方差的模型被称为过拟合，这意味着它在学习训练数据中的噪声而不是潜在的模式。这也可能导致较差的泛化性能。偏差是模型的平均预测值与正确值之间的差异。具有高偏差的模型被称为欠拟合，这意味着它没有学习数据中的潜在模式。这可能导致模型在未见过的数据上表现不佳。这两个组成部分之间通常存在权衡：低精度的分类器往往具有高方差，反之亦然。机器学习的目标是找到一个具有良好偏差和方差平衡的分类器。这被称为偏差-方差权衡。一般来说，很难找到一个既具有低偏差又具有低方差的分类器。然而，可以使用一些技术来减少偏差和方差，例如特征选择、正则化和交叉验证。
- en: To improve the accuracy of the model, it is assumed that the different classifiers
    can make different errors on each sample, but agree on the correct classification.
    By averaging the outputs of the classifiers, the overall error is reduced by averaging
    the error components. Combining the outputs by the model does not necessarily
    guarantee significantly better classification performance than other models, but
    it does help to reduce the likelihood of selecting a classifier with disappointing
    performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高模型的准确性，假设不同的分类器可以在每个样本上犯不同的错误，但会在正确的分类上达成一致。通过平均分类器的输出，通过平均误差成分来降低整体误差。通过模型组合输出并不一定保证比其他模型有显著更好的分类性能，但它有助于降低选择表现不佳的分类器的可能性。
- en: 'There are several ensemble methods. They mainly differ in the way they process
    the data. The most common are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种集成方法。它们主要在处理数据的方式上有所不同。最常见的是以下几种：
- en: '**Bagging**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bagging**'
- en: '**Random forest**'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**'
- en: '**Boosting**'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Boosting**'
- en: The first ensemble models were developed in the 1990s, but at the time of writing,
    there is no specific study that can help a user select the most appropriate meta-classifier.
    These methods are widely used to solve a variety of learning problems, including
    feature selection. In the subsequent sections, we will analyze the basics of each
    kind of ensemble technique.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的集成模型是在 1990 年代开发的，但在撰写本文时，没有具体的研究可以帮助用户选择最合适的元分类器。这些方法被广泛用于解决各种学习问题，包括特征选择。在接下来的章节中，我们将分析每种集成技术的基本原理。
- en: Bagging definition and meaning
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging 定义和意义
- en: '**Bagging** is an ensemble learning method that collects the predictions of
    weak learners (basic models), compares them, and combines them into a single prediction.
    The distinguishing feature of bagging is how the training sets for each decision
    tree are selected.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging** 是一种集成学习方法，它收集弱学习器（基本模型）的预测，比较它们，并将它们组合成一个单一预测。bagging 的独特之处在于每个决策树的训练集是如何选择的。'
- en: To create each training set, bagging uses sampling with replacement, known as
    bootstrapping, on the original dataset. **Bootstrapping** is a statistical resampling
    technique that’s used to approximate the sample distribution of an estimator.
    This means that data is randomly selected from the original training set, with
    the possibility of some data being selected multiple times. This procedure increases
    the diversity of the decision trees that are produced.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建每个训练集，bagging 使用了原始数据集上的有放回抽样，这被称为 bootstrapping。**Bootstrapping** 是一种统计重抽样技术，用于近似估计量的样本分布。这意味着数据是从原始训练集中随机选择的，有些数据可能会被多次选择。这种程序增加了产生的决策树的多样性。
- en: A special feature of this method is that it works best when the basic learners
    (weak learners) are not very stable and therefore very sensitive to variations
    in the training data. In the presence of stable basic learners, the overall performance
    may even deteriorate.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个特殊之处在于，当基本学习器（弱学习器）不太稳定且因此对训练数据的变动非常敏感时，它效果最好。在存在稳定的基本学习器的情况下，整体性能甚至可能恶化。
- en: The basic concept behind bagging is to combine multiple models that are unstable
    but have a low bias to produce a low variance overall model. The logic behind
    bagging is based on the idea that the expected value of the mean of identically
    distributed random variables is equal to the expected value of a single random
    variable.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: bagging 的基本概念是结合多个不稳定但偏差低的模型，以产生一个整体方差低的模型。bagging 的逻辑基于这样一个想法：同分布随机变量的期望值等于单个随机变量的期望值。
- en: 'The bagging equation is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: bagging 方程如下：
- en: y _ predicted = 1 / T Σ _ t = 1 ^ T f _ t(x)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: y _ predicted = 1 / T Σ _ t = 1 ^ T f _ t(x)
- en: 'Here, we have the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '*T* is the number of models in the ensemble'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T* 是集成中的模型数量'
- en: '*f_t(x)* is the prediction of the t-th model'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f_t(x)* 是第 t 个模型的预测'
- en: Therefore, the goal is to reduce variance by exploiting this averaging operation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标是利用这种平均操作来降低方差。
- en: Typically, the decision tree is an appropriate choice for this approach because,
    despite its simplicity, it can effectively capture non-linear relationships and
    interactions between variables.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，决策树是这种方法的一个合适选择，因为尽管它很简单，但它可以有效地捕捉变量之间的非线性关系和交互。
- en: The key parameter for adjusting bagging is the number of trees. However, simulation
    studies and empirical evidence have shown that the performance of this model is
    not overly influenced by the choice of this parameter. Consequently, it is sufficient
    to choose a sufficiently large number of trees to obtain satisfactory results.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 调整袋装法的关键参数是树的数量。然而，模拟研究和实证证据表明，该模型的性能并不过度受此参数选择的影响。因此，选择足够多的树以获得满意的结果就足够了。
- en: In summary, bagging is an ensemble learning approach that’s based on repeatedly
    sampling training data and combining weak learner predictions and is particularly
    effective when working with basic models that are sensitive to variations in the
    data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，袋装法是一种基于反复采样训练数据并组合弱学习器预测的集成学习方法，当与对数据变化敏感的基本模型一起使用时尤其有效。
- en: Discovering random forest
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现随机森林
- en: '**Random forest** is a special case of bagging, with the basic aim of reducing
    variance by averaging over many models. Again, models based on decision trees
    are often the ideal choice as they can effectively capture the complex interaction
    structures in the data, which can be difficult to achieve with other approaches.
    By generating a large number of trees, bias tends to be low.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是袋装法的一种特殊情况，其基本目的是通过平均多个模型来减少方差。再次强调，基于决策树的模型通常是理想的选择，因为它们可以有效地捕捉数据中的复杂交互结构，这与其他方法难以实现。通过生成大量树，偏差往往较低。'
- en: Although the conceptual basis of both techniques is similar, random forest can
    produce superior results with the correct adjustment of model parameters. This
    is due to its ensemble learning approach and the ability to tune its hyperparameters
    effectively. This is because, although the trees have errors, the use of bagging
    allows trees to be generated from an identical distribution so that the expected
    value of each tree is equal to the expected value of the mean of the *N* trees
    generated.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两种技术的概念基础相似，但通过正确调整模型参数，随机森林可以产生更优的结果。这得益于其集成学习方法以及有效调整超参数的能力。这是因为，尽管树存在误差，但使用袋装法允许从相同的分布中生成树，使得每棵树的期望值等于生成的
    *N* 棵树的平均值的期望值。
- en: The bias of bagged models is similar to that of individual trees, so improvement
    focuses on reducing variance. This is the distinguishing feature of random forest
    models, which aim to improve the variance of bagging by reducing the correlation
    between trees.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装模型的偏差与单个树相似，因此改进的重点在于降低方差。这是随机森林模型的特点，旨在通过减少树之间的相关性来提高袋装法的方差。
- en: 'For *N* identically distributed random variables, each with variance, *σ2*,
    and with variance of the mean over *N* variables equal to *(1/N)σ2*, if the variables
    are identically distributed but not necessarily independent, and a positive pairwise
    correlation, ρ, is assumed, the variance of the mean is expressed as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *N* 个具有方差 *σ2* 的相同分布的随机变量，当 *N* 个变量的平均方差等于 *(1/N)*σ2* 时，如果变量是相同分布但不必定独立的，并且假设存在正的成对相关系数
    ρ，则平均方差的表示如下：
- en: Variance of the mean = (1 / N) σ 2+ [(N − 1) / N]ρ σ 2
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 平均方差 = (1 / N) σ2 + [(N - 1) / N]ρ σ2
- en: In this formula, the first term represents the variance of the individual random
    variables divided by *N*, which represents the variance of the mean when the variables
    are independent. The second term considers the pairwise correlation between the
    random variables, increasing the variance of the mean according to how correlated
    they are.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，第一项表示单个随机变量的方差除以 *N*，代表当变量独立时的平均方差。第二项考虑了随机变量之间的成对相关性，根据它们的关联程度增加平均方差。
- en: The technique that’s used by random forest is to reduce the correlation between
    the trees, and hence the variance is to grow the trees by randomly selecting the
    input variables. The main difference with bagging is that random forest selects
    fewer candidate variables for partitioning than the total number of variables
    available.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林使用的技术是减少树之间的相关性，因此通过随机选择输入变量来生长树，从而降低方差。与袋装法的主要区别在于，随机森林选择的分区候选变量数量少于可用的变量总数。
- en: 'Random forest is a method that combines a series of decision trees to produce
    more accurate results. These random decision trees are constructed while taking
    into account two key concepts:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是一种结合一系列决策树以产生更精确结果的方法。在构建这些随机决策树时，考虑了两个关键概念：
- en: '**Random sampling of the training data**: During the training phase, each tree
    in a random forest learns by focusing only on a random sample of the original
    data. These samples are extracted with replacement, a technique known as bootstrapping,
    which means that some data can be used multiple times within the same tree. While
    this theoretically produces a high variance in each tree compared to training
    on a specific dataset, the overall forest (the set of trees) tends to have a lower
    variance without a significant increase in bias.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据的随机抽样**：在训练阶段，随机森林中的每一棵树都通过仅关注原始数据的一个随机样本来学习。这些样本是通过替换技术提取的，称为重抽样，这意味着某些数据可以在同一棵树中多次使用。虽然从理论上讲，与在特定数据集上训练相比，这会在每棵树中产生较高的方差，但整体森林（即树集）的方差往往较低，而偏差的增加并不显著。'
- en: '**Random feature sets for partition choice**: In random forest, only a subset
    of the available features is considered for partitioning the nodes in each decision
    tree. Typically, this subset is the square root of the total number of features.
    For example, if there are 16 features, only four random features are considered
    at each node in each decision tree to perform the partitioning. Of course, it
    is also possible to train trees by considering all the features at each node,
    as you would typically do in regression.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机特征集用于分区选择**：在随机森林中，仅考虑可用特征的一个子集来对每个决策树中的节点进行分区。通常，这个子集是特征总数的平方根。例如，如果有16个特征，每个决策树中的每个节点只考虑四个随机特征来进行分区。当然，也可以通过在每个节点考虑所有特征来训练树，就像在回归中通常所做的那样。'
- en: In summary, random forest combines hundreds or thousands of decision trees,
    each trained on a slightly different set of data and using only a limited number
    of features for subdivisions. The final predictions of random forest are obtained
    by averaging the predictions of each decision tree, which means that each tree
    contributes equally to the final decision.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，随机森林结合了数百或数千棵决策树，每棵树都在略微不同的数据集上训练，并且仅使用有限数量的特征进行细分。随机森林的最终预测是通过平均每棵决策树的预测得到的，这意味着每棵树对最终决策的贡献是相等的。
- en: Boosting algorithms explained
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升算法解释
- en: '**Boosting** is another approach that can be applied to decision trees to improve
    their predictive ability. Like bagging, boosting can be applied to both regression
    and classification statistical models.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升法**是另一种可以应用于决策树以增强其预测能力的方法。与袋装法类似，提升法可以应用于回归和分类统计模型。'
- en: Both Bagging and boosting can be applied to decision trees to predict their
    abilities. However, there are some differences. The crucial aspect to focus on
    in bagging is the independence of each tree generated by a subset from the others.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装和提升都可以应用于决策树以预测其能力。然而，它们之间有一些差异。在袋装法中，关键要关注的是由子集生成的每棵树与其他树的独立性。
- en: Boosting, however, takes a different approach. Instead of generating completely
    independent trees as in bagging, boosting trains a sequence of trees iteratively.
    Each successive tree tries to correct the errors of the previous ones. In other
    words, the subsets of data used to train successive trees are weighted according
    to the erroneous predictions of the previous trees. This continuous process aims
    to progressively improve the performance of the overall model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，提升法采取了不同的方法。它不是像袋装法那样生成完全独立的树，而是迭代地训练一系列树。每一棵后续的树都试图纠正前一棵树的错误。换句话说，用于训练后续树的子数据集是根据前一棵树的错误预测进行加权的。这个持续的过程旨在逐步提高整体模型的表现。
- en: In summary, while bagging is based on creating independent trees from subsets
    of data, boosting focuses on the iterative training of trees that correct the
    errors of their predecessors. Both are used to improve the predictive capabilities
    of decision trees, but they take different approaches.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然袋装法基于从数据子集中创建独立的树，但提升法侧重于迭代训练的树来纠正其前驱的错误。两者都用于提高决策树的预测能力，但它们采用不同的方法。
- en: The special feature of boosting is the sequential construction of decision trees
    – that is, each tree is constructed based on the information processed and obtained
    from the previous tree.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的特殊之处在于决策树的顺序构建——也就是说，每一棵树都是基于从上一棵树处理和获得的信息构建的。
- en: Unlike other techniques that attempt to model a decision tree on the entire
    dataset, or at least a training set, the goal of boosting is to obtain a predictive
    model with incremental learning. We say stepwise learning because a first model
    is built on the data of the training set. A second model is then built that attempts
    to correct the errors of the first model. Rather than building a new model based
    on the previous *Y*-prediction, the approach is to build the next tree by trying
    to model the errors that were obtained as best as possible. The idea behind this
    is to create sequential weak learners (weak models), each of which tries to correct
    the errors of the previous model. In boosting, each subsequent tree in the ensemble
    is trained on the residuals of the previous tree, which are the differences between
    the actual and predicted values. This approach allows each tree to focus on correcting
    the errors of the previous trees, leading to improved overall accuracy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 与试图在整个数据集或至少是训练集上建模决策树的其他技术不同，提升的目标是获得具有增量学习的预测模型。我们称之为逐步学习，因为第一个模型是基于训练集的数据构建的。然后构建第二个模型，试图纠正第一个模型的错误。而不是基于之前的*Y*预测建立新的模型，方法是通过尽可能好地建模获得的错误来构建下一个树。这个想法是在创建序列弱学习器（弱模型），每个都试图纠正前一个模型的错误。在提升中，集成中的每个后续树都是基于前一个树的残差进行训练的，这些残差是实际值和预测值之间的差异。这种方法允许每棵树专注于纠正前一个树的错误，从而提高整体准确性。
- en: Each of these trees may have as few final nodes as possible, even though the
    initial dataset is large. This may be due to the parameters of the algorithm or
    because smaller trees may perform better in correcting the errors of the previous
    tree.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 即使初始数据集很大，这些树中的每一个也可能有尽可能少的最终节点。这可能是由于算法的参数，也可能是由于较小的树可能在纠正前一个树的错误方面表现更好。
- en: The goal, tree by tree, is to improve the final model in the areas where it
    is not performing best.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是逐棵树地改进模型在表现不佳的领域的最终模型。
- en: 'Boosting offers three parameters that can be manipulated to optimize the modeling
    of decision trees:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提升提供了三个可以操作的参数，可以用来优化决策树的建模：
- en: '**Number of trees** (**T**): As with bagging techniques, increasing the number
    of trees (*T*) generally leads to greater prediction accuracy. However, it is
    important to note that too high a value of *T* can increase the risk of overfitting
    the model. For example, compared to random forest, boosting tends to be more sensitive
    to overfitting at high values of *T*.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的数量**（**T**）：与袋装技术一样，增加树的数量（*T*）通常会导致更高的预测准确性。然而，需要注意的是，*T*的值过高会增加模型过拟合的风险。例如，与随机森林相比，提升在*T*值较高时更容易过拟合。'
- en: '**Learning rate** (**λ**): This parameter indicates the learning rate of the
    tree and usually has a low value, typically between 0.01 and 0.001\. The value
    of λ has a huge impact on the results of the model. A very low value of λ requires
    a larger number of trees (*T*) to obtain a well-performing model. The choice of
    the optimal value of λ can significantly influence the behavior of the model.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**（**λ**）：这个参数表示树的 学习率，通常具有很低的值，通常在0.01和0.001之间。λ的值对模型的结果有巨大的影响。非常低的λ值需要更多的树（*T*）来获得一个表现良好的模型。λ最佳值的选取可以显著影响模型的行为。'
- en: '**Number of splits** (**s**): *s* indicates the number of splits and has a
    value of one by default, which means that you get “stumps” or trees with only
    one parent node and two leaf nodes. Although a value of 1 for *s* is often used
    to speed up the calculation, changing it can significantly affect the model results.
    By changing the value of *s*, it is possible to obtain trees with different structures
    and complexities.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割数**（**s**）：*s*表示分割数，默认值为1，这意味着你得到“树桩”或只有一个父节点和两个叶节点的树。虽然通常使用1作为*s*的值来加快计算速度，但改变它可以显著影响模型结果。通过改变*s*的值，可以获得具有不同结构和复杂性的树。'
- en: 'There are several boosting techniques, each with variations in how trees are
    constructed or weighted to improve overall predictive ability. Some of the main
    boosting techniques are as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种提升技术，每种技术都有不同的构建树或加权的变体，以提高整体预测能力。以下是一些主要的提升技术：
- en: '**Adaptive Boosting** (**AdaBoost**): AdaBoost is one of the earliest boosting
    algorithms and one of the most popular. In this approach, each training sample
    is given an initial weight, and subsequent iterations focus on correcting errors
    made in previous iterations. Weak learning trees, often stumps, are built based
    on these weights. AdaBoost gives more weight to past errors, allowing subsequent
    trees to focus on improving predictions for previously misclassified examples.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应提升**（**AdaBoost**）：AdaBoost是最早的提升算法之一，也是最流行的之一。在这种方法中，每个训练样本被赋予一个初始权重，后续迭代专注于纠正前一次迭代中犯的错误。基于这些权重构建弱学习树，通常是树桩。AdaBoost给予过去的错误更多的权重，使后续的树能够专注于改进先前错误分类的示例的预测。'
- en: '**Gradient boosting**: Gradient boosting is a generic boosting framework that
    can be implemented in different ways. In gradient boosting, trees are built sequentially,
    but unlike AdaBoost, each tree tries to reduce the gradient of the loss function.
    This means that each successive tree focuses on examples that were misclassified
    or mispredicted by the model, gradually improving performance.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升**：梯度提升是一种通用的提升框架，可以以不同的方式实现。在梯度提升中，树是顺序构建的，但与AdaBoost不同，每一棵树都试图减少损失函数的梯度。这意味着每一棵后续的树都专注于模型错误分类或错误预测的示例，逐渐提高性能。'
- en: '**eXtreme Gradient Boosting** (**XGBoost**): XGBoost is an advanced implementation
    of gradient boosting that has gained popularity in ML competitions. It introduces
    several optimizations and regularizations to improve model performance and stability.
    XGBoost is known for its speed and can be used for both classification and regression
    problems.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**极端梯度提升**（**XGBoost**）：XGBoost是梯度提升的高级实现，在机器学习竞赛中获得了流行。它引入了几个优化和正则化来提高模型性能和稳定性。XGBoost以其速度著称，可用于分类和回归问题。'
- en: Each of these boosting techniques has its characteristics and specific adaptations
    for different data types and ML problems. The choice of boosting algorithm will
    depend on the specific needs of your problem and the desired performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些提升技术各有其特点，针对不同数据类型和机器学习问题有特定的适应方法。提升算法的选择将取决于你问题的具体需求和期望的性能。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied NLP, which automatically processes information that’s
    transmitted through spoken or written language. To begin, we analyzed the basic
    concepts of NLP by identifying the tasks that can be tackled and then moved on
    to the main approaches concerning text analysis and text generation. We then moved
    on to analyze corpora, words, and sentence tokenization. Corpora offers authentic
    language data for examination, with words serving as the fundamental components
    of expression, and sentence tokenization organizing the text into coherent units
    for in-depth analysis.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了自然语言处理（NLP），它自动处理通过口语或书面语言传输的信息。首先，我们通过识别可以解决的问题分析了NLP的基本概念，然后转向涉及文本分析和文本生成的主要方法。然后，我们转向分析语料库、单词和句子分词。语料库提供了用于检查的真实语言数据，单词作为表达的基本组成部分，句子分词将文本组织成连贯的单位以进行深入分析。
- en: In the second part of this chapter, we analyzed a practical case of using NLP
    for labeling movie reviews. This is a sentiment analysis problem that aims to
    automatically identify the polarity of a textual comment. In this example, we
    were able to practically learn which tools to use in MATLAB to perform this type
    of analysis. In the final part of this chapter, we analyzed ensemble learning
    techniques to improve the performance of the algorithms. We understood the differences
    between boosting and bagging and discovered the different boosting techniques.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们分析了使用NLP对电影评论进行标注的实际案例。这是一个旨在自动识别文本评论极性的情感分析问题。在这个例子中，我们实际上学习了在MATLAB中执行此类分析应使用哪些工具。在本章的最后部分，我们分析了集成学习方法以改进算法的性能。我们理解了提升和袋装之间的差异，并发现了不同的提升技术。
- en: In the next chapter, we will understand the basic concepts of image processing
    and computer vision. We will discover the MATLAB tools for computer vision and
    how to implement a MATLAB model for object recognition. We will also understand
    transfer learning, domain adaptation, multi-task learning and saliency maps, feature
    importance scores, and gradient-based attribution methods.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解图像处理和计算机视觉的基本概念。我们将发现计算机视觉的MATLAB工具以及如何实现用于物体识别的MATLAB模型。我们还将了解迁移学习、领域自适应、多任务学习和显著性图、特征重要性分数以及基于梯度的归因方法。
