- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural Language Processing Using MATLAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) automatically processes information
    conveyed through spoken or written language. This task is fraught with difficulty
    and complexity, largely due to the innate ambiguity of human language. To enable
    **machine learning** (**ML**) and interaction with the world in ways typical of
    humans, it is essential not only to store data but also to teach machines how
    to translate this data simultaneously into meaningful concepts. As natural language
    interacts with the environment, it generates predictive knowledge. In this chapter,
    we will learn the basic concepts of NLP and how to build a model to label sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring corpora and word and sentence tokenize
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a MATLAB model to label sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding gradient boosting techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce basic ML concepts. To understand these topics,
    a basic knowledge of algebra and mathematical modeling is needed. You will also
    require working knowledge of the MATLAB environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work with the MATLAB code in this chapter, you’ll need the following files
    (available on GitHub at [https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition](https://github.com/PacktPublishing/MATLAB-for-Machine-Learning-second-edition)):'
  prefs: []
  type: TYPE_NORMAL
- en: '`IMDBSentimentClassification.m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImdbDataset.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP is a field that’s dedicated to the development of technology that enables
    computers to interact with, understand, and generate human language in a way that
    mimics natural human communication. This involves various techniques and approaches
    aimed at processing and analyzing the complexities of natural languages, such
    as English, Chinese, Arabic, and more. The goal is to bridge the gap between human
    language and computer language, allowing computers to comprehend and generate
    text as if they were engaging in a conversation with a human interlocutor (*Figure
    7**.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – NLP tasks](img/B21156_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – NLP tasks
  prefs: []
  type: TYPE_NORMAL
- en: NLP strives to develop information technology tools for analyzing, comprehending,
    and creating texts in a manner that resonates with human understanding, mimicking
    interactions with another human rather than a machine. Natural language, both
    spoken and written, represents the most instinctive and widespread mode of communication.
    In contrast to formal languages, it holds a greater level of intricacy, often
    carrying connotations and uncertainties, which renders its processing quite challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP encompasses a range of tasks, including but not limited to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text understanding**: This involves extracting meaning and information from
    text. It includes tasks such as sentiment analysis (determining the emotional
    tone of a text), named entity recognition (identifying names of people, places,
    organizations, and so on), and text classification (categorizing text into predefined
    classes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language generation**: This aspect focuses on generating coherent and contextually
    appropriate human-like text. It includes tasks such as machine translation (translating
    text from one language to another), text summarization (creating concise summaries
    of longer texts), and dialogue generation (constructing natural-sounding conversational
    responses).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speech recognition**: NLP also extends to the realm of spoken language. Speech
    recognition technology converts spoken language into text, enabling applications
    such as voice assistants and transcription services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language models**: Recent advances in NLP have led to the development of
    large language models such as GPT-3, which can generate remarkably human-like
    text based on the input it receives. These models are trained on massive amounts
    of text data and can be fine-tuned for various NLP tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots and virtual assistants**: NLP is the foundation of chatbots and
    virtual assistants, which can engage in text-based or voice-based conversations
    with users, providing information, assistance, and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The field of NLP is interdisciplinary, drawing from computer science, linguistics,
    cognitive psychology, and more. It involves working with linguistic structures,
    statistical models, ML algorithms, and **deep learning** (**DL**) techniques to
    process, understand, and generate human language. While significant progress has
    been made, NLP still faces challenges, such as handling context, understanding
    nuances, and truly comprehending the intricacies of human communication:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.2 – Text analysis and text generation in \uFEFFNLP](img/B21156_07_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Text analysis and text generation in NLP
  prefs: []
  type: TYPE_NORMAL
- en: 'The pursuits in this domain encompass two main objectives: text analysis and
    text generation. These principles give rise to the subsequent disciplines, known
    as **natural language analysis** (**NLA**) and **natural language generation**
    (**NLG**). We’ll delve deep into both.'
  prefs: []
  type: TYPE_NORMAL
- en: NLA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This field centers on enhancing machines’ grasp of natural language. It involves
    transforming a natural language text into a structured and unequivocal representation.
    NLA involves the process of enabling machines to comprehend and interpret human
    language. Its primary goal is to bridge the gap between the unstructured nature
    of natural language and the structured representation that computers can work
    with. NLA encompasses several tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Syntax analysis**: This involves parsing the grammatical structure of sentences
    to understand the relationships between words and their roles (subject, object,
    verb, and so on). It helps in identifying how words are organized to convey meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic understanding**: NLA seeks to comprehend the meaning of words and
    phrases in context. It involves extracting the underlying concepts and intentions
    behind the words used in a text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Named entity recognition** (**NER**): NER is the process of identifying and
    classifying named entities, such as the names of people, organizations, locations,
    dates, and more, within a text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Sentiment analysis aims to determine the emotional
    tone expressed in a piece of text, classifying it as positive, negative, neutral,
    or even more nuanced emotions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coreference resolution**: Coreference refers to cases where different words
    or phrases in a text refer to the same entity. Coreference resolution identifies
    and connects these references to provide a coherent understanding of the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text classification**: Text classification involves categorizing text into
    predefined classes or categories based on its content. This is used for tasks
    such as spam detection, topic classification, and content tagging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information extraction**: This involves extracting specific information or
    structured data from unstructured text. An example of this is extracting dates,
    events, relationships, or numerical data from news articles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency parsing**: Dependency parsing analyzes the grammatical relationships
    between words in a sentence, often represented as a tree structure that shows
    how words depend on each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language modeling**: Language models, often trained using ML techniques,
    are used to predict the likelihood of words or phrases occurring based on the
    context of the surrounding text. These models play a crucial role in understanding
    the probabilities and patterns of language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parsing ambiguity**: Languages often contain ambiguous structures that can
    be interpreted in multiple ways. NLA aims to resolve such ambiguities to arrive
    at the intended meaning of a sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLA draws from linguistic theories, computational linguistics, and ML. It’s
    a complex field that continues to evolve, driven by advances in technology and
    the increasing demand for machines to understand and process human language in
    diverse applications, including search engines, chatbots, sentiment analysis tools,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: NLG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, the emphasis is on enabling machines to construct sentences in natural
    language. This involves developing applications that can produce accurate sentences
    in a specific language. NLG is a pivotal aspect of NLP that revolves around the
    creation of human-like text by computers. The main objective of NLG is to enable
    machines to generate coherent, contextually appropriate, and linguistically accurate
    textual content, mimicking the way humans communicate. NLG encompasses various
    techniques and processes to transform structured data or information into readable
    and understandable language. Here are some key points regarding NLG:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine translation**: One of the earliest forms of NLG, machine translation
    involves translating text from one language into another while retaining its meaning
    and context. Modern machine translation systems often utilize neural network-based
    approaches for improved accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**: NLG is employed in summarizing longer texts, such as
    articles or documents, by condensing the main ideas and relevant information into
    a concise and coherent summary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dialogue generation**: NLG powers chatbots, virtual assistants, and other
    conversational agents. It involves constructing natural-sounding responses in
    conversations, considering the ongoing context and user inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data-to-text generation**: NLG is used to convert structured data, such as
    statistical figures or database entries, into human-readable narratives. An example
    of this is generating weather reports from weather data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content creation**: NLG systems can create articles, reports, and other content
    based on predefined templates or prompts. This can be particularly useful for
    generating routine reports or news updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized messaging**: NLG can be employed to generate personalized messages
    or recommendations tailored to individual users – for instance, crafting product
    recommendations based on a user’s browsing history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storytelling and creative writing**: Some NLG systems can generate fictional
    stories, poetry, or creative pieces of writing. While these might not be as advanced
    as human-generated content, they showcase the potential of NLG in creative domains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic code generation**: In programming, NLG can be used to generate
    code snippets or documentation based on high-level descriptions or specifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical reports**: NLG can assist in generating medical reports or patient
    summaries based on electronic health records and medical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLG techniques can range from rule-based approaches to more advanced ML methods,
    such as **recurrent neural networks** (**RNNs**) and transformer models such as
    **generative pre-trained transformer** (**GPT**). These models learn patterns
    and structures from vast amounts of text data to produce coherent and contextually
    relevant language.
  prefs: []
  type: TYPE_NORMAL
- en: NLG is integral in applications where conveying information in a human-readable
    format is essential, making technology more accessible and user-friendly. While
    NLG systems have made significant progress, challenges still exist, including
    maintaining coherence over longer texts, ensuring accuracy, and avoiding biases
    in generated content.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing NLP tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NLP encompasses a series of tasks within text analysis, resulting in a layered
    structure. The foundational strata upon which sentence analysis relies are outlined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Morphology analysis**: The objective of this stage is to break down input
    language strings into sets of tokens, which correspond to discrete words, sub-words,
    and punctuation elements. Through the process of tokenization, the text is fragmented,
    resulting in a sequence of tokens, with each token representing a word from the
    text. Within this stage, two concepts hold significance: the stem and the lemma.
    The stem is the foundational form of a word and is obtained by removing inflections
    (such as verb conjugations or noun plurals) from its altered version. Conversely,
    the lemma represents the standardized form of the word, chosen conventionally
    to encompass all its inflected variations. This phase involves identifying both
    the stem and lemma for each word, which can be accomplished through two distinct
    operations: stemming and lemmatization, respectively. This information becomes
    instrumental in subsequent analysis stages. The rationale behind this approach
    lies in the efficient utilization of memory – maintaining rules based on word
    components and their combinations that form specific inflected forms is much more
    resource-effective than managing each word as an individual element within an
    extensive inventory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Syntax analysis**: To grasp the meaning of a sentence, it’s insufficient
    to merely comprehend the definitions of its constituent words; it’s equally crucial
    to discern the relationships between these words. This stage addresses the syntactic
    examination of the provided text. All components of speech are recognized and
    encompass verbs, nouns, adjectives, adverbs, prepositions, and pronouns. The process
    that annotates each word with its respective part of speech is termed **part-of-speech
    tagging**. This process is divided into two sub-processes: firstly, shallow parsing
    generates a binary tree in which elementary segments – namely, the **nominal part**
    (**NP**) and the **verbal part** (**VP**) – are identified. Secondly, full parsing
    produces a syntactic tree that designates the syntactic role of each word within
    the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic analysis**: The semantic analysis phase leverages insights garnered
    from preceding analytical stages, encompassing the meanings of individual words
    and their interconnected relationships, to interpret the overall significance
    of the sentence. The process of named entity recognition is employed to detect
    and classify groups of words that collectively form an entity, which might include
    personal names, countries, events, and the like. Semantic processing endeavors
    to deduce the potential interpretation of a sentence, with a particular focus
    on the interplay between the meanings of the words within the sentence being examined.
    This level of processing may encompass semantic disambiguation for words that
    possess multiple meanings, akin to how disambiguation is performed for words that
    can adopt multiple syntactic roles. Diverse methodologies can be employed for
    disambiguation, some of which involve assessing the frequency of each meaning
    within a specific corpus, while others consider contextual cues. Additionally,
    certain methods tap into pragmatic knowledge within the document’s domain to aid
    in disambiguation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pragmatic analysis**: This phase revolves around identifying the contextual
    environment in which the text is situated and the subsequent utilization of that
    context. Specifically, pragmatics delves into how the context influences the interpretation
    of meanings. In this context, *context* refers to the surrounding circumstances,
    including a range of non-linguistic factors (such as social dynamics, environmental
    cues, and psychological factors) that impact linguistic expressions. Indeed, human
    language is not solely rooted in its morphological, syntactic, and semantic attributes;
    it is also deeply intertwined with external knowledge linked to the situation
    in which a sentence is embedded. Within pragmatic analysis, a distinction is often
    drawn between the literal meaning of an utterance and the intended meaning of
    the speaker. The literal meaning pertains to the direct interpretation of the
    expression, while the speaker’s intention refers to the underlying concept the
    speaker aims to convey. For accurate interpretation of communication, several
    factors may be essential. These include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the roles and statuses of the conversational participants
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing the spatiotemporal context of the situation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Possessing knowledge about the subject matter being discussed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to comprehend implied meanings from another speaker is termed **pragmatic
    competence**. Despite its significance, this type of analysis is still not extensively
    detailed in the literature, primarily due to the considerable challenges it poses.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing automatic processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From an automated processing perspective, the syntactic level presents fewer
    challenges. Complex sentences can be analyzed, their grammatical accuracy identified,
    and their syntactic structures reconstructed seamlessly. The semantic level, on
    the other hand, proves significantly more intricate. In simpler scenarios, the
    following approach is feasible:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence meaning is deduced from the meanings of individual words once the sentence’s
    syntactic structure has been discerned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The meanings of individual words are derived from a readily accessible dictionary
    through automated means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, numerous issues arise, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the same word can bear distinct meanings in diverse contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Secondly, the syntactic structure of a sentence can be ambiguous, occasionally
    attributing different structures to the same sequence of words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, certain idiomatic phrases possess meanings separate from the literal
    interpretations derived from composing the meanings of their constituent words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Artificial intelligence endeavors to tackle these challenges by leveraging a
    suitable knowledge base that the language processing program can access.
  prefs: []
  type: TYPE_NORMAL
- en: The pragmatic level introduces an even more intricate terrain. The primary challenge
    stems from our capacity, during a conversation, to anticipate the mental states
    of our conversational partners. In essence, effective communication necessitates
    the representation of interlocutors’ intentions, which are only partially evident
    in their spoken words.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve introduced the basic concepts of NLP, let’s analyze the specific
    contents that allow us to approach the problem from a practical point of view.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring corpora and word and sentence tokenizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The analysis of corpora, words, and sentence tokenization forms the basis for
    comprehensive language understanding. Corpora provides real-world language data
    for analysis, words constitute the elements of expression, and sentence tokenization
    structures the text into meaningful units for further investigation. This trio
    of concepts plays a central role in advancing linguistic research and enhancing
    NLP capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Corpora
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In linguistics and NLP, corpora refer to extensive collections of written or
    spoken texts that serve as valuable sources of data for linguistic analysis and
    language-related studies. Corpora provides a diverse range of language samples,
    enabling researchers to examine patterns, trends, and variations in language usage,
    syntax, and semantics across different contexts and genres.
  prefs: []
  type: TYPE_NORMAL
- en: 'Linguistic corpora represent sizable collections of spoken or written texts,
    often originating from authentic communication contexts (including speeches or
    newspaper articles). These collections are digitized and frequently accompanied
    by computerized tools for convenient access:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Word cloud of corpora basic key concepts. The words are displayed
    in different sizes, with the most frequent words being the largest. Word clouds
    are often used to identify the key concepts in a corpus](img/B21156_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Word cloud of corpora basic key concepts. The words are displayed
    in different sizes, with the most frequent words being the largest. Word clouds
    are often used to identify the key concepts in a corpus
  prefs: []
  type: TYPE_NORMAL
- en: Corpora serve the purpose of examining how language is practically utilized
    and validating overarching trends through statistical analysis. They hold a pivotal
    role in contemporary lexicography, aiding in tasks such as selecting lemmas based
    on their usage frequency, identifying typical linguistic structures involving
    specific words, and grasping subtle nuances of meaning within various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, corpora wield significance in the advancement of language technologies
    such as automatic translation and speech recognition. In these applications, corpora
    are employed to construct statistical language models. They also find utility
    in language education, supporting the creation of instructional resources. Particularly
    for advanced learners, corpora enable the deduction of word properties and linguistic
    structures by observing their contextual applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various examples of corpora; the most used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Brown Corpus**: One of the first and most well-known corpora in English,
    it contains over a million words of text from various sources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The New York Times Corpus**: A large corpus of news articles from the New
    York Times, spanning from 1987 to 2007'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Project Gutenberg Corpus**: A collection of over 60,000 free electronic
    books in the public domain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Linguistic Data Consortium** (**LDC**) **Corpora**: A repository of a
    wide variety of corpora, including those for different languages and domains'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MATLAB provides a variety of tools for accessing and processing corpora. Some
    of the most commonly used tools are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Text Analysis Toolbox**: A comprehensive toolbox for text processing,
    including functions for tokenization, stemming, lemmatization, and sentiment analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Natural Language Processing Toolbox**: A toolbox for NLP tasks, such
    as named entity recognition, part-of-speech tagging, and dependency parsing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Web Services Toolbox**: A toolbox for accessing and interacting with
    web services, including those that provide access to corpora'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Words are the foundational units of language that carry meaning and convey ideas.
    The analysis of words involves investigating their forms, meanings, relationships,
    and usage patterns. This examination can encompass aspects such as morphological
    structure, part-of-speech categorization, frequency distribution, and semantic
    associations. Studying words allows linguists to understand how language is structured
    and how meanings are conveyed through vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: In ML and DL tasks, words are represented as vectors of numbers. This is done
    because ML algorithms can only work with numerical data. The process of converting
    words into vectors is called word embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of word embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag of words** (**BoW**): In **BoW**, each unique word in a corpus is assigned
    a unique index. A document is then represented as a vector of the counts of each
    word in the document. Let’s assume vocab size = 4; here, a sample document that
    states “I love cats” is represented as [ 1 1 1 0]. In BoW, we simply represent
    the document by the frequency of each word. For example, if we have a vocabulary
    of 1,000 words, then the whole document is represented by a 1,000-dimensional
    vector, where the *ith* entry of the vector represents the frequency of the *ith*
    vocabulary word in the document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Word2vec**: Word2vec is a more sophisticated approach to word embedding that
    takes into account the context in which words appear. Word2vec learns two types
    of embeddings: word embeddings and context embeddings. Word embeddings are vectors
    that represent individual words, while context embeddings are vectors that represent
    the words that appear around a given word. For example, the word embedding for
    “cat” might be [0.1, 0.2, 0.3], while the context embedding for “cat” might be
    [0.4, 0.5, 0.6].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence tokenize
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the **sentence tokenization** process, we identify atomic elements known
    as tokens within each sentence. These tokens serve as the basis for analyzing
    and evaluating the sentence itself. Consequently, during the tokenization phase,
    we not only recognize and assess these elements but also occasionally convert
    negative constructs. This results in the text being divided into tokens, which
    can be thought of as *indivisible* units. This task is relatively straightforward
    for languages that use spaces to separate words but becomes considerably more
    intricate for languages with continuous spelling systems. Focusing on languages
    such as English, which fall into the former category, a token can be defined as
    any sequence of characters enclosed by spaces. However, it’s worth noting that
    this definition allows for numerous exceptions. In languages where word boundaries
    aren’t explicitly indicated in written text, this process is referred to as word
    segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Word tokenization process](img/B21156_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Word tokenization process
  prefs: []
  type: TYPE_NORMAL
- en: 'In the broader context of tokenization, several challenges need to be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of spaces separating words from preceding or succeeding punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instances where sequences of characters are not separated by spaces should be
    treated as two separate tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Occasions when sequences of characters are separated by spaces should be considered
    a single token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing uppercase and lowercase variations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Navigating variations in spelling conventions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, at the end of this procedure, we identify a string with an assigned
    meaning, which we refer to as a **token**. Each token is structured as a pair
    comprising a token name and an optional token value.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have examined the basics of NLP in detail, we can proceed to analyze
    the implementation of a sentence classifier in MATLAB.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a MATLAB model to label sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss a very interesting topic that is very popular
    in today’s society. I am referring to the importance of reviews in influencing
    a customer’s interest in making the right decision.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing sentiment analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sentiment analysis, a technique that utilizes NLP, extracts and analyzes subjective
    information from text. Analyzing vast datasets reveals collective opinions that
    impact various domains. While manual sentiment analysis is challenging, automated
    methods have emerged. However, automating language modeling is complex and costly
    due to the nuances of human language. Additionally, the methodology varies across
    languages, increasing complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A major challenge lies in determining the polarity of opinions. Polarity classification
    is subjective, with one sentence perceived differently by individuals based on
    their value systems. The rise of social media has heightened interest in sentiment
    analysis. As online expressions proliferate, this field has become valuable for
    businesses to promote products, identify opportunities, and protect their reputation.
    Challenges arise from the use of simplistic terms in sentiment analysis algorithms
    to convey opinions. Cultural influences, linguistic disparities, and contextual
    factors make converting written text into a binary positive or negative sentiment
    highly intricate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Sentiment analysis polarity](img/B21156_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Sentiment analysis polarity
  prefs: []
  type: TYPE_NORMAL
- en: Advanced sentiment analysis endeavors to pinpoint specific moods, such as happiness,
    sadness, or anger. For instance, it can involve tasks such as categorizing a song
    review not just as positive or negative, but also predicting a numerical score,
    conducting a comprehensive analysis of hotel reviews, and providing ratings for
    various aspects such as comfort, noise levels, and design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, it’s feasible to categorize a given text into one of two classes:
    objective or subjective. A text may contain objective information, as seen in
    a news article, or it can be subjective, such as the expression of political opinions
    in an interview. The subjectivity of sentences often hinges on their context,
    and even an objective document may feature subjective phrases, such as quotations.
    Discerning subjectivity versus objectivity can be more challenging than classifying
    polarity, as it relies heavily on the surrounding textual context.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis offers several approaches, with the most commonly employed
    methods falling into four broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lexicon-based methods**: These techniques identify emotional keywords within
    the text and assign them a predetermined affinity to represent specific emotions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rule-based methods**: This approach classifies text based on explicit emotional
    categories, relying on the presence of unambiguous emotional words such as *happy*,
    *sad*, or *bored*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical methods**: In this category, the aim is to ascertain the source
    of the sentiment (the subject) and the target (the object being evaluated). To
    gauge opinions in context and identify the assessed attribute, grammatical relations
    among the words are examined through an extensive analysis of the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML methods**: These methods employ various learning algorithms to deduce
    sentiment, often utilizing a dataset with predefined classifications (supervised
    methods). The learning process is iterative, requiring the construction of models
    that associate polarity with different types of comments and, when necessary,
    topic analysis for a more comprehensive understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The utilization of emojis and emoticons is on the rise. Instead of relying solely
    on words, people often express their emotions using tiny round faces and other
    symbols. These visual elements prove valuable for swiftly and pleasantly conveying
    one’s message. Using DL architectures, it is possible to learn the relationship
    between emoticons and sentiments. For example, a DL method might be trained on
    a large corpus of text and emoticons to learn the features of emoticons that are
    most predictive of sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how to apply the tools made available by DL to tackle a practical
    case of sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Movie review sentiment analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reviews have become an essential element for businesses to thrive, providing
    valuable feedback from customers and driving purchasing decisions. They serve
    as a reliable source of information for prospective buyers and offer sellers insights
    to enhance their products and services. Moreover, they foster user engagement
    and amplify the power of word-of-mouth marketing, significantly impacting a company’s
    online reputation. Securing positive reviews is crucial for businesses of all
    sizes to maintain profitability.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of social media and the internet has dramatically transformed consumer
    buying behaviors. Potential buyers now actively seek online reviews and information
    before making purchasing decisions, relying less on traditional word-of-mouth
    recommendations. They have become more independent and discerning, carefully considering
    their choices.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, film critics played a dominant role in shaping consumer preferences,
    influencing movie choices and fostering critical thinking. However, the advent
    of consumer-generated reviews has shifted the paradigm, with consumer opinions
    now holding greater weight in influencing purchasing decisions. This democratization
    of opinion has empowered consumers to share their experiences and shape the cultural
    landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the task of classifying movie reviews as either positive or negative sentiment,
    we will employ a short version of the IMDb Movie Reviews dataset. This dataset
    is specifically designed for binary sentiment classification and contains 748
    reviews opportunely labeled. The dataset was adequately revisited in this example
    using only two features: review and class.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how to build a model for classifying sentences using an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Using an LSTM model for label sentences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An LSTM model is an advanced form of **recurrent neural network** (**RNN**)
    that can learn long-term dependencies within sequences, making it a powerful tool
    for processing sequential data, such as text, speech, and time series data. This
    makes it ideal for tasks such as label sentence classification, where the goal
    is to assign a label to a sentence based on its overall sentiment. LSTM has been
    successfully applied to a variety of tasks, including sentiment analysis, machine
    translation, speech recognition, and natural language generation.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to implement a model to label sentences in MATLAB, we will use
    movie reviews that have been made by a lot of online user:.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we need to import the dataset into the MATLAB workspace. You can
    find the dataset in `.xlsx` format on the project’s GitHub page (`ImdbDataset.xlsx`).
    To simplify this task, the dataset has been appropriately trimmed. Once you’ve
    downloaded the file, all you need to do is specify the root folder’s path. Then,
    you can load the data into the MATLAB workspace like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After loading the data, we need to visualize the distribution of the classes
    in the dataset by creating a histogram. This will help us assess whether the data
    is evenly distributed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The diagram shown in *Figure 7**.6* will be drawn:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive](img/B21156_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6 – Class distribution of the movie review: 0 negative, 1 positive'
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent step involves dividing the data into training and validation
    sets. Partition the data into a training set and a separate held-out set for validation
    and testing purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `cvpartition()` function establishes a random dataset partition. You can
    employ this partition to create training and testing sets for assessing a statistical
    model through cross-validation. Set the holdout percentage to 30%; this means
    that 70% of the data will be allocated for training, while the remaining 30% will
    be reserved for validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will utilize the training option to obtain the training indices and
    the test option to obtain the testing indices for cross-validation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Following this, we will retrieve the review data and corresponding classes
    from the partitioned tables:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At this point, our data has been prepared for use, and we can start preprocessing
    the review data. We will employ a function designed for tokenization and text
    preprocessing called `preprocessText`. This function carries out the following
    tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizes the text using `tokenizedDocument`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Converts the text into lowercase using `lower`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Removes punctuation using `erasePunctuation`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will apply the `preprocessText` function to both the training and validation
    data for preprocessing:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After that, we have to apply the word encoding technique. This method involves
    transforming words from a human language, such as English, into a numerical or
    symbolic format that’s suitable for a wide range of computational tasks, with
    a particular focus on applications in NLP and ML.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To transform the documents into sequences of numeric indices, we will employ
    the `wordEncoding()` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function associates words within a vocabulary with numeric indices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To use the reviews as input for an LSTM, all inputs must have the same length.
    However, the reviews in the dataset vary in length. One solution is to truncate
    longer reviews and pad shorter ones to ensure they are all the same length. To
    truncate and pad the reviews, start by choosing a target length. Then, truncate
    longer documents and pad shorter ones. To achieve optimal results, the target
    length should be short enough that you can avoid discarding a significant amount
    of data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To find an appropriate target length, we can visualize a histogram of the lengths
    of the training documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start, we counted the length of each review. Then, we drew a histogram of
    the distribution. The following chart was printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.7 – Tokens distribution of the movie reviews](img/B21156_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – Tokens distribution of the movie reviews
  prefs: []
  type: TYPE_NORMAL
- en: 'By analyzing the tokens distribution of the movie reviews, we’ll notice that
    most reviews have several tokens between 0 and 30\. So, we can choose this number
    as the target length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to convert the reviews into numeric indices sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To do this, we used the `doc2sequence()` function, which transforms documents
    into sequences that are suitable for DL. This function provides a cell array containing
    the numeric indices of words in the documents, as determined by the word encoding
    provided. Each element within the sequences array is a vector consisting of indices
    representing the words within the respective document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will proceed with defining the architecture of the LSTM network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To feed sequence data into the network, incorporate a sequence input layer with
    an input size of 1\. A sequence input with 1 dimension means each numerically
    encoded token is represented by a 1x1 scalar. Following that, introduce a word
    embedding layer with a dimension of 150, which should match the number of words
    specified in the word encoding. Word embedding layers learn to map words to vectors
    in a way that captures the meaning of the words. Subsequently, include an LSTM
    layer with 50 hidden units. For a sequence-to-label classification task using
    the LSTM layer, configure `OutputMode` last. Finally, add a fully connected layer
    that’s the same size as the number of classes, followed by a softmax layer and
    a classification layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The architecture of the LSTM that’s been defined is shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have to specify the training options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following options were set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`adam solver`: Adam, derived from *Adaptive Moment Estimation*, is a widely
    embraced optimization algorithm in the realms of ML and DL. Its primary role is
    to adjust the parameters, while encompassing weights and biases, within a model
    as it undergoes training. Adam is especially renowned for its effectiveness in
    training neural networks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MiniBatchSize`: This is a hyperparameter that dictates the number of examples,
    which are individual data points or samples, that are employed in each training
    iteration during the execution of Adam or a similar optimization algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GradientThreshold`: This specifies a threshold value for gradients during
    the training process. It is used to make training models more stable and efficient,
    particularly when dealing with RNNs or deep architectures.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Shuffle`: This option shuffles the data in each epoch to increase the degree
    of randomness in the algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValidationData`: This option specifies the data used for the validation process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Plots`: With this option, the training process will be checked through a plot
    that upgrades the evolution of the process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Verbose`: The output of the training process will be hidden.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can start the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot will be printed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Training process of the LSTM](img/B21156_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Training process of the LSTM
  prefs: []
  type: TYPE_NORMAL
- en: The training process processed the data by building a network capable of returning
    a validation accuracy of 75.89%. Given the small amount of data available, this
    result can be considered satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the performance of the model, we can try the simulation with a slightly
    large number of LSTM units (100 instead of 50). However, changing the embedding
    dimension (from 150 to 100) could increase the validation accuracy to at least
    85%.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve analyzed how to build an LSTM to classify movie reviews, let’s
    learn how to improve the accuracy of predictions using ensemble methods.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding gradient boosting techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To improve the performance of an algorithm, we can perform a series of steps
    and use different techniques, depending on the type of algorithm and the specific
    problems being addressed. The first approach involves a thorough analysis of the
    data to identify possible inaccuracies or shortcomings. In addition, many algorithms
    have parameters that can be adjusted to achieve better performance – not to mention
    techniques such as feature scaling or feature selection. A popular technique is
    to combine the capabilities offered by different algorithms to achieve better
    overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching ensemble learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of **ensemble learning** involves the use of multiple models combined
    in a way that maximizes performance by exploiting their strengths and mitigating
    their relative weaknesses. These ensemble learning methods are based on weak learning
    models that do not achieve high levels of accuracy on their own, but when combined
    can produce robust predictions. In the context of ensemble learning, a **weak
    learner** is a model that can produce slightly better results than randomly, while
    a **strong learner** approaches an ideal model and can overcome problems typical
    of traditional models, such as overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each model that’s used returns predictions that may or may not be correct.
    The prediction error, in a supervised learning problem, is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: error = bias 2 + variance
  prefs: []
  type: TYPE_NORMAL
- en: 'The two fundamental components of error are precision and variance. The goal
    of models that use ensemble approaches is to obtain reliable predictions by trying
    to reduce both **variance** and **bias**. Variance is the variability of a model’s
    prediction for a given data point. A model with high variance is said to be overfitting,
    which means that it is learning the noise in the training data rather than the
    underlying patterns. This can also lead to poor generalization performance. Bias
    is the difference between the average prediction of a model and the correct value.
    A model with high bias is said to be underfitting, which means that it is not
    learning the underlying patterns in the data. This can lead to poor generalization
    performance, where the model does not perform well on unseen data. There is often
    a trade-off between these two components: classifiers with low accuracy tend to
    have high variance and vice versa. The goal of ML is to find a classifier that
    has a good balance of bias and variance. This is known as the bias-variance trade-off.
    In general, it is difficult to find a classifier that has both low bias and low
    variance. However, several techniques can be used to reduce bias and variance,
    such as feature selection, regularization, and cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: To improve the accuracy of the model, it is assumed that the different classifiers
    can make different errors on each sample, but agree on the correct classification.
    By averaging the outputs of the classifiers, the overall error is reduced by averaging
    the error components. Combining the outputs by the model does not necessarily
    guarantee significantly better classification performance than other models, but
    it does help to reduce the likelihood of selecting a classifier with disappointing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ensemble methods. They mainly differ in the way they process
    the data. The most common are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first ensemble models were developed in the 1990s, but at the time of writing,
    there is no specific study that can help a user select the most appropriate meta-classifier.
    These methods are widely used to solve a variety of learning problems, including
    feature selection. In the subsequent sections, we will analyze the basics of each
    kind of ensemble technique.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging definition and meaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bagging** is an ensemble learning method that collects the predictions of
    weak learners (basic models), compares them, and combines them into a single prediction.
    The distinguishing feature of bagging is how the training sets for each decision
    tree are selected.'
  prefs: []
  type: TYPE_NORMAL
- en: To create each training set, bagging uses sampling with replacement, known as
    bootstrapping, on the original dataset. **Bootstrapping** is a statistical resampling
    technique that’s used to approximate the sample distribution of an estimator.
    This means that data is randomly selected from the original training set, with
    the possibility of some data being selected multiple times. This procedure increases
    the diversity of the decision trees that are produced.
  prefs: []
  type: TYPE_NORMAL
- en: A special feature of this method is that it works best when the basic learners
    (weak learners) are not very stable and therefore very sensitive to variations
    in the training data. In the presence of stable basic learners, the overall performance
    may even deteriorate.
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept behind bagging is to combine multiple models that are unstable
    but have a low bias to produce a low variance overall model. The logic behind
    bagging is based on the idea that the expected value of the mean of identically
    distributed random variables is equal to the expected value of a single random
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bagging equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: y _ predicted = 1 / T Σ _ t = 1 ^ T f _ t(x)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*T* is the number of models in the ensemble'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f_t(x)* is the prediction of the t-th model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the goal is to reduce variance by exploiting this averaging operation.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the decision tree is an appropriate choice for this approach because,
    despite its simplicity, it can effectively capture non-linear relationships and
    interactions between variables.
  prefs: []
  type: TYPE_NORMAL
- en: The key parameter for adjusting bagging is the number of trees. However, simulation
    studies and empirical evidence have shown that the performance of this model is
    not overly influenced by the choice of this parameter. Consequently, it is sufficient
    to choose a sufficiently large number of trees to obtain satisfactory results.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, bagging is an ensemble learning approach that’s based on repeatedly
    sampling training data and combining weak learner predictions and is particularly
    effective when working with basic models that are sensitive to variations in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Random forest** is a special case of bagging, with the basic aim of reducing
    variance by averaging over many models. Again, models based on decision trees
    are often the ideal choice as they can effectively capture the complex interaction
    structures in the data, which can be difficult to achieve with other approaches.
    By generating a large number of trees, bias tends to be low.'
  prefs: []
  type: TYPE_NORMAL
- en: Although the conceptual basis of both techniques is similar, random forest can
    produce superior results with the correct adjustment of model parameters. This
    is due to its ensemble learning approach and the ability to tune its hyperparameters
    effectively. This is because, although the trees have errors, the use of bagging
    allows trees to be generated from an identical distribution so that the expected
    value of each tree is equal to the expected value of the mean of the *N* trees
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: The bias of bagged models is similar to that of individual trees, so improvement
    focuses on reducing variance. This is the distinguishing feature of random forest
    models, which aim to improve the variance of bagging by reducing the correlation
    between trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'For *N* identically distributed random variables, each with variance, *σ2*,
    and with variance of the mean over *N* variables equal to *(1/N)σ2*, if the variables
    are identically distributed but not necessarily independent, and a positive pairwise
    correlation, ρ, is assumed, the variance of the mean is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Variance of the mean = (1 / N) σ 2+ [(N − 1) / N]ρ σ 2
  prefs: []
  type: TYPE_NORMAL
- en: In this formula, the first term represents the variance of the individual random
    variables divided by *N*, which represents the variance of the mean when the variables
    are independent. The second term considers the pairwise correlation between the
    random variables, increasing the variance of the mean according to how correlated
    they are.
  prefs: []
  type: TYPE_NORMAL
- en: The technique that’s used by random forest is to reduce the correlation between
    the trees, and hence the variance is to grow the trees by randomly selecting the
    input variables. The main difference with bagging is that random forest selects
    fewer candidate variables for partitioning than the total number of variables
    available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random forest is a method that combines a series of decision trees to produce
    more accurate results. These random decision trees are constructed while taking
    into account two key concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random sampling of the training data**: During the training phase, each tree
    in a random forest learns by focusing only on a random sample of the original
    data. These samples are extracted with replacement, a technique known as bootstrapping,
    which means that some data can be used multiple times within the same tree. While
    this theoretically produces a high variance in each tree compared to training
    on a specific dataset, the overall forest (the set of trees) tends to have a lower
    variance without a significant increase in bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random feature sets for partition choice**: In random forest, only a subset
    of the available features is considered for partitioning the nodes in each decision
    tree. Typically, this subset is the square root of the total number of features.
    For example, if there are 16 features, only four random features are considered
    at each node in each decision tree to perform the partitioning. Of course, it
    is also possible to train trees by considering all the features at each node,
    as you would typically do in regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, random forest combines hundreds or thousands of decision trees,
    each trained on a slightly different set of data and using only a limited number
    of features for subdivisions. The final predictions of random forest are obtained
    by averaging the predictions of each decision tree, which means that each tree
    contributes equally to the final decision.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Boosting** is another approach that can be applied to decision trees to improve
    their predictive ability. Like bagging, boosting can be applied to both regression
    and classification statistical models.'
  prefs: []
  type: TYPE_NORMAL
- en: Both Bagging and boosting can be applied to decision trees to predict their
    abilities. However, there are some differences. The crucial aspect to focus on
    in bagging is the independence of each tree generated by a subset from the others.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting, however, takes a different approach. Instead of generating completely
    independent trees as in bagging, boosting trains a sequence of trees iteratively.
    Each successive tree tries to correct the errors of the previous ones. In other
    words, the subsets of data used to train successive trees are weighted according
    to the erroneous predictions of the previous trees. This continuous process aims
    to progressively improve the performance of the overall model.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, while bagging is based on creating independent trees from subsets
    of data, boosting focuses on the iterative training of trees that correct the
    errors of their predecessors. Both are used to improve the predictive capabilities
    of decision trees, but they take different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The special feature of boosting is the sequential construction of decision trees
    – that is, each tree is constructed based on the information processed and obtained
    from the previous tree.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other techniques that attempt to model a decision tree on the entire
    dataset, or at least a training set, the goal of boosting is to obtain a predictive
    model with incremental learning. We say stepwise learning because a first model
    is built on the data of the training set. A second model is then built that attempts
    to correct the errors of the first model. Rather than building a new model based
    on the previous *Y*-prediction, the approach is to build the next tree by trying
    to model the errors that were obtained as best as possible. The idea behind this
    is to create sequential weak learners (weak models), each of which tries to correct
    the errors of the previous model. In boosting, each subsequent tree in the ensemble
    is trained on the residuals of the previous tree, which are the differences between
    the actual and predicted values. This approach allows each tree to focus on correcting
    the errors of the previous trees, leading to improved overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these trees may have as few final nodes as possible, even though the
    initial dataset is large. This may be due to the parameters of the algorithm or
    because smaller trees may perform better in correcting the errors of the previous
    tree.
  prefs: []
  type: TYPE_NORMAL
- en: The goal, tree by tree, is to improve the final model in the areas where it
    is not performing best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting offers three parameters that can be manipulated to optimize the modeling
    of decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of trees** (**T**): As with bagging techniques, increasing the number
    of trees (*T*) generally leads to greater prediction accuracy. However, it is
    important to note that too high a value of *T* can increase the risk of overfitting
    the model. For example, compared to random forest, boosting tends to be more sensitive
    to overfitting at high values of *T*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate** (**λ**): This parameter indicates the learning rate of the
    tree and usually has a low value, typically between 0.01 and 0.001\. The value
    of λ has a huge impact on the results of the model. A very low value of λ requires
    a larger number of trees (*T*) to obtain a well-performing model. The choice of
    the optimal value of λ can significantly influence the behavior of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of splits** (**s**): *s* indicates the number of splits and has a
    value of one by default, which means that you get “stumps” or trees with only
    one parent node and two leaf nodes. Although a value of 1 for *s* is often used
    to speed up the calculation, changing it can significantly affect the model results.
    By changing the value of *s*, it is possible to obtain trees with different structures
    and complexities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are several boosting techniques, each with variations in how trees are
    constructed or weighted to improve overall predictive ability. Some of the main
    boosting techniques are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adaptive Boosting** (**AdaBoost**): AdaBoost is one of the earliest boosting
    algorithms and one of the most popular. In this approach, each training sample
    is given an initial weight, and subsequent iterations focus on correcting errors
    made in previous iterations. Weak learning trees, often stumps, are built based
    on these weights. AdaBoost gives more weight to past errors, allowing subsequent
    trees to focus on improving predictions for previously misclassified examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosting**: Gradient boosting is a generic boosting framework that
    can be implemented in different ways. In gradient boosting, trees are built sequentially,
    but unlike AdaBoost, each tree tries to reduce the gradient of the loss function.
    This means that each successive tree focuses on examples that were misclassified
    or mispredicted by the model, gradually improving performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**eXtreme Gradient Boosting** (**XGBoost**): XGBoost is an advanced implementation
    of gradient boosting that has gained popularity in ML competitions. It introduces
    several optimizations and regularizations to improve model performance and stability.
    XGBoost is known for its speed and can be used for both classification and regression
    problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these boosting techniques has its characteristics and specific adaptations
    for different data types and ML problems. The choice of boosting algorithm will
    depend on the specific needs of your problem and the desired performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied NLP, which automatically processes information that’s
    transmitted through spoken or written language. To begin, we analyzed the basic
    concepts of NLP by identifying the tasks that can be tackled and then moved on
    to the main approaches concerning text analysis and text generation. We then moved
    on to analyze corpora, words, and sentence tokenization. Corpora offers authentic
    language data for examination, with words serving as the fundamental components
    of expression, and sentence tokenization organizing the text into coherent units
    for in-depth analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of this chapter, we analyzed a practical case of using NLP
    for labeling movie reviews. This is a sentiment analysis problem that aims to
    automatically identify the polarity of a textual comment. In this example, we
    were able to practically learn which tools to use in MATLAB to perform this type
    of analysis. In the final part of this chapter, we analyzed ensemble learning
    techniques to improve the performance of the algorithms. We understood the differences
    between boosting and bagging and discovered the different boosting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand the basic concepts of image processing
    and computer vision. We will discover the MATLAB tools for computer vision and
    how to implement a MATLAB model for object recognition. We will also understand
    transfer learning, domain adaptation, multi-task learning and saliency maps, feature
    importance scores, and gradient-based attribution methods.
  prefs: []
  type: TYPE_NORMAL
