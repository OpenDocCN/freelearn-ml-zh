["```py\nconda env create –f mlewp-chapter05.yml \n```", "```py\n    FROM python:3.10-slim as builder \n    ```", "```py\n    COPY . /src\n    RUN pip install --user --no-cache-dir -r requirements.txt \n    ```", "```py\n    FROM python:3.10-slim as app\n    COPY --from=builder /root/.local /root/.local\n    COPY --from=builder /src . \n    ```", "```py\n    ENV PATH=/root/.local:$PATH \n    ```", "```py\n    EXPOSE 5000 \n    ```", "```py\n    CMD [\"python3\", \"app.py\"] \n    ```", "```py\n    docker build -t basic-ml-microservice:latest \n    ```", "```py\n    docker images --format \"table {{.ID}}\\t{{.CreatedAt}}\\t{{.Repository}}\" \n    ```", "```py\n    docker run --rm -it -p 8080:5000 basic-ml-microservice:latest \n    ```", "```py\n    FROM python:3.10-slim as builder\n    COPY . /src\n    RUN pip install --user --no-cache-dir -r requirements.txt\n    FROM python:3.10-slim as app\n    COPY --from=builder /root/.local /root/.local\n    COPY --from=builder /src .\n    ENV PATH=/root/.local:$PATH\n    EXPOSE 5000\n    CMD [\"python3\", \"app.py\"] \n    ```", "```py\n    aws ecr create-repository \n        --repository-name basic-ml-microservice\n        --image-scanning-configuration scanOnPush=true \n        --region eu-west-1 \n    ```", "```py\n    aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin <ECR_REPOSITORY_URI> \n    ```", "```py\n    docker build --tag basic-ml-microservice:local \n    ```", "```py\n    docker tag basic-ml-microservice:local <ECR_REPOSITORY_URI> \n    ```", "```py\n    docker push <YOUR_AWS_ID>.dkr.ecr.eu-west-1.amazonaws.com/basic-ml-microservice:latest \n    ```", "```py\n    import datetime\n    from datetime import timedelta\n    from airflow import DAG\n    from airflow.operators.python import PythonOperator\n    from airflow.utils.dates import days_ago \n    ```", "```py\n    default_args = {\n        'owner': 'Andrew McMahon',\n        'depends_on_past': False,\n        'start_date': days_ago(31),\n        'email': ['example@example.com'],\n        'email_on_failure': False,\n        'email_on_retry': False,\n        'retries': 1,\n        'retry_delay': timedelta(minutes=2)\n    } \n    ```", "```py\n    with DAG(\n        dag_id=\"classification_pipeline\",\n        start_date=datetime.datetime(2021, 10, 1),\n        schedule_interval=\"@daily\",\n        catchup=False,\n    ) as dag: \n    ```", "```py\n     get_data_task = PythonOperator(\n            task_id=\"get_data\",\n            python_callable=get_data\n        ) \n    ```", "```py\n     train_model_task = PythonOperator(\n            task_id=\"train_model\",\n            python_callable=train_model\n        ) \n    ```", "```py\n     persist_model_task = PythonOperator(\n            task_id=\"persist_model\",\n            python_callable=persist_model\n        ) \n    ```", "```py\n    get_data_task >> train_model_task >> persist_model_task \n    ```", "```py\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"airflow:CreateWebLoginToken\",\n            \"Resource\": [\n                \"arn:aws:airflow:{your-region}:YOUR_ACCOUNT_ID:role/{your-                 environment-name}/{airflow-role}\"\n            ]\n        }\n    ]\n} \n```", "```py\n    name: Upload DAGS to S3\n    on:\n      push:\n        branches: [ main ]\n      pull_request:\n        branches: [ main ] \n    ```", "```py\n    jobs:\n      deploy:\n        name: Upload DAGS to Amazon S3\n        runs-on: ubuntu-latest\n        steps:\n          - name: Checkout\n            uses: actions/checkout@v2\n          - name: Configure AWS credentials from account\n            uses: aws-actions/configure-aws-credentials@v1\n            with:\n              aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n              aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n              aws-region: us-east-1 \n    ```", "```py\n     - name: Copy files to bucket with the AWS CLI\n            run: |\n              aws s3 cp ./dags s3://github-actions-ci-cd-tests\n              --recursive--include \"*.py\" \n    ```", "```py\npip install zenml \n```", "```py\npip install zenml[server] \n```", "```py\npip install zenml[templates] \n```", "```py\nzenml init —-template \n```", "```py\nfrom zenml.pipelines import pipeline\n\n@pipeline()\ndef model_training_pipeline(\n    data_loader,\n    data_processor,\n    data_splitter,\n    model_trainer,\n    model_evaluator,\n):\n    dataset = data_loader()\n    processed_dataset = data_processor(dataset=dataset)\n    train_set, test_set = data_splitter(dataset=processed_dataset)\n    model = model_trainer(train_set=train_set)\n    model_evaluator(\n        model=model,\n        train_set=train_set,\n        test_set=test_set,\n    ) \n```", "```py\nfrom zenml.enums import StrEnum\nfrom zenml.logger import get_logger\nfrom zenml.steps import (\n   BaseParameters,\n   Output,\n   step,\n) \n```", "```py\n@step\ndef data_loader(\n    params: DataLoaderStepParameters,\n    ) -> pd.DataFrame:\n    # Load the dataset indicated in the step parameters and format it as a\n    # pandas DataFrame\n    if params.dataset == SklearnDataset.wine:\n        dataset = load_wine(as_frame=True).frame\n    elif params.dataset == SklearnDataset.iris:\n        dataset = load_iris(as_frame=True).frame\n    elif params.dataset == SklearnDataset.breast_cancer:\n        dataset = load_breast_cancer(as_frame=True).frame\n    elif params.dataset == SklearnDataset.diabetes:\n        dataset = load_diabetes(as_frame=True).frame\n    logger.info(f\"Loaded dataset {params.dataset.value}:\n                                 %s\", dataset.info())\n    logger.info(dataset.head())\n    return dataset \n```", "```py\n@step\ndef data_processor(\n    params: DataProcessorStepParameters,\n    dataset: pd.DataFrame,\n    ) -> pd.DataFrame:\n    if params.drop_na:\n        # Drop rows with missing values\n        dataset = dataset.dropna()\n    if params.drop_columns:\n        # Drop columns\n        dataset = dataset.drop(columns=params.drop_columns)\n    if params.normalize:\n        # Normalize the data\n        target = dataset.pop(\"target\")\n        dataset = (dataset - dataset.mean()) / dataset.std()\n        dataset[\"target\"] = target\n    return dataset \n```", "```py\n@step\ndef data_splitter(\n    params: DataSplitterStepParameters,\n    dataset: pd.DataFrame,\n    ) -> Output(train_set=pd.DataFrame, test_set=pd.DataFrame,):\n    # Split the dataset into training and dev subsets\n    train_set, test_set = train_test_split(\n        dataset,\n        test_size=params.test_size,\n        shuffle=params.shuffle,\n        stratify=dataset[\"target\"] if params.stratify else None,\n        random_state=params.random_state,\n    )\n    return train_set, test_set \n```", "```py\nfrom zenml.enums import StrEnum\nfrom zenml.logger import get_logger\nfrom zenml.steps import (\n    BaseParameters,\n    Output,\n    step,\n)\nfrom artifacts import ModelMetadata\nfrom materializers import ModelMetadataMaterializer\nlogger = get_logger(__name__) \n```", "```py\nfrom typing import Any, Dict\nfrom sklearn.base import ClassifierMixin\n\nclass ModelMetadata:\n    def __init__(self) -> None:\n        self.metadata: Dict[str, Any] = {}\n    def collect_metadata(\n        self,\n        model: ClassifierMixin,\n        train_accuracy: float,\n        test_accuracy: float,\n        ) -> None:\n        self.metadata = dict(\n          model_type = model.__class__.__name__,\n          train_accuracy = train_accuracy,\n          test_accuracy = test_accuracy,\n        )\n\n    def print_report(self) -> None:\n        \"\"\"Print a user-friendly report from the model metadata.\"\"\"\n        print(f\"\"\"\n        Model type: {self.metadata.get('model_type')}\n        Accuracy on train set: {self.metadata.get('train_accuracy')}\n        Accuracy on test set: {self.metadata.get('test_accuracy')}\n        \"\"\") \n```", "```py\nfrom zenml.materializers.base_materializer import BaseMaterializer\n\nclass ModelMetadataMaterializer(BaseMaterializer):\n    # This needs to point to the artifact data type(s) associated with the\n    # materializer\n    ASSOCIATED_TYPES = (ModelMetadata,)\n    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA\n\n    def save(self, model_metadata: ModelMetadata) -> None:\n        super().save(model_metadata)\n        # Dump the model metadata directly into the artifact store as a\n        YAML file\n        with fileio.open(os.path.join(self.uri, 'model_metadata.yaml'),\n                         'w') as f:\n            f.write(yaml.dump(model_metadata.metadata))\n    def load(self, data_type: Type[ModelMetadata]) -> ModelMetadata:\n        super().load(data_type)\n        with fileio.open(os.path.join(self.uri, 'data.txt'), 'r') as f:\n            model_metadata = ModelMetadata()\n            model_metadata.metadata = yaml.safe_load(f.read())\n        return model_metadata \nChapter 4, *Packaging Up*.\n```", "```py\npython run.py \n```", "```py\nzenml up \n```", "```py\n    curl -Lo ./kind https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-linux-amd64 && \\\n    chmod +x ./kind && \\\n    mv ./kind /{YOUR_KIND_DIRECTORY}/kind \n    ```", "```py\n    brew install kind \n    ```", "```py\n    sudo apt-get install kubectl \n    ```", "```py\n    brew install kubernetes-cli \n    ```", "```py\n    kubectl version --client --output=yaml \n    ```", "```py\n    clientVersion:\n      buildDate: \"2023-01-18T15:51:24Z\"\n      compiler: gc\n      gitCommit: 8f94681cd294aa8cfd3407b8191f6c70214973a4\n      gitTreeState: clean\n      gitVersion: v1.26.1\n      goVersion: go1.19.5\n      major: \"1\"\n      minor: \"26\"\n      platform: darwin/arm64\n    kustomizeVersion: v4.5.7 \n    ```", "```py\n    kind create cluster –name=mlewp \n    ```", "```py\n    Creating cluster \"mlewp\" ...\n      Ensuring node image (kindest/node:v1.25.3) ![](img/B19525_05_001.png) \n      Preparing nodes ![](img/B19525_05_002.png)\n      Writing configuration ![](img/B19525_05_003.png)\n      Starting control-plane ![](img/B19525_05_004.png)\n      Installing CNI ![](img/B19525_05_005.png)\n      Installing StorageClass ![](img/B19525_05_006.png)\n    Set kubectl context to \"kind-mlewp\"\n    You can now use your cluster with:\n    kubectl cluster-info --context kind-mlewp\n    Thanks for using kind! ![](img/B19525_05_007.png) \n    ```", "```py\n    export PIPELINE_VERSION=1.8.5\n    kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n    kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\n    kubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\" \n    ```", "```py\nkubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80 \n```", "```py\nimport kfp\n\nclient = kfp.Client(host=\"http://localhost:8080\") \n```", "```py\npip install kfp –upgrade \n```", "```py\npip list | grep kfp \n```", "```py\nkfp                      1.8.19\nkfp-pipeline-spec        0.1.16\nkfp-server-api           1.8.5 \n```", "```py\n    from kfp import Client\n    import kfp.dsl\n    from kfp.v2 import dsl\n    from kfp.v2.dsl import Dataset\n    from kfp.v2.dsl import Input\n    from kfp.v2.dsl import Model\n    from kfp.v2.dsl import Output \n    ```", "```py\n    @dsl.component(packages_to_install=['pandas==1.3.5'])\n    def create_dataset(iris_dataset: Output[Dataset]):\n        import pandas as pd\n        csv_url = \"https://archive.ics.uci.edu/ml/machine-learning-\n                   databases/iris/iris.data\"\n        col_names = [\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\",\n                     \"Petal_Width\", \"Labels\"]\n        df = pd.read_csv(csv_url)\n        df.columns = col_names\n        with open(iris_dataset.path, 'w') as f: \n            df.to_csv(f) \n    ```", "```py\n    @dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])\n    def normalize_dataset(\n        input_iris_dataset: Input[Dataset],\n        normalized_iris_dataset: Output[Dataset],\n        standard_scaler: bool,\n        min_max_scaler: bool,\n    ):\n        if standard_scaler is min_max_scaler:\n            raise ValueError(\n                'Exactly one of standard_scaler or min_max_scaler must\n                 be True.')\n        import pandas as pd\n        from sklearn.preprocessing import MinMaxScaler\n        from sklearn.preprocessing import StandardScaler\n        with open(input_iris_dataset.path) as f:\n            df = pd.read_csv(f)\n        labels = df.pop('Labels')\n        if standard_scaler:\n            scaler = StandardScaler()\n        if min_max_scaler:\n            scaler = MinMaxScaler()\n        df = pd.DataFrame(scaler.fit_transform(df))\n        df['Labels'] = labels\n        with open(normalized_iris_dataset.path, 'w') as f:\n            df.to_csv(f) \n    ```", "```py\n    @dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-\n                   learn==1.0.2'])\n    def train_model(\n        normalized_iris_dataset: Input[Dataset],\n        model: Output[Model],\n        n_neighbors: int,\n    ):\n        import pickle\n        import pandas as pd\n        from sklearn.model_selection import train_test_split\n        from sklearn.neighbors import KNeighborsClassifier\n        with open(normalized_iris_dataset.path) as f:\n            df = pd.read_csv(f)\n        y = df.pop('Labels')\n        X = df\n        X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                           random_state=0)\n        clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n        clf.fit(X_train, y_train)\n        with open(model.path, 'wb') as f:\n            pickle.dump(clf, f) \n    ```", "```py\n    @dsl.pipeline(name='iris-training-pipeline')\n    def my_pipeline(\n        standard_scaler: bool,\n        min_max_scaler: bool,\n        neighbors: List[int],\n    ):\n        create_dataset_task = create_dataset()\n        normalize_dataset_task = normalize_dataset(\n            input_iris_dataset=create_dataset_task\n            .outputs['iris_dataset'],\n            standard_scaler=True,\n            min_max_scaler=False)\n        with dsl.ParallelFor(neighbors) as n_neighbors:\n            train_model(\n                normalized_iris_dataset=normalize_dataset_task\n                .outputs['normalized_iris_dataset'],\n                n_neighbors=n_neighbors) \n    ```", "```py\n    endpoint = '<KFP_UI_URL>'\n    kfp_client = Client(host=endpoint)\n    run = kfp_client.create_run_from_pipeline_func(my_pipeline,\n        mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n        arguments={\n            'min_max_scaler': True,\n            'standard_scaler': False,\n            'neighbors': [3, 6, 9]\n        },\n    )\n    url = f'{endpoint}/#/runs/details/{run.run_id}'\n    print(url) \n    ```", "```py\n    python basic_pipeline.py \n    ```", "```py\nhttp://localhost:8080/#/runs/details/<UID> \n```", "```py\n`cmplr = compiler.Compiler()`\n`cmplr.compile(my_pipeline, package_path='my_pipeline.yaml')` \n```"]