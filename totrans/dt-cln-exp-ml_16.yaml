- en: '*Chapter 12*: K-Nearest Neighbors for Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**K-nearest neighbors** (**KNN**) is a good choice for a classification model
    when there are not many observations or features and predicting class membership
    does not need to be very efficient. It is a lazy learner, so it is quicker to
    fit than other classification algorithms but considerably slower at classifying
    new observations. It can also yield less accurate predictions at the extremes,
    though this can be improved by adjusting *k* appropriately. We will consider these
    choices carefully in the model we will develop in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: KNN is perhaps the most straightforward non-parametric algorithm we could select,
    making it a good diagnostic tool. No assumptions need to be made about the distribution
    of features or the relationship that features have with the target. There are
    not many hyperparameters to tune, and the two key hyperparameters – nearest neighbors
    and the distance metric – are quite easy to interpret.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: KNN can be used successfully for both binary and multiclass problems without
    any extensions to the algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topic:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of KNN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN for binary classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN for multiclass classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the usual scikit-learn libraries, we will need the `imblearn`
    (Imbalanced Learn) library to run the code in this chapter. This library helps
    us handle significant class imbalance. `imblearn` can be installed with `pip install
    imbalanced-learn`, or `conda install -c conda-forge imbalanced-learn` if you are
    using Anaconda. All the code has been tested using scikit-learn versions 0.24.2
    and 1.0.2.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of KNN
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN might be the most intuitive algorithm that we will discuss in this book.
    The idea is to find *k* instances whose attributes are most similar, where that
    similarity matters for the target. That last clause is an important, though perhaps
    obvious, qualification. We care about similarity among those attributes associated
    with the target’s value.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: For each observation where we need to predict the target, KNN finds the *k*
    training observations whose features are most similar to those of that observation.
    When the target is categorical, KNN selects the most frequent value of the target
    for the *k* training observations. (We often select an odd value for *k* for classification
    problems to avoid ties.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: By *training* observations, I mean those observations that have known target
    values. No real training is done with KNN since it’s a lazy learner. I will discuss
    that in more detail in this section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the use of KNN for classification with values
    of 1 and 3 for *k*. When **k=1**, we would predict that our new observation, **X**,
    would be in the circle class. When **k=3**, it would be assigned to the square
    class:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – KNN with values of 1 and 3 for k ](img/B17978_12_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – KNN with values of 1 and 3 for k
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'But what do we mean by similar, or nearest, instances? There are several ways
    to measure similarity, but the most common measure is the Euclidean distance.
    The Euclidean distance is the sum of the squared difference between two points.
    This may remind you of the Pythagorean theorem. The Euclidean distance from point
    *a* to point *b* is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们所说的相似或最近的实例是什么意思呢？有几种方法可以衡量相似性，但最常用的度量是欧几里得距离。欧几里得距离是两点之间平方差的和。这可能会让你想起勾股定理。从点
    *a* 到点 *b* 的欧几里得距离如下：
- en: '![](img/B17978_12_0011.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17978_12_0011.jpg)'
- en: 'A reasonable alternative to the Euclidean distance is the Manhattan distance.
    The Manhattan distance from point *a* to point *b* is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离的一个合理的替代方案是曼哈顿距离。从点 *a* 到点 *b* 的曼哈顿距离如下：
- en: '![](img/B17978_12_002.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17978_12_002.jpg)'
- en: 'The default distance measure in scikit-learn is Minkowski. The Minkowski distance
    from point *a* to point *b* is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的默认距离度量是闵可夫斯基距离。从点 *a* 到点 *b* 的闵可夫斯基距离如下：
- en: '![](img/B17978_12_003.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图](img/B17978_12_003.jpg)'
- en: Notice that when *p* is 1, it is the same as the Manhattan distance. When *p*
    is 2, it is the same as the Euclidean distance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到当 *p* 为1时，它与曼哈顿距离相同。当 *p* 为2时，它与欧几里得距离相同。
- en: 'The Manhattan distance is sometimes called the taxicab distance. This is because
    it reflects the distance between two points along a path on a grid. The following
    diagram illustrates the Manhattan distance and compares it to the Euclidean distance:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离有时被称为出租车距离。这是因为它反映了两个点在网格路径上的距离。以下图表说明了曼哈顿距离并将其与欧几里得距离进行了比较：
- en: '![Figure 12.2 – The Euclidean and Manhattan measures of distance ](img/B17978_12_0021.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 欧几里得和曼哈顿距离度量](img/B17978_12_0021.jpg)'
- en: Figure 12.2 – The Euclidean and Manhattan measures of distance
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 欧几里得和曼哈顿距离度量
- en: Using the Manhattan distance can yield better results when features are very
    different in terms of type or scale. However, we can treat the choice of distance
    measure as an empirical question; that is, we can try both (or other distance
    measures) and see which gives us the best-performing model. We will demonstrate
    this with a grid search in the next section.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用曼哈顿距离可以在特征类型或尺度差异很大时产生更好的结果。然而，我们可以将距离度量的选择视为一个经验问题；也就是说，我们可以尝试两者（或其他的距离度量）并看看哪个给我们带来性能最好的模型。我们将在下一节通过网格搜索来演示这一点。
- en: As you likely suspect, KNN models are sensitive to the choice of *k*. Lower
    values of *k* will result in a model that attempts to identify subtle distinctions
    between observations. There is a substantial risk of overfitting at very low values
    of *k*. But at high values of *k*, our model may not be flexible enough. We are
    once again confronted with the variance-bias trade-off. Lower *k* values result
    in less bias and more variance, while higher values result in the opposite.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所怀疑的那样，KNN模型对 *k* 的选择很敏感。较低的 *k* 值会导致一个试图识别观察之间细微差异的模型。在非常低的 *k* 值时，存在过度拟合的实质性风险。但在
    *k* 值较高时，我们的模型可能不够灵活。我们再次面临方差-偏差权衡。较低的 *k* 值导致偏差较少而方差较多，而较高的值则相反。
- en: There is no definitive answer to the choice of *k*. But a good rule of thumb
    is to use the square root of the number of observations. However, just as we would
    do for the distance measure, we should test a model’s performance at different
    values of *k*. KNN is a non-parametric algorithm. No assumptions are made about
    the attributes of the underlying data, such as linearity or normally distributed
    features. This makes KNN quite flexible. It can be used to model a variety of
    relationships between features and the target.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *k* 的选择没有明确的答案。但一个好的经验法则是使用观察数的平方根。然而，就像我们对距离度量所做的那样，我们应该测试模型在不同 *k* 值下的性能。KNN是一种非参数算法。不对底层数据的属性做出假设，例如线性或正态分布的特征。这使得KNN非常灵活。它可以用来模拟特征与目标之间的各种关系。
- en: As mentioned previously, KNN is a lazy learner algorithm. No calculations are
    performed at training time. The learning happens only during testing. This has
    its advantages and disadvantages. It may not be a good choice when there are many
    instances or dimensions in the data, and the speed of predictions matters. KNN
    also tends not to perform well when we have sparse data, such as datasets that
    contain many 0 values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，KNN是一种懒惰学习算法。在训练时间不进行任何计算。学习仅在测试时发生。这有其优点和缺点。当数据中有许多实例或维度时，它可能不是一个好的选择，而且预测速度很重要。KNN也往往在稀疏数据上表现不佳，例如包含许多0值的数据集。
- en: We will use KNN in the next section to build a binary classification model,
    before constructing a couple of multiclass models in the following section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用KNN构建一个二分类模型，然后在下一节构建几个多分类模型。
- en: KNN for binary classification
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN用于二分类
- en: The KNN algorithm has some of the same advantages as the decision tree algorithm.
    No prior assumptions about the distribution of features or residuals have to be
    met. It is a suitable algorithm for the heart disease model we tried to build
    in the last two chapters. The dataset is not very large (30,000 observations)
    and does not have too many features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法与决策树算法有一些相同的优点。不需要满足关于特征或残差的分布的先验假设。它是我们试图在前两章中构建的心脏病模型的一个合适的算法。数据集不是很大（30,000个观测值）并且没有太多特征。
- en: Note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The heart disease dataset is available for public download at [https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).
    It is derived from the United States Center for Disease Control survey data on
    more than 400,000 individuals from 2020\. I have randomly sampled 30,000 observations
    from this dataset for the analysis in this section. Data columns include whether
    respondents ever had heart disease, body mass index, smoking history, heavy alcohol
    drinking, age, diabetes, and kidney disease.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏病数据集可在[https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)公开下载。它来源于2020年美国疾病控制与预防中心对超过40万人的调查数据。我已经从这个数据集中随机抽取了30,000个观测值用于本节的分析。数据列包括受访者是否曾经患有心脏病、体重指数、吸烟史、大量饮酒、年龄、糖尿病和肾病。
- en: 'Let’s get started with our model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的模型：
- en: 'First, we must load some of the same libraries we have used over the last couple
    of chapters. We will also load `KneighborsClassifier`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须加载我们在过去几章中使用的一些相同的库。我们还将加载`KneighborsClassifier`：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `healthinfo` module contains all of the code we used in [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*, to load the health information data and do the preprocessing.
    There is no need to repeat those steps here. If you have not read [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*, it might be helpful to at least scan the code in the second
    section of that chapter. That will give you a better sense of the features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`healthinfo`模块包含了我们在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*中使用的所有代码，用于加载健康信息数据并进行预处理。这里没有必要重复这些步骤。如果你还没有阅读[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*，至少浏览一下该章节的第二部分的代码可能会有所帮助。这将让你更好地了解特征。'
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module and
    display the feature names:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们获取由`healthinfo`模块处理过的数据并显示特征名称：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can use K-fold cross-validation to assess this model. We discussed K-fold
    cross-validation in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation*. We will indicate that we want 10 splits repeated 10 times.
    The defaults are `5` and `10`, respectively.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用K折交叉验证来评估这个模型。我们已经在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*中讨论了K折交叉验证。我们将指定我们想要重复10次10个分割。默认值分别是`5`和`10`。
- en: 'The precision of our model, the rate at which we are correct when we predict
    heart disease, is exceptionally poor at `0.17`. Sensitivity, the rate at which
    we predict heart disease when there is heart disease, is also low at `0.56`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的精确度，即我们预测心脏病时的正确率，异常低，为`0.17`。灵敏度，即存在心脏病时预测心脏病的比率，也较低，为`0.56`：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can improve the performance of our model with some hyperparameter tuning.
    Let''s create a dictionary for several neighbors and distance metrics. We will
    also try different values for the number of features selected with our `filter`
    method:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过一些超参数调整来提高我们模型的性能。让我们为几个邻居和距离度量创建一个字典。我们还将尝试使用我们的`filter`方法选择不同数量的特征：
- en: '[PRE3]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will base the scoring in the grid search on the area under the **receiver
    operating characteristic curve** (**ROC curve**). We covered ROC curves in [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model Evaluation*:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在网格搜索中的评分将基于**接收者操作特征曲线**（**ROC曲线**）下的面积。我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*中介绍了ROC曲线：
- en: '[PRE4]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can use the best estimator attribute of the randomized grid search to get
    the selected features from `selectkbest`:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用随机网格搜索的最佳估计器属性从`selectkbest`获取选定的特征：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also take a look at the best parameters and the best score. 11 features
    (out of 17) were selected, as we saw in the previous step. A *k* (`n_neighbors`)
    of `254` and the Manhattan distance metric were the other hyperparameters of the
    highest scoring model:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以查看最佳参数和最佳得分。11个特征（17个特征中的11个）被选中，正如我们在上一步中看到的。一个*k*（`n_neighbors`）为`254`和曼哈顿距离度量是得分最高的模型的另一个超参数：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s look at some more metrics for this model. We are doing much better with
    sensitivity but not with any of the other metrics:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的更多指标。我们在敏感性方面做得很好，但其他指标并不好：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We should also plot the confusion matrix. To do this, we can look at the relatively
    decent sensitivity. Here, we correctly identify most of the actual positives as
    positive. However, this comes at the expense of many false positives. We can see
    this in the precision score from the previous step. Most of the time we predict
    positive, we are wrong:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该绘制混淆矩阵。为此，我们可以查看相对较好的敏感性。在这里，我们正确地将大多数实际阳性识别为阳性。然而，这是以许多假阳性为代价的。我们可以从上一步的精确度得分中看到这一点。大多数时候我们预测阳性，我们都是错误的：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This produces the following plot:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning
    ](img/B17978_12_0031.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 超参数调整后的心脏病混淆矩阵](img/B17978_12_0031.jpg)'
- en: Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 超参数调整后的心脏病混淆矩阵
- en: In this section, you learned how to use KNN with a binary target. We can follow
    very similar steps to use KNN for a multiclass classification problem.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了如何使用具有二进制目标的KNN。我们可以遵循非常相似的步骤来使用KNN进行多类分类问题。
- en: KNN for multiclass classification
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KNN多类分类
- en: Constructing a KNN multiclass model is quite straightforward since it involves
    no special extensions to the algorithm, such as those needed to adapt logistic
    regression to targets with more than two values. We can see this by working with
    the same machine failure data that we worked with in the *Multinomial logistic
    regression* section of [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 构建KNN多类模型相当简单，因为它不需要对算法进行特殊扩展，例如将逻辑回归应用于具有两个以上值的目标所需的扩展。我们可以通过使用我们在[*第10章*](B17978_10_ePub.xhtml#_idTextAnchor126)，*逻辑回归*中的*多项式逻辑回归*部分使用的相同机器故障数据来看到这一点。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset on machine failure is available for public use at [https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification).
    There are 10,000 observations, 12 features, and two possible targets. One is binary
    and specifies whether the machine failed or did not. The other contains types
    of failure. The instances in this dataset are synthetic, generated by a process
    designed to mimic machine failure rates and causes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这份关于机器故障的数据集可在[https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)公开使用。有10,000个观测值，12个特征，以及两个可能的靶标。一个是二进制靶标，指定机器是否故障。另一个包含故障类型。该数据集中的实例是合成的，由一个旨在模仿机器故障率和原因的过程生成。
- en: 'Let’s build our machine failure type model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建我们的机器故障类型模型：
- en: 'First, let’s load the now-familiar modules:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载现在熟悉的模块：
- en: '[PRE9]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s load the machine failure data and take a look at its structure. There
    are 10,000 observations and no missing data. There is a combination of categorical
    and numerical data:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载机器故障数据并查看其结构。共有10,000个观测值，没有缺失数据。数据包括分类数据和数值数据的组合：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s also look at a few observations:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看一些观测值：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We should also do some frequencies on the categorical features. The overwhelming
    majority of observations, 97%, have no failures. This rather stark class imbalance
    will likely be difficult to model. There are three machine types – high quality,
    low quality, and medium quality:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s collapse some of the `failtype` values and check our work. First, we
    will define a function, `setcode`, to map the failure type text to a failure type
    code. We will assign random failures and tool wear failures to code `5` for other
    failures:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We should look at some descriptive statistics for our numeric features:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we are ready to create training and testing DataFrames. We will use the
    failure type code we just created for our target:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s set up the column transformation. For the numerical features, we
    will set outliers to the median and then scale the data. We will use min-max scaling,
    which will return values from 0 to 1 (the default for `MinMaxScaler`). We are
    using this scaler, rather than the standard scaler, to avoid negative values.
    The feature selection method we will use later, `selectkbest`, does not work with
    negative values:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s also take a peek at the columns that we will have after the encoding.
    We need this in advance of the oversampling we will do since the `SMOTENC` module
    needs the column indexes of the categorical features. We are oversampling to handle
    the significant class imbalance. We discussed this in more detail in [*Chapter
    11*](B17978_11_ePub.xhtml#_idTextAnchor135), *Decision Trees and Random Forest
    Classification*:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we will set up a pipeline for our model. The pipeline will do a column
    transformation, oversampling using `SMOTENC`, feature selection with `selectkbest`,
    and then run a KNN model. Remember that we have to pass the column indexes of
    the categorical features to `SMOTENC` for it to run correctly:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we are ready to fit our model. We will do a randomized grid search to
    identify the best value for *k* and the distance metric for the KNN. We will also
    search for the best *k* value for the feature selection:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s take a look at what our grid search found. All the features except for
    `processtemperature` were worth keeping in the model. The best values for *k*
    and the distance metric for the KNN were `125` and `minkowski`, respectively.  The
    best score, based on the area under the ROC curve, was `0.9`:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s look at a confusion matrix. Looking at the first row, we can see that
    a sizeable number of failures were found when no failure happened. However, our
    model does correctly identify most of the actual heat, power, and overstrain failures.
    This may not be a horrible precision and sensitivity trade-off. Depending on the
    problem, we may accept a large number of false positives to get an acceptable
    level of sensitivity in our model:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This produces the following plot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Confusion matrix for machine failure type after hyperparameter
    tuning ](img/B17978_12_004.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Confusion matrix for machine failure type after hyperparameter
    tuning
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 超参数调整后机器故障类型的混淆矩阵
- en: 'We should also look at a classification report. You may remember from [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for* *Model Evaluation*,
    that the macro average is a simple average across classes. Here, we are more interested
    in the weighted average. The weighted F1-score is not bad at `0.81`. Recall that
    F1 is the harmonic mean of precision and sensitivity:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一个分类报告。你可能还记得从[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*为模型评估做准备*，宏平均是简单地在类别间取平均。在这里，我们更感兴趣的是加权平均。加权F1分数为`0.81`并不差。记住，F1是精确率和敏感度的调和平均数：
- en: '[PRE22]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The class imbalance for the machine failure type makes it particularly difficult
    to model. Still, our KNN model does relatively well, assuming that the large number
    of false positives is not problematic. In this case, a false positive may not
    be nearly as much of an issue as a false negative. It may just involve doing more
    checks on machines that seem to be in danger of failing. If we compare that with
    being surprised by an actual machine failure, a bias toward sensitivity rather
    than precision may be appropriate.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 机器故障类型的类别不平衡使得建模特别困难。尽管如此，我们的KNN模型表现相对较好，假设大量假阳性不是问题。在这种情况下，一个假阳性可能不像一个假阴性那样成问题。它可能只是需要对看似有故障风险的机器进行更多检查。如果我们将其与实际机器故障的惊讶相比，偏向于敏感度而不是精确度可能是合适的。
- en: Let’s try KNN on another multiclass problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在另一个多类问题上尝试KNN。
- en: KNN for letter recognition
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字母识别的KNN
- en: We can take pretty much the same approach we just took for predicting machine
    failure with letter recognition. So long as we have features that do a good job
    of discriminating between the letters, KNN is a reasonable choice for that model.
    We will try this in this section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取与预测机器故障时使用字母识别相同的策略。只要我们有能够很好地区分字母的特征，KNN就是该模型的合理选择。我们将在本节尝试这种方法。
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will work with letter recognition data in this section. It is available for
    public use at [https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition](https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition).
    There are 26 letters (all capitals) and 20 different fonts. 16 different features
    capture different attributes of each letter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用字母识别数据。这些数据可在[https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition](https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition)公开使用。有26个字母（全部为大写）和20种不同的字体。16个不同的特征捕捉每个字母的不同属性。
- en: 'Let’s build the model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建模型：
- en: 'First, we will load the same libraries we have already been using:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载我们已经使用过的相同库：
- en: '[PRE23]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we will load the data and look at the first few instances. There are 20,000
    observations and 17 columns. `letter` is our target:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将加载数据并查看前几个实例。有20,000个观测值和17列。`letter`是我们的目标：
- en: '[PRE24]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, let’s create training and testing DataFrames:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建训练和测试数据框：
- en: '[PRE25]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, let’s instantiate a KNN instance. We will also set up stratified K-fold
    cross-validation and a dictionary for the hyperparameters. We will search for
    the best hyperparameter for *k* (`n_neighbors`) and the distance metric:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实例化一个KNN实例。我们还将设置分层K折交叉验证和超参数的字典。我们将寻找*k*（`n_neighbors`）和距离度量的最佳超参数：
- en: '[PRE26]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we are ready to do an exhaustive grid search. We are doing an exhaustive
    search here since we do not have a lot of hyperparameters to check. The best-performing
    distance metric is Euclidean. The best value for *k* for nearest neighbors is
    `3`. This model gets us nearly 95% accuracy:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好进行彻底的网格搜索。在这里我们进行彻底搜索是因为我们没有很多超参数需要检查。表现最好的距离度量是欧几里得距离。最近邻的*k*值是`3`。这个模型使我们几乎达到95%的准确率：
- en: '[PRE27]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s generate predictions and plot a confusion matrix:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们生成预测并绘制一个混淆矩阵：
- en: '[PRE28]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This produces the following plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 12.5 – Confusion matrix for letter predictions ](img/B17978_12_005.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – 字母预测的混淆矩阵](img/B17978_12_005.jpg)'
- en: Figure 12.5 – Confusion matrix for letter predictions
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 字母预测的混淆矩阵
- en: Let’s quickly summarize what we’ve learned in this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速总结本章所学的内容。
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter demonstrated how easy it is to use KNN for binary or multiclass
    classification. Since KNN does not make assumptions about normality or linearity,
    it can be used in cases where logistic regression may not yield the best results.
    This flexibility does bring with it a real risk of overfitting, so care has to
    be taken with the choice of *k*. We also explored how to hyperparameter tune both
    binary and multiclass models in this chapter. Finally, KNN is not a great option
    when we care about the speed of our predictions or if we are working with a large
    dataset. Decision tree or random forest classification, which we explored in the
    previous chapter, is often a better choice in those cases.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了使用KNN进行二分类或多分类分类是多么容易。由于KNN不对正态性或线性做出假设，因此它可以在逻辑回归可能不会产生最佳结果的情况下使用。这种灵活性确实带来了过拟合的真实风险，因此在选择*k*时必须谨慎。我们还在本章探讨了如何调整二分类和多分类模型的超参数。最后，当我们在乎预测速度或处理大型数据集时，KNN并不是一个很好的选择。在上一章中我们探讨的决策树或随机森林分类，在这些情况下通常是一个更好的选择。
- en: Another really good choice is support vector classification. We will explore
    support vector classification in the next chapter.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常好的选择是支持向量分类。我们将在下一章探讨支持向量分类。
