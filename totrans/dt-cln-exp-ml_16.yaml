- en: '*Chapter 12*: K-Nearest Neighbors for Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**K-nearest neighbors** (**KNN**) is a good choice for a classification model
    when there are not many observations or features and predicting class membership
    does not need to be very efficient. It is a lazy learner, so it is quicker to
    fit than other classification algorithms but considerably slower at classifying
    new observations. It can also yield less accurate predictions at the extremes,
    though this can be improved by adjusting *k* appropriately. We will consider these
    choices carefully in the model we will develop in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: KNN is perhaps the most straightforward non-parametric algorithm we could select,
    making it a good diagnostic tool. No assumptions need to be made about the distribution
    of features or the relationship that features have with the target. There are
    not many hyperparameters to tune, and the two key hyperparameters – nearest neighbors
    and the distance metric – are quite easy to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: KNN can be used successfully for both binary and multiclass problems without
    any extensions to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topic:'
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of KNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN for binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN for multiclass classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the usual scikit-learn libraries, we will need the `imblearn`
    (Imbalanced Learn) library to run the code in this chapter. This library helps
    us handle significant class imbalance. `imblearn` can be installed with `pip install
    imbalanced-learn`, or `conda install -c conda-forge imbalanced-learn` if you are
    using Anaconda. All the code has been tested using scikit-learn versions 0.24.2
    and 1.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts of KNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KNN might be the most intuitive algorithm that we will discuss in this book.
    The idea is to find *k* instances whose attributes are most similar, where that
    similarity matters for the target. That last clause is an important, though perhaps
    obvious, qualification. We care about similarity among those attributes associated
    with the target’s value.
  prefs: []
  type: TYPE_NORMAL
- en: For each observation where we need to predict the target, KNN finds the *k*
    training observations whose features are most similar to those of that observation.
    When the target is categorical, KNN selects the most frequent value of the target
    for the *k* training observations. (We often select an odd value for *k* for classification
    problems to avoid ties.)
  prefs: []
  type: TYPE_NORMAL
- en: By *training* observations, I mean those observations that have known target
    values. No real training is done with KNN since it’s a lazy learner. I will discuss
    that in more detail in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the use of KNN for classification with values
    of 1 and 3 for *k*. When **k=1**, we would predict that our new observation, **X**,
    would be in the circle class. When **k=3**, it would be assigned to the square
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – KNN with values of 1 and 3 for k ](img/B17978_12_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – KNN with values of 1 and 3 for k
  prefs: []
  type: TYPE_NORMAL
- en: 'But what do we mean by similar, or nearest, instances? There are several ways
    to measure similarity, but the most common measure is the Euclidean distance.
    The Euclidean distance is the sum of the squared difference between two points.
    This may remind you of the Pythagorean theorem. The Euclidean distance from point
    *a* to point *b* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_12_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A reasonable alternative to the Euclidean distance is the Manhattan distance.
    The Manhattan distance from point *a* to point *b* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_12_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The default distance measure in scikit-learn is Minkowski. The Minkowski distance
    from point *a* to point *b* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_12_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Notice that when *p* is 1, it is the same as the Manhattan distance. When *p*
    is 2, it is the same as the Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Manhattan distance is sometimes called the taxicab distance. This is because
    it reflects the distance between two points along a path on a grid. The following
    diagram illustrates the Manhattan distance and compares it to the Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – The Euclidean and Manhattan measures of distance ](img/B17978_12_0021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – The Euclidean and Manhattan measures of distance
  prefs: []
  type: TYPE_NORMAL
- en: Using the Manhattan distance can yield better results when features are very
    different in terms of type or scale. However, we can treat the choice of distance
    measure as an empirical question; that is, we can try both (or other distance
    measures) and see which gives us the best-performing model. We will demonstrate
    this with a grid search in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: As you likely suspect, KNN models are sensitive to the choice of *k*. Lower
    values of *k* will result in a model that attempts to identify subtle distinctions
    between observations. There is a substantial risk of overfitting at very low values
    of *k*. But at high values of *k*, our model may not be flexible enough. We are
    once again confronted with the variance-bias trade-off. Lower *k* values result
    in less bias and more variance, while higher values result in the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: There is no definitive answer to the choice of *k*. But a good rule of thumb
    is to use the square root of the number of observations. However, just as we would
    do for the distance measure, we should test a model’s performance at different
    values of *k*. KNN is a non-parametric algorithm. No assumptions are made about
    the attributes of the underlying data, such as linearity or normally distributed
    features. This makes KNN quite flexible. It can be used to model a variety of
    relationships between features and the target.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, KNN is a lazy learner algorithm. No calculations are
    performed at training time. The learning happens only during testing. This has
    its advantages and disadvantages. It may not be a good choice when there are many
    instances or dimensions in the data, and the speed of predictions matters. KNN
    also tends not to perform well when we have sparse data, such as datasets that
    contain many 0 values.
  prefs: []
  type: TYPE_NORMAL
- en: We will use KNN in the next section to build a binary classification model,
    before constructing a couple of multiclass models in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: KNN for binary classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The KNN algorithm has some of the same advantages as the decision tree algorithm.
    No prior assumptions about the distribution of features or residuals have to be
    met. It is a suitable algorithm for the heart disease model we tried to build
    in the last two chapters. The dataset is not very large (30,000 observations)
    and does not have too many features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The heart disease dataset is available for public download at [https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).
    It is derived from the United States Center for Disease Control survey data on
    more than 400,000 individuals from 2020\. I have randomly sampled 30,000 observations
    from this dataset for the analysis in this section. Data columns include whether
    respondents ever had heart disease, body mass index, smoking history, heavy alcohol
    drinking, age, diabetes, and kidney disease.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started with our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must load some of the same libraries we have used over the last couple
    of chapters. We will also load `KneighborsClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `healthinfo` module contains all of the code we used in [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*, to load the health information data and do the preprocessing.
    There is no need to repeat those steps here. If you have not read [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*, it might be helpful to at least scan the code in the second
    section of that chapter. That will give you a better sense of the features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s grab the data that’s been processed by the `healthinfo` module and
    display the feature names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can use K-fold cross-validation to assess this model. We discussed K-fold
    cross-validation in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation*. We will indicate that we want 10 splits repeated 10 times.
    The defaults are `5` and `10`, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The precision of our model, the rate at which we are correct when we predict
    heart disease, is exceptionally poor at `0.17`. Sensitivity, the rate at which
    we predict heart disease when there is heart disease, is also low at `0.56`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can improve the performance of our model with some hyperparameter tuning.
    Let''s create a dictionary for several neighbors and distance metrics. We will
    also try different values for the number of features selected with our `filter`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will base the scoring in the grid search on the area under the **receiver
    operating characteristic curve** (**ROC curve**). We covered ROC curves in [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for Model Evaluation*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the best estimator attribute of the randomized grid search to get
    the selected features from `selectkbest`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also take a look at the best parameters and the best score. 11 features
    (out of 17) were selected, as we saw in the previous step. A *k* (`n_neighbors`)
    of `254` and the Manhattan distance metric were the other hyperparameters of the
    highest scoring model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at some more metrics for this model. We are doing much better with
    sensitivity but not with any of the other metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also plot the confusion matrix. To do this, we can look at the relatively
    decent sensitivity. Here, we correctly identify most of the actual positives as
    positive. However, this comes at the expense of many false positives. We can see
    this in the precision score from the previous step. Most of the time we predict
    positive, we are wrong:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning
    ](img/B17978_12_0031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Heart disease confusion matrix after hyperparameter tuning
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to use KNN with a binary target. We can follow
    very similar steps to use KNN for a multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: KNN for multiclass classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Constructing a KNN multiclass model is quite straightforward since it involves
    no special extensions to the algorithm, such as those needed to adapt logistic
    regression to targets with more than two values. We can see this by working with
    the same machine failure data that we worked with in the *Multinomial logistic
    regression* section of [*Chapter 10*](B17978_10_ePub.xhtml#_idTextAnchor126),
    *Logistic Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This dataset on machine failure is available for public use at [https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification).
    There are 10,000 observations, 12 features, and two possible targets. One is binary
    and specifies whether the machine failed or did not. The other contains types
    of failure. The instances in this dataset are synthetic, generated by a process
    designed to mimic machine failure rates and causes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build our machine failure type model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the now-familiar modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s load the machine failure data and take a look at its structure. There
    are 10,000 observations and no missing data. There is a combination of categorical
    and numerical data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also look at a few observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also do some frequencies on the categorical features. The overwhelming
    majority of observations, 97%, have no failures. This rather stark class imbalance
    will likely be difficult to model. There are three machine types – high quality,
    low quality, and medium quality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s collapse some of the `failtype` values and check our work. First, we
    will define a function, `setcode`, to map the failure type text to a failure type
    code. We will assign random failures and tool wear failures to code `5` for other
    failures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should look at some descriptive statistics for our numeric features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to create training and testing DataFrames. We will use the
    failure type code we just created for our target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s set up the column transformation. For the numerical features, we
    will set outliers to the median and then scale the data. We will use min-max scaling,
    which will return values from 0 to 1 (the default for `MinMaxScaler`). We are
    using this scaler, rather than the standard scaler, to avoid negative values.
    The feature selection method we will use later, `selectkbest`, does not work with
    negative values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also take a peek at the columns that we will have after the encoding.
    We need this in advance of the oversampling we will do since the `SMOTENC` module
    needs the column indexes of the categorical features. We are oversampling to handle
    the significant class imbalance. We discussed this in more detail in [*Chapter
    11*](B17978_11_ePub.xhtml#_idTextAnchor135), *Decision Trees and Random Forest
    Classification*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will set up a pipeline for our model. The pipeline will do a column
    transformation, oversampling using `SMOTENC`, feature selection with `selectkbest`,
    and then run a KNN model. Remember that we have to pass the column indexes of
    the categorical features to `SMOTENC` for it to run correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to fit our model. We will do a randomized grid search to
    identify the best value for *k* and the distance metric for the KNN. We will also
    search for the best *k* value for the feature selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a look at what our grid search found. All the features except for
    `processtemperature` were worth keeping in the model. The best values for *k*
    and the distance metric for the KNN were `125` and `minkowski`, respectively.  The
    best score, based on the area under the ROC curve, was `0.9`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at a confusion matrix. Looking at the first row, we can see that
    a sizeable number of failures were found when no failure happened. However, our
    model does correctly identify most of the actual heat, power, and overstrain failures.
    This may not be a horrible precision and sensitivity trade-off. Depending on the
    problem, we may accept a large number of false positives to get an acceptable
    level of sensitivity in our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Confusion matrix for machine failure type after hyperparameter
    tuning ](img/B17978_12_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Confusion matrix for machine failure type after hyperparameter
    tuning
  prefs: []
  type: TYPE_NORMAL
- en: 'We should also look at a classification report. You may remember from [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for* *Model Evaluation*,
    that the macro average is a simple average across classes. Here, we are more interested
    in the weighted average. The weighted F1-score is not bad at `0.81`. Recall that
    F1 is the harmonic mean of precision and sensitivity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The class imbalance for the machine failure type makes it particularly difficult
    to model. Still, our KNN model does relatively well, assuming that the large number
    of false positives is not problematic. In this case, a false positive may not
    be nearly as much of an issue as a false negative. It may just involve doing more
    checks on machines that seem to be in danger of failing. If we compare that with
    being surprised by an actual machine failure, a bias toward sensitivity rather
    than precision may be appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try KNN on another multiclass problem.
  prefs: []
  type: TYPE_NORMAL
- en: KNN for letter recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can take pretty much the same approach we just took for predicting machine
    failure with letter recognition. So long as we have features that do a good job
    of discriminating between the letters, KNN is a reasonable choice for that model.
    We will try this in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will work with letter recognition data in this section. It is available for
    public use at [https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition](https://archive-beta.ics.uci.edu/ml/datasets/letter+recognition).
    There are 26 letters (all capitals) and 20 different fonts. 16 different features
    capture different attributes of each letter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the same libraries we have already been using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will load the data and look at the first few instances. There are 20,000
    observations and 17 columns. `letter` is our target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s instantiate a KNN instance. We will also set up stratified K-fold
    cross-validation and a dictionary for the hyperparameters. We will search for
    the best hyperparameter for *k* (`n_neighbors`) and the distance metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to do an exhaustive grid search. We are doing an exhaustive
    search here since we do not have a lot of hyperparameters to check. The best-performing
    distance metric is Euclidean. The best value for *k* for nearest neighbors is
    `3`. This model gets us nearly 95% accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s generate predictions and plot a confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Confusion matrix for letter predictions ](img/B17978_12_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Confusion matrix for letter predictions
  prefs: []
  type: TYPE_NORMAL
- en: Let’s quickly summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter demonstrated how easy it is to use KNN for binary or multiclass
    classification. Since KNN does not make assumptions about normality or linearity,
    it can be used in cases where logistic regression may not yield the best results.
    This flexibility does bring with it a real risk of overfitting, so care has to
    be taken with the choice of *k*. We also explored how to hyperparameter tune both
    binary and multiclass models in this chapter. Finally, KNN is not a great option
    when we care about the speed of our predictions or if we are working with a large
    dataset. Decision tree or random forest classification, which we explored in the
    previous chapter, is often a better choice in those cases.
  prefs: []
  type: TYPE_NORMAL
- en: Another really good choice is support vector classification. We will explore
    support vector classification in the next chapter.
  prefs: []
  type: TYPE_NORMAL
