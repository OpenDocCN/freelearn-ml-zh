["```py\nfrom xgboost import XGBClassifier\n...\ndataset = datasets.fetch_covtype()\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset.data, dataset.target, random_state=179\n    )\ny_train = y_train - 1\ny_test = y_test – 1\nxgb = XGBClassifier(\n    n_estimators=150, max_leaves=120, learning_rate=0.09\n    )\nxgb = xgb.fit(X_train, y_train)\nf1_score(y_test, xgb.predict(X_test), average=\"macro\")\n```", "```py\ntrain_data.sample(5)[[\"age\", \"education\", \"marital_status\", \"hours_per_week\", \"income_bracket\"]]\n```", "```py\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {\n    \"workclass\": sort_none_last(list(train_data[\"workclass\"].unique())),\n    \"education\": sort_none_last(list(train_data[\"education\"].unique())),\n    \"marital_status\": sort_none_last(list(train_data[\"marital_status\"].unique())),\n    \"occupation\": sort_none_last(list(train_data[\"occupation\"].unique())),\n    \"relationship\": sort_none_last(list(train_data[\"relationship\"].unique())),\n    \"race\": sort_none_last(list(train_data[\"race\"].unique())),\n    \"gender\": sort_none_last(list(train_data[\"gender\"].unique())),\n    \"native_country\": sort_none_last(list(train_data[\"native_country\"].unique())),\n    \"income_bracket\": sort_none_last(list(train_data[\"income_bracket\"].unique())),\n}\n```", "```py\nfor c in CATEGORICAL_FEATURES_WITH_VOCABULARY.keys():\n    for dataset in [train_data, test_data]:\n        dataset[c] = dataset[c].astype('category')\n        dataset[c] = dataset[c].astype('category')\n```", "```py\ndef map_to_index(val, vocab):\n    if val is None:\n        return None\n    return vocab.index(val)\nfor dataset in (train_data, test_data):\n    for feature, vocab in CATEGORICAL_FEATURES_WITH_VOCABULARY.items():\n        dataset[feature] = dataset[feature].map(lambda val: map_to_index(val, vocab))\n```", "```py\ntrain_data.isnull().sum()\ntrain_data.drop_duplicates(inplace=True)\ntrain_data.describe()\n```", "```py\ncounts = np.bincount(train_data[\"income_bracket\"])\nclass_weight = {\n    0: counts[0] / train_data.shape[0],\n    1: counts[1] / train_data.shape[0]\n}\n```", "```py\nscale_pos_weight = class_weight[0]/class_weight[1]\n```", "```py\nmodel = lgb.LGBMClassifier(force_row_wise=True, boosting_type=\"gbdt\", scale_pos_weight=scale_pos_weight)\nmodel = model.fit(X_train, y_train)\n```", "```py\nmodel = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight)\nmodel = model.fit(X_train, y_train)\n```", "```py\nlookup = layers.StringLookup(\n                vocabulary=vocabulary,\n                mask_token=None,\n                num_oov_indices=0,\n                output_mode=\"int\",\n            )\n```", "```py\ntrain_data_description = train_data.describe()\nCOLUMN_DEFAULTS = [\n    train_data_description[feature_name][\"mean\"] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n    for feature_name in HEADERS\n]\n```", "```py\noptimizer = tfa.optimizers.AdamW(\n    learning_rate=learning_rate,\n    weight_decay=weight_decay\n)\nmodel.compile(\n    optimizer=optimizer,\n    loss=keras.losses.BinaryCrossentropy(),\n    metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n             f1_metric,\n             precision_metric,\n             recall_metric],\n)\ntrain_dataset = get_dataset_from_csv(\n    train_data_file, batch_size, shuffle=True\n)\nvalidation_dataset = get_dataset_from_csv(\n    test_data_file, batch_size\n)\n```", "```py\ncallback = keras.callbacks.EarlyStopping(\n    monitor='loss', patience=3\n)\nhistory = model.fit(\n    train_dataset,\n    epochs=num_epochs,\n    validation_data=validation_dataset,\n    class_weight=class_weight,\n    callbacks=[callback]\n)\nmodel.evaluate(validation_dataset, verbose=0)\n```", "```py\nmodel = lgb.LGBMClassifier(force_row_wise=True, boosting_type=\"dart\", learning_rate=0.0023, max_bin=384, n_estimators=300, scale_pos_weight=scale_pos_weight, verbose=-1)\nscores = cross_val_score(model, X, y, scoring=\"f1_macro\")\nprint(f\"Mean F1-score: {scores.mean()}\")\n```"]