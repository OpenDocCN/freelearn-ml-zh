<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer263">
			<h1 id="_idParaDest-218"><em class="italic"><a id="_idTextAnchor217"/>Chapter 14</em>: Model Deployment, Endpoints, and Operations</h1>
			<p>In the previous chapter, we learned how to build efficient and scalable recommender engines through feature engineering, natural language processing, and distributed algorithms.</p>
			<p>In this chapter, we will tackle the next step after training a <strong class="bold">recommender engine</strong> or any machine learning model; we are going to deploy and operate the ML model. This will require us to package and register the model, build an execution runtime, build a web service, and deploy all components to an execution target.</p>
			<p>First, we will take a look at all the required preparations to deploy ML models to production. You will learn the steps that are required in a typical deployment process, how to package and register trained models, how to define and build inferencing environments, and how to choose a deployment target to run the model.</p>
			<p>In the next section, we will learn how to build a web service for a real-time scoring service, similar to Azure Cognitive Services, but using custom models and custom code. We will look into model endpoints, controlled rollouts, and endpoint schemas so that the models can be deployed without downtime and can be integrated into other services. Finally, we will also build a batch-scoring solution that can be scheduled or triggered through a web service or pipeline.</p>
			<p>In the last section, we will focus on how to monitor and operate your ML scoring services. In order to optimize performance and cost, you need to keep track not only of system-level metrics but also of telemetry data and scoring results to detect model or data drift. By the end of this section, you will be able to confidently deploy, tune, and optimize your scoring infrastructure in Azure.</p>
			<p>In this chapter, you will cover the following topics:</p>
			<ul>
				<li>Preparations for model deployments</li>
				<li>Deploying ML models in Azure</li>
				<li>ML operations in Azure</li>
			</ul>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor218"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create model deployments and endpoints:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0</strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0</strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2</strong></li>
				<li><strong class="source-inline">joblib 1.0.1</strong></li>
				<li><strong class="source-inline">numpy 1.19.5</strong></li>
				<li><strong class="source-inline">tensorflow 2.6.0</strong></li>
				<li><strong class="source-inline">pandas 1.3.3</strong></li>
				<li><strong class="source-inline">requests 2.25.1</strong></li>
				<li><strong class="source-inline">nvidia-smi 0.1.3</strong></li>
			</ul>
			<p>Similar to previous chapters, you can run this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. However, all scripts need to be scheduled to execute in Azure.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter14</a>.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor219"/>Preparations for model deployments</h1>
			<p>Throughout this<a id="_idIndexMarker1569"/> book, we have learned how to experiment with, train, and optimize various ML models to perform classification, regression, anomaly detection, image recognition, text understanding, and recommendations. Having successfully trained our ML model, we now want to package and deploy this model to production with tools in Azure.</p>
			<p>In this section, we will learn about the most important preparation steps that are required to deploy a trained model to production using Azure Machine Learning. We will discuss the different components involved in a standardized deployment, customizing a deployment, auto-deployments, and<a id="_idIndexMarker1570"/> how to choose the right deployment target. Let's delve into it.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor220"/>Understanding the components of an ML model</h2>
			<p>Independent of <a id="_idIndexMarker1571"/>the use case, there are similar preparation steps required for putting an ML model to production. First, the trained model needs to be registered in the model registry. This will allow us to track the model version and binaries and fetch a specific version of the model in a deployment. Second, we need to specify the deployment assets (for example, the environment, libraries, assets, and scoring file). These assets define exactly how the model is loaded and initialized, how user input is parsed, how the model is executed, and how the output is passed back to the user. Finally, we need to choose a compute target to run the model.</p>
			<p>When using Azure Machine Learning for deployments, there is a well-defined list of things you need to specify in order to deploy and run an ML model as a web service. This list includes the following components:</p>
			<ul>
				<li><strong class="bold">A trained model</strong>: The <a id="_idIndexMarker1572"/>model definition and parameters</li>
				<li><strong class="bold">An inferencing environment</strong>: A <a id="_idIndexMarker1573"/>configuration describing the environment, for example, as a Docker file</li>
				<li><strong class="bold">A scoring file</strong>: The web service code to parse user inputs and outputs and invoke the<a id="_idIndexMarker1574"/> model</li>
				<li><strong class="bold">A runtime</strong>: The<a id="_idIndexMarker1575"/> runtime for the scoring file, for example, Python or PySpark</li>
				<li><strong class="bold">A compute target</strong>: The <a id="_idIndexMarker1576"/>compute<a id="_idIndexMarker1577"/> environment to run the <a id="_idIndexMarker1578"/>web service, for example, <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) or <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>)</li>
			</ul>
			<p>Let's look into these five components in more detail:</p>
			<ol>
				<li>First, we need a <a id="_idIndexMarker1579"/>trained model. A model (depending on the framework, libraries, and algorithm used) consists of one or multiple files storing the model parameters and structure. In scikit-learn, this could be a pickled estimator; in <strong class="bold">LightGBM</strong>, this could be a serialized list of decision trees; and <a id="_idIndexMarker1580"/>in Keras, this could be a model definition and a binary blob storing the model weights. We call this the <em class="italic">model</em>, and we store and version it in Blob storage. At the startup time of your scoring service, the model will be loaded into the scoring runtime.</li>
				<li>Besides the model, we also need <a id="_idIndexMarker1581"/>an execution environment, which can be defined via <strong class="source-inline">InferenceConfig</strong>. In Azure Machine Learning deployments, the environment will be built into a <em class="italic">Docker</em> image and stored in your private Docker registry. During the deployment process, Azure Machine Learning will automatically build the Docker image from the provided environment configuration and load it into the private registry in your workspace.</li>
			</ol>
			<p>In Azure Machine Learning deployments, you can select predefined ML environments or configure your own environments and Docker base images. On top of the base image, you can define a list of Python <em class="italic">Pip</em> or <em class="italic">Conda</em> dependencies, enable GPU support, or configure custom Docker steps. The environment, including all required packages, will automatically be provided during runtime and set up on the Docker image. On top of this, the environment can be registered and versioned by the Azure Machine Learning service. This makes it easy to track, reuse, and organize your deployment environments.</p>
			<ol>
				<li value="3">Next, we<a id="_idIndexMarker1582"/> need a so-called scoring file. This file typically loads the model and provides a function to <a id="_idIndexMarker1583"/>score the model when given some data as input. Depending on the type of deployment, you need to provide a scoring file for either a (real-time) synchronous scoring service or an asynchronous batch-scoring service. The scoring files should be tracked in your version control system and will be mounted in the Docker image.</li>
				<li>To complete <strong class="source-inline">InferenceConfig</strong>, we are missing one last but important step: the Python runtime, used to run your scoring file. Currently, Python and PySpark are the only supported runtimes.</li>
				<li>Finally, we need an execution target that defines the compute infrastructure that the Docker image should be executed on. In Azure, this is called the compute target and is defined through the deployment configuration. The compute target can be a managed <a id="_idIndexMarker1584"/>Kubernetes cluster (such as AKS), a container instance (such as ACI), <strong class="bold">Azure Machine Learning Compute</strong> (<strong class="bold">AmlCompute</strong>), or one of the many other Azure compute services.<p class="callout-heading">Important Note</p><p class="callout">The preceding components are only required for managed deployments within Azure Machine Learning. Nothing prevents you from fetching the model binaries in another environment or running an inferencing environment (the Docker image) on your on-premises compute target.</p></li>
			</ol>
			<p>If you simply want to deploy a<a id="_idIndexMarker1585"/> standard model file, such as scikit-learn, <strong class="bold">ONNX</strong>, or TensorFlow models, you can also use the <a id="_idIndexMarker1586"/>built-in <em class="italic">auto-deployment</em> capabilities in Azure Machine Learning. Instead of providing all the preceding components, auto-deployment requires only the name and version of the used framework and a resource configuration, for example, the number of CPUs and the amount of RAM to execute. Azure Machine Learning will do the rest; it will provide all the required configurations and deploy the model to an ACI. This makes it easy to deploy standard models with no more than one line of code – great for development, debugging, and testing.</p>
			<p>Now that we know the basic deployment components in Azure Machine Learning, we can move on and look at an example of registering a model to prepare it for deployment.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor221"/>Registering your models in a model registry</h2>
			<p>The first step of the <a id="_idIndexMarker1587"/>deployment process should happen during or after the training and optimization process, namely <em class="italic">registering</em> the best model from each run<a id="_idIndexMarker1588"/> in the Azure Machine Learning model registry. Independent of whether your training script produces a single model, a model ensemble, or a model combined with multiple files, you should always store the training artifacts and register the best model from each run in your Azure Machine Learning workspace.</p>
			<p>It takes one additional line of code in your training script to store a model and register it in Azure Machine Learning and, therefore, never lose your training artifacts and models. The Blob storage and model registry are directly integrated with your workspace and so the process is tightly integrated into the training process. Once a model is registered, Azure Machine Learning provides a convenient interface to load the model from the registry.</p>
			<p>Let's take a<a id="_idIndexMarker1589"/> quick look at what this means for your training script:</p>
			<ol>
				<li value="1">Let's define the run context and train the <strong class="source-inline">sklearn</strong> classifier:<p class="source-code">Run = Run.get_context()</p><p class="source-code">exp = run.experiment</p><p class="source-code"># train your model</p><p class="source-code">clf, test_acc = train_sklearn_mnist()</p></li>
				<li>Next, we write a small helper function that returns the best test accuracy metric from all previous runs. We will use this metric to check whether the new model performs better than all previous runs:<p class="source-code">Def get_metrics(exp, metric):</p><p class="source-code">  for run in Run.list(exp, status='Completed'):</p><p class="source-code">    yield run.get_metrics().get(metric)</p><p class="source-code">m_name = 'Test accuracy'</p><p class="source-code">best_acc = max(get_metrics(exp, m_name), default=0)</p></li>
				<li>Next, we check whether the model has better performance than all previous runs and register it in the model factory as a new version:<p class="source-code">Import joblib</p><p class="source-code"># serialize the model and write it to disk</p><p class="source-code">joblib.dump(clf, 'outputs/model.pkl')</p><p class="source-code">if test_acc &gt; best_acc:</p><p class="source-code">  model = <strong class="bold">run.register_model</strong>(</p><p class="source-code">    model_name='sklearn_mnist',</p><p class="source-code">    model_path='outputs/model.pkl')</p><p class="source-code">  print(model.name, model.id, model.version, sep='\t')</p></li>
			</ol>
			<p>In the preceding code block, we first use the <strong class="source-inline">joblib.dump</strong><strong class="source-inline">()</strong> function to serialize and store a trained classifier to disk. We then call the <strong class="source-inline">run.model_register()</strong> function to upload the trained model to the default datastore and register the model to the disk. This will automatically track and version the model by name and link it to the current training run.</p>
			<ol>
				<li value="4">Once <a id="_idIndexMarker1590"/>your model is stored in the model registry of your Azure Machine Learning workspace, you can use it for deployments and retrieve it by name in any debugging, testing, or experimentation step. You can simply request the latest model by name, for example, by running the following snippet on your local machine:<p class="source-code">import joblib</p><p class="source-code">from azureml.core.model import Model</p><p class="source-code">model_path = Model.get_model_path('sklearn_mnist')</p><p class="source-code">model = joblib.load(model_path)</p></li>
			</ol>
			<p>All we did in the preceding code is run <strong class="source-inline">Model.get_model_path()</strong> to retrieve the latest version of a model by name. We can also specify a version number to load a specific model from the registry.</p>
			<p>A built-in <a id="_idIndexMarker1591"/>model registry is one of the functionalities of the Azure Machine Learning workspace that gets you hooked and makes you never want to miss a model registry, experiment run, and metrics tracking in the future. It gives you great flexibility and transparency when working with model artifacts in different environments and during different experiments.</p>
			<p>In the preceding example, we didn't provide any metadata about the trained model and, therefore, Azure Machine Learning couldn't infer anything from the model artifact. However, if we provide additional information about the model, Azure Machine Learning can autogenerate some of the required deployment configurations for you to enable auto-deployments. Let's take a look at this in the next section.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor222"/>Auto-deployments of registered models</h2>
			<p>If<a id="_idIndexMarker1592"/> you stick to the standard functionality provided in<a id="_idIndexMarker1593"/> scikit-learn, TensorFlow, or ONNX, you can also take advantage of auto-deployments in Azure Machine Learning. This will allow you to deploy registered models to testing, experimentation, or production environments without defining any of the required deployment configurations, assets, and service endpoints.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Azure Machine Learning model auto-deployment will automatically make your model available as a web service. If you provide model metadata during training, you can invoke auto-deployment using a single command, <strong class="source-inline">Model.deploy()</strong>.</p>
			<p>Let's take a look at how we need to change the previous example to take advantage of auto-deployments:</p>
			<ol>
				<li value="1">First, we define the resource configuration of the model as shown in the following code block:<p class="source-code">From azureml.core.resource_configuration import \</p><p class="source-code">  ResourceConfiguration</p><p class="source-code">resource_config = ResourceConfiguration(</p><p class="source-code">  cpu=1, memory_in_gb=2.0, gpu=0)</p></li>
				<li>Next, we need to define the framework and framework version when registering the model. To do so, we need to add this additional information to the model by extending the <strong class="source-inline">Model.register()</strong> arguments, as shown in the following snippet:<p class="source-code">From azureml.core import Model</p><p class="source-code">model = run.register_model(</p><p class="source-code">  model_name='sklearn_mnist',</p><p class="source-code">  model_path='outputs/model.pkl',</p><p class="source-code">  <strong class="bold">model_framework=Model.Framework.SCIKITLEARN,</strong></p><p class="source-code">  <strong class="bold">model_framework_version='0.24.2',</strong></p><p class="source-code">  resource_configuration= resource_config)</p></li>
			</ol>
			<p>In the<a id="_idIndexMarker1594"/> preceding code, we added the framework <a id="_idIndexMarker1595"/>and framework version to the model registry, as well as the resource configuration for this specific model. The model itself is stored in a standard format in one of the supported frameworks (scikit-learn, ONNX, or TensorFlow). This metadata is added to the model in the model registry. This is all the configuration required to auto-deploy this model as a real-time web service in a single line of code.</p>
			<ol>
				<li value="3">Finally, we call the <strong class="source-inline">Model.deploy()</strong> function to start the deployment process. This will build the deployment runtime as a Docker image, register it in your container registry, and start the image as a managed container instance, including the scoring file, REST service abstraction, and telemetry collection:<p class="source-code">Service_name = 'my-sklearn-service'</p><p class="source-code">service = <strong class="bold">Model.deploy</strong>(ws, service_name, [model])</p></li>
				<li>To retrieve the URL of the scoring service once the deployment is finished, we run the following code:<p class="source-code">service.wait_for_deployment(show_output=True)</p><p class="source-code">print(service.state)</p><p class="source-code">print("Scoring URL: " + service.scoring_uri)</p></li>
			</ol>
			<p>If you want <a id="_idIndexMarker1596"/>more granular control over the execution <a id="_idIndexMarker1597"/>environment, endpoint configuration, and compute target, you can use the advanced inference, deployment, and service configurations in order to customize your deployment. Let's now take a look at customized deployments.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor223"/>Customizing your deployment environment</h2>
			<p>As you <a id="_idIndexMarker1598"/>have seen in the previous chapters, the number of libraries, frameworks, and customization steps to transform data with an ML model is huge. Azure Machine Learning gives us enough flexibility to configure ML scoring services that can reflect these customizations. In this section, we will learn how to customize the deployment to include libraries and frameworks. Let's dive a bit deeper into these individual deployment steps.</p>
			<p>In the Azure Machine Learning service, you use an execution environment to specify a base Docker image, Python runtime, and all the dependent packages required to score your model. Like models, environments can also be registered and versioned in Azure, so both the Docker artifacts and the metadata are stored, versioned, and tracked in your workspace. This makes it simple to keep track of your environment changes, figure out which environment was used for a specific run, jump back and forth between multiple versions of an environment, and share an environment for multiple projects.</p>
			<p>Perform the following steps to build and package your deployment in Docker:</p>
			<ol>
				<li value="1">Let's start by writing a helper function to create environments on the fly. This snippet is very useful when creating environments programmatically based on a list of packages. We will also automatically add the <strong class="source-inline">azureml-defaults</strong> package to each environment:<p class="source-code">From azureml.core import Environment</p><p class="source-code">from azureml.core.conda_dependencies import \</p><p class="source-code">  CondaDependencies</p><p class="source-code">def get_env(name="my-env", packages=None):</p><p class="source-code">  packages = packages or []</p><p class="source-code">  packages += ['azureml-defaults']</p><p class="source-code">  conda_deps = CondaDependencies.create(</p><p class="source-code">    pip_packages=packages)</p><p class="source-code">  env = <strong class="bold">Environment</strong>(name=name)</p><p class="source-code">  <strong class="bold">env.python.conda_dependencies = conda_deps</strong></p><p class="source-code">  return env</p></li>
			</ol>
			<p>As you can see in <a id="_idIndexMarker1599"/>the preceding code block, we first initialize an <strong class="source-inline">Environment</strong> instance and then add multiple <strong class="source-inline">conda</strong> packages. We assign the <strong class="source-inline">conda</strong> dependencies by overriding the <strong class="source-inline">env.python.conda_dependencies</strong> property with the <strong class="source-inline">conda_deps</strong> dependencies. Using the same approach, we can also override Docker, Spark, and any additional Python settings using <strong class="source-inline">env.docker</strong> and <strong class="source-inline">env.spark</strong>, respectively.</p>
			<ol>
				<li value="2">Next, we can define a custom environment to use for experimentation, training, or deployment:<p class="source-code">myenv = get_env(name="PythonEnv",</p><p class="source-code">                packages=["numpy",</p><p class="source-code">                          "scikit-learn", </p><p class="source-code">                          "tensorflow"])</p></li>
				<li>In the next step, you can now register the environment using a descriptive name. This will add a new version of the current environment configuration to your environment with the same name:<p class="source-code">myenv.register(ws, name="PythonEnv")</p></li>
				<li>You can also retrieve the environment from the registry using the following code. This is also useful when you have registered a base environment that can be reused and extended for multiple experiments:<p class="source-code">myenv = Environment.get(ws, name="PythonEnv")</p></li>
				<li>As with the <strong class="source-inline">model</strong> registry, you can also load environments using a specified version as an <a id="_idIndexMarker1600"/>additional argument. Once you have configured an execution environment, you can combine it with a scoring file to an <strong class="source-inline">InferenceConfig</strong> object. The scoring file implements all functionalities to load the model from the registry and evaluate it given some input data. The configuration can be defined as follows:<p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">inference_config = InferenceConfig(</p><p class="source-code">  entry_script="score.py",</p><p class="source-code">  environment=myenv)</p></li>
			</ol>
			<p>We can see, in the preceding example, that we simply specify a relative path to the scoring script in the local authoring environment. Therefore, you first have to create this scoring file; we will go through two examples of batch and real-time scoring in the following sections.</p>
			<ol>
				<li value="6">To build an environment, we can simply trigger a build of the Docker image:<p class="source-code">from azureml.core import Image</p><p class="source-code">build = myenv.build(ws)</p><p class="source-code">build.wait_for_completion(show_output=True)</p></li>
				<li>The environment will be packaged and registered as a Docker image in your private container registry, containing the Docker base image and all specified libraries. If you want to package the model and the scoring file, you can package the model <a id="_idIndexMarker1601"/>instead. This is done automatically when deploying the model or can be forced by using the <strong class="source-inline">Model.package</strong> function. Let's load the model from the previous section and package and register the image:<p class="source-code">model_path = Model.get_model('sklearn_mnist')</p><p class="source-code">package = Model.package(ws, [model], inference_config)</p><p class="source-code">package.wait_for_creation(show_output=True)</p><p class="callout-heading">Important Note</p><p class="callout">The Azure ML SDK documentation contains a detailed list of possible configuration options, which you can find at <a href="https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)">https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment(class)</a>.</p></li>
			</ol>
			<p>The preceding code will build and package your deployment as a Docker image. In the next section, we will find out how to choose the best compute target to execute your ML deployment.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor224"/>Choosing a deployment target in Azure</h2>
			<p>One <a id="_idIndexMarker1602"/>of the great advantages of Azure Machine<a id="_idIndexMarker1603"/> Learning services is that they are tightly integrated with many other Azure services. This is extremely helpful with deployments where we want to run Docker images of the ML service on a managed service within Azure. These compute targets can be configured and leveraged for automatic deployment through Azure Machine Learning.</p>
			<p>If your job is to productionize ML training and deployment pipelines, you might not necessarily be an expert in Kubernetes. If that's the case, you might come to enjoy the tight integration of the management of Azure compute services in the Azure Machine Learning SDK. Similar to creating training environments, you can create GPU clusters, managed Kubernetes clusters, or simple container instances from within the authoring environment (for example, the Jupyter notebook orchestrating your ML workflow).</p>
			<p>We can follow a <a id="_idIndexMarker1604"/>general recommendation for choosing a specific<a id="_idIndexMarker1605"/> service, similar to choosing a compute service for regular application deployments; so, we trade off simplicity, cost, scalability, flexibility, and operational expense between the compute services that can easily start a web service from a Docker image.</p>
			<p>Here are recommendations<a id="_idIndexMarker1606"/> of when to use each Azure compute service:</p>
			<ul>
				<li>For quick experiments and local testing, use Docker and local deployment targets in Azure Machine Learning.</li>
				<li>For testing and experimentation, use ACI. It is easy to set up and configure, and it is made to run container images.</li>
				<li>For deployments of scalable real-time web services with GPU support, use AKS. This managed Kubernetes cluster is a lot more flexible and scalable, but also a lot harder to operate.</li>
				<li>For batch deployments, use Azure Machine Learning clusters, the same compute cluster environment we already used for training.</li>
			</ul>
			<p>For quick experiments, you can deploy your service locally using <strong class="source-inline">LocalWebservice</strong> as a deployment target. To do so, you can run the following snippet on your local machine, providing the scoring file and environment in the inferencing configuration:</p>
			<p class="source-code">From azureml.core.webservice import LocalWebservice</p>
			<p class="source-code">deployment_config = <strong class="bold">LocalWebservice.deploy_configuration</strong>(</p>
			<p class="source-code">  port=8890)</p>
			<p class="source-code">service = Model.deploy(ws,</p>
			<p class="source-code">  name=service_name,</p>
			<p class="source-code">  models=[model],</p>
			<p class="source-code">  inference_config=inference_config,</p>
			<p class="source-code">  deployment_config=deployment_config)</p>
			<p class="source-code">service.wait_for_deployment(show_output=True)</p>
			<p class="source-code">print(service.state)</p>
			<p>As you <a id="_idIndexMarker1607"/>can see, once your model is registered, you<a id="_idIndexMarker1608"/> can deploy it to multiple compute targets depending on your use case. While we have covered a few different configuration options, we haven't yet discussed multiple deployment options and scoring files. We will do this in the next section.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor225"/>Deploying ML models in Azure</h1>
			<p>Broadly<a id="_idIndexMarker1609"/> speaking, there are two common approaches to<a id="_idIndexMarker1610"/> deploying ML models, namely deploying them as synchronous real-time web services and as asynchronous batch-scoring services. Please note that the same model could be deployed as two different services, serving different use cases. The deployment type depends heavily on the batch size and response time of the scoring pattern of the model. Small batch sizes with fast responses require a horizontally scalable real-time web service, whereas large batch sizes and slow response times require horizontally and vertically scalable batch services.</p>
			<p>The deployment of a text-understanding model (for example, an entity recognition model or sentiment analysis) could include a real-time web service that evaluates the model whenever a new comment is posted to an app, as well as a batch scorer in another ML pipeline to extract relevant features from training data. With the former, we want to serve each request as quickly as possible, and so we will evaluate a small batch size synchronously. With the latter, we are evaluating large amounts of data, and so we will evaluate a large batch size asynchronously. Our aim is that, once the model is packaged and registered, we <a id="_idIndexMarker1611"/>can reuse it for either a task or use<a id="_idIndexMarker1612"/> case.</p>
			<p>In this section, we will take a look at these deployment approaches and build one service for real-time scoring and one for batch-scoring. We will also evaluate different options to manage and perform deployments for scoring services.</p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor226"/>Building a real-time scoring service</h2>
			<p>In this section, we <a id="_idIndexMarker1613"/>will build a real-time <a id="_idIndexMarker1614"/>scoring service in Azure Machine Learning. We will look into the required scoring file that will power the web service, as well as the configuration to start the service on an AKS cluster.</p>
			<p>For this example, we will train an NLP Hugging Face transformer model to perform sentiment analysis on user input. Our aim is to build our own Cognitive Services Text Analytics API that uses a custom model that is trained or fine-tuned on a custom dataset.</p>
			<p>To do so, we will train a sentiment analysis pipeline, save it, and register it as a model in Azure Machine Learning, as shown in the following snippet:</p>
			<p class="source-code">clf = train(name="sentiment-analysis")</p>
			<p class="source-code">clf.save_pretrained("outputs/sentiment-analysis")</p>
			<p class="source-code">model = Model.register(ws,</p>
			<p class="source-code">  model_name='sentiment-analysis',</p>
			<p class="source-code">  model_path='outputs/sentiment-analysis')</p>
			<p>Once we have the model, we start building the web service by taking a look at the scoring file. The scoring file will be loaded when the web service starts and gets invoked for every request to the ML service. Therefore, we use the scoring file to load the ML model, parse the user data from a request, invoke the ML model, and return the results of the ML model. To do so, you need to provide the <strong class="source-inline">init()</strong> and <strong class="source-inline">run()</strong> functions in the scoring file, where the <strong class="source-inline">run()</strong> function is run once when the service starts, and the <strong class="source-inline">run</strong> method is invoked with user inputs for every request. The following example shows a simple scoring file:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">scoring_file_example.py</p>
			<p class="source-code">def <strong class="bold">init()</strong>:</p>
			<p class="source-code">  print("Initializing service")</p>
			<p class="source-code">def <strong class="bold">run(data)</strong>:</p>
			<p class="source-code">  print("Received a new request with data: ", data)</p>
			<p>Now that we have<a id="_idIndexMarker1615"/> the trained model and <a id="_idIndexMarker1616"/>we know the structure of the scoring file, we can go ahead and build our custom web service:</p>
			<ol>
				<li value="1">Let's start with the initialization of the service. We first define a global model variable, and then fetch the model path from the <strong class="source-inline">AZUREML_MODEL_DIR</strong> environment variable. This variable contains the location of the model on the local disk. Next, we load the model using the Hugging Face <strong class="source-inline">AutoModel</strong> transformer:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Scoring_file.py</p>
			<p class="source-code">from transformers import AutoModel</p>
			<p class="source-code">from azureml.core import Model</p>
			<p class="source-code">def init():</p>
			<p class="source-code">  global model</p>
			<p class="source-code">  model_path = os.getenv("AZUREML_MODEL_DIR")</p>
			<p class="source-code">  model = AutoModel.from_pretrained(model_path,</p>
			<p class="source-code">                                    from_tf=True)</p>
			<ol>
				<li value="2">Next, we tackle the actual inferencing part of the web service. To do so, we need to parse incoming requests, invoke the NLP model, and return the prediction to the caller:</li>
			</ol>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Scoring_file.py</p>
			<p class="source-code">import json</p>
			<p class="source-code">def run(request):</p>
			<p class="source-code">    try:</p>
			<p class="source-code">        data = json.loads(request)</p>
			<p class="source-code">        text = data['query']</p>
			<p class="source-code">        sentiment = model(text)</p>
			<p class="source-code">        result = {'sentiment': sentiment}</p>
			<p class="source-code">        return result</p>
			<p class="source-code">    except Exception as e:</p>
			<p class="source-code">        return str(e)</p>
			<p>In the <strong class="source-inline">run()</strong> function, we are provided with a <strong class="source-inline">request</strong> object. This object contains the body of the request sent to the service. As we expect JSON input, we parse the request body as a JSON object and access the input string via the <strong class="source-inline">query</strong> property. We expect a client to send a valid request that contains exactly this schema. Finally, we return a prediction that will be automatically serialized into JSON and returned to the caller.</p>
			<ol>
				<li value="3">Let's deploy<a id="_idIndexMarker1617"/> the service to <a id="_idIndexMarker1618"/>an ACI compute target for testing purposes. To do so, we need to update the deployment configuration to contain the ACI resource configuration:<p class="source-code">from azureml.core.webservice import AciWebservice</p><p class="source-code">deploy_config = AciWebservice.deploy_configuration(</p><p class="source-code">  cpu_cores=1,</p><p class="source-code">  memory_gb=1)</p><p class="callout-heading">Important Note</p><p class="callout">You can find more information about <a id="_idIndexMarker1619"/>Azure Container Instance in the official documentation at <a href="https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview">https://docs.microsoft.com/en-us/azure/container-instances/container-instances-overview</a>.</p></li>
				<li>Next, we<a id="_idIndexMarker1620"/> pass the<a id="_idIndexMarker1621"/> environment and scoring file to the inferencing configuration:<p class="source-code">from azureml.core.model import InferenceConfig</p><p class="source-code">env = get_env(name="sentiment-analysis",</p><p class="source-code">              package=["tensorflow", "transformers"])</p><p class="source-code">inference_config = InferenceConfig(</p><p class="source-code">  environment=env,</p><p class="source-code">  source_directory="code",</p><p class="source-code">  entry_script="scoring_file.py",</p><p class="source-code">)</p></li>
				<li>Having all the required components, we can finally pass the model, the inferencing configuration, and the deployment configuration to the <strong class="source-inline">Model.deploy</strong> method and start the deployment:<p class="source-code">service_name = "sentiment-analysis"</p><p class="source-code">service = Model.deploy(ws,</p><p class="source-code">  name=service_name,</p><p class="source-code">  models=[model],</p><p class="source-code">  inference_config=inference_config,</p><p class="source-code">  deployment_config=deploy_config)</p><p class="source-code">service.wait_for_deployment(show_output=True)</p><p class="source-code">print(service.state)</p></li>
				<li>Once the <a id="_idIndexMarker1622"/>service is up <a id="_idIndexMarker1623"/>and running, we can try a test request to the service to make sure everything is working properly. By default, Azure Machine Learning services use key-based (primary and secondary) authentication. Let's retrieve the key from the service and send some test data to the deployed service:<p class="source-code">import requests</p><p class="source-code">import json</p><p class="source-code">from azureml.core import Webservice</p><p class="source-code">service = Webservice(ws, name="sentiment-analysis")</p><p class="source-code">scoring_uri = service.scoring_uri</p><p class="source-code"># If the service is authenticated</p><p class="source-code">key, _ = service.get_keys()</p><p class="source-code"># Set the appropriate headers</p><p class="source-code">headers = {"Content-Type": "application/json"}</p><p class="source-code">headers["Authorization"] = f"Bearer {key}"</p><p class="source-code">data = {"query": "AzureML is quite good."}</p><p class="source-code">resp = requests.post(scoring_uri,</p><p class="source-code">                     data=json.dumps(data),</p><p class="source-code">                     headers=headers)</p><p class="source-code">print(resp.text)</p></li>
			</ol>
			<p>The preceding snippet fetches the service URL and access key and sends the JSON <a id="_idIndexMarker1624"/>encoded <a id="_idIndexMarker1625"/>data to the ML model deployment as a <strong class="source-inline">POST</strong> request.</p>
			<p>That's it! You have deployed your sentiment analysis model successfully and tested it from Python. However, using the service endpoint and token, you can also send requests from any other programming language or HTTP client to your service.</p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor227"/>Deploying to Azure Kubernetes Services</h2>
			<p>We have<a id="_idIndexMarker1626"/> successfully deployed our sentiment analysis <a id="_idIndexMarker1627"/>model to ACI. As a next step, however, we want to deploy it to AKS. While ACI is fantastic for quickly getting Docker containers deployed, AKS is a service for complex container-based production workloads. Among other features, AKS supports authentication, autoscaling, GPU support, replicas, and advanced metrics and logging.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find more information about <a id="_idIndexMarker1628"/>Azure Kubernetes Services in the official documentation at <a href="https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes">https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes</a>.</p>
			<p>Let's now deploy this service to an AKS cluster so we can take advantage of the GPU acceleration and autoscaling:</p>
			<ol>
				<li value="1">First, we need to define our required infrastructure:<p class="source-code">from azureml.core.compute import AksCompute, \</p><p class="source-code">  ComputeTarget</p><p class="source-code"># Configure AKS cluster with NVIDIA Tesla P40 GPU</p><p class="source-code">prov_config = AksCompute.provisioning_configuration(</p><p class="source-code">  vm_size="Standard_ND6s")</p><p class="source-code">aks_name = 'aks-ml-prod'</p><p class="source-code"># Create the cluster</p><p class="source-code">aks_target = ComputeTarget.create(ws,</p><p class="source-code">  name=aks_name,</p><p class="source-code">  provisioning_configuration=prov_config)</p><p class="source-code"># Wait for the create process to complete</p><p class="source-code">aks_target.wait_for_completion(show_output=True)</p></li>
			</ol>
			<p>In the preceding code, we created an AKS configuration and a new AKS cluster as an Azure Machine Learning compute target from this configuration. All this happens completely within your authoring environment.</p>
			<ol>
				<li value="2">If you <a id="_idIndexMarker1629"/>already have an AKS cluster <a id="_idIndexMarker1630"/>up and running, you can simply use this cluster for Azure Machine Learning. To do so, you have to pass the resource group and cluster name to the <strong class="source-inline">AksCompute.attach_configuration()</strong> method. Then, set the resource group that contains the AKS cluster and the cluster name:<p class="source-code">resource_group = 'my-rg'</p><p class="source-code">cluster_name = 'aks-ml-prod'</p><p class="source-code">attach_config = <strong class="bold">AksCompute.attach_configuration(</strong></p><p class="source-code">  resource_group = resource_group,</p><p class="source-code">  cluster_name=cluster_name)</p><p class="source-code">aks_target = ComputeTarget.attach(ws,</p><p class="source-code">  cluster_name,</p><p class="source-code">  attach_config)</p></li>
				<li>Once we <a id="_idIndexMarker1631"/>have a reference to the cluster, we <a id="_idIndexMarker1632"/>can deploy the ML model to the cluster. This step is similar to the previous one:<p class="source-code">deploy_config = <strong class="bold">AksWebservice.deploy_configuration(</strong></p><p class="source-code">  cpu_cores=1,</p><p class="source-code">  memory_gb=1,</p><p class="source-code">  gpu_cores=1)</p><p class="source-code">service = Model.deploy(ws,</p><p class="source-code">  service_name,</p><p class="source-code">  [model],</p><p class="source-code">  inference_config,</p><p class="source-code">  deploy_config,</p><p class="source-code">  aks_target)</p><p class="source-code">service.wait_for_deployment(show_output=True)</p><p class="source-code">print(service.state)</p><p class="source-code">print(service.get_logs())</p></li>
			</ol>
			<p>As you can see in the <a id="_idIndexMarker1633"/>preceding example, apart from <a id="_idIndexMarker1634"/>attaching the AKS clusters as a target to Azure Machine Learning, the model deployment is identical to the example using ACI.</p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor228"/>Defining a schema for scoring endpoints</h2>
			<p>In the previous<a id="_idIndexMarker1635"/> example, we parse the user input from<a id="_idIndexMarker1636"/> JSON and expect it to contain a <strong class="source-inline">query</strong> parameter. To help users and services consuming your service endpoint, it would be useful to tell users which parameters the service is expecting. This is a common problem when building web service APIs.</p>
			<p>To solve this, Azure Machine Learning provides <a id="_idIndexMarker1637"/>an innovative way to autogenerate an <strong class="bold">OpenAPI Specification</strong> (<strong class="bold">OAS</strong>), previously called the <strong class="bold">Swagger Specification</strong>. This <a id="_idIndexMarker1638"/>specification can be accessed by consumers of the API through the schema endpoint. This provides an automated standardized way to specify and consume the service's data format and can be used to autogenerate clients. One example <a id="_idIndexMarker1639"/>is <strong class="bold">Swagger Codegen</strong>, which can be used to generate Java and C# clients for your new ML service.</p>
			<p>You can enable automatic schema generation for pandas, NumPy, PySpark, and standard Python objects in your service through annotations in Python. First, you need to include <strong class="source-inline">azureml-defaults</strong> and <strong class="source-inline">inference-schema</strong> as PIP packages in your environment. Then, you can autogenerate the schema by providing sample input and output data for your endpoint, as shown in the following example:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">scoring_file.py</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">input_sample = np.array([[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]])</p>
			<p class="source-code">output_sample = np.array([3726.995])</p>
			<p class="source-code"><strong class="bold">@input_schema('data', NumpyParameterType(input_sample))</strong></p>
			<p class="source-code"><strong class="bold">@output_schema(NumpyParameterType(output_sample))</strong></p>
			<p class="source-code">def run(data):</p>
			<p class="source-code">  # data is a np.array</p>
			<p class="source-code">  pass</p>
			<p>In the preceding<a id="_idIndexMarker1640"/> example, we defined the schema for a<a id="_idIndexMarker1641"/> NumPy-based model through sample data and annotations in the <strong class="source-inline">run()</strong> method.</p>
			<p>We can also pick up the sentiment analysis model and allow it to receive multiple input queries. To do this, we can deserialize the user input into a pandas DataFrame object and return an array of predictions as a result, as shown in the following example. Note that this basically adds batch prediction capabilities to our real-time web service:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">scoring_file.py</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">input_sample = pd.DataFrame(data=[</p>
			<p class="source-code">  {'query": "AzureML is quite good."}])</p>
			<p class="source-code">output_sample = np.array([np.array(["POSITIVE", 0.95])])</p>
			<p class="source-code"><strong class="bold">@input_schema('data', PandasParameterType(input_sample))</strong></p>
			<p class="source-code"><strong class="bold">@output_schema(NumpyParameterType(output_sample))</strong></p>
			<p class="source-code">def run(data):</p>
			<p class="source-code">  # data is a pd.DataFrame</p>
			<p class="source-code">  pass</p>
			<p>Defining <a id="_idIndexMarker1642"/>example inputs and outputs is everything that is required to autogenerate an API specification that your clients can use to validate <a id="_idIndexMarker1643"/>endpoints and arguments or to autogenerate clients. This is also the same format that can be used to create ML services that can be automatically integrated into Power BI, as shown in <a href="B17928_15_ePub.xhtml#_idTextAnchor238"><em class="italic">Chapter 15</em></a>,<em class="italic"> Model Interoperability, Hardware Optimization, and Integrations</em>.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor229"/>Managing model endpoints</h2>
			<p>Each model<a id="_idIndexMarker1644"/> deployment contains a URL to send requests to the model; online scoring services provide a URL to process online predictions, and batch-scoring services provide a URL to trigger batch predictions. While this makes it easy to spin up and query a service, one big problem remains during a deployment, namely, that the service URL changes with each deployment. This leads to the issue that we can't control which service a user request will hit.</p>
			<p>To solve this problem, we need to hide model deployment URLs behind a fixed service URL and provide a mechanism to resolve a user request to a specific service. In Azure Machine Learning, the component that fulfills this is called <a id="_idIndexMarker1645"/>an <strong class="bold">endpoint</strong>, which can expose multiple deployments under a fixed endpoint URL.</p>
			<p>The following figure shows the concept of endpoints and deployments. Customers send requests to the endpoint, and we configure the endpoint to route the request to one of the services. During a deployment, we would add the new model version behind the same scoring endpoint, and incrementally start service requests from the new <strong class="bold">(green)</strong> version instead of the previous <strong class="bold">(blue)</strong> version:</p>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/B17928_14_001.jpg" alt="Figure 14.1 – Azure Machine Learning endpoints and deployments " width="628" height="236"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Azure Machine Learning endpoints and deployments</p>
			<p>This type of <a id="_idIndexMarker1646"/>deployment is also called blue-green deployment. First, you serve all traffic from the old service and start the new service. Once the new service is up and running, and the health checks have finished successfully, the service is registered under the endpoint, and it will start serving requests. Finally, if there are no active requests left on the old service, you can shut it down.</p>
			<p>This process is a very safe way to update stateless application services with zero or minimal downtime. It also helps you to fall back on the old service if the new one doesn't deploy successfully.</p>
			<p>Azure Machine Learning provides multiple types of endpoints, depending on the model deployment mechanism:</p>
			<ul>
				<li><strong class="bold">Online endpoints</strong>: For real-time <a id="_idIndexMarker1647"/>online deployments:<ul><li><strong class="bold">Managed online endpoints</strong>: For<a id="_idIndexMarker1648"/> managed Azure Machine Learning deployments</li><li><strong class="bold">Kubernetes online endpoints</strong>: For<a id="_idIndexMarker1649"/> managed AKS deployments</li></ul></li>
				<li><strong class="bold">Batch endpoints</strong>: For <a id="_idIndexMarker1650"/>batch-scoring deployments</li>
			</ul>
			<p>On the top level, we distinguish between online and batch endpoints. While online endpoints are used for synchronous scoring based on web service deployments, batch endpoints are used for asynchronous scoring based on pipeline deployments.</p>
			<p>For online endpoints, we distinguish based on the deployment target between managed and Kubernetes-based online endpoints. This is an analog to the different compute targets and features for online scoring.</p>
			<p>Let's take a look at how to<a id="_idIndexMarker1651"/> configure<a id="_idIndexMarker1652"/> endpoints for AKS: </p>
			<ol>
				<li value="1">First, we configure the endpoint details as shown in the following snippet:<p class="source-code">from azureml.core.webservice import AksEndpoint</p><p class="source-code">endpoint_config = <strong class="bold">AksEndpoint.deploy_configuration</strong>(</p><p class="source-code">  version_name="version-1",</p><p class="source-code">  tag'={'modelVersion':'1'}, </p><p class="source-code">  namespace="nlp", </p><p class="source-code">  traffic_percentile=100)</p></li>
			</ol>
			<p>The endpoint configuration serves as a deployment configuration for the AKS compute target. </p>
			<ol>
				<li value="2">Next, we provide both the endpoint configuration and the compute target to the <strong class="source-inline">Model.deploy</strong> method:<p class="source-code">endpoint_name"= "sentiment-analysis"</p><p class="source-code">endpoint = Model.deploy(ws,</p><p class="source-code">  endpoint_name,</p><p class="source-code">  [model],</p><p class="source-code">  inference_config,</p><p class="source-code">  <strong class="bold">endpoint_config</strong>,</p><p class="source-code">  <strong class="bold">aks_target</strong>)</p><p class="source-code">endpoint.wait_for_deployment(show_output=True)</p><p class="source-code">print(endpoint.state)</p></li>
			</ol>
			<p>The deployment <a id="_idIndexMarker1653"/>will return an endpoint that can now be used to connect to the<a id="_idIndexMarker1654"/> service and add additional configuration. In the next section, we will look at more use cases of endpoints and will see how to add additional deployments to the AKS endpoint.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor230"/>Controlled rollouts and A/B testing</h2>
			<p>Another benefit of endpoints is to perform controlled rollouts<a id="_idIndexMarker1655"/> and incremental testing of new model versions. ML model deployments are similar to deployments of new features in application development. We might not want to roll out this new feature to all users at once, but first, test whether the new feature improves our business metrics for a small group of users.</p>
			<p>New ML model deployments should never be uncontrolled or based on personal feelings or preferences; a deployment should always be based on hard metrics and real evidence. The best and most systematic way to test and roll out changes to your users is to define a key metric, roll out your new model to one section of the users (group B), and serve the old model to the remaining section of the users (group A). Once the metrics for the users in group B exceed the metrics from group A over a defined period, you can confidently roll out the feature to all your users.</p>
			<p>This concept is called <strong class="bold">A/B testing</strong> and<a id="_idIndexMarker1656"/> is used in many tech companies to roll out new services and features. As you can see in the following diagram, you split your traffic into a control group and a challenger group, where only the latter is served the new model:</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/B17928_14_002.jpg" alt="Figure 14.2 – A/B testing using endpoints " width="830" height="197"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.2 – A/B testing using endpoints</p>
			<p>A/B testing and blue-green deployments<a id="_idIndexMarker1657"/> work very well together, as they are really similar approaches. Both require the deployment of a fully functional service that is accessible to a subset of your users through routing policies. If you use Azure Machine Learning for your deployment and rollout strategy, you are very well covered. First, all deployments through Azure Machine Learning to ACI or AKS are blue-green deployments, which makes it easy for you to fall back on a previous version of your model.</p>
			<p>Azure Machine Learning deployments on AKS support up to six model versions behind the same endpoint to implement either blue-green deployments or A/B testing strategies. You can then define policies to split the traffic between these endpoints; for example, you can split traffic by percentage. Here is a small code example of how to create another version on an <a id="_idIndexMarker1658"/>AKS endpoint that should serve another version of your model to 50% of the users:</p>
			<ol>
				<li value="1">Let's first update the original deployment to serve as the control version and serve 50% of the traffic:<p class="source-code">endpoint.<strong class="bold">update_version</strong>(</p><p class="source-code">  version_name="version-1",</p><p class="source-code">  traffic_percentile=50,</p><p class="source-code">  is_default=True,</p><p class="source-code">  is_control_version_type=True)</p></li>
				<li>Next, we add the challenger version, which is a deployment of <strong class="source-inline">test_model</strong>. As you can see in the following snippet, you can also supply a different inference configuration to the new deployment:<p class="source-code">endpoint.<strong class="bold">create_version</strong>(</p><p class="source-code">  version_name="version-2",</p><p class="source-code">  inference_config=inference_config,</p><p class="source-code">  models=[<strong class="bold">test_model</strong>],</p><p class="source-code">  tags={'modelVersion':'2'},</p><p class="source-code">  description="my second version",</p><p class="source-code">  traffic_percentile=50)</p></li>
				<li>Finally, we start the deployment of the updated endpoints:<p class="source-code">endpoint.wait_for_deployment(show_output=True)</p><p class="source-code">print(endpoint.state)</p></li>
			</ol>
			<p>In the preceding code, we show the preview feature of controlled rollouts for Azure Machine Learning and AKS. We use a different combination of model and inference configuration to deploy a separate service under the same endpoint. The traffic splitting now happens automatically through routing in Kubernetes. However, in order to align with a previous section of this chapter, we can expect this functionality to improve in the future as it gets used by many customers when rolling out ML models.</p>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor231"/>Implementing a batch-scoring pipeline</h2>
			<p>Operating batch-scoring services is very similar to the previously discussed online-scoring approach; you <a id="_idIndexMarker1659"/>provide an environment, compute target, and scoring script. However, in your scoring file, you would rather pass a path to a Blob storage location with a new batch of data instead of the data itself. You can then use your scoring function to process the data asynchronously and output the predictions to a different storage location, back to the Blob storage, or push the data asynchronously to the calling service.</p>
			<p>It is up to you how you implement your scoring file, as it is simply a Python script that you control. The only difference in the deployment process is that the batch-scoring script will be deployed as a computation on an Azure Machine Learning cluster, scheduled periodically through a pipeline, or triggered through a REST service. Therefore, it is important that your scoring script can be configured through command-line parameters. Remember that what makes batch scoring different is that we don't send the data to the scoring script, but instead, we send a path to the data and a path to write the output asynchronously.</p>
			<p>A batch-scoring script is typically wrapped in a pipeline step, deployed as a pipeline, and triggered from a REST service or batch-scoring endpoint. The pipeline can be configured to use an Azure Machine Learning cluster for execution. In this section, we will reuse all of the concepts we have previously seen in <a href="B17928_08_ePub.xhtml#_idTextAnchor135"><em class="italic">Chapter 8</em></a>, <em class="italic">Azure Machine Learning Pipelines</em>, and apply them to a batch-scoring pipeline step. Let's build a batch-scoring pipeline that scores images using the Inception v3 <strong class="bold">DNN</strong> model:</p>
			<ol>
				<li value="1">First, we <a id="_idIndexMarker1660"/>define a configurable batch size. In both the pipeline configuration and the scoring file, you can take advantage of parallelizing your work in the Azure Machine Learning cluster:<p class="source-code">from azureml.pipeline.core.graph import \</p><p class="source-code">  PipelineParameter</p><p class="source-code">batch_size_param = <strong class="bold">PipelineParameter</strong>(</p><p class="source-code">  name="param_batch_size",</p><p class="source-code">  default_value=20)</p></li>
				<li>Next, we define a pipeline step that will call the batch-scoring script:<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p><p class="source-code">batch_score_step = <strong class="bold">PythonScriptStep</strong>(</p><p class="source-code">  name="batch_scoring",</p><p class="source-code">  script_name="batch_scoring.py",</p><p class="source-code">  arguments=[</p><p class="source-code">   "--dataset_path", input_images,</p><p class="source-code">   "--model_name", "inception",</p><p class="source-code">   "--label_dir", label_dir,</p><p class="source-code">   "--output_dir", output_dir,</p><p class="source-code">   "--batch_size", batch_size_param],</p><p class="source-code">  compute_target=compute_target,</p><p class="source-code">  inputs=[input_images, label_dir],</p><p class="source-code">  outputs=[output_dir],</p><p class="source-code">  runconfig=amlcompute_run_config)</p></li>
				<li>Finally, we <a id="_idIndexMarker1661"/>wrap the pipeline step in a pipeline. To test the batch-processing step, we submit the pipeline as an experiment to the Azure Machine Learning workspace:<p class="source-code">from azureml.core import Experiment</p><p class="source-code">from azureml.pipeline.core import Pipeline</p><p class="source-code"><strong class="bold">pipeline = Pipeline(ws, steps=[batch_score_step])</strong></p><p class="source-code">exp = Experiment(ws, 'batch_scoring')</p><p class="source-code">pipeline_run = exp.submit(pipeline,</p><p class="source-code">  pipeline_params={"param_batch_size": 20})</p></li>
				<li>Using this pipeline configuration, we call our scoring script with the relevant parameters. The pipeline is submitted as an experiment in Azure Machine Learning, which gives us access to all the features in runs and experiments in Azure. One feature would be that we can simply download the output from the experiment when it has finished running:<p class="source-code">pipeline_run.wait_for_completion(show_output=True)</p><p class="source-code">step_run = list(pipeline_run.get_children())[0]</p><p class="source-code">step_run.download_file("./outputs/result-labels.txt")</p></li>
				<li>If the batch-scoring file produces a nice CSV output containing names and predictions, we <a id="_idIndexMarker1662"/>can now display the results using the following pandas functionality:<p class="source-code">import pandas as pd</p><p class="source-code">df = pd.read_csv(</p><p class="source-code">  "./outputs/result-labels.txt",</p><p class="source-code">  delimiter=":",</p><p class="source-code">  header=None)</p><p class="source-code">df.columns = ["Filename", "Prediction"]</p><p class="source-code">df.head()</p></li>
				<li>Let's go ahead and publish the pipeline as a REST service:<p class="source-code">published_pipeline = pipeline_run.publish_pipeline(</p><p class="source-code">  name="Inception_v3_scoring",</p><p class="source-code">  description="Batch scoring using Inception v3",</p><p class="source-code">  version="1.0")</p><p class="source-code">published_id = published_pipeline.id</p><p class="source-code">rest_endpoint = published_pipeline.endpoint</p></li>
				<li>To run the published pipeline as a service through HTTP, we now need to use token-based authentication:<p class="source-code">from azureml.core.authentication import \</p><p class="source-code">  AzureCliAuthentication</p><p class="source-code">cli_auth = AzureCliAuthentication()</p><p class="source-code">aad_token = cli_auth.get_authentication_header()</p></li>
				<li>Having retrieved<a id="_idIndexMarker1663"/> the authentication token, we can now run the published pipeline:<p class="source-code">import requests</p><p class="source-code"># Specify batch size when running the pipeline</p><p class="source-code">response = requests.post(</p><p class="source-code">  rest_endpoint,</p><p class="source-code">  headers=aad_token,</p><p class="source-code">  json={</p><p class="source-code">   "ExperimentName": "batch_scoring",</p><p class="source-code">   "ParameterAssignments": {</p><p class="source-code">     "param_batch_size": 50</p><p class="source-code">    }</p><p class="source-code">  })</p><p class="source-code">run_id = response.json()["Id"]</p></li>
			</ol>
			<p>That's it! You can now<a id="_idIndexMarker1664"/> trigger your batch-scoring pipeline using the REST endpoint. The data will be processed, and the results will be provided in a file that can be consumed programmatically or piped into the next pipeline step for further processing.</p>
			<p>Running a batch-scoring pipeline on an Azure Machine Learning service is a bit different from running a synchronous scoring service. While the real-time scoring service uses Azure Machine Learning deployments and AKS or ACI as popular compute targets, batch-scoring models are usually deployed as published pipelines on top of AmlCompute. The benefit of a published pipeline is that it can be used as a REST service, which can trigger and parameterize the pipeline.</p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor232"/>ML operations in Azure</h1>
			<p>You successfully registered a<a id="_idIndexMarker1665"/> trained model, an environment, a scoring file, and<a id="_idIndexMarker1666"/> an inference configuration in the previous section. You optimized your model for scoring and deployed it to a managed Kubernetes cluster. You autogenerated client SDKs for your ML services. So, can you finally lean back and enjoy the success of your hard work? Well, not yet! First, we need to make sure that we have all our monitoring in place so that we can observe and react to anything happening to our deployment.</p>
			<p>First, the good points: with Azure Machine Learning deployments and managed compute targets, you will get many things included out of the box with either Azure, Azure Machine Learning, or your service used as a compute target. Tools such as the <strong class="bold">Azure Dashboard</strong> on the Azure Portal, <strong class="bold">Azure Monitor</strong>, and <strong class="bold">Azure Log Analytics</strong> make it easy to centralize <a id="_idIndexMarker1667"/>log and debug information. Once your data is available through Log Analytics, it <a id="_idIndexMarker1668"/>can be queried, analyzed, visualized, alerted, and/or used for<a id="_idIndexMarker1669"/> automation using Azure Automation. A great deployment and operations process should utilize these tools integrated with Azure and the Azure services.</p>
			<p>The first thing that should come to mind when operating any application is measuring software and hardware metrics. It's essential to know the memory consumption, CPU usage, I/O latency, and network bandwidth of your application. Particularly for an ML service, you should always have an eye on performance bottlenecks and resource utilization for cost optimization. For large GPU-accelerated DNNs, it is essential to know your system in order to scale efficiently. These metrics allow you to scale your infrastructure vertically, and so move to bigger or smaller nodes when needed.</p>
			<p>Another <a id="_idIndexMarker1670"/>monitoring target for general application deployments should be<a id="_idIndexMarker1671"/> your users' telemetry data (how they are using your service, how often they use it, and which parts of the service they use). This will help you to scale horizontally and add more nodes or remove nodes when needed.</p>
			<p>The final important portion to measure from your scoring service, if possible, is the user input over time and the scoring results. For optimal prediction performance, it is essential to understand what type of data users are sending to your service, and how similar this data is to the training data. It's relatively certain that your model will require retraining at some point, and monitoring the input data will help you to define a time that this is required (for example, through a data drift metric).</p>
			<p>Let's take a look at how we can monitor the Azure Machine Learning deployments and keep track of all these metrics in Azure.</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor233"/>Profiling models for optimal resource configuration</h2>
			<p>Azure Machine <a id="_idIndexMarker1672"/>Learning provides a handy<a id="_idIndexMarker1673"/> tool to help you evaluate the required resources for your ML model deployment through model profiling. This will help you estimate the number of CPUs and the amount of memory required to operate your scoring service at a specific throughput.</p>
			<p>Let's take a look at the model profile of the model that we trained during the real-time scoring example:</p>
			<ol>
				<li value="1">First, you need to define <strong class="source-inline">test_data</strong> in the same format as the JSON request for your ML service; so, have <strong class="source-inline">test_data</strong> embedded in a JSON object under the <strong class="source-inline">data</strong> root property. Please note that if you defined a different format in your scoring file, then you need to use your own custom format:<p class="source-code">import json</p><p class="source-code">test_data = json.dump'({'data': [</p><p class="source-code">    [1,2,3,4,5,6,7,8,9,10]</p><p class="source-code">]})</p></li>
				<li>Then, you can use the <strong class="source-inline">Model.profile()</strong> method to profile a model and evaluate the CPU and memory consumption of the service. This will start up your model, fire requests with <strong class="source-inline">test_data</strong> provided to it, and measure the resource utilization at the same time:<p class="source-code">profile = Model.profile(ws,</p><p class="source-code">  service_name,</p><p class="source-code">  [model],</p><p class="source-code">  inference_config,</p><p class="source-code">  test_data)</p><p class="source-code">profile.wait_for_profiling(True)</p><p class="source-code">print(profile.get_results())</p></li>
				<li>The output<a id="_idIndexMarker1674"/> contains a list of resources, plus<a id="_idIndexMarker1675"/> a recommended value for the profiled model, as shown in the following snippet:<p class="source-code">{'cpu': 1.0, 'memoryInGB': 0.5}</p></li>
			</ol>
			<p>It is good to run the model profiling tool before doing a production deployment, and this will help you set meaningful default values for your resource configuration. To further optimize and decide whether you need to scale up or down, vertically or horizontally, you need to measure, track, and observe various other metrics. We will discuss monitoring and scaling more in the last section of this chapter.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor234"/>Collecting logs and infrastructure metrics</h2>
			<p>If you are new to cloud services, or Azure specifically, log and metric collection can be a bit overwhelming at first. Logs and metrics are generated in different layers in your application and can be either infrastructure- or application-based and collected automatically or manually. Then, there are diagnostic metrics that are emitted automatically but need to be enabled manually. In this section, we will briefly discuss how to collect this metric for the <a id="_idIndexMarker1676"/>three main managed compute targets in the Azure Machine Learning service: ACI, AKS, and AmlCompute.</p>
			<p>By default, you will get access to <a id="_idIndexMarker1677"/>infrastructure metrics and logs through Azure Monitor. It will automatically collect Azure resources and guest OS metrics and logs, and provide metrics and query interfaces for logs based on Log Analytics. Azure Monitor should be used to track resource utilization (for example, CPU, RAM, disk space, disk I/O, and network bandwidth), which then can be pinned to dashboards or alerted on. You can even set up automatic autoscaling based on these metrics.</p>
			<p>Metrics are mostly collected as distributions over time and reported back at certain time intervals. So, instead of seeing thousands of values per second, you are asked to choose an aggregate for each metric, for example, the average of each interval. For most monitoring cases, I would recommend you either look at the 95th percentile (or maximum aggregation, for metrics where lower is better) to avoid smoothing any spikes during the aggregation process. In AKS, you are provided with four different views of your metrics through Azure Monitor: clusters, nodes, controllers, and containers.</p>
			<p>More detailed resource, guest, and virtualization host logs of your Azure Machine Learning deployment can be accessed by enabling diagnostic settings and providing a separate Log Analytics instance. This will automatically load the log data into your Log Analytics workspace, where you can efficiently query all your logs, analyze them, and create visualization and/or alerts.</p>
			<p>It is strongly recommended to take advantage of the diagnostic settings, as they give you insights into your Azure infrastructure. This is especially helpful when you need to debug problems in your ML service (for example, failing containers, non-starting services, crashes, application freezes, and slow response times). Another great use case for Log Analytics is to collect, store, and analyze your application log. In AKS, you can send the Kubernetes master node logs, <em class="italic">kubelet</em> logs, and API server logs to Log Analytics.</p>
			<p>One metric that is very <a id="_idIndexMarker1678"/>important to track for ML training clusters and<a id="_idIndexMarker1679"/> deployments, but is unfortunately not tracked automatically, is the GPU resource utilization. Due to this problem, GPU resource utilization has to be monitored and collected at the application level.</p>
			<p>The most effective way to solve this for AKS deployments is to run a GPU logger service as a sidecar with your application, which collects resource statistics and sends them to <strong class="bold">Application Insights</strong> (<strong class="bold">App Insights</strong>), a <a id="_idIndexMarker1680"/>service that collects application metrics. Both App Insights and Log Analytics use the same data storage technology under the hood: Azure Data Explorer. However, default integrations for App Insights provide mainly application metrics such as access logs, while Log Analytics provides system logs.</p>
			<p>In AmlCompute, we need to start a separate monitoring thread from your application code to monitor GPU utilization. Then, for Nvidia GPUs, we use a wrapper around the <strong class="source-inline">nvidia-smi</strong> monitoring utility, for example, the <strong class="source-inline">nvidia-ml-py3</strong> Python package. To send data to App Insights, we simply use the Azure SDK for App Insights. Here is a tiny code example showing you how to achieve this:</p>
			<p class="source-code">from applicationinsights import TelemetryClient</p>
			<p class="source-code">import nvidia_smi</p>
			<p class="source-code">nvidia_smi.nvmlInit()</p>
			<p class="source-code"># Get handle for card id 0</p>
			<p class="source-code">dev_handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)</p>
			<p class="source-code">res = nvidia_smi.nvmlDeviceGetUtilizationRates(dev_handle)</p>
			<p class="source-code"># Submit GPU metrics to AppInsights</p>
			<p class="source-code">tc = TelemetryClient("&lt;insert appinsights key")</p>
			<p class="source-code">tc.track_metric("gpu", res.gpu)</p>
			<p class="source-code">tc.track_metric("gpu-gpu-mem", res.memory)</p>
			<p>In the preceding code, we first used the <strong class="source-inline">nvidia-ml-py3</strong> wrapper on top of <strong class="source-inline">nvidia-smi</strong> to return a handle<a id="_idIndexMarker1681"/> to the current GPU. Please note that when you have multiple GPUs, you can also iterate over them and report multiple metrics. Then, we used the <strong class="source-inline">TelemetryClient</strong> API from App Insights to report these metrics back to a central place, where we can then visualize, analyze, and alert on these values.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor235"/>Tracking telemetry and application metrics</h2>
			<p>We briefly touched on Azure App Insights in the previous section. It is a great service for automatically collecting application metrics from your services, for example, Azure Machine Learning deployments. It<a id="_idIndexMarker1682"/> also provides an SDK to collect any user-defined application metric that you want to track.</p>
			<p>To automatically track<a id="_idIndexMarker1683"/> user metrics, we need to deploy the model using Azure Machine Learning deployments to AKS or ACI. This will not only collect the web service metadata but also the model's predictions. To do so, you need to enable App Insights' diagnostics, as well as data model collection, or enable App Insights via the Python API:</p>
			<p class="source-code">from azureml.core.webservice import Webservice</p>
			<p class="source-code">aks_service= Webservice(ws, "aks-deployment")</p>
			<p class="source-code">aks_service.update(enable_app_insights=True)</p>
			<p>In the preceding snippet, we can activate App Insights' metrics directly from the Python authoring environment. While this is a simple argument in the service class, it gives you an incredible insight into the deployment.</p>
			<p>Two important metrics to measure are data drift coefficients for both training data and model predictions. We will learn more about this in the next section.</p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor236"/>Detecting data drift</h2>
			<p>One<a id="_idIndexMarker1684"/> important problem in ML is when to retrain your models. Should you always retrain when new training data is available, for example, daily, weekly, monthly or yearly? Do we need to retrain at all, or is the training data still relevant? Measuring <strong class="bold">data drift</strong> will help to answer these questions.</p>
			<p>By automatically tracking the user input and the model predictions, you can compare a statistical variation between the training data and the user input per feature dimension, as well as the training labels with the model prediction. The variation of the training data and actual data is what is referred to as data drift and should be tracked and monitored regularly. Data drift leads to model performance degradation over time, and so needs to be monitored. The best case is to set up monitoring and alerts to understand when your deployed model differs too much from the training data and so needs to be retrained.</p>
			<p>Azure Machine Learning provides useful abstractions to implement data drift monitors and alerts based on registered <strong class="bold">datasets</strong>, and can automatically expose data drift metrics in Application Insights. Computing the data drift requires two datasets: a baseline, which is usually the training dataset, and a target dataset, which is usually a dataset constructed from the inputs of the scoring service:</p>
			<ol>
				<li value="1">First, we define the target and baseline datasets. These datasets must contain a column that represents the date and time of each observation:<p class="source-code">from azureml.core import Workspace, Dataset</p><p class="source-code">from datetime import datetime</p><p class="source-code">ws = Workspace.from_config()</p><p class="source-code">ds_target = Dataset.get_by_name(ws, 'housing-data')</p><p class="source-code">ds_baseline = ds_target.time_before(</p><p class="source-code">  datetime(2022, 1, 1))</p></li>
				<li>Next, we <a id="_idIndexMarker1685"/>can set up email alerting for the monitor. This can be done in many different ways, but for the purpose of this example, we set up an email alert directly on the data drift monitor:<p class="source-code">from azureml.datadrift import AlertConfiguration</p><p class="source-code">alert_config = AlertConfiguration(</p><p class="source-code">  email_addresses=['&lt;insert email address&gt;'])</p></li>
				<li>Now, we can set up the data drift monitor providing all the previous details. We configure the monitor for three specific features <strong class="source-inline">['a', 'b', 'c']</strong>, to measure drift on a monthly cadence with a delay of 24 hours. An alert is created when the target dataset drifts more than 25% from the baseline data:<p class="source-code">from azureml.datadrift import DataDriftDetector</p><p class="source-code">monitor = DataDriftDetector.create_from_datasets(ws,</p><p class="source-code">  "data-drift-monitor",</p><p class="source-code">  ds_baseline,</p><p class="source-code">  ds_target,</p><p class="source-code">  compute_target=compute_target,</p><p class="source-code">  frequency='Month',</p><p class="source-code">  feature_list=['a', 'b', 'c'],</p><p class="source-code">  alert_config=alert_config,</p><p class="source-code">  drift_threshold=0.25,</p><p class="source-code">  latency=24)</p></li>
				<li>Finally, we can enable the monitor schedule to run periodically:<p class="source-code">monitor.enable_schedule() </p></li>
			</ol>
			<p>Data drift is an<a id="_idIndexMarker1686"/> essential operational metric to look at when operating ML deployments. Setting up monitors and alarms will help you get alerted early when the distribution of your data deviates too much from the training data and, therefore, requires you to retrain the model.</p>
			<h1 id="_idParaDest-238"><a id="_idTextAnchor237"/>Summary</h1>
			<p>In this chapter, we learned how to take a trained model and deploy it as a managed service in Azure through a few simple lines of code. To do so, we learned how to prepare a model for deployment and looked into Azure Machine Learning auto-deployments and customized deployments.</p>
			<p>We then took an NLP sentiment analysis model and deployed it as a real-time scoring service to ACI and AKS. We also learned how to define the service schema and how to roll out new versions effectively using endpoints and blue-green deployments. Finally, we learned how to integrate a model in a pipeline for asynchronous batch scoring.</p>
			<p>In the last section, we learned about monitoring and operating your models using Azure Machine Learning services. We proposed to monitor CPU, memory, and GPU metrics as well as telemetry data. We also learned how to measure the data drift of your service by collecting user input and model output over time. Detecting data drift is an important metric that allows you to know when a model needs to be retrained.</p>
			<p>In the next chapter, we will apply the learned knowledge and take a look at model interoperability, hardware optimization, and integration into other Azure services.</p>
		</div>
	</div>
</div>
</body></html>