["```py\n    from sklearn.ensemble import RandomForestClassifier\n    model = RandomForestClassifier (\n      max_depth, max_features, n_estimators\n    )\n    model.fit(train_X, train_y) \n    ```", "```py\n    from sklearn.metrics import accuracy_score\n    acc = accuracy_score (true_label, predicted_label) \n    ```", "```py\n    import joblib\n    joblib.dump(model, \"saved_model_name.joblib\") \n    ```", "```py\n    from sklearn.pipeline import Pipeline\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    pipe = Pipeline([('scaler', StandardScaler()), (RF, RandomForestClassifier())])\n    pipe.fit(X_train, y_train) \n    ```", "```py\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder.appName('appname').getOrCreate() \n    ```", "```py\n    dataFrame = spark.read.format('csv').load(file_path) \n    ```", "```py\n    from pyspark.ml.feature import StandardScaler\n    scaler = StandardScaler(inputCol=\"features\",  outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n    scalerModel = scaler.fit(dataFrame)\n    scaledData = scalerModel.transform(dataFrame) \n    ```", "```py\n    from pyspark.ml.classification import LogisticRegression\n    lr_algo = LogisticRegression(\n      maxIter regParam, elasticNetParam\n    )\n    lr_model = lr_algo.fit(dataFrame) \n    ```", "```py\n    From pyspark.ml.evaluation import BinaryClassificationEvaluator\n    dataset = spark.createDataFrame(scoreAndLabels, [\"raw\", \"label\"])\n    evaluator = BinaryClassificationEvaluator()\n    evaluator.setRawPredictionCol(\"raw\")\n    evaluator.evaluate(dataset)\n    evaluator.evaluate(dataset, {evaluator.metricName: \"areaUnderPR\"}) \n    ```", "```py\n    from pyspark.ml import Pipeline\n    from pyspark.ml.classification import LogisticRegression\n    from pyspark.ml.feature import HashingTF, Tokenizer\n    lr_tokenizer = Tokenizer(inputCol, outputCol)\n    lr_hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol)\n    lr_algo = LogisticRegression(maxIter, regParam)\n    lr_pipeline = Pipeline(stages=[lr_tokenizer, lr_hashingTF, lr_algo])\n    lr_model = lr_pipeline.fit(training) \n    ```", "```py\n    import mleap.pyspark\n    from pyspark.ml import Pipeline, PipelineModel\n    lr_model.serializeToBundle(\"saved_file_path\", lr_model.transform(dataframe)) \n    ```", "```py\n    ! pip3 install --upgrade tensorflow \n    ```", "```py\n    import numpy as np\n    import tensorflow as tf\n    train, test = tf.keras.datasets.fashion_mnist.load_data()\n    images, labels = train\n    labels = labels.astype(np.int32)\n    images = images/256  \n    train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n    train_ds = train_ds.batch(32) \n    ```", "```py\n    from matplotlib import pyplot as plt\n    print (\"label:\" + str(labels[0]))\n    pixels = images[0]\n    plt.imshow(pixels, cmap='gray')\n    plt.show() \n    ```", "```py\n    model = tf.keras.Sequential([\n       tf.keras.layers.Flatten(),\n       tf.keras.layers.Dense(100, activation=\"relu\"),\n       tf.keras.layers.Dense(50, activation=\"relu\"),\n       tf.keras.layers.Dense(10),\n       tf.keras.layers.Softmax()\n    ])\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n    model.fit(train_ds, epochs=10) \n    ```", "```py\n    images_test, labels_test = test\n    labels_test = labels_test.astype(np.int32)\n    images_test = images_test/256  \n\n    test_ds = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n    test_ds = train_ds.batch(32)\n    test_ds = train_ds.shuffle(30)\n    results = model.evaluate(test_ds)\n    print(\"test loss, test acc:\", results) \n    ```", "```py\n    predictions = model.predict(test[0])\n    predicted_labels = np.argmax(predictions, axis=1)\n    m = tf.keras.metrics.Accuracy()\n    m.update_state(predicted_labels, test[1])\n    m.result().numpy() \n    ```", "```py\n    model.save(\"my_model.keras\") \n    ```", "```py\n    !pip3 install torch\n    !pip3 install torchvision \n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import torch\n    from torchvision import datasets, transforms\n    from torch import nn, optim\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,),)])\n    trainset = datasets.MNIST('pytorch_data/train/', download=True, train=True, transform=transform)\n    valset = datasets.MNIST('pytorch_data/test/', download=True, train=False, transform=transform)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) \n    ```", "```py\n    model = nn.Sequential(nn.Linear(784, 128),\n                          nn.ReLU(),\n                          nn.Linear(128, 64),\n                          nn.ReLU(),\n                          nn.Linear(64, 10)) \n    ```", "```py\n    images, labels = next(iter(trainloader))\n    pixels = images[0][0]\n    plt.imshow(pixels, cmap='gray')\n    plt.show() \n    ```", "```py\n    criterion = nn.CrossEntropyLoss()\n    images = images.view(images.shape[0], -1)\n    output = model(images)\n    loss = criterion(output, labels)\n    optimizer = optim.Adam(model.parameters(), lr=0.003) \n    ```", "```py\n    epochs = 15\n    for e in range(epochs):\n        running_loss = 0\n    for images, labels in trainloader:\n            images = images.view(images.shape[0], -1)\n            optimizer.zero_grad()\n            output = model(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        else:\n            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader))) \n    ```", "```py\n    valloader = torch.utils.data.DataLoader(valset, batch_size=valset.data.shape[0], shuffle=True)\n    val_images, val_labels = next(iter(valloader))\n    val_images = val_images.view(val_images.shape[0], -1)\n    predictions = model (val_images)\n    predicted_labels = np.argmax(predictions.detach().numpy(), axis=1)\n    from sklearn.metrics import accuracy_score\n    accuracy_score(val_labels.detach().numpy(), predicted_labels) \n    ```", "```py\n    torch.save(model, './model/my_mnist_model.pt') \n    ```"]