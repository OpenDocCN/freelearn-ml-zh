- en: Statistical and Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The technical requirements for this chapter remain the same as those we detailed
    in [Chapter 1](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml), *Get Closer to Your
    Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Visit the GitHub repository to get the dataset and the code. These are arranged
    by chapter and by the name of the topic. For the linear regression dataset and
    code, for example, visit `.../Chapter 3/Linear regression`.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multiple linear regression is a technique used to train a linear model, that
    assumes that there are linear relationships between multiple predictor variables
    (![](img/a873ef1f-dd8a-4450-8afa-5e5c3c4b2dc5.png)) and a continuous target variable
    (![](img/9d832a44-5813-46fb-9d89-681fc4ed016a.png)). The general equation for
    a multiple linear regression with m predictor variables is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/088198d7-1233-4571-9fcc-61c6dba157ad.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/19a78dc6-d3ee-4492-b89e-6d42851d79b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training a linear regression model involves estimating the values of the coefficients
    for each of the predictor variables denoted by the letter ![](img/a343305a-25c0-4a07-8783-5045b92efe76.png).
    In the preceding equation, ![](img/9c4fefc7-e6fe-4dd2-a801-a379f072ae3c.png) denotes
    an error term, which is normally distributed, and has zero mean and constant variance.
    This is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8302e115-092f-45d0-b4be-8689554c67a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Various techniques can be used to build a linear regression model. The most
    frequently used is the **ordinary least square** (**OLS**) estimate. The OLS method
    is used to produce a linear regression line that seeks to minimize the sum of
    the squared error. The error is the distance from an actual data point to the
    regression line. The sum of the squared error measures the aggregate of the squared
    difference between the training instances, which are each of our data points,
    and the values predicted by the regression line. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20b55abb-3b52-41db-8c2e-e1d7b36c8d0c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/bcc37182-bc5c-4ce3-9f45-8e124cd79d81.png) is
    the actual training instance and ![](img/5d58def7-dceb-4c83-8add-15c9237d55b0.png)
    is the value predicted by the regression line.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of machine learning, gradient descent is a common technique that
    can be used to optimize the coefficients of predictor variables by minimizing
    the training error of the model through multiple iterations. Gradient descent
    starts by initializing the coefficients to zero. Then, the coefficients are updated
    with the intention of minimizing the error. Updating the coefficients is an iterative
    process and is performed until a minimum squared error is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: In the gradient descent technique, a hyperparameter called the **learning rate**,
    denoted
  prefs: []
  type: TYPE_NORMAL
- en: by ![](img/6ae71158-dd81-4087-b5ce-5d8e76ce02c4.png) is provided to the algorithm. This
    parameter determines how fast the algorithm moves toward the optimal value of
    the coefficients. If ![](img/556b7d9a-e0a6-4d3f-af81-cda64717e7ed.png) is very
    large, the algorithm might skip the optimal solution. If it is too small, however,
    the algorithm might have too many iterations to converge to the optimum coefficient
    values. For this reason, it is important to use the right value for ![](img/f2895db5-0f99-4a3e-801f-5b9594ecf991.png).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the gradient descent method to train our linear
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml), *Get Closer To
    Your Data*, we took the `HousePrices.csv` file and looked at how to manipulate
    and prepare our data. We also analyzed and treated the missing values in the dataset.
    We will now use this final dataset for our model-building exercise, using linear
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we will start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory with the `os.chdir()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our data. We prefix the DataFrame name with `df_` so that we can
    understand it easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's move on to building our model. We will start by identifying our numerical
    and categorical variables. We study the correlations using the correlation matrix
    and the correlation plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll take a look at the variables and the variable types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then look at the correlation matrix. The `corr()` method computes the
    pairwise correlation of columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides this, we''d also like to study the correlation between the predictor
    variables and the response variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We may also want to sort our correlation by absolute values. In order to do
    this, we can use the following command: `corr_with_target[abs(corr_with_target).argsort()[::-1]]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can look at the correlation plot using the `heatmap()` function from the
    `seaborn` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the correlation plot. Note that we have removed
    the upper triangle of the heatmap using the `np.zeros_like()` and `np.triu_indices_from()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b5aa08a-231d-4df4-b201-8b520478b0ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's explore our data by visualizing other variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can look at the distribution of our target variable, `SalePrice`, using
    a histogram with a kernel density estimator as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot gives us the distribution plot for the `SalePrice`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e333a93b-7cdc-4307-955a-02f3ee7132e4.png)'
  prefs: []
  type: TYPE_IMG
- en: In statistics, **kernel density estimation** (**KDE**) is a non-parametric way
    to estimate the probability density function of a random variable. Kernel density
    estimation is a fundamental data smoothing problem where inferences about the
    population are based on a finite data sample. KDE is a technique that provides
    you with a smooth curve given a set of data. It can be handy if you want to visualize
    the shape of some data, as a kind of continuous replacement for the discrete values
    plotted in a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use `JointGrid()` from our `seaborn` package to plot a combination
    of plots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we are able to plot the scatter plot for GarageArea
    and SalePrice, while also plotting the histogram for each of these variables on
    each axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d410aa20-1b45-465d-83a8-bcddeded8f74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now scale our numeric variables using min-max normalization. To do this,
    we first need to select only the numeric variables from our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now apply the min-max scaling to our numeric variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following table, we can see that our numeric variables have been scaled
    down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2ba2525-15ca-4bca-8972-48bff32dddfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will perform one-hot encoding on our categorical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now created a DataFrame with only numeric variables that have been
    scaled. We have also created a DataFrame with only categorical variables that
    have been encoded. Let''s combine the two DataFrames into a single DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then concatenate the `SalePrice` variable to our `df_housedata` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create our training and testing datasets using the `train_test_split`
    class from `sklearn.model_selection`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use `SGDRegressor()` to build a linear model. We fit this linear
    model by minimizing the regularized empirical loss with SGD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: By running the preceding code, we find out that the coefficient of determination
    is roughly 0.81.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `r2_score()` takes two arguments. The first argument should be the
    true values, not the predicted values, otherwise, it would return an incorrect
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'We check the **root mean square error** (**RMSE**) on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code provides output to the effect that the RMSE equals
    36459.44.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now plot the actual and predicted values using `matplotlib.pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot with our actual values and the predicted values will look
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11ce1f99-0212-467f-aa91-f014135dccc1.png)'
  prefs: []
  type: TYPE_IMG
- en: Because the chart shows most values in approximately a 45-degree diagonal line,
    our predicted values are quite close to the actual values, apart from a few.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we looked at the variable types. We saw that the dataset had both
    numeric and non-numeric variables. In *Step 2*, we used the `Pearson` method to
    calculate the pairwise correlation among all the numeric variables. After that,
    in *Step 3*, we saw how all of the predictor variables are related to the target
    variable. We also looked at how to sort correlation coefficients by their absolute
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 4*, we painted a heatmap to visualize the correlation between the
    variables. Then, we introduced two functions from the NumPy library: `zeros_like()`
    and `triu_indices_from()`. The `zeros_like()` function takes the correlation matrix
    as an input and returns an array of zeros with the same shape and type as the
    given array. `triu_indices_from()` returns the indices for the upper triangle
    of the array. We used these two functions to mask the upper triangular part of
    the correlation plot. We called the `heatmap()` function from the `seaborn` library
    to paint a correlation heat map and passed our correlation matrix to it. We also
    set the color of the matrix using `cmap="YlGnBu"` and the size of the legend bar
    using `cbar_kws={"shrink": 0.5}`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`numpy.tril_indices_from()` returns the indices for the lower triangle of the
    array.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Step 5*, we looked at the distribution of the target variable, `SalePrice`.
    In *Step 6*, we used `JointGrid()` from the `seaborn` library to show how it is
    possible to plot a scatter plot for two numeric variables with a regression line,
    along with plotting the distribution of both variables on the axis in the same
    chart. In *Steps 7* and *8*, we selected only the numeric variables and scaled
    the variables using min-max normalization. This scales the values to a numeric
    range of data between 0 and 1\. This is also called feature scaling, and is performed
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3749017-4989-4796-a144-a72591fca108.png)'
  prefs: []
  type: TYPE_IMG
- en: In *Step 9*, *Step **10*, and *Step **11*, we performed one-hot encoding on
    the categorical variables and added the encoded variables to the DataFrame. We
    also dropped the original categorical variables. In *Step 12*, we split our dataset
    into a training set and a testing set. In *Step 13*, we built our linear regression
    model using `SGDRegressor()` and printed the coefficient of determination. Finally,
    in *Step 14*, we plotted the predicted and actual values to see how well our model
    performed.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a linear regression model, given the following hypothesis function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a6e344d-3744-49ef-8abb-d5b3b90d5c73.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the cost function for ![](img/dbdd5107-ab3c-4439-9eb4-d37c9f398178.png) is
    the **mean squared error** (**MSE**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3437f7b5-298e-43d2-aadb-27b6cbd92af3.png)'
  prefs: []
  type: TYPE_IMG
- en: In this formula, ![](img/330bdebb-32ae-4643-bb20-9c1536e89174.png) represents
    the number of training instances. ![](img/cdaa24f4-3130-4e53-9433-6ed55b2a2c5e.png) and ![](img/6bbb6455-36e9-4530-8abd-4cf149a36eb3.png) are
    the input vector and the target vector for the i^(th) training instance respectively,
    while ![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) represents the parameters
    or coefficients for each input variable. ![](img/fde84455-2d8b-45cf-91cf-7e050c0b2754.png) is
    the predicted value for the i^(th) training instance using the  ![](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png) parameters.
    The MSE is always non-negative and the closer it gets to zero, the better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MSE is higher when the model performs poorly on the training data. The
    objective of the learning algorithm, therefore, is to find the value of **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **such
    that the MSE is minimized. This can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ce4ff82-022a-4132-853b-1f5ed9d6bc39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The stochastic gradient descent method finds the values of ![](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png) that
    minimize the cost function. In order to minimize the cost function, it keeps changing
    the ![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) parameters by calculating
    the slope of the derivative of the cost function. It starts by initializing the
    **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **parameters to zero. The
    **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **parameters are updated at
    each step of the gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0922d000-fa2c-4c1e-b2c0-ce4a5835d302.png)'
  prefs: []
  type: TYPE_IMG
- en: The number of updates required for the algorithm to converge will increase with
    the increase in the training data. However, as the training data gets larger and
    larger, it is quite possible for the algorithm to converge much before every instance
    in the training data is learnt. In other words, the increase in the training data
    size need not increase the training time needed to train the best possible model
    where the test error is at its least.
  prefs: []
  type: TYPE_NORMAL
- en: Every training instance will modify **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**.
    The algorithm averages these **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **values to
    calculate the final **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png).**
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf8d8412-f33e-49a7-a87f-4bd993db62d7.png) is the learning rate, which
    tells the algorithm how rapidly to move toward the minimum. A large ![](img/ad3709fe-d549-4f9c-8f87-c34446880063.png) might
    miss the minimum error, while a small ![](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png) might
    take a longer time for the algorithm to run.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding section, we used a `SGDRegressor()` function, but we opted
    for the default values of the hyperparameters. We are now going to change ![](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png) to
    0.0000001 and the `max_iter` value to 2000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`max_iter` is an integer value that tells the algorithm the maximum number
    of passes it can make over the training data. This is also known as the number
    of epochs.'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the preceding code gives the result that the RMSE drops from 36,459
    to 31,222 and the coefficient of determination improved from 0.81 to 0.86\. These
    results will vary for every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scikit-learn documentation on regression metrics: [https://bit.ly/2D6Wn8s](https://bit.ly/2D6Wn8s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The scikit-learn guide to density estimation: [https://bit.ly/2RlnlMj](https://bit.ly/2RlnlMj)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we noted that linear regression is a good choice when
    the target variable is continuous. We're now going to move on to look at a binomial
    logistic regression model, which can predict the probability that an observation
    falls into one of two categories of a dichotomous target variable based on one
    or more predictor variables. A binomial logistic regression is often referred
    to as logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is similar to linear regression, except that the dependent
    variable is measured on a dichotomous scale. Logistic regression allows us to
    model a relationship between multiple predictor variables and a dichotomous target
    variable. However, unlike linear regression, in the case of logistic regression,
    the linear function is used as an input to another function, such as ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c667dd69-61a9-421f-b5b8-f1f0d4b797e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png) is the sigmoid or logistic
    function. The sigmoid function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a62d322-fb87-4998-b8ae-ab4d8a51b87d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph represents a sigmoid curve in which the values of the y-axis
    lie between 0 and 1\. It crosses the axis at 0.5\. :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/309120d7-1ddd-42ee-a168-2a77cbdcc19f.png)'
  prefs: []
  type: TYPE_IMG
- en: The output, which lies between 0 and 1, is the probability of the positive class.
    We can interpret the output of our hypothesis function as positive if the value
    returned is ![](img/5ca126ce-5e50-4933-a3e3-d96ecc68495f.png) 0.5\. Otherwise,
    we interpret it as negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of logistic regression, we use a cost function known as cross-entropy.
    This takes the following form for binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d56c0b91-cce1-442c-a47c-855294fd4253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For *y=1* and *y=0*, we get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf567852-7a51-4677-b73c-589245f03199.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-entropy increases as the predicted probability diverges from the actual
    label. A higher divergence results in a higher cross-entropy value. In the case
    of linear regression, we saw that we can minimize the cost using gradient descent.
    In the case of logistic regression, we can also use gradient descent to update
    the coefficients and minimize the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the `SGDClassfier()` implementation of scikit-learn. `SGDClassifier()`
    implements regularized linear models with stochastic gradient descent, which,
    for large datasets, is much faster than gradient descent. This is because gradient
    descent considers the whole training dataset, while stochastic gradient descent only considers
    one random point while updating the weights.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `SGDClassifier` might not perform as well as logistic regression.
    It is likely to require hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to use a dataset that contains information on
    default payments, demographics, credit data, payment history, and bill statements
    of credit card clients in Taiwan from April 2005 to September 2005\. This dataset
    is taken from the UCI ML repository and is available at GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory with the `os.chdir()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our data. We will prefix the name of the DataFrame with `df_` to
    make it easier to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We will now move on to look at building our model using `SGDClassifier()`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s start by looking at the variables and data types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''re going to take a look at our dataset using the `read_csv()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will take a look at the datatypes using `dtypes`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will drop the ID column as we do not need this here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the previous section, we saw how to explore correlations among the variables.
    We will skip this here, but readers are advised to check for correlation as multicollinearity
    might have an impact on the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'However, we will check if there are any null values, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then separate the predictor and response variables. We will also split
    our training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We standardize our predictor variables using `StandardScaler()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We then move our model using `SGDClassifier()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate out the probabilities of one class. In this case, we will look
    at class 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the accuracy of our model on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then see the **area under curve** (**AUC**) value of the **receiver
    operating characteristic** (**ROC**) curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We plot our ROC curve as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the ROC curve with the AUC value annotated on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e8dc280-5e3d-4290-9b75-67691fa3be5e.png)'
  prefs: []
  type: TYPE_IMG
- en: The model can be improved by tuning the hyperparameters. It can also be improved
    through feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we looked at the dimensions of our dataset. In *Step 2*, we took
    a glimpse at the datatypes of the variables and noticed that all our variables
    were numeric in nature. In *Step 3*, we dropped the ID column since it is of no
    use for our exercise. We skipped looking at the correlations between the variables,
    but it is recommended that the reader adds this step in order to fully understand
    and analyze the data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we moved on to check whether we had any missing values in our dataset.
    We noticed that our dataset had no missing values in this case. In *Step 5*, we
    separated the predictor and response variable and also split our dataset into
    a training dataset, which was 70% of the data, and a testing dataset, which was
    30% of the data. In *Step 6,* we used `StandardScaler()` from `sklearn.metrics`
    to standardize our predictor variables in both the training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: After that, in *Step 7*, we used `SGDClassifier()` from `sklearn.linear_model`
    to build our logistic regression model using the stochastic gradient descent method.
    We set our hyperparameters, such as alpha, loss, `max_iter`, and penalty. We set
    `loss='log'` in order to use the SGDClassifier for logistic regression. We used `predict_proba()`
    to predict the probabilities for our test observations, which provided us with
    the probabilities of both classes for all the test observations.
  prefs: []
  type: TYPE_NORMAL
- en: With `loss` set to `hinge`, `SGDClassifier()` provides a linear SVM. (We will
    cover SVMs in the upcoming section). The loss can be set to other values, such
    as `squared_hinge`, which is the same as `hinge` but is quadratically penalized.
  prefs: []
  type: TYPE_NORMAL
- en: In *Steps 8* and *9*, we filtered out the probabilities for class 1 and looked
    at our model score. In *Steps 10* and *11*, we looked at the AUC value and plotted
    our ROC curve. We will explore more about hyperparameter tuning for each technique
    in upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might have noticed that, in *Step 7*, we used a hyperparameter penalty of
    `l2`. The penalty is the regularization term and `l2` is the default value. The
    hyperparameter penalty can also be set to `l1`; however, that may lead to a sparse
    solution, pushing most coefficients to zero. More information about this topic
    can be found at the following link: [https://bit.ly/2RjbSwM](https://bit.ly/2RjbSwM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn guide to classification metrics: [https://bit.ly/2NUJl2C](https://bit.ly/2NUJl2C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Naive Bayes algorithm** is a probabilistic learning method. It is known
    as **Naive** because it assumes that all events in this word are independent,
    which is actually quite rare. However, in spite of this assumption, the Naive
    Bayesian algorithm has proven over time to provide great performance in terms
    of its prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian probability theory is based on the principle that the estimated
    likelihood of an event or a potential outcome should be based on the evidence
    at hand across multiple trials. Bayes’ theorem provides a way to calculate the
    probability of a given class, given some knowledge about prior observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c581d7c-8c29-4748-ba3b-99eb7954db06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The different elements of this theorem can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**p(class|observation)**: This is the probability that the class holds given
    the observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**P(observation)**: This is the prior probability that the training data is
    observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(class)**: This is the prior probability of the class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p(observation|class)**: This is the probability of the observations given
    that the class holds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, if *H* is the space for the possible hypothesis, the most probable
    hypothesis, class![](img/0a15aa65-15d1-4624-9cfb-78a865c25282.png)H, is the one
    that maximizes *p(class|observation)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a new observation with attributes![](img/a99e07ac-c800-4c4d-b285-7a4dabea94b8.png),
    the Bayes algorithm classifies it as the most probable value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773ce01b-85c7-4960-93eb-a4bbfbde8b68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given the conditional independence assumption, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0396de06-03a5-4e98-8aea-570307939933.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The prediction of the Naive Bayesian Classifier is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/479f2a2b-86d2-4cb3-9d50-2bae310f93de.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Naive Bayes classifier is one of the most basic algorithms that can be applied
    in text classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the `spam.csv` file, which can be downloaded from
    the GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: This `spam.csv` dataset has two columns. One column holds messages and the other
    column holds the message type, which states whether it is a spam message or a
    ham message. We will apply the Naive Bayes technique to predict whether a message
    is likely to be spam or ham.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We set your working directory with the `os.chdir()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our data. As we did in the previous sections, we will prefix the
    name of the DataFrame with `df_` so that we can read it easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now move on to look at how to build our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading the data, we use the `head()` function to take a look it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see that there are two columns: labels
    and message. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb8ebbd7-0a60-4695-b9e9-c45f8d9c1e33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then use the `describe()` function to look at a few metrics in each of the
    columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da9a2829-efe3-493d-931c-789a261bb4e4.png)'
  prefs: []
  type: TYPE_IMG
- en: For the object datatype, the result of `describe()` will provide `metrics`,
    `count`, `unique`, `top`, and `freq`. `top` refers to the most common value, while
    `freq` is the frequency of this value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also take a look at the metrics by message type, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding command, we see the count, number of unique values, and
    frequency for each class of the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70ed29ff-2391-4a27-9fa3-5787c278c298.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To analyze our dataset even further, let''s take a look at the word count and
    the character count for each message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The `lambda` function is used to create small, anonymous functions in Python.
    A `lambda` function can take any number of arguments, but can only have one expression.
    This function is passed as a parameter to other functions, such as `map`, `apply`,
    `reduce`, or `filter`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the preceding code will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4816fd7-3a96-4529-9c72-baaa7691844d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, `labels` is our target variable. We have two classes: `spam`
    and `ham`. We can see the distribution of spam and ham messages using a bar plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b69426-36e2-4535-8c53-5c6ffc1df48f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code block, we will label `spam` as `1`, and `ham` as `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that, in the following screenshot, under the `labels` variable, all
    ham and spam messages are now labelled as 0 and 1 respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/524cff97-dbb5-4df5-973d-6c638d867809.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now split our data into training and testing samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to convert the collection of messages to a matrix of token counts.
    This can be done using `CountVectorizer()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We proceed to build our model with the Naive Bayes algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We load the required libraries for the evaluation metrics, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We now check our accuracy by evaluating the model with the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/04a4ca07-c4d9-4266-8681-0165d9e1f05b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we check the accuracy of our test data by evaluating the model with the
    unseen test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code block, we print performance metrics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4ba0b37-2483-4806-99a9-5da4a042a38e.png)'
  prefs: []
  type: TYPE_IMG
- en: These results may vary with different samples and hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1,* we looked at our dataset. In *Step 2* and *Step **3*, we looked
    at the statistics for the `ham` and spam class labels. In *Step 4*, we extended
    our analysis by looking at the word count and the character count for each of
    the messages in our dataset. In *Step 5*, we saw the distribution of our target
    variables (ham and spam), while in *Step 6* we encoded our class labels for the
    target variable with the numbers `1` and `0`. In S*tep* 7, we split our dataset
    into training and testing samples. In *Step 8*, we used `CountVectorizer()` from
    `sklearn.feature_extraction.text` to convert the collection of messages to a matrix
    of token counts.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not provide a dictionary in advance and do not use an analyzer that
    does some kind of feature selection, then the number of features will be equal
    to the vocabulary size found by analyzing the data. For more information on this,
    see the following: [https://bit.ly/1pBh3T1](https://bit.ly/1pBh3T1).
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 9* and *Step **10*, we built our model and imported the required classes
    from `sklearn.metrics` to measure the various scores respectively. In *Step 11*
    and *12*, we checked the accuracy of our training and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm comes in multiple variations. These include the Multivariate
    Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Multinomial Naive
    Bayes algorithms. These variations can be applied to solve different problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multivariate Bernoulli Naive Bayes**: This algorithm is used when the feature
    vectors provide a binary representation of whether a word or feature occurs in
    the document or not. Every token in the feature vector of a document is associated
    with either the `1` or `0` values. `1` represents a token in which the word occurs,
    and `0` represents a token in which the word does not occur. The Multivariate
    Bernoulli Naive Bayes algorithm can be used in situations in which the absence
    of a particular word matters, such as in the detection of spam content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multinomial Naive Bayes**: This is used when multiple occurrences of words
    are to be considered in classification problems. In this variation, text documents
    are characterized by the frequency of the term, instead of binary values. Frequency
    is a discrete count that refers to how many times a given word or token appears
    in a document. The Multinomial Naive Bayes algorithm can be used for topic modeling,
    which is a method for finding a group of words that best represent the key information
    in a corpus of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian Multinomial Naive Bayes**:In scenarios where we have continuous
    features, one way to deal with continuous data in Naive Bayes classifications
    is to discretize the features. Alternatively, we can apply the Gaussian Multinomial
    Naive Bayesalgorithm.This assumes the features follow a normal distribution and
    uses a Gaussian kernel to calculate the class probabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In scikit-learn, `CountVectorizer()` counts the number of times a word shows
    up in the document and uses that value as its weight. You can also use `TfidfVectorizer()`,
    where the weight assigned to each token depends on both its frequency in a document
    and how often the term recurs in the entire corpus. You can find more on `TfidfVectorizer`
    at the following link: [https://bit.ly/2sJCoVN](https://bit.ly/2sJCoVN).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn documentation on the Naive Bayes classifier for multivariate
    Bernoulli models: [https://bit.ly/2y3fASv](https://bit.ly/2y3fASv).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn documentation on the Naive Bayes classifier for multinomial
    models: [https://bit.ly/2P4Ohic](https://bit.ly/2P4Ohic).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees, a non-parametric supervised learning method, are popular algorithms
    used for predictive modeling. The most well-known decision tree algorithms include
    the **iterative dichotomizer** (**ID3**), C4.5, CART, and C5.0. ID3 is only applicable
    for categorical features. C4.5 is an improvement on ID3 and has the ability to
    handle missing values and continuous attributes. The tree-growing process involves
    finding the best split at each node using the information gain. However, the C4.5
    algorithm converts a continuous attribute into a dichotomous categorical attribute
    by splitting at a suitable threshold value that can produce maximum information
    gain.
  prefs: []
  type: TYPE_NORMAL
- en: Leo Breiman, a distinguished statistician, introduced a decision tree algorithm called
    the **Classification and Regression Tree** (**CART**). CART, unlike ID3 and C4.5,
    can produce decision trees that can be used for both classification and regression
    problems. This algorithm also forms the basis for the important random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are built using recursive partitioning, which splits the data
    into subsets based on several dichotomous independent attributes. This recursive
    process may split the data multiple times until the splitting process terminates
    after a particular stopping criterion is reached. The best split is the one that
    maximizes a splitting criterion. For classification learning, the techniques used
    as the splitting criterion are entropy and information gain, the Gini index, and
    the gain ratio. For regression tasks, however, standard deviation reduction is
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The C4.5 and C5.0 algorithms use entropy (also known as **Shannon entropy**)
    and information gain to identify the optimal attributes and decide on the splitting
    criterion. Entropy is a probabilistic measure of uncertainty or randomness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, entropy can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4048cca7-177c-491b-9c35-cfc9b0465f38.png)'
  prefs: []
  type: TYPE_IMG
- en: In the case of a two-class attribute, entropy can range from 0 to 1\. For an
    n-class attribute, entropy can take values between 0 to ![](img/f16f2000-eeff-4579-9301-f9ea353aa172.png). For
    a homogeneous variable, where there is just a single class, the entropy would
    be zero because the probability of that class being zero is 1 and ![](img/deeeb9d3-eb9f-4989-af93-113b21e38502.png) .
  prefs: []
  type: TYPE_NORMAL
- en: To use entropy to identify the most identified attributes at which to split,
    the algorithm calculates the change in homogeneity that would result from the
    split at each possible attribute. This change is known as information gain. Constructing
    a decision tree is all about finding the attribute that returns the highest information
    gain. This information gain is based on the decrease in entropy after a dataset
    is split at an attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Information gain is calculated as the difference between the entropy before
    the split and the entropy after the split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ffc038-e4e1-47f6-a3c9-e896b3a52ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: The higher the information gain, the better a feature is. Information gain is
    calculated for all features. The algorithm chooses the feature with the highest
    information gain to create the root node. The information gain is calculated at
    each node to select the best feature for that node.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain is also known as Kullback-Leibler divergence. This measures
    the difference between two probability distributions over the same variable. Put
    simply, if you have two probability distributions, the KL divergence measures
    the similarity of the two distributions. If the KL divergence is 0, the two distributions
    are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gini index is a measure of the degree of impurity and can also be used to
    identify the optimal attributes for the splitting criterion. It is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e2db890-f8f0-407b-874a-b8ad80ec9f2d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *p* is the probability of a training instance belonging
    to a particular class. With regards to the Gini index, the lower the impurity,
    the better it is.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build our model with q decision tree algorithm, we will use the `backorders.csv`
    file, which can be downloaded from the following GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset has 23 columns. The target variable is `went_on_backorder`. This
    identifies whether a product has gone on back order. The other 22 variables are
    the predictor variables. A description of the data is provided in the code that
    comes with this book:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory with the `os.chdir()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our data. As we have done previously, we are going to prefix the
    name of the DataFrame with `df_` to make it easier to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now move on to building our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we want to look at the dimensions of the dataset and the data using
    the `shape` and `head()` functions. We also take a look at the statistics of the
    numeric variables using `describe()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'If you get your output in scientific notation, you can change to view it in
    standard form instead by executing the following command: `pd.options.display.float_format
    = ‘{:.2f}’.format`'
  prefs: []
  type: TYPE_NORMAL
- en: 'With `dtypes`, we get to see the data types of each of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that `sku` is an identifier and will be of no use to us for our
    model-building exercise. We will, therefore, drop `sku` from our DataFrame as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check whether there are any missing values with the `isnull().sum()`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d2293ce-455b-424b-9f7b-3f62144d8dc5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since the number of missing values in the `lead_time` variable is about 5%,
    we will remove all the observations where `lead_time` is missing for our initial
    analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to encode our categorical variables. We select only the categorical
    variables and call `pd.get_dummies()` to dummy-code the non-numeric variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding code, we get to see the datatypes. We notice that dummy-coded
    variables are all of the unsigned integer (`uint8`) type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d4f6117-f192-4a9a-b747-39ae88f81f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then look at our target variable distribution as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that our data has a fairly balanced distribution, with approximately
    81% of the observations belonging to class 0 and 19% belonging to class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/383927b2-0b30-4b40-8d34-32912524e6f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now split our data into training and testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We will build our first model with `DecisionTreeClassifier()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'With `model_DT_Gini`, we can see the default values of the hyperparameters
    that have been used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb169144-daba-48b2-8cf7-9559c5d93ac6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use the model to predict our class labels using both our training and
    our testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the accuracy along with the count of **True Negative** (**TN**),
    **False Positive** (**FP**), **False Negative** (**FN**), and **True Positive**
    (**TP**) values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d44ff442-1bd0-4719-9164-291d7426b989.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now use a `plot_confusion_matrix` function to plot our confusion matrix.
    This function is taken from [http://scikit-learn.org](http://scikit-learn.org)
    and is readily available there, so we won''t show this function here. It is, however,
    provided with the code in the book for your reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then see the amount of TNs, FPs, FNs, and TPs in our confusion matrix
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1616f4ad-61b7-4a4e-af45-f6c8de1ea3b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can change the hyperparameters to tune our model. We can also perform a
    grid search to find the hyperparameter values that supply optimum results. We
    can use the following code to set the hyperparameter values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use `GridSearchCV()` to grid search the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding command, we can see the best parameter values among
    those provided using `best_params_`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use the model that is selected using the `GridSearchCV()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to see the metrics per-label, we can also use the `classification_report`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'This step gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a15b729-30a8-4a17-b26d-81c41d1636f9.png)'
  prefs: []
  type: TYPE_IMG
- en: These results will vary depending on the samples used and the hyperparameter
    tuning.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we took a look at the dimensions of our dataset. We also saw the
    statistics of our numerical variables. In *Step 2*, we looked at the datatypes
    of each of our variables. In *Step 3*, we dropped the `sku` attribute, because
    it is an identifier that will be of no use to us for our model. In *Step 4*, we
    checked for missing values and noticed that the `lead_time` attribute had 3,403
    missing values, which is roughly 5% of the total number of observations. In *Step
    5*, we dropped the observations for which the `lead_time` had missing values.
    Note that there are various strategies to impute missing values, but we haven't
    considered these in this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 6*, we used `get_dummies()` from the pandas library with `drop_first=True`
    as one of the parameters to perform a k-1 dummy coding on the categorical variables. In
    *Step 7*, we took a look at the distribution of our target variable. We see the
    class labels, 0 and 1, are in the ratio of 19%-81% approximately, which is not
    very well balanced. However, we had enough observations for both classes to proceed
    to our next steps. In *Step 8*, we separated our predictor and response variables.
    We also split our dataset to create a training dataset and a testing dataset. In
    *Step 9*, we used a `DecisionTreeClassifier()` to build our model. We noted the
    default hyperparameters values and noticed that, by default, `DecisionTreeClassifier()` uses
    the Gini impurity measure as the splitting criterion.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 10*, we used the model to predict our test sample. We took a note of
    the overall accuracy and the amount of TP, TN, FP, and FN values that we achieved.
    In *Step 11*, we used `plot_confusion_matrix()` to plot these values in the form
    of a confusion matrix. Please note that `plot_confusion_matrix()` is readily available
    at [https://bit.ly/2MdyDU9](https://bit.ly/2MdyDU9) and is also provided with
    the book in the code folder for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at changing the hyperparameter values to fine-tune our model.
    We performed a grid search to find the optimum hyperparameter values. In *Step
    12*, we defined the combination of values for our hyperparameters that we want
    to apply to our grid search algorithm. In *Step 13* and *14*, we used `GridSearchCV()`
    to look for the optimum hyperparameters. In *Step 15*, we used the model returned
    by the grid search to predict our test observations. Finally, in *Step 16*, we
    used `classification_report()` from `sklearn.metrics` to generate various scores
    including `precision`, `recall`, `f1-score`, and `support`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, a model can classify training data perfectly but faces difficulty
    when working with new data. This problem is known as **overfitting**. The model
    fails to generalize to the new test data.
  prefs: []
  type: TYPE_NORMAL
- en: We allow a recursive splitting process to repeat until we terminate the leaf
    node because we cannot split the data further. This model would fit the training
    data perfectly but leads to poor performance. For this reason, tree-based models
    are susceptible to overfitting. To overcome this, we need to control the depth
    of our decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to avoid overfitting. One method is to terminate the
    growth before a perfect classification of the training data is made. The following
    approaches can be adopted to implement this stopping method:'
  prefs: []
  type: TYPE_NORMAL
- en: Stop when a tree reaches the maximum number of levels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop when a subset contains fewer than a defined number of training instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stop when the minimum information gain is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another method is to allow the data to overfit, and then to prune the tree after
    it is constructed. This involves eliminating nodes that are not clearly relevant,
    which also minimizes the size of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The scikit-learn documentation on the decision tree classifier: [https://bit.ly/1Ymrzjw](https://bit.ly/1Ymrzjw)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn documentation on the decision tree regressor: [https://bit.ly/2xMNSua](https://bit.ly/2xMNSua)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **support vector machine** (**SVM**) is a popular machine learning algorithm
    for supervised learning. It can be used for both classification and regression
    problems. In classification learning, SVM performs classifications by finding
    an optimal separating hyperplane that differentiates two classes of observations.
    If the data is linearly separable and one-dimensional, we may have a point that
    separates the data. In two-dimensional space, the data can be separated by a straight
    line, while a plane separates data in three-dimensional space. When we have more
    than three dimensions, this is called a hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a linear SVM, a dataset *X* with *n* feature vectors is represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fb2c247-c973-48d5-a61c-081f8b4a7124.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A bipolar target variable *Y* is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb1770ec-6461-4385-8495-94d959b99152.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The hyperplane is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6d2790e-d2d0-4fd6-a451-f9b13f70d3bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For an SVM, the two classes are represented as -1 and +1 instead of 1 and 0\.
    The hyperplane can, therefore, be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d429da2-7ea8-41f7-9389-ddd5de0db021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To classify the data, we have the following two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dafe97b7-53a1-4bfb-a6f6-c96829d976f3.png)'
  prefs: []
  type: TYPE_IMG
- en: However, it's quite possible that there are a lot of hyperplanes that correctly
    classify the training data. There might be infinite solutions of *w* and *b* that
    hold for the preceding rules. An algorithm such as a perceptron learning algorithm
    will just find any linear classifier. SVM, however, finds the optimal hyperplane,
    which is at a maximum distance from any data point. The further the data points
    lie from the hyperplane, the more confident we are that they have been correctly
    classified. We would therefore like the data points to be as far away from the
    hyperplane as possible, while still being able to classify them correctly. The
    best hyperplane is the one that has the maximum margin between the two classes.
    This is known as the maximum-margin hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: It's possible for SVM to choose the most important vectors that define the separation
    hyperplane from the training data. These are the data points that lie closest
    to the hyperplane and are known as support vectors. Support vectors are the data
    points that are hardest to classify. At the same time, these represent high-quality
    data. If you remove all the other data points and use only the support vectors,
    you can get back the exact decision hyperplane and the margin using the same SVM
    model. The number of data points does not really matter, just the support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We normalize the weights *w* and *b* so that the support vectors satisfy the
    following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7b23e99-c87b-4838-b936-059d6ee20202.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, the classification rules change to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb754845-562f-4184-8d35-c979706a615c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equations can be combined and represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fb465eb-6c11-4010-ada3-8a5e1df77834.png)'
  prefs: []
  type: TYPE_IMG
- en: The initial SVM algorithms could only be used in the case of linearly separable
    data. These are known as hard-margin SVMs. However, hard-margin SVMs can work
    only when the data is completely linearly separable and if doesn't have any noise.
    In the case of noise or outliers, a hard-margin SVM might fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vladimir Vapnik proposed soft-margin SVMs to deal with data that is non-linearly separable
    by using slack variables. Slack variables allows for errors to be made while fitting
    the model to the training dataset. In hard-margin classification, we will get
    a decision boundary with a small margin. In soft-margin classification, we will
    get a decision boundary with a larger margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a46c8e11-1bfb-4664-98ab-008ff1d437df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'SVMs can also perform non-linear classification extremely well using something
    called a kernel trick. This refers to transformations in which the predictor variables
    are implicitly mapped to a higher-dimensional feature space. Popular kernel types
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radial basis function (RBF) kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different kernel functions are available for various decision functions. We
    can add kernel functions together to achieve even more complex planes.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to use the `bank.csv` file, which is based on
    bank marketing data and which you can download from GitHub. This data is related
    to a Portuguese bank''s direct marketing campaigns that took place over phone
    calls. The goal is to predict whether the client will subscribe to a term deposit:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We set our working directory with the `os.chdir()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our data. We will again prefix the name of the DataFrame with `df_`
    to make it easier to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''re going to look at checking null values, standardizing
    numeric values, and one-hot-encoding categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following command, we can see we that we have ten categorical variables
    and seven numerical variables in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'With the following command, we notice there are no missing values, so we can
    proceed with our next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the class balance in our target variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We can convert our target class to the binary values 1 and 0 with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now perform one-hot encoding on our categorical variables. We only select
    variables that are categorical in nature. In the following code, we use `category_column_names`
    to provide the names of the non-numeric variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We run a loop over each of the non-numerical variables to perform one-hot encoding
    on them and add them back to the DataFrame. We will also delete the original non-numerical
    variables after performing one-hot encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate the predictor and response variables as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We also split our data into training and testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We then build our first model using SVC with the default kernel, **radial basis
    function** (**RBF**):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'We check our training and testing accuracy via the SVC model built with the
    RBF kernel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3b0f08d-0164-4b38-9bd4-f0013851e31e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rebuild our SVC model with a polynomial kernel as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output with the polynomial kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6429ace-6e67-4813-b1c9-d0fd31463280.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also build an SVC model with the linear kernel. Instead of `kernel=''ploy''`,
    we can replace this with `kernel=''linear''` in the preceding code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'With the linear kernel, we get the following accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/595facc0-1ed8-4282-a9cd-7d76ae51a0de.png)'
  prefs: []
  type: TYPE_IMG
- en: Our results will vary depending on the different types of kernel and other hyperparameter
    values used.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we looked at the data types of our variables. We noticed that we
    have ten categories and seven numerical variables. In *Step 2*, we checked for
    missing values and saw that there were no missing values in our dataset. In *Step
    3*, we checked the class balance of our target variable and found out that it
    has the values of `yes` and `no`. In *Step 4*, we converted our target variable
    to 1 and 0 to represent `yes` and `no` respectively. In *Steps 5* and *6*, we
    performed one-hot encoding on the non-numerical variables.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we separate the predictor and response variables and in *Step 8*,
    we split our dataset into training and testing datasets. After that, in *Step
    9*, we used `SVC()` from `sklearn.svm` with the default RBF kernel to build our
    model. We applied it to our training and testing data to predict the class. In
    *Step 10*, we checked the accuracy of our training and testing data. In *Step
    11*, we changed our hyperparameter to set the kernel to polynomial. We noticed
    that training accuracy remained more or less the same, but the test accuracy improved.
  prefs: []
  type: TYPE_NORMAL
- en: With the polynomial kernel, the default degree is 3\. You can change the polynomial
    degree to a higher degree and note of the change in the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we changed the kernel to linear to see if the results improved
    compared to the polynomial kernel. We did not, however, see any significant improvement.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we have seen how to use various kernels in our code. Kernel
    functions must be symmetrical. Preferably, they should have a positive (semi)
    definite gram matrix. A gram matrix is the matrix of all the possible inner products
    of V, where V is the set of m vectors. For convenience, we consider positive semi-definite
    and positive-definite functions indifferently. In practice, a positive definiteness
    of kernel matrices ensures that kernel algorithms converge to a unique solution.
  prefs: []
  type: TYPE_NORMAL
- en: A **linear kernel** is the simplest of all kernels available. It works well
    with text classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear kernel is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/523469ff-9905-47b2-b6c5-b7bafe00005c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **c** is the constant term.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **polynomial kernel** has two parameters: a constant and the degree. A polynomial
    kernel with no constant and a degree of 1 is simply a linear kernel. As the degree
    of the polynomial kernel increases, the decision function becomes more complex.
    With higher degrees, it is possible to get good training accuracy, but the model
    might fail to generalize to unseen data, leading to overfitting. The polynomial
    kernel is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/837f7b87-065c-485b-ab70-78c4b14d3931.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png) is the slope, d is the
    degree of the kernel, and c is the constant term.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **radial basis function kernel (RBF)**, also known as the Gaussian kernel,
    is a more complicated kernel and can outperform polynomial kernels. The RBF kernel
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a85ebd6-761c-4a94-a8dd-65bbadcb0b30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ![](img/33e24c69-55f0-4345-a50f-b8e7491d809f.png) parameter can be tuned
    to increase the performance of the kernel. This is important: with an over-estimated ![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png),
    the kernel can lose its non-linear power and behave more linearly. On the other
    hand, if ![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png) is underestimated,
    the decision function can be highly sensitive to noise in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all kernels are strictly positive-definite. The sigmoid kernel function,
    though is quite widely used, is not positive-definite. The sigmoid function is
    given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdaf800d-a3a4-489d-bf26-ff6a6a61d939.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png) is the slope and **c**
    is the constant term. Note that an SVM with a sigmoid kernel is the same as a
    two-layer perceptron neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a kernel trick to an SVM model can give us new models. How do we choose
    which kernel to use? The first approach is to try out the RBF kernel, since it
    works pretty well most of the time. However, it is a good idea to use other kernels
    and validate your results. Using the right kernel with the right dataset can help
    you build the best SVM models.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More on the positive definite matrix can be found here: [https://bit.ly/2NnGeLK](https://bit.ly/2NnGeLK).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive definite kernels are a generalization of the positive definite matrix.
    You can find out more about this here: [https://bit.ly/2NlsIs1](https://bit.ly/2NlsIs1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scikit-learn documentation on support vector regression: [https://bit.ly/2OFZ8ix](https://bit.ly/2OFZ8ix).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
