- en: Statistical and Machine Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计与机器学习算法
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下内容：
- en: Multiple linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Logistic regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Naive Bayes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Support vector machines
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The technical requirements for this chapter remain the same as those we detailed
    in [Chapter 1](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml), *Get Closer to Your
    Data*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求与我们在[第1章](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml)“接近你的数据”中详细说明的要求相同。
- en: Visit the GitHub repository to get the dataset and the code. These are arranged
    by chapter and by the name of the topic. For the linear regression dataset and
    code, for example, visit `.../Chapter 3/Linear regression`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 访问GitHub仓库以获取数据集和代码。这些数据集和代码按章节和主题名称排列。例如，对于线性回归数据集和代码，请访问`.../Chapter 3/Linear
    regression`。
- en: Multiple linear regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'Multiple linear regression is a technique used to train a linear model, that
    assumes that there are linear relationships between multiple predictor variables
    (![](img/a873ef1f-dd8a-4450-8afa-5e5c3c4b2dc5.png)) and a continuous target variable
    (![](img/9d832a44-5813-46fb-9d89-681fc4ed016a.png)). The general equation for
    a multiple linear regression with m predictor variables is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 多元线性回归是一种用于训练线性模型的技术，它假设多个预测变量（![img/a873ef1f-dd8a-4450-8afa-5e5c3c4b2dc5.png]）与连续目标变量（![img/9d832a44-5813-46fb-9d89-681fc4ed016a.png]）之间存在线性关系。具有m个预测变量的多元线性回归的一般方程如下：
- en: '![](img/088198d7-1233-4571-9fcc-61c6dba157ad.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![img/088198d7-1233-4571-9fcc-61c6dba157ad.png]'
- en: '![](img/19a78dc6-d3ee-4492-b89e-6d42851d79b0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![img/19a78dc6-d3ee-4492-b89e-6d42851d79b0.png]'
- en: 'Training a linear regression model involves estimating the values of the coefficients
    for each of the predictor variables denoted by the letter ![](img/a343305a-25c0-4a07-8783-5045b92efe76.png).
    In the preceding equation, ![](img/9c4fefc7-e6fe-4dd2-a801-a379f072ae3c.png) denotes
    an error term, which is normally distributed, and has zero mean and constant variance.
    This is represented as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练线性回归模型涉及估计每个预测变量（用字母表示）的系数的值，如上图所示 ![img/a343305a-25c0-4a07-8783-5045b92efe76.png]。在先前的方程中，![img/9c4fefc7-e6fe-4dd2-a801-a379f072ae3c.png]
    表示误差项，它服从正态分布，具有零均值和恒定方差。这可以表示如下：
- en: '![](img/8302e115-092f-45d0-b4be-8689554c67a9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![img/8302e115-092f-45d0-b4be-8689554c67a9.png]'
- en: 'Various techniques can be used to build a linear regression model. The most
    frequently used is the **ordinary least square** (**OLS**) estimate. The OLS method
    is used to produce a linear regression line that seeks to minimize the sum of
    the squared error. The error is the distance from an actual data point to the
    regression line. The sum of the squared error measures the aggregate of the squared
    difference between the training instances, which are each of our data points,
    and the values predicted by the regression line. This can be represented as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种技术来构建线性回归模型。最常用的是**普通最小二乘法**（OLS）估计。OLS方法用于生成一个线性回归线，该线试图最小化平方误差的总和。误差是实际数据点到回归线的距离。平方误差的总和衡量了训练实例（每个数据点）与回归线预测值之间的平方差的总体。这可以表示如下：
- en: '![](img/20b55abb-3b52-41db-8c2e-e1d7b36c8d0c.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![img/20b55abb-3b52-41db-8c2e-e1d7b36c8d0c.png]'
- en: In the preceding equation, ![](img/bcc37182-bc5c-4ce3-9f45-8e124cd79d81.png) is
    the actual training instance and ![](img/5d58def7-dceb-4c83-8add-15c9237d55b0.png)
    is the value predicted by the regression line.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的方程中，![img/bcc37182-bc5c-4ce3-9f45-8e124cd79d81.png] 是实际的训练实例，而 ![img/5d58def7-dceb-4c83-8add-15c9237d55b0.png]
    是回归线预测的值。
- en: In the context of machine learning, gradient descent is a common technique that
    can be used to optimize the coefficients of predictor variables by minimizing
    the training error of the model through multiple iterations. Gradient descent
    starts by initializing the coefficients to zero. Then, the coefficients are updated
    with the intention of minimizing the error. Updating the coefficients is an iterative
    process and is performed until a minimum squared error is achieved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，梯度下降是一种常用的技术，可以通过最小化模型的训练误差（通过多次迭代）来优化预测变量的系数。梯度下降首先将系数初始化为零。然后，通过更新系数以最小化误差的目的来更新系数。更新系数是一个迭代过程，并且会一直执行，直到达到最小平方误差。
- en: In the gradient descent technique, a hyperparameter called the **learning rate**,
    denoted
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降技术中，有一个称为**学习率**的超参数，表示为
- en: by ![](img/6ae71158-dd81-4087-b5ce-5d8e76ce02c4.png) is provided to the algorithm. This
    parameter determines how fast the algorithm moves toward the optimal value of
    the coefficients. If ![](img/556b7d9a-e0a6-4d3f-af81-cda64717e7ed.png) is very
    large, the algorithm might skip the optimal solution. If it is too small, however,
    the algorithm might have too many iterations to converge to the optimum coefficient
    values. For this reason, it is important to use the right value for ![](img/f2895db5-0f99-4a3e-801f-5b9594ecf991.png).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`![](img/6ae71158-dd81-4087-b5ce-5d8e76ce02c4.png)`提供给算法。此参数决定了算法向系数最优值移动的速度。如果`![](img/556b7d9a-e0a6-4d3f-af81-cda64717e7ed.png)`非常大，算法可能会跳过最优解。然而，如果它太小，算法可能需要太多迭代才能收敛到最优系数值。因此，使用正确的`![](img/f2895db5-0f99-4a3e-801f-5b9594ecf991.png)`值非常重要。
- en: In this recipe, we will use the gradient descent method to train our linear
    regression model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用梯度下降法来训练我们的线性回归模型。
- en: Getting ready
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In [Chapter 1](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml), *Get Closer To
    Your Data*, we took the `HousePrices.csv` file and looked at how to manipulate
    and prepare our data. We also analyzed and treated the missing values in the dataset.
    We will now use this final dataset for our model-building exercise, using linear
    regression:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](2dbc8735-7eef-42ba-8b19-5644632c3197.xhtml)《更接近你的数据》中，我们查看了`HousePrices.csv`文件，并探讨了如何操作和准备我们的数据。我们还分析了数据集中的缺失值。现在，我们将使用这个最终数据集进行我们的模型构建练习，使用线性回归：
- en: 'In the following code block, we will start by importing the required libraries:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们将首先导入所需的库：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We set our working directory with the `os.chdir()` command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`os.chdir()`命令设置我们的工作目录：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s read our data. We prefix the DataFrame name with `df_` so that we can
    understand it easily:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取我们的数据。我们使用`df_`作为DataFrame名称的前缀，以便我们能够轻松理解：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Let's move on to building our model. We will start by identifying our numerical
    and categorical variables. We study the correlations using the correlation matrix
    and the correlation plots.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建我们的模型。我们将从识别数值变量和分类变量开始。我们使用相关矩阵和相关图来研究相关性。
- en: 'First, we''ll take a look at the variables and the variable types:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将查看变量及其类型：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll then look at the correlation matrix. The `corr()` method computes the
    pairwise correlation of columns:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看相关矩阵。`corr()`方法计算列之间的成对相关系数：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Besides this, we''d also like to study the correlation between the predictor
    variables and the response variable:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们还想研究预测变量与响应变量之间的相关性：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We may also want to sort our correlation by absolute values. In order to do
    this, we can use the following command: `corr_with_target[abs(corr_with_target).argsort()[::-1]]`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能想要按绝对值对相关性进行排序。为了做到这一点，我们可以使用以下命令：`corr_with_target[abs(corr_with_target).argsort()[::-1]]`
- en: 'We can look at the correlation plot using the `heatmap()` function from the
    `seaborn` package:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`seaborn`包中的`heatmap()`函数查看相关图：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot is the correlation plot. Note that we have removed
    the upper triangle of the heatmap using the `np.zeros_like()` and `np.triu_indices_from()` functions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是相关图。请注意，我们已使用`np.zeros_like()`和`np.triu_indices_from()`函数移除了热力图的上方三角形：
- en: '![](img/6b5aa08a-231d-4df4-b201-8b520478b0ee.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b5aa08a-231d-4df4-b201-8b520478b0ee.png)'
- en: Let's explore our data by visualizing other variables.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过可视化其他变量来探索我们的数据。
- en: 'We can look at the distribution of our target variable, `SalePrice`, using
    a histogram with a kernel density estimator as follows:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下方式使用带有核密度估计器的直方图查看我们的目标变量`SalePrice`的分布：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following screenshot gives us the distribution plot for the `SalePrice`
    variable:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了`SalePrice`变量的分布图：
- en: '![](img/e333a93b-7cdc-4307-955a-02f3ee7132e4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e333a93b-7cdc-4307-955a-02f3ee7132e4.png)'
- en: In statistics, **kernel density estimation** (**KDE**) is a non-parametric way
    to estimate the probability density function of a random variable. Kernel density
    estimation is a fundamental data smoothing problem where inferences about the
    population are based on a finite data sample. KDE is a technique that provides
    you with a smooth curve given a set of data. It can be handy if you want to visualize
    the shape of some data, as a kind of continuous replacement for the discrete values
    plotted in a histogram.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，**核密度估计**（**KDE**）是一种非参数方法，用于估计随机变量的概率密度函数。核密度估计是一个基本的数据平滑问题，其中关于总体的推断基于有限的数据样本。KDE
    是一种技术，它提供了一组数据的一个平滑曲线。如果你想要可视化某些数据，将其作为直方图中离散值的一种连续替代，这可能会很有用。
- en: 'We can also use `JointGrid()` from our `seaborn` package to plot a combination
    of plots:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用 `seaborn` 包中的 `JointGrid()` 来绘制组合图表：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the preceding code, we are able to plot the scatter plot for GarageArea
    and SalePrice, while also plotting the histogram for each of these variables on
    each axis:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们能够绘制出 GarageArea 和 SalePrice 的散点图，同时在每个轴上绘制这些变量的直方图：
- en: '![](img/d410aa20-1b45-465d-83a8-bcddeded8f74.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d410aa20-1b45-465d-83a8-bcddeded8f74.png)'
- en: 'Let''s now scale our numeric variables using min-max normalization. To do this,
    we first need to select only the numeric variables from our dataset:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用最小-最大归一化来缩放我们的数值变量。为此，我们首先需要从我们的数据集中选择仅包含数值变量：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will now apply the min-max scaling to our numeric variables:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将对我们的数值变量应用最小-最大缩放：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the following table, we can see that our numeric variables have been scaled
    down:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的表中，我们可以看到我们的数值变量已经被缩放：
- en: '![](img/c2ba2525-15ca-4bca-8972-48bff32dddfe.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2ba2525-15ca-4bca-8972-48bff32dddfe.png)'
- en: 'Now, we will perform one-hot encoding on our categorical variables:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将对我们的分类变量执行独热编码：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We have now created a DataFrame with only numeric variables that have been
    scaled. We have also created a DataFrame with only categorical variables that
    have been encoded. Let''s combine the two DataFrames into a single DataFrame:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建了一个只包含缩放后的数值变量的 DataFrame。我们还创建了一个只包含编码后的分类变量的 DataFrame。让我们将这两个 DataFrame
    合并成一个 DataFrame：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can then concatenate the `SalePrice` variable to our `df_housedata` DataFrame:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将 `SalePrice` 变量连接到我们的 `df_housedata` DataFrame：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can create our training and testing datasets using the `train_test_split`
    class from `sklearn.model_selection`:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `sklearn.model_selection` 中的 `train_test_split` 类来创建我们的训练和测试数据集：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now use `SGDRegressor()` to build a linear model. We fit this linear
    model by minimizing the regularized empirical loss with SGD:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `SGDRegressor()` 来构建一个线性模型。我们通过最小化 SGD 的正则化经验损失来拟合这个线性模型：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By running the preceding code, we find out that the coefficient of determination
    is roughly 0.81.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行前面的代码，我们发现确定系数大约为 0.81。
- en: Note that `r2_score()` takes two arguments. The first argument should be the
    true values, not the predicted values, otherwise, it would return an incorrect
    result.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`r2_score()` 函数接受两个参数。第一个参数应该是真实值，而不是预测值，否则它将返回一个错误的结果。
- en: 'We check the **root mean square error** (**RMSE**) on the test data:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查测试数据上的 **均方根误差**（**RMSE**）：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Running the preceding code provides output to the effect that the RMSE equals
    36459.44.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码提供了 RMSE 等于 36459.44 的输出。
- en: 'We now plot the actual and predicted values using `matplotlib.pyplot`:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用 `matplotlib.pyplot` 来绘制实际值和预测值：
- en: '[PRE17]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The resulting plot with our actual values and the predicted values will look
    as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值和预测值的结果图将如下所示：
- en: '![](img/11ce1f99-0212-467f-aa91-f014135dccc1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11ce1f99-0212-467f-aa91-f014135dccc1.png)'
- en: Because the chart shows most values in approximately a 45-degree diagonal line,
    our predicted values are quite close to the actual values, apart from a few.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因为图表显示大多数值大约在 45 度的对角线上，所以我们的预测值与实际值非常接近，除了少数几个。
- en: How it works...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we looked at the variable types. We saw that the dataset had both
    numeric and non-numeric variables. In *Step 2*, we used the `Pearson` method to
    calculate the pairwise correlation among all the numeric variables. After that,
    in *Step 3*, we saw how all of the predictor variables are related to the target
    variable. We also looked at how to sort correlation coefficients by their absolute
    values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们查看变量类型。我们注意到数据集既有数值变量也有非数值变量。在*步骤2*中，我们使用`Pearson`方法计算所有数值变量之间的成对相关性。之后，在*步骤3*中，我们看到了所有预测变量与目标变量之间的关系。我们还探讨了如何按绝对值对相关系数进行排序。
- en: 'In *Step 4*, we painted a heatmap to visualize the correlation between the
    variables. Then, we introduced two functions from the NumPy library: `zeros_like()`
    and `triu_indices_from()`. The `zeros_like()` function takes the correlation matrix
    as an input and returns an array of zeros with the same shape and type as the
    given array. `triu_indices_from()` returns the indices for the upper triangle
    of the array. We used these two functions to mask the upper triangular part of
    the correlation plot. We called the `heatmap()` function from the `seaborn` library
    to paint a correlation heat map and passed our correlation matrix to it. We also
    set the color of the matrix using `cmap="YlGnBu"` and the size of the legend bar
    using `cbar_kws={"shrink": 0.5}`.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '在*步骤4*中，我们绘制了一个热图来可视化变量之间的相关性。然后，我们介绍了来自NumPy库的两个函数：`zeros_like()`和`triu_indices_from()`。`zeros_like()`函数接受相关性矩阵作为输入，并返回一个与给定数组具有相同形状和类型的零数组。`triu_indices_from()`返回数组上三角的索引。我们使用这两个函数来屏蔽相关性图的上三角部分。我们调用`seaborn`库中的`heatmap()`函数来绘制相关性热图，并将我们的相关性矩阵传递给它。我们还使用`cmap="YlGnBu"`设置矩阵的颜色，并使用`cbar_kws={"shrink":
    0.5}`设置图例条的尺寸。'
- en: '`numpy.tril_indices_from()` returns the indices for the lower triangle of the
    array.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy.tril_indices_from()`返回数组下三角的索引。'
- en: 'In *Step 5*, we looked at the distribution of the target variable, `SalePrice`.
    In *Step 6*, we used `JointGrid()` from the `seaborn` library to show how it is
    possible to plot a scatter plot for two numeric variables with a regression line,
    along with plotting the distribution of both variables on the axis in the same
    chart. In *Steps 7* and *8*, we selected only the numeric variables and scaled
    the variables using min-max normalization. This scales the values to a numeric
    range of data between 0 and 1\. This is also called feature scaling, and is performed
    using the following formula:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤5*中，我们查看目标变量`SalePrice`的分布。在*步骤6*中，我们使用`seaborn`库的`JointGrid()`来展示如何为两个数值变量绘制带有回归线的散点图，同时在同一图表中绘制两个变量的分布。在*步骤7*和*步骤8*中，我们只选择数值变量，并使用最小-最大归一化对变量进行缩放。这会将值缩放到0到1之间的数值范围。这也被称为特征缩放，并使用以下公式执行：
- en: '![](img/b3749017-4989-4796-a144-a72591fca108.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3749017-4989-4796-a144-a72591fca108.png)'
- en: In *Step 9*, *Step **10*, and *Step **11*, we performed one-hot encoding on
    the categorical variables and added the encoded variables to the DataFrame. We
    also dropped the original categorical variables. In *Step 12*, we split our dataset
    into a training set and a testing set. In *Step 13*, we built our linear regression
    model using `SGDRegressor()` and printed the coefficient of determination. Finally,
    in *Step 14*, we plotted the predicted and actual values to see how well our model
    performed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤9*、*步骤10*和*步骤11*中，我们对分类变量进行了独热编码，并将编码后的变量添加到DataFrame中。我们还删除了原始的分类变量。在*步骤12*中，我们将数据集分为训练集和测试集。在*步骤13*中，我们使用`SGDRegressor()`构建了线性回归模型，并打印了确定系数。最后，在*步骤14*中，我们绘制了预测值和实际值，以查看模型的性能如何。
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Consider a linear regression model, given the following hypothesis function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个线性回归模型，假设以下假设函数：
- en: '![](img/7a6e344d-3744-49ef-8abb-d5b3b90d5c73.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a6e344d-3744-49ef-8abb-d5b3b90d5c73.png)'
- en: In this case, the cost function for ![](img/dbdd5107-ab3c-4439-9eb4-d37c9f398178.png) is
    the **mean squared error** (**MSE**).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，对于![](img/dbdd5107-ab3c-4439-9eb4-d37c9f398178.png)，成本函数是**均方误差**（**MSE**）。
- en: 'The formula is as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 公式如下：
- en: '![](img/3437f7b5-298e-43d2-aadb-27b6cbd92af3.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3437f7b5-298e-43d2-aadb-27b6cbd92af3.png)'
- en: In this formula, ![](img/330bdebb-32ae-4643-bb20-9c1536e89174.png) represents
    the number of training instances. ![](img/cdaa24f4-3130-4e53-9433-6ed55b2a2c5e.png) and ![](img/6bbb6455-36e9-4530-8abd-4cf149a36eb3.png) are
    the input vector and the target vector for the i^(th) training instance respectively,
    while ![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) represents the parameters
    or coefficients for each input variable. ![](img/fde84455-2d8b-45cf-91cf-7e050c0b2754.png) is
    the predicted value for the i^(th) training instance using the  ![](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png) parameters.
    The MSE is always non-negative and the closer it gets to zero, the better.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![图片](img/330bdebb-32ae-4643-bb20-9c1536e89174.png)代表训练实例的数量。![图片](img/cdaa24f4-3130-4e53-9433-6ed55b2a2c5e.png)和![图片](img/6bbb6455-36e9-4530-8abd-4cf149a36eb3.png)分别是第i个训练实例的输入向量和目标向量，而![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)代表每个输入变量的参数或系数。![图片](img/fde84455-2d8b-45cf-91cf-7e050c0b2754.png)是使用![图片](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png)参数预测的第i个训练实例的预测值。均方误差（MSE）总是非负的，且越接近零，表示效果越好。
- en: 'The MSE is higher when the model performs poorly on the training data. The
    objective of the learning algorithm, therefore, is to find the value of **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **such
    that the MSE is minimized. This can be represented as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在训练数据上表现不佳时，均方误差（MSE）会更高。因此，学习算法的目的是找到**![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**的值，以使MSE最小化。这可以表示如下：
- en: '![](img/2ce4ff82-022a-4132-853b-1f5ed9d6bc39.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ce4ff82-022a-4132-853b-1f5ed9d6bc39.png)'
- en: 'The stochastic gradient descent method finds the values of ![](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png) that
    minimize the cost function. In order to minimize the cost function, it keeps changing
    the ![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) parameters by calculating
    the slope of the derivative of the cost function. It starts by initializing the
    **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **parameters to zero. The
    **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **parameters are updated at
    each step of the gradient descent:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降法找到![图片](img/8a9e74ee-919c-441b-a52a-8fdf77b8d324.png)的值，以最小化成本函数。为了最小化成本函数，它通过计算成本函数导数的斜率来不断改变![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)参数。它首先将![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)参数初始化为零。![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)参数在梯度下降的每一步都会更新：
- en: '![](img/0922d000-fa2c-4c1e-b2c0-ce4a5835d302.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0922d000-fa2c-4c1e-b2c0-ce4a5835d302.png)'
- en: The number of updates required for the algorithm to converge will increase with
    the increase in the training data. However, as the training data gets larger and
    larger, it is quite possible for the algorithm to converge much before every instance
    in the training data is learnt. In other words, the increase in the training data
    size need not increase the training time needed to train the best possible model
    where the test error is at its least.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 算法收敛所需的更新次数会随着训练数据的增加而增加。然而，随着训练数据量的增大，算法可能在训练数据中的每个实例都学习之前就收敛了。换句话说，训练数据量的增加不一定需要增加训练最佳模型所需的时间，其中测试误差最小。
- en: Every training instance will modify **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**.
    The algorithm averages these **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png) **values to
    calculate the final **![](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png).**
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练实例都会修改**![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**。算法将这些**![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**值平均，以计算最终的**![图片](img/2ee5e3fa-1067-4dde-a7a9-9db710105e9c.png)**。
- en: '![](img/bf8d8412-f33e-49a7-a87f-4bd993db62d7.png) is the learning rate, which
    tells the algorithm how rapidly to move toward the minimum. A large ![](img/ad3709fe-d549-4f9c-8f87-c34446880063.png) might
    miss the minimum error, while a small ![](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png) might
    take a longer time for the algorithm to run.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/bf8d8412-f33e-49a7-a87f-4bd993db62d7.png)是学习率，它告诉算法以多快的速度向最小值移动。一个大的![图片](img/ad3709fe-d549-4f9c-8f87-c34446880063.png)可能会错过最小误差，而一个小的![图片](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png)可能会使算法运行时间更长。'
- en: 'In the preceding section, we used a `SGDRegressor()` function, but we opted
    for the default values of the hyperparameters. We are now going to change ![](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png) to
    0.0000001 and the `max_iter` value to 2000:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了`SGDRegressor()`函数，但选择了超参数的默认值。现在我们将![图片](img/374cefed-6b08-4a0c-966c-6c701c4ee21a.png)改为0.0000001，并将`max_iter`值改为2000：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`max_iter` is an integer value that tells the algorithm the maximum number
    of passes it can make over the training data. This is also known as the number
    of epochs.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_iter` 是一个整数值，它告诉算法它可以在训练数据上进行的最大遍历次数。这也被称为epoch的数量。'
- en: In our case, the preceding code gives the result that the RMSE drops from 36,459
    to 31,222 and the coefficient of determination improved from 0.81 to 0.86\. These
    results will vary for every iteration.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，前面的代码给出了RMSE从36,459下降到31,222，决定系数从0.81提高到0.86的结果。这些结果会因每次迭代而异。
- en: See also
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'The scikit-learn documentation on regression metrics: [https://bit.ly/2D6Wn8s](https://bit.ly/2D6Wn8s)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn关于回归指标的文档：[https://bit.ly/2D6Wn8s](https://bit.ly/2D6Wn8s)
- en: 'The scikit-learn guide to density estimation: [https://bit.ly/2RlnlMj](https://bit.ly/2RlnlMj)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn关于密度估计的指南：[https://bit.ly/2RlnlMj](https://bit.ly/2RlnlMj)
- en: Logistic regression
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: In the previous section, we noted that linear regression is a good choice when
    the target variable is continuous. We're now going to move on to look at a binomial
    logistic regression model, which can predict the probability that an observation
    falls into one of two categories of a dichotomous target variable based on one
    or more predictor variables. A binomial logistic regression is often referred
    to as logistic regression.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们提到当目标变量是连续的时，线性回归是一个不错的选择。我们现在将转向查看二元逻辑回归模型，该模型可以根据一个或多个预测变量预测观察值落入二元目标变量的两个类别之一的概率。二元逻辑回归通常被称为逻辑回归。
- en: 'Logistic regression is similar to linear regression, except that the dependent
    variable is measured on a dichotomous scale. Logistic regression allows us to
    model a relationship between multiple predictor variables and a dichotomous target
    variable. However, unlike linear regression, in the case of logistic regression,
    the linear function is used as an input to another function, such as ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归类似于线性回归，不同之处在于因变量是在二元 scale 上测量的。逻辑回归使我们能够对多个预测变量和一个二元目标变量之间的关系进行建模。然而，与线性回归不同，在逻辑回归的情况下，线性函数被用作另一个函数的输入，例如 ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png)：
- en: '![](img/c667dd69-61a9-421f-b5b8-f1f0d4b797e7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c667dd69-61a9-421f-b5b8-f1f0d4b797e7.png)'
- en: 'Here, ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png) is the sigmoid or logistic
    function. The sigmoid function is given as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![](img/0abef8f3-a09d-48ce-bed0-de61f72b600d.png) 是S形或逻辑函数。S形函数如下所示：
- en: '![](img/9a62d322-fb87-4998-b8ae-ab4d8a51b87d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a62d322-fb87-4998-b8ae-ab4d8a51b87d.png)'
- en: 'The following graph represents a sigmoid curve in which the values of the y-axis
    lie between 0 and 1\. It crosses the axis at 0.5\. :'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 graph 表示一个S形曲线，其中y轴的值介于0和1之间。它在0.5处穿过轴：
- en: '![](img/309120d7-1ddd-42ee-a168-2a77cbdcc19f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/309120d7-1ddd-42ee-a168-2a77cbdcc19f.png)'
- en: The output, which lies between 0 and 1, is the probability of the positive class.
    We can interpret the output of our hypothesis function as positive if the value
    returned is ![](img/5ca126ce-5e50-4933-a3e3-d96ecc68495f.png) 0.5\. Otherwise,
    we interpret it as negative.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值介于0和1之间，是正类的概率。如果我们假设函数返回的值是 ![](img/5ca126ce-5e50-4933-a3e3-d96ecc68495f.png) 0.5，我们可以将输出解释为正。否则，我们将其解释为负。
- en: 'In the case of logistic regression, we use a cost function known as cross-entropy.
    This takes the following form for binary classification:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归的情况下，我们使用一个称为交叉熵的成本函数。对于二元分类，它具有以下形式：
- en: '![](img/d56c0b91-cce1-442c-a47c-855294fd4253.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d56c0b91-cce1-442c-a47c-855294fd4253.png)'
- en: 'For *y=1* and *y=0*, we get the following results:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *y=1* 和 *y=0*，我们得到以下结果：
- en: '![](img/cf567852-7a51-4677-b73c-589245f03199.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf567852-7a51-4677-b73c-589245f03199.png)'
- en: Cross-entropy increases as the predicted probability diverges from the actual
    label. A higher divergence results in a higher cross-entropy value. In the case
    of linear regression, we saw that we can minimize the cost using gradient descent.
    In the case of logistic regression, we can also use gradient descent to update
    the coefficients and minimize the cost function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测概率偏离实际标签时，交叉熵会增加。更大的偏差会导致更高的交叉熵值。在线性回归的情况下，我们看到了使用梯度下降最小化成本的方法。在逻辑回归的情况下，我们也可以使用梯度下降来更新系数并最小化成本函数。
- en: In this recipe, we will use the `SGDClassfier()` implementation of scikit-learn. `SGDClassifier()`
    implements regularized linear models with stochastic gradient descent, which,
    for large datasets, is much faster than gradient descent. This is because gradient
    descent considers the whole training dataset, while stochastic gradient descent only considers
    one random point while updating the weights.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用scikit-learn的 `SGDClassfier()` 实现。`SGDClassifier()` 实现了正则化线性模型和随机梯度下降，对于大型数据集来说，它比梯度下降要快得多。这是因为梯度下降考虑了整个训练数据集，而随机梯度下降只考虑更新权重时的一个随机点。
- en: By default, `SGDClassifier` might not perform as well as logistic regression.
    It is likely to require hyperparameter tuning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`SGDClassifier` 可能不如逻辑回归表现好。它可能需要调整超参数。
- en: Getting ready
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this section, we''re going to use a dataset that contains information on
    default payments, demographics, credit data, payment history, and bill statements
    of credit card clients in Taiwan from April 2005 to September 2005\. This dataset
    is taken from the UCI ML repository and is available at GitHub:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个包含2005年4月至2005年9月台湾信用卡客户逾期支付、人口统计、信用数据、支付历史和账单信息的数据集。这个数据集来自UCI
    ML存储库，可在GitHub上找到：
- en: 'We will start by importing the required libraries:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的库：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We set our working directory with the `os.chdir()` command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `os.chdir()` 命令设置我们的工作目录：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s read our data. We will prefix the name of the DataFrame with `df_` to
    make it easier to read:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取我们的数据。我们将以 `df_` 为前缀命名DataFrame，使其更容易阅读：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will now move on to look at building our model using `SGDClassifier()`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 `SGDClassifier()` 来构建我们的模型：
- en: How to do it...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s start by looking at the variables and data types:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看变量和数据类型开始：
- en: 'First, we''re going to take a look at our dataset using the `read_csv()` function:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用 `read_csv()` 函数查看我们的数据集：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will take a look at the datatypes using `dtypes`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `dtypes` 来查看数据类型：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will drop the ID column as we do not need this here:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将删除ID列，因为我们在这里不需要它：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the previous section, we saw how to explore correlations among the variables.
    We will skip this here, but readers are advised to check for correlation as multicollinearity
    might have an impact on the model.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何探索变量之间的相关性。这里我们将跳过这一部分，但建议读者检查相关性，因为多重共线性可能会影响模型。
- en: 'However, we will check if there are any null values, as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，我们将检查是否存在任何空值，如下所示：
- en: '[PRE25]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will then separate the predictor and response variables. We will also split
    our training and testing data:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将分离预测变量和响应变量。我们还将分割我们的训练数据和测试数据：
- en: '[PRE26]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We standardize our predictor variables using `StandardScaler()`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `StandardScaler()` 标准化预测变量：
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then move our model using `SGDClassifier()`:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接着使用 `SGDClassifier()` 移动我们的模型：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We separate out the probabilities of one class. In this case, we will look
    at class 1:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将分离出一个类别的概率。在这种情况下，我们将查看类别1：
- en: '[PRE29]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We check the accuracy of our model on the training data:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查模型在训练数据上的准确性：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We can then see the **area under curve** (**AUC**) value of the **receiver
    operating characteristic** (**ROC**) curve:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以接着看到**曲线下面积**（**AUC**）值和**接收者操作特征**（**ROC**）曲线：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We plot our ROC curve as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如下绘制我们的ROC曲线：
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following graph shows the ROC curve with the AUC value annotated on it:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了带有AUC值的ROC曲线：
- en: '![](img/1e8dc280-5e3d-4290-9b75-67691fa3be5e.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e8dc280-5e3d-4290-9b75-67691fa3be5e.png)'
- en: The model can be improved by tuning the hyperparameters. It can also be improved
    through feature selection.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整超参数，可以提高模型。也可以通过特征选择来改进。
- en: How it works...
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we looked at the dimensions of our dataset. In *Step 2*, we took
    a glimpse at the datatypes of the variables and noticed that all our variables
    were numeric in nature. In *Step 3*, we dropped the ID column since it is of no
    use for our exercise. We skipped looking at the correlations between the variables,
    but it is recommended that the reader adds this step in order to fully understand
    and analyze the data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在**步骤1**中，我们查看了我们数据集的维度。在**步骤2**中，我们瞥了一眼变量的数据类型，并注意到所有变量都是数值型的。在**步骤3**中，我们删除了ID列，因为它对我们的练习没有用。我们跳过了查看变量之间的相关性，但建议读者添加这一步，以便完全理解和分析数据。
- en: In *Step 4*, we moved on to check whether we had any missing values in our dataset.
    We noticed that our dataset had no missing values in this case. In *Step 5*, we
    separated the predictor and response variable and also split our dataset into
    a training dataset, which was 70% of the data, and a testing dataset, which was
    30% of the data. In *Step 6,* we used `StandardScaler()` from `sklearn.metrics`
    to standardize our predictor variables in both the training and testing datasets.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在**步骤 4**中，我们继续检查数据集中是否有缺失值。在这种情况下，我们注意到数据集中没有缺失值。在**步骤 5**中，我们将预测变量和响应变量分开，并将数据集分为训练数据集，占数据的
    70%，以及测试数据集，占数据的 30%。在**步骤 6**中，我们使用来自 `sklearn.metrics` 的 `StandardScaler()`
    对训练和测试数据集中的预测变量进行标准化。
- en: After that, in *Step 7*, we used `SGDClassifier()` from `sklearn.linear_model`
    to build our logistic regression model using the stochastic gradient descent method.
    We set our hyperparameters, such as alpha, loss, `max_iter`, and penalty. We set
    `loss='log'` in order to use the SGDClassifier for logistic regression. We used `predict_proba()`
    to predict the probabilities for our test observations, which provided us with
    the probabilities of both classes for all the test observations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，在**步骤 7**中，我们使用来自 `sklearn.linear_model` 的 `SGDClassifier()` 通过随机梯度下降法构建我们的逻辑回归模型。我们设置了超参数，如
    alpha、loss、`max_iter` 和 penalty。我们将 `loss='log'` 设置为使用 SGDClassifier 进行逻辑回归。我们使用
    `predict_proba()` 来预测测试观察结果的概率，这为我们提供了所有测试观察结果两个类别的概率。
- en: With `loss` set to `hinge`, `SGDClassifier()` provides a linear SVM. (We will
    cover SVMs in the upcoming section). The loss can be set to other values, such
    as `squared_hinge`, which is the same as `hinge` but is quadratically penalized.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `loss` 设置为 `hinge`，`SGDClassifier()` 提供了线性 SVM。（我们将在下一节中介绍 SVM）。损失可以设置为其他值，例如
    `squared_hinge`，它与 `hinge` 相同，但进行了二次惩罚。
- en: In *Steps 8* and *9*, we filtered out the probabilities for class 1 and looked
    at our model score. In *Steps 10* and *11*, we looked at the AUC value and plotted
    our ROC curve. We will explore more about hyperparameter tuning for each technique
    in upcoming sections.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在**步骤 8**和**步骤 9**中，我们过滤掉了类别 1 的概率，并查看我们的模型得分。在**步骤 10**和**步骤 11**中，我们查看 AUC
    值并绘制了 ROC 曲线。我们将在接下来的章节中更深入地探讨每种技术的高斯参数调整。
- en: See also
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: You might have noticed that, in *Step 7*, we used a hyperparameter penalty of
    `l2`. The penalty is the regularization term and `l2` is the default value. The
    hyperparameter penalty can also be set to `l1`; however, that may lead to a sparse
    solution, pushing most coefficients to zero. More information about this topic
    can be found at the following link: [https://bit.ly/2RjbSwM](https://bit.ly/2RjbSwM)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在**步骤 7**中，我们使用了 `l2` 的超参数惩罚。惩罚是正则化项，`l2` 是默认值。超参数惩罚也可以设置为 `l1`；然而，这可能会导致稀疏解，将大多数系数推向零。更多关于这个主题的信息可以在以下链接中找到：[https://bit.ly/2RjbSwM](https://bit.ly/2RjbSwM)
- en: The scikit-learn guide to classification metrics: [https://bit.ly/2NUJl2C](https://bit.ly/2NUJl2C)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn 分类度量指南：[https://bit.ly/2NUJl2C](https://bit.ly/2NUJl2C)
- en: Naive Bayes
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单贝叶斯
- en: The **Naive Bayes algorithm** is a probabilistic learning method. It is known
    as **Naive** because it assumes that all events in this word are independent,
    which is actually quite rare. However, in spite of this assumption, the Naive
    Bayesian algorithm has proven over time to provide great performance in terms
    of its prediction accuracy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯算法**是一种概率学习方法。它被称为**朴素**，因为它假设这个单词中的所有事件都是独立的，这实际上是非常罕见的。然而，尽管有这个假设，朴素贝叶斯算法在时间上已经证明在预测准确性方面提供了很好的性能。'
- en: The Bayesian probability theory is based on the principle that the estimated
    likelihood of an event or a potential outcome should be based on the evidence
    at hand across multiple trials. Bayes’ theorem provides a way to calculate the
    probability of a given class, given some knowledge about prior observations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯概率理论基于这样一个原则，即事件或潜在结果估计的似然性应该基于多个试验中手头的证据。贝叶斯定理提供了一种计算给定类别的概率的方法，前提是了解先前的观察结果。
- en: 'This can be written as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以写成以下形式：
- en: '![](img/4c581d7c-8c29-4748-ba3b-99eb7954db06.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c581d7c-8c29-4748-ba3b-99eb7954db06.png)'
- en: 'The different elements of this theorem can be explained as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理的不同元素可以解释如下：
- en: '**p(class|observation)**: This is the probability that the class holds given
    the observation.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p(class|observation)**：这是在给定观察结果的情况下类别存在的概率。'
- en: '**P(observation)**: This is the prior probability that the training data is
    observed.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**P(observation)**：这是训练数据被观察到的先验概率。'
- en: '**p(class)**: This is the prior probability of the class.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p(class)**: 这是类别的先验概率。'
- en: '**p(observation|class)**: This is the probability of the observations given
    that the class holds.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**p(observation|class)**: 这是给定类别成立的条件下观察值的概率。'
- en: In other words, if *H* is the space for the possible hypothesis, the most probable
    hypothesis, class![](img/0a15aa65-15d1-4624-9cfb-78a865c25282.png)H, is the one
    that maximizes *p(class|observation)*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果 *H* 是可能假设的空间，那么最可能的假设，即类别！[](img/0a15aa65-15d1-4624-9cfb-78a865c25282.png)H，是最大化
    *p(class|observation)* 的那个。
- en: 'Given a new observation with attributes![](img/a99e07ac-c800-4c4d-b285-7a4dabea94b8.png),
    the Bayes algorithm classifies it as the most probable value:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个具有属性！[](img/a99e07ac-c800-4c4d-b285-7a4dabea94b8.png)的新观察值，贝叶斯算法将其分类为最可能的值：
- en: '![](img/773ce01b-85c7-4960-93eb-a4bbfbde8b68.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/773ce01b-85c7-4960-93eb-a4bbfbde8b68.png)'
- en: 'Given the conditional independence assumption, we have the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 基于条件独立性假设，我们有以下内容：
- en: '![](img/0396de06-03a5-4e98-8aea-570307939933.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0396de06-03a5-4e98-8aea-570307939933.png)'
- en: 'The prediction of the Naive Bayesian Classifier is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器的预测如下：
- en: '![](img/479f2a2b-86d2-4cb3-9d50-2bae310f93de.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/479f2a2b-86d2-4cb3-9d50-2bae310f93de.png)'
- en: Getting ready
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A Naive Bayes classifier is one of the most basic algorithms that can be applied
    in text classification problems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于朴素贝叶斯的分类器是可以在文本分类问题中应用的最基本的算法之一。
- en: In this recipe, we will use the `spam.csv` file, which can be downloaded from
    the GitHub.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用可以从 GitHub 下载的 `spam.csv` 文件。
- en: This `spam.csv` dataset has two columns. One column holds messages and the other
    column holds the message type, which states whether it is a spam message or a
    ham message. We will apply the Naive Bayes technique to predict whether a message
    is likely to be spam or ham.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `spam.csv` 数据集有两个列。一个列包含消息，另一个列包含消息类型，它表明这是一条垃圾邮件还是正常邮件。我们将应用朴素贝叶斯技术来预测一条消息是否可能是垃圾邮件或正常邮件。
- en: 'We will start by importing the required libraries:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的库：
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We set your working directory with the `os.chdir()` command:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `os.chdir()` 命令设置工作目录：
- en: '[PRE34]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s read our data. As we did in the previous sections, we will prefix the
    name of the DataFrame with `df_` so that we can read it easily:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取我们的数据。正如我们在前面的章节中所做的那样，我们将以 `df_` 为前缀命名 DataFrame，这样我们就可以轻松地读取它：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How to do it...
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Let's now move on to look at how to build our model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来了解一下如何构建我们的模型。
- en: 'After reading the data, we use the `head()` function to take a look it:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据后，我们使用 `head()` 函数查看它：
- en: '[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the following screenshot, we can see that there are two columns: labels
    and message. The output is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到有两个列：标签和消息。输出如下：
- en: '![](img/bb8ebbd7-0a60-4695-b9e9-c45f8d9c1e33.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb8ebbd7-0a60-4695-b9e9-c45f8d9c1e33.png)'
- en: 'We then use the `describe()` function to look at a few metrics in each of the
    columns:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后使用 `describe()` 函数查看每个列的一些指标：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This gives us the following metrics:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下指标：
- en: '![](img/da9a2829-efe3-493d-931c-789a261bb4e4.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da9a2829-efe3-493d-931c-789a261bb4e4.png)'
- en: For the object datatype, the result of `describe()` will provide `metrics`,
    `count`, `unique`, `top`, and `freq`. `top` refers to the most common value, while
    `freq` is the frequency of this value.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对象数据类型，`describe()` 函数的结果将提供 `metrics`、`count`、`unique`、`top` 和 `freq`。`top`
    指的是最常见的值，而 `freq` 是这个值的频率。
- en: 'We can also take a look at the metrics by message type, as follows:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以查看按消息类型划分的指标，如下所示：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'With the preceding command, we see the count, number of unique values, and
    frequency for each class of the target variable:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的命令，我们看到目标变量的每个类别的计数、唯一值的数量和频率：
- en: '![](img/70ed29ff-2391-4a27-9fa3-5787c278c298.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70ed29ff-2391-4a27-9fa3-5787c278c298.png)'
- en: 'To analyze our dataset even further, let''s take a look at the word count and
    the character count for each message:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进一步分析我们的数据集，让我们看看每个消息的单词数和字符数：
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `lambda` function is used to create small, anonymous functions in Python.
    A `lambda` function can take any number of arguments, but can only have one expression.
    This function is passed as a parameter to other functions, such as `map`, `apply`,
    `reduce`, or `filter`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda` 函数用于在 Python 中创建小型、匿名函数。一个 `lambda` 函数可以接受任意数量的参数，但只能有一个表达式。这个函数作为参数传递给其他函数，如
    `map`、`apply`、`reduce` 或 `filter`。'
- en: 'The output of the preceding code will look as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出将如下所示：
- en: '![](img/c4816fd7-3a96-4529-9c72-baaa7691844d.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4816fd7-3a96-4529-9c72-baaa7691844d.png)'
- en: 'In this case, `labels` is our target variable. We have two classes: `spam`
    and `ham`. We can see the distribution of spam and ham messages using a bar plot:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，`labels`是我们的目标变量。我们有两个类别：`spam`和`ham`。我们可以使用条形图查看spam和ham消息的分布：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following is the output of the preceding code:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是前面代码的输出：
- en: '![](img/05b69426-36e2-4535-8c53-5c6ffc1df48f.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b69426-36e2-4535-8c53-5c6ffc1df48f.png)'
- en: 'In the following code block, we will label `spam` as `1`, and `ham` as `0`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们将`spam`标记为`1`，将`ham`标记为`0`：
- en: '[PRE41]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Notice that, in the following screenshot, under the `labels` variable, all
    ham and spam messages are now labelled as 0 and 1 respectively:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在下面的屏幕截图下，在`labels`变量下，所有ham和spam消息现在分别标记为0和1：
- en: '![](img/524cff97-dbb5-4df5-973d-6c638d867809.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/524cff97-dbb5-4df5-973d-6c638d867809.png)'
- en: 'We will now split our data into training and testing samples:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将数据分为训练样本和测试样本：
- en: '[PRE42]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We need to convert the collection of messages to a matrix of token counts.
    This can be done using `CountVectorizer()`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将消息集合转换为标记计数的矩阵。这可以通过使用`CountVectorizer()`来完成：
- en: '[PRE43]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We proceed to build our model with the Naive Bayes algorithm:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续使用朴素贝叶斯算法构建我们的模型：
- en: '[PRE44]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We load the required libraries for the evaluation metrics, as follows:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载用于评估指标的所需库，如下所示：
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We now check our accuracy by evaluating the model with the training data:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在通过使用训练数据评估模型来检查我们的准确性：
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output of this is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出的结果如下：
- en: '![](img/04a4ca07-c4d9-4266-8681-0165d9e1f05b.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04a4ca07-c4d9-4266-8681-0165d9e1f05b.png)'
- en: 'Now we check the accuracy of our test data by evaluating the model with the
    unseen test data:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们通过使用未见过的测试数据评估模型来检查我们的测试数据准确性：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With the preceding code block, we print performance metrics as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码块，我们按以下方式打印性能指标：
- en: '![](img/e4ba0b37-2483-4806-99a9-5da4a042a38e.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4ba0b37-2483-4806-99a9-5da4a042a38e.png)'
- en: These results may vary with different samples and hyperparameters.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果可能因不同的样本和超参数而有所不同。
- en: How it works...
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1,* we looked at our dataset. In *Step 2* and *Step **3*, we looked
    at the statistics for the `ham` and spam class labels. In *Step 4*, we extended
    our analysis by looking at the word count and the character count for each of
    the messages in our dataset. In *Step 5*, we saw the distribution of our target
    variables (ham and spam), while in *Step 6* we encoded our class labels for the
    target variable with the numbers `1` and `0`. In S*tep* 7, we split our dataset
    into training and testing samples. In *Step 8*, we used `CountVectorizer()` from
    `sklearn.feature_extraction.text` to convert the collection of messages to a matrix
    of token counts.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们查看我们的数据集。在*步骤2*和*步骤3*中，我们查看`ham`和垃圾邮件类标签的统计信息。在*步骤4*中，我们通过查看数据集中每条消息的单词计数和字符计数来扩展我们的分析。在*步骤5*中，我们看到了目标变量（ham和垃圾邮件）的分布，而在*步骤6*中，我们使用数字`1`和`0`对目标变量的类标签进行编码。在*步骤7*中，我们将数据集分为训练样本和测试样本。在*步骤8*中，我们使用`sklearn.feature_extraction.text`中的`CountVectorizer()`将消息集合转换为标记计数的矩阵。
- en: If you do not provide a dictionary in advance and do not use an analyzer that
    does some kind of feature selection, then the number of features will be equal
    to the vocabulary size found by analyzing the data. For more information on this,
    see the following: [https://bit.ly/1pBh3T1](https://bit.ly/1pBh3T1).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你事先没有提供字典，也没有使用进行某种特征选择的分析器，那么特征的数量将与通过分析数据找到的词汇表大小相等。有关更多信息，请参阅以下链接：[https://bit.ly/1pBh3T1](https://bit.ly/1pBh3T1)。
- en: In *Step 9* and *Step **10*, we built our model and imported the required classes
    from `sklearn.metrics` to measure the various scores respectively. In *Step 11*
    and *12*, we checked the accuracy of our training and testing datasets.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤9*和*步骤10*中，我们构建了我们的模型，并从`sklearn.metrics`中导入所需的类来分别测量各种分数。在*步骤11*和*步骤12*中，我们检查了训练和测试数据集的准确性。
- en: There's more...
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The Naive Bayes algorithm comes in multiple variations. These include the Multivariate
    Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Multinomial Naive
    Bayes algorithms. These variations can be applied to solve different problems.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法有多种变体。这包括多元伯努利朴素贝叶斯、多项式朴素贝叶斯和高斯多项式朴素贝叶斯算法。这些变体可以应用于解决不同的问题。
- en: '**Multivariate Bernoulli Naive Bayes**: This algorithm is used when the feature
    vectors provide a binary representation of whether a word or feature occurs in
    the document or not. Every token in the feature vector of a document is associated
    with either the `1` or `0` values. `1` represents a token in which the word occurs,
    and `0` represents a token in which the word does not occur. The Multivariate
    Bernoulli Naive Bayes algorithm can be used in situations in which the absence
    of a particular word matters, such as in the detection of spam content.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元伯努利朴素贝叶斯**: 当特征向量提供文档中单词或特征是否出现时的二进制表示时，使用此算法。文档特征向量中的每个标记都与`1`或`0`值相关联。`1`表示单词出现的标记，而`0`表示单词不出现的标记。多元伯努利朴素贝叶斯算法可用于特定单词缺失很重要的情况，例如在垃圾邮件内容的检测中。'
- en: '**Multinomial Naive Bayes**: This is used when multiple occurrences of words
    are to be considered in classification problems. In this variation, text documents
    are characterized by the frequency of the term, instead of binary values. Frequency
    is a discrete count that refers to how many times a given word or token appears
    in a document. The Multinomial Naive Bayes algorithm can be used for topic modeling,
    which is a method for finding a group of words that best represent the key information
    in a corpus of documents.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式朴素贝叶斯**: 当在分类问题中考虑单词的多次出现时使用。在这个变体中，文本文档由术语的频率来表征，而不是二进制值。频率是一个离散计数，指的是给定单词或标记在文档中出现的次数。多元多项式朴素贝叶斯算法可用于主题建模，这是一种在文档语料库中找到最能代表关键信息的一组单词的方法。'
- en: '**Gaussian Multinomial Naive Bayes**:In scenarios where we have continuous
    features, one way to deal with continuous data in Naive Bayes classifications
    is to discretize the features. Alternatively, we can apply the Gaussian Multinomial
    Naive Bayesalgorithm.This assumes the features follow a normal distribution and
    uses a Gaussian kernel to calculate the class probabilities.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯多项式朴素贝叶斯**: 在我们拥有连续特征的情况下，处理朴素贝叶斯分类中的连续数据的一种方法是将特征离散化。或者，我们可以应用高斯多项式朴素贝叶斯算法。这假设特征遵循正态分布，并使用高斯核来计算类概率。'
- en: See also
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'In scikit-learn, `CountVectorizer()` counts the number of times a word shows
    up in the document and uses that value as its weight. You can also use `TfidfVectorizer()`,
    where the weight assigned to each token depends on both its frequency in a document
    and how often the term recurs in the entire corpus. You can find more on `TfidfVectorizer`
    at the following link: [https://bit.ly/2sJCoVN](https://bit.ly/2sJCoVN).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`CountVectorizer()`计算单词在文档中出现的次数，并使用该值作为其权重。您还可以使用`TfidfVectorizer()`，其中分配给每个标记的权重取决于其在文档中的频率以及整个语料库中术语出现的频率。您可以在以下链接中找到有关`TfidfVectorizer`的更多信息：[https://bit.ly/2sJCoVN](https://bit.ly/2sJCoVN)。
- en: The scikit-learn documentation on the Naive Bayes classifier for multivariate
    Bernoulli models: [https://bit.ly/2y3fASv](https://bit.ly/2y3fASv).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn关于多元伯努利朴素贝叶斯分类器的文档：[https://bit.ly/2y3fASv](https://bit.ly/2y3fASv)。
- en: The scikit-learn documentation on the Naive Bayes classifier for multinomial
    models: [https://bit.ly/2P4Ohic](https://bit.ly/2P4Ohic).
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn关于多项式朴素贝叶斯分类器的文档：[https://bit.ly/2P4Ohic](https://bit.ly/2P4Ohic)。
- en: Decision trees
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees, a non-parametric supervised learning method, are popular algorithms
    used for predictive modeling. The most well-known decision tree algorithms include
    the **iterative dichotomizer** (**ID3**), C4.5, CART, and C5.0. ID3 is only applicable
    for categorical features. C4.5 is an improvement on ID3 and has the ability to
    handle missing values and continuous attributes. The tree-growing process involves
    finding the best split at each node using the information gain. However, the C4.5
    algorithm converts a continuous attribute into a dichotomous categorical attribute
    by splitting at a suitable threshold value that can produce maximum information
    gain.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树，一种非参数监督学习方法，是用于预测建模的流行算法。最著名的决策树算法包括**迭代二分器**（**ID3**）、C4.5、CART和C5.0。ID3仅适用于分类特征。C4.5是ID3的改进，具有处理缺失值和连续属性的能力。树生长过程涉及在每个节点使用信息增益找到最佳分割。然而，C4.5算法通过在可以产生最大信息增益的合适阈值处分割，将连续属性转换为二分分类属性。
- en: Leo Breiman, a distinguished statistician, introduced a decision tree algorithm called
    the **Classification and Regression Tree** (**CART**). CART, unlike ID3 and C4.5,
    can produce decision trees that can be used for both classification and regression
    problems. This algorithm also forms the basis for the important random forest algorithm.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 著名统计学家Leo Breiman提出了一种名为**分类与回归树**（**CART**）的决策树算法。与ID3和C4.5不同，CART可以生成既能用于分类又能用于回归问题的决策树。此算法也是重要随机森林算法的基础。
- en: Decision trees are built using recursive partitioning, which splits the data
    into subsets based on several dichotomous independent attributes. This recursive
    process may split the data multiple times until the splitting process terminates
    after a particular stopping criterion is reached. The best split is the one that
    maximizes a splitting criterion. For classification learning, the techniques used
    as the splitting criterion are entropy and information gain, the Gini index, and
    the gain ratio. For regression tasks, however, standard deviation reduction is
    used.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是通过递归分割构建的，它根据几个二分独立属性将数据分割成子集。此递归过程可能会多次分割数据，直到达到特定的停止标准后分割过程终止。最佳的分割是最大化分割标准的分割。对于分类学习，用作分割标准的技巧是熵、信息增益、基尼指数和增益率。然而，对于回归任务，则使用标准差减少。
- en: The C4.5 and C5.0 algorithms use entropy (also known as **Shannon entropy**)
    and information gain to identify the optimal attributes and decide on the splitting
    criterion. Entropy is a probabilistic measure of uncertainty or randomness.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: C4.5和C5.0算法使用熵（也称为**香农熵**）和信息增益来识别最佳属性并决定分割标准。熵是衡量不确定性或随机性的概率度量。
- en: 'Mathematically, entropy can be expressed as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，熵可以表示如下：
- en: '![](img/4048cca7-177c-491b-9c35-cfc9b0465f38.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4048cca7-177c-491b-9c35-cfc9b0465f38.png)'
- en: In the case of a two-class attribute, entropy can range from 0 to 1\. For an
    n-class attribute, entropy can take values between 0 to ![](img/f16f2000-eeff-4579-9301-f9ea353aa172.png). For
    a homogeneous variable, where there is just a single class, the entropy would
    be zero because the probability of that class being zero is 1 and ![](img/deeeb9d3-eb9f-4989-af93-113b21e38502.png) .
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类属性的情况下，熵的范围从0到1。对于n分类属性，熵可以取0到![图片](img/f16f2000-eeff-4579-9301-f9ea353aa172.png)之间的值。对于只有一个类的同质变量，熵为零，因为该类为零的概率是1，![图片](img/deeeb9d3-eb9f-4989-af93-113b21e38502.png)。
- en: To use entropy to identify the most identified attributes at which to split,
    the algorithm calculates the change in homogeneity that would result from the
    split at each possible attribute. This change is known as information gain. Constructing
    a decision tree is all about finding the attribute that returns the highest information
    gain. This information gain is based on the decrease in entropy after a dataset
    is split at an attribute.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用熵来识别分割的最佳属性，算法会计算在每个可能的属性上分割所导致的同质性变化。这种变化被称为信息增益。构建决策树就是寻找能够返回最高信息增益的属性。这种信息增益是基于在属性上分割数据集后熵的减少。
- en: 'Information gain is calculated as the difference between the entropy before
    the split and the entropy after the split:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益被计算为分割前后的熵之差：
- en: '![](img/95ffc038-e4e1-47f6-a3c9-e896b3a52ecb.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/95ffc038-e4e1-47f6-a3c9-e896b3a52ecb.png)'
- en: The higher the information gain, the better a feature is. Information gain is
    calculated for all features. The algorithm chooses the feature with the highest
    information gain to create the root node. The information gain is calculated at
    each node to select the best feature for that node.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益越高，特征越好。算法会对所有特征计算信息增益。算法选择信息增益最高的特征来创建根节点。在每个节点上计算信息增益以选择该节点的最佳特征。
- en: Information gain is also known as Kullback-Leibler divergence. This measures
    the difference between two probability distributions over the same variable. Put
    simply, if you have two probability distributions, the KL divergence measures
    the similarity of the two distributions. If the KL divergence is 0, the two distributions
    are equal.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益也称为Kullback-Leibler散度。这衡量的是两个概率分布在同一变量上的差异。简单来说，如果你有两个概率分布，KL散度衡量这两个分布的相似性。如果KL散度为0，则两个分布相等。
- en: 'The Gini index is a measure of the degree of impurity and can also be used to
    identify the optimal attributes for the splitting criterion. It is calculated
    as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Gini指数是衡量不纯度程度的指标，也可以用来识别用于分割标准的最佳属性。其计算方法如下：
- en: '![](img/0e2db890-f8f0-407b-874a-b8ad80ec9f2d.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e2db890-f8f0-407b-874a-b8ad80ec9f2d.png)'
- en: In the preceding formula, *p* is the probability of a training instance belonging
    to a particular class. With regards to the Gini index, the lower the impurity,
    the better it is.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*p*是训练实例属于特定类的概率。关于Gini指数，不纯度越低，越好。
- en: Getting ready
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To build our model with q decision tree algorithm, we will use the `backorders.csv`
    file, which can be downloaded from the following GitHub.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用q决策树算法构建我们的模型，我们将使用`backorders.csv`文件，该文件可以从以下GitHub下载。
- en: 'This dataset has 23 columns. The target variable is `went_on_backorder`. This
    identifies whether a product has gone on back order. The other 22 variables are
    the predictor variables. A description of the data is provided in the code that
    comes with this book:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集有23列。目标变量是`went_on_backorder`。它标识了一个产品是否已经进入补货。其他22个变量是预测变量。数据描述包含在此书的代码中：
- en: 'We will start by importing the required libraries:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的库：
- en: '[PRE48]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We set our working directory with the `os.chdir()` command:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`os.chdir()`命令设置工作目录：
- en: '[PRE49]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s read our data. As we have done previously, we are going to prefix the
    name of the DataFrame with `df_` to make it easier to understand:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取数据。和之前一样，我们将DataFrame的名称前缀设置为`df_`以使其更容易理解：
- en: '[PRE50]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How to do it...
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s now move on to building our model:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续构建我们的模型：
- en: 'First, we want to look at the dimensions of the dataset and the data using
    the `shape` and `head()` functions. We also take a look at the statistics of the
    numeric variables using `describe()`:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们想通过`shape`和`head()`函数查看数据集的维度和数据。我们还使用`describe()`查看数值变量的统计信息：
- en: '[PRE51]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If you get your output in scientific notation, you can change to view it in
    standard form instead by executing the following command: `pd.options.display.float_format
    = ‘{:.2f}’.format`'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的输出以科学记数法显示，你可以通过执行以下命令将其更改为标准形式：`pd.options.display.float_format = ‘{:.2f}’.format`
- en: 'With `dtypes`, we get to see the data types of each of the variables:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`dtypes`，我们可以看到每个变量的数据类型：
- en: '[PRE52]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We can see that `sku` is an identifier and will be of no use to us for our
    model-building exercise. We will, therefore, drop `sku` from our DataFrame as
    follows:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到`sku`是一个标识符，在我们的模型构建练习中不会对我们有任何帮助。因此，我们将`sku`从DataFrame中删除，如下所示：
- en: '[PRE53]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can check whether there are any missing values with the `isnull().sum()`
    command:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`isnull().sum()`命令检查是否有任何缺失值：
- en: '[PRE54]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We take a look at the following screenshot:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看一下以下截图：
- en: '![](img/5d2293ce-455b-424b-9f7b-3f62144d8dc5.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d2293ce-455b-424b-9f7b-3f62144d8dc5.png)'
- en: 'Since the number of missing values in the `lead_time` variable is about 5%,
    we will remove all the observations where `lead_time` is missing for our initial
    analysis:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`lead_time`变量中缺失值的数量大约为5%，我们将删除所有`lead_time`缺失的观测值以进行初步分析：
- en: '[PRE55]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We now need to encode our categorical variables. We select only the categorical
    variables and call `pd.get_dummies()` to dummy-code the non-numeric variables:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要编码我们的分类变量。我们只选择分类变量，并调用`pd.get_dummies()`对非数值变量进行虚拟编码：
- en: '[PRE56]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'With the preceding code, we get to see the datatypes. We notice that dummy-coded
    variables are all of the unsigned integer (`uint8`) type:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的代码，我们可以看到数据类型。我们注意到虚拟编码的变量都是无符号整数(`uint8`)类型：
- en: '![](img/6d4f6117-f192-4a9a-b747-39ae88f81f8c.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d4f6117-f192-4a9a-b747-39ae88f81f8c.png)'
- en: 'We will then look at our target variable distribution as follows:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将如下查看目标变量的分布：
- en: '[PRE57]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We can see that our data has a fairly balanced distribution, with approximately
    81% of the observations belonging to class 0 and 19% belonging to class 1:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的数据分布相当平衡，大约81%的观测值属于类别0，19%属于类别1：
- en: '![](img/383927b2-0b30-4b40-8d34-32912524e6f9.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/383927b2-0b30-4b40-8d34-32912524e6f9.png)'
- en: 'We will now split our data into training and testing datasets:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将数据集分为训练集和测试集：
- en: '[PRE58]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We will build our first model with `DecisionTreeClassifier()`:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`DecisionTreeClassifier()`构建我们的第一个模型：
- en: '[PRE59]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With `model_DT_Gini`, we can see the default values of the hyperparameters
    that have been used:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`model_DT_Gini`，我们可以看到已使用的超参数的默认值：
- en: '![](img/bb169144-daba-48b2-8cf7-9559c5d93ac6.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb169144-daba-48b2-8cf7-9559c5d93ac6.png)'
- en: 'We can use the model to predict our class labels using both our training and
    our testing datasets:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用模型来使用我们的训练集和测试集预测我们的类别标签：
- en: '[PRE60]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This gives us the accuracy along with the count of **True Negative** (**TN**),
    **False Positive** (**FP**), **False Negative** (**FN**), and **True Positive**
    (**TP**) values:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了准确率以及**真阴性**（**TN**）、**假阳性**（**FP**）、**假阴性**（**FN**）和**真阳性**（**TP**）值的计数：
- en: '![](img/d44ff442-1bd0-4719-9164-291d7426b989.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d44ff442-1bd0-4719-9164-291d7426b989.png)'
- en: 'We will now use a `plot_confusion_matrix` function to plot our confusion matrix.
    This function is taken from [http://scikit-learn.org](http://scikit-learn.org)
    and is readily available there, so we won''t show this function here. It is, however,
    provided with the code in the book for your reference:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将使用`plot_confusion_matrix`函数来绘制我们的混淆矩阵。此函数来自[http://scikit-learn.org](http://scikit-learn.org)，在那里它很容易获得，所以我们不会在这里展示此函数。然而，它已包含在书中的代码中供您参考：
- en: '[PRE61]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We can then see the amount of TNs, FPs, FNs, and TPs in our confusion matrix
    plot:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在混淆矩阵图中看到TNs、FPs、FNs和TPs的数量：
- en: '![](img/1616f4ad-61b7-4a4e-af45-f6c8de1ea3b5.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1616f4ad-61b7-4a4e-af45-f6c8de1ea3b5.png)'
- en: 'We can change the hyperparameters to tune our model. We can also perform a
    grid search to find the hyperparameter values that supply optimum results. We
    can use the following code to set the hyperparameter values:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以更改超参数来调整我们的模型。我们也可以执行网格搜索以找到提供最佳结果的超参数值。我们可以使用以下代码来设置超参数值：
- en: '[PRE62]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We will use `GridSearchCV()` to grid search the parameters:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`GridSearchCV()`来进行参数网格搜索：
- en: '[PRE63]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'After running the preceding command, we can see the best parameter values among
    those provided using `best_params_`:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行上述命令后，我们可以看到使用`best_params_`提供的最佳参数值：
- en: '[PRE64]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'You can use the model that is selected using the `GridSearchCV()` function:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用使用`GridSearchCV()`函数选择的模型：
- en: '[PRE65]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In order to see the metrics per-label, we can also use the `classification_report`,
    as follows:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查看每个标签的指标，我们还可以使用`classification_report`，如下所示：
- en: '[PRE66]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This step gives us the following output:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤给出了以下输出：
- en: '![](img/5a15b729-30a8-4a17-b26d-81c41d1636f9.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a15b729-30a8-4a17-b26d-81c41d1636f9.png)'
- en: These results will vary depending on the samples used and the hyperparameter
    tuning.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果将根据所使用的样本和超参数调整而变化。
- en: How it works...
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we took a look at the dimensions of our dataset. We also saw the
    statistics of our numerical variables. In *Step 2*, we looked at the datatypes
    of each of our variables. In *Step 3*, we dropped the `sku` attribute, because
    it is an identifier that will be of no use to us for our model. In *Step 4*, we
    checked for missing values and noticed that the `lead_time` attribute had 3,403
    missing values, which is roughly 5% of the total number of observations. In *Step
    5*, we dropped the observations for which the `lead_time` had missing values.
    Note that there are various strategies to impute missing values, but we haven't
    considered these in this exercise.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们检查了数据集的维度。我们还看到了数值变量的统计信息。在*步骤2*中，我们检查了每个变量的数据类型。在*步骤3*中，我们删除了`sku`属性，因为它是一个标识符，对我们构建模型将没有帮助。在*步骤4*中，我们检查了缺失值，并注意到`lead_time`属性有3,403个缺失值，这大约是总观察值的5%。在*步骤5*中，我们删除了`lead_time`有缺失值的观测值。请注意，有各种策略可以用来插补缺失值，但在这个练习中我们没有考虑这些策略。
- en: In *Step 6*, we used `get_dummies()` from the pandas library with `drop_first=True`
    as one of the parameters to perform a k-1 dummy coding on the categorical variables. In
    *Step 7*, we took a look at the distribution of our target variable. We see the
    class labels, 0 and 1, are in the ratio of 19%-81% approximately, which is not
    very well balanced. However, we had enough observations for both classes to proceed
    to our next steps. In *Step 8*, we separated our predictor and response variables.
    We also split our dataset to create a training dataset and a testing dataset. In
    *Step 9*, we used a `DecisionTreeClassifier()` to build our model. We noted the
    default hyperparameters values and noticed that, by default, `DecisionTreeClassifier()` uses
    the Gini impurity measure as the splitting criterion.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤6*中，我们使用pandas库中的`get_dummies()`函数，并将`drop_first=True`作为参数之一，对分类变量执行k-1虚拟编码。在*步骤7*中，我们检查了目标变量的分布。我们看到类别标签0和1的大致比例为19%-81%，这并不非常平衡。然而，我们有两个类别的足够观察结果，可以继续下一步。在*步骤8*中，我们分离了预测变量和响应变量。我们还分割了数据集以创建训练集和测试集。在*步骤9*中，我们使用`DecisionTreeClassifier()`构建我们的模型。我们注意到了默认的超参数值，并注意到默认情况下，`DecisionTreeClassifier()`使用Gini不纯度作为分割标准。
- en: In *Step 10*, we used the model to predict our test sample. We took a note of
    the overall accuracy and the amount of TP, TN, FP, and FN values that we achieved.
    In *Step 11*, we used `plot_confusion_matrix()` to plot these values in the form
    of a confusion matrix. Please note that `plot_confusion_matrix()` is readily available
    at [https://bit.ly/2MdyDU9](https://bit.ly/2MdyDU9) and is also provided with
    the book in the code folder for this chapter.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤10*中，我们使用模型来预测我们的测试样本。我们记录了整体准确率和TP、TN、FP和FN值的数量。在*步骤11*中，我们使用`plot_confusion_matrix()`将这些值以混淆矩阵的形式绘制出来。请注意，`plot_confusion_matrix()`在[https://bit.ly/2MdyDU9](https://bit.ly/2MdyDU9)上即可获得，并且也包含在本章代码文件夹中。
- en: We then looked at changing the hyperparameter values to fine-tune our model.
    We performed a grid search to find the optimum hyperparameter values. In *Step
    12*, we defined the combination of values for our hyperparameters that we want
    to apply to our grid search algorithm. In *Step 13* and *14*, we used `GridSearchCV()`
    to look for the optimum hyperparameters. In *Step 15*, we used the model returned
    by the grid search to predict our test observations. Finally, in *Step 16*, we
    used `classification_report()` from `sklearn.metrics` to generate various scores
    including `precision`, `recall`, `f1-score`, and `support`.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后查看改变超参数值以微调我们的模型。我们执行网格搜索以找到最佳的超参数值。在*步骤12*中，我们定义了要应用于网格搜索算法的超参数值的组合。在*步骤13*和*步骤14*中，我们使用`GridSearchCV()`来寻找最佳的超参数。在*步骤15*中，我们使用网格搜索返回的模型来预测我们的测试观测值。最后，在*步骤16*中，我们使用`sklearn.metrics`中的`classification_report()`生成包括`precision`、`recall`、`f1-score`和`support`在内的各种分数。
- en: There's more...
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Sometimes, a model can classify training data perfectly but faces difficulty
    when working with new data. This problem is known as **overfitting**. The model
    fails to generalize to the new test data.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一个模型可以完美地分类训练数据，但在处理新数据时却面临困难。这个问题被称为**过拟合**。模型无法推广到新的测试数据。
- en: We allow a recursive splitting process to repeat until we terminate the leaf
    node because we cannot split the data further. This model would fit the training
    data perfectly but leads to poor performance. For this reason, tree-based models
    are susceptible to overfitting. To overcome this, we need to control the depth
    of our decision tree.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们允许递归分割过程重复，直到我们终止叶节点，因为我们无法进一步分割数据。这个模型会完美地拟合训练数据，但会导致性能不佳。因此，基于树的模型容易过拟合。为了克服这一点，我们需要控制决策树的深度。
- en: 'There are multiple ways to avoid overfitting. One method is to terminate the
    growth before a perfect classification of the training data is made. The following
    approaches can be adopted to implement this stopping method:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以避免过拟合。一种方法是在训练数据被完美分类之前停止增长。以下方法可以采用来实现这种停止方法：
- en: Stop when a tree reaches the maximum number of levels
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当树达到最大层数时停止
- en: Stop when a subset contains fewer than a defined number of training instances
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当子集包含少于定义的培训实例数时停止
- en: Stop when the minimum information gain is reached
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当达到最小信息增益时停止
- en: Another method is to allow the data to overfit, and then to prune the tree after
    it is constructed. This involves eliminating nodes that are not clearly relevant,
    which also minimizes the size of the decision tree.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是允许数据过拟合，然后在树构建后剪枝。这涉及到消除那些明显不相关的节点，同时也最小化了决策树的大小。
- en: See also
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: The scikit-learn documentation on the decision tree classifier: [https://bit.ly/1Ymrzjw](https://bit.ly/1Ymrzjw)
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于决策树分类器的scikit-learn文档：[https://bit.ly/1Ymrzjw](https://bit.ly/1Ymrzjw)
- en: The scikit-learn documentation on the decision tree regressor: [https://bit.ly/2xMNSua](https://bit.ly/2xMNSua)
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于决策树回归器的scikit-learn文档：[https://bit.ly/2xMNSua](https://bit.ly/2xMNSua)
- en: Support vector machines
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: A **support vector machine** (**SVM**) is a popular machine learning algorithm
    for supervised learning. It can be used for both classification and regression
    problems. In classification learning, SVM performs classifications by finding
    an optimal separating hyperplane that differentiates two classes of observations.
    If the data is linearly separable and one-dimensional, we may have a point that
    separates the data. In two-dimensional space, the data can be separated by a straight
    line, while a plane separates data in three-dimensional space. When we have more
    than three dimensions, this is called a hyperplane.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（SVM）是一种流行的监督学习机器学习算法。它可以用于分类和回归问题。在分类学习中，SVM通过找到一个最优的分离超平面来区分两个观察类。如果数据是线性可分的且是一维的，我们可能有一个点可以分离数据。在二维空间中，数据可以通过一条直线分离，而在三维空间中，数据可以通过一个平面分离。当我们有超过三个维度时，这被称为超平面。'
- en: 'For a linear SVM, a dataset *X* with *n* feature vectors is represented as
    follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性SVM，一个具有 *n* 个特征向量的数据集 *X* 表示如下：
- en: '![](img/5fb2c247-c973-48d5-a61c-081f8b4a7124.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fb2c247-c973-48d5-a61c-081f8b4a7124.png)'
- en: 'A bipolar target variable *Y* is written as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 双极目标变量 *Y* 的表示如下：
- en: '![](img/eb1770ec-6461-4385-8495-94d959b99152.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eb1770ec-6461-4385-8495-94d959b99152.png)'
- en: 'The hyperplane is given by the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 超平面由以下公式给出：
- en: '![](img/e6d2790e-d2d0-4fd6-a451-f9b13f70d3bd.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e6d2790e-d2d0-4fd6-a451-f9b13f70d3bd.png)'
- en: 'For an SVM, the two classes are represented as -1 and +1 instead of 1 and 0\.
    The hyperplane can, therefore, be written as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM，两个类别用-1和+1表示，而不是1和0。因此，超平面可以写成以下形式：
- en: '![](img/4d429da2-7ea8-41f7-9389-ddd5de0db021.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d429da2-7ea8-41f7-9389-ddd5de0db021.png)'
- en: 'To classify the data, we have the following two rules:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类数据，我们有以下两个规则：
- en: '![](img/dafe97b7-53a1-4bfb-a6f6-c96829d976f3.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dafe97b7-53a1-4bfb-a6f6-c96829d976f3.png)'
- en: However, it's quite possible that there are a lot of hyperplanes that correctly
    classify the training data. There might be infinite solutions of *w* and *b* that
    hold for the preceding rules. An algorithm such as a perceptron learning algorithm
    will just find any linear classifier. SVM, however, finds the optimal hyperplane,
    which is at a maximum distance from any data point. The further the data points
    lie from the hyperplane, the more confident we are that they have been correctly
    classified. We would therefore like the data points to be as far away from the
    hyperplane as possible, while still being able to classify them correctly. The
    best hyperplane is the one that has the maximum margin between the two classes.
    This is known as the maximum-margin hyperplane.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很可能存在许多可以正确分类训练数据的超平面。对于前面的规则，可能存在无限多个 *w* 和 *b* 的解。例如，感知器学习算法将只找到任何线性分类器。然而，SVM找到的是最优超平面，它距离任何数据点最远。数据点离超平面越远，我们对其正确分类的信心就越大。因此，我们希望数据点尽可能远离超平面，同时仍然能够正确分类它们。最佳超平面是两个类别之间具有最大边缘的超平面。这被称为最大边缘超平面。
- en: It's possible for SVM to choose the most important vectors that define the separation
    hyperplane from the training data. These are the data points that lie closest
    to the hyperplane and are known as support vectors. Support vectors are the data
    points that are hardest to classify. At the same time, these represent high-quality
    data. If you remove all the other data points and use only the support vectors,
    you can get back the exact decision hyperplane and the margin using the same SVM
    model. The number of data points does not really matter, just the support vectors.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）可以从训练数据中选择定义分离超平面的最重要的向量。这些是距离超平面最近的点，被称为支持向量。支持向量是分类最困难的点。同时，这些也是高质量的数据。如果你移除所有其他数据点，只使用支持向量，你可以使用相同的SVM模型恢复出精确的决策超平面和边缘。数据点的数量并不重要，重要的是支持向量。
- en: 'We normalize the weights *w* and *b* so that the support vectors satisfy the
    following condition:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将权重 *w* 和 *b* 归一化，使得支持向量满足以下条件：
- en: '![](img/b7b23e99-c87b-4838-b936-059d6ee20202.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7b23e99-c87b-4838-b936-059d6ee20202.png)'
- en: 'As a result, the classification rules change to the following:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分类规则变为以下：
- en: '![](img/fb754845-562f-4184-8d35-c979706a615c.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fb754845-562f-4184-8d35-c979706a615c.png)'
- en: 'The preceding equations can be combined and represented as follows:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程可以组合并表示如下：
- en: '![](img/2fb465eb-6c11-4010-ada3-8a5e1df77834.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fb465eb-6c11-4010-ada3-8a5e1df77834.png)'
- en: The initial SVM algorithms could only be used in the case of linearly separable
    data. These are known as hard-margin SVMs. However, hard-margin SVMs can work
    only when the data is completely linearly separable and if doesn't have any noise.
    In the case of noise or outliers, a hard-margin SVM might fail.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 初始SVM算法只能在数据线性可分的情况下使用。这些被称为硬间隔SVMs。然而，硬间隔SVMs只能在数据完全线性可分且没有噪声的情况下工作。在噪声或异常值的情况下，硬间隔SVM可能会失败。
- en: 'Vladimir Vapnik proposed soft-margin SVMs to deal with data that is non-linearly separable
    by using slack variables. Slack variables allows for errors to be made while fitting
    the model to the training dataset. In hard-margin classification, we will get
    a decision boundary with a small margin. In soft-margin classification, we will
    get a decision boundary with a larger margin:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: Vladimir Vapnik提出了软间隔SVMs来处理通过松弛变量非线性可分的数据。松弛变量允许在拟合训练数据集时犯错误。在硬间隔分类中，我们将得到一个小的边缘决策边界。在软间隔分类中，我们将得到一个较大的边缘决策边界：
- en: '![](img/a46c8e11-1bfb-4664-98ab-008ff1d437df.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a46c8e11-1bfb-4664-98ab-008ff1d437df.png)'
- en: 'SVMs can also perform non-linear classification extremely well using something
    called a kernel trick. This refers to transformations in which the predictor variables
    are implicitly mapped to a higher-dimensional feature space. Popular kernel types
    include the following:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs可以使用所谓的核技巧进行非常出色的非线性分类。这指的是将预测变量隐式映射到更高维特征空间的变换。流行的核类型包括以下几种：
- en: Linear kernels
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性核
- en: Polynomial kernels
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式核
- en: Radial basis function (RBF) kernels
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 径向基函数（RBF）核
- en: Sigmoid kernels
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid核
- en: Different kernel functions are available for various decision functions. We
    can add kernel functions together to achieve even more complex planes.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的核函数适用于不同的决策函数。我们可以将核函数相加，以实现更复杂的平面。
- en: Getting ready
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this chapter, we are going to use the `bank.csv` file, which is based on
    bank marketing data and which you can download from GitHub. This data is related
    to a Portuguese bank''s direct marketing campaigns that took place over phone
    calls. The goal is to predict whether the client will subscribe to a term deposit:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用`bank.csv`文件，该文件基于银行营销数据，您可以从GitHub下载。这些数据与葡萄牙银行在电话中进行直接营销活动相关。目标是预测客户是否会订阅定期存款：
- en: 'We will start by importing the required libraries:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的库：
- en: '[PRE67]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We set our working directory with the `os.chdir()` command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`os.chdir()`命令设置工作目录：
- en: '[PRE68]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s read our data. We will again prefix the name of the DataFrame with `df_`
    to make it easier to understand:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取我们的数据。我们将再次使用前缀`df_`来命名DataFrame，以便更容易理解：
- en: '[PRE69]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How to do it...
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'In this section, we''re going to look at checking null values, standardizing
    numeric values, and one-hot-encoding categorical variables:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看检查空值、标准化数值和独热编码分类变量：
- en: 'With the following command, we can see we that we have ten categorical variables
    and seven numerical variables in the dataset:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，我们可以看到在数据集中有十个分类变量和七个数值变量：
- en: '[PRE70]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'With the following command, we notice there are no missing values, so we can
    proceed with our next steps:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，我们注意到没有缺失值，因此我们可以继续下一步：
- en: '[PRE71]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can check the class balance in our target variable as follows:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以如下检查目标变量的类别平衡：
- en: '[PRE72]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We can convert our target class to the binary values 1 and 0 with the following
    command:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令将目标类别转换为二进制值1和0：
- en: '[PRE73]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can now perform one-hot encoding on our categorical variables. We only select
    variables that are categorical in nature. In the following code, we use `category_column_names`
    to provide the names of the non-numeric variables:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以对分类变量进行独热编码。我们只选择本质上是分类的变量。在下面的代码中，我们使用`category_column_names`来提供非数值变量的名称：
- en: '[PRE74]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We run a loop over each of the non-numerical variables to perform one-hot encoding
    on them and add them back to the DataFrame. We will also delete the original non-numerical
    variables after performing one-hot encoding:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对每个非数值变量运行循环，对它们进行独热编码，并将它们添加回DataFrame。我们还将删除独热编码后的原始非数值变量：
- en: '[PRE75]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'We separate the predictor and response variables as follows:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如下分离预测变量和响应变量：
- en: '[PRE76]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We also split our data into training and testing datasets:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将数据分为训练集和测试集：
- en: '[PRE77]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We then build our first model using SVC with the default kernel, **radial basis
    function** (**RBF**):'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们然后使用默认核的SVC构建我们的第一个模型，**径向基函数**（**RBF**）：
- en: '[PRE78]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We check our training and testing accuracy via the SVC model built with the
    RBF kernel:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We get the following output:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3b0f08d-0164-4b38-9bd4-f0013851e31e.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
- en: 'We can rebuild our SVC model with a polynomial kernel as follows:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We get the following output with the polynomial kernel:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6429ace-6e67-4813-b1c9-d0fd31463280.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
- en: 'We can also build an SVC model with the linear kernel. Instead of `kernel=''ploy''`,
    we can replace this with `kernel=''linear''` in the preceding code:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'With the linear kernel, we get the following accuracy:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/595facc0-1ed8-4282-a9cd-7d76ae51a0de.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
- en: Our results will vary depending on the different types of kernel and other hyperparameter
    values used.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-416
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we looked at the data types of our variables. We noticed that we
    have ten categories and seven numerical variables. In *Step 2*, we checked for
    missing values and saw that there were no missing values in our dataset. In *Step
    3*, we checked the class balance of our target variable and found out that it
    has the values of `yes` and `no`. In *Step 4*, we converted our target variable
    to 1 and 0 to represent `yes` and `no` respectively. In *Steps 5* and *6*, we
    performed one-hot encoding on the non-numerical variables.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 7*, we separate the predictor and response variables and in *Step 8*,
    we split our dataset into training and testing datasets. After that, in *Step
    9*, we used `SVC()` from `sklearn.svm` with the default RBF kernel to build our
    model. We applied it to our training and testing data to predict the class. In
    *Step 10*, we checked the accuracy of our training and testing data. In *Step
    11*, we changed our hyperparameter to set the kernel to polynomial. We noticed
    that training accuracy remained more or less the same, but the test accuracy improved.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: With the polynomial kernel, the default degree is 3\. You can change the polynomial
    degree to a higher degree and note of the change in the model's performance.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 12*, we changed the kernel to linear to see if the results improved
    compared to the polynomial kernel. We did not, however, see any significant improvement.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we have seen how to use various kernels in our code. Kernel
    functions must be symmetrical. Preferably, they should have a positive (semi)
    definite gram matrix. A gram matrix is the matrix of all the possible inner products
    of V, where V is the set of m vectors. For convenience, we consider positive semi-definite
    and positive-definite functions indifferently. In practice, a positive definiteness
    of kernel matrices ensures that kernel algorithms converge to a unique solution.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: A **linear kernel** is the simplest of all kernels available. It works well
    with text classification problems.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear kernel is presented as follows:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/523469ff-9905-47b2-b6c5-b7bafe00005c.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: Here, **c** is the constant term.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'A **polynomial kernel** has two parameters: a constant and the degree. A polynomial
    kernel with no constant and a degree of 1 is simply a linear kernel. As the degree
    of the polynomial kernel increases, the decision function becomes more complex.
    With higher degrees, it is possible to get good training accuracy, but the model
    might fail to generalize to unseen data, leading to overfitting. The polynomial
    kernel is represented as follows:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**多项式核**有两个参数：一个常数和次数。没有常数且次数为1的多项式核实际上就是一个线性核。随着多项式核次数的增加，决策函数变得更加复杂。较高的次数可以获得良好的训练精度，但模型可能无法泛化到未见过的数据，导致过拟合。多项式核表示如下：'
- en: '![](img/837f7b87-065c-485b-ab70-78c4b14d3931.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/837f7b87-065c-485b-ab70-78c4b14d3931.png)'
- en: Here, ![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png) is the slope, d is the
    degree of the kernel, and c is the constant term.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png)是斜率，d是核的次数，c是常数项。
- en: 'The **radial basis function kernel (RBF)**, also known as the Gaussian kernel,
    is a more complicated kernel and can outperform polynomial kernels. The RBF kernel
    is given as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '**径向基函数核（RBF）**，也称为高斯核，是一个更复杂的核，可以超越多项式核。RBF核如下所示：'
- en: '![](img/1a85ebd6-761c-4a94-a8dd-65bbadcb0b30.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a85ebd6-761c-4a94-a8dd-65bbadcb0b30.png)'
- en: 'The ![](img/33e24c69-55f0-4345-a50f-b8e7491d809f.png) parameter can be tuned
    to increase the performance of the kernel. This is important: with an over-estimated ![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png),
    the kernel can lose its non-linear power and behave more linearly. On the other
    hand, if ![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png) is underestimated,
    the decision function can be highly sensitive to noise in the training data.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 参数![](img/33e24c69-55f0-4345-a50f-b8e7491d809f.png)可以被调整以提高核的性能。这一点很重要：如果![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png)被高估，核可能会失去其非线性能力，表现得更加线性。另一方面，如果![](img/e1273c32-d0b3-4a18-9f6b-ea8aa5a8b505.png)被低估，决策函数可能会对训练数据中的噪声非常敏感。
- en: 'Not all kernels are strictly positive-definite. The sigmoid kernel function,
    though is quite widely used, is not positive-definite. The sigmoid function is
    given as follows:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有核函数都是严格正定的。虽然sigmoid核函数相当广泛地被使用，但它不是正定的。sigmoid函数如下所示：
- en: '![](img/cdaf800d-a3a4-489d-bf26-ff6a6a61d939.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdaf800d-a3a4-489d-bf26-ff6a6a61d939.png)'
- en: Here, ![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png) is the slope and **c**
    is the constant term. Note that an SVM with a sigmoid kernel is the same as a
    two-layer perceptron neural network.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/cd03e4d1-7215-4c12-9417-06d231ce8fec.png)是斜率，**c**是常数项。请注意，具有sigmoid核的支持向量机与双层感知器神经网络相同。
- en: Adding a kernel trick to an SVM model can give us new models. How do we choose
    which kernel to use? The first approach is to try out the RBF kernel, since it
    works pretty well most of the time. However, it is a good idea to use other kernels
    and validate your results. Using the right kernel with the right dataset can help
    you build the best SVM models.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 将核技巧添加到SVM模型中可以给我们带来新的模型。我们如何选择使用哪种核？第一种方法是尝试RBF核，因为它大多数时候都表现得相当好。然而，使用其他核并验证你的结果是个好主意。使用正确的核和正确的数据集可以帮助你构建最好的SVM模型。
- en: See also
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: More on the positive definite matrix can be found here: [https://bit.ly/2NnGeLK](https://bit.ly/2NnGeLK).
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于正定矩阵的信息可以在这里找到：[https://bit.ly/2NnGeLK](https://bit.ly/2NnGeLK)。
- en: Positive definite kernels are a generalization of the positive definite matrix.
    You can find out more about this here: [https://bit.ly/2NlsIs1](https://bit.ly/2NlsIs1).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正定核是正定矩阵的推广。你可以在这里了解更多信息：[https://bit.ly/2NlsIs1](https://bit.ly/2NlsIs1)。
- en: The scikit-learn documentation on support vector regression: [https://bit.ly/2OFZ8ix](https://bit.ly/2OFZ8ix).
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn关于支持向量回归的文档：[https://bit.ly/2OFZ8ix](https://bit.ly/2OFZ8ix)。
