<html><head></head><body>
		<div id="_idContainer318">
			<h1 id="_idParaDest-116"><em class="italic"><a id="_idTextAnchor128"/>Chapter 10</em>: AutoML in the Enterprise</h1>
			<p class="author-quote"><a id="_idTextAnchor129"/>"Harnessing machine learning can be transformational, but for it to be successful, enterprises need leadership from the top. This means understanding that when machine learning changes one part of the business — the product mix, for example — then other parts must also change. This can include everything from marketing and production to supply chain, and even hiring and incentive systems." </p>
			<p class="author-quote">– Erik Brynjolfsson, Director of the MIT Initiative on the Digital Economy</p>
			<p>Automated <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) is an enabler and an accelerator that unleashes the promise for organizations to expedite the analytics life cycle without having data scientists as a bottleneck. In earlier chapters, you learned how to perform automated ML using multiple hyperscalers, including open source tools, AWS, Azure, and GCP. </p>
			<p>This chapter, however, is quite different since here we will explore the enterprise use of automated ML. We will look into applying automated ML in real-world applications and discuss the pros and cons of such approaches. Model interpretability and transparency are areas of great interest in automated ML. We will explore models of trust in ML, providing a playbook for applying automated ML in an enterprise.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Does my organization need automated ML?</li>
				<li>Automated ML – an accelerator for enterprise advanced analytics</li>
				<li>Automated ML challenges and opportunities</li>
				<li>Establishing trust – model interpretability and transparency in automated ML</li>
				<li>Introducing automated ML in an organization</li>
				<li>Call to action – where do I go next? </li>
			</ul>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor130"/>Does my organization need automated ML?</h1>
			<p>Technology decision-makers <a id="_idIndexMarker481"/>and stakeholders don't like fads, and you probably shouldn't either. Building and using technology for the sake of technology has limited business value in a vertical enterprise; the technology has to solve a business problem or provide an innovative differentiation to be relevant. Therefore, this inquiry becomes very significant: does an organization really need automated ML or is it just one of those steps in the AI and ML maturity cycle that we can live without? Would this investment<a id="_idIndexMarker482"/> result in <strong class="bold">Return on Investment</strong> (<strong class="bold">ROI</strong>), or would it become one of those unused platforms that sounded like a good idea at the time?</p>
			<p>Let's try to answer these questions by looking at the value proposition of automated ML and see whether it makes a good fit for your organization. As a technology stakeholder, envision yourself as someone trying to build an enterprise AI playbook and deciding whether to invest in and utilize or disregard the utility of automated ML.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor131"/>Clash of the titans – automated ML versus data scientists</h2>
			<p>In an organization, large or small, you <a id="_idIndexMarker483"/>would first need to communicate this idea to your own data science team. It could be a large group of people with a mix of PhDs, ML engineers, and data scientists, or it could be a close-knit group of startup bootstrappers, whose lead data science expert is an engineer with Jupyter notebooks installed on their machine. In either case, you would need convincing arguments to present a case for automated ML.</p>
			<p>We have said this before but will gladly repeat it: automated ML is not going to kill data scientist jobs any time soon. Having said that, you would be hard-pressed to find a data scientist who acknowledges the efficiency of using feature engineering, hyperparameter optimization, or neural architecture search using automated ML. As data scientists, we tend to think, sometimes rightfully so, that data science is an art form that cannot be brute-forced. There is a lot of subject matter expertise and knowledge that goes into model tuning, and therefore it is better left with humans who know what they are doing. The problem is that this model doesn't scale:</p>
			<div>
				<div id="_idContainer316" class="IMG---Figure">
					<img src="image/Figure_10.1_B16890.jpg" alt="Figure 10.1 – Life of a data scientist&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Life of a data scientist</p>
			<p>There is a perpetual<a id="_idIndexMarker484"/> shortage of qualified data scientists and other qualified individuals who can build, tune, monitor, and deploy models at scale. Our organizations are data-rich but starving for insights. We have seen several critical business intelligence, experimentation, and insight projects pushed down the priority stack for <a id="_idIndexMarker485"/>revenue-centric programs. Organizations need automated ML to let <strong class="bold">subject matter experts</strong> (<strong class="bold">SMEs</strong>) and citizen data scientists build data experiments and test their hypotheses without needing a PhD in ML. Would they get things wrong? I am sure. However, automated ML will enable and empower them to build advanced ML models and test their hypotheses. </p>
			<p>Therefore, we must support this democratization of AI. This is not to say that a mission-critical credit risk model trained by automated ML should be rolled out to production without proper due diligence and testing – that wouldn't be the case even with handcrafted models. An organization must make sure that performance, robustness, model decay, adversarial attacks, outlier parameters, and accuracy matrices and KPIs apply to all models equally. </p>
			<p>In a nutshell, ML progress in an enterprise is slow-paced. It is tough to scale and not very well automated. Collaboration among business teams and data scientists is difficult and actual operationalized models delivering business value are few and far between. Automated ML brings the promise of solving these problems and gives additional tools to data scientists to ease the manual drudgery of feature engineering, hyperparameter optimization, and neural architecture search. </p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor132"/>Automated ML – an accelerator for enterprise advanced analytics</h1>
			<p>While building your AI <a id="_idIndexMarker486"/>playbook and reimagining the AI talent strategy for your organization, you should consider automated ML as an <a id="_idIndexMarker487"/>accelerator. The following are some of the reasons why you would want to consider using automated ML for your organization. </p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor133"/>The democratization of AI with human-friendly insights</h2>
			<p>Automated ML is<a id="_idIndexMarker488"/> rapidly becoming an inherent part of all major ML and deep learning platforms and will play an important part in democratizing advanced analytics. All major platforms tout these capabilities, but for it to be an accelerator for an enterprise, automated ML must play an important role in the democratization of AI. The toolset should enable a citizen data scientist to perform daunting ML tasks with ease and get human-friendly insights. Anything short of explainable, transparent, and repeatable AI and automated ML would not be the advanced analytics accelerator you had hoped for. </p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor134"/>Augmented intelligence</h2>
			<p>Automated ML is becoming ingrained in <a id="_idIndexMarker489"/>most modern data science platforms, and therefore will be commoditized as part of the MLOps life cycle. For a data scientist, the biggest value proposition is the ease of feature engineering, data preprocessing, algorithmic selection, and hyperparameter tuning, that is, augmenting the data scientist's skillset. MLOps platforms with built-in automated ML also provide training and tuning, model monitoring and management, and head-to-head model comparison capabilities for A/B testing. This excellent suite of tools becomes extremely helpful to strengthen a data scientist's skills, hence automated ML proves to be an augmented intelligence platform for data scientists and domain SMEs alike. </p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor135"/>Automated ML challenges and opportunities</h1>
			<p>We have discussed the benefits of automated ML, but all these advantages are not without their fair share of challenges. Automated ML is not a silver bullet and there are<a id="_idIndexMarker490"/> several scenarios where it would not work. The following are some challenges and scenarios where automated ML may not be the best fit. </p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor136"/>Not having enough data</h2>
			<p>The size of the dataset is a critical component for automated ML to work well. When feature engineering, hyperparameter optimization, and neural architectural search are used on small datasets, they do not yield good results. The dataset has to be significantly large for automated ML tools to do their job effectively. If this is not the case with your dataset, you might want to try the alternative approach of building models manually. </p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor137"/>Model performance</h2>
			<p>In a small number of cases, the performance you get from out-of-the-box models may not work – you may have to hand-tune the model for performance or apply custom heuristics for improvement. </p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor138"/>Domain expertise and special use cases</h2>
			<p>In a case where your model requires significant subject matter expertise and rules built into it, the gains from automated ML models may not provide good returns. </p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor139"/>Computational costs</h2>
			<p>Automated ML is inherently computationally expensive. If the datasets are extremely large, you might consider using local compute resources (which are arguably cheaper) to avoid the expensive costs associated with using cloud machines. In these cases, costs incurred in training the model may outweigh the optimization benefits – caveat emptor. </p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor140"/>Embracing the learning curve</h2>
			<p>Anything worth doing is never easy, such as exercise or ML. Automated ML takes the brunt of the work out of repetitive and tedious tasks, but there is still a learning curve. Most platforms dub their automated ML products as zero-code, low-code, or no-code approaches; however, you would still need to get yourself familiar with the tool. What happens when your results don't match your intuition or hypothesis based on years of subject matter expertise? How do you fine-tune the model based on identified important features? Which models perform well on training data but poorly on production datasets and why? These are practical considerations your citizen data scientists and stakeholders would need to go through as part of this learning <a id="_idIndexMarker491"/>curve. A lot of this learning and adaption would depend on the tool you select and how easy it makes life for you.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor141"/>Stakeholder adaption</h2>
			<p>Every new technology faces the challenge of adaption – and automated ML would be dead on arrival due to its very nature. Your enterprise AI strategy should include training and incorporate the learning curve and potential disruption associated with the introduction of new technology such as automated ML. Building example templates and starter kits would help get stakeholders up to speed. We have seen in practice that getting junior data scientists and developers on board helps socialize new technology.</p>
			<p>Let's proceed toward the next section, where we will discuss various techniques that can help in building trust in models trained by automated ML.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor142"/>Establishing trust – model interpretability and transparency in automated ML</h1>
			<p>Establishing trust in the model<a id="_idIndexMarker492"/> trained by automated ML can appear to be a challenging value proposition. Explaining to the business leaders, auditors, and stakeholders responsible for automated decision management that they can trust an algorithm to train and build a model that will be used for a potentially mission-critical system requires that you don't treat it any different from a "man-made" ML model. Model monitoring and observability requirements do not change based on the technique used to build the model. Reproducible model training and quality measurements, such as validating data, component integration, model quality, bias, and fairness, are also required as part of any ML development life cycle. </p>
			<p>Let's explore some of the approaches and techniques we can use to build trust in automated ML models and ensure governance measures.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor143"/>Feature importance</h2>
			<p>Feature importance, or how <a id="_idIndexMarker493"/>much a specific attribute contributes to the result of a prediction either positively or negatively, is a model inspection technique that is offered by most, if not all, automated ML frameworks. In earlier chapters, you have seen how AWS, GCP, and Azure all offer a view of feature importance scores for their trained models. This information can be used by domain SMEs and citizen data scientists to ensure the model is accurate. </p>
			<p>Feature importance is not only helpful in validating the hypothesis but may also provide insights into data that was previously unknown. Data scientists, with the help of domain SMEs, can use feature importance to ensure that it does not exhibit bias against any protected class and see whether any of the preferences are against the law. For instance, if a loan decision algorithm has a bias toward a specific gender, ethnicity, or race, that would be illegal and unethical in most scenarios. On the contrary, if a breast cancer database shows a significant gender bias, that is due to a biological construct and hence is not a societal or implicit bias to be mitigated or addressed. Testing an automated ML model, or any model for that matter, for feature importance with perturbation makes a good sanity check for correctness, robustness, and bias. </p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor144"/>Counterfactual analysis</h2>
			<p>In algorithmic explainability, counterfactual <a id="_idIndexMarker494"/>analysis belongs to the class of example-based explanations. In simple terms, it uses causal analysis to show what would have happened if an event has or has not occurred. For example, in a biased loan model, counterfactual analysis will reveal that while keeping all the other factors unchanged, modifying the ZIP code of a loan applicant has an impact on the outcome. This shows bias in the model against people in a specific geography, possibly a sign of latent racial bias manifested in the ZIP code as a proxy. Besides bias, counterfactual analysis can also reveal mistakes in <a id="_idIndexMarker495"/> model assumptions that can be quite beneficial. </p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor145"/>Data science measures for model accuracy</h2>
			<p>Standard ML <a id="_idIndexMarker496"/>approaches for performance estimation, model selection, and algorithm comparison should be applied to ensure the accuracy and robustness of the trained model. Some standard ways of validating a ML model are shown in the following figure:</p>
			<div>
				<div id="_idContainer317" class="IMG---Figure">
					<img src="image/Figure_10.2_B16890.jpg" alt="Figure 10.2 – Standard measures for data science &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Standard measures for data science </p>
			<p>For performance estimation in a large dataset, recommended approaches include checking for the confidence interval via normal approximation, as well as train/test splits. For smaller datasets, repeating k-fold cross-validation, leave-one-out cross-validation, and confidence interval testing helps to ensure good estimates for performance. For model selection with large datasets, three-way holdout methods of training validation, testing split, and repeated k-fold cross-validation with independent test sets work well. To compare the model and algorithms, applying multiple independent training and<a id="_idIndexMarker497"/> test sets for algorithm comparison, McNemar's test, and Cochran's test is prescribed, while for smaller datasets, nested cross-validations work quite effectively. </p>
			<p>For automated ML-based models to be accurate and robust, we need to ensure that we perform explainability measures across the entire life cycle. Therefore, checks need to be performed pre-modeling – that is, characterizing the input data – during the modeling – that is, designing explainable model architectures and algorithms – and post-modeling – that is, extracting explanations from outputs.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor146"/>Pre-modeling explainability </h2>
			<p>The pre-modeling<a id="_idIndexMarker498"/> explainability starts with exploratory data analysis and dataset statistical description and sanitization, that is, descriptions of the variables, metadata, provenance, statistics, between variables (pair plots and heatmaps), ground truth correlations, and probabilistic models generating synthetic data. This explainability extends to explainable feature engineering followed by interpretable prototype selections and identification of meaningful outliers. </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor147"/>During-modeling explainability </h2>
			<p>When using automated <a id="_idIndexMarker499"/>ML algorithms, choose to adopt a more explainable model family when given the option; linear models, decision trees, rule sets, decision sets, generalized additive models, and case-based reasoning methods are more explainable than complex black-box models.</p>
			<p>Hybrid explainable models are also used to design explainable model architectures and algorithms such as <strong class="bold">D</strong><strong class="bold">eep K-Nearest Neighbors</strong> (<strong class="bold">DKNN</strong>), <strong class="bold">Deep Weighted Averaging Classifier</strong> (<strong class="bold">DWAC</strong>), <strong class="bold">Self-Explaining Neural Network</strong> (<strong class="bold">SENN</strong>), <strong class="bold">Contextual Explanation Networks</strong> (<strong class="bold">CENs</strong>), and <strong class="bold">Bag-of-features Networks</strong> (<strong class="bold">BagNets</strong>). The approach of joining prediction and explanation using models such as <strong class="bold">Teaching Explanations for Decisions</strong> (<strong class="bold">TED</strong>), multimodal explanations, and rationalizing neural predictions greatly helps explain the models. Visual explanations are, of course, quite effective since a picture is worth a thousand words, unless it's in a higher dimension because then it becomes quite confusing, such as postmodern art. </p>
			<p>Explainability using architectural <a id="_idIndexMarker500"/>adjustments and regularizations as an artifact uses explainable <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>), explainable deep architecture, and <a id="_idIndexMarker501"/>attention-based models, which are being used in natural language processing, time series analysis, and computer vision. </p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor148"/>Post-modeling explainability </h2>
			<p>There are various built-in tools for post-modeling explainability that are part of automated ML toolkits and hyperscalers. These tools include macro explanations and input-based explanations or perturbations to see how input manipulations can potentially impact the outputs. Macro explanation-based approaches, such as importance scores, decision rules, decision trees, dependency plots, verbal explanations, and counterfactual explanations, are great resources for domain SMEs to get their head around the outcomes of a trained model.</p>
			<p>There are also explanation estimation methods that try to probe the proverbial black box, including perturbation-based training (LIME), backward propagation, proxy model, activation optimization, and SHAP. </p>
			<p>As described in the preceding methods, there is no singular way of establishing trust in ML models, whether they are manually trained or built via automated ML toolkits. The only way to accomplish this is by following engineering best practices; validation, reproducibility, experimental audit trail, and explainability are the best-known methods to verify and ensure it works. You may not need all these approaches as part of your ML workflow but know that these and various other approaches to validate and monitor your model throughout the life cycle are critical to ensure the success of your enterprise ML operationalization. </p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor149"/>Introducing automated ML in an organization</h1>
			<p>Now that you have reviewed the automated ML platforms and the open source ecosystem and understand how it works under the hood, wouldn't you like to introduce automated ML in your organization? So, how do you do it? Here are some pointers to guide you through the process. </p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor150"/>Brace for impact </h2>
			<p>Andrew Ng is the founder and CEO of Landing AI, the former VP and chief scientist of Baidu, the co-chairman and co-founder of Coursera, the former founder and leader of Google Brain, and an adjunct professor at Stanford University. He has written extensively about AI and ML and his courses are seminal for anyone starting out with ML and deep learning. In his HBR article on AI in the enterprise, he poses five key questions to validate whether an AI project would be successful. We believe that this applies equally well to automated ML projects. The questions you should ask are as follows:  </p>
			<ul>
				<li>Does the project give you a quick win?</li>
				<li>Is the project either too trivial or too unwieldy in size?</li>
				<li>Is your project specific to your industry? </li>
				<li>Are you accelerating your pilot project with credible partners? </li>
				<li>Is your project creating value? </li>
			</ul>
			<p>These questions help not only to choose a meaningful project but also to seek business value and technical diligence. Building a small team and appointing a leader as an automated ML champion would help further the cause. </p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor151"/>Choosing the right automated ML platform</h2>
			<p>Open source, AWS, GCP, Azure, or something else – what automated ML platform should you use?</p>
			<p>There are few considerations you should keep in mind when choosing an automated ML platform. First of all, if you are cloud-native, that is, your data and compute reside in the cloud, it would make way more sense to use the automated ML offered by that specific cloud provider for all practical purposes. If you have a hybrid model, try to keep your automated ML compute close to the data you plan to use, wherever it resides. The feature parity might not make such a significant difference; however, not having access to the right data definitely would. </p>
			<p>Generally speaking, cloud compute and storage resources for automated ML can get expensive quickly, providing you are working with large models and doing multiple simultaneous experiments. If you have compute resources and data available in an on-prem scenario, that is an ideal playground without increasing your hyperscaler's bill. However, this comes with the responsibility of setting up the infrastructure and doing the setup for an on-prem toolkit. So, if cost is not a major concern and you want to quickly explore what automated ML can do for you, cloud-based toolkits would make an ideal companion. Depending on your relationship with your cloud provider, you might also be able to get a discount. </p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor152"/>The importance of data </h2>
			<p>From the examples in previous chapters, it must have become painfully obvious that data, and a lot of it, is the single most important thing needed for a successful automated ML project. Smaller datasets do not provide good accuracy and do not make a good use case for automated ML. If you do not have large enough datasets to work with, or your use case does not lend itself well to automated ML, maybe it's not the right tool for the job. </p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor153"/>The right messaging for your audience </h2>
			<p>Automated ML promises to deliver immense value to both data scientists and SMEs; therefore, you should carefully craft your pitch. </p>
			<p>For business leaders and stakeholders, it is a business enablement tool that will help citizen data scientists to expedite development; business users can test their hypotheses and execute experiments in a faster manner.</p>
			<p>For your data science allies, you should introduce automated ML as a tool that helps augment their capabilities by taking away the repetitive and mundane tasks from their day-to-day job. Among many things, automated ML can help take away the drudgery of sifting through a dataset for important features and can help identify the right parameters and models through a large search space. It will expedite the training of new data scientists and will help other engineers who like to dabble in data science experiments gain a firm understanding of the fundamentals by doing it. If your fellow data scientists and ML engineers see value in this technology and do not feel threatened by it, they will be willing to adapt. This would not be the first time they have heard of this amazing technology and would love to use it, albeit with a healthy dose of skepticism. </p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor154"/>Call to action – where do I go next? </h1>
			<p>All good things must end, and so does this book. Whew! We covered a lot of ground here. Automated ML is an active area of research and development, and in this book, we tried to give you a breadth-first overview of the fundamentals, key benefits, and platforms. We explained the underlying techniques of automated feature engineering, model and hyperparameter learning, and neural architecture search with examples from open source toolkits and cloud platforms. We covered a detailed walkthrough of three major cloud platforms, namely Microsoft Azure, AWS, and GCP. With the step-by-step walkthroughs, you saw the automated ML feature set offered in each platform by building ML models and trying them out. </p>
			<p>The learning journey does not end here. There are several great references provided in the book where you can further do a deep dive to learn more about the topic. Automated ML platforms, especially cloud platforms, are always in flux, so by the time you read this, some of the screens and functionality might already have changed. The best way to keep up with these hyperscalers is to keep pace with the change, the only constant. </p>
			<p>As a parting thought, we strongly believe what Sir Richard Branson said, that <em class="italic">"you don't learn to walk by following rules. You learn by doing, and by falling over."</em> The best way to learn something is by doing it – execution beats knowledge that doesn't get applied. Go ahead and try it out, and you will be glad to have the metaphorical coding battle scars.</p>
			<p>Happy coding! </p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor155"/>References and further reading</h1>
			<p>For more information on the topics covered in this chapter, you can refer to the given links:</p>
			<ul>
				<li><em class="italic">How to Choose Your First AI Project</em> by Andrew Ng: <a href="https://hbr.org/2019/02/how-to-choose-your-first-ai-project">https://hbr.org/2019/02/how-to-choose-your-first-ai-project</a></li>
				<li><em class="italic">Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation</em>, December 2019, Frontiers in Neuroscience, 13:1346, DOI: 10.3389/fnins.2019.01346</li>
				<li><em class="italic">Interpretable ML – A Guide for Making Black Box Models Explainable</em> by Christoph Molnar: <a href="https://christophm.github.io/interpretable-ml-book/">https://christophm.github.io/interpretable-ml-book/</a></li>
			</ul>
		</div>
	</body></html>