<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer044" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-36"><a id="_idTextAnchor035" class="calibre6 pcalibre pcalibre1"/>2</h1>
<h1 id="_idParaDest-37" class="calibre5"><a id="_idTextAnchor036" class="calibre6 pcalibre pcalibre1"/>Understanding the ML Model Development Life Cycle</h1>
<p class="calibre3">In this chapter, we will explore the different steps that exist in a typical AI/ML project. This information is an important foundation for an AI/ML solutions architect role because you will need to advise companies on how to implement these steps efficiently. It is also a foundation for the rest of the contents of this book, as in later chapters, you will create your own machine learning projects, and it’s important that you understand the steps in the process. We will also explore the concept of MLOps in this book and how the ML model development life cycle serves as the basis for the <span>MLOps paradigm.</span></p>
<p class="calibre3">This chapter covers the <span>following topics:</span></p>
<ul class="calibre16">
<li class="calibre8">An overview of the ML model development <span>life cycle</span></li>
<li class="calibre8">Common challenges encountered in the ML model development <span>life cycle</span></li>
<li class="calibre8">Best practices for overcoming <span>common challenges</span></li>
</ul>
<h1 id="_idParaDest-38" class="calibre5"><a id="_idTextAnchor037" class="calibre6 pcalibre pcalibre1"/>An overview of the ML model development life cycle</h1>
<p class="calibre3">You may be familiar with the concept <a id="_idIndexMarker108" class="calibre6 pcalibre pcalibre1"/>of the <strong class="bold">software development life cycle</strong> (<strong class="bold">SDLC</strong>), which is taught in computer science classes in schools all over the world. The SDLC concept began to be formulated in the 1960s and early 1970s, and by now it is a well-established and well-understood <a id="_idIndexMarker109" class="calibre6 pcalibre pcalibre1"/>process that is used in various formats by pretty much every company that develops software. Without formalized processes for people to follow when developing software, it would be difficult for companies to efficiently produce high-quality software and the software development industry would be quite chaotic. In fact, that’s how the software development industry was in its early years, and that’s how the machine learning industry currently is for most companies. Only in the past couple of years has the industry started to establish some structure around how companies should develop ML models and their <span>related applications.</span></p>
<p class="calibre3">In this section, we provide a high-level overview of the ML model development life cycle, outlining each of the steps that you will encounter in most machine learning projects. Let’s begin with a<a id="_idIndexMarker110" class="calibre6 pcalibre pcalibre1"/> quick recap of the SDLC. We will make references to this more well-established set of processes <span>where relevant.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The term <strong class="bold">MDLC</strong>, representing <strong class="bold">model development life cycle</strong>, was originally coined by a friend and colleague of mine<a id="_idIndexMarker111" class="calibre6 pcalibre pcalibre1"/> named Fei Yuan. He and I worked on building an MLOps process at Amazon before the term “MLOps” began to be used in the industry. That’s not to say that we were the only people trying to automate the steps in data science projects. For example, the technical reviewer for this book shared with me that he and some colleagues had implemented MLOps-type workloads in the early 2000’s with SAS and a process model called CRISP-DM, which stands for CRoss Industry Standard Process for Data Mining. You can learn more about CRISP-DM at the following URL: <a href="https://www.datascience-pm.com/crisp-dm-2/" class="calibre6 pcalibre pcalibre1">https://www.datascience-pm.com/crisp-dm-2/</a>. Fortunately, in recent years, MLOps has become an important and popular concept in ML model development, and now there are lots of MLOps tools available, which we will cover later in <span>this book.</span></p>
<h2 id="_idParaDest-39" class="calibre9"><a id="_idTextAnchor038" class="calibre6 pcalibre pcalibre1"/>SDLC – a quick recap</h2>
<p class="calibre3">One of the first and simplest<a id="_idIndexMarker112" class="calibre6 pcalibre pcalibre1"/> versions of the SDLC is known as the Waterfall model because the flow of activities in the process is<a id="_idIndexMarker113" class="calibre6 pcalibre pcalibre1"/> sequential, where the deliverables from each activity serve as dependencies for the next activity in the flow. <span><em class="italic">Figure 2</em></span><em class="italic">.1</em> shows the original Waterfall diagram from a paper titled <em class="italic">Managing the development of large software systems</em> by Winston Royce <span>in 1970.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer023">
<img alt="Figure 2.1: SDLC Waterfall model" src="image/B18143_02_1.jpg" class="calibre32"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.1: SDLC Waterfall model</p>
<p class="calibre3">As we can see, the process starts by gathering and analyzing the requirements that the system needs to satisfy and then designing, coding, and testing the software before deploying it for use. After the resulting software has been deployed, you then need to manage it via ongoing<a id="_idIndexMarker114" class="calibre6 pcalibre pcalibre1"/> operations activities. This model was later updated to include feedback loops between the various stages. For example, the feedback from testing could result in an updated coding step, which in turn could result in an updated program design step, and <span>so on.</span></p>
<p class="calibre3">A well-known limitation of the Waterfall model is that the process doesn’t facilitate the rapid innovation or flexibility that is required in today’s fast-paced software development industry, whereby new requirements often come to light during the development, testing, and deployment phases. The software design needs to be updated frequently, and the various stages in the process are more cyclical in nature to allow updates to occur more flexibly (see <span><em class="italic">Figure 2</em></span><em class="italic">.2</em>). As a result, newer development methodologies such as Agile have emerged. Nevertheless, the methodical sequence of events from gathering requirements through to designing, coding, testing, deploying, and monitoring software still exists in various forms in system design projects, and this extends to ML systems <span>and projects.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer024">
<img alt="Figure 2.2: A cyclical approach to software development, often referred to as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)" src="image/B18143_02_2.jpg" class="calibre33"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.2: A cyclical approach to software development, often referred to as DevOps (source: https://openclipart.org/download/313185/1546764098.svg)</p>
<h2 id="_idParaDest-40" class="calibre9"><a id="_idTextAnchor039" class="calibre6 pcalibre pcalibre1"/>Typical ML project stages</h2>
<p class="calibre3">Interestingly, it took a while for the lessons from the traditional software development industry to be applied to ML model <a id="_idIndexMarker115" class="calibre6 pcalibre pcalibre1"/>development. When ML development suddenly experienced a huge uptick in popularity during the past few years, many companies dived into the race without formalized processes in place, and as a result, companies ran into their own random unexpected issues without much ability to standardize across the industry. Fortunately, lessons have been learned from the early pioneers in this process, and standardized project activities have emerged. The<a id="_idIndexMarker116" class="calibre6 pcalibre pcalibre1"/> following are the kinds of steps that you can expect to take in most ML <span>development projects:</span></p>
<ol class="calibre7">
<li class="calibre8">Gather, analyze, and understand the business requirements for which the model will <span>be developed.</span></li>
<li class="calibre8">Find and gather <span>relevant data.</span></li>
<li class="calibre8">Explore and understand the contents of <span>the data.</span></li>
<li class="calibre8">Transform or manipulate the data for ML model training, which may include feature engineering and storing features for use in later steps. This step is often also closely linked to <em class="italic">step 6</em> because the selected algorithm may have specific requirements regarding how the data needs to <span>be presented.</span></li>
<li class="calibre8">For supervised learning<a id="_idIndexMarker117" class="calibre6 pcalibre pcalibre1"/> models, label the data if the required labels are not already present in <span>the dataset.</span></li>
<li class="calibre8">Pick an algorithm<a id="_idIndexMarker118" class="calibre6 pcalibre pcalibre1"/> that suits the requirements of the <span>business case.</span></li>
<li class="calibre8">Train <span>a model.</span></li>
<li class="calibre8">Configure and <span>tune hyperparameters.</span></li>
<li class="calibre8">Deploy <span>the model.</span></li>
<li class="calibre8">Monitor the model <span>after deployment.</span></li>
</ol>
<p class="calibre3"><span><em class="italic">Figure 2</em></span><em class="italic">.3</em> shows a visual representation of these steps, and we will dive into these steps in detail in the <span>coming sections:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer025">
<img alt="Figure 2.3: Typical ML project stages" src="image/B18143_02_03.jpg" class="calibre34"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.3: Typical ML project stages</p>
<p class="calibre3">As you can see, there are some similarities between the ML model development process and the traditional SDLC, but <a id="_idIndexMarker119" class="calibre6 pcalibre pcalibre1"/>there are also some differences that are unique to ML model development. Most notable is the inclusion of data in the process. The fact that we now need to include the manipulation of data in our overall process adds a lot of complexity, as you will see when we go through each of the process steps in more detail. It should be noted that <a id="_idIndexMarker120" class="calibre6 pcalibre pcalibre1"/>each step in the overall life cycle is often cyclical in nature, whereby the data science team may need to perform each task—or combinations of tasks—multiple times, with different inputs and outputs, using a trial-and-error methodology until they find the optimal approach to use for <span>each step.</span></p>
<h3 class="calibre11">Gathering, analyzing, and understanding the business requirements</h3>
<p class="calibre3">This step is often omitted from <a id="_idIndexMarker121" class="calibre6 pcalibre pcalibre1"/>ML life cycle diagrams because such diagrams usually focus on the technical steps, which follow later in our project. This can be considered as step zero because this generally needs to happen before any of the technical steps in our project begin. Just like in traditional software development, the overall process must begin with gathering and understanding the business requirements that the model will be built to address. For example, will the models produced by our project be used to forecast sales revenue for the next year or are we setting out to build an application that will monitor people’s health data and provide them with health-related recommendations based on that data? The business requirements influence the decisions we make in the later steps of our project, such as what kinds of data we need to gather, what ML algorithms we will use to train our models, and what<a id="_idIndexMarker122" class="calibre6 pcalibre pcalibre1"/> kinds of metrics we will measure in relation to our <span>models’ performance.</span></p>
<p class="calibre3">In this part of an AI/ML project, the solutions architect will work with business leaders to understand what they want to achieve from a business perspective and will then work with technical personnel to translate the business requirements into technical requirements. Defining the technical requirements is one of the first steps in defining the overall strategy to meet the business objectives outlined by the business leadership. This includes identifying any constraints that may exist, such as working with data scientists to determine what kinds of data would be required to address the business goals and whether that data can be gathered, generated, or procured <span>from somewhere.</span></p>
<h3 class="calibre11">Finding and gathering relevant data</h3>
<p class="calibre3">We briefly touched on this topic in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. Data is what ML models learn from, so without data, there is no machine<a id="_idIndexMarker123" class="calibre6 pcalibre pcalibre1"/> learning. If the project team—including the data scientists and data engineers (we will explain these roles in more detail later)—cannot figure out how to get the data that would be required to meet the business objective, then the project could be a non-starter right from the beginning, so this is a critical step in the process. Sources of data vary based on the type of project, but the following are some examples of data that could be used for various AI/ML <span>use cases:</span></p>
<ul class="calibre16">
<li class="calibre8">Historical data that contains the details of customer credit card transactions and/or <span>banking transactions</span></li>
<li class="calibre8">Data relating to what customers are <span>purchasing online</span></li>
<li class="calibre8">Housing sales data in a <span>particular region</span></li>
<li class="calibre8">Log entry data that contains details of a technical system’s <span>operational events</span></li>
<li class="calibre8">Health data that is tracked by wearable devices such as watches or <span>fitness trackers</span></li>
<li class="calibre8">Data gathered from people filling in forms <span>or surveys</span></li>
<li class="calibre8">Data streamed<a id="_idIndexMarker124" class="calibre6 pcalibre pcalibre1"/> from <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices such as factory conveyor belts or a fleet of <span>construction vehicles</span></li>
</ul>
<p class="calibre3">As you can see, there are different types of data that can be used for many different purposes. The data science team’s first major task will be to define and locate the data that needs to be used for the <a id="_idIndexMarker125" class="calibre6 pcalibre pcalibre1"/>project. This is not an atomic activity in that it does not need to happen all at once at the beginning of a project. Often, the science team begins with some idea of the data they need, and then they may refine the data requirements based on testing and feedback in later steps of <span>the project.</span></p>
<h3 class="calibre11">Exploring and understanding the data</h3>
<p class="calibre3">When the data science team has gathered data that they believe could be used to address the business requirements, they usually don’t just dive into training ML models on that data. Instead, they usually need to inspect the data to assess whether it really can be used adequately for the<a id="_idIndexMarker126" class="calibre6 pcalibre pcalibre1"/> purposes of the project. Raw data is often not in an optimal state to be used by some ML algorithms. Let’s take a couple of examples from our list of potential data sources. If we’re using data gathered from people filling in forms or surveys, people may input the details incorrectly. They may leave some fields blank or misspell some of the details during input. As another example, if we’re using data from sensors such as wearable health trackers or other IoT devices such as mechanical machinery sensors, those sensors may malfunction and record corrupted data. As such, data scientists often need to inspect the data and look for errors, anomalies, or potentially corrupted data. In <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, we also mentioned that data scientists may want to get statistical details regarding the data, such as the range of values that are generally seen for particular variables in the data or other statistical distribution details. In the hands-on activities later in this book, we will be using data visualization tools and other data inspection tools to explore and understand the contents of <span>our datasets.</span></p>
<h3 class="calibre11">Transforming or manipulating the data for ML model training</h3>
<p class="calibre3">Missing or corrupted data can<a id="_idIndexMarker127" class="calibre6 pcalibre pcalibre1"/> cause problems when training ML models. Some algorithms that need to operate on numeric data will produce errors if they encounter non-numeric values, including null and garbled/corrupted characters. Even for algorithms that can handle such values gracefully, the values may skew the learning process in unexpected ways, thus affecting the performance of the <span>resulting models.</span></p>
<p class="calibre3">When data scientists find that a dataset isn’t perfectly ready for use in training ML models, they don’t usually just give up but rather try to make changes to the data to bring it closer to the desired <a id="_idIndexMarker128" class="calibre6 pcalibre pcalibre1"/>state. We call this<a id="_idIndexMarker129" class="calibre6 pcalibre pcalibre1"/> process <span><strong class="bold">feature engineering</strong></span><span>.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Some literature publications use the term “feature engineering” only to refer to the process of creating new features from existing features (such as our price per square foot example), while other literature uses the same term to describe all activities related to manipulating features in our dataset, including replacing <span>missing values.</span></p>
<p class="calibre3">This could include data cleaning (or cleansing) techniques, such as replacing missing data with something more meaningful. As an example, let’s imagine that some medical condition is more likely to occur as a person gets older and we want to build a model that predicts the likelihood of this condition occurring. In this case, a person’s age would be an important input feature in our dataset. During our data exploration activities, if we discover that some of the records in our dataset are missing the age value, we could compute the average age of all of the people in our dataset and replace each missing age value with the average age value. Alternatively, we could replace each value with the modal (i.e., the most frequently occurring) age value. Either of these cases would at least be better than having missing or corrupted values in our dataset during the <span>training process.</span></p>
<p class="calibre3">Also, the optimal variables and values for addressing specific business requirements may not be readily available in whatever raw data we can access. Instead, data scientists often need to combine data from different sources and come up with clever ways to derive new data from the available data. A very simple example would be if we specifically need a person’s age as an input variable but the dataset only contains their date of birth. In that case, the data scientist could add another feature, age, to the dataset, and subtract the date of birth from the current date in order to calculate the person’s age. A slightly more complex example would be if we wanted to predict housing prices and we determined that price per square foot would be an important input feature for our model but our dataset only contains the total price for each house and the total area of each house in square feet. In that case, to create our price per square foot input feature for each house, we could divide the total cost of each house by the total area of that house in square feet and then add that as a feature in <span>our dataset.</span></p>
<p class="calibre3">It’s important to understand that when a data scientist has created the features that are important for training their model, they will often want to store those features somewhere for later<a id="_idIndexMarker130" class="calibre6 pcalibre pcalibre1"/> use, rather than needing to re-create them again and again. Later in this book, we will explore tools that have been developed for <span>this purpose.</span></p>
<h3 class="calibre11">Data labeling</h3>
<p class="calibre3">As we discussed in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, supervised<a id="_idIndexMarker131" class="calibre6 pcalibre pcalibre1"/> learning algorithms rely on labels in the dataset during the training process, which tell the models what the correct answers are for the types of data relationships that the model is trying to learn. <span><em class="italic">Figure 2</em></span><em class="italic">.4</em> shows our example of a <span>labeled dataset.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer026">
<img alt="Figure 2.4: An example of labels (highlighted in green) in a dataset" src="image/B18143_02_4.jpg" class="calibre35"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.4: An example of labels (highlighted in green) in a dataset</p>
<p class="calibre3">If you’re lucky, you will find a dataset that can be used to address your business requirements and already contains the requisite labels or “correct answers” for the variables that you want to predict. If not, you will need to add the labels to the dataset. Again, considering that your dataset could contain millions of data points, this could be a significantly complex and time-consuming task to take on. And, just like any of the other features in your dataset, the quality of your labels directly impacts how reliable your model’s predictions will be. With this in mind, you will need access to a labor force that can accurately label your dataset and other tools that facilitate <span>the labeling.</span></p>
<p class="calibre3">Another data preparation step that is used for supervised learning algorithms is to split the dataset into three different subsets, which are used for the training, validation, and testing of the model, respectively. We will describe the use of these subsets in the model training section later in <span>this chapter.</span></p>
<p class="calibre3">An important thing to call out at this point is the concept<a id="_idIndexMarker132" class="calibre6 pcalibre pcalibre1"/> of <strong class="bold">data leakage</strong>, which refers to a scenario in which information from outside the training dataset is used to create the model. This can cause the model to perform well on the training data (because it has information it wouldn’t have in a real-world scenario) but perform poorly in production due to these <span>unintentional hints.</span></p>
<p class="calibre3">There are various causes that can lead to data leakage, such as how and when we split our datasets during our data science project, or how we label our data. For example, during data preparation activities such as labeling or feature engineering, we could accidentally include knowledge that would not be available to the model in a real-world application when the model needs to make predictions. Consider a scenario in which we are using historical data to train our model. We may accidentally include information that only became available after the events that are represented in the dataset actually occurred. While this data may be relevant and may help influence the outcome, it will harm our model performance if that <a id="_idIndexMarker133" class="calibre6 pcalibre pcalibre1"/>information would not be available to our model in a real world scenario at the time when our model needs to make <span>a prediction.</span></p>
<h3 class="calibre11">Picking an algorithm and model architecture</h3>
<p class="calibre3">There are lots of different types of ML algorithms that can be used for various purposes, with new algorithms and <a id="_idIndexMarker134" class="calibre6 pcalibre pcalibre1"/>model architecture patterns emerging regularly. In some <a id="_idIndexMarker135" class="calibre6 pcalibre pcalibre1"/>cases, choosing your approach is an easy decision because there are some algorithms and model architectures that are particularly suited to specific use cases. For example, if you want to implement a computer vision use case, then something such as a convolutional neural network architecture would be a good starting point. On the other hand, choosing what kind of ML algorithm and implementation to use for a specific problem can be a difficult task that often depends on the experience of the data science team. For example, an experienced team of data scientists will have worked on many different projects and developed a working understanding of what kinds of algorithms work best for various circumstances, whereas a less experienced data science team may need to perform a lot more experimentation with various algorithms and <span>model architectures.</span></p>
<p class="calibre3">In addition to direct business requirements, such as “we need a computer vision model to identify manufacturing defects”, the chosen algorithm can also depend on less-tangible business requirements, such as “the model needs to run on limited computing resources” or “the explainability of the model is extremely important in this use case.” Each of the aforementioned requirements puts different constraints on the types of algorithms the data science team could select for the given <span>use case.</span></p>
<p class="calibre3">As with most of the steps in the overall AI/ML project life cycle, selecting the best algorithm and model architecture to use can require the data science team to implement a cyclical trial-and-error approach, whereby they may experiment with different algorithms, architectures, and inputs/outputs until they find the optimal implementation. We will be exploring various algorithms and their unique characteristics in the hands-on activities later in this book, but overall, it is best to start with a simple baseline model, so that we have a starting point to compare metrics and understand the base dataset. Then, we can test out more complex models and assess whether they <span>perform better.</span></p>
<h3 class="calibre11">Training a model</h3>
<p class="calibre3">This is probably the most well-known activity in the AI/ML project life cycle. It’s where the model actually learns from the <a id="_idIndexMarker136" class="calibre6 pcalibre pcalibre1"/>data. For unsupervised algorithms, this is where they may form the clusters that we talked about in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, for example. For supervised algorithms, this is where our training, validation, and testing datasets come into the picture. In <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, we briefly talked about how linear algebra and calculus can be used in machine learning. If we take our linear regression example, this is exactly where those concepts would come into play. Our model would first try to find the relationships between the features and the labeled target outputs. That is, it would try to find the coefficients for each of our features, which, when used in combination (e.g., by adding them all together), would produce the labeled target output. It tries to calculate the coefficients that would work for all data points in the dataset, and to do this, it needs to scan through all of the items in the <span>training dataset.</span></p>
<p class="calibre3">The model usually starts this process with a random guess, so it inevitably is incorrect on the first try. However, it then calculates the errors and makes adjustments to minimize those errors in future iterations through the dataset. There are a number of different methods and algorithms that can be used to minimize errors, but a very popular method is called gradient descent. We <a id="_idIndexMarker137" class="calibre6 pcalibre pcalibre1"/>briefly mentioned gradient descent in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, but we’ll talk about it in more detail here. In gradient descent, the algorithm works on finding the minimum point of what we call the loss function, which is a representation of the errors that are produced when our model tries to guess the coefficients of the features that produce the labeled outputs for each data point in our dataset. <em class="italic">Equation 2.1</em> shows the <a id="_idIndexMarker138" class="calibre6 pcalibre pcalibre1"/>equation for calculating the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) as an example for a loss function for <span>linear regression:</span></p>
<p class="calibre3"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mi&gt;E&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" height="86" src="image/1.png" width="319" class="calibre36"/></p>
<p class="img---caption" lang="en-US" xml:lang="en-US">Equation 2.1: Mean squared error formula</p>
<p class="calibre3">In <em class="italic">Equation 2.1</em>, <em class="italic">n</em> represents the number of data points in <span>our dataset.</span></p>
<p class="calibre3">To understand what this formula is saying, let’s start with the section in <span>parenthesis: </span><span><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:math&gt;" height="52" src="image/2.png" width="145" class="calibre37"/></span></p>
<p class="calibre3"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" height="34" src="image/3.png" width="24" class="calibre38"/> represents our model’s predicted target variable for each datapoint and <img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mover accent=&quot;true&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;^&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" height="43" src="image/4.png" width="32" class="calibre39"/> represents the true target variable’s value for <span>each datapoint.</span></p>
<p class="calibre3">Within the parentheses in <em class="italic">Equation 2.1</em>, we subtract the true value from our model’s predicted value in order to calculate the error of our model’s prediction, similar to what we described in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, and we then square the result. In this case, we are calculating what’s referred to as<a id="_idIndexMarker139" class="calibre6 pcalibre pcalibre1"/> the <strong class="bold">Euclidean distance</strong>, which is the distance between the predicted value and the true value in two-dimensional space. Squaring the result also has the effect of removing negative values from the results of <span>our subtractions.</span></p>
<p class="calibre3">The summation symbol, <em class="italic">Σ</em> (sigma), in the equation represents adding up all of the calculated errors for all data<a id="_idIndexMarker140" class="calibre6 pcalibre pcalibre1"/> points in our training dataset. Then, we divide the final result—i.e., the total error for all predictions—by the number of data points in our dataset in order to calculate the average (or mean) error for all of <span>our predictions.</span></p>
<p class="calibre3">Remember that we want to minimize the error in each training iteration, by finding the minimum point of this loss<a id="_idIndexMarker141" class="calibre6 pcalibre pcalibre1"/> function (also referred to as the <strong class="bold">objective function</strong>). To get an understanding of what it means to find the minimum point in a loss function, it helps if we can graph the function. <span><em class="italic">Figure 2</em></span><em class="italic">.5</em> shows an example of a two-dimensional loss function graph for <span>the MSE:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer031">
<img alt="Figure 2.5: MSE loss function graph showing minimum point" src="image/B18143_02_5.jpg" class="calibre40"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.5: MSE loss function graph showing minimum point</p>
<p class="calibre3">Each time the algorithm calculates the loss from each training iteration, that loss value can be represented as a point on the graph. Considering that we want to move toward the minimum point in order to minimize the loss, we want to take a step downward on the graph. Whenever we want to move from one point to another (even when moving our bodies around in real life), there are two aspects of the movement that we need to determine: direction and magnitude, i.e., in which direction do we want to move and how far? This is where gradient descent comes into the picture. It helps us to determine the direction in which <a id="_idIndexMarker142" class="calibre6 pcalibre pcalibre1"/>we should move in order to progress towards the minimum point. Let’s take a look at how it works in <span>more detail.</span></p>
<p class="calibre3">Imagine the graph in <span><em class="italic">Figure 2</em></span><em class="italic">.5</em> is a valley between two mountains and we are standing at a point that represents the loss calculated in our most recent training iteration. That point is somewhere on the side of the valley, such as the location shown in <span><em class="italic">Figure 2</em></span><span><em class="italic">.6</em></span><span>.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer032">
<img alt="Figure 2.6: MSE loss function graph showing current location" src="image/B18143_02_6.jpg" class="calibre41"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.6: MSE loss function graph showing current location</p>
<p class="calibre3">For a human, walking downhill is somewhat instinctive because we have sensory input that tells us which way is downhill. For example, our feet can feel the slope of the hill at our current location, we can feel the pull of gravity downwards, and we may also be able to see our surroundings and therefore see which way is downhill. However, our gradient descent algorithm does not have these sensory inputs and it can only use mathematics to find out which way is downhill. Then, it needs to use programmatic methods to define what it means to take a step in that direction. Our algorithm knows the current location on the graph and it can calculate the derivative of the function (a concept from differential calculus) to determine the slope of the graph at the current location. <span><em class="italic">Figure 2</em></span><em class="italic">.7</em> shows<a id="_idIndexMarker143" class="calibre6 pcalibre pcalibre1"/> an example of the slope of a line at a point on the graph, which represents the derivative of the function at <span>that point.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer033">
<img alt="Figure 2.7: Derivative of the loss function at a particular point" src="image/B18143_02_7.jpg" class="calibre42"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.7: Derivative of the loss function at a particular point</p>
<p class="calibre3">When the derivative has been calculated, this information can be used by our algorithm to take a step toward the minimum point. <em class="italic">Equation 2.2</em> shows how each next step is calculated for gradient descent in the context of <span>linear regression:</span></p>
<p class="calibre3"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mfrac&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="94" src="image/5.png" width="471" class="calibre43"/></p>
<p class="img---caption" lang="en-US" xml:lang="en-US">Equation 2.2: Gradient descent for linear regression</p>
<p class="calibre3">In <em class="italic">Equation 2.2</em>, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" height="51" src="image/6.png" width="33" class="calibre44"/>represents the location on the graph, and Ɵ represents the vector of coefficients for the features of each data point in our dataset. Remember that we are trying to find the set of coefficients for our features that results in the least error between our model’s predictions and the true values of our data points’ <span>target variables:</span></p>
<ul class="calibre16">
<li class="calibre8"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="47" src="image/7.png" width="116" class="calibre45"/>then represents the predicted target variable for each <span>data point</span></li>
<li class="calibre8"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/math&gt;" height="40" src="image/8.png" width="55" class="calibre46"/>represents the true target value for each <span>data point</span></li>
</ul>
<p class="calibre3">As we can see, in the broader set of parentheses, we are again subtracting the true target value from the predicted target variable value for each data point. This is because <em class="italic">Equation 2.2</em> is derived from <em class="italic">Equation 2.1</em> (the mathematical proof of this derivation is omitted here <span>for simplicity).</span></p>
<p class="calibre3"><em class="italic">m</em> represents the number of features we have for each data point in <span>our dataset.</span></p>
<p class="calibre3"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;/mml:math&gt;" height="19" src="image/9.png" width="23" class="calibre47"/> is what we refer to as the <strong class="bold">learning rate</strong>. It’s one<a id="_idIndexMarker144" class="calibre6 pcalibre pcalibre1"/> of the hyperparameters of our algorithm and it determines the size of the step we should take; i.e., the magnitude by which we should move in the <span>selected direction.</span></p>
<p class="calibre3">Altogether, <img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;/msub&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mfenced&gt;&lt;/msup&gt;&lt;/mfenced&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="61" src="image/10.png" width="431" class="calibre48"/>represents the derivative of the loss function at our current location on the graph. Therefore, <em class="italic">Equation 2.2</em> says that our next location will be equal to our<a id="_idIndexMarker145" class="calibre6 pcalibre pcalibre1"/> current location minus the derivative of the loss function at our current location on the graph multiplied by the learning rate. This then takes a step toward the minimum point, whereby the derivative of the loss function determines the direction and the combination with the learning rate defines <span>the magnitude.</span></p>
<p class="calibre3">It’s very important to note that <em class="italic">Equation 2.2</em> represents just one step in the gradient descent process. We iterate over this process many times, in each iteration scanning through the dataset and inputting our estimated coefficients, calculating the error/loss, and then reducing the loss by using gradient descent to step toward the loss function’s minimum point. We may never exactly reach the minimum point but even if we can get quite close, our model’s estimates and predictions could be <span>acceptably accurate.</span></p>
<p class="calibre3">We should note that there are different configurations for gradient descent. In batch gradient descent, we would go through the entire training set in each iteration. Alternatively, we could implement mini-batch gradient descent, in which case each iteration would process subsets of the training dataset. This approach is less thorough but can be more efficient. A popular implementation is known as stochastic gradient descent (the word “stochastic” means “random”). In stochastic gradient descent, we take a subset of <a id="_idIndexMarker146" class="calibre6 pcalibre pcalibre1"/>random samples from our dataset in each iteration, which could be as little as one data point in each sample. The key here is that because we take a random subset in each iteration, we start from a different point in our feature space each time. At first, this seems somewhat erratic, as we jump around to different points in our feature space. However, this approach has been shown to be quite effective in minimizing the overall loss function. It can also help to avoid something referred to as <strong class="bold">local minima</strong>, which <a id="_idIndexMarker147" class="calibre6 pcalibre pcalibre1"/>refers to the fact that some loss functions are not as simple as the one we showed in <span><em class="italic">Figure 2</em></span><em class="italic">.5</em>. They may have multiple peaks and valleys, in which case the bottom of any of the valleys could be considered a type of minimum point, but a local minimum may not be the <a id="_idIndexMarker148" class="calibre6 pcalibre pcalibre1"/>overall minimum of the function, which is referred to as the <strong class="bold">global minimum</strong>. <span><em class="italic">Figure 2</em></span><em class="italic">.8</em> shows an<a id="_idIndexMarker149" class="calibre6 pcalibre pcalibre1"/> example of multiple minima <span>and maxima:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer040">
<img alt="Figure 2.8: Local and global minima and maxima" src="image/B18143_02_8.jpg" class="calibre49"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.8: Local and global minima and maxima</p>
<p class="calibre3">Although we focused on two-dimensional loss functions in this section, loss functions can have more than<a id="_idIndexMarker150" class="calibre6 pcalibre pcalibre1"/> two dimensions, and the same concepts also apply in <span>higher-dimensional space.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In this section, we chose a specific algorithm (linear regression) and type of dataset (tabular) to illustrate an example of the model training process. There are, of course, other algorithms and types of data for different use cases, and the training processes for those use cases would have their own unique implementations. However, the overall model training process generally involves processing the input data to try to find some kind of useful pattern or relationship and then incrementally honing the accuracy of that pattern or relationship in a repetitive way until some determined threshold of accuracy is met or until the training is deemed to be ineffective (if the model is failing to effectively learn any useful patterns or relationships) and therefore <span>is terminated.</span></p>
<h3 class="calibre11">Configuring and tuning hyperparameters</h3>
<p class="calibre3">Hyperparameters are <a id="_idIndexMarker151" class="calibre6 pcalibre pcalibre1"/>parameters that define aspects of how your model training jobs run. They are not the parameters in your dataset from which your models learn but rather external configuration options related to how<a id="_idIndexMarker152" class="calibre6 pcalibre pcalibre1"/> the model training process is executed. To start with a simple example: we’ve discussed that training jobs often need to cycle through the training dataset multiple times in order to learn patterns in the data. One of the hyperparameters you may configure for a model training job would be to specify how many times it should cycle over the dataset. This is often referred to as the <a id="_idIndexMarker153" class="calibre6 pcalibre pcalibre1"/>number of <strong class="bold">epochs</strong>, where an epoch represents one iteration through the <span>training dataset.</span></p>
<p class="calibre3">Choosing the optimal values for your hyperparameters is another activity that usually requires a lot of trial-and-error attempts, in which you may need to experiment with different combinations of hyperparameter values in order to find the optimal settings to maximize your model training performance. Let’s continue our simple example of configuring the number of times the training job should process through the training dataset. Considering that the model could potentially learn more information each time it runs through the dataset, we may at first think that deciding on the number of epochs would be a simple choice, i.e., that we should just set this value to be very high so that the model learns more from the data. However, in reality, this is often not the best option because it’s not always true that a model learns more useful information every time it processes through the dataset. There are concepts in machine learning called <strong class="bold">underfitting</strong> and <strong class="bold">overfitting</strong>, which we<a id="_idIndexMarker154" class="calibre6 pcalibre pcalibre1"/> will explore in the <em class="italic">challenges</em> section of this chapter. They<a id="_idIndexMarker155" class="calibre6 pcalibre pcalibre1"/> relate to problems in which continued training on the existing dataset will fail to yield <span>desired results.</span></p>
<p class="calibre3">Even in cases when the model <a id="_idIndexMarker156" class="calibre6 pcalibre pcalibre1"/>does learn useful information each time it processes the dataset, it often reaches <a id="_idIndexMarker157" class="calibre6 pcalibre pcalibre1"/>a point where the rate at which it is learning new information will taper off or reach a plateau. When this happens, it would be inefficient to continue running through the dataset again and again. Bear in mind that it can be very expensive to train a model on a large dataset, so you do not want to keep processing the data when the model is reaching a plateau in its learning. We can measure the rate of learning by <a id="_idIndexMarker158" class="calibre6 pcalibre pcalibre1"/>generating a <strong class="bold">learning curve</strong> graph, which graphs the training errors during the training process. <span><em class="italic">Figure 2</em></span><em class="italic">.9</em> shows an example of a <span>learning curve:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer041">
<img alt="Figure 2.9: Learning curve graph" src="image/B18143_02_09.jpg" class="calibre50"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.9: Learning curve graph</p>
<p class="calibre3">In <span><em class="italic">Figure 2</em></span><em class="italic">.9</em>, the gap between the blue and red lines represents the error between our model’s predictions on the training dataset and the testing dataset. When the graph shows that the gap between the blue and red lines is not significantly decreasing, then we know that continued training will not make our model significantly more accurate, and it may be a good point at which to stop the <span>training process.</span></p>
<p class="calibre3">An analogy would be if we had a student who had read a book so many times that they memorized every word and thoroughly <a id="_idIndexMarker159" class="calibre6 pcalibre pcalibre1"/>understood every concept in the book. At that point, it would be unnecessary <a id="_idIndexMarker160" class="calibre6 pcalibre pcalibre1"/>to continue instructing the student to read the book again and again because they would no longer learn anything new from that book. Now let’s also imagine that we needed to pay the student every time they read the book, which is analogous to paying for the computing resources required to train the model on <span>the dataset.</span></p>
<p class="calibre3">Configuring the number of epochs is just one example of trying to find the best configuration for a particular hyperparameter. Different types of algorithms have different types of hyperparameters that pertain to them, and trying to test all of the various combinations of hyperparameter values can be a painstaking and time-consuming task for data scientists. Fortunately, Google Cloud has a tool that performs this task for us in an automated fashion, which we will explore later in <span>this book.</span></p>
<h3 class="calibre11">Deploying the model</h3>
<p class="calibre3">Finally! We have found the right<a id="_idIndexMarker161" class="calibre6 pcalibre pcalibre1"/> combinations of data, algorithms, and hyperparameter values and we’ve gotten to a point where our model is ready to be used in the real world, which we refer to as <strong class="bold">hosting</strong> or <strong class="bold">serving</strong> our model. Getting<a id="_idIndexMarker162" class="calibre6 pcalibre pcalibre1"/> to this point has required a lot of work. We may have trained hundreds<a id="_idIndexMarker163" class="calibre6 pcalibre pcalibre1"/> of different versions of our model to get it to a point at which it’s ready to be deployed in production. Our data science team may have had to experiment with lots of different dataset versions and transformations and lots of different algorithms and hyperparameter values in order to finally get some meaningful results or insights. Up until only around five years ago, performing all of those steps and tracking their results would have been a very slow, manual, and painstaking process. It still can be a lot of work, but at least now there are tools that automate a lot of these steps and they can be completed much more quickly, perhaps taking weeks instead of months. Later in this book, we’ll talk about <a id="_idIndexMarker164" class="calibre6 pcalibre pcalibre1"/>something called AutoML, which can reduce this entire process down to a few minutes or hours in just a few <span>short commands!</span></p>
<p class="calibre3">Deploying our model can be as simple as packaging it into a Docker container and deploying the container on a server, although we will usually want to create some kind of web-based API to facilitate access to the model by applications. We will do this in a hands-on activity later. Also, when we cover MLOps, we’ll see how it may make sense for us to create pipelines to automate the deployment of our model, in much the same way as we use CI/CD<a id="_idIndexMarker165" class="calibre6 pcalibre pcalibre1"/> pipelines to build and deploy regular <span>software applications.</span></p>
<h3 class="calibre11">Monitoring the model after deployment</h3>
<p class="calibre3">You might think that once you’ve successfully tested and deployed the model to production then your job is done. However, the fun doesn’t stop there! Just like regular software, you need to monitor the<a id="_idIndexMarker166" class="calibre6 pcalibre pcalibre1"/> performance of your model on an ongoing basis. This includes traditional monitoring, such as keeping track of how many requests your model is serving in a given timeframe (per second, for example), how long it takes for your model to respond to a request (latency), and whether these metrics are changing over time. However, ML models also have additional requirements that need to be monitored, such as ML-specific metrics such as those mentioned in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a> (MAE, MSE, accuracy, precision, etc.). These metrics help us to understand how our models are performing from an inference perspective, so we need to monitor them and ensure they continue to meet our <span>business requirements.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The stages in the ML model development life cycle that you’ve learned about in this section form the basis for understanding MLOps and AutoML. We dedicate an entire chapter of this book to ML engineering and MLOps on Google Cloud, but for now, at a high level, you can consider the goals of MLOps and AutoML to be the automation of all of the steps in the ML model development life cycle. You’ll see in the chapter on MLOps that there are tools we can use to create pipelines that automate all of the outlined steps. We can have complex combinations of pipelines within pipelines, which would automate everything from preparing and transforming the input data to training and deploying the models, monitoring the models in production, and automatically kicking off the entire process all over again if we detect that our model has stopped providing desirable results and we want to retrain the models on updated data. This would provide a self-healing model ecosystem, which helps to keep our models up to date on an <span>ongoing basis.</span></p>
<h2 id="_idParaDest-41" class="calibre9"><a id="_idTextAnchor040" class="calibre6 pcalibre pcalibre1"/>Roles and personas in AI/ML projects</h2>
<p class="calibre3">Throughout this book, we mention various roles such as data scientist, data engineer, and ML engineer. We <a id="_idIndexMarker167" class="calibre6 pcalibre pcalibre1"/>also mention more traditional roles such as software engineer, project manager, stakeholder, and business leader. The traditional roles have been defined in the industry for decades now, so we will not define them here, but there is often confusion about the newer roles that are specific to AI/ML projects, so we’ll take some time to briefly describe them here. In small teams, it should be noted that a single person may perform all of <span>these roles:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Data engineer</strong>: Data engineers are usually involved in the earlier stages of the data science project—specifically the<a id="_idIndexMarker168" class="calibre6 pcalibre pcalibre1"/> data gathering, exploration, and transformation stages. A data engineer will often be tasked with finding relevant data and cleaning it up to be used in later stages of <span>the project.</span></li>
<li class="calibre8"><strong class="bold">Data scientist</strong>: A data scientist is usually the role we associate with actually training ML models. They will usually <a id="_idIndexMarker169" class="calibre6 pcalibre pcalibre1"/>also perform data gathering, exploration, and transformation activities as they iterate through various model training experiments. In some cases, data scientists will be the more senior members working on the project and they may provide direction to the data engineers and ML engineers. They will often be responsible for the resulting ML models that are created, although those models are created and deployed with help from the data engineers and <span>ML engineers.</span></li>
<li class="calibre8"><strong class="bold">ML engineer</strong>: The ML engineer role usually refers to a software engineer who has ML or data science expertise. They understand ML concepts and they are experts in the stages of the model <a id="_idIndexMarker170" class="calibre6 pcalibre pcalibre1"/>development life cycle. They are usually the bridge that brings DevOps expertise to an ML project in order to create an MLOps workload. When a data scientist creates a model that they believe is ready to be used in production, they may work with an ML engineer to put all of the mechanisms in place to actually deploy the model into production via an <span>MLOps pipeline.</span></li>
</ul>
<p class="calibre3">Now that we’ve covered the major steps and concepts found in a typical AI/ML project, let’s take a look at the kinds of pitfalls that companies often run into when trying to implement <span>such projects.</span></p>
<h1 id="_idParaDest-42" class="calibre5"><a id="_idTextAnchor041" class="calibre6 pcalibre pcalibre1"/>Common challenges encountered in the ML model development life cycle</h1>
<p class="calibre3">For some of the stages in the ML model development life cycle, we’ve already discussed various challenges that you are likely to encounter in those stages. However, in this section, we specifically call <a id="_idIndexMarker171" class="calibre6 pcalibre pcalibre1"/>out major challenges that you need to be aware of <a id="_idIndexMarker172" class="calibre6 pcalibre pcalibre1"/>as an AI/ML solutions architect interacting with companies who are implementing AI/ML workloads. In the <em class="italic">Best practices for overcoming common challenges</em> section later in this chapter, we’ll look at ways to overcome many of <span>these challenges.</span></p>
<h2 id="_idParaDest-43" class="calibre9"><a id="_idTextAnchor042" class="calibre6 pcalibre pcalibre1"/>Finding and gathering relevant data</h2>
<p class="calibre3">One of our first major challenges is finding relevant data for the business problem our models are being built to address. We presented some examples of potential data sources in the previous section, and in <a id="_idIndexMarker173" class="calibre6 pcalibre pcalibre1"/>some cases, the data you need may be readily available to you, but finding the relevant data is not always straightforward for data scientists and data engineers. The following are some common challenges with regard to finding and accessing the <span>right data:</span></p>
<ul class="calibre16">
<li class="calibre8">If you work in a large company, the data may exist somewhere in your company owned by another team or organization, but you may not know about it or you may not know how to <span>find it.</span></li>
<li class="calibre8">The data might need to be created from a combination of different data sources that are dispersed throughout your company, all owned by <span>disjointed organizations.</span></li>
<li class="calibre8">The data could contain sensitive information, and therefore be subject to regulations and restrictions regarding how it needs to be stored <span>and accessed.</span></li>
<li class="calibre8">You may need to consult with experts in order to find, validate, and understand the data, e.g., financial data, medical data, atmospheric data, or other data related to fields of <span>specific expertise.</span></li>
<li class="calibre8">The data may be stored in databases that are restricted for use only in production <span>transaction operations.</span></li>
<li class="calibre8">You may not know whether you can trust the contents of the data. For example, is <span>it accurate?</span></li>
<li class="calibre8">The data could contain<a id="_idIndexMarker174" class="calibre6 pcalibre pcalibre1"/> inherent biases <a id="_idIndexMarker175" class="calibre6 pcalibre pcalibre1"/>or other <span>unknown challenges.</span></li>
</ul>
<h2 id="_idParaDest-44" class="calibre9"><a id="_idTextAnchor043" class="calibre6 pcalibre pcalibre1"/>Picking an algorithm and model architecture</h2>
<p class="calibre3">When it comes to picking an algorithm or<a id="_idIndexMarker176" class="calibre6 pcalibre pcalibre1"/> model architecture to use, one of the biggest challenges initially can be just figuring out where to start. You don’t want to spend months just experimenting with different options before finding a useful implementation or never finding a <span>useful implementation.</span></p>
<h2 id="_idParaDest-45" class="calibre9"><a id="_idTextAnchor044" class="calibre6 pcalibre pcalibre1"/>Data labeling</h2>
<p class="calibre3">Data labeling can be a very manual task, requiring humans to go through enormous amounts of data and add the labels for each data point. Tools have been built in recent years that automate some labeling tasks or make the tasks easier for humans to perform, but there is still a need to <a id="_idIndexMarker177" class="calibre6 pcalibre pcalibre1"/>have humans involved in labeling datasets. So, a major challenge that can exist for companies is finding and hiring a strong data-labeling workforce. Bear in mind that some data labeling tasks may require specific expertise. For example, consider a dataset consisting of medical images. A specific characteristic in the image could indicate the presence of a particular medical condition. It often requires special training to be able to read the medical images and identify the specific characteristics in question, so this is not a task for which you could hire any random person. As we’ve discussed previously, if your labels are not accurate, then your models will not be accurate, and in the example of medical imaging described here, this could have critical implications for a medical<a id="_idIndexMarker178" class="calibre6 pcalibre pcalibre1"/> facility that is tasked with diagnosing life-threatening <span>medical conditions.</span></p>
<h2 id="_idParaDest-46" class="calibre9"><a id="_idTextAnchor045" class="calibre6 pcalibre pcalibre1"/>Training models</h2>
<p class="calibre3">Two classic challenges with regard to training models are the problems of <strong class="bold">underfitting</strong> and <strong class="bold">overfitting</strong>. These challenges relate to how well <a id="_idIndexMarker179" class="calibre6 pcalibre pcalibre1"/>your model learns a relationship or pattern in <a id="_idIndexMarker180" class="calibre6 pcalibre pcalibre1"/><span>your data.</span></p>
<p class="calibre3">In the case of supervised learning, we <a id="_idIndexMarker181" class="calibre6 pcalibre pcalibre1"/>generally split our dataset into the three subsets mentioned earlier: training, validation, and testing. The validation set is usually used in hyperparameter tuning, the training dataset is what the model is trained on, and the testing dataset is how we evaluate the trained model. We evaluate the model based on the metrics that we’ve defined for that model, such as accuracy, precision, or MSE. In this context, the test dataset is new data that the model did not see during the model training process and we want to determine if the model’s predictions are accurate when it sees this new data—i.e., we want to determine how well the model <strong class="bold">generalizes</strong> to new data. If a model provides very accurate predictions on the training dataset but inaccurate or less accurate <a id="_idIndexMarker182" class="calibre6 pcalibre pcalibre1"/>predictions on the testing dataset then we say that the model is overfitting. This means it fits too closely to the training data, and it cannot perform well when it is exposed to <span>new data.</span></p>
<p class="calibre3">On the other hand, if we find that the model is not performing well on either dataset (training or testing), then we say it <span>is underfitting.</span></p>
<p class="calibre3"><span><em class="italic">Figure 2</em></span><em class="italic">.10</em> shows an example of fitting, overfitting, and underfitting for a classification model that is trying to determine the difference between the blue and red data points. The black boundary line represents overfitting because it fits much too precisely to the dataset, and the purple line represents underfitting because it does not do a good job of capturing the differences between the blue and red dots. The green line does a pretty good job of separating the blue and red dots; it’s not completely perfect, but it could be an acceptable model that generalizes well to the characteristics of the <span>data points.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer042">
<img alt="Figure 2.10: Example of fitting, overfitting, and underfitting" src="image/B18143_02_10.jpg" class="calibre51"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.10: Example of fitting, overfitting, and underfitting</p>
<p class="calibre3">In addition to classic training challenges such as those mentioned, there are other challenges that relate to the <a id="_idIndexMarker183" class="calibre6 pcalibre pcalibre1"/>overall process of how training<a id="_idIndexMarker184" class="calibre6 pcalibre pcalibre1"/> is performed as part of a broader AI/ML project, such as lineage tracking, as we mentioned in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>. In large AI/ML projects, there may be multiple teams of data scientists performing experiments and training hundreds of models. Keeping track of their results and sharing them with each other can be very challenging in <span>large projects.</span></p>
<h2 id="_idParaDest-47" class="calibre9"><a id="_idTextAnchor046" class="calibre6 pcalibre pcalibre1"/>Configuring and tuning hyperparameters</h2>
<p class="calibre3">Finding the optimal set of hyperparameter values can be almost as challenging as curating a usable dataset<a id="_idIndexMarker185" class="calibre6 pcalibre pcalibre1"/> or choosing the correct algorithm to use. Considering that hyperparameters affect how our model training jobs operate, each job can take a long time to run, and there can <a id="_idIndexMarker186" class="calibre6 pcalibre pcalibre1"/>be thousands of combinations of hyperparameter values to explore, doing this manually can be very challenging<a id="_idIndexMarker187" class="calibre6 pcalibre pcalibre1"/> <span>and time-consuming.</span></p>
<h2 id="_idParaDest-48" class="calibre9"><a id="_idTextAnchor047" class="calibre6 pcalibre pcalibre1"/>Evaluating models</h2>
<p class="calibre3">Although we generally<a id="_idIndexMarker188" class="calibre6 pcalibre pcalibre1"/> perform some validation and testing during the training and hyperparameter tuning processes, we should thoroughly evaluate our model before deploying it in production. In the section titled “Common challenges in developing machine learning applications” in <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, we discussed the challenge of delivering business value from data science projects. We called out the need for data scientists to work with business stakeholders to thoroughly understand the business requirements that the target AI/ML system is intended to address. The evaluation step in our data science project is where we check to ensure that the models and solutions we’ve created adequately address those business requirements, based on the metrics we defined for measuring success. In addition to the data science team evaluating the models, it may also be relevant at this point to review the results with the business stakeholders to ensure they align with expectations. If we find that the results are not satisfactory, we usually need to retry the process from earlier in the data science lifecycle, perhaps with new or different data, different algorithms, and/or different hyperparameter values. We may need to repeat these steps in an interactive process until we appropriately satisfy the <span>business requirements.</span></p>
<h2 id="_idParaDest-49" class="calibre9"><a id="_idTextAnchor048" class="calibre6 pcalibre pcalibre1"/>Deploying models</h2>
<p class="calibre3">When deploying our model, we’ll need to <a id="_idIndexMarker189" class="calibre6 pcalibre pcalibre1"/>select the kinds of computing resources that are required to serve our model adequately. Depending on the model architecture, we may need to include GPUs/TPUs, usually also in combination with CPUs, and, of course, RAM. A really important activity at this <a id="_idIndexMarker190" class="calibre6 pcalibre pcalibre1"/>point in the project is to “right-size” these components. To do this, we’ll need to estimate how much of each type of component will be required to serve our model as accurately as possible given the traffic we expect to receive in terms of requests per second to our model. Why is this so important? Well, model hosting is often reported by companies to be by far their number one cost when it comes to their AI/ML. It can account for up to 90% of a company’s AI/ML costs. So, if we configure our servers with more resources than we need, it will increase those costs. On the other hand, if we don’t configure enough resources, we will not be able to handle the number of requests coming from our clients, resulting in disruption of service from our models. Fortunately, cloud AI/ML services such as Vertex will auto-scale our model-hosting infrastructure to meet increased demand, but we still need to get the sizing as accurate as possible for each server in order to control <span>our costs.</span></p>
<p class="calibre3">Something we also need to keep in mind when deploying our models is how quickly our applications need to get responses for the inference requests. We will need to keep this in mind as we architect our model <span>hosting infrastructure.</span></p>
<h2 id="_idParaDest-50" class="calibre9"><a id="_idTextAnchor049" class="calibre6 pcalibre pcalibre1"/>Monitoring models after deployment</h2>
<p class="calibre3">In addition to monitoring the various metrics associated with our models, an important concept to call out at this point is something referred to as <strong class="bold">drift</strong>, which can be represented in various formats, such <a id="_idIndexMarker191" class="calibre6 pcalibre pcalibre1"/>as model <a id="_idIndexMarker192" class="calibre6 pcalibre pcalibre1"/>drift, data drift, or concept drift. To explain the concept of drift, we will first dive a bit deeper into the relationship between the model training process and how models operate <span>in production.</span></p>
<p class="calibre3">Note that during model training, the model <a id="_idIndexMarker193" class="calibre6 pcalibre pcalibre1"/>was exposed to data in a specific format and with specific constraints, and that’s how it learned. Let’s refer to the state of the input data as its <strong class="bold">shape</strong>, which refers to the format and constraints of that data. When we deploy our model to production and we expose it to new data in order to get predictions from the model, we want to ensure that the shape of the new data matches the shape of the training data as much as possible. We are not referring to the contents of the data but rather how the data is represented to <span>the model.</span></p>
<p class="calibre3">During our training process, we may have performed transformations on raw data to make it more suitable for training the model. If so, we need to perform the same kinds of transformations on any new data that we send to the model to make predictions. In the real world, however, data can change over time, and therefore the shape of the data can change over time. We refer to this as drift, which is when the raw data out in the real world has fundamentally changed in some way since we trained our model. Let’s look at a couple of examples of drift in order to clarify <span>the point:</span></p>
<p class="calibre3"><span><strong class="bold">Example one</strong></span><span>:</span></p>
<p class="calibre3">We trained our model on data that was gathered from customers filling in forms online. Our model tries to predict whether those customers would likely respond well to a specific marketing campaign in which we would send them targeted emails with discounts on shoes. Recently, an administrator decided that they wanted to capture some additional information from customers and that some of the previously captured data is no longer relevant, so they added some fields to the form and removed some other fields. Now, when new customers fill in the form and that data is sent to our model, there will be extra fields in the input that the model has never seen before and the other fields that it expects to see will no longer be there. This could affect our model’s ability to interpret and use the input data effectively, causing errors or <span>incorrect predictions.</span></p>
<p class="calibre3"><span><strong class="bold">Example two</strong></span><span>:</span></p>
<p class="calibre3">We have built a model that estimates how quickly we can deliver products to customers. The model uses inputs from a number of different sources. One of the sources is a dataset that contains historical data on how long it took to deliver products to customers in past deliveries. We receive an update of that dataset every day, which contains the details of all of the orders that were delivered the previous day. A critical feature in that dataset is the delivery time, which is measured in days. This dataset is created by a system that is owned by a different organization in our company, so we do not have control over that dataset. Our delivery process has become a lot more efficient recently, and some products are being delivered the same day, so the delivery time has now been updated to be <a id="_idIndexMarker194" class="calibre6 pcalibre pcalibre1"/>measured in hours instead of days. However, nobody informed our data science team about this change. Now, our model looks at the delivery time feature, and because the <a id="_idIndexMarker195" class="calibre6 pcalibre pcalibre1"/>unit of measurement has changed, its predictions for new delivery times <span>are incorrect.</span></p>
<p class="calibre3">Another interesting example is something with which we all are somewhat familiar. Many large retailers use AI/ML models to forecast what to stock in their inventory based on data related to what customers are purchasing and they try to look for emerging trends to identify changes in consumer behavior. In the first few weeks of the COVID-19 pandemic, there was an enormous and sudden shift in what everybody wanted to buy. The models were probably surprised to find that everybody was suddenly very interested in one thing in particular, which up until then, was generally sold at a very predictable rate. What was this thing that the models predicted everybody was suddenly interested in? <span>Toilet paper!</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer043">
<img alt="Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)" src="image/B18143_02_11.jpg" class="calibre52"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 2.11: Empty toilet paper shelves in a store (source: https://commons.wikimedia.org/wiki/File:COVID-19_-_Toilet_Paper_Shortage_%2849740588227%29.jpg)</p>
<p class="calibre3">There are also much more subtle changes that can happen to our data over time. We’ve already discussed how data scientists often want to inspect data before training a model, and one of the aspects they often want to inspect is the statistical distribution of the values of each of the <a id="_idIndexMarker196" class="calibre6 pcalibre pcalibre1"/>features (mean, mode, maximum, minimum, etc.). These are important details <a id="_idIndexMarker197" class="calibre6 pcalibre pcalibre1"/>because they give us an understanding of what kinds of values our features are generally expected to contain. During inspection, this can help us to identify outliers that could indicate erroneous data or that may inform us of some other characteristic of the data that we were previously unaware of. We can also apply this knowledge to make predictions during production. We can analyze the data that is sent to our models for inference purposes, and if we see that the statistical distributions have changed in a consistent way, then it could alert us to potential data corruption or to the fact that the data has genuinely changed, which could indicate a need to update our models by training them on new data that matches the updated shape that we’re observing in the <span>real world.</span></p>
<p class="calibre3">As we can see, drift can lead to our models becoming less accurate or providing erroneous results, and we therefore need to monitor specifically for this by inspecting the data that we’re observing in production, as well as monitoring the expected metrics for our models. If we see that our model’s metrics are declining over time—or suddenly—this could be an indication of drift between the kind of data the model was trained on and the kind of data <a id="_idIndexMarker198" class="calibre6 pcalibre pcalibre1"/>it observes <span>in production.</span></p>
<h1 id="_idParaDest-51" class="calibre5"><a id="_idTextAnchor050" class="calibre6 pcalibre pcalibre1"/>Best practices for overcoming common challenges</h1>
<p class="calibre3">This section contains pointers and best practices that companies have developed over time to address many of the challenges discussed in the <span>previous section.</span></p>
<h2 id="_idParaDest-52" class="calibre9"><a id="_idTextAnchor051" class="calibre6 pcalibre pcalibre1"/>Finding and gathering relevant data</h2>
<p class="calibre3">We discussed data silos <a id="_idIndexMarker199" class="calibre6 pcalibre pcalibre1"/>being a common challenge in large companies, as well as restrictions with regard to how data is stored and accessed, especially sensitive data that could be subject to various regulations and compliance requirements. A key to overcoming these challenges is to break down the silos by creating centralized data lakes and data discovery mechanisms, such as a searchable data catalog that contains metadata describing the various <a id="_idIndexMarker200" class="calibre6 pcalibre pcalibre1"/>datasets in our data lake. To ensure that our data is stored and accessed securely, we need to implement robust encryption and permission-based access mechanisms. We will explore these topics in more detail in later chapters, and we will also perform some hands-on activities regarding detecting and addressing bias and other problems in <span>our datasets.</span></p>
<p class="calibre3">In cases where our data exists in databases that are restricted specifically for transactional business operations, we could<a id="_idIndexMarker201" class="calibre6 pcalibre pcalibre1"/> implement a <strong class="bold">change data capture</strong> (<strong class="bold">CDC</strong>) solution that replicates our data to a data lake that can then be used for data analytics and <span>AI/ML workloads.</span></p>
<p class="calibre3">Considering that the data-gathering process happens at the very beginning of our AI/ML workload, it’s critical that we implement data quality checks at this point in order to prevent issues later in our workload. For example, we know that training our models on corrupt data will result in errors or less accurate model outputs. Bear in mind that in most ML use cases, we are periodically training our models on updated data that’s coming from some source, perhaps from a system that is owned and operated by another team or organization. Therefore, if we create data quality checks as this data is coming into our workload and we detect data quality issues, we should implement mechanisms that prevent further steps in our process from proceeding. Otherwise, performing the downstream steps in our process, such as data transformations and model training, would be a waste of time and money and could lead to worse consequences in production, such <a id="_idIndexMarker202" class="calibre6 pcalibre pcalibre1"/>as <span>malfunctioning models.</span></p>
<h2 id="_idParaDest-53" class="calibre9"><a id="_idTextAnchor052" class="calibre6 pcalibre pcalibre1"/>Data labeling</h2>
<p class="calibre3">If your company is having difficulties <a id="_idIndexMarker203" class="calibre6 pcalibre pcalibre1"/>finding a workforce to perform your labeling tasks, Google Cloud’s data labeling service can help you to get your data <span>labeled appropriately.</span></p>
<h2 id="_idParaDest-54" class="calibre9"><a id="_idTextAnchor053" class="calibre6 pcalibre pcalibre1"/>Picking an algorithm and model architecture</h2>
<p class="calibre3">Where should we start when picking an algorithm? This is a common challenge, and as a result, data scientists have been constructing solutions to try to make this easier. In this section, we’ll describe a tiered framework for approaching a new AI/ML project in <span>this context:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Tier 1</strong>: You can see if a packaged solution already exists for your business problem. For example, Google Cloud has created packaged solutions for lots of different types of use cases such as computer vision, NLP, and forecasting. We’ll be covering these solutions in more detail in the <span>coming chapters.</span></li>
<li class="calibre8"><strong class="bold">Tier 2</strong>: If you want to create and deploy your own model without doing any of the work required to do so, check out Google Cloud’s AutoML functionality to see if it meets your needs. We’ll also explore this in a hands-on activity in a later chapter in <span>this book.</span></li>
<li class="calibre8"><strong class="bold">Tier 3</strong>: If you want to get started with a model that has been trained by somebody else, there are numerous hubs and “model zoos” that exist for data scientists to share models they’ve created. Analogous to software libraries in traditional software development, these are assets that have been created by other data scientists for specific purposes, which you can reuse rather than starting from scratch to implement the same functionality. For example, you can find pre-trained models for various use cases in Google Cloud’s AI <span>Hub (</span><span>https://cloud.google.com/ai-hub/docs/introduction</span><span>).</span></li>
<li class="calibre8"><strong class="bold">Tier 4</strong>: If the previous options do not meet your specific needs, you can create your own custom models. In this case, Google Cloud Vertex AI provides built-in algorithms for common use cases such as linear regression, image classification, object detection, and many more, or you can install your own custom model to run on Vertex AI. Vertex AI provides many tools for each step in the AI/ML project life cycle, and we <a id="_idIndexMarker204" class="calibre6 pcalibre pcalibre1"/>will explore most of them in <span>this book.</span></li>
</ul>
<h2 id="_idParaDest-55" class="calibre9"><a id="_idTextAnchor054" class="calibre6 pcalibre pcalibre1"/>Training models</h2>
<p class="calibre3">There are some established<a id="_idIndexMarker205" class="calibre6 pcalibre pcalibre1"/> methods to address overfitting and underfitting. One cause for overfitting can be that the model did not get access to a sufficient amount of different data points from which to learn an appropriate pattern. Looking at a very extreme case of this, let’s imagine that we have just one data point in our dataset and our model processes this data point over and over again until it finds a set of coefficients that it can use to relate the input features accurately to the target output for that data point. Now, whenever it sees that same data point, it can easily and accurately predict the target variable. However, if we show it a new datapoint with a similar structure—i.e., the same number and types of features—but different values for those features, it’s unlikely that our model will accurately predict the output for the new datapoint because it has only learned the specific characteristics of a single data point during training. This is a case of overfitting, whereby the model works really well on the specific data point on which it was trained but does not make accurate predictions for other <span>data points.</span></p>
<p class="calibre3">One method that can help to address overfitting in this regard is to provide more data points when training our model. If our algorithm has seen thousands or millions of data points during training, it’s much more likely to have built a more generalized model that has a broader understanding of the feature space and the relationship between the features and the target variables. Therefore, when it sees a new data point, it may be able to make more accurate predictions for the target variable of that new <span>data point.</span></p>
<p class="calibre3">There is a trade-off that we need to keep in mind in this context: although our training process is likely to build a more generalized model as we provide more and more data points, we need to keep in mind that training models on large datasets can be expensive. We may find that after the model has seen millions of data points, each new training iteration is only increasing the model’s generalization metrics by very small amounts. For example, if our model currently has a 99.67% accuracy rate and each training iteration increases its accuracy by 0.0001% but it costs thousands of dollars to do so, it may not make sense from a financial perspective to keep training the model on more and more data points, especially if our business considers 99.5% to be accurate enough to meet the business needs. This last point is important—the trade-off in training costs versus increases in accuracy is dependent on the business requirements. If we’re building a model for a medical diagnosis use case or a model whose forecasting accuracy can cost our company millions of dollars if it is incorrect by 0.001%, then it may be worth it to keep training the model on more data points. In any case, what you will generally need to do is define a threshold at which the business considers the model’s metrics to be sufficient and measure the increase in that metric as the model is trained on more data points. If you see that the metric begins to plateau after a certain amount of data points, it may be time to stop the <span>training process.</span></p>
<p class="calibre3">It should be noted that adding more data points is not always an option because you may only have a limited dataset to begin with and it may be difficult to gather more real-world data for your<a id="_idIndexMarker206" class="calibre6 pcalibre pcalibre1"/> specific use case. In these scenarios, you may be able to generate synthetic data with similar characteristics as your real-world data or use mechanisms to optimize the use of your existing dataset during the training process, such as maximizing the training dataset by using cross-validation, which we’ll explore in a hands-on activity later in <span>this book.</span></p>
<p class="calibre3">Another potential cause of overfitting is if the model is too “complex”, by which we mean that too many features may be used for each data point in the training dataset. Again, taking an extreme example, if each data point has thousands of features, the model will learn very specific relationships between the features and the training dataset, which may not generalize well to other data points. In this case, a solution could be to remove features that are not deemed to be critical to figuring out the optimal relationship <a id="_idIndexMarker207" class="calibre6 pcalibre pcalibre1"/>between the features and the target variables. Selecting the relevant features can be a challenge in itself, and we will explore mechanisms such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) to help<a id="_idIndexMarker208" class="calibre6 pcalibre pcalibre1"/> select the most <span>relevant features.</span></p>
<p class="calibre3">The opposite of overfitting, then, is underfitting, and one potential cause of underfitting is if the model is too simple, in which case there may not be a sufficient number of features for each data point in the dataset for the model to determine a meaningful relationship between the features and the target variables. Of course, in this case, we would want to find or generate additional features than can help our model to learn more meaningful relationships between those features and the <span>target variables.</span></p>
<p class="calibre3">To address the challenge of keeping track of experiments and their results in large-scale ML projects, we will use Vertex ML Metadata, which tracks all of our experiments and their inputs and outputs for <a id="_idIndexMarker209" class="calibre6 pcalibre pcalibre1"/>us (i.e., the lineage of our data and ML <span>model artifacts).</span></p>
<h2 id="_idParaDest-56" class="calibre9"><a id="_idTextAnchor055" class="calibre6 pcalibre pcalibre1"/>Configuring and tuning hyperparameters</h2>
<p class="calibre3">There are some methodical ways in which to explore what we call the “hyperparameter space”, which means all of <a id="_idIndexMarker210" class="calibre6 pcalibre pcalibre1"/>the different possible values for our hyperparameters. The following are some <span>popular methods:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Random search</strong>: The random search <a id="_idIndexMarker211" class="calibre6 pcalibre pcalibre1"/>approach uses a subsampling technique in which hyperparameter values are selected at<a id="_idIndexMarker212" class="calibre6 pcalibre pcalibre1"/> random for each training job experiment. This will not result in all possible values of every hyperparameter being tested, but it can often be quite an efficient method for finding an effective set of <span>hyperparameter values.</span></li>
<li class="calibre8"><strong class="bold">Grid search</strong>: The grid search<a id="_idIndexMarker213" class="calibre6 pcalibre pcalibre1"/> approach to hyperparameter <a id="_idIndexMarker214" class="calibre6 pcalibre pcalibre1"/>tuning is the most exhaustive because it will try out every possible combination of values of each hyperparameter. This means that it will generally take a lot more time than a random search approach. Also, bear in mind that each training job costs money, so if you have a large hyperparameter space, this can be very expensive or <span>even infeasible.</span></li>
<li class="calibre8"><strong class="bold">Bayesian optimization</strong>: Earlier in this chapter, we talked about using gradient descent to optimize a function<a id="_idIndexMarker215" class="calibre6 pcalibre pcalibre1"/> by finding its minimum point. Bayesian optimization is another type of <a id="_idIndexMarker216" class="calibre6 pcalibre pcalibre1"/>optimization technique. It’s quite a complex process, and it is usually more efficient than the other approaches mentioned previously. Fortunately, Google Cloud’s Vertex AI Vizier service will perform Bayesian optimization for you, so if you use that tool, you won’t need to implement <span>it yourself.</span><p class="calibre3">If you are interested in diving into the inner workings of Bayesian optimization, I recommend referring to the following <span>paper: </span><a href="https://arxiv.org/abs/1807.02811" class="calibre6 pcalibre pcalibre1"><span>https://arxiv.org/abs/1807.02811</span></a></p></li>
</ul>
<p class="calibre3">Google Cloud’s Vertex AI Vizier service will run lots of training job experiments for you, trying out many different combinations of hyperparameter values for each experiment, and will find the optimal hyperparameter values to run your ML training <span>jobs efficiently.</span></p>
<p class="callout-heading">Key tip</p>
<p class="callout">To save yourself a lot of painstaking work and time when doing hyperparameter optimization, use a cloud service that has been built for that purpose, such as Google Cloud’s Vertex AI <span>Vizier service.</span></p>
<h2 id="_idParaDest-57" class="calibre9"><a id="_idTextAnchor056" class="calibre6 pcalibre pcalibre1"/>Deploying models</h2>
<p class="calibre3">Latency is often a key factor <a id="_idIndexMarker217" class="calibre6 pcalibre pcalibre1"/>in the deployment of our models, in that we need to ensure our model hosting infrastructure meets the latency requirements expected by our <span>client applications.</span></p>
<p class="calibre3">One decision point that <a id="_idIndexMarker218" class="calibre6 pcalibre pcalibre1"/>comes into view in this context is whether we have a batch or online use case. In an online use case, a client sends a piece of input data to our model and it waits to receive an inference response. This usually happens when our client application needs an answer quickly, such as when a customer is performing a transaction on our website and we want to see if the transaction seems fraudulent. This is a real-time use case, and therefore the latency generally needs to be very low; perhaps a few milliseconds. You will usually need to work with business leaders to define the <span>acceptable latency.</span></p>
<p class="calibre3">In a batch use case, our model can process large amounts of data at a time. For example, as input to our model at inference time, we may provide a large file containing thousands or millions of data points for which we want our model to make predictions, and our model could work for hours on processing those inputs and save all of the inference results as outputs in another file, which we could <span>reference later.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">Batch use cases are usually associated with scenarios in which we do not require low latency. However, ironically, there is also a scenario in which batch use cases can actually help provide lower latency<a id="_idIndexMarker219" class="calibre6 pcalibre pcalibre1"/> at inference time. Consider a scenario in which we’re running a retail website and we want to get insights from our users’ purchasing histories in order to recommend products that customers may be interested in buying when they visit our website. Depending on the amount of historical data we have, it could take a long time to process that data. Therefore, we don’t want to do this in real time when a customer visits our website. Instead, we could periodically run a batch inference job every night and store our results in a file or a key–value database. Then, when customers visit our site, we can fetch the pre-computed inferences from our file or database. In this scenario, fetching a value from a file or database will usually be much quicker than performing an online inference in real time. Note that this would only be suitable for certain use cases. For example, it would not work for a transactional fraud evaluation use case because we would need real-time characteristics from the ongoing transaction for that scenario. Consequently, as a data scientist or AI/ML solutions architect, you will need to determine what kind of inferencing approach works best for each of your <span>use cases.</span></p>
<h2 id="_idParaDest-58" class="calibre9"><a id="_idTextAnchor057" class="calibre6 pcalibre pcalibre1"/>Monitoring models after deployment</h2>
<p class="calibre3">If you detect that drift is <a id="_idIndexMarker220" class="calibre6 pcalibre pcalibre1"/>occurring in production, this is a sign that you may need to update your models. We recommend putting mechanisms in place to automate the retraining of your models with updated data if you detect drift, especially if you’re managing large numbers of <a id="_idIndexMarker221" class="calibre6 pcalibre pcalibre1"/>models. I’ve worked with organizations that are running hundreds of models simultaneously in production, and it’s not feasible to manually monitor, curate, and retrain all of those models on an ongoing basis. In that scenario, we’ve implemented MLOps frameworks that would retrain the models on updated data whenever a model’s metrics consistently dropped below a preconfigured threshold that we considered to be acceptable. The MLOps framework would then test the new model, and if the new model’s metrics outperformed the current model’s metrics in production, it would replace the production model with the <span>new model.</span></p>
<p class="calibre3">When it comes to defining and <a id="_idIndexMarker222" class="calibre6 pcalibre pcalibre1"/>monitoring the model’s operational metrics in production, you can use Google Cloud Monitoring for <span>that purpose.</span></p>
<h1 id="_idParaDest-59" class="calibre5"><a id="_idTextAnchor058" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we explored a quick recap of the traditional SDLC and introduced the concept of the ML model development life cycle. We discussed each of the steps that we usually encounter in most AI/ML projects, and then we dived into specific challenges that generally exist in each step. Finally, we covered approaches and best practices that companies have learned over time to help them address some of those <span>common challenges.</span></p>
<p class="calibre3">In the next chapter, we’ll begin to explore the various different services in Google Cloud that can be used to implement <span>AI/ML workloads.</span></p>
</div>
</div></body></html>