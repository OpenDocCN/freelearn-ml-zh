<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;7.&#xA0;Model Evaluation"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Model Evaluation </h1></div></div></div><p class="calibre7">In this chapter, we will cover the following topics:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Estimating model performance with k-fold cross-validation</li><li class="listitem">Performing cross-validation with the e1071 package</li><li class="listitem">Performing cross-validation with the caret package</li><li class="listitem">Ranking the variable importance with the caret package</li><li class="listitem">Ranking the variable importance with the rminer package</li><li class="listitem">Finding highly correlated features with the caret package</li><li class="listitem">Selecting features using the caret package</li><li class="listitem">Measuring the performance of a regression model</li><li class="listitem">Measuring the prediction performance with the confusion matrix</li><li class="listitem">Measuring the prediction performance using ROCR</li><li class="listitem">Comparing an ROC curve using the caret package</li><li class="listitem">Measuring performance differences between models with the caret package</li></ul></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;Model Evaluation">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec77" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Model evaluation is performed to ensure that a fitted model can accurately predict responses<a id="id593" class="calibre1"/> for future or unknown subjects. Without model evaluation, we might train models that over-fit in the training data. To prevent overfitting, we can employ packages, such as <code class="email">caret</code>, <code class="email">rminer</code>, and <code class="email">rocr</code> to evaluate the performance of the fitted model. Furthermore, model evaluation can help select the optimum model, which is more robust and can accurately predict responses for future subjects.</p><p class="calibre7">In the following chapter, we will discuss how one can implement a simple R script or use one of the packages (for example, <code class="email">caret</code> or <code class="email">rminer</code>) to evaluate the performance of a fitted model.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Estimating model performance with k-fold cross-validation"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec78" class="calibre1"/>Estimating model performance with k-fold cross-validation</h1></div></div></div><p class="calibre7">The k-fold cross-validation technique is a common technique used to estimate the performance <a id="id594" class="calibre1"/>of a classifier as it overcomes the problem of over-fitting. For k-fold cross-validation, the method does not use the entire dataset to build the model, instead it splits the data into a training dataset and a testing dataset. Therefore, the model built with a training dataset can then be used to assess the performance of the model on the testing dataset. By performing n repeats of the k-fold validation, we can then use the average of <span class="strong"><em class="calibre8">n</em></span> accuracies to truly assess the performance of the built model. In this recipe, we will illustrate how to perform a k-fold cross-validation.</p></div>

<div class="book" title="Estimating model performance with k-fold cross-validation">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec261" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to train the support vector machine. For those who have not prepared the dataset, please refer to <a class="calibre1" title="Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic" href="part0060_split_000.html#page">Chapter 5</a>, <span class="strong"><em class="calibre8">Classification (I) – Tree, Lazy, and Probabilistic</em></span>, for detailed information.</p></div></div>

<div class="book" title="Estimating model performance with k-fold cross-validation">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec262" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to cross-validate the telecom <code class="email">churn</code> dataset:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Split the index into <code class="email">10</code> fold using the cut function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ind = cut(1:nrow(churnTrain), breaks=10, labels=F)</strong></span>
</pre></div></li><li class="listitem" value="2">Next, use <code class="email">for</code> loop to perform a 10 fold cross-validation, repeated <code class="email">10</code> times:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; accuracies = c()</strong></span>
<span class="strong"><strong class="calibre2">&gt; for (i in 1:10) {</strong></span>
<span class="strong"><strong class="calibre2">+   fit = svm(churn ~., churnTrain[ind != i,])</strong></span>
<span class="strong"><strong class="calibre2">+   predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">+   correct_count = sum(predictions == churnTrain[ind == i,c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">+   accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)</strong></span>
<span class="strong"><strong class="calibre2">+ }</strong></span>
</pre></div></li><li class="listitem" value="3">You can then print the accuracies:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; accuracies</strong></span>
<span class="strong"><strong class="calibre2"> [1] 0.9341317 0.8948949 0.8978979 0.9459459 0.9219219 0.9281437 0.9219219 0.9249249 0.9189189 0.9251497</strong></span>
</pre></div></li><li class="listitem" value="4">Lastly, you can generate average accuracies with the <code class="email">mean</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mean(accuracies)</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.9213852</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Estimating model performance with k-fold cross-validation">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec263" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id595" class="calibre1"/> recipe, we implement a simple script performing 10-fold cross-validations. We first generate an index with 10 fold with the <code class="email">cut</code> function. Then, we implement a <code class="email">for</code> loop to perform a 10-fold cross-validation 10 times. Within the loop, we first apply <code class="email">svm</code> on <code class="email">9</code> folds of data as the training set. We then use the fitted model to predict the label of the rest of the data (the testing dataset). Next, we use the sum of the correctly predicted labels to generate the accuracy. As a result of this, the loop stores 10 generated accuracies. Finally, we use the <code class="email">mean</code> function to retrieve the average of the accuracies.</p></div></div>

<div class="book" title="Estimating model performance with k-fold cross-validation">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec264" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">If you wish to perform the k-fold validation with the use of other models, simply replace the line to generate the variable fit to whatever classifier you prefer. For example, if you would like to assess the Naïve Bayes model with a 10-fold cross-validation, you just need to replace the calling function from <code class="email">svm</code> to <code class="email">naiveBayes</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; for (i in 1:10) {</strong></span>
<span class="strong"><strong class="calibre2">+   fit = naiveBayes(churn ~., churnTrain[ind != i,])</strong></span>
<span class="strong"><strong class="calibre2">+   predictions = predict(fit, churnTrain[ind == i, ! names(churnTrain) %in% c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">+   correct_count = sum(predictions == churnTrain[ind == i,c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">+   accuracies = append(correct_count / nrow(churnTrain[ind == i,]), accuracies)</strong></span>
<span class="strong"><strong class="calibre2">+ }</strong></span>
</pre></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing cross-validation with the e1071 package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec79" class="calibre1"/>Performing cross-validation with the e1071 package</h1></div></div></div><p class="calibre7">Besides<a id="id596" class="calibre1"/> implementing a <code class="email">loop</code> function <a id="id597" class="calibre1"/>to perform the k-fold cross-validation, you can use the <code class="email">tuning</code> function (for example, <code class="email">tune.nnet</code>,<code class="email"> tune.randomForest</code>,<code class="email"> tune.rpart</code>,<code class="email"> tune.svm</code>, and <code class="email">tune.knn</code>.) within the <code class="email">e1071</code> package to obtain the minimum error value. In this recipe, we will illustrate how to use <code class="email">tune.svm</code> to perform the 10-fold cross-validation and obtain the optimum classification model.</p></div>

<div class="book" title="Performing cross-validation with the e1071 package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec265" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we continue to use the telecom <code class="email">churn</code> dataset as the input data source to perform 10-fold cross-validation.</p></div></div>

<div class="book" title="Performing cross-validation with the e1071 package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec266" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform<a id="id598" class="calibre1"/> the following steps to<a id="id599" class="calibre1"/> retrieve the minimum estimation error using cross-validation:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Apply <code class="email">tune.svm</code> on the training dataset, <code class="email">trainset</code>, with the 10-fold cross-validation as the tuning control. (If you find an error message, such as <code class="email">could not find function predict.func</code>, please clear the workspace, restart the R session and reload the <code class="email">e1071</code> library again):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tuned = tune.svm(churn~., data = trainset, gamma = 10^-2, cost = 10^2, tunecontrol=tune.control(cross=10))</strong></span>
</pre></div></li><li class="listitem" value="2">Next, you can obtain the summary information of the model, tuned:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(tuned)</strong></span>

<span class="strong"><strong class="calibre2">Error estimation of 'svm' using 10-fold cross validation: 0.08164651</strong></span>
</pre></div></li><li class="listitem" value="3">Then, you can access the performance details of the tuned model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tuned$performances</strong></span>
<span class="strong"><strong class="calibre2">  gamma cost      error dispersion</strong></span>
<span class="strong"><strong class="calibre2">1  0.01  100 0.08164651 0.02437228</strong></span>
</pre></div></li><li class="listitem" value="4">Lastly, you can use the optimum model to generate a classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svmfit = tuned$best.model</strong></span>
<span class="strong"><strong class="calibre2">&gt; table(trainset[,c("churn")], predict(svmfit))</strong></span>
<span class="strong"><strong class="calibre2">     </strong></span>
<span class="strong"><strong class="calibre2">       yes   no</strong></span>
<span class="strong"><strong class="calibre2">  yes  234  108</strong></span>
<span class="strong"><strong class="calibre2">  no    13 1960</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing cross-validation with the e1071 package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec267" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The <code class="email">e1071</code> package provides miscellaneous functions to build and assess models, therefore, you do not need to reinvent the wheel to evaluate a fitted model. In this recipe, we use the <code class="email">tune.svm</code> function to tune the svm model with the given formula, dataset, gamma, cost, and control functions. Within the <code class="email">tune.control</code> options, we configure the option as <code class="email">cross=10</code>, which performs a 10-fold cross validation during the tuning process. The tuning process will eventually return the minimum estimation error, performance detail, and<a id="id600" class="calibre1"/> the best model<a id="id601" class="calibre1"/> during the tuning process. Therefore, we can obtain the performance measures of the tuning and further use the optimum model to generate a classification table.</p></div></div>

<div class="book" title="Performing cross-validation with the e1071 package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec268" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In the <code class="email">e1071</code> package, the <code class="email">tune</code> function uses a grid search to tune parameters. For those interested in other tuning functions, use the help function to view the <code class="email">tune</code> document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?e1071::tune</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Performing cross-validation with the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec80" class="calibre1"/>Performing cross-validation with the caret package</h1></div></div></div><p class="calibre7">The <code class="email">Caret</code> (classification and regression training) package contains many functions in regard to <a id="id602" class="calibre1"/>the training process for <a id="id603" class="calibre1"/>regression and classification problems. Similar to the <code class="email">e1071</code> package, it also contains a function to perform the k-fold cross validation. In this recipe, we will demonstrate how to the perform k-fold cross validation using the <code class="email">caret</code> package.</p></div>

<div class="book" title="Performing cross-validation with the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec269" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to perform the k-fold cross validation.</p></div></div>

<div class="book" title="Performing cross-validation with the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec270" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to perform the k-fold cross-validation with the <code class="email">caret</code> package:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, set up the control parameter to train with the 10-fold cross validation in <code class="email">3</code> repetitions:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; control = trainControl(method="repeatedcv", number=10, repeats=3)</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you can train the classification model on telecom churn data with <code class="email">rpart</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model = train(churn~., data=trainset, method="rpart", preProcess="scale", trControl=control)</strong></span>
</pre></div></li><li class="listitem" value="3">Finally, you can examine the output of the generated model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model</strong></span>
<span class="strong"><strong class="calibre2">CART </strong></span>

<span class="strong"><strong class="calibre2">2315 samples</strong></span>
<span class="strong"><strong class="calibre2">  16 predictor</strong></span>
<span class="strong"><strong class="calibre2">   2 classes: 'yes', 'no' </strong></span>

<span class="strong"><strong class="calibre2">Pre-processing: scaled </strong></span>
<span class="strong"><strong class="calibre2">Resampling: Cross-Validated (10 fold, repeated 3 times) </strong></span>

<span class="strong"><strong class="calibre2">Summary of sample sizes: 2084, 2083, 2082, 2084, 2083, 2084, ... </strong></span>

<span class="strong"><strong class="calibre2">Resampling results across tuning parameters:</strong></span>

<span class="strong"><strong class="calibre2">  cp      Accuracy  Kappa  Accuracy SD  Kappa SD</strong></span>
<span class="strong"><strong class="calibre2">  0.0556  0.904     0.531  0.0236       0.155   </strong></span>
<span class="strong"><strong class="calibre2">  0.0746  0.867     0.269  0.0153       0.153   </strong></span>
<span class="strong"><strong class="calibre2">  0.0760  0.860     0.212  0.0107       0.141   </strong></span>

<span class="strong"><strong class="calibre2">Accuracy was used to select the optimal model using the largest value.</strong></span>
<span class="strong"><strong class="calibre2">The final value used for the model was cp = 0.05555556.</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Performing cross-validation with the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec271" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id604" class="calibre1"/>recipe, we demonstrate how<a id="id605" class="calibre1"/> convenient it is to conduct the k-fold cross-validation using the <code class="email">caret</code> package. In the first step, we set up the training control and select the option to perform the 10-fold cross-validation in three repetitions. The process of repeating the k-fold validation is called repeated k-fold validation, which is used to test the stability of the model. If the model is stable, one should get a similar test result. Then, we apply <code class="email">rpart</code> on the training dataset with the option to scale the data and to train the model with the options configured in the previous step.</p><p class="calibre7">After the training process is complete, the model outputs three resampling results. Of these results, the model with <code class="email">cp=0.05555556</code> has the largest accuracy value (<code class="email">0.904</code>), and is therefore selected as the optimal model for classification.</p></div></div>

<div class="book" title="Performing cross-validation with the caret package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec272" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">You <a id="id606" class="calibre1"/>can configure the <code class="email">resampling</code> function in <code class="email">trainControl</code>, in which you can specify <code class="email">boot</code>, <code class="email">boot632</code>, <code class="email">cv</code>, <code class="email">repeatedcv</code>, <code class="email">LOOCV</code>, <code class="email">LGOCV</code>, <code class="email">none</code>, <code class="email">oob</code>, <code class="email">adaptive</code>_<code class="email">cv</code>, <code class="email">adaptive_boot</code>, or <code class="email">adaptive_LGOCV</code>. To view more detailed information of <a id="id607" class="calibre1"/>how to choose the resampling method, view the <code class="email">trainControl</code> document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?trainControl</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Ranking the variable importance with the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec81" class="calibre1"/>Ranking the variable importance with the caret package</h1></div></div></div><p class="calibre7">After building a supervised learning model, we can estimate the importance of features. This<a id="id608" class="calibre1"/> estimation employs a sensitivity analysis to measure the effect on the output of a given model when the inputs are varied. In this recipe, we will show you how to rank the variable importance with the <code class="email">caret</code> package.</p></div>

<div class="book" title="Ranking the variable importance with the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec273" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by storing the fitted <code class="email">rpart</code> object in the <code class="email">model</code> variable.</p></div></div>

<div class="book" title="Ranking the variable importance with the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec274" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to rank the variable importance with the <code class="email">caret</code> package:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you can estimate the variable importance with the <code class="email">varImp</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; importance = varImp(model, scale=FALSE)</strong></span>
<span class="strong"><strong class="calibre2">&gt; importance</strong></span>
<span class="strong"><strong class="calibre2">rpart variable importance</strong></span>

<span class="strong"><strong class="calibre2">                              Overall</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls 116.015</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes             106.988</strong></span>
<span class="strong"><strong class="calibre2">total_day_charge              100.648</strong></span>
<span class="strong"><strong class="calibre2">international_planyes          86.789</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_planyes             25.974</strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge               23.097</strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes              23.097</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages          19.885</strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes              6.347</strong></span>
<span class="strong"><strong class="calibre2">total_eve_calls                 0.000</strong></span>
<span class="strong"><strong class="calibre2">total_day_calls                 0.000</strong></span>
<span class="strong"><strong class="calibre2">total_night_charge              0.000</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls                0.000</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge               0.000</strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes             0.000</strong></span>
<span class="strong"><strong class="calibre2">total_night_calls               0.000</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you<a id="id609" class="calibre1"/> can generate the variable importance plot with the <code class="email">plot</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(importance)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00123.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 1: The visualization of variable importance using the caret package</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Ranking the variable importance with the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec275" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we first use the <code class="email">varImp</code> function to retrieve the variable importance and obtain the summary. The overall results show the sensitivity measure of each attribute. Next, we plot the variable importance in terms of rank, which shows that the <code class="email">number_customer_service_calls</code> attribute is the most important variable in the sensitivity measure.</p></div></div>

<div class="book" title="Ranking the variable importance with the caret package">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec276" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">In some classification packages, such as <code class="email">rpart</code>, the object generated from the training model <a id="id610" class="calibre1"/>contains the variable importance. We can examine the variable importance by accessing the output object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(rpart)</strong></span>
<span class="strong"><strong class="calibre2">&gt; model.rp = rpart(churn~., data=trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; model.rp$variable.importance</strong></span>
<span class="strong"><strong class="calibre2">            total_day_minutes              total_day_charge </strong></span>
<span class="strong"><strong class="calibre2">                   111.645286                    110.881583 </strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls            total_intl_minutes </strong></span>
<span class="strong"><strong class="calibre2">                    58.486651                     48.283228 </strong></span>
<span class="strong"><strong class="calibre2">            total_intl_charge              total_eve_charge </strong></span>
<span class="strong"><strong class="calibre2">                    47.698379                     47.166646 </strong></span>
<span class="strong"><strong class="calibre2">            total_eve_minutes            international_plan </strong></span>
<span class="strong"><strong class="calibre2">                    47.166646                     42.194508 </strong></span>
<span class="strong"><strong class="calibre2">             total_intl_calls         number_vmail_messages </strong></span>
<span class="strong"><strong class="calibre2">                    36.730344                     19.884863 </strong></span>
<span class="strong"><strong class="calibre2">              voice_mail_plan             total_night_calls </strong></span>
<span class="strong"><strong class="calibre2">                    19.884863                      7.195828 </strong></span>
<span class="strong"><strong class="calibre2">              total_eve_calls            total_night_charge </strong></span>
<span class="strong"><strong class="calibre2">                     3.553423                      1.754547 </strong></span>
<span class="strong"><strong class="calibre2">          total_night_minutes               total_day_calls </strong></span>
<span class="strong"><strong class="calibre2">                     1.754547                      1.494986  </strong></span>
</pre></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Ranking the variable importance with the rminer package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec82" class="calibre1"/>Ranking the variable importance with the rminer package</h1></div></div></div><p class="calibre7">Besides <a id="id611" class="calibre1"/>using the <code class="email">caret</code> package to generate variable importance, you can use the <code class="email">rminer</code> package to generate the variable importance of a classification model. In the following recipe, we will illustrate how to use <code class="email">rminer</code> to obtain the variable importance of a fitted model.</p></div>

<div class="book" title="Ranking the variable importance with the rminer package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec277" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to rank the variable importance.</p></div></div>

<div class="book" title="Ranking the variable importance with the rminer package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec278" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to rank the variable importance with <code class="email">rminer</code>:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install and load the package, <code class="email">rminer</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("rminer")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(rminer)</strong></span>
</pre></div></li><li class="listitem" value="2">Fit the svm model with the training set:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; model=fit(churn~.,trainset,model="svm")</strong></span>
</pre></div></li><li class="listitem" value="3">Use the <code class="email">Importance</code> function to obtain the variable importance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; VariableImportance=Importance(model,trainset,method="sensv")</strong></span>
</pre></div></li><li class="listitem" value="4">Plot the <a id="id612" class="calibre1"/>variable importance ranked by the variance:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; L=list(runs=1,sen=t(VariableImportance$imp),sresponses=VariableImportance$sresponses)</strong></span>
<span class="strong"><strong class="calibre2">&gt; mgraph(L,graph="IMP",leg=names(trainset),col="gray",Grid=10)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00124.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 2: The visualization of variable importance using the <code class="email">rminer</code> package</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Ranking the variable importance with the rminer package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec279" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Similar to the <code class="email">caret</code> package, the <code class="email">rminer</code> package can also generate the variable importance of a classification model. In this recipe, we first train the svm model on the training dataset, <code class="email">trainset</code>, with the <code class="email">fit</code> function. Then, we use the <code class="email">Importance</code> function to rank the variable importance with a sensitivity measure. Finally, we use <code class="email">mgraph</code> to plot the rank of the variable importance. Similar to the result obtained from using the <code class="email">caret</code> package, <code class="email">number_customer_service_calls</code> is the most important variable in the measure of sensitivity.</p></div></div>

<div class="book" title="Ranking the variable importance with the rminer package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec280" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The <code class="email">rminer</code> package provides many classification models for one to choose from. If <a id="id613" class="calibre1"/>you are interested in using models other than svm, you can view these options with the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?rminer::fit</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Finding highly correlated features with the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec83" class="calibre1"/>Finding highly correlated features with the caret package</h1></div></div></div><p class="calibre7">When performing regression or classification, some models perform better if highly correlated <a id="id614" class="calibre1"/>attributes are removed. The <code class="email">caret</code> package provides the <code class="email">findCorrelation</code> function, which can be used to find attributes that are highly correlated to each other. In this recipe, we will demonstrate how to find highly correlated features using the <code class="email">caret</code> package.</p></div>

<div class="book" title="Finding highly correlated features with the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec281" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source to find highly correlated features.</p></div></div>

<div class="book" title="Finding highly correlated features with the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec282" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to find highly correlated attributes:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Remove the features that are not coded in numeric characters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; new_train = trainset[,! names(churnTrain) %in% c("churn", "international_plan", "voice_mail_plan")]</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you can obtain the correlation of each attribute:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt;cor_mat = cor(new_train)</strong></span>
</pre></div></li><li class="listitem" value="3">Next, we use <code class="email">findCorrelation</code> to search for highly correlated attributes with a cut off equal to 0.75:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; highlyCorrelated = findCorrelation(cor_mat, cutoff=0.75)</strong></span>
</pre></div></li><li class="listitem" value="4">We then obtain the name of highly correlated attributes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; names(new_train)[highlyCorrelated]</strong></span>
<span class="strong"><strong class="calibre2">[1] "total_intl_minutes"  "total_day_charge"    "total_eve_minutes"   "total_night_minutes"</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Finding highly correlated features with the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec283" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id615" class="calibre1"/>recipe, we search for highly correlated attributes using the <code class="email">caret</code> package. In order to retrieve the correlation of each attribute, one should first remove nonnumeric attributes. Then, we perform correlation to obtain a correlation matrix. Next, we use <code class="email">findCorrelation</code> to find highly correlated attributes with the cut off set to 0.75. We finally obtain the names of highly correlated (with a correlation coefficient over 0.75) attributes, which are <code class="email">total_intl_minutes</code>, <code class="email">total_day_charge</code>, <code class="email">total_eve_minutes</code>, and <code class="email">total_night_minutes</code>. You can consider removing some highly correlated attributes and keep one or two attributes for better accuracy.</p></div></div>

<div class="book" title="Finding highly correlated features with the caret package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec284" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In addition to the <code class="email">caret</code> package, you can use the <code class="email">leaps</code>, <code class="email">genetic</code>, and <code class="email">anneal</code> functions in the <code class="email">subselect</code> package to achieve the same goal</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Selecting features using the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec84" class="calibre1"/>Selecting features using the caret package</h1></div></div></div><p class="calibre7">The feature <a id="id616" class="calibre1"/>selection method searches the subset of features with minimized predictive errors. We can apply feature selection to identify which attributes are required to build an accurate model. The <code class="email">caret</code> package provides a recursive feature elimination function, <code class="email">rfe</code>, which can help automatically select the required features. In the following recipe, we will demonstrate how to use the <code class="email">caret</code> package to perform feature selection.</p></div>

<div class="book" title="Selecting features using the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec285" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will continue to use the telecom <code class="email">churn</code> dataset as the input data source for feature selection.</p></div></div>

<div class="book" title="Selecting features using the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec286" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to select features:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Transform the feature named as <code class="email">international_plan</code> of the training dataset, <code class="email">trainset</code>, to <code class="email">intl_yes</code> and <code class="email">intl_no</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; intl_plan = model.matrix(~ trainset.international_plan - 1, data=data.frame(trainset$international_plan))</strong></span>
<span class="strong"><strong class="calibre2">&gt; colnames(intl_plan) = c("trainset.international_planno"="intl_no", "trainset.international_planyes"= "intl_yes")</strong></span>
</pre></div></li><li class="listitem" value="2">Transform the feature named as <code class="email">voice_mail_plan</code> of the training dataset, <code class="email">trainset</code>, to <code class="email">voice_yes</code> and <code class="email">voice_no</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; voice_plan = model.matrix(~ trainset.voice_mail_plan - 1, data=data.frame(trainset$voice_mail_plan))</strong></span>
<span class="strong"><strong class="calibre2">&gt; colnames(voice_plan) = c("trainset.voice_mail_planno" ="voice_no", "trainset.voice_mail_planyes"="voidce_yes")</strong></span>
</pre></div></li><li class="listitem" value="3">Remove<a id="id617" class="calibre1"/> the <code class="email">international_plan</code> and <code class="email">voice_mail_plan</code> attributes and combine the training dataset, <code class="email">trainset</code> with the data frames, <code class="email">intl_plan</code> and <code class="email">voice_plan</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; trainset$international_plan = NULL</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset$voice_mail_plan = NULL</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset = cbind(intl_plan,voice_plan, trainset)</strong></span>
</pre></div></li><li class="listitem" value="4">Transform the feature named as <code class="email">international_plan</code> of the testing dataset, <code class="email">testset</code>, to <code class="email">intl_yes</code> and <code class="email">intl_no</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; intl_plan = model.matrix(~ testset.international_plan - 1, data=data.frame(testset$international_plan))</strong></span>
<span class="strong"><strong class="calibre2">&gt; colnames(intl_plan) = c("testset.international_planno"="intl_no", "testset.international_planyes"= "intl_yes")</strong></span>
</pre></div></li><li class="listitem" value="5">Transform the feature named as <code class="email">voice_mail_plan</code> of the training dataset, <code class="email">trainset</code>, to <code class="email">voice_yes</code> and <code class="email">voice_no</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; voice_plan = model.matrix(~ testset.voice_mail_plan - 1, data=data.frame(testset$voice_mail_plan))</strong></span>
<span class="strong"><strong class="calibre2">&gt; colnames(voice_plan) = c("testset.voice_mail_planno" ="voice_no", "testset.voice_mail_planyes"="voidce_yes")</strong></span>
</pre></div></li><li class="listitem" value="6">Remove the <code class="email">international_plan</code> and <code class="email">voice_mail_plan</code> attributes and combine the testing dataset, <code class="email">testset</code> with the data frames, <code class="email">intl_plan</code> and <code class="email">voice_plan</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; testset$international_plan = NULL</strong></span>
<span class="strong"><strong class="calibre2">&gt; testset$voice_mail_plan = NULL</strong></span>
<span class="strong"><strong class="calibre2">&gt; testset = cbind(intl_plan,voice_plan, testset)</strong></span>
</pre></div></li><li class="listitem" value="7">We then create a feature selection algorithm using linear discriminant analysis:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ldaControl = rfeControl(functions = ldaFuncs, method = "cv")</strong></span>
</pre></div></li><li class="listitem" value="8">Next, we perform a backward feature selection on the training dataset, <code class="email">trainset</code> using subsets from 1 to 18:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ldaProfile = rfe(trainset[, !names(trainset) %in% c("churn")], trainset[,c("churn")],sizes = c(1:18), rfeControl = ldaControl)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ldaProfile</strong></span>

<span class="strong"><strong class="calibre2">Recursive feature selection</strong></span>

<span class="strong"><strong class="calibre2">Outer resampling method: Cross-Validated (10 fold) </strong></span>

<span class="strong"><strong class="calibre2">Resampling performance over subset size:</strong></span>

<span class="strong"><strong class="calibre2"> Variables Accuracy  Kappa AccuracySD KappaSD Selected</strong></span>
<span class="strong"><strong class="calibre2">         1   0.8523 0.0000   0.001325 0.00000         </strong></span>
<span class="strong"><strong class="calibre2">         2   0.8523 0.0000   0.001325 0.00000         </strong></span>
<span class="strong"><strong class="calibre2">         3   0.8423 0.1877   0.015468 0.09787         </strong></span>
<span class="strong"><strong class="calibre2">         4   0.8462 0.2285   0.016593 0.09610         </strong></span>
<span class="strong"><strong class="calibre2">         5   0.8466 0.2384   0.020710 0.09970         </strong></span>
<span class="strong"><strong class="calibre2">         6   0.8466 0.2364   0.019612 0.09387         </strong></span>
<span class="strong"><strong class="calibre2">         7   0.8458 0.2315   0.017551 0.08670         </strong></span>
<span class="strong"><strong class="calibre2">         8   0.8458 0.2284   0.016608 0.09536         </strong></span>
<span class="strong"><strong class="calibre2">         9   0.8475 0.2430   0.016882 0.10147         </strong></span>
<span class="strong"><strong class="calibre2">        10   0.8514 0.2577   0.014281 0.08076         </strong></span>
<span class="strong"><strong class="calibre2">        11   0.8518 0.2587   0.014124 0.08075         </strong></span>
<span class="strong"><strong class="calibre2">        12   0.8544 0.2702   0.015078 0.09208        *</strong></span>
<span class="strong"><strong class="calibre2">        13   0.8544 0.2721   0.015352 0.09421         </strong></span>
<span class="strong"><strong class="calibre2">        14   0.8531 0.2663   0.018428 0.11022         </strong></span>
<span class="strong"><strong class="calibre2">        15   0.8527 0.2652   0.017958 0.10850         </strong></span>
<span class="strong"><strong class="calibre2">        16   0.8531 0.2684   0.017897 0.10884         </strong></span>
<span class="strong"><strong class="calibre2">        17   0.8531 0.2684   0.017897 0.10884         </strong></span>
<span class="strong"><strong class="calibre2">        18   0.8531 0.2684   0.017897 0.10884         </strong></span>

<span class="strong"><strong class="calibre2">The top 5 variables (out of 12):</strong></span>
<span class="strong"><strong class="calibre2">   total_day_charge, total_day_minutes, intl_no, number_customer_service_calls, total_eve_charge</strong></span>
</pre></div></li><li class="listitem" value="9">Next, we <a id="id618" class="calibre1"/>can plot the selection result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(ldaProfile, type = c("o", "g"))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00125.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 3: The feature selection result</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="10">We <a id="id619" class="calibre1"/>can then examine the best subset of the variables:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ldaProfile$optVariables</strong></span>
<span class="strong"><strong class="calibre2"> [1] "total_day_charge"             </strong></span>
<span class="strong"><strong class="calibre2"> [2] "total_day_minutes"            </strong></span>
<span class="strong"><strong class="calibre2"> [3] "intl_no"                      </strong></span>
<span class="strong"><strong class="calibre2"> [4] "number_customer_service_calls"</strong></span>
<span class="strong"><strong class="calibre2"> [5] "total_eve_charge"             </strong></span>
<span class="strong"><strong class="calibre2"> [6] "total_eve_minutes"            </strong></span>
<span class="strong"><strong class="calibre2"> [7] "voidce_yes"                   </strong></span>
<span class="strong"><strong class="calibre2"> [8] "total_intl_calls"             </strong></span>
<span class="strong"><strong class="calibre2"> [9] "number_vmail_messages"        </strong></span>
<span class="strong"><strong class="calibre2">[10] "total_intl_charge"            </strong></span>
<span class="strong"><strong class="calibre2">[11] "total_intl_minutes"           </strong></span>
<span class="strong"><strong class="calibre2">[12] "total_night_minutes"  </strong></span>
</pre></div></li><li class="listitem" value="11">Now, we can examine the fitted model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ldaProfile$fit</strong></span>
<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">lda(x, y)</strong></span>

<span class="strong"><strong class="calibre2">Prior probabilities of groups:</strong></span>
<span class="strong"><strong class="calibre2">      yes        no </strong></span>
<span class="strong"><strong class="calibre2">0.1477322 0.8522678 </strong></span>

<span class="strong"><strong class="calibre2">Group means:</strong></span>
<span class="strong"><strong class="calibre2">    total_day_charge total_day_minutes   intl_no</strong></span>
<span class="strong"><strong class="calibre2">yes         35.00143          205.8877 0.7046784</strong></span>
<span class="strong"><strong class="calibre2">no          29.62402          174.2555 0.9351242</strong></span>
<span class="strong"><strong class="calibre2">    number_customer_service_calls total_eve_charge</strong></span>
<span class="strong"><strong class="calibre2">yes                      2.204678         18.16702</strong></span>
<span class="strong"><strong class="calibre2">no                       1.441460         16.96789</strong></span>
<span class="strong"><strong class="calibre2">    total_eve_minutes voidce_yes total_intl_calls</strong></span>
<span class="strong"><strong class="calibre2">yes          213.7269  0.1666667         4.134503</strong></span>
<span class="strong"><strong class="calibre2">no           199.6197  0.2954891         4.514445</strong></span>
<span class="strong"><strong class="calibre2">    number_vmail_messages total_intl_charge</strong></span>
<span class="strong"><strong class="calibre2">yes              5.099415          2.899386</strong></span>
<span class="strong"><strong class="calibre2">no               8.674607          2.741343</strong></span>
<span class="strong"><strong class="calibre2">    total_intl_minutes total_night_minutes</strong></span>
<span class="strong"><strong class="calibre2">yes           10.73684            205.4640</strong></span>
<span class="strong"><strong class="calibre2">no            10.15119            201.4184</strong></span>

<span class="strong"><strong class="calibre2">Coefficients of linear discriminants:</strong></span>
<span class="strong"><strong class="calibre2">                                       LD1</strong></span>
<span class="strong"><strong class="calibre2">total_day_charge               0.715025524</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes             -0.130486470</strong></span>
<span class="strong"><strong class="calibre2">intl_no                        2.259889324</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls -0.421997335</strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge              -2.390372793</strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes              0.198406977</strong></span>
<span class="strong"><strong class="calibre2">voidce_yes                     0.660927935</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls               0.066240268</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages         -0.003529233</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge              2.315069869</strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes            -0.693504606</strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes           -0.002127471</strong></span>
</pre></div></li><li class="listitem" value="12">Finally, we can calculate the performance across resamples:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; postResample(predict(ldaProfile, testset[, !names(testset) %in% c("churn")]), testset[,c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">Accuracy     Kappa</strong></span>
<span class="strong"><strong class="calibre2">0.8605108 0.2672027</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Selecting features using the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec287" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id620" class="calibre1"/>recipe, we perform feature selection using the <code class="email">caret</code> package. As there are factor-coded attributes within the dataset, we first use a function called <code class="email">model.matrix</code> to transform the factor-coded attributes into multiple binary attributes. Therefore, we transform the <code class="email">international_plan</code> attribute to <code class="email">intl_yes</code> and <code class="email">intl_no</code>. Additionally, we transform the <code class="email">voice_mail_plan</code> attribute to <code class="email">voice_yes</code> and <code class="email">voice_no</code>.</p><p class="calibre7">Next, we set up control parameters for training using the cross-validation method, <code class="email">cv</code>, with the linear discriminant function, <code class="email">ldaFuncs</code>. Then, we use the recursive feature elimination, <code class="email">rfe</code>, to perform feature selection with the use of the <code class="email">control</code> function, <code class="email">ldaFuncs</code>. The <code class="email">rfe</code> function generates the summary of feature selection, which contains resampling a performance over the subset size and top variables.</p><p class="calibre7">We can then use the obtained model information to plot the number of variables against accuracy. From Figure 3, it is obvious that using 12 features can obtain the best accuracy. In addition to this, we can retrieve the best subset of the variables in (12 variables in total) the fitted model. Lastly, we can calculate the performance across resamples, which yields an accuracy of 0.86 and a kappa of 0.27.</p></div></div>

<div class="book" title="Selecting features using the caret package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec288" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">In order to specify the algorithm used to control feature selection, one can change the control function specified in <code class="email">rfeControl</code>. Here are some of the options you can use:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">caretFuncs      SVM (caret)
lmFuncs     lm (base)
rfFuncs         RF(randomForest)
treebagFuncs     DT (ipred)
ldaFuncs       lda(base)
nbFuncs       NB(klaR)
gamFuncs      gam(gam)</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring the performance of the regression model"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec85" class="calibre1"/>Measuring the performance of the regression model</h1></div></div></div><p class="calibre7">To <a id="id621" class="calibre1"/>measure the performance of a regression model, we can calculate the distance from predicted output and the actual output<a id="id622" class="calibre1"/> as a quantifier of the performance of the model. Here, we often use the <span class="strong"><strong class="calibre2">root mean square error</strong></span> (<span class="strong"><strong class="calibre2">RMSE</strong></span>), <span class="strong"><strong class="calibre2">relative square error</strong></span> (<span class="strong"><strong class="calibre2">RSE</strong></span>) and <a id="id623" class="calibre1"/>R-Square as common measurements. In the following recipe, we will illustrate how to compute these measurements from a built regression model.</p></div>

<div class="book" title="Measuring the performance of the regression model">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec289" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this <a id="id624" class="calibre1"/>recipe, we will use the <code class="email">Quartet</code> dataset, which contains four regression datasets, as our input data source.</p></div></div>

<div class="book" title="Measuring the performance of the regression model">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec290" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to measure the performance of the regression model:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Load the <code class="email">Quartet</code> dataset from the <code class="email">car</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(car)</strong></span>
<span class="strong"><strong class="calibre2">&gt; data(Quartet)</strong></span>
</pre></div></li><li class="listitem" value="2">Plot the attribute, <code class="email">y3</code>, against x using the <code class="email">lm</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(Quartet$x, Quartet$y3)</strong></span>
<span class="strong"><strong class="calibre2">&gt; lmfit = lm(Quartet$y3~Quartet$x)</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(lmfit, col="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00126.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 4: The linear regression plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="3">You can retrieve predicted values by using the <code class="email">predict</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predicted= predict(lmfit, newdata=Quartet[c("x")])</strong></span>
</pre></div></li><li class="listitem" value="4">Now, you can calculate the root mean square error:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; actual = Quartet$y3</strong></span>
<span class="strong"><strong class="calibre2">&gt; rmse = (mean((predicted - actual)^2))^0.5</strong></span>
<span class="strong"><strong class="calibre2">&gt; rmse</strong></span>
<span class="strong"><strong class="calibre2">[1] 1.118286</strong></span>
</pre></div></li><li class="listitem" value="5">You <a id="id625" class="calibre1"/>can calculate the relative square error:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mu = mean(actual)</strong></span>
<span class="strong"><strong class="calibre2">&gt; rse = mean((predicted - actual)^2) / mean((mu - actual)^2) </strong></span>
<span class="strong"><strong class="calibre2">&gt; rse</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.333676</strong></span>
</pre></div></li><li class="listitem" value="6">Also, you can use R-Square as a measurement:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; rsquare = 1 - rse</strong></span>
<span class="strong"><strong class="calibre2">&gt; rsquare</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.666324</strong></span>
</pre></div></li><li class="listitem" value="7">Then, you can plot attribute, y3, against x using the <code class="email">rlm</code> function from the MASS package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(MASS)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(Quartet$x, Quartet$y3)</strong></span>
<span class="strong"><strong class="calibre2">&gt; rlmfit = rlm(Quartet$y3~Quartet$x)</strong></span>
<span class="strong"><strong class="calibre2">&gt; abline(rlmfit, col="red")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00127.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 5: The robust linear regression plot on the Quartet dataset</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="8">You<a id="id626" class="calibre1"/> can then retrieve the predicted value using the <code class="email">predict</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predicted = predict(rlmfit, newdata=Quartet[c("x")])</strong></span>
</pre></div></li><li class="listitem" value="9">Next, you can calculate the root mean square error using the distance of the predicted and actual value:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; actual = Quartet$y3</strong></span>
<span class="strong"><strong class="calibre2">&gt; rmse = (mean((predicted - actual)^2))^0.5</strong></span>
<span class="strong"><strong class="calibre2">&gt; rmse</strong></span>
<span class="strong"><strong class="calibre2">[1] 1.279045</strong></span>
</pre></div></li><li class="listitem" value="10">Calculate the relative square error between the predicted and actual labels:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; mu = mean(actual)</strong></span>
<span class="strong"><strong class="calibre2">&gt; rse =mean((predicted - actual)^2) / mean((mu - actual)^2) </strong></span>
<span class="strong"><strong class="calibre2">&gt; rse</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.4365067</strong></span>
</pre></div></li><li class="listitem" value="11">Now, you can calculate the R-Square value:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; rsquare = 1 - rse</strong></span>
<span class="strong"><strong class="calibre2">&gt; rsquare</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.5634933</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring the performance of the regression model">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec291" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">The <a id="id627" class="calibre1"/>measurement of the performance of the regression model employs the distance between the predicted value and the actual value. We often use these three measurements, root mean square error, relative square error, and R-Square, as the quantifier of the performance of regression models. In this recipe, we first load the <code class="email">Quartet</code> data from the <code class="email">car</code> package. We then use the <code class="email">lm</code> function to fit the linear model, and add the regression line on a scatter plot of the x variable against the <code class="email">y3</code> variable. Next, we compute the predicted value using the predict function, and<a id="id628" class="calibre1"/> begin to compute the <span class="strong"><strong class="calibre2">root mean square error</strong></span> (<span class="strong"><strong class="calibre2">RMSE</strong></span>), <span class="strong"><strong class="calibre2">relative square error</strong></span> (<span class="strong"><strong class="calibre2">RSE</strong></span>), and R-Square for the built model.</p><p class="calibre7">As this<a id="id629" class="calibre1"/> dataset has an outlier at <code class="email">x=13</code>, we would like to quantify how the outlier affects the performance measurement. To achieve this, we first train a regression model using the <code class="email">rlm</code> function from the <code class="email">MASS</code> package. Similar to the previous step, we then generate a performance measurement of the root square mean error, relative error and R-Square. From the output measurement, it is obvious that the mean square error and the relative square errors of the <code class="email">lm</code> model are smaller than the model built by <code class="email">rlm</code>, and the score of R-Square shows that the model built with <code class="email">lm</code> has a greater prediction power. However, for the actual scenario, we should remove the outlier at <code class="email">x=13</code>. This comparison shows that the outlier may be biased toward the performance measure and may lead us to choose the wrong model.</p></div></div>

<div class="book" title="Measuring the performance of the regression model">
<div class="book" title="There's more…"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec292" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre7">If you would like to perform cross-validation on a linear regression model, you can use the <code class="email">tune</code> function within the <code class="email">e1071</code> package:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tune(lm, y3~x, data = Quartet)</strong></span>
<span class="strong"><strong class="calibre2">Error estimation of 'lm' using 10-fold cross validation: 2.33754</strong></span>
</pre></div><p class="calibre7">Other than the <code class="email">e1071</code> package, you can use the <code class="email">train</code> function from the <code class="email">caret</code> package to perform cross-validation. In addition to this, you can also use <code class="email">cv.lm</code> from the <code class="email">DAAG</code> package to achieve the same goal.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring prediction performance with a confusion matrix"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec86" class="calibre1"/>Measuring prediction performance with a confusion matrix</h1></div></div></div><p class="calibre7">To <a id="id630" class="calibre1"/>measure the performance of a classification model, we can first generate a classification table based on our predicted label and actual label. Then, we can use a confusion matrix to obtain performance measures such as precision, recall, specificity, and accuracy. In this recipe, we will demonstrate how to retrieve a confusion matrix using the <code class="email">caret</code> package.</p></div>

<div class="book" title="Measuring prediction performance with a confusion matrix">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec293" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this<a id="id631" class="calibre1"/> recipe, we will continue to use the telecom <code class="email">churn</code> dataset as our example dataset.</p></div></div>

<div class="book" title="Measuring prediction performance with a confusion matrix">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec294" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to generate a classification measurement:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Train an svm model using the training dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.model= train(churn ~ .,</strong></span>
<span class="strong"><strong class="calibre2">+                   data = trainset,</strong></span>
<span class="strong"><strong class="calibre2">+                   method = "svmRadial")</strong></span>
</pre></div></li><li class="listitem" value="2">You can then predict labels using the fitted model, <code class="email">svm.model</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.pred = predict(svm.model, testset[,! names(testset) %in% c("churn")])</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you can generate a classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(svm.pred, testset[,c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">        </strong></span>
<span class="strong"><strong class="calibre2">svm.pred yes  no</strong></span>
<span class="strong"><strong class="calibre2">     yes  73  16</strong></span>
<span class="strong"><strong class="calibre2">     no   68 861</strong></span>
</pre></div></li><li class="listitem" value="4">Lastly, you can generate a confusion matrix using the prediction results and the actual labels from the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(svm.pred, testset[,c("churn")])</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">          Reference</strong></span>
<span class="strong"><strong class="calibre2">Prediction yes  no</strong></span>
<span class="strong"><strong class="calibre2">       yes  73  16</strong></span>
<span class="strong"><strong class="calibre2">       no   68 861</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9175          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8989, 0.9337)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 2.273e-08       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.5909          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 2.628e-08       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.51773         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.98176         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.82022         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.92680         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.07171         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.08743         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.74974         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes              </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring prediction performance with a confusion matrix">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec295" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id632" class="calibre1"/> recipe, we demonstrate how to obtain a confusion matrix to measure the performance of a classification model. First, we use the <code class="email">train</code> function from the <code class="email">caret</code> package to train an svm model. Next, we use the <code class="email">predict</code> function to extract the predicted labels of the svm model using the testing dataset. Then, we perform the <code class="email">table</code> function to obtain the classification table based on the predicted and actual labels. Finally, we use the <code class="email">confusionMatrix</code> function from the <code class="email">caret</code> package to a generate a confusion matrix to measure the performance of the classification model.</p></div></div>

<div class="book" title="Measuring prediction performance with a confusion matrix">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec296" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">If you are interested in the available methods that can be used in the <code class="email">train</code> function, you can refer to this website: <a class="calibre1" href="http://topepo.github.io/caret/modelList.html">http://topepo.github.io/caret/modelList.html</a></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring prediction performance using ROCR"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec87" class="calibre1"/>Measuring prediction performance using ROCR</h1></div></div></div><p class="calibre7">A <span class="strong"><strong class="calibre2">receiver operating characteristic</strong></span> (<span class="strong"><strong class="calibre2">ROC</strong></span>) curve is a plot that illustrates the performance <a id="id633" class="calibre1"/>of a binary classifier system, and <a id="id634" class="calibre1"/>plots the true positive rate against the false positive rate for different cut points. We most commonly use this plot to calculate the <span class="strong"><strong class="calibre2">area under curve</strong></span> (<span class="strong"><strong class="calibre2">AUC</strong></span>) to<a id="id635" class="calibre1"/> measure the performance of a classification model. In this recipe, we will demonstrate how to illustrate an ROC curve and calculate the AUC to measure the performance of a classification model.</p></div>

<div class="book" title="Measuring prediction performance using ROCR">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec297" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this <a id="id636" class="calibre1"/>recipe, we will continue using the telecom <code class="email">churn</code> dataset as our example dataset.</p></div></div>

<div class="book" title="Measuring prediction performance using ROCR">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec298" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to generate two different classification examples with different costs:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, you <a id="id637" class="calibre1"/>should install and load the <code class="email">ROCR</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("ROCR")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(ROCR)</strong></span>
</pre></div></li><li class="listitem" value="2">Train the svm model using the training dataset with a probability equal to <code class="email">TRUE</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svmfit=svm(churn~ ., data=trainset, prob=TRUE)</strong></span>
</pre></div></li><li class="listitem" value="3">Make predictions based on the trained model on the testing dataset with the probability set as <code class="email">TRUE</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt;pred=predict(svmfit,testset[, !names(testset) %in% c("churn")], probability=TRUE)</strong></span>
</pre></div></li><li class="listitem" value="4">Obtain the probability of labels with <code class="email">yes</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; pred.prob = attr(pred, "probabilities") </strong></span>
<span class="strong"><strong class="calibre2">&gt; pred.to.roc = pred.prob[, 2] </strong></span>
</pre></div></li><li class="listitem" value="5">Use the <code class="email">prediction</code> function to generate a prediction result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; pred.rocr = prediction(pred.to.roc, testset$churn)</strong></span>
</pre></div></li><li class="listitem" value="6">Use the <code class="email">performance</code> function to obtain the performance measurement:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; perf.rocr = performance(pred.rocr, measure = "auc", x.measure = "cutoff") </strong></span>
<span class="strong"><strong class="calibre2">&gt; perf.tpr.rocr = performance(pred.rocr, "tpr","fpr") </strong></span>
</pre></div></li><li class="listitem" value="7">Visualize the ROC curve using the <code class="email">plot</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(perf.tpr.rocr, colorize=T,main=paste("AUC:",(perf.rocr@y.values)))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00128.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 6: The ROC curve for the svm classifier performance</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring prediction performance using ROCR">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec299" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id638" class="calibre1"/>recipe, we demonstrated how to generate an ROC curve to illustrate the performance of a binary classifier. First, we should install and load the library, <code class="email">ROCR</code>. Then, we use svm, from the <code class="email">e1071</code> package, to train a classification model, and then use the model to predict labels for the testing dataset. Next, we use the prediction function (from the package, <code class="email">ROCR</code>) to generate prediction results. We then adapt the performance function to obtain the performance measurement of the true positive rate against the false positive rate. Finally, we use the <code class="email">plot</code> function to visualize the ROC plot, and add the value of AUC on the title. In this example, the AUC value is 0.92, which indicates that the svm classifier performs well in classifying telecom user churn datasets.</p></div></div>

<div class="book" title="Measuring prediction performance using ROCR">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec300" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For <a id="id639" class="calibre1"/>those interested in the concept and terminology of ROC, you can refer to <a class="calibre1" href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">http://en.wikipedia.org/wiki/Receiver_operating_characteristic</a></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Comparing an ROC curve using the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec88" class="calibre1"/>Comparing an ROC curve using the caret package</h1></div></div></div><p class="calibre7">In previous chapters, we introduced many classification methods; each method has its own advantages and disadvantages. However, when it comes to the problem of how to choose<a id="id640" class="calibre1"/> the best fitted model, you need to compare all the performance measures generated from different prediction models. To make the comparison easy, the caret package allows us to generate and compare the performance of models. In this recipe, we will use the function provided by the <code class="email">caret</code> package to compare different algorithm trained models on the same dataset.</p></div>

<div class="book" title="Comparing an ROC curve using the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec301" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">Here, we will continue to use telecom dataset as our input data source.</p></div></div>

<div class="book" title="Comparing an ROC curve using the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec302" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to generate an ROC curve of each fitted model:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Install and load the library, <code class="email">pROC</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("pROC")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library("pROC")</strong></span>
</pre></div></li><li class="listitem" value="2">Set up the training control with a 10-fold cross-validation in 3 repetitions:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; control = trainControl(method = "repeatedcv",</strong></span>
<span class="strong"><strong class="calibre2">+                            number = 10,</strong></span>
<span class="strong"><strong class="calibre2">+                            repeats = 3,</strong></span>
<span class="strong"><strong class="calibre2">+                            classProbs = TRUE,</strong></span>
<span class="strong"><strong class="calibre2">+                            summaryFunction = twoClassSummary)</strong></span>
</pre></div></li><li class="listitem" value="3">Then, you can train a classifier on the training dataset using <code class="email">glm</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; glm.model= train(churn ~ .,</strong></span>
<span class="strong"><strong class="calibre2">+                     data = trainset,</strong></span>
<span class="strong"><strong class="calibre2">+                     method = "glm",</strong></span>
<span class="strong"><strong class="calibre2">+                     metric = "ROC",</strong></span>
<span class="strong"><strong class="calibre2">+                     trControl = control)</strong></span>
</pre></div></li><li class="listitem" value="4">Also, you can train a classifier on the training dataset using <code class="email">svm</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; svm.model= train(churn ~ .,</strong></span>
<span class="strong"><strong class="calibre2">+                   data = trainset,</strong></span>
<span class="strong"><strong class="calibre2">+                   method = "svmRadial",</strong></span>
<span class="strong"><strong class="calibre2">+                   metric = "ROC",</strong></span>
<span class="strong"><strong class="calibre2">+                   trControl = control)</strong></span>
</pre></div></li><li class="listitem" value="5">To see how <code class="email">rpart</code> performs on the training data, we use the <code class="email">rpart</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; rpart.model= train(churn ~ .,</strong></span>
<span class="strong"><strong class="calibre2">+                   data = trainset,</strong></span>
<span class="strong"><strong class="calibre2">+                   method = "rpart",</strong></span>
<span class="strong"><strong class="calibre2">+                   metric = "ROC",</strong></span>
<span class="strong"><strong class="calibre2">+                   trControl = control)</strong></span>
</pre></div></li><li class="listitem" value="6">You<a id="id641" class="calibre1"/> can make predictions separately based on different trained models:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; glm.probs = predict(glm.model, testset[,! names(testset) %in% c("churn")], type = "prob")</strong></span>
<span class="strong"><strong class="calibre2">&gt; svm.probs = predict(svm.model, testset[,! names(testset) %in% c("churn")], type = "prob")</strong></span>
<span class="strong"><strong class="calibre2">&gt; rpart.probs = predict(rpart.model, testset[,! names(testset) %in% c("churn")], type = "prob")</strong></span>
</pre></div></li><li class="listitem" value="7">You can generate the ROC curve of each model, and plot the curve on the same figure:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; glm.ROC = roc(response = testset[,c("churn")],</strong></span>
<span class="strong"><strong class="calibre2">+                predictor =glm.probs$yes,</strong></span>
<span class="strong"><strong class="calibre2">+                levels = levels(testset[,c("churn")]))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(glm.ROC, type="S", col="red") </strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">roc.default(response = testset[, c("churn")], predictor = glm.probs$yes,     levels = levels(testset[, c("churn")]))</strong></span>

<span class="strong"><strong class="calibre2">Data: glm.probs$yes in 141 controls (testset[, c("churn")] yes) &gt; 877 cases (testset[, c("churn")] no).</strong></span>
<span class="strong"><strong class="calibre2">Area under the curve: 0.82</strong></span>

<span class="strong"><strong class="calibre2">&gt; svm.ROC = roc(response = testset[,c("churn")],</strong></span>
<span class="strong"><strong class="calibre2">+                predictor =svm.probs$yes,</strong></span>
<span class="strong"><strong class="calibre2">+                levels = levels(testset[,c("churn")]))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(svm.ROC, add=TRUE, col="green") </strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">roc.default(response = testset[, c("churn")], predictor = svm.probs$yes,     levels = levels(testset[, c("churn")]))</strong></span>

<span class="strong"><strong class="calibre2">Data: svm.probs$yes in 141 controls (testset[, c("churn")] yes) &gt; 877 cases (testset[, c("churn")] no).</strong></span>
<span class="strong"><strong class="calibre2">Area under the curve: 0.9233</strong></span>

<span class="strong"><strong class="calibre2">&gt; rpart.ROC = roc(response = testset[,c("churn")],</strong></span>
<span class="strong"><strong class="calibre2">+                predictor =rpart.probs$yes,</strong></span>
<span class="strong"><strong class="calibre2">+                levels = levels(testset[,c("churn")]))</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(rpart.ROC, add=TRUE, col="blue")</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">roc.default(response = testset[, c("churn")], predictor = rpart.probs$yes,     levels = levels(testset[, c("churn")]))</strong></span>

<span class="strong"><strong class="calibre2">Data: rpart.probs$yes in 141 controls (testset[, c("churn")] yes) &gt; 877 cases (testset[, c("churn")] no).</strong></span>
<span class="strong"><strong class="calibre2">Area under the curve: 0.7581</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00129.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 7: The ROC curve for the performance of three classifiers</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Comparing an ROC curve using the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec303" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Here, we demonstrate how we can compare fitted models by illustrating their ROC curve in one <a id="id642" class="calibre1"/>figure. First, we set up the control of the training process with a 10-fold cross validation in 3 repetitions with the performance evaluation in <code class="email">twoClassSummary</code>. After setting up control of the training process, we then apply <code class="email">glm</code>, <code class="email">svm</code>, and <code class="email">rpart</code> algorithms on the training dataset to fit the classification models. Next, we can make a prediction based on each generated model and plot the ROC curve, respectively. Within the generated figure, we find that the model trained by svm has the largest area under curve, which is 0.9233 (plotted in green), the AUC of the <code class="email">glm</code> model (red) is 0.82, and the AUC of the <code class="email">rpart</code> model (blue) is 0.7581. From <span class="strong"><em class="calibre8">Figure 7</em></span>, it is obvious that <code class="email">svm</code> performs the best among all the fitted models on this training dataset (without requiring tuning).</p></div></div>

<div class="book" title="Comparing an ROC curve using the caret package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec304" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">We use another ROC visualization package, <code class="email">pROC</code>, which can be employed to display and analyze ROC curves. If you would like to know more about the package, please use the <code class="email">help</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; help(package="pROC")</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring performance differences between models with the caret package"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec89" class="calibre1"/>Measuring performance differences between models with the caret package</h1></div></div></div><p class="calibre7">In the previous recipe, we introduced how to generate ROC curves for each generated model, and have the curve plotted on the same figure. Apart from using an ROC curve, one <a id="id643" class="calibre1"/>can use the resampling method to generate statistics of each fitted model in ROC, sensitivity and specificity metrics. Therefore, we can use these statistics to compare the performance differences between each model. In the following recipe, we will introduce how to measure performance differences between fitted models with the <code class="email">caret</code> package.</p></div>

<div class="book" title="Measuring performance differences between models with the caret package">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec305" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">One needs to have completed the previous recipe by storing the <code class="email">glm</code> fitted model, <code class="email">svm</code> fitted model, and the <code class="email">rpart</code> fitted model into <code class="email">glm.model</code>, <code class="email">svm.model</code>, and <code class="email">rpart.model</code>, respectively.</p></div></div>

<div class="book" title="Measuring performance differences between models with the caret package">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec306" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to measure performance differences between each fitted model:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Resample the three generated models:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; cv.values = resamples(list(glm = glm.model, svm=svm.model, rpart = rpart.model))</strong></span>
</pre></div></li><li class="listitem" value="2">Then, you <a id="id644" class="calibre1"/>can obtain a summary of the resampling result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(cv.values)</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">summary.resamples(object = cv.values)</strong></span>

<span class="strong"><strong class="calibre2">Models: glm, svm, rpart </strong></span>
<span class="strong"><strong class="calibre2">Number of resamples: 30 </strong></span>

<span class="strong"><strong class="calibre2">ROC </strong></span>
<span class="strong"><strong class="calibre2">        Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's</strong></span>
<span class="strong"><strong class="calibre2">glm   0.7206  0.7847 0.8126 0.8116  0.8371 0.8877    0</strong></span>
<span class="strong"><strong class="calibre2">svm   0.8337  0.8673 0.8946 0.8929  0.9194 0.9458    0</strong></span>
<span class="strong"><strong class="calibre2">rpart 0.2802  0.7159 0.7413 0.6769  0.8105 0.8821    0</strong></span>

<span class="strong"><strong class="calibre2">Sens </strong></span>
<span class="strong"><strong class="calibre2">         Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's</strong></span>
<span class="strong"><strong class="calibre2">glm   0.08824  0.2000 0.2286 0.2194  0.2517 0.3529    0</strong></span>
<span class="strong"><strong class="calibre2">svm   0.44120  0.5368 0.5714 0.5866  0.6424 0.7143    0</strong></span>
<span class="strong"><strong class="calibre2">rpart 0.20590  0.3742 0.4706 0.4745  0.5929 0.6471    0</strong></span>

<span class="strong"><strong class="calibre2">Spec </strong></span>
<span class="strong"><strong class="calibre2">        Min. 1st Qu. Median   Mean 3rd Qu.   Max. NA's</strong></span>
<span class="strong"><strong class="calibre2">glm   0.9442  0.9608 0.9746 0.9701  0.9797 0.9949    0</strong></span>
<span class="strong"><strong class="calibre2">svm   0.9442  0.9646 0.9746 0.9740  0.9835 0.9949    0</strong></span>
<span class="strong"><strong class="calibre2">rpart 0.9492  0.9709 0.9797 0.9780  0.9848 0.9949    0</strong></span>
</pre></div></li><li class="listitem" value="3">Use <code class="email">dotplot</code> to plot the resampling result in the ROC metric:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dotplot(cv.values, metric = "ROC")</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00130.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 8: The dotplot of resampling result in ROC metric</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="4">Also, you <a id="id645" class="calibre1"/>can use a box-whisker plot to plot the resampling result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; bwplot(cv.values, layout = c(3, 1))</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00131.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 9: The box-whisker plot of resampling result</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring performance differences between models with the caret package">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec307" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we demonstrate how to measure the performance differences among three fitted models using the resampling method. First, we use the <code class="email">resample</code> function to generate the statistics of each fitted model (<code class="email">svm.model</code>, <code class="email">glm.model</code>, and <code class="email">rpart.model</code>). Then, we <a id="id646" class="calibre1"/>can use the <code class="email">summary</code> function to obtain the statistics of these three models in the ROC, sensitivity and specificity metrics. Next, we can apply a <code class="email">dotplot</code> on the resampling result to see how ROC varied between each model. Last, we use a box-whisker plot on the resampling results to show the box-whisker plot of different models in the ROC, sensitivity and specificity metrics on a single plot.</p></div></div>

<div class="book" title="Measuring performance differences between models with the caret package">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec308" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">Besides using <code class="email">dotplot</code> and <code class="email">bwplot</code> to measure performance differences, one can use <code class="email">densityplot</code>, <code class="email">splom</code>, and <code class="email">xyplot</code> to visualize the performance differences of each fitted model in the ROC, sensitivity, and specificity metrics.</li></ul></div></div></div></body></html>