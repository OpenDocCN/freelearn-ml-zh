- en: Chapter 2. Understanding Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we begin our exploration of machine learning models and techniques.
    The ultimate objective of machine learning is to *generalize* the facts from some
    empirical sample data. This is called **generalization**, and is essentially the
    ability to use these inferred facts to accurately perform at an accurate rate
    on new, unseen data. The two broad categories of machine learning are **supervised**
    learning and **unsupervised** learning. The term **supervised learning** is used
    to describe the task of machine learning in which an understanding or a model
    is formulated from some labeled data. By labeled, we mean that the sample data
    is associated with some observed value. In a basic sense, the model is a statistical
    description of the data and how the data varies over different parameters. The
    initial data used by supervised machine learning techniques to create the model
    is called the **training data** of the model. On the other hand, unsupervised
    learning techniques estimate models by finding patterns in unlabeled data. As
    the data used by unsupervised learning techniques is unlabeled, there is often
    no definite yes-or-no-based reward system to determine if an estimated model is
    accurate and correct.
  prefs: []
  type: TYPE_NORMAL
- en: We will now examine *linear regression*, which is an interesting model that
    can be used for prediction. As a type of supervised learning, regression models
    are created from some data in which a number of parameters are somehow combined
    to produce several target values. The model actually describes the relation between
    the target value and the model's parameters, and can be used to predict a target
    value when supplied with the values for the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: We will first study linear regression with single as well as multiple variables,
    and then describe the algorithms that can be used to formulate machine learning
    models from some given data. We will study the reasoning behind these models and
    simultaneously demonstrate how we can implement the algorithms to create these
    models in Clojure.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding single-variable linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We often come across situations where we would need to create an approximate
    model from some sample data. This model can then be used to predict more such
    data when its required parameters are supplied. For example, we might want to
    study the frequency of rainfall on a given day in a particular city, which we
    will assume varies depending on the humidity on that day. A formulated model could
    be useful in predicting the possibility of rainfall on a given day if we know
    the humidity on that day. We start formulating a model from some data by first
    fitting a straight line (that is, an equation) with some parameters and coefficients
    over this data. This type of model is called a **linear regression** model. We
    can think of linear regression as a way of fitting a straight line, ![Understanding
    single-variable linear regression](img/4351OS_02_01.jpg), over the sample data,
    if we assume that the sample data has only a single dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The linear regression model is simply described as a linear equation that represents
    the **regressand** or **dependent variable** of the model. The formulated regression
    model can have one to several parameters depending on the available data, and
    these parameters of the model are also termed as **regressors**, **features**,
    or **independent variables** of the model. We will first explore linear regression
    models with a single independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example problem for using linear regression with a single variable would
    be to predict the probability of rainfall on a particular day, which depends on
    the humidity on that day. This training data can be represented in the following
    tabular form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_85.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a single-variable linear model, the dependent variable must vary with respect
    to a single parameter. Thus, our sample data essentially consists of two vectors,
    that is, one for the values of the dependent parameter *Y* and the other for the
    values of the independent variable *X*. Both vectors have the same length. This
    data can be formally represented as two vectors, or single column matrices, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s quickly define the following two matrices in Clojure, *X* and *Y*, to
    represent some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we define 10 points of data; these points can be easily plotted on a
    scatter graph using the following Incanter `scatter-plot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays the following scatter plot of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous scatter plot is a simple representation of the 10 data points that
    we defined in `X` and `Y`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `scatter-plot` function can be found in the `charts` namespace of the Incanter
    library. The namespace declaration of a file using this function should look similar
    to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a visualization of our data, let''s estimate a linear model
    over the given data points. We can generate a linear model of any data using the
    `linear-model` function from the Incanter library. This function returns a map
    that describes the formulated model and also a lot of useful data about this model.
    For starters, we can plot the linear model over our previous scatter plot by using
    the `:fitted` key-value pair from this map. We first get the value of the `:fitted`
    key from the returned map and add it to the scatter plot using the `add-lines`
    function; this is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following self-explanatory plot of the linear model
    over the scatter plot we defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The previous plot depicts the linear model `samp-linear-model` as a straight
    line drawn over the 10 data points that we defined in `X` and `Y`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `linear-model` function can be found in the `stats` namespace of the Incanter
    library. The namespace declaration of a file using `linear-model` should look
    similar to the following declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Well, it looks like Incanter's `linear-model` function did most of the work
    for us. Essentially, this function creates a linear model of our data by using
    the **ordinary-least squares** (**OLS**) curve-fitting algorithm. We will soon
    dive into the details of this algorithm, but let's first understand how exactly
    a curve is fit onto some given data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first define how a straight line can be represented. In coordinate geometry,
    a line is simply a function of an independent variable, *x*, which has a given
    slope, *m*, and an intercept, *c*. The function of the line *y* can be formally
    written as ![Understanding single-variable linear regression](img/4351OS_02_01.jpg).
    The slope of the line represents how much the value of *y* changes when the value
    of *x* varies. The intercept of this equation is just where the line meets the
    *y* axis of the plot. Note that the equation *y* is not the same as *Y*, which
    actually represents the values of the equation that we have been provided with.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogous to this definition of a straight line from coordinate geometry, we
    formally define the linear regression model with a single variable using our definition
    of the matrices *X* and *Y*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This definition of the linear model with a single variable is actually quite
    versatile since we can use the same equation to define a linear model with multiple
    variables; we will see this later in the chapter. In the preceding definition,
    the term ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    is a coefficient that represents the linear scale of *y* with respect to *x*.
    In terms of geometry, it's simply the slope of a line that fits the given data
    in matrices *X* and *Y*. Since *X* is a matrix or vector, ![Understanding single-variable
    linear regression](img/4351OS_02_10.jpg) can also be thought of as a scaling factor
    for the matrix *X*.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the term ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is another coefficient that explains the value of *y* when *x* is zero. In other
    words, it's the *y* intercept of the equation. The coefficient ![Understanding
    single-variable linear regression](img/4351OS_02_10.jpg) of the formulated model
    is termed as the **regression coefficient** or **effect** of the linear model,
    and the coefficient ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is termed as the **error term** or **bias** of the model. A model may even have
    several regression coefficients, as we will see later in this chapter. It turns
    out that the error ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    is actually just another regression coefficient and can be conventionally mentioned
    along with the other effects of the model. Interestingly, this error determines
    the scatter or variance of the data in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the map returned by the `linear-model` function from our earlier example,
    we can easily inspect the coefficients of the generated model. The returned map
    has a `:coefs` key that maps to a vector containing the coefficients of the model.
    By convention, the error term is also included in this vector, simply as another
    coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we've defined a linear model over our data. It's obvious that not all the
    points will be on a line that is plotted to represent the formulated model. Each
    data point has some deviation from the linear model's plot over the *y* axis,
    and this deviation can be either positive or negative. To represent the overall
    deviation of the model from the given data, we use the *residual sum of squares*,
    *mean-squared error*, and *root mean-squared error* functions. The values of these
    three functions represent a scalar measure of the amount of error in the formulated
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between the terms *error* and *residual* is that an error is
    a measure of the amount by which an observed value differs from its expected value,
    while a residual is an estimate of the unobservable statistical error, which is
    simply not modeled or understood by the statistical model that we are using. We
    can say that, in a set of observed values, the difference between an observed
    value and the mean of all values is a residual. The number of residuals in a formulated
    model must be equal to the number of observed values of the dependent variable
    in the sample data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `:residuals` keyword to fetch the residuals from the linear
    model generated by the `linear-model` function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The **sum of squared errors of prediction** (**SSE**) is simply the sum of errors
    in a formulated model. Note that in the following equation, the sign of the error
    term ![Understanding single-variable linear regression](img/4351OS_02_12.jpg)
    isn't significant since we square this difference value; thus, it will always
    produce a positive value. The SSE is also termed as the **residual sum of squares**
    (**RSS**).
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `linear-model` function also calculates the SSE of the formulated model,
    and this value can be retrieved using the `:sse` keyword; this is illustrated
    in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The **mean-squared error** (**MSE**) measures the average magnitude of errors
    in a formulated model without considering the direction of the errors. We can
    calculate this value by squaring the differences of all the given values of the
    dependent variable and their corresponding predicted values on the formulated
    linear model, and calculating the mean of these squared errors. The MSE is also
    termed as the **mean-squared prediction error** of a model. If the MSE of a formulated
    model is zero, then we can say that the model fits the given data perfectly. Of
    course, this is practically impossible for real data, although we could find a
    set of values that produce an MSE of zero in theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given set of *N* values of the dependent variable ![Understanding single-variable
    linear regression](img/4351OS_02_14.jpg) and an estimated set of values ![Understanding
    single-variable linear regression](img/4351OS_02_15.jpg) calculated from a formulated
    model, we can formally represent the MSE function of the formulated model ![Understanding
    single-variable linear regression](img/4351OS_02_16.jpg) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The **root mean-squared error** (**RMSE**) or **root-mean squared deviation**
    is simply the square root of the MSE and is often used to measure the deviation
    of a formulated linear model. The RMSE is partial to larger errors, and is hence
    scale-dependent. This means that the RMSE is particularly useful when large errors
    are undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can formally define the RMSE of a formulated model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another measure of the accuracy of a formulated linear model is the **coefficient
    of determination**, which is written as ![Understanding single-variable linear
    regression](img/4351OS_02_19.jpg). The coefficient of determination indicates
    how well the formulated model fits the given sample data, and is defined as follows.
    This coefficient is defined in terms of the mean of observed values in the sample
    data ![Understanding single-variable linear regression](img/4351OS_02_20.jpg),
    the SSE, and the total sum of errors ![Understanding single-variable linear regression](img/4351OS_02_21.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can retrieve the calculated value of ![Understanding single-variable linear
    regression](img/4351OS_02_19.jpg) from the model generated by the `linear-model`
    function by using the `:r-square` keyword as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In order to formulate a model that best fits the sample data, we should strive
    to minimize the previously described values. For some given data, we can formulate
    several models and calculate the total error for each model. This calculated error
    can then be used to determine which formulated model is the best fit for the data,
    thus selecting the optimal linear model for the given data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the MSE of a formulated model, the model is said to have a **cost
    function**. The problem of fitting a linear model over some data is equivalent
    to the problem of minimizing the cost function of a formulated linear model. The
    cost function, which is represented as ![Understanding single-variable linear
    regression](img/4351OS_02_23.jpg), can be simply thought of as a function of the
    parameters of a formulated model. Generally, this cost function translates to
    the MSE of a model. Since the RMSE varies with the formulated parameters of the
    model, the following cost function of the model is a function of these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This brings us to the following formal definition of the problem of fitting
    a linear regression model over some data for the estimated effects ![Understanding
    single-variable linear regression](img/4351OS_02_10.jpg) and ![Understanding single-variable
    linear regression](img/4351OS_02_11.jpg) of a linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This definition states that we can estimate a linear model, represented by the
    parameters ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    and ![Understanding single-variable linear regression](img/4351OS_02_11.jpg),
    by determining the values of these parameters, for which the cost function ![Understanding
    single-variable linear regression](img/4351OS_02_23.jpg) takes on the least possible
    value, ideally zero.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding equation, the ![Understanding single-variable linear regression](img/4351OS_02_26.jpg)
    expression represents the standard norm *N*-dimensional Euclidian space of the
    cost function. By the term *norm*, we mean a function that has only positive values
    in the *N*-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s visualize how the Euclidian space of the cost function of a formulated
    model varies with respect to the parameters of the model. For this, let''s assume
    that the ![Understanding single-variable linear regression](img/4351OS_02_11.jpg)
    parameter that represents the constant error is zero. A plot of the cost function
    ![Understanding single-variable linear regression](img/4351OS_02_27.jpg) of the
    linear model over the parameter ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    will ideally appear as a parabolic curve, similar to the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a single parameter, ![Understanding single-variable linear regression](img/4351OS_02_10.jpg),
    we can plot the preceding chart, which has two dimensions. Similarly, for two
    parameters, ![Understanding single-variable linear regression](img/4351OS_02_10.jpg)
    and ![Understanding single-variable linear regression](img/4351OS_02_11.jpg),
    of the formulated model, a plot of three dimensions is produced. This plot appears
    bowl-shaped or having a convex surface, as illustrated in the following diagram.
    Also, we can generalize this for *N* parameters of the formulated model and produce
    a plot of ![Understanding single-variable linear regression](img/4351OS_02_30.jpg)
    dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding single-variable linear regression](img/4351OS_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The gradient descent algorithm is one of the simplest, although not the most
    efficient techniques to formulate a linear model that has the least possible value
    for the cost function or error of the model. This algorithm essentially finds
    the local minimum of the cost function for a formulated linear model.
  prefs: []
  type: TYPE_NORMAL
- en: As we previously described, a three-dimensional plot of the cost function for
    a single-variable linear regression model would appear as a convex or bowl-shaped
    surface with a *global minimum*. By minimum, we mean that the cost function has
    the least possible value at this point on the surface of the plot. The gradient
    descent algorithm essentially starts from any point on the surface and performs
    a sequence of steps to approach the local minimum of the surface.
  prefs: []
  type: TYPE_NORMAL
- en: This process can be imagined as dropping a ball into a valley or between two
    adjacent hills, as a result of which the ball slowly rolls towards the point that
    has the least elevation above sea level. The algorithm is repeated until the value
    of the apparent cost function from the current point on the surface converges
    to zero, which figuratively means that the ball rolling down the hill comes to
    a stop, as we described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, gradient descent may not really work if there are multiple local
    minimums on the surface of the plot. However, for an appropriately scaled single-variable
    linear regression model, the surface of the plot always has a single global minimum,
    as we illustrated earlier. Thus, we can still use the gradient descent algorithm
    in such situations to find the global minimum of the surface of the plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gist of this algorithm is that we start from some point on the surface
    and then take several steps towards the lowest point. We can formally represent
    this with the following equality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding gradient descent](img/4351OS_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, we start from the point represented by ![Understanding gradient descent](img/4351OS_02_33.jpg)
    on the plot of the cost function *J*, and incrementally subtract the product of
    the first-order partial derivative of the cost function ![Understanding gradient
    descent](img/4351OS_02_34.jpg), which is derived with respect to the parameters
    of the formulated model. This means that we slowly step downwards on the surface
    towards the local minimum, until we cannot find a lower point on the surface.
    The term ![Understanding gradient descent](img/4351OS_02_35.jpg) determines how
    large our steps towards the local minimum are, and is called the *step* of the
    gradient descent algorithm. We repeat this iteration until the difference between
    ![Understanding gradient descent](img/4351OS_02_36.jpg) and ![Understanding gradient
    descent](img/4351OS_02_33.jpg) converges to zero, or at least reduces to a threshold
    value close to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of stepping down towards the local minimum of the surface of the
    cost functions plot is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding gradient descent](img/4351OS_02_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding illustration is a contour diagram of the surface of the plot,
    in which the circular lines connect the points with an equal height. We start
    from the point ![Understanding gradient descent](img/4351OS_02_39.jpg) and perform
    a single iteration of the gradient descent algorithm to step down the surface
    to point ![Understanding gradient descent](img/4351OS_02_40.jpg). We repeat this
    process until we reach the local minimum of the surface with respect to the initial
    starting point ![Understanding gradient descent](img/4351OS_02_39.jpg). Note that,
    through each iteration, the size of the step reduces since the slope of a tangent
    to this surface also tends to zero as we approach the local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single-variable linear regression model with an error constant ![Understanding
    gradient descent](img/4351OS_02_11.jpg) that is equal to zero, we can simplify
    the partial derivative component ![Understanding gradient descent](img/4351OS_02_34.jpg)
    of the gradient descent algorithm. When there is only one parameter of the model,
    ![Understanding gradient descent](img/4351OS_02_10.jpg), the first order partial
    derivate is simply the slope of a tangent at that point on the surface of the
    plot. Thus, we calculate the slope of this tangent and take a step in the direction
    of this slope such that we arrive at a point of elevation above the *y* axis.
    This is shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding gradient descent](img/4351OS_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can implement this simplified version of the gradient descent algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, we begin from the point `x-start` and recursively
    apply the gradient descent algorithm until the value `x-new` converges. Note that
    this process is implemented as a tail recursive function using the `loop` form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using partial differentiation, we can formally express how both the parameters
    ![Understanding gradient descent](img/4351OS_02_10.jpg) and ![Understanding gradient
    descent](img/4351OS_02_11.jpg) can be calculated using the gradient descent algorithm
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding gradient descent](img/4351OS_02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding multivariable linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A multivariable linear regression model can have multiple variables or features,
    as opposed to the linear regression model with a single variable that we previously
    studied. Interestingly, the definition of a linear model with a single variable
    can itself be extended via matrices to be applied to multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can extend our previous example for predicting the probability of rainfall
    on a particular day to a model with multiple variables by including more independent
    variables, such as the minimum and maximum temperatures, in the sample data. Thus,
    the training data for a multivariable linear regression model will look similar
    to the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_43.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For a multivariable linear regression model, the training data is defined by
    two matrices, *X* and *Y*. Here, *X* is an ![Understanding multivariable linear
    regression](img/4351OS_02_44.jpg) matrix, where *P* is the number of independent
    variables in the model. The matrix *Y* is a vector of length *N*, just like in
    a linear model with a single variable. This model is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_45.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the following example of multivariable linear regression in Clojure, we
    will not generate the sample data through code but use the sample data from the
    Incanter library. We can fetch any dataset using the Incanter library's `get-dataset`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the upcoming example, the `sel`, `to-matrix`, and `get-dataset` functions
    from the Incanter library can be imported into our namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can fetch the **Iris** dataset by calling the `get-dataset` function with
    the `:iris` keyword argument; this is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We first define the variable `iris` as a matrix using the `to-matrix` and `get-dataset`
    functions, and then define two matrices `X` and `Y`. Here, `Y` is actually a vector
    of 150 values, or a matrix of size ![Understanding multivariable linear regression](img/4351OS_02_46.jpg),
    while `X` is a matrix of size ![Understanding multivariable linear regression](img/4351OS_02_47.jpg).
    Hence, `X` can be used to represent the values of four independent variables,
    and `Y` represents the values of the dependent variable. Note that the `sel` function
    is used to select a set of columns from the `iris` matrix. In fact, we could select
    many more such columns from the `iris` data matrix, but we will use only four
    in the following example for the sake of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset that we used in the previous code example is the *Iris* dataset,
    which is available in the Incanter library. This dataset has quite a bit of historical
    significance, as it was used by Sir Ronald Fisher to first develop the **linear
    discriminant analysis** (**LDA**) method for classification (for more information,
    refer to "The Species Problem in Iris"). This dataset contains 50 samples of three
    distinct species of the Iris plant, namely *Setosa*, *Versicolor*, and *Virginica*.
    Four features of the flowers of these species are measured in each sample, namely
    the petal width, petal length, sepal width, and sepal length. Note that we will
    encounter this dataset several times over the course of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, the `linear-model` function accepts a matrix with multiple columns,
    so we can use this function to fit a linear regression model over both single
    variable and multivariable data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code example, we plot the linear model using the `xy-plot`
    function while providing optional parameters to specify the labels of the axes
    in the defined plot. Also, we specify the range of the *x* axis by generating
    a vector using the `range` function. The `plot-iris-linear-model` function generates
    the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_48.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the curve in the plot produced from the previous example doesn't appear
    to have any definitive shape, we can still use this generated model to estimate
    or predict the value of the dependent variable by supplying values for the independent
    variables to the formulated model. In order to do this, we must first define the
    relationship between the dependent and independent variables of a linear regression
    model with multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: A linear regression model of *P* independent variables produces ![Understanding
    multivariable linear regression](img/4351OS_02_50.jpg) regression coefficients,
    since we include the error constant along with the other coefficients of the model
    and also define an extra variable ![Understanding multivariable linear regression](img/4351OS_02_39.jpg),
    which is always *1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `linear-model` function agrees with the proposition that the number of
    coefficients *P* in the formulated model is always one more than the total number
    of independent variables in the sample data *N*; this is shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We formally express the relationship between a multivariable regression model''s
    dependent and independent variables as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_52.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since the variable ![Understanding multivariable linear regression](img/4351OS_02_39.jpg)
    is always *1* in the preceding equation, the value ![Understanding multivariable
    linear regression](img/4351OS_02_53.jpg) is analogous to the error constant ![Understanding
    multivariable linear regression](img/4351OS_02_11.jpg) from the definition of
    a linear model with a single variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define a single vector to represent all the coefficients of the previous
    equation as ![Understanding multivariable linear regression](img/4351OS_02_10.jpg).
    This vector is termed as the **parameter vector** of the formulated regression
    model. Also, the independent variables of the model can be represented by a vector.
    Thus, we can define the regression variable *Y* as the product of the transpose
    of the parameter vector and the vector of independent variables of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Polynomial functions can also be reduced to the standard form by substituting
    a single variable for every higher-order variable in the polynomial equation.
    For example, consider the following polynomial equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can substitute the variables ![Understanding multivariable linear regression](img/4351OS_02_56.jpg)
    for ![Understanding multivariable linear regression](img/4351OS_02_57.jpg) to
    reduce the equation to the standard form of a multivariable linear regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This brings us to the following formal definition of the cost function for
    a linear model with multiple variables, which is simply an extension of the definition
    of the cost function for a linear model with a single variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that in the preceding definition, we can use the individual coefficients
    of the model interchangeably with the parameter vector ![Understanding multivariable
    linear regression](img/4351OS_02_10.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogous to our problem definition of fitting a model with a single variable
    over some given data, we can define the problem of formulating a multivariable
    linear model as the problem of minimizing the preceding cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding multivariable linear regression](img/4351OS_02_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent with multiple variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can apply the gradient descent algorithm to find the local minimum of a model
    with multiple variables. Of course, since we have multiple coefficients in the
    model, we have to apply the algorithm for all these coefficients as opposed to
    just two coefficients in a regression model with a single variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent algorithm can thus be used to find the values of all the
    coefficients in the parameter vector ![Gradient descent with multiple variables](img/4351OS_02_10.jpg)
    of a multivariable linear regression model, and is formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gradient descent with multiple variables](img/4351OS_02_60.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding definition, the term ![Gradient descent with multiple variables](img/4351OS_02_61.jpg)
    simply refers to the sample values for the ![Gradient descent with multiple variables](img/4351OS_02_62.jpg)
    independent variable in the formulated model. Also, the variable ![Gradient descent
    with multiple variables](img/4351OS_02_63.jpg) is always *1*. Thus, this definition
    can be applied to just the two coefficients that correspond to our previous definition
    of the gradient descent algorithm for a linear regression model with a single
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we''ve seen earlier, the gradient descent algorithm can be applied to a
    linear regression model with both single and multivariables. For some models,
    however, the gradient descent algorithm can actually take a lot of iterations,
    or rather time, to converge the estimated values of the model''s coefficients.
    Sometimes, the algorithm can also diverge, and thus we will be unable to calculate
    the model''s coefficients in such circumstances. Let''s examine some of the factors
    that affect the behavior and performance of this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: All the features of the sample data must be scaled with respect to each other.
    By scaling, we mean that all the values for the independent variables in the sample
    data take on a similar range of values. Ideally, all independent variables must
    have observed values between *-1* and *1*. This can be formally expressed as follows:![Gradient
    descent with multiple variables](img/4351OS_02_64.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can normalize the observed values for the independent variables about the
    mean of these values. We can further normalize this data by using the standard
    deviation of the observed values. In summary, we substitute the values with those
    produced by subtracting the mean of these values, ![Gradient descent with multiple
    variables](img/4351OS_02_65.jpg),and dividing the resulting expression by the
    standard deviation ![Gradient descent with multiple variables](img/4351OS_02_66.jpg).
    This is shown in the following formula:![Gradient descent with multiple variables](img/4351OS_02_67.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stepping or learning rate, ![Gradient descent with multiple variables](img/4351OS_02_35.jpg),
    is another important factor that determines how fast the algorithm converges towards
    the values of the parameters of the formulated model. Ideally, the stepping rate
    should be selected so that the differences between the old and new iterated values
    of the parameters of the model have an optimal amount of change in every iteration.
    On one hand, if this value is too large, the algorithm could even produce diverging
    values for the parameters of the model after each iteration. Thus, the algorithm
    will never find a global minimum in this case. On the other hand, a small value
    for this rate could result in slowing down the algorithm through an unnecessarily
    large number of iterations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Ordinary Least Squares
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another technique to estimate the parameter vector of a linear regression model
    is the **Ordinary Least Squares** (**OLS**) method. The OLS method essentially
    works by minimizing the sum of squared errors in a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sum of squared errors of prediction, or SSE, of a linear regression model
    can be defined in terms of the model''s actual and expected values as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_68.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding definition of the SSE can be factorized using matrix products
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_69.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can solve the preceding equation for the estimated parameter vector ![Understanding
    Ordinary Least Squares](img/4351OS_02_10.jpg) by using the definition of a global
    minimum. Since this equation is a form of quadratic equation and the term ![Understanding
    Ordinary Least Squares](img/4351OS_02_70.jpg) is always greater than zero, the
    global minimum of the surface of the cost function can be defined as the point
    at which the rate of change of the slope of a tangent to the surface at that point
    is zero. Also, the plot is a function of the parameters of the linear model, and
    so the equation of the surface plot should be differentiated by the estimated
    parameter vector ![Understanding Ordinary Least Squares](img/4351OS_02_10.jpg).
    We can thus solve this equation for the optimal parameter vector ![Understanding
    Ordinary Least Squares](img/4351OS_02_10.jpg) of the formulated model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_71.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The last equation in the preceding derivation gives us the definition of the
    optimal parameter vector ![Understanding Ordinary Least Squares](img/4351OS_02_10.jpg),
    which is formally expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding Ordinary Least Squares](img/4351OS_02_72.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can implement the preceding definition of the parameter vector through the
    OLS method using the core.matrix library''s `transpose` and `inverse` functions
    and the Incanter library''s `bind-columns` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we first add a column in which each element is `1`, as the first column
    of the matrix `MX` uses the `bind-columns` function. The extra column that we
    add represents the independent variable ![Understanding Ordinary Least Squares](img/4351OS_02_39.jpg),
    whose value is always `1`. We then use the `transpose` and `inverse` functions
    to calculate the estimated coefficients of the linear regression model for the
    data in matrices `MX` and `MY`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the current example, the `bind-columns` function from the Incanter library
    can be imported into our namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The previously defined function can be applied to the matrices that we have
    previously defined (*X* and *Y*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `ols-linear-model-coefs` is simply the variable and `ols-linear-model`
    is a matrix with a single column, which is represented as a vector. We perform
    this conversion using the `as-vec` function from the clatrix library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can actually verify that the coefficients estimated by the `ols-linear-model`
    function are practically equal to the ones generated by the Incanter library''s
    `linear-model` function, which is illustrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the last expression in the preceding code example, we find the difference
    between the coefficients produced by the `ols-linear-model` function, the difference
    produced by the `linear-model` function, and check whether each of these differences
    is less than `0.0001`.
  prefs: []
  type: TYPE_NORMAL
- en: Using linear regression for prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we've determined the coefficients of a linear regression model, we can
    use these coefficients to predict the value of the dependent variable of the model.
    The predicted value is defined by the linear regression model as the sum of the
    products of each coefficient and the value of its corresponding independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily define the following generic function, which when supplied with
    the coefficients and values of independent variables, predicts the value of the
    dependent variable for a given formulated linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding function, we use a precondition to assert the number of coefficients
    and the values of independent variables. This function expects that the number
    of values of the independent variables is one less than the number of coefficients
    of the model, as we add an extra parameter to represent an independent variable
    whose value is always *1*. The function then calculates the product of the corresponding
    coefficients and the values of the independent variables using the `map` function,
    and then calculates the sum of these product terms using the `reduce` function.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear regression estimates some given training data using a linear equation;
    this solution may not always be the best fit for the given data. Of course, it
    depends largely on the problem that we are trying to model. **Regularization**
    is a commonly used technique to provide a better fit for the data. Generally,
    a given model is regularized by reducing the effect of some of the independent
    variables of the model. Alternatively, we could model it as a higher-order polynomial.
    Regularization isn't exclusive to linear regression, and most machine learning
    algorithms use some form of regularization in order to create a more accurate
    model from the given training data.
  prefs: []
  type: TYPE_NORMAL
- en: A model is said to be **underfit** or **high bias** when it doesn't estimate
    the dependent variable to a value that is close to the observed values of the
    dependent variable in the training data. On the other hand, a model can also be
    called **overfit**, or said to have **high variance**, when the estimated model
    fits the data perfectly, but isn't general enough to be useful for prediction.
    Overfit models often describe random errors or noise in the training data instead
    of the underlying relationship between the dependent and independent variables
    of the model. The best fit regression model generally lies in between the models
    created by underfitting and overfitting models and can be obtained through the
    process of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'A commonly used method for the regularization of an underfit or overfit model
    is **Tikhnov regularization**. In statistics, this method is also called **ridge
    regression**. We can describe the general form of Tikhnov regularization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_73.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Suppose *A* represents a mapping from the vector of independent variables *x*
    to the dependent variable *y*. The value *A* is analogous to the parameter vector
    of a regression model. The relationship between the vector *x* and the observed
    values of the dependent variable, written as *b*, can be expressed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'An underfit model has a significant error, or rather deviation, with respect
    to the actual data. We should strive to minimize this error. This can be formally
    expressed as follows and is based on the sum of residues of the estimated model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_74.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Tikhnov regularization adds a penalized least squares term to the preceding
    equation to prevent overfitting and is formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_75.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The term ![Understanding regularization](img/4351OS_02_76.jpg) in the preceding
    equation is called the regularization matrix. In the simplest form of Tikhnov
    regularization, this matrix takes the value ![Understanding regularization](img/4351OS_02_77.jpg),
    where ![Understanding regularization](img/4351OS_02_78.jpg) is a constant. Although
    applying this equation to a regression model is beyond the scope of this book,
    we can use Tikhnov regularization to produce a linear regression model with the
    following cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_79.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, the term ![Understanding regularization](img/4351OS_02_80.jpg)
    is called the regularization parameter of the model. This value must be chosen
    appropriately as larger values for this parameter could produce an underfit model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the previously defined cost function, we can apply a gradient descent
    to determine the parameter vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_81.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also apply regularization to the OLS method of determining the parameter
    vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_02_82.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *L* is called the smoothing matrix, and can take
    on the following forms. Note that we've used the latter form of the definition
    of *L* in [Chapter 1](ch01.html "Chapter 1. Working with Matrices"), *Working
    with Matrices*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding regularization](img/4351OS_01_0100.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, when the regularization parameter ![Understanding regularization](img/4351OS_02_80.jpg)
    in the preceding equation is *0*, the regularized solution reduces to the original
    solution using the OLS method.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve studied linear regression and a couple of algorithms
    that can be used to formulate an optimal linear regression model from some sample
    data. The following are some of the other points that we covered:'
  prefs: []
  type: TYPE_NORMAL
- en: We discussed linear regression with single and multiple variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implemented the gradient descent algorithm to formulate a linear regression
    model with one variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We implemented the **Ordinary Least Squares** (**OLS**) method to find the coefficients
    of an optimal linear regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduced regularization and how it could be applied to linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapter, we will study a different area of machine learning,
    that is, classification. Classification is also a form of regression and is used
    to categorize data into different classes or groups.
  prefs: []
  type: TYPE_NORMAL
