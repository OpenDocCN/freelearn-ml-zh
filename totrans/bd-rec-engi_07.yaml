- en: Chapter 7. Building Real-Time Recommendation Engines with Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：使用Spark构建实时推荐引擎
- en: In this day and age, the need to build scalable real-time recommendations is
    increasing day by day. With more internet users using e-commerce sites for their
    purchases, these e-commerce sites have realized the potential of understanding
    the patterns of the users' purchase behavior to improve their business, and to
    serve their customers on a very personalized level. To build a system which caters
    to a huge user base and generates recommendations in real time, we need a modern,
    fast scalable system. **Apache Spark**, which is a special framework designed
    for distributed in-memory data processing, comes to our rescue. Spark applies
    a set of transformations and actions to distributed data to build real-time data
    mining applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时代，构建可扩展的实时推荐的需求日益增长。随着越来越多的互联网用户使用电子商务网站进行购买，这些电子商务网站已经意识到理解用户购买行为模式以改善其业务、在非常个性化的层面上服务客户的潜力。为了构建一个能够满足庞大用户群并实时生成推荐的系统，我们需要一个现代、快速可扩展的系统。专为分布式内存数据处理设计的**Apache
    Spark**框架为我们提供了帮助。Spark通过对分布式数据进行一系列转换和操作来构建实时数据挖掘应用。
- en: In the previous chapters, we learned about implementing similarity-based collaborative
    filtering approaches, such as user-based collaborative filtering and content-based
    collaborative filtering. Though the similarity-based approaches are a huge success
    in commercial applications, there came into existence model-based recommender
    models, such as matrix factorization models, which have improved the performance
    of recommendation engine models. In this chapter, we will learn about the model-based
    approach of collaborative filtering, moving away from the heuristic-based similarity
    approaches. Also, we will focus on implementing the model-based collaborative
    filtering approach using Spark.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了实现基于相似度的协同过滤方法，例如基于用户的协同过滤和基于内容的协同过滤。尽管基于相似度的方法在商业应用中取得了巨大成功，但基于模型的推荐模型也应运而生，例如矩阵分解模型，这些模型提高了推荐引擎模型的性能。在本章中，我们将学习基于模型的协同过滤方法，远离基于启发式的相似度方法。此外，我们还将关注使用Spark实现基于模型的协同过滤方法。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: What is in Spark 2.0
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0有什么新功能
- en: Setting up the pyspark environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置pyspark环境
- en: Basic Spark concepts
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本Spark概念
- en: The MLlib recommendation engine module
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib推荐引擎模块
- en: The Alternating Least Squares algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交替最小二乘算法
- en: Data exploration of the Movielens-100k dataset
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Movielens-100k数据集的数据探索
- en: Building model-based recommendation engines using ALS
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ALS构建基于模型的推荐引擎
- en: Evaluating the recommendation engine model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估推荐引擎模型
- en: Parameter tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调整
- en: About Spark 2.0
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于Spark 2.0
- en: '**Apache Spark** is a fast, powerful, easy-to-use, distributed, in-memory,
    and open source cluster computing framework built to perform advanced analytics.
    It was originally developed at UC Berkeley in 2009\. Spark has been widely adopted
    by enterprises across a wide range of industries since its inception.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**是一个快速、强大、易于使用、分布式、内存中、开源的集群计算框架，旨在执行高级分析。它最初于2009年在加州大学伯克利分校开发。自推出以来，Spark已被众多行业的众多企业广泛采用。'
- en: One of the main advantages of Spark is that it takes all the complexities away
    from us, such as resources scheduling, job submissions, executions, tracking,
    between-node communication, fault tolerance, and all low-level operations that
    are inherent features of parallel processing. The Spark framework helps us write
    programs to run on the clusters in parallel.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的主要优势之一是它为我们移除了所有复杂性，例如资源调度、作业提交、执行、跟踪、节点间通信、容错以及所有并行处理固有的低级操作。Spark框架帮助我们编写在集群上并行运行的程序。
- en: Spark can be run both as a standalone mode and as a cluster mode. Spark can
    be easily integrated with Hadoop platforms.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以以独立模式或集群模式运行。Spark可以轻松集成到Hadoop平台中。
- en: As a general-purpose computing engine, Spark with its in-memory data processing
    capability and easy-to-use APIs allows us to efficiently work on a wide range
    of large-scale data processing tasks, such as streaming applications, machine
    learning, or interactive SQL queries over large datasets that require iterative
    access.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种通用计算引擎，Spark凭借其内存数据处理能力和易于使用的API，使我们能够高效地处理各种大规模数据处理任务，例如流应用、机器学习或对大型数据集进行迭代访问的交互式SQL查询。
- en: Spark can be easily integrated with many applications, data sources, storage
    platforms, and environments, and exposes high-level APIs in Java, Python, and
    R to work with. Spark has proved to be broadly useful for a wide range of large-scale
    data processing tasks, over and above machine learning and iterative analytics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以轻松地与许多应用程序、数据源、存储平台和环境集成，并暴露Java、Python和R的高级API以进行工作。Spark已被证明在机器学习和迭代分析之外，对于广泛的规模数据处理任务具有广泛的应用价值。
- en: '![About Spark 2.0](img/image00398.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![关于Spark 2.0](img/image00398.jpeg)'
- en: 'Credits: Databricks'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 信用：Databricks
- en: Spark architecture
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark架构
- en: 'The Apache Spark ecosystem contains many components to work with distributed,
    in-memory, and machine-learning data processing tools. The main components of
    Spark are discussed in the following sub-sections. Spark works on a master-slave
    architecture; a high-level architecture is shown in the following diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark生态系统包含许多组件，用于处理分布式、内存中以及机器学习数据处理工具。Spark的主要组件将在以下子节中讨论。Spark基于主从架构；高级架构如下所示：
- en: '![Spark architecture](img/image00399.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Spark架构](img/image00399.jpeg)'
- en: 'Credits: Databricks'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 信用：Databricks
- en: 'The Spark cluster works on the master-slave architecture. The Spark Core execution
    engine accepts requests from clients and passes them to the master node. The driver
    program in the master communicates with the worker node executors to get the work
    done as shown in the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark集群采用主从架构。Spark Core执行引擎接受来自客户端的请求并将它们传递给主节点。主节点中的驱动程序程序与工作节点执行器通信以完成任务，如下所示：
- en: '![Spark architecture](img/image00400.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Spark架构](img/image00400.jpeg)'
- en: '**Spark driver program**: The driver program acts as a master node in a Spark
    cluster, which hosts the SparkContext for Spark applications. It receives the
    client request and co-ordinates with the cluster manager which manages the worker
    nodes. The driver program splits the original request into tasks and schedules
    them to run on executors in worker nodes. All the processes in Spark are Java
    processes. The SparkContext creates **Resilient Distributed Datasets** (**RDD**),
    an immutable, distributable collection of datasets partitioned across nodes, and
    performs a series of transformations and actions to compute the final output.
    We will learn more about RDDs in the latter sections.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark驱动程序程序**：驱动程序程序在Spark集群中充当主节点，为主应用提供SparkContext。它接收客户端请求并与管理工作节点的集群管理器协调。驱动程序程序将原始请求拆分为任务并将它们调度到工作节点上的执行器上运行。Spark中的所有进程都是Java进程。SparkContext创建**弹性分布式数据集**（**RDD**），这是一个不可变、可分发的数据集集合，分布在节点之间，并执行一系列转换和操作以计算最终输出。我们将在后面的章节中了解更多关于RDD的信息。'
- en: '**Worker nodes**: A worker contains executors, where the actual task execution
    happens in the form of Java processes. Each worker runs its own Spark instance
    and is the main compute node in Spark. When a SparkContext is created, each worker
    node starts its own executors to receive the tasks.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**工作节点**：工作节点包含执行器，实际任务执行以Java进程的形式发生。每个工作节点运行自己的Spark实例，是Spark中的主要计算节点。当创建SparkContext时，每个工作节点启动自己的执行器以接收任务。'
- en: '**Executors**: These are the main task executioners of Spark applications.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行器**：这些是Spark应用程序的主要任务执行器。'
- en: Spark components
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark组件
- en: 'In this section, we will see the core components of the Spark ecosystem. The
    following diagram shows the Apache Spark Ecosystem:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到Spark生态系统的核心组件。以下图表显示了Apache Spark生态系统：
- en: '![Spark components](img/image00401.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Spark组件](img/image00401.jpeg)'
- en: Credits:Databricks
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 信用：Databricks
- en: Spark Core
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark Core
- en: '**Spark Core** is the core part of the Spark platform: the execution engine.
    All other functionalities are built on top of Spark Core. This provides all the
    capabilities of Spark, such as in-memory distributed computation, and fast, easy-to-use
    APIs.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark Core**是Spark平台的核心部分：执行引擎。所有其他功能都是建立在Spark Core之上的。这提供了Spark的所有功能，如内存中分布式计算、快速且易于使用的API。'
- en: Structured data with Spark SQL
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark SQL的Structured数据
- en: '**Spark SQL** is a component on top of Spark Core. It is a spark module that
    provides support for structured and semi-structured data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark SQL**是Spark Core之上的一个组件。它是一个提供对结构化和半结构化数据支持的Spark模块。'
- en: Spark SQL provides a unified approach, to allow users to query the data objects
    in an interactive SQL type, such as applying select, where you can group data
    objects by the kind of operations through data abstraction APIs, such as DataFrames.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一种统一的方法，允许用户以交互式SQL类型查询数据对象，例如应用选择，您可以通过数据抽象API（如DataFrame）按操作类型对数据对象进行分组。
- en: A considerable amount of time will be dedicated to data exploration, exploratory
    analysis, and SQL-like interactions. Spark SQL, which provides DataFrames, also
    acts as a distributed SQL query engine; for instance, in R, the DataFrames in
    Spark 2.0, the data is stored as rows and columns, access to which is allowed
    as an SQL table with all the structural information, such as data types.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将大量时间投入到数据探索、探索性分析和类似SQL的交互中。Spark SQL，它提供了DataFrame，还充当一个分布式SQL查询引擎；例如，在R中，Spark
    2.0的DataFrame将数据存储为行和列，允许以SQL表的形式访问，其中包含所有结构信息，如数据类型。
- en: Streaming analytics with Spark Streaming
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark Streaming进行流式分析
- en: '**Spark Streaming** is another Spark module that enables users to process and
    analyze both batch and streaming data in real time, to perform interactive and
    analytical applications. Spark Streaming provides **Discretized Stream** (**DStream**),
    a high-level abstraction, to represent a continuous stream of data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**Spark Streaming** 是另一个Spark模块，它允许用户实时处理和分析批量和流式数据，以执行交互式和分析应用。Spark Streaming提供了高级抽象**离散流**（**DStream**），以表示连续的数据流。'
- en: 'The main features of Spark Streaming API are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming API的主要特性如下：
- en: Scalable
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展
- en: High throughput
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高吞吐量
- en: Fault-tolerant
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错
- en: Processes live stream of incoming data
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理实时流入的数据流
- en: Can connect to real-time data sources and process real-time incoming data on
    the go
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以连接到实时数据源，并实时处理实时传入的数据
- en: Can apply complex machine learning and graph processing algorithms on streaming
    data
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在流式数据上应用复杂的机器学习和图处理算法
- en: '![Streaming analytics with Spark Streaming](img/image00402.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark Streaming进行流式分析](img/image00402.jpeg)'
- en: Machine learning with MLlib
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MLlib进行机器学习
- en: '**MLlib** is another module in Spark, built on top of Spark Core. This machine
    learning library is developed with an objective to make practical machine learning
    scalable and easy to use.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLlib** 是Spark中另一个模块，建立在Spark Core之上。这个机器学习库的开发目标是使实用的机器学习可扩展且易于使用。'
- en: 'This library provides tools for data scientists, such as the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库为数据科学家提供了以下工具：
- en: Machine learning algorithms for regression, classification, clustering, and
    recommendation engines
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于回归、分类、聚类和推荐引擎的机器学习算法
- en: Feature extraction, feature transformation, dimensionality reduction, and feature
    selection
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取、特征转换、降维和特征选择
- en: Pipeline tools for streamlining machine learning processes for construction,
    evaluation, and tuning the over process of solving a machine learning problem
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于简化机器学习过程的管道工具，包括构建、评估和调整解决机器学习问题的过程
- en: Persistence of storing and loading machine learning models and pipelines
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储和加载机器学习模型和管道的持久性
- en: Utilities, such as linear algebra and statistical tasks
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如线性代数和统计任务之类的实用工具
- en: When starting Spark 2.0, the old MLlib model is replaced with ML library, which
    is built with DataFrames APIs, providing more optimizations and making uniform
    APIs across all languages.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动Spark 2.0时，旧的MLlib模型被替换为使用DataFrame API构建的ML库，提供了更多优化，并使所有语言的API统一。
- en: Graph computation with GraphX
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GraphX进行图计算
- en: '**GraphX** is a new Spark API for building graph-based systems. It is a graph-parallel
    processing computation engine and distributed framework, which is built on top
    of Spark Core. This project was started with the objective of unifying the graph-parallel
    and data distribution framework into a single Spark API. GraphX enables users
    to process data both as RDDs and graphs.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphX** 是一个用于构建基于图系统的Spark新API。它是一个图并行处理计算引擎和分布式框架，建立在Spark Core之上。该项目始于将图并行和数据分布框架统一到单个Spark
    API的目标。GraphX允许用户以RDD和图的形式处理数据。'
- en: 'GraphX provides many features, such as the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GraphX提供了许多功能，例如以下：
- en: Property graphs
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属性图
- en: Graph-based algorithms, such as PageRank, connected components, and Graph Builders,
    which are used to build graphs
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图的算法，如PageRank、连通组件和Graph Builders，用于构建图
- en: Basic graph computational components, such as subgraph, joinVertices, aggregateMessages,
    Pregel API, and so on
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本图计算组件，如子图、joinVertices、aggregateMessages、Pregel API等
- en: Though graph models are not within the scope of this book, we will learn some
    fundamentals of graph applications in [Chapter 8](part0057.xhtml#aid-1MBG21 "Chapter 8. 
    Building Real-Time Recommendations with Neo4j"), *Building Real-Time Recommendations
    with Neo4j*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图模型不在此书的范围内，我们将在[第8章](part0057.xhtml#aid-1MBG21 "第8章。使用Neo4j构建实时推荐")中学习一些图应用的基本原理，*使用Neo4j构建实时推荐*。
- en: Benefits of Spark
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark的优势
- en: The main advantages of Spark are, it is fast, has in-memory framework, contains
    several APIs, making it very easy to use, its Unified Engine for large quantities
    of data, and its machine learning components. Unlike Map-Reduce model with its
    batch mode, which is slower and contains lot of programming, Spark is faster,
    with real-time and easy to code framework.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的主要优势是速度快，具有内存框架，包含多个API，使其非常易于使用，其统一引擎适用于大量数据，以及其机器学习组件。与具有批量模式的Map-Reduce模型相比，Spark更快，具有实时和易于编码的框架。
- en: 'The following diagram shows the above mentioned benefits:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了上述提到的优势：
- en: '![Benefits of Spark](img/image00403.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![Spark的优势](img/image00403.jpeg)'
- en: Setting up Spark
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Spark
- en: Spark runs on both Windows and UNIX-like systems (for example, Linux, Mac OS).
    It's easy to run locally on one machine; all you need is to have Java installed
    on your system PATH or the `JAVA_HOME` environment variable pointing to a Java
    installation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在Windows和类UNIX系统（例如Linux、Mac OS）上运行。在单台机器上本地运行很简单；您只需要在系统PATH或`JAVA_HOME`环境变量中安装Java即可。
- en: Spark runs on Java 7+, Python 2.6+/3.4+ and R 3.1+. For the Scala API, Spark
    2.0.0 uses Scala 2.11\. You will need to use a compatible Scala version (2.11.x).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在Java 7+、Python 2.6+/3.4+和R 3.1+上运行。对于Scala API，Spark 2.0.0使用Scala 2.11。您需要使用兼容的Scala版本（2.11.x）。
- en: 'Get Spark from the downloads page of the project website:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从项目网站的下载页面获取Spark：
- en: '[http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz)'
- en: Spark needs to be built against a specific version of Hadoop in order to access
    **Hadoop Distributed File System** (**HDFS**), as well as standard and custom
    Hadoop input sources.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了访问**Hadoop分布式文件系统**（**HDFS**）以及标准和自定义Hadoop输入源，Spark需要针对特定版本的Hadoop进行构建。
- en: Spark requires the Scala programming language (version 2.10.4 at the time of
    writing this book) in order to run. Fortunately, the prebuilt binary package comes
    with the Scala runtime packages included, so you don't need to install Scala separately
    in order to get started. However, you will need to have a **Java Runtime Environment**
    (**JRE**) or **Java Development Kit** (**JDK**) installed (take a look at the
    software and hardware list in this book's code bundle for installation instructions).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行，Spark需要Scala编程语言（本书撰写时为版本2.10.4）。幸运的是，预构建的二进制包包含了Scala运行时包，因此您无需单独安装Scala即可开始。但是，您需要安装**Java运行时环境**（**JRE**）或**Java开发工具包**（**JDK**）（请参阅本书代码包中的软件和硬件列表以获取安装说明）。
- en: 'Once you have downloaded the Spark binary package, unpack the contents of the
    package and change it into the newly created directory by running the following
    commands:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Spark二进制包后，请运行以下命令解压包内容并将其更改到新创建的目录中：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Spark places user scripts to run Spark in the bin directory. You can test whether
    everything is working correctly by running one of the example programs included
    in Spark:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Spark将用户脚本放置在bin目录中以运行Spark。您可以通过运行Spark中包含的示例程序之一来测试是否一切正常：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can run Spark interactively with Scala using the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令使用Scala与Spark进行交互式运行：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `--master` option specifies the master URL for a distributed cluster, or
    you can use `local` to run locally with one thread or `local[N]` to run locally
    with N threads. You should start by using `local` for testing. For a full list
    of options, run the Spark shell with the `--help` option.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`--master`选项指定了分布式集群的主URL，或者您可以使用`local`在本地使用一个线程运行，或者使用`local[N]`在本地使用N个线程运行。您应该从使用`local`进行测试开始。要获取完整的选项列表，请使用`--help`选项运行Spark
    shell。'
- en: Source:[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)
- en: About SparkSession
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于SparkSession
- en: From Spark 2.0, the `SparkSession` will be the entry point for Spark applications.
    The `SparkSession` serves as the main interactive access point for underlying
    Spark functionalities and Spark programming capabilities, such as DataFrames API
    and Dataset API. We use `SparkSession` to create DataFrame objects.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 2.0 开始，`SparkSession` 将成为 Spark 应用的入口点。`SparkSession` 作为对底层 Spark 功能和
    Spark 编程能力（如 DataFrame API 和 Dataset API）的主要交互访问点，我们使用 `SparkSession` 来创建 DataFrame
    对象。
- en: In the earlier versions of Spark, we used to create `SparkConf`, `SparkContext`,
    or `SQLContext` to interact with Spark, but since Spark 2.0, this has been taken
    care of by `SparkSession` by encapsulating `SparkConf`, `SparkContext` automatically.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 的早期版本中，我们通常使用 `SparkConf`、`SparkContext` 或 `SQLContext` 与 Spark 进行交互，但自从
    Spark 2.0 以来，这已经由 `SparkSession` 通过封装 `SparkConf` 和 `SparkContext` 自动处理。
- en: When you start Spark in the shell command, `SparkSession` is created automatically
    as `spark`
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 shell 命令中启动 Spark 时，`SparkSession` 会自动创建为 `spark`
- en: 'We can programmatically create SparkSession, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式程序化地创建 `SparkSession`：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Resilient Distributed Datasets (RDD)
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（RDD）
- en: The core of Spark is Resilient Distributed Datasets, in short, RDD. RDD is an
    immutable distributed collection of objects of some datatype of your data, partitioned
    across nodes on your cluster. This RDD is fault-tolerant, that is, a property
    of the system that is able to operate continuously, even in the event of failure
    by reconstructing the failed partition.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的核心是弹性分布式数据集，简称 RDD。RDD 是一个不可变分布式集合，包含你数据中某些数据类型的对象，分布在集群的节点上。这个 RDD 是容错的，也就是说，系统具有在发生故障时能够持续运行的能力，即使是通过重建失败的分区。
- en: In short, we can say that RDD is a distributed dataset abstraction, which allows
    iterative operations on very large-scale cluster systems in a fault-tolerant way.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以这样说，RDD 是一种分布式数据集抽象，它允许以容错方式在非常大的集群系统中进行迭代操作。
- en: 'RDDs can be created in a number of ways, such as parallelizing an existing
    collection of data objects, or referencing an external file system, such as HDFS:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 可以通过多种方式创建，例如并行化现有数据对象集合，或引用外部文件系统，如 HDFS：
- en: 'Creating RDD from an existing data object:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从现有数据对象创建 RDD：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Creating RDD from a referenced file:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从引用的文件创建 RDD：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'RDD supports two types of operations:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 支持两种类型的操作：
- en: '**Transformations**: This operation creates new RDDs from existing RDDs, which
    are immutable'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：此操作从现有的 RDD 创建新的 RDD，这些 RDD 是不可变的'
- en: '**Actions**: This operation returns values after performing computation on
    the dataset'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：此操作在执行数据集上的计算后返回值'
- en: These RDD transformations are executed lazily only when the final results are
    required. We can recreate or recompute the RDDs any number of times, or we can
    persist them by caching them in memory if we know we may need them in the future.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 RDD 转换仅在需要最终结果时才会被惰性执行。我们可以重新创建或重新计算 RDD 任意次数，或者如果我们知道我们可能需要在将来使用它们，我们可以通过在内存中缓存它们来持久化它们。
- en: About ML Pipelines
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于 ML 流程
- en: 'The ML Pipelines API in Spark 2.0 is the way to use a standard workflow when solving
    machine learning problems. Every machine learning problem will undergo a sequence
    of steps, such as the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.0 中的 ML 流程 API 是在解决机器学习问题时使用标准工作流程的方式。每个机器学习问题都将经历一系列步骤，如下所示：
- en: Loading data.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Feature extraction.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取。
- en: Model training.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练。
- en: Evaluation.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估。
- en: Predictions.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测。
- en: Model tuning.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型调优。
- en: 'If we closely observe the aforementioned steps, we can see the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察上述步骤，我们可以看到以下内容：
- en: The ML process follows a series of steps as if it is a workflow.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习过程遵循一系列步骤，就像一个工作流程。
- en: Often, we require more than one algorithm while solving a machine learning problem;
    for example, a text classification problem might require a feature extraction
    algorithm for feature extraction.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解决机器学习问题时，我们通常需要不止一个算法；例如，一个文本分类问题可能需要一个特征提取算法来进行特征提取。
- en: Generating predictions on test data may require many data transformations or
    data-preprocessing steps, which are used during model training. For example, in
    text classification problems, making predictions on test data involves data pre-processing
    steps, such as tokenization and feature extraction, before applying them to a
    generated classification model, which was used during model creation on training
    data.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试数据上生成预测可能需要许多数据转换或数据预处理步骤，这些步骤在模型训练期间使用。例如，在文本分类问题中，在测试数据上做出预测涉及数据预处理步骤，如标记化和特征提取，然后再将这些步骤应用于在训练数据创建模型时使用的生成的分类模型。
- en: The preceding steps form one of the main motivating factors behind introducing
    the ML Pipeline API. The ML Pipeline module allows users to define a sequence
    of stages, so that it's easy to use. The API framework allows the ML process to
    scale on a distributed platform and accommodate very large datasets, reuse some
    components, and so on.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤是引入 ML Pipeline API 的主要动机之一。ML Pipeline 模块允许用户定义一系列阶段，使得使用起来非常方便。API 框架允许
    ML 流程在分布式平台上进行扩展，并适应非常大的数据集，重用一些组件，等等。
- en: 'The components of the ML Pipeline module are as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ML Pipeline 模块的组件如下：
- en: '**DataFrame**: As mentioned earlier, DataFrame is a way of representing the
    data in the Spark framework.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataFrame**：如前所述，DataFrame 是在 Spark 框架中表示数据的一种方式。'
- en: '**Transformers**: Transformers take input DataFrames and transform data into
    new DataFrames. Transformation classes contain the `transform()` method to do
    the transformations.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：转换器接收输入 DataFrame 并将数据转换成新的 DataFrame。转换器类包含 `transform()` 方法来进行转换。'
- en: '**Estimators**: Estimators compute the final results. An Estimator class makes
    use of the `fit()` method to compute results.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估算器**：估算器计算最终结果。估算器类使用 `fit()` 方法来计算结果。'
- en: '**Pipeline**: This is a set of Transformers and Estimators stacked as a workflow.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道**：这是一个将转换器和估算器堆叠为工作流程的集合。'
- en: '**Parameters**: This refers to the set of parameters that may be used by both
    Transformers and Estimators.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：这指的是转换器和估算器都可能使用的参数集。'
- en: 'We''ll illustrate this for the simple text document workflow. The following
    figure is for the training time usage of a pipeline:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以简单的文本文档工作流程为例来展示这一点。以下图是管道的训练时间使用情况：
- en: '![About ML Pipelines](img/image00404.jpeg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![关于 ML Pipeline](img/image00404.jpeg)'
- en: Find below the explanation for the steps shown in the preceding figure. The
    blue boxes are Transformers and the red box is the Estimator.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对前面图中所示步骤的解释。蓝色框是转换器，红色框是估算器。
- en: The Tokenizer Transformer takes the text column of a DataFrame as input and
    returns a new DataFrame column containing tokens.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tokenizer 转换器接收 DataFrame 的文本列作为输入，并返回一个包含标记的新 DataFrame 列。
- en: The HashingTF Transformer takes in the tokens DataFrame from the previous step
    as input and creates new DataFrame features as output.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HashingTF 转换器接收来自前一步骤的标记 DataFrame 作为输入，并创建新的 DataFrame 特征作为输出。
- en: Now the LogisticRegression Estimator takes in the features DataFrame, fits a
    logistic regression model, and creates a PipelineModel transformer.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，LogisticRegression 估算器接收特征 DataFrame，拟合逻辑回归模型，并创建一个 PipelineModel 转换器。
- en: First we build a pipeline, which is an Estimator, then on this Pipeline we apply
    `fit()` method which produces a PipelineModel, a Transformer, that can be used
    on test data or at prediction time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们构建一个管道，这是一个估算器，然后在这个管道上应用 `fit()` 方法，这将产生一个 PipelineModel，这是一个转换器，可以在测试数据或预测时使用。
- en: Source:[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)
- en: 'The following figure illustrates this usage:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这种用法：
- en: '![About ML Pipelines](img/image00405.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![关于 ML Pipeline](img/image00405.jpeg)'
- en: In the preceding figure, when we want to make predictions on the test data,
    we observe that first the test data has to pass through a series of data-preprocessing
    steps, which are very much identical to the aforementioned training step. After
    the pre-processing step is completed, the features of the test data are applied
    to the logistic regression model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，当我们想要在测试数据上做出预测时，我们观察到测试数据首先必须通过一系列数据预处理步骤，这些步骤与上述训练步骤非常相似。预处理步骤完成后，测试数据的特征应用于逻辑回归模型。
- en: In order to make the data-preprocessing and feature extraction steps identical,
    we will pass the test data to the PipelineModel (logistic regression model) by
    calling the `transform()` to generate the predictions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据预处理和特征提取步骤相同，我们将通过调用 `transform()` 将测试数据传递给 PipelineModel（逻辑回归模型）以生成预测。
- en: Collaborative filtering using Alternating Least Square
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交替最小二乘法的协同过滤
- en: 'In this section, let''s explain the Matrix Factorization Model (MF) and the
    Alternating Least Squares method. Before we get to know about the Matrix Factorization
    Model, we''ll define the objective once again. Imagine we have ratings given to
    items by a number of users. Let''s define the ratings given by users on items
    in a matrix form given by *R*, as shown in the following diagram:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们解释矩阵分解模型（MF）和交替最小二乘法。在我们了解矩阵分解模型之前，我们再次定义目标。想象一下，我们有一些用户对物品给出的评分。让我们用矩阵形式定义用户对物品给出的评分，如下所示：
- en: '![Collaborative filtering using Alternating Least Square](img/image00406.jpeg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![使用交替最小二乘法的协同过滤](img/image00406.jpeg)'
- en: In the preceding diagram, we observe that user Ted has rated items B and D as
    4 and 3 respectively. In a collaborative filtering approach, the first step before
    generating recommendations is to fill the empty spaces, that is, to predict the
    non-rated items. Once the non-rated item ratings are filled, we suggest new items
    to the users by ranking the newly filled items.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们观察到用户Ted分别对物品B和D给出了4和3的评分。在协同过滤方法中，在生成推荐之前的第一步是填充空白区域，即预测未评分的项。一旦填充了未评分项的评分，我们通过对新填充的项进行排序，向用户推荐新的物品。
- en: 'In the previous chapters, we have seen neighbouring methods using Euclidean
    distances and cosine distances to predict the missing values. In this section,
    we will adopt a new method to fill the missing non-rated items. This approach
    is called the matrix factorization method. This is a mathematical approach, which
    uses matrix decomposition methods. This method is explained as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经看到了使用欧几里得距离和余弦距离的邻近方法来预测缺失值。在本节中，我们将采用一种新的方法来填充缺失的非评分项。这种方法被称为矩阵分解法。这是一个数学方法，它使用矩阵分解方法。该方法解释如下：
- en: A matrix can be decomposed into two low rank matrices, which, when multiplied
    back, will result in a single matrix approximately equal to the original matrix.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个矩阵可以被分解成两个低秩矩阵，当它们相乘时，将得到一个与原始矩阵近似相等的单个矩阵。
- en: Let's say a rating matrix *R* of size *U X M* can be decomposed into two low
    rank matrices *P* and *Q* of size *U X K* and *M X K* respectively, where *K*
    is called the rank of the matrix.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个大小为 *U X M* 的评分矩阵 *R* 可以分解成两个低秩矩阵 *P* 和 *Q*，分别具有大小 *U X K* 和 *M X K*，其中
    *K* 被称为矩阵的秩。
- en: 'In the following example, let''s say the original matrix of size *4 X 4* is
    decomposed into two matrices: *P (4 X 2)* and *Q (4 X 2)*. Multiplying back *P*
    and *Q* will give us the original matrix of size *4 X 4* and values approximately
    equal to those of the original matrix:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，假设原始矩阵的大小为 *4 X 4* 被分解成两个矩阵：*P (4 X 2)* 和 *Q (4 X 2)*。将 *P* 和 *Q* 相乘将给出原始矩阵的大小
    *4 X 4* 以及与原始矩阵值大致相等的值：
- en: '![Collaborative filtering using Alternating Least Square](img/image00407.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![使用交替最小二乘法的协同过滤](img/image00407.jpeg)'
- en: The principle of matrix factorization is used in recommendation engines, to
    fill the non-rated items. The assumption in applying the aforementioned principle
    to recommendation engines is that the ratings given by users on items are based
    on some latent features. These latent features are applicable to both users and
    items, that is, a user rates an item because of some of his personal preferences
    for it. Also, the user rates items because of certain features of the items.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解原理用于推荐引擎中，以填充未评分的项。将上述原理应用于推荐引擎的假设是，用户对物品给出的评分基于一些潜在特征。这些潜在特征适用于用户和物品，也就是说，用户因为对某个物品的个人偏好而评分该物品。同样，用户因为物品的某些特征而评分物品。
- en: 'Using this assumption, when the matrix factorization method is applied to the
    ratings matrix, we decompose the original ratings matrix into two matrices follows
    as user-latent factor matrix, P, and item-latent factor matrix:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个假设，当矩阵分解方法应用于评分矩阵时，我们将原始评分矩阵分解成两个矩阵，如下所示：用户潜在因子矩阵 *P* 和物品潜在因子矩阵：
- en: '![Collaborative filtering using Alternating Least Square](img/image00408.jpeg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![使用交替最小二乘法的协同过滤](img/image00408.jpeg)'
- en: 'Now, let''s come back to the machine learning approach; you must be wondering
    what the learning in this approach is. Observe the following formula:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到机器学习方法；你一定想知道这种方法中的学习是什么。观察以下公式：
- en: '![Collaborative filtering using Alternating Least Square](img/image00409.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![使用交替最小二乘法的协同过滤](img/image00409.jpeg)'
- en: We have learnt that when we multiply back the two latent factor matrices, we
    get the approximate original matrix. Now, in order to improve the accuracy of
    the model, that is, to learn the optimal factor vectors, P and Q, we define an
    optimization function, shown in the preceding formula, which minimizes the regularized
    squared error between the original ratings matrix and the resultant after the
    product of the latent matrices. The latter part of the preceding equation is the
    regularization imposed to avoid over-fitting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，当我们乘回两个潜在因子矩阵时，我们得到近似的原始矩阵。现在，为了提高模型的准确性，即学习最优的因子向量P和Q，我们定义了一个优化函数，如前一个公式所示，该函数最小化了原始评分矩阵与潜在矩阵乘积后的结果之间的正则化平方误差。前一个方程的后半部分是施加的正则化，以避免过拟合。
- en: '**Alternating Least Squares** is the optimization technique to minimize the
    aforementioned loss function. In general, we use stochastic gradient descent to
    optimize the loss function. For the Spark recommendation module, the ALS technique
    has been used for minimizing the loss function.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**交替最小二乘法**是用于最小化上述损失函数的优化技术。通常，我们使用随机梯度下降来优化损失函数。对于Spark推荐模块，ALS技术已被用于最小化损失函数。'
- en: In the ALS method, we calculate the optimal latent factor vectors alternatively
    by fixing one of the two factor vectors, that is, we calculate the user latent
    vector by fixing the item-latent feature vector as constant and vice versa.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在ALS方法中，我们通过交替固定两个因子向量中的一个来计算最优的潜在因子向量，即通过固定物品-潜在特征向量作为常数来计算用户潜在向量，反之亦然。
- en: 'The main benefits of the ALS approach are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ALS方法的主要优点如下：
- en: This approach can be easily parallelized
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此方法可以轻松并行化
- en: In most cases, we deal with sparse datasets in recommendation engine problems,
    and ALS is more efficient in handling sparsity compared to the stochastic gradient
    descent
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们在推荐引擎问题中处理稀疏数据集，与随机梯度下降相比，ALS在处理稀疏性方面更有效率。
- en: 'The Spark implementation of the recommendation engine module in spark.ml has
    the following parameters:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: spark.ml中推荐引擎模块的Spark实现具有以下参数：
- en: '**numBlocks**: This is the number of blocks the users and items will be partitioned
    into in order to parallelize computation (defaults to 10)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**numBlocks**: 这是指用户和物品将被划分成多少个块以并行化计算（默认为10）'
- en: '**rank**: This refers to the number of latent factors in the model (defaults
    to 10)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**rank**: 这指的是模型中的潜在因子数量（默认为10）'
- en: '**maxIter**: This is the maximum number of iterations to run (defaults to 10)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxIter**: 这是运行的最大迭代次数（默认为10）'
- en: '**regParam**: This parameter specifies the regularization parameter in ALS
    (defaults to 0.1)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**regParam**: 此参数指定ALS中的正则化参数（默认为0.1）'
- en: '**implicitPrefs**: This parameter specifies whether to use the explicit feedback
    ALS variant or the one adapted for implicit feedback data (defaults to false,
    which means it''s using explicit feedback)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**implicitPrefs**: 此参数指定是否使用显式反馈的ALS变体或适用于隐式反馈数据的变体（默认为false，表示使用显式反馈）'
- en: '**alpha**: This is a parameter applicable to the implicit feedback variant
    of ALS, which governs the baseline confidence in preference observations (defaults
    to 1.0)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**alpha**: 这是适用于ALS隐式反馈变体的参数，它控制偏好观察的基线置信度（默认为1.0）'
- en: '**nonnegative**: This parameter specifies whether or not to use non-negative
    constraints for least squares (defaults to false)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nonnegative**: 此参数指定是否为最小二乘法使用非负约束（默认为false）'
- en: Model based recommender system using pyspark
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于pyspark的模型推荐系统
- en: 'Software details for the use case are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 用例的软件细节如下：
- en: Spark 2.0
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0
- en: 'Python API: pyspark'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Python API: pyspark'
- en: Centos 6
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Centos 6
- en: Python 3.4
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.4
- en: 'Start the Spark session using pyspark, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pyspark启动Spark会话，如下所示：
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following screenshot shows the Spark session created by running the above
    `pyspark` command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了运行上述`pyspark`命令创建的Spark会话：
- en: '![Model based recommender system using pyspark](img/image00410.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![基于pyspark的模型推荐系统](img/image00410.jpeg)'
- en: To build the recommendation engine using Spark, we make use of Spark 2.0 capabilities,
    such as DataFrames, RDD, Pipelines, and Transforms available in Spark MLlib, which
    has was explained earlier.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Spark构建推荐引擎，我们利用Spark 2.0的能力，如DataFrame、RDD、Pipelines和Transforms，这些在Spark
    MLlib中已有解释。
- en: Unlike earlier heurist approaches, such as k-nearest neighboring approaches
    used for building recommendation engines, in Spark, matrix factorization methods
    are used for building recommendation engines and the Alternating Least Squares
    (ALS) method is used for generating model-based collaborative filtering.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期启发式方法不同，例如用于构建推荐引擎的k-最近邻方法，在Spark中，使用矩阵分解方法构建推荐引擎，交替最小二乘（ALS）方法用于生成基于模型的协同过滤。
- en: MLlib recommendation engine module
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLlib推荐引擎模块
- en: In this section, let's learn about the different methods present in the MLlib
    recommendation engine module. The current recommendation engine module helps us
    build the model-based collaborative filtering approach using the Alternating Least
    Squares matrix factorization model to generate recommendations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解MLlib推荐引擎模块中存在的不同方法。当前的推荐引擎模块帮助我们使用交替最小二乘矩阵分解模型构建基于模型的协同过滤方法来生成推荐。
- en: 'The main methods available for building collaborative filtering are as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 构建协同过滤的主要方法如下：
- en: '`ALS()`: The `ALS()` constructor is invoked and its instance is created with
    all the required parameters, such as user column name, item column name, rating
    column name, rank, regularization parameter (regParam), maximum iterations (maxIter),
    and so on supplied.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALS()`: 调用 `ALS()` 构造函数并使用所有必需的参数创建其实例，例如用户列名、项目列名、评分列名、排名、正则化参数（regParam）、最大迭代次数（maxIter）等。'
- en: '`fit()`: The `fit()` method is used to generate the model. This method takes
    the following parameters:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit()`: 使用 `fit()` 方法生成模型。此方法接受以下参数：'
- en: '`dataset`: input dataset of type `pyspark.sql.DataFrame`([http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame))'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataset`: 输入数据集，类型为 `pyspark.sql.DataFrame`([http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame))'
- en: '`params`: this is an optional param map which contains required parameters
    listed above.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`: 这是一个可选的参数映射，其中包含上述列出的必需参数。'
- en: 'Returns: The `fit()` method returns fitted models.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回：`fit()` 方法返回拟合模型。
- en: '`Transform()`: The `transform()` method is used to generate the predictions.
    The `transform()` method takes in the following:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transform()`: 使用 `transform()` 方法生成预测。`transform()` 方法接受以下内容：'
- en: Test data (DataFrame datatype)
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据（DataFrame数据类型）
- en: Optional additional parameters that embed previously defined parameters.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选的附加参数，其中包含先前定义的参数。
- en: Returns a predictions (DataFrame object)
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回预测（DataFrame对象）
- en: The recommendation engine approach
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐引擎方法
- en: 'Now let''s get into the actual implementation of the recommendation engine.
    We use the following approach to build the recommendation engine using Spark:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实际实现推荐引擎。我们使用以下方法在Spark中构建推荐引擎：
- en: Start the Spark environment.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark环境。
- en: Load the data.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Explore the data source.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索数据源。
- en: Use the MLlib recommendation engine module to generate the recommendations using
    ALS instance.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MLlib推荐引擎模块和ALS实例生成推荐。
- en: Generate the recommendations.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成推荐。
- en: Evaluate the model.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估模型。
- en: Using the cross_validation approach, apply the parameter tuning model to tune
    the parameter and select the best model, and then generate recommendations.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证方法，应用参数调整模型调整参数并选择最佳模型，然后生成推荐。
- en: Implementation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: Like any other recommendation engine, the first step is to load the data into
    the analytics environment (into the Spark environment in our case). When we start
    the Spark environment in the 2.0 version, SparkContext and SparkSession will be
    created at the load time.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他推荐引擎一样，第一步是将数据加载到分析环境中（在我们的案例中是Spark环境）。当我们启动2.0版本的Spark环境时，SparkContext和SparkSession将在加载时创建。
- en: Before we get into the implementation part, let's review the data for a while.
    In this chapter, we use the MovieLens 100K Dataset to build collaborative filtering
    recommendation engines, both user-based and item-based. The dataset contains 943
    user ratings on 1,682 movies. The ratings are on a scale of 1-5.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入实现部分之前，让我们回顾一下数据。在本章中，我们使用 MovieLens 100K 数据集来构建基于用户和基于物品的协同过滤推荐引擎。该数据集包含
    943 个用户对 1,682 部电影的评分。评分在 1-5 的范围内。
- en: As a first step, we shall make use of SparkContext (sc) to load the data into the
    Spark environment.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将使用 SparkContext (sc) 将数据加载到 Spark 环境中。
- en: Data loading
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据加载
- en: 'To load the data, run the below command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据，请运行以下命令：
- en: '[PRE7]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Loaded data will be a spark RDD type-run the below command to find out the
    data type of the data object:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的数据将是 spark RDD 类型 - 运行以下命令以找出数据对象的数据类型：
- en: '[PRE8]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Total length of the data loaded is given by:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据的总长度如下所示：
- en: '[PRE9]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To load the first record in the loaded data:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载已加载数据中的第一条记录：
- en: '[PRE10]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that the header information is located as the first row in the data
    object separated by `\t`; the column names of the data object are `UserID`, `ItemId`,
    `Rating`, and `Timestamp`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，标题信息位于数据对象的第一行，由 `\t` 分隔；数据对象的列名为 `UserID`、`ItemId`、`Rating` 和 `Timestamp`。
- en: 'For our purposes, we don''t require Timestamp information, so we can remove
    this field from the data RDD:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们不需要时间戳信息，因此我们可以从数据 RDD 中删除此字段：
- en: 'To check the first 5 rows of the data RDD, we use take() action method:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查数据 RDD 的前 5 行，我们使用 take() 动作方法：
- en: '[PRE11]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The MLlib recommendation engine module expects the data to be without any header
    information. So let''s remove the header information, that is, remove the first
    line from the data RDD object, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 推荐引擎模块期望数据不包含任何标题信息。因此，让我们删除标题信息，即从数据 RDD 对象中删除第一行，如下所示：
- en: 'Extract the first row from the data RDD object:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据 RDD 对象中提取第一行：
- en: '[PRE12]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use `filter()` method and lambda expression to remove the first header row
    from the data. The lambda expression below is applied for each row and each row
    is compared with the header to check if the extracted row is the header or not.
    If the extracted row is found to be the header then that row is filtered:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `filter()` 方法和 lambda 表达式从数据中删除第一行标题。下面的 lambda 表达式应用于每一行，并且每一行都与标题进行比较，以检查提取的行是否为标题。如果发现提取的行是标题，则该行将被过滤：
- en: '[PRE13]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let us check the count of the data RDD object; it has reduced from 100001
    to 100000:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查数据 RDD 对象的计数；它已从 100001 减少到 100000：
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let us check the first row, we can observe that the header has been successfully
    removed:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查第一行，我们可以观察到标题已经成功删除：
- en: '[PRE15]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we have loaded the data into the Spark environment, let''s format
    the data into a proper shape, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据加载到 Spark 环境中，让我们将数据格式化为适当的形状，如下所示：
- en: Load the required functions for building the recommendation engine, such as
    ALS, the Matrix Factorization Model, and the Rating function from the MLlib recommendation
    module.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载用于构建推荐引擎所需的函数，例如 ALS、矩阵分解模型和 Rating 函数，来自 MLlib 推荐模块。
- en: Extract each row from the data RDD and split by `\t` to separate each column
    using the `map()` and lambda expressions.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据 RDD 中提取每一行，并使用 `map()` 和 lambda 表达式通过 `\t` 分隔每个列。
- en: In the resultant set, let's create a Rating row object for each of the lines
    extracted in the previous step
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在结果集中，让我们为之前步骤中提取的每一行创建一个 Rating 行对象
- en: When the following expression is applied on the entire dataset, a pipelined
    RDD object is created:![Data loading](img/image00411.jpeg)
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当以下表达式应用于整个数据集时，将创建一个管道化的 RDD 对象：![数据加载](img/image00411.jpeg)
- en: 'Check the data type of ratings object using type:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 type 检查 ratings 对象的数据类型：
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Check the first 5 records of the ratings PipelinedRDD object by running the
    following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下代码检查 ratings PipelinedRDD 对象的前 5 条记录：
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can observe from the preceding result that each row in the original raw data
    RDD object turns into a kind of list of Rating row objects stacked into PipelinedRDD.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从前面的结果中观察到，原始原始数据 RDD 对象中的每一行都变成了 Rating 行对象的一种列表，堆叠成 PipelinedRDD。
- en: Data exploration
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'Now that we have loaded the data, let''s spend some time exploring the data.
    Let''s use the Spark 2.0 DataFrame API capabilities to explore the data:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载数据，让我们花些时间探索数据。让我们使用 Spark 2.0 DataFrame API 功能来探索数据：
- en: 'Compute the total number of unique users by first selecting the `''user''`
    column and then using `distinct()` function to remove the duplicate `userId`:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先选择`'user'`列，然后使用`distinct()`函数来移除重复的`userId`来计算唯一用户的总数：
- en: '[PRE18]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following screenshot shows the results of the previous query:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了上一个查询的结果：
- en: '![Data exploration](img/image00412.jpeg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00412.jpeg)'
- en: 'Total number of unique users:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一用户的总数：
- en: '[PRE19]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Total number of unique items:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一物品的总数：
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Display first 5 unique products:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 显示前5个唯一产品：
- en: '[PRE21]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following screenshot shows the results of the previous query:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了上一个查询的结果：
- en: '![Data exploration](img/image00413.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00413.jpeg)'
- en: 'Number of rated products by each user:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户评价的产品数量：
- en: '[PRE22]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The previous results explain that User 26 has rated 107 movies and user 29 has
    rated 34 movies.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的结果解释说用户26评价了107部电影，用户29评价了34部电影。
- en: 'Number of records for each rating type:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每个评分类型的记录数：
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following screenshot shows the results of the previous query:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了之前查询的结果：
- en: '![Data exploration](img/image00414.jpeg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00414.jpeg)'
- en: 'In the following code, we make use of the `numpy` scientific computing package
    in Python, used for working with arrays: `matplotlibe` - a visualizing package
    in Python:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用了Python中的`numpy`科学计算包，用于处理数组：`matplotlibe` - Python中的可视化包：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Data exploration](img/image00415.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00415.jpeg)'
- en: 'Statistics of ratings per user:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户的评分统计：
- en: '[PRE25]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Data exploration](img/image00416.jpeg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00416.jpeg)'
- en: 'Individual counts of ratings per user:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户的评分计数：
- en: '[PRE26]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Data exploration](img/image00417.jpeg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![数据探索](img/image00417.jpeg)'
- en: 'Average rating given by each user:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 每个用户给出的平均评分：
- en: '[PRE27]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Average rating per movie:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 每部电影的平均评分：
- en: '[PRE28]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Building the basic recommendation engine
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建基本的推荐引擎
- en: 'Divide the original data into training and test datasets randomly as follows,
    using the `randomSplit()` method:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`randomSplit()`方法如下随机地将原始数据分为训练集和测试集：
- en: (training, test) = ratings.randomSplit([0.8, 0.2])
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: (`training`, `test`) = `ratings.randomSplit([0.8, 0.2])`
- en: 'Counting the number of instances in the training dataset:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 计算训练数据集中的实例数量：
- en: '[PRE29]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Counting the number of instances in the test set:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试集中的实例数量：
- en: '[PRE30]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let's now build a recommendation engine model using the ALS algorithm available
    in the MLlib library of Spark.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用Spark MLlib库中可用的ALS算法构建一个推荐引擎模型。
- en: 'For this, we use the following methods and parameters:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用以下方法和参数：
- en: Load the ALS module into the Spark environment.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ALS模块加载到Spark环境中。
- en: Call the `ALS.train()` method to train the model.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`ALS.train()`方法来训练模型。
- en: Pass the required parameters, such as rank, number of iterations (maxIter),
    and training data to the `ALS.train()` method.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所需的参数，如排名、迭代次数（maxIter）和训练数据传递给`ALS.train()`方法。
- en: 'Let''s understand the parameters now:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这些参数：
- en: '**Rank**: This parameter is the number of latent factors of users and items
    to be used in the model. The default is 10.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rank**：此参数是模型中要使用的用户和物品的潜在因子数量。默认值为10。'
- en: '**maxIter**: This is the number of iterations the model has to run. The default
    is 10.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**maxIter**：这是模型必须运行的迭代次数。默认值为10。'
- en: 'Build the recommendation model using Alternating Least Squares:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交替最小二乘法构建推荐模型：
- en: 'Setting rank and maxIter parameters:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 设置rank和maxIter参数：
- en: '[PRE31]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Calling `train()` method with training data, rank, maxIter params, `model =
    ALS.train`(training, rank, numIterations):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据、排名、maxIter参数调用`train()`方法，`model = ALS.train`(training, rank, numIterations)：
- en: '[PRE32]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Checking the model as below, we observe that `Matrixfactorizationmodel` object
    is created:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如下检查模型，我们观察到创建了`Matrixfactorizationmodel`对象：
- en: '[PRE33]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Making predictions
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进行预测
- en: Now that we have created the model, let's predict the values of ratings on the
    test set we created earlier.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了模型，让我们预测我们之前创建的测试集中的评分值。
- en: The ALS module has provided many methods discussed in the following sections
    for making predictions, recommending users, and recommending items to users, user
    features, item features, and so on. Let's run the methods one by one.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ALS模块在以下章节中提供了许多方法，用于进行预测、推荐用户、向用户推荐项目、用户特征、项目特征等。让我们逐一运行这些方法。
- en: 'Before we proceed to predictions, we shall first create test data in a way
    that is acceptable to the prediction methods, as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行预测之前，我们首先以预测方法可接受的方式创建测试数据，如下所示：
- en: 'The following code extracts each row in the test data and extracts `userID,
    ItemID` and puts it in testdata `PipelinedRDD` object:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码提取测试数据中的每一行，并提取`userID, ItemID`，然后将其放入`testdata PipelinedRDD`对象中：
- en: '[PRE34]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following code shows the original test data sample:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了原始测试数据样本：
- en: '[PRE35]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following code displays the formatted data required for making predictions:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了用于预测的格式化数据：
- en: '[PRE36]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The prediction methods are as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 预测方法如下：
- en: '`predict()`: The predict method will the predict rating for a given user and
    item and is given as follows:'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict()`: 预测方法将为给定用户和项目的评分进行预测，如下所示：'
- en: 'This method is used when we want to make predictions for a combination of user
    and item:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要对用户和物品的组合进行预测时，使用此方法：
- en: '[PRE37]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can observe below that the prediction value for a user `119` and movie `392`
    is `4.3926091845289275`: just see above the original value for the same combination
    in test data:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，用户`119`和电影`392`的预测值为`4.3926091845289275`：只需查看测试数据中相同组合的原始值：
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`predictall()`: This method is used when we want to predict values for all
    the test data in one go, given as follows:'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictall()`: 当我们希望一次性预测所有测试数据的值时，使用此方法，如下所示：'
- en: '[PRE39]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Use the following code to check the data type:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码检查数据类型：
- en: '[PRE40]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Use the following code displays the first five predictions:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码显示前五个预测：
- en: '[PRE41]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: User-based collaborative filtering
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于用户的协同过滤
- en: Now let's recommend items (movies) to users. The ALS recommendation module contains
    the `recommendProductsForUsers()` method to generate the top-N item recommendations
    for users.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为用户推荐物品（电影）。ALS推荐模块包含`recommendProductsForUsers()`方法，用于为用户生成top-N物品推荐。
- en: 'The `recommendProductsForUsers()` method takes integers as the input parameter,
    which indicates the top-N recommendations; for example, to generate the top 10
    recommendations to the users, we pass 10 as value to the `recommendProductsForUsers()`
    method, as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '`recommendProductsForUsers()`方法接受整数作为输入参数，该参数表示前N个推荐；例如，要生成对用户的top 10推荐，我们将10作为值传递给`recommendProductsForUsers()`方法，如下所示：'
- en: '[PRE42]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use the following code shows that recommendations are generated for all the
    943 users:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码可以展示为所有943个用户生成推荐：
- en: '[PRE43]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let us see the recommendations for the first two users: 96 and 784:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前两个用户（96和784）的推荐：
- en: '[PRE44]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Model evaluation
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Now let''s evaluate the model accuracy. For this, we choose the Root Mean Squared
    Error method to calculate the model accuracy. We can do it either manually, as
    shown next, or call a defined function available in the Spark MLlib: Create a
    `ratesAndPreds` object by joining the original ratings and predictions:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型精度。为此，我们选择均方根误差方法来计算模型精度。我们可以手动完成，如下所示，或者调用Spark MLlib中可用的定义好的函数：通过连接原始评分和预测来创建`ratesAndPreds`对象：
- en: '[PRE45]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following code will calculate the mean squared error:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将计算均方误差：
- en: '[PRE46]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Model selection and hyperparameter tuning
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型选择和超参数调整
- en: The most important step in any machine learning task is to use model evaluation
    or model selection to find the optimal parameters that the fit the data. Spark
    provides infrastructure to tune and model evaluation, for individual algorithms
    or for the entire model building pipeline. Users may tune the entire pipeline
    model or tune individual components of the pipeline. MLlib provides model selection
    tools such as `CrossValidator` class and `TrainValidationSplit` class.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习任务中最重要的步骤是使用模型评估或模型选择来找到最适合数据的最佳参数。Spark提供了调整和模型评估的基础设施，用于单个算法或整个模型构建管道。用户可以调整整个管道模型或调整管道的各个组件。MLlib提供了模型选择工具，如`CrossValidator`类和`TrainValidationSplit`类。
- en: 'The above mentioned classes require the following items:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的类需要以下项目：
- en: '**Estimator** algorithm or Pipeline to tune'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**估算器**算法或管道进行调整'
- en: '**Set of ParamMaps**: parameters to choose from, sometimes called a *parameter
    grid* to search over'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ParamMaps集合**：可供选择的参数，有时称为搜索的*参数网格*'
- en: '**Evaluator**: metric to measure how well a fitted model does on held-out test
    data'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估器**：衡量拟合模型在保留的测试数据上的表现好坏的指标'
- en: 'At a high level, these model selection tools work as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，这些模型选择工具的工作方式如下：
- en: They split the input data into separate training and test datasets
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们将输入数据分割成独立的训练和测试数据集
- en: For each (training and test) pair, they iterate through the set of `ParamMaps`
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个（训练和测试）对，他们遍历`ParamMaps`集合
- en: For each `ParamMap`, they fit the Estimator using those parameters, get the
    fitted Model, and evaluate the Model's performance using the Evaluator
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个 `ParamMap`，他们使用这些参数拟合 Estimator，获取拟合的模型，并使用 Evaluator 评估模型性能。
- en: They select the Model produced by the best-performing set of parameters
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们选择由表现最佳的一组参数产生的模型
- en: The MLlib supports various evaluation classes for performing evaluation tasks,
    such as the `RegressionEvaluator` class for regression based problems, the `BinaryClassificationEvaluator`
    class for binary classification problems, and the `MulticlassClassificationEvaluator`
    class for multiclass classification problems. For constructing a parameter grid,
    we can use the `paramGridBuilder` class.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 支持各种用于执行评估任务的评估类，例如用于基于回归问题的 `RegressionEvaluator` 类，用于二元分类问题的 `BinaryClassificationEvaluator`
    类，以及用于多类分类问题的 `MulticlassClassificationEvaluator` 类。为了构建参数网格，我们可以使用 `paramGridBuilder`
    类。
- en: Cross-Validation
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'The **Cross-Validation** approach is one of the most popular approaches in
    evaluating the datamining models and in choosing optimal parameters for building
    the best estimation model. MLlib offers two types of evaluation classes: the `CrossValidator`
    and `TrainValidationSplit` classes.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证** 方法是在评估数据挖掘模型和选择构建最佳估计模型的最佳参数时最流行的方法之一。MLlib 提供两种类型的评估类：`CrossValidator`
    和 `TrainValidationSplit` 类。'
- en: CrossValidator
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉验证
- en: The `CrossValidator` class takes the input dataset and splits it into multiple
    dataset folds, which can be used as training and test sets. Using these datasets,
    the `CrossValidator` class builds multiple models and finds optimal parameters
    and stores in `ParamMap`. After identifying the best `ParamMap`, the `CrossValidator`
    class finally computes the best model using the entire dataset. For example, let's
    say we choose the five-fold cross-validation; the `CrossValidator` class splits
    the original dataset into five sub-datasets with each sub-dataset containing training
    and test sets. The `CrossValidator` class chooses each fold set at a time and
    estimates the model parameters. Finally, `CrossValidator` computes the average
    of the evaluation metric to store the best parameters in `ParamMap`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`CrossValidator` 类接受输入数据集并将其分割成多个数据集折，这些折可以用作训练集和测试集。使用这些数据集，`CrossValidator`
    类构建多个模型，找到最佳参数并存储在 `ParamMap` 中。在确定最佳 `ParamMap` 之后，`CrossValidator` 类最终使用整个数据集计算最佳模型。例如，假设我们选择了五折交叉验证；`CrossValidator`
    类将原始数据集分割成五个子数据集，每个子数据集包含训练集和测试集。`CrossValidator` 类一次选择一个折集并估计模型参数。最后，`CrossValidator`
    计算评估指标的均值，将最佳参数存储在 `ParamMap` 中。'
- en: Train-Validation Split
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练-验证分割
- en: Spark MLlib provides another class for estimating the optimal parameters using
    `TrainValidationSplit`. Unlike `CrossValidator`, this class estimates the optimal
    parameters on a single dataset. For example, the `TrainValidatorSplit` class divides
    the input data into trainset and test sets of size 3/4 and 1/4, and optimal parameters
    are chosen using these sets.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 提供了另一个类 `TrainValidationSplit` 来使用 `TrainValidationSplit` 估计最佳参数。与
    `CrossValidator` 不同，这个类在单个数据集上估计最佳参数。例如，`TrainValidatorSplit` 类将输入数据分成大小为 3/4
    的训练集和 1/4 的测试集，并使用这些集选择最佳参数。
- en: Now, let's understand the recommendation engine model we built earlier.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解我们之前构建的推荐引擎模型。
- en: The tuning model is present in the MLlib of Spark 2.0 and makes use of DataFrame
    API features. So in order to accommodate this, our first step is to convert the
    original dataset ratings to DataFrame.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 调优模型存在于 Spark 2.0 的 MLlib 中，并使用 DataFrame API 功能。因此，为了适应这一点，我们的第一步是将原始数据集评分转换为
    DataFrame。
- en: 'For conversion, we use the `sqlContext` object and the `createDataFrame()`
    method to convert the ratings RDD object to a `DataFrame` object, as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 对于转换，我们使用 `sqlContext` 对象和 `createDataFrame()` 方法将评分 RDD 对象转换为 `DataFrame` 对象，如下所示：
- en: '[PRE47]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'SQL Context object is created when starting the spark session using pyspark:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pyspark 启动 spark 会话时创建 SQL Context 对象：
- en: '[PRE48]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Creating a DataFrame object from the ratings rdd object as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如下从评分 RDD 对象创建 DataFrame 对象：
- en: '[PRE49]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Display first 20 records of `dataframe` object:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 显示 `dataframe` 对象的前 20 条记录：
- en: '[PRE50]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following screenshot shows the results of the previous query:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了先前查询的结果：
- en: '![Train-Validation Split](img/image00418.jpeg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![训练-验证分割](img/image00418.jpeg)'
- en: 'Creating random samples of training set and test set using `randomSplit()`
    method:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `randomSplit()` 方法创建训练集和测试集的随机样本：
- en: '[PRE51]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Load modules required for running parameter `tuningmodel`:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 加载运行参数 `tuningmodel` 所需的模块：
- en: '[PRE52]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Call the ALS method available in MLlib to building the recommendation engine.
    The following method `ALS()` takes only column values of training data, such as
    UserID, ItemId, and Rating. The other parameters, such as rank, number of iterations,
    learning parameters, and so on will be passed as the `ParamGridBuilder` object
    to the cross-validation method.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 调用MLlib中可用的ALS方法来构建推荐引擎。以下`ALS()`方法仅接受训练数据的列值，如UserID、ItemId和Rating。其他参数，如rank、迭代次数、学习参数等，将作为`ParamGridBuilder`对象传递给交叉验证方法。
- en: 'As mentioned earlier, the model tuning pipeline require Estimators, a set of
    ParamMaps, and Evaluators. Let''s create each one of them, as follows:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型调优管道需要Estimators、一组ParamMaps和Evaluators。让我们按照以下方式创建每一个：
- en: 'Estimator objects as stated earlier, estimators take algorithm or pipeline
    objects as input. Let us build one pipeline object as follows:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Estimator对象接受算法或管道对象作为输入。让我们按照以下方式构建一个管道对象：
- en: 'Calling ALS algorithm:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 调用ALS算法：
- en: '[PRE53]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Checking the type of `als` object:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 检查`als`对象的类型：
- en: '[PRE54]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let us see how the default parameters are set for the ALS model:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看ALS模型默认参数是如何设置的：
- en: '[PRE55]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'From the preceding result, we observe that the model is set to its default
    values for rank as 10, maxIter as 10, and blocksize as 10:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果中，我们观察到模型将秩设置为默认值10，最大迭代次数maxIter为10，块大小blocksize也为10：
- en: 'Create pipeline object and setting the created als model as a stage in the
    pipeline:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管道对象并将创建的als模型作为管道中的一个阶段：
- en: '[PRE56]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Setting the ParamMaps/parameters
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置ParamMaps/参数
- en: 'Let''s observe the `ALS()` method closely and logically and infer the parameters
    that can be used for parameter tuning:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细和逻辑地观察`ALS()`方法，并推断出可用于参数调整的参数：
- en: '**rank:** We know that rank is the number of latent features for users and
    items, which, by default, is 10, but if we do not have the optimal number of latent
    features for a given dataset, this parameter can be taken up for tuning the model
    by giving a range of values between 8 and 12 - the choice is left to the users.
    Due to the computational cost, we restrict the values to 8 -12, but readers are
    free to try other values.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '**rank:** 我们知道rank是用户和物品的潜在特征数量，默认值为10，但如果我们没有给定数据集的最佳潜在特征数量，则可以通过在8和12之间给出一个值范围来调整模型，选择权留给用户。由于计算成本，我们将值限制在8-12之间，但读者可以自由尝试其他值。'
- en: '**MaxIter:** MaxIter is the number of times the model is made to run; it''s
    set to a default 10\. We can select this parameter also for tuning as we do not
    know the optimal iterations at which the model performs well; we select between
    10 and 15.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**MaxIter:** MaxIter是模型运行次数；默认设置为10。我们可以选择这个参数进行调优，因为我们不知道模型表现良好的最佳迭代次数；我们在10和15之间进行选择。'
- en: '**reqParams:** regParams is the learning parameter set between 1 and 10.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**reqParams:** regParams是介于1和10之间的学习参数集。'
- en: 'Loading `CrossValidation` and `ParamGridBuilder` modules to create range of
    parameters:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`CrossValidation`和`ParamGridBuilder`模块以创建参数范围：
- en: '[PRE57]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Setting the evaluator object
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置评估器对象
- en: 'As stated earlier, the evaluator object sets the evaluation metric to evaluate
    the model during multiple runs in the cross validation method:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，评估器对象在交叉验证方法中设置评估指标以在多次运行中评估模型：
- en: 'Loading the `RegressionEvaluator` model:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`RegressionEvaluator`模型：
- en: '[PRE58]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Calling `RegressionEvaluator()` method with evaluation metric set to `rmse`
    and `evaluation` column set to Rating:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`RegressionEvaluator()`方法调用，将评估指标设置为`rmse`并将`evaluation`列设置为Rating：
- en: '[PRE59]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now that we have prepared all the required objects for running the cross-validation
    method, that is, `Estimator`, `paramMaps`, and `Evaluator`, let's run the model.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为运行交叉验证方法准备了所有必需的对象，即`Estimator`、`paramMaps`和`Evaluator`，让我们运行模型。
- en: 'The cross-validation method gives us the best optimal model out of all the
    executed models:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证方法从所有执行过的模型中给出最佳最优模型：
- en: '[PRE60]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Running the model using `fit()` method:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`fit()`方法运行模型：
- en: '[PRE61]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Summary
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about model-based collaborative filtering using
    matrix factorization methods using ALS. We used the Python API to access the Spark
    framework and ran the ALS collaborative filtering. In the beginning of the chapter,
    we refreshed our knowledge of Spark with all the basics that are required to run
    the recommendation engines, such as what Spark is, the Spark ecosystem, components
    of Spark, SparkSession, DataFrames, RDD, and so on. As explained then, we explored
    the MovieLens data, built a basic recommendation engine, evaluated the model,
    and used parameter tuning to improve the model. In the next chapter, we shall
    learn about building recommendations using Graph database - Neo4j.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了使用 ALS 算法的矩阵分解方法进行基于模型的协同过滤。我们使用 Python API 访问 Spark 框架并运行了 ALS 协同过滤。在本章的开头，我们回顾了
    Spark 的基础知识，这些基础知识是运行推荐引擎所必需的，例如 Spark 是什么，Spark 生态系统，Spark 的组件，SparkSession，DataFrames，RDD
    等等。正如当时所解释的，我们探索了 MovieLens 数据，构建了一个基本的推荐引擎，评估了模型，并使用参数调整来改进模型。在下一章中，我们将学习如何使用图数据库
    Neo4j 构建推荐。
