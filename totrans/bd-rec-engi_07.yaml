- en: Chapter 7. Building Real-Time Recommendation Engines with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this day and age, the need to build scalable real-time recommendations is
    increasing day by day. With more internet users using e-commerce sites for their
    purchases, these e-commerce sites have realized the potential of understanding
    the patterns of the users' purchase behavior to improve their business, and to
    serve their customers on a very personalized level. To build a system which caters
    to a huge user base and generates recommendations in real time, we need a modern,
    fast scalable system. **Apache Spark**, which is a special framework designed
    for distributed in-memory data processing, comes to our rescue. Spark applies
    a set of transformations and actions to distributed data to build real-time data
    mining applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we learned about implementing similarity-based collaborative
    filtering approaches, such as user-based collaborative filtering and content-based
    collaborative filtering. Though the similarity-based approaches are a huge success
    in commercial applications, there came into existence model-based recommender
    models, such as matrix factorization models, which have improved the performance
    of recommendation engine models. In this chapter, we will learn about the model-based
    approach of collaborative filtering, moving away from the heuristic-based similarity
    approaches. Also, we will focus on implementing the model-based collaborative
    filtering approach using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the pyspark environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Spark concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MLlib recommendation engine module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Alternating Least Squares algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data exploration of the Movielens-100k dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building model-based recommendation engines using ALS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the recommendation engine model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Spark** is a fast, powerful, easy-to-use, distributed, in-memory,
    and open source cluster computing framework built to perform advanced analytics.
    It was originally developed at UC Berkeley in 2009\. Spark has been widely adopted
    by enterprises across a wide range of industries since its inception.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of Spark is that it takes all the complexities away
    from us, such as resources scheduling, job submissions, executions, tracking,
    between-node communication, fault tolerance, and all low-level operations that
    are inherent features of parallel processing. The Spark framework helps us write
    programs to run on the clusters in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Spark can be run both as a standalone mode and as a cluster mode. Spark can
    be easily integrated with Hadoop platforms.
  prefs: []
  type: TYPE_NORMAL
- en: As a general-purpose computing engine, Spark with its in-memory data processing
    capability and easy-to-use APIs allows us to efficiently work on a wide range
    of large-scale data processing tasks, such as streaming applications, machine
    learning, or interactive SQL queries over large datasets that require iterative
    access.
  prefs: []
  type: TYPE_NORMAL
- en: Spark can be easily integrated with many applications, data sources, storage
    platforms, and environments, and exposes high-level APIs in Java, Python, and
    R to work with. Spark has proved to be broadly useful for a wide range of large-scale
    data processing tasks, over and above machine learning and iterative analytics.
  prefs: []
  type: TYPE_NORMAL
- en: '![About Spark 2.0](img/image00398.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Credits: Databricks'
  prefs: []
  type: TYPE_NORMAL
- en: Spark architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Apache Spark ecosystem contains many components to work with distributed,
    in-memory, and machine-learning data processing tools. The main components of
    Spark are discussed in the following sub-sections. Spark works on a master-slave
    architecture; a high-level architecture is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark architecture](img/image00399.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Credits: Databricks'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark cluster works on the master-slave architecture. The Spark Core execution
    engine accepts requests from clients and passes them to the master node. The driver
    program in the master communicates with the worker node executors to get the work
    done as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark architecture](img/image00400.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '**Spark driver program**: The driver program acts as a master node in a Spark
    cluster, which hosts the SparkContext for Spark applications. It receives the
    client request and co-ordinates with the cluster manager which manages the worker
    nodes. The driver program splits the original request into tasks and schedules
    them to run on executors in worker nodes. All the processes in Spark are Java
    processes. The SparkContext creates **Resilient Distributed Datasets** (**RDD**),
    an immutable, distributable collection of datasets partitioned across nodes, and
    performs a series of transformations and actions to compute the final output.
    We will learn more about RDDs in the latter sections.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Worker nodes**: A worker contains executors, where the actual task execution
    happens in the form of Java processes. Each worker runs its own Spark instance
    and is the main compute node in Spark. When a SparkContext is created, each worker
    node starts its own executors to receive the tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Executors**: These are the main task executioners of Spark applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will see the core components of the Spark ecosystem. The
    following diagram shows the Apache Spark Ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark components](img/image00401.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Credits:Databricks
  prefs: []
  type: TYPE_NORMAL
- en: Spark Core
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Spark Core** is the core part of the Spark platform: the execution engine.
    All other functionalities are built on top of Spark Core. This provides all the
    capabilities of Spark, such as in-memory distributed computation, and fast, easy-to-use
    APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured data with Spark SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Spark SQL** is a component on top of Spark Core. It is a spark module that
    provides support for structured and semi-structured data.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL provides a unified approach, to allow users to query the data objects
    in an interactive SQL type, such as applying select, where you can group data
    objects by the kind of operations through data abstraction APIs, such as DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: A considerable amount of time will be dedicated to data exploration, exploratory
    analysis, and SQL-like interactions. Spark SQL, which provides DataFrames, also
    acts as a distributed SQL query engine; for instance, in R, the DataFrames in
    Spark 2.0, the data is stored as rows and columns, access to which is allowed
    as an SQL table with all the structural information, such as data types.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming analytics with Spark Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Spark Streaming** is another Spark module that enables users to process and
    analyze both batch and streaming data in real time, to perform interactive and
    analytical applications. Spark Streaming provides **Discretized Stream** (**DStream**),
    a high-level abstraction, to represent a continuous stream of data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main features of Spark Streaming API are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault-tolerant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processes live stream of incoming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can connect to real-time data sources and process real-time incoming data on
    the go
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can apply complex machine learning and graph processing algorithms on streaming
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Streaming analytics with Spark Streaming](img/image00402.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Machine learning with MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**MLlib** is another module in Spark, built on top of Spark Core. This machine
    learning library is developed with an objective to make practical machine learning
    scalable and easy to use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This library provides tools for data scientists, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning algorithms for regression, classification, clustering, and
    recommendation engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction, feature transformation, dimensionality reduction, and feature
    selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline tools for streamlining machine learning processes for construction,
    evaluation, and tuning the over process of solving a machine learning problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistence of storing and loading machine learning models and pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilities, such as linear algebra and statistical tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When starting Spark 2.0, the old MLlib model is replaced with ML library, which
    is built with DataFrames APIs, providing more optimizations and making uniform
    APIs across all languages.
  prefs: []
  type: TYPE_NORMAL
- en: Graph computation with GraphX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**GraphX** is a new Spark API for building graph-based systems. It is a graph-parallel
    processing computation engine and distributed framework, which is built on top
    of Spark Core. This project was started with the objective of unifying the graph-parallel
    and data distribution framework into a single Spark API. GraphX enables users
    to process data both as RDDs and graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GraphX provides many features, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Property graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph-based algorithms, such as PageRank, connected components, and Graph Builders,
    which are used to build graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic graph computational components, such as subgraph, joinVertices, aggregateMessages,
    Pregel API, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though graph models are not within the scope of this book, we will learn some
    fundamentals of graph applications in [Chapter 8](part0057.xhtml#aid-1MBG21 "Chapter 8. 
    Building Real-Time Recommendations with Neo4j"), *Building Real-Time Recommendations
    with Neo4j*.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main advantages of Spark are, it is fast, has in-memory framework, contains
    several APIs, making it very easy to use, its Unified Engine for large quantities
    of data, and its machine learning components. Unlike Map-Reduce model with its
    batch mode, which is slower and contains lot of programming, Spark is faster,
    with real-time and easy to code framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the above mentioned benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Benefits of Spark](img/image00403.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark runs on both Windows and UNIX-like systems (for example, Linux, Mac OS).
    It's easy to run locally on one machine; all you need is to have Java installed
    on your system PATH or the `JAVA_HOME` environment variable pointing to a Java
    installation.
  prefs: []
  type: TYPE_NORMAL
- en: Spark runs on Java 7+, Python 2.6+/3.4+ and R 3.1+. For the Scala API, Spark
    2.0.0 uses Scala 2.11\. You will need to use a compatible Scala version (2.11.x).
  prefs: []
  type: TYPE_NORMAL
- en: 'Get Spark from the downloads page of the project website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz](http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz)'
  prefs: []
  type: TYPE_NORMAL
- en: Spark needs to be built against a specific version of Hadoop in order to access
    **Hadoop Distributed File System** (**HDFS**), as well as standard and custom
    Hadoop input sources.
  prefs: []
  type: TYPE_NORMAL
- en: Spark requires the Scala programming language (version 2.10.4 at the time of
    writing this book) in order to run. Fortunately, the prebuilt binary package comes
    with the Scala runtime packages included, so you don't need to install Scala separately
    in order to get started. However, you will need to have a **Java Runtime Environment**
    (**JRE**) or **Java Development Kit** (**JDK**) installed (take a look at the
    software and hardware list in this book's code bundle for installation instructions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the Spark binary package, unpack the contents of the
    package and change it into the newly created directory by running the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark places user scripts to run Spark in the bin directory. You can test whether
    everything is working correctly by running one of the example programs included
    in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run Spark interactively with Scala using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `--master` option specifies the master URL for a distributed cluster, or
    you can use `local` to run locally with one thread or `local[N]` to run locally
    with N threads. You should start by using `local` for testing. For a full list
    of options, run the Spark shell with the `--help` option.
  prefs: []
  type: TYPE_NORMAL
- en: Source:[http://spark.apache.org/docs/latest/](http://spark.apache.org/docs/latest/)
  prefs: []
  type: TYPE_NORMAL
- en: About SparkSession
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From Spark 2.0, the `SparkSession` will be the entry point for Spark applications.
    The `SparkSession` serves as the main interactive access point for underlying
    Spark functionalities and Spark programming capabilities, such as DataFrames API
    and Dataset API. We use `SparkSession` to create DataFrame objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the earlier versions of Spark, we used to create `SparkConf`, `SparkContext`,
    or `SQLContext` to interact with Spark, but since Spark 2.0, this has been taken
    care of by `SparkSession` by encapsulating `SparkConf`, `SparkContext` automatically.
  prefs: []
  type: TYPE_NORMAL
- en: When you start Spark in the shell command, `SparkSession` is created automatically
    as `spark`
  prefs: []
  type: TYPE_NORMAL
- en: 'We can programmatically create SparkSession, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Resilient Distributed Datasets (RDD)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of Spark is Resilient Distributed Datasets, in short, RDD. RDD is an
    immutable distributed collection of objects of some datatype of your data, partitioned
    across nodes on your cluster. This RDD is fault-tolerant, that is, a property
    of the system that is able to operate continuously, even in the event of failure
    by reconstructing the failed partition.
  prefs: []
  type: TYPE_NORMAL
- en: In short, we can say that RDD is a distributed dataset abstraction, which allows
    iterative operations on very large-scale cluster systems in a fault-tolerant way.
  prefs: []
  type: TYPE_NORMAL
- en: 'RDDs can be created in a number of ways, such as parallelizing an existing
    collection of data objects, or referencing an external file system, such as HDFS:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating RDD from an existing data object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating RDD from a referenced file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'RDD supports two types of operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformations**: This operation creates new RDDs from existing RDDs, which
    are immutable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: This operation returns values after performing computation on
    the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These RDD transformations are executed lazily only when the final results are
    required. We can recreate or recompute the RDDs any number of times, or we can
    persist them by caching them in memory if we know we may need them in the future.
  prefs: []
  type: TYPE_NORMAL
- en: About ML Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The ML Pipelines API in Spark 2.0 is the way to use a standard workflow when solving
    machine learning problems. Every machine learning problem will undergo a sequence
    of steps, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature extraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If we closely observe the aforementioned steps, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The ML process follows a series of steps as if it is a workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, we require more than one algorithm while solving a machine learning problem;
    for example, a text classification problem might require a feature extraction
    algorithm for feature extraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating predictions on test data may require many data transformations or
    data-preprocessing steps, which are used during model training. For example, in
    text classification problems, making predictions on test data involves data pre-processing
    steps, such as tokenization and feature extraction, before applying them to a
    generated classification model, which was used during model creation on training
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding steps form one of the main motivating factors behind introducing
    the ML Pipeline API. The ML Pipeline module allows users to define a sequence
    of stages, so that it's easy to use. The API framework allows the ML process to
    scale on a distributed platform and accommodate very large datasets, reuse some
    components, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The components of the ML Pipeline module are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**: As mentioned earlier, DataFrame is a way of representing the
    data in the Spark framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers**: Transformers take input DataFrames and transform data into
    new DataFrames. Transformation classes contain the `transform()` method to do
    the transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimators**: Estimators compute the final results. An Estimator class makes
    use of the `fit()` method to compute results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline**: This is a set of Transformers and Estimators stacked as a workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters**: This refers to the set of parameters that may be used by both
    Transformers and Estimators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll illustrate this for the simple text document workflow. The following
    figure is for the training time usage of a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![About ML Pipelines](img/image00404.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Find below the explanation for the steps shown in the preceding figure. The
    blue boxes are Transformers and the red box is the Estimator.
  prefs: []
  type: TYPE_NORMAL
- en: The Tokenizer Transformer takes the text column of a DataFrame as input and
    returns a new DataFrame column containing tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The HashingTF Transformer takes in the tokens DataFrame from the previous step
    as input and creates new DataFrame features as output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now the LogisticRegression Estimator takes in the features DataFrame, fits a
    logistic regression model, and creates a PipelineModel transformer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First we build a pipeline, which is an Estimator, then on this Pipeline we apply
    `fit()` method which produces a PipelineModel, a Transformer, that can be used
    on test data or at prediction time.
  prefs: []
  type: TYPE_NORMAL
- en: Source:[http://spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates this usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![About ML Pipelines](img/image00405.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, when we want to make predictions on the test data,
    we observe that first the test data has to pass through a series of data-preprocessing
    steps, which are very much identical to the aforementioned training step. After
    the pre-processing step is completed, the features of the test data are applied
    to the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: In order to make the data-preprocessing and feature extraction steps identical,
    we will pass the test data to the PipelineModel (logistic regression model) by
    calling the `transform()` to generate the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering using Alternating Least Square
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s explain the Matrix Factorization Model (MF) and the
    Alternating Least Squares method. Before we get to know about the Matrix Factorization
    Model, we''ll define the objective once again. Imagine we have ratings given to
    items by a number of users. Let''s define the ratings given by users on items
    in a matrix form given by *R*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering using Alternating Least Square](img/image00406.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we observe that user Ted has rated items B and D as
    4 and 3 respectively. In a collaborative filtering approach, the first step before
    generating recommendations is to fill the empty spaces, that is, to predict the
    non-rated items. Once the non-rated item ratings are filled, we suggest new items
    to the users by ranking the newly filled items.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have seen neighbouring methods using Euclidean
    distances and cosine distances to predict the missing values. In this section,
    we will adopt a new method to fill the missing non-rated items. This approach
    is called the matrix factorization method. This is a mathematical approach, which
    uses matrix decomposition methods. This method is explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A matrix can be decomposed into two low rank matrices, which, when multiplied
    back, will result in a single matrix approximately equal to the original matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say a rating matrix *R* of size *U X M* can be decomposed into two low
    rank matrices *P* and *Q* of size *U X K* and *M X K* respectively, where *K*
    is called the rank of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, let''s say the original matrix of size *4 X 4* is
    decomposed into two matrices: *P (4 X 2)* and *Q (4 X 2)*. Multiplying back *P*
    and *Q* will give us the original matrix of size *4 X 4* and values approximately
    equal to those of the original matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering using Alternating Least Square](img/image00407.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The principle of matrix factorization is used in recommendation engines, to
    fill the non-rated items. The assumption in applying the aforementioned principle
    to recommendation engines is that the ratings given by users on items are based
    on some latent features. These latent features are applicable to both users and
    items, that is, a user rates an item because of some of his personal preferences
    for it. Also, the user rates items because of certain features of the items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this assumption, when the matrix factorization method is applied to the
    ratings matrix, we decompose the original ratings matrix into two matrices follows
    as user-latent factor matrix, P, and item-latent factor matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering using Alternating Least Square](img/image00408.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s come back to the machine learning approach; you must be wondering
    what the learning in this approach is. Observe the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Collaborative filtering using Alternating Least Square](img/image00409.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We have learnt that when we multiply back the two latent factor matrices, we
    get the approximate original matrix. Now, in order to improve the accuracy of
    the model, that is, to learn the optimal factor vectors, P and Q, we define an
    optimization function, shown in the preceding formula, which minimizes the regularized
    squared error between the original ratings matrix and the resultant after the
    product of the latent matrices. The latter part of the preceding equation is the
    regularization imposed to avoid over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alternating Least Squares** is the optimization technique to minimize the
    aforementioned loss function. In general, we use stochastic gradient descent to
    optimize the loss function. For the Spark recommendation module, the ALS technique
    has been used for minimizing the loss function.'
  prefs: []
  type: TYPE_NORMAL
- en: In the ALS method, we calculate the optimal latent factor vectors alternatively
    by fixing one of the two factor vectors, that is, we calculate the user latent
    vector by fixing the item-latent feature vector as constant and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main benefits of the ALS approach are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This approach can be easily parallelized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most cases, we deal with sparse datasets in recommendation engine problems,
    and ALS is more efficient in handling sparsity compared to the stochastic gradient
    descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Spark implementation of the recommendation engine module in spark.ml has
    the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**numBlocks**: This is the number of blocks the users and items will be partitioned
    into in order to parallelize computation (defaults to 10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**rank**: This refers to the number of latent factors in the model (defaults
    to 10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**maxIter**: This is the maximum number of iterations to run (defaults to 10)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**regParam**: This parameter specifies the regularization parameter in ALS
    (defaults to 0.1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**implicitPrefs**: This parameter specifies whether to use the explicit feedback
    ALS variant or the one adapted for implicit feedback data (defaults to false,
    which means it''s using explicit feedback)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**alpha**: This is a parameter applicable to the implicit feedback variant
    of ALS, which governs the baseline confidence in preference observations (defaults
    to 1.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**nonnegative**: This parameter specifies whether or not to use non-negative
    constraints for least squares (defaults to false)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model based recommender system using pyspark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Software details for the use case are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python API: pyspark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centos 6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Start the Spark session using pyspark, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the Spark session created by running the above
    `pyspark` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model based recommender system using pyspark](img/image00410.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To build the recommendation engine using Spark, we make use of Spark 2.0 capabilities,
    such as DataFrames, RDD, Pipelines, and Transforms available in Spark MLlib, which
    has was explained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike earlier heurist approaches, such as k-nearest neighboring approaches
    used for building recommendation engines, in Spark, matrix factorization methods
    are used for building recommendation engines and the Alternating Least Squares
    (ALS) method is used for generating model-based collaborative filtering.
  prefs: []
  type: TYPE_NORMAL
- en: MLlib recommendation engine module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's learn about the different methods present in the MLlib
    recommendation engine module. The current recommendation engine module helps us
    build the model-based collaborative filtering approach using the Alternating Least
    Squares matrix factorization model to generate recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main methods available for building collaborative filtering are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ALS()`: The `ALS()` constructor is invoked and its instance is created with
    all the required parameters, such as user column name, item column name, rating
    column name, rank, regularization parameter (regParam), maximum iterations (maxIter),
    and so on supplied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fit()`: The `fit()` method is used to generate the model. This method takes
    the following parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset`: input dataset of type `pyspark.sql.DataFrame`([http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame))'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`params`: this is an optional param map which contains required parameters
    listed above.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Returns: The `fit()` method returns fitted models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Transform()`: The `transform()` method is used to generate the predictions.
    The `transform()` method takes in the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test data (DataFrame datatype)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optional additional parameters that embed previously defined parameters.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns a predictions (DataFrame object)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The recommendation engine approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s get into the actual implementation of the recommendation engine.
    We use the following approach to build the recommendation engine using Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the Spark environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the data source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the MLlib recommendation engine module to generate the recommendations using
    ALS instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the cross_validation approach, apply the parameter tuning model to tune
    the parameter and select the best model, and then generate recommendations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like any other recommendation engine, the first step is to load the data into
    the analytics environment (into the Spark environment in our case). When we start
    the Spark environment in the 2.0 version, SparkContext and SparkSession will be
    created at the load time.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into the implementation part, let's review the data for a while.
    In this chapter, we use the MovieLens 100K Dataset to build collaborative filtering
    recommendation engines, both user-based and item-based. The dataset contains 943
    user ratings on 1,682 movies. The ratings are on a scale of 1-5.
  prefs: []
  type: TYPE_NORMAL
- en: As a first step, we shall make use of SparkContext (sc) to load the data into the
    Spark environment.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To load the data, run the below command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Loaded data will be a spark RDD type-run the below command to find out the
    data type of the data object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Total length of the data loaded is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To load the first record in the loaded data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the header information is located as the first row in the data
    object separated by `\t`; the column names of the data object are `UserID`, `ItemId`,
    `Rating`, and `Timestamp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we don''t require Timestamp information, so we can remove
    this field from the data RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the first 5 rows of the data RDD, we use take() action method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The MLlib recommendation engine module expects the data to be without any header
    information. So let''s remove the header information, that is, remove the first
    line from the data RDD object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the first row from the data RDD object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Use `filter()` method and lambda expression to remove the first header row
    from the data. The lambda expression below is applied for each row and each row
    is compared with the header to check if the extracted row is the header or not.
    If the extracted row is found to be the header then that row is filtered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us check the count of the data RDD object; it has reduced from 100001
    to 100000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us check the first row, we can observe that the header has been successfully
    removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have loaded the data into the Spark environment, let''s format
    the data into a proper shape, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the required functions for building the recommendation engine, such as
    ALS, the Matrix Factorization Model, and the Rating function from the MLlib recommendation
    module.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract each row from the data RDD and split by `\t` to separate each column
    using the `map()` and lambda expressions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the resultant set, let's create a Rating row object for each of the lines
    extracted in the previous step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the following expression is applied on the entire dataset, a pipelined
    RDD object is created:![Data loading](img/image00411.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check the data type of ratings object using type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the first 5 records of the ratings PipelinedRDD object by running the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can observe from the preceding result that each row in the original raw data
    RDD object turns into a kind of list of Rating row objects stacked into PipelinedRDD.
  prefs: []
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have loaded the data, let''s spend some time exploring the data.
    Let''s use the Spark 2.0 DataFrame API capabilities to explore the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the total number of unique users by first selecting the `''user''`
    column and then using `distinct()` function to remove the duplicate `userId`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of the previous query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/image00412.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Total number of unique users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Total number of unique items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Display first 5 unique products:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of the previous query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/image00413.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Number of rated products by each user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The previous results explain that User 26 has rated 107 movies and user 29 has
    rated 34 movies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of records for each rating type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of the previous query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data exploration](img/image00414.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code, we make use of the `numpy` scientific computing package
    in Python, used for working with arrays: `matplotlibe` - a visualizing package
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![Data exploration](img/image00415.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Statistics of ratings per user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Data exploration](img/image00416.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Individual counts of ratings per user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Data exploration](img/image00417.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Average rating given by each user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Average rating per movie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Building the basic recommendation engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Divide the original data into training and test datasets randomly as follows,
    using the `randomSplit()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: (training, test) = ratings.randomSplit([0.8, 0.2])
  prefs: []
  type: TYPE_NORMAL
- en: 'Counting the number of instances in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Counting the number of instances in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Let's now build a recommendation engine model using the ALS algorithm available
    in the MLlib library of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we use the following methods and parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the ALS module into the Spark environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call the `ALS.train()` method to train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the required parameters, such as rank, number of iterations (maxIter),
    and training data to the `ALS.train()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s understand the parameters now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rank**: This parameter is the number of latent factors of users and items
    to be used in the model. The default is 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**maxIter**: This is the number of iterations the model has to run. The default
    is 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Build the recommendation model using Alternating Least Squares:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting rank and maxIter parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `train()` method with training data, rank, maxIter params, `model =
    ALS.train`(training, rank, numIterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the model as below, we observe that `Matrixfactorizationmodel` object
    is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Making predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have created the model, let's predict the values of ratings on the
    test set we created earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The ALS module has provided many methods discussed in the following sections
    for making predictions, recommending users, and recommending items to users, user
    features, item features, and so on. Let's run the methods one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed to predictions, we shall first create test data in a way
    that is acceptable to the prediction methods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code extracts each row in the test data and extracts `userID,
    ItemID` and puts it in testdata `PipelinedRDD` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code shows the original test data sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code displays the formatted data required for making predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`predict()`: The predict method will the predict rating for a given user and
    item and is given as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This method is used when we want to make predictions for a combination of user
    and item:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe below that the prediction value for a user `119` and movie `392`
    is `4.3926091845289275`: just see above the original value for the same combination
    in test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`predictall()`: This method is used when we want to predict values for all
    the test data in one go, given as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code to check the data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code displays the first five predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: User-based collaborative filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let's recommend items (movies) to users. The ALS recommendation module contains
    the `recommendProductsForUsers()` method to generate the top-N item recommendations
    for users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `recommendProductsForUsers()` method takes integers as the input parameter,
    which indicates the top-N recommendations; for example, to generate the top 10
    recommendations to the users, we pass 10 as value to the `recommendProductsForUsers()`
    method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following code shows that recommendations are generated for all the
    943 users:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us see the recommendations for the first two users: 96 and 784:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let''s evaluate the model accuracy. For this, we choose the Root Mean Squared
    Error method to calculate the model accuracy. We can do it either manually, as
    shown next, or call a defined function available in the Spark MLlib: Create a
    `ratesAndPreds` object by joining the original ratings and predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will calculate the mean squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Model selection and hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important step in any machine learning task is to use model evaluation
    or model selection to find the optimal parameters that the fit the data. Spark
    provides infrastructure to tune and model evaluation, for individual algorithms
    or for the entire model building pipeline. Users may tune the entire pipeline
    model or tune individual components of the pipeline. MLlib provides model selection
    tools such as `CrossValidator` class and `TrainValidationSplit` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The above mentioned classes require the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Estimator** algorithm or Pipeline to tune'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set of ParamMaps**: parameters to choose from, sometimes called a *parameter
    grid* to search over'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluator**: metric to measure how well a fitted model does on held-out test
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At a high level, these model selection tools work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They split the input data into separate training and test datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each (training and test) pair, they iterate through the set of `ParamMaps`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each `ParamMap`, they fit the Estimator using those parameters, get the
    fitted Model, and evaluate the Model's performance using the Evaluator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They select the Model produced by the best-performing set of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MLlib supports various evaluation classes for performing evaluation tasks,
    such as the `RegressionEvaluator` class for regression based problems, the `BinaryClassificationEvaluator`
    class for binary classification problems, and the `MulticlassClassificationEvaluator`
    class for multiclass classification problems. For constructing a parameter grid,
    we can use the `paramGridBuilder` class.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Cross-Validation** approach is one of the most popular approaches in
    evaluating the datamining models and in choosing optimal parameters for building
    the best estimation model. MLlib offers two types of evaluation classes: the `CrossValidator`
    and `TrainValidationSplit` classes.'
  prefs: []
  type: TYPE_NORMAL
- en: CrossValidator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `CrossValidator` class takes the input dataset and splits it into multiple
    dataset folds, which can be used as training and test sets. Using these datasets,
    the `CrossValidator` class builds multiple models and finds optimal parameters
    and stores in `ParamMap`. After identifying the best `ParamMap`, the `CrossValidator`
    class finally computes the best model using the entire dataset. For example, let's
    say we choose the five-fold cross-validation; the `CrossValidator` class splits
    the original dataset into five sub-datasets with each sub-dataset containing training
    and test sets. The `CrossValidator` class chooses each fold set at a time and
    estimates the model parameters. Finally, `CrossValidator` computes the average
    of the evaluation metric to store the best parameters in `ParamMap`.
  prefs: []
  type: TYPE_NORMAL
- en: Train-Validation Split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark MLlib provides another class for estimating the optimal parameters using
    `TrainValidationSplit`. Unlike `CrossValidator`, this class estimates the optimal
    parameters on a single dataset. For example, the `TrainValidatorSplit` class divides
    the input data into trainset and test sets of size 3/4 and 1/4, and optimal parameters
    are chosen using these sets.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's understand the recommendation engine model we built earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The tuning model is present in the MLlib of Spark 2.0 and makes use of DataFrame
    API features. So in order to accommodate this, our first step is to convert the
    original dataset ratings to DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'For conversion, we use the `sqlContext` object and the `createDataFrame()`
    method to convert the ratings RDD object to a `DataFrame` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'SQL Context object is created when starting the spark session using pyspark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating a DataFrame object from the ratings rdd object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Display first 20 records of `dataframe` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results of the previous query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Train-Validation Split](img/image00418.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Creating random samples of training set and test set using `randomSplit()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Load modules required for running parameter `tuningmodel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Call the ALS method available in MLlib to building the recommendation engine.
    The following method `ALS()` takes only column values of training data, such as
    UserID, ItemId, and Rating. The other parameters, such as rank, number of iterations,
    learning parameters, and so on will be passed as the `ParamGridBuilder` object
    to the cross-validation method.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the model tuning pipeline require Estimators, a set of
    ParamMaps, and Evaluators. Let''s create each one of them, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Estimator objects as stated earlier, estimators take algorithm or pipeline
    objects as input. Let us build one pipeline object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling ALS algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Checking the type of `als` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us see how the default parameters are set for the ALS model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding result, we observe that the model is set to its default
    values for rank as 10, maxIter as 10, and blocksize as 10:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create pipeline object and setting the created als model as a stage in the
    pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Setting the ParamMaps/parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s observe the `ALS()` method closely and logically and infer the parameters
    that can be used for parameter tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**rank:** We know that rank is the number of latent features for users and
    items, which, by default, is 10, but if we do not have the optimal number of latent
    features for a given dataset, this parameter can be taken up for tuning the model
    by giving a range of values between 8 and 12 - the choice is left to the users.
    Due to the computational cost, we restrict the values to 8 -12, but readers are
    free to try other values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MaxIter:** MaxIter is the number of times the model is made to run; it''s
    set to a default 10\. We can select this parameter also for tuning as we do not
    know the optimal iterations at which the model performs well; we select between
    10 and 15.'
  prefs: []
  type: TYPE_NORMAL
- en: '**reqParams:** regParams is the learning parameter set between 1 and 10.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading `CrossValidation` and `ParamGridBuilder` modules to create range of
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Setting the evaluator object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As stated earlier, the evaluator object sets the evaluation metric to evaluate
    the model during multiple runs in the cross validation method:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the `RegressionEvaluator` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling `RegressionEvaluator()` method with evaluation metric set to `rmse`
    and `evaluation` column set to Rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have prepared all the required objects for running the cross-validation
    method, that is, `Estimator`, `paramMaps`, and `Evaluator`, let's run the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-validation method gives us the best optimal model out of all the
    executed models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the model using `fit()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about model-based collaborative filtering using
    matrix factorization methods using ALS. We used the Python API to access the Spark
    framework and ran the ALS collaborative filtering. In the beginning of the chapter,
    we refreshed our knowledge of Spark with all the basics that are required to run
    the recommendation engines, such as what Spark is, the Spark ecosystem, components
    of Spark, SparkSession, DataFrames, RDD, and so on. As explained then, we explored
    the MovieLens data, built a basic recommendation engine, evaluated the model,
    and used parameter tuning to improve the model. In the next chapter, we shall
    learn about building recommendations using Graph database - Neo4j.
  prefs: []
  type: TYPE_NORMAL
