<html><head></head><body>
		<div id="_idContainer199" class="Content">
			<h1 id="_idParaDest-256"><em class="italics"><a id="_idTextAnchor259"/>Chapter 6:</em></h1>
		</div>
		<div id="_idContainer200" class="Content">
			<h1 id="_idParaDest-257"><a id="_idTextAnchor260"/>Feature Selection and Dimensionality Reduction</h1>
		</div>
		<div id="_idContainer201" class="Content">
			<h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Implement feature engineering techniques such as discretization, one-hot encoding, and transformation</li>
				<li class="bullets">Execute feature selection methods on a real-world dataset using univariate feature selection, correlation matrix, and model-based feature importance ranking</li>
				<li class="bullets">Apply feature reduction using principal component analysis (PCA) for dimensionality reduction, variable reduction with clustering, and linear discriminant analysis (LDA)</li>
				<li class="bullets">Implement PCA and LDA and observe the differences between them</li>
			</ul>
			<p>In this chapter, we will explore the feature selection and dimensionality reduction methods to build an effective feature set and hence improve the model performance.</p>
		</div>
		<div id="_idContainer249" class="Content">
			<h2 id="_idParaDest-258"><a id="_idTextAnchor261"/>Introduction</h2>
			<p>In the last two chapters (on regression and classification), we focused on understanding and implementing the various machine learning algorithms in the supervised learning category on a given dataset pertaining to a problem.</p>
			<p>In this chapter, we will focus more on effectively using the features of the dataset to build the best performing model. Often in many datasets, the feature space is quite large (with many features). The model performance takes a hit as the patterns are hard to find and often much noise is present in the data. Feature selections are specific methods that are used to identify the importance of each feature and assign a score to each. We can then select the top 10 or 15 features (or even more) based on the score for building our model.</p>
			<p>Another possibility is to create new variables using a linear combination of all the input variables. This helps in keeping the representation of all variables and reducing the dimensionality of feature space. However, such a reduction often reduces the explainable variance. In this chapter, we will focus on the three major actions we perform on the dataset for improving model performance:</p>
			<ul>
				<li><strong class="bold">Feature engineering: </strong>Essentially transforms the features so the machine learning algorithms will work</li>
				<li><strong class="bold">Selection: </strong>Selects the features with high importance to bring out the best performance of the model</li>
				<li><strong class="bold">Reduction: </strong>Reduces the feature dimensionality by representing a higher order dataset into a lower dimension</li>
			</ul>
			<p>All three are closely related yet different in how they function.</p>
			<p>In this chapter, in addition to the Beijing PM2.5 dataset, we will use the Los Angeles ozone pollution data, 1976, provided in the <strong class="inline">mlbench</strong> library of R. It is a data frame with 366 observations on 13 variables, where each observation is of one day.</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/C12624_06_01.jpg" alt="Figure 6.1: List of variables in Los Angeles ozone pollution data, 1976&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.1: List of variables in Los Angeles ozone pollution data, 1976</h6>
			<p>Originally, the dataset was provided for the problem of predicting the daily maximum one-hour-average ozone readings (the fourth variable in the table). Both Beijing PM2.5 and Los Angeles ozone datasets resonate with the effects of pollutants on our environment.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor262"/>Feature Engineering</h2>
			<p>The algorithms we use in machine learning will perform based on the quality and goodness of the data; they do not have any intelligence of their own. The better and innovative you become in designing features, the better the model performance. <strong class="keyword">Feature engineering</strong> in many ways helps in bringing the best out of data. The term feature engineering essentially refers to the process of the <strong class="bold">derivation</strong> and <strong class="bold">transformation</strong> of given features, thus better characterizing the meaning of the features and representing the underlying problem of the predictive model. By this process, we anticipate the improvement in the model's <strong class="bold">predictability power</strong> and <strong class="bold">accuracy</strong>.</p>
			<h3 id="_idParaDest-260"><a id="_idTextAnchor263"/>Discretization</h3>
			<p>In <em class="italics">Chapter 3</em>, <em class="italics">Introduction to Supervised Learning</em>, we converted the numeric values of a 3-hour rolling average of PM2.5 in the Beijing dataset to the binary values 1 and 0 for logistic regression, based on the threshold of 35, where 1 means <strong class="bold">normal</strong> and 0 means <strong class="bold">above normal</strong>. The process is called <strong class="keyword">discretization</strong>, also commonly referred to as <strong class="bold">binning, or</strong> in our case, <strong class="bold">binary discretization</strong>. More broadly, in applied mathematics, discretization is the process of transferring continuous functions, models, variables, and equations into discrete counterparts. Now, let's perform the process on a variable.</p>
			<h3 id="_idParaDest-261"><a id="_idTextAnchor264"/>Exercise 77: Performing Binary Discretization</h3>
			<p>In this exercise, we will create a binary variable using the <strong class="inline">pm2.5</strong> variable of the Beijing PM2.5 dataset. Binary discretization of the <strong class="inline">pm2.5</strong> variable will create a column that will be 1 if the PM2.5 level is greater than 35, else it will be 0. This process will help us create a discrete categorical variable (to be called <strong class="inline">pollution_level</strong>) from a continuous numeric variable.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Start with reading the Beijing PM2.5 dataset using the following command:<p class="snippet">PM25 &lt;- read.csv("PRSA_data_2010.1.1-2014.12.31.csv")</p></li>
				<li>Load the following libraries:<p class="snippet">library(dplyr)</p><p class="snippet">library(lubridate)</p><p class="snippet">library(tidyr)</p><p class="snippet">library(ggplot2)</p><p class="snippet">library(grid)</p><p class="snippet">library(zoo)</p></li>
				<li>Combine year, month, day, and hour into a <strong class="inline">datetime</strong> variable using the with function from the lubridate package:<p class="snippet">PM25$datetime &lt;- with(PM25, ymd_h(sprintf('%04d%02d%02d%02d', year, month, day,hour)))</p></li>
				<li>Now, remove any row with an NA in the column:<p class="snippet">PM25_subset &lt;- na.omit(PM25[,c("datetime","pm2.5")])</p></li>
				<li>Using the zoo structure, compute the moving average every <strong class="inline">3</strong> hours:<p class="snippet">PM25_three_hour_pm25_avg &lt;- rollapply(zoo(PM25_subset$pm2.5,PM25_subset$datetime), 3, mean)</p></li>
				<li>Next, convert the output of the moving average into a DataFrame:<p class="snippet">PM25_three_hour_pm25_avg &lt;- as.data.frame(PM25_three_hour_pm25_avg)</p></li>
				<li>Now, put the timestamp in the row names into the main columns:<p class="snippet">PM25_three_hour_pm25_avg$timestamp &lt;- row.names(PM25_three_hour_pm25_avg)</p></li>
				<li>Get rid of the row names (optional):<p class="snippet">row.names(PM25_three_hour_pm25_avg) &lt;- NULL</p></li>
				<li>Rename the columns:<p class="snippet">colnames(PM25_three_hour_pm25_avg) &lt;- c("avg_pm25","timestamp")</p></li>
				<li>Create two levels based on the PM2.5 average. <strong class="inline">0</strong> implies <strong class="bold">normal</strong> and <strong class="inline">1</strong> implies <strong class="bold">above the normal</strong>:<p class="snippet">PM25_three_hour_pm25_avg$pollution_level &lt;- ifelse(PM25_three_hour_pm25_avg$avg_pm25 &lt;= 35, 0,1)</p></li>
				<li>Randomly select 10 rows using the following command:<p class="snippet">r_index &lt;- sample(nrow(PM25_three_hour_pm25_avg),10)</p></li>
				<li>Print the output using the following command:<p class="snippet">PM25_three_hour_pm25_avg[r_index,]</p><p class="snippet">##        avg_pm25           timestamp pollution_level</p><p class="snippet">## 405   399.33333 2010-01-18 21:00:00               1</p><p class="snippet">## 3694  142.33333 2010-06-14 23:00:00               1</p><p class="snippet">## 8072   14.33333 2010-12-31 05:00:00               0</p><p class="snippet">## 3502  107.00000 2010-06-01 14:00:00               1</p><p class="snippet">## 20828  80.33333 2012-07-21 16:00:00               1</p><p class="snippet">## 32010  95.66667 2013-11-15 20:00:00               1</p><p class="snippet">## 3637  103.33333 2010-06-12 14:00:00               1</p><p class="snippet">## 4736  192.66667 2010-07-29 11:00:00               1</p><p class="snippet">## 22053  37.33333 2012-09-17 19:00:00               1</p><p class="snippet">## 7135   32.33333 2010-11-22 02:00:00               0</p></li>
			</ol>
			<p>Observe that the variable <strong class="inline">pollution_level</strong> is now a binary categorical variable, which we created in Step 11. The dataset with <strong class="inline">pollution_level</strong> as an output variable could be used with any classification algorithm.</p>
			<h3 id="_idParaDest-262"><a id="_idTextAnchor265"/>Multi-Category Discretization</h3>
			<p>A more general form of discretization is to divide the range of values of a continuous variable into many smaller ranges of values using appropriate cut-points. One way of identifying the appropriate cut-point is to analyze the distribution of the variable.</p>
			<p>Using the following code, plot a histogram of <strong class="inline">avg_pm25</strong> with <strong class="inline">binwidth</strong> of <strong class="inline">30 (</strong>meaning the range of values will be divided into ranges of size <strong class="inline">30)</strong>:</p>
			<p class="snippet">ggplot(PM25_three_hour_pm25_avg, aes(x=avg_pm25)) +   geom_histogram(binwidth = 30,color="darkblue", fill="lightblue")</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/C12624_06_02.jpg" alt="Figure 6.2: Histogram of 3-hour rolling average of PM2.5 values from Beijing dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.2: Histogram of 3-hour rolling average of PM2.5 values from Beijing dataset</h6>
			<p>The plot in <em class="italics">Figure 6.2</em> shows the right skewness in the variable, which means the majority of values are on the left of the range of values, mostly concentrated between 0 and 250. Such skewness inhibits the model from generalizing, hence it has a lower predictive power. Now, let's explore how we can utilize multi-category discretization to improve this scenario.</p>
			<h3 id="_idParaDest-263"><a id="_idTextAnchor266"/>Exercise 78: Demonstrating the Use of Quantile Function</h3>
			<p>In this exercise, we will demonstrate the use of the <strong class="bold">quantile function</strong>, which gives cut-points corresponding to the 0th, 25th, 50th, 75th, and 100th percentile points from the values of <strong class="inline">avg_pm25</strong>.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the required libraries and packages.</li>
				<li>Find the quantiles on <strong class="inline">avg_pm25</strong>:<p class="snippet">avg_pm25 &lt;- PM25_three_hour_pm25_avg$avg_pm25</p><p class="snippet">quartiles = quantile(round(avg_pm25), seq(0,1, .25), include.lowest=T)</p></li>
				<li>Next, calculate the vertical lines on the quantile points:<p class="snippet">ggplot(PM25_three_hour_pm25_avg, aes(x=avg_pm25))+  geom_histogram(binwidth = 30,color="darkblue", fill="lightblue")+    geom_vline(xintercept=quartiles,            color="blue", linetype="dashed", size=1)</p><p>The plot is as follows:</p><div id="_idContainer204" class="IMG---Figure"><img src="image/C12624_06_03.jpg" alt=""/></div><h6>Figure 6.3 Histogram of 3-hour rolling average of PM2.5 values from Beijing dataset with cut-lines corresponding to 0th, 25th, 50th, 75th, and 100th percentile points</h6><p>The following code snippet creates the variable <strong class="inline">avg_pm25_quartiles</strong> in the dataset, which represents the five percentile points on the values of <strong class="inline">avg_pm25</strong>. This new variable could be used in modeling after <strong class="bold">one-hot encoding,</strong> which we will discuss in the next section.</p></li>
				<li>Let's use the following code to add a new variable <strong class="inline">avg_pm25_quartiles</strong> in the dataset:<p class="snippet">PM25_three_hour_pm25_avg$avg_pm25_quartiles &lt;- as.integer(cut(avg_pm25,breaks=quantile(round(avg_pm25), seq(0,1, .25), include.lowest=T)))</p></li>
			</ol>
			<p>We have just seen how discretization helps to remove any data skewness before modelling.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor267"/>One-Hot Encoding</h2>
			<p>One-hot encoding is a process of binarizing the categorical variable. This is done by transforming a categorical variable with <em class="italics">n</em> unique values into <em class="italics">n</em> unique columns in the datasets while keeping the number of rows the same. The following table shows how the wind direction column is transformed into five binary columns. For example, the row number <strong class="bold">1</strong> has the value <strong class="bold">North</strong>, so we get a <strong class="bold">1</strong> in the corresponding column named <strong class="bold">Direction_N</strong> and <strong class="bold">0</strong> in the remaining columns. So on for the other rows. Note that out of these sample five rows of data, the direction <strong class="bold">West</strong> is not present. However, the larger dataset would have got the value for us to have the column <strong class="bold">Direction_W</strong>.</p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/C12624_06_04.jpg" alt="Figure 6.4 Transforming a categorical variable into Binary 1s and 0s using one-hot encoding&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.4 Transforming a categorical variable into Binary 1s and 0s using one-hot encoding</h6>
			<p>One primary reason for converting categorical variables (such as the one shown in the previous table) to binary columns is related to the limitation of many machine learning algorithms, which can only deal with numerical values. However, in order to convert the categorical variables into a numerical variable, we have to represent it with some mapping value, such as <strong class="inline">North = 1</strong>, <strong class="inline">South = 2</strong>, <strong class="inline">West = 3</strong>, and so on. The problem with such encoding is that the values <strong class="inline">1</strong>, <strong class="inline">2</strong>, and <strong class="inline">3</strong> are integers, where <strong class="inline">3&gt;2&gt;1</strong>; however, this is not the case with wind direction. </p>
			<p>The interpretation is entirely wrong. Binary one-hot encoding overcomes this challenge by creating one column for each value in the categorical variable, thus giving us a more elegant representation. We can now use any algorithm from machine learning on such data as long as it satisfies the type of problem.</p>
			<h3 id="_idParaDest-265"><a id="_idTextAnchor268"/>Exercise 79: Using One-Hot Encoding</h3>
			<p>In this exercise, we will use the one-hot encoding for creating one column for each value in the categorical variable.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the required libraries and packages.</li>
				<li>Create the <strong class="inline">OzoneData</strong> variable and store the value of <strong class="inline">ozone1.csv</strong> using the <strong class="inline">read.csv</strong> function:<p class="snippet">OzoneData &lt;- read.csv("ozone1.csv", stringsAsFactors=F)</p></li>
				<li>Import the required <strong class="inline">caret</strong> packages into the system:<p class="snippet">library(caret)</p></li>
				<li>Create input datasets:<p class="snippet">OzoneData$Day_of_week &lt;- as.factor(OzoneData$Day_of_week) </p><p class="snippet">OzoneData_OneHot &lt;- dummyVars(" ~ .", data = OzoneData)</p></li>
				<li>Create the response DataFrame:<p class="snippet">OzoneData_OneHot &lt;- data.frame(predict(OzoneData_OneHot, newdata = OzoneData))</p></li>
				<li>Plot the data using the <strong class="inline">head()</strong> function:<p class="snippet">head(OzoneData_OneHot)</p><p>The output is as follows:</p><p class="snippet">##   Month Day_of_month Day_of_week.1 Day_of_week.2 Day_of_week.3</p><p class="snippet">## 1     1            1             0             0             0</p><p class="snippet">## 2     1            2             0             0             0</p><p class="snippet">## 3     1            3             0             0             0</p><p class="snippet">## 4     1            4             0             0             0</p><p class="snippet">## 5     1            5             1             0             0</p><p class="snippet">## 6     1            6             0             1             0</p><p class="snippet">##   Day_of_week.4 Day_of_week.5 Day_of_week.6 Day_of_week.7 ozone_reading</p><p class="snippet">## 1             1             0             0             0             3</p><p class="snippet">## 2             0             1             0             0             3</p><p class="snippet">## 3             0             0             1             0             3</p><p class="snippet">## 4             0             0             0             1             5</p><p class="snippet">## 5             0             0             0             0             5</p><p class="snippet">## 6             0             0             0             0             6</p><p class="snippet">##   pressure_height Wind_speed Humidity Temperature_Sandburg</p><p class="snippet">## 1            5480          8 20.00000             40.53473</p><p class="snippet">## 2            5660          6 40.96306             38.00000</p><p class="snippet">## 3            5710          4 28.00000             40.00000</p><p class="snippet">## 4            5700          3 37.00000             45.00000</p><p class="snippet">## 5            5760          3 51.00000             54.00000</p><p class="snippet">## 6            5720          4 69.00000             35.00000</p><p class="snippet">##   Temperature_ElMonte Inversion_base_height Pressure_gradient</p><p class="snippet">## 1            39.77461              5000.000               -15</p><p class="snippet">## 2            46.74935              4108.904               -14</p><p class="snippet">## 3            49.49278              2693.000               -25</p><p class="snippet">## 4            52.29403               590.000               -24</p><p class="snippet">## 5            45.32000              1450.000                25</p><p class="snippet">## 6            49.64000              1568.000                15</p><p class="snippet">##   Inversion_temperature Visibility</p><p class="snippet">## 1              30.56000        200</p><p class="snippet">## 2              48.02557        300</p><p class="snippet">## 3              47.66000        250</p><p class="snippet">## 4              55.04000        100</p><p class="snippet">## 5              57.02000         60</p><p class="snippet">## 6              53.78000         60</p></li>
			</ol>
			<p>Observe the <strong class="inline">OneHot</strong> variable we have created in the <strong class="inline">OzoneData</strong> DataFrame. After one-hot encoding, each value (1 to 7) in <strong class="inline">Day_of_week</strong> is represented as a separate column.</p>
			<h3 id="_idParaDest-266"><a id="_idTextAnchor269"/>Activity 11: Converting the CBWD Feature of the Beijing PM2.5 Dataset into One-Hot Encoded Columns</h3>
			<p>In this activity, we will learn how to convert any categorical variable into a one-hot encoded vector. Particularly, we will convert the CBWD feature of the Beijing PM2.5 dataset into one-hot encoded columns. Many machine learning algorithms work only on numerical features; in such cases, it becomes imperative to use one-hot encoding. </p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">Read the Beijing PM2.5 dataset.</li>
				<li>Create a variable <strong class="inline">cbwd_one_hot</strong> for storing the result of the <strong class="inline">dummyVars</strong> function with <strong class="inline">~ cbwd</strong> as its first argument.</li>
				<li>Use the output of the <strong class="inline">predict()</strong> function on <strong class="inline">cbwd_one_hot</strong>.</li>
				<li>Remove the original <strong class="inline">cbwd</strong> variable from the <strong class="inline">PM25</strong> DataFrame.</li>
				<li>Using the <strong class="inline">cbind()</strong> function, add <strong class="inline">cbwd_one_hot</strong> to the <strong class="inline">PM25</strong> DataFrame.</li>
				<li>Print the top six rows of <strong class="inline">PM25</strong>.<p>The output is as follows:</p><p class="snippet">##   No year month day hour pm2.5 DEWP TEMP PRES   Iws Is Ir cbwd.cv cbwd.NE</p><p class="snippet">## 1  1 2010     1   1    0    NA  -21  -11 1021  1.79  0  0       0       0</p><p class="snippet">## 2  2 2010     1   1    1    NA  -21  -12 1020  4.92  0  0       0       0</p><p class="snippet">## 3  3 2010     1   1    2    NA  -21  -11 1019  6.71  0  0       0       0</p><p class="snippet">## 4  4 2010     1   1    3    NA  -21  -14 1019  9.84  0  0       0       0</p><p class="snippet">## 5  5 2010     1   1    4    NA  -20  -12 1018 12.97  0  0       0       0</p><p class="snippet">## 6  6 2010     1   1    5    NA  -19  -10 1017 16.10  0  0       0       0</p><p class="snippet">##   cbwd.NW cbwd.SE</p><p class="snippet">## 1       1       0</p><p class="snippet">## 2       1       0</p><p class="snippet">## 3       1       0</p><p class="snippet">## 4       1       0</p><p class="snippet">## 5       1       0</p><p class="snippet">## 6       1       0</p><h4>Note</h4><p class="callout">The solution for this activity can be found on page 459.</p></li>
			</ol>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor270"/>Log Transformation</h2>
			<p>The most common technique to correct for skewed distribution is to find an appropriate mathematical function that has an inverse. One such function is a log, represented as follows:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/C12624_06_15.jpg" alt=""/>
				</div>
			</div>
			<p>In other words, <img src="image/C12624_06_16.png" alt=""/> is the <img src="image/C12624_06_17.png" alt=""/> of <img src="image/C12624_06_18.png" alt=""/> to the base <img src="image/C12624_06_19.png" alt=""/>. The inverse, to find the <img src="image/C12624_06_19a.png" alt=""/>, can be computed as follows:</p>
			<div>
				<div id="_idContainer212" class="IMG---Figure">
					<img src="image/C12624_06_20.jpg" alt=""/>
				</div>
			</div>
			<p>This transformation gives the ability to handle the skewness in the data; at the same time, the original value can be easily computed once the model is built. The most popular log transformation is the natural <img src="image/C12624_06_21.png" alt=""/>, where <img src="image/C12624_06_22.png" alt=""/> is the mathematical constant <img src="image/C12624_06_23.png" alt=""/>, which equals roughly <strong class="inline">2.71828</strong>.</p>
			<p>One useful property of the log function is that it handles the data skewness elegantly. For example, the following code demonstrates the difference between <strong class="inline">log(10000)</strong> and <strong class="inline">log(1000000)</strong> as just <strong class="inline">4.60517</strong>. The number <img src="image/C12624_06_24.png" alt=""/> is 100 times bigger than <img src="image/C12624_06_25.png" alt=""/>. This reduces the skewness that we otherwise let the model handle, which it might not do sufficiently.</p>
			<p class="snippet">#Natural Log</p>
			<p class="snippet">log(10000)</p>
			<p class="snippet">## [1] 9.21034</p>
			<p class="snippet"># 10 times bigger value</p>
			<p class="snippet">log(100000)</p>
			<p class="snippet">## [1] 11.51293</p>
			<p class="snippet"># 100 times bigger value</p>
			<p class="snippet">log(1000000)</p>
			<p class="snippet">## [1] 13.81551</p>
			<p>Let's see the result of applying the natural log on the 3-hour rolling average of the PM2.5 values.</p>
			<h3 id="_idParaDest-268"><a id="_idTextAnchor271"/>Exercise 80: Performing Log Transformation</h3>
			<p>In this exercise, we will draw a histogram of the <strong class="inline">avg_pm25</strong> variable with log transformation and compare it with the skewed distribution of the original values.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the required libraries and packages.</li>
				<li>Create a histogram of <strong class="inline">avg_pm25</strong>:<p class="snippet">ggplot(PM25_three_hour_pm25_avg, aes(x=avg_pm25))+  geom_histogram(color="darkblue", fill="lightblue")</p><p>The output is as follows:</p><div id="_idContainer218" class="IMG---Figure"><img src="image/C12624_06_05.jpg" alt="Figure 6.5 Histogram of the 3-hour rolling average of PM2.5 values from the Beijing dataset&#13;&#10;"/></div><h6>Figure 6.5 Histogram of the 3-hour rolling average of PM2.5 values from the Beijing dataset</h6></li>
				<li>Create a histogram of <strong class="inline">log_avg_pm25</strong>:<p class="snippet">ggplot(PM25_three_hour_pm25_avg, aes(x=log_avg_pm25))+  geom_histogram(color="darkblue", fill="lightblue")</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/C12624_06_06.jpg" alt="Figure 6.6 Histogram of the natural log of the 3-hour rolling average of PM2.5 values from the Beijing dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.6 Histogram of the natural log of the 3-hour rolling average of PM2.5 values from the Beijing dataset</h6>
			<p>In this exercise, we drew a plot to show the 3-hour rolling average of the PM2.5 values from the Beijing dataset and contrasted it with the histogram of the natural log of the 3-hour rolling average of the PM2.5 values from the Beijing dataset. Taking the log made the histogram look more symmetrical around the mean and the skewness.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor272"/>Feature Selection</h2>
			<p>While <strong class="bold">feature engineering</strong> ensures that the quality and data issues are rectified, <strong class="bold">feature selection</strong> helps with determining the right set of features for improving the performance of the model. Feature selection techniques identify the features that contribute the most in the prediction ability of the model. Features with less importance inhibit the model's ability to learn from the independent variable.</p>
			<p>Feature selection offers benefits such as:</p>
			<ul>
				<li>Reducing overfitting</li>
				<li>Improving accuracy</li>
				<li>Reducing the time to train the model</li>
			</ul>
			<h3 id="_idParaDest-270"><a id="_idTextAnchor273"/>Univariate Feature Selection</h3>
			<p>A statistical test such as the <strong class="bold">chi-squared</strong> <img src="image/C12624_06_26.png" alt=""/> test is a popular method to select features with a strong relationship to the dependent or target variable. It mainly works on categorical features in a classification problem. So, for this to work on a numerical variable, one needs to make the feature into categorical using discretization.</p>
			<p>In the most general form, chi-squared statistics could be computed as follows:</p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/C12624_06_27.jpg" alt=""/>
				</div>
			</div>
			<p>This tests whether or not there is a significant difference between observed frequency and expected frequency. A higher chi-squared value establishes a stronger dependence of the target variable and the particular feature. More formally:</p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/C12624_06_28.jpg" alt=""/>
				</div>
			</div>
			<p>Where:</p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/C12624_06_29_33.jpg" alt=""/>
				</div>
			</div>
			<h3 id="_idParaDest-271"><a id="_idTextAnchor274"/>Exercise 81: Exploring Chi-Squared</h3>
			<p>In this exercise, we will compute the chi-squared statistic for all the variables in the <strong class="inline">Ozone</strong> dataset. The top five variables with the highest chi-squared value will be our best feature for modelling.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the required libraries and packages.</li>
				<li>Create a variable named <strong class="inline">OzoneData</strong> and assign the value from the <strong class="inline">read.csv</strong> function:<p class="snippet">OzoneData &lt;- read.csv("ozone1.csv", stringsAsFactors=F)</p></li>
				<li>Now, set the <strong class="inline">path</strong> as illustrated here:<p class="snippet">path="C:\\Program Files\\Java\\jdk1.8.0_92"</p></li>
				<li>Next, use the Sys.getenv function to obtain the values of the environment variables:<p class="snippet">if (Sys.getenv("JAVA_HOME")!="")  Sys.setenv(JAVA_HOME=path)</p></li>
				<li>Install the required packages using the following command:<p class="snippet">install.packages("rJava")</p><p class="snippet">install.packages("FSelector")</p></li>
				<li>Import the <strong class="inline">rJava</strong> package:<p class="snippet">library(rJava)</p><p class="snippet">## Warning: package 'rJava' was built under R version 3.2.5</p><p class="snippet">library(FSelector)#For method</p><p class="snippet">library(mlbench)# For data</p></li>
				<li>Calculate the chi-squared statistics:<p class="snippet">weights&lt;- chi.squared(ozone_reading~., OzoneData)</p></li>
				<li>Print the results:<p class="snippet">print(weights)</p><p>The output is as follows:</p><p class="snippet">##                       attr_importance</p><p class="snippet">## Month                       0.4240813</p><p class="snippet">## Day_of_month                0.0000000</p><p class="snippet">## Day_of_week                 0.0000000</p><p class="snippet">## pressure_height             0.4315521</p><p class="snippet">## Wind_speed                  0.0000000</p><p class="snippet">## Humidity                    0.3923034</p><p class="snippet">## Temperature_Sandburg        0.5191951</p><p class="snippet">## Temperature_ElMonte         0.5232244</p><p class="snippet">## Inversion_base_height       0.6160403</p><p class="snippet">## Pressure_gradient           0.4120630</p><p class="snippet">## Inversion_temperature       0.5283836</p><p class="snippet">## Visibility                  0.4377749</p></li>
				<li>Select the top five variables:<p class="snippet">subset&lt;- cutoff.k(weights, 5)</p></li>
				<li>Print the final formula that can be used for classification:<p class="snippet">f&lt;- as.simple.formula(subset, "Class")</p><p class="snippet">print(f)</p><p>The output is as follows:</p><p class="snippet">## Class ~ Inversion_base_height + Inversion_temperature + Temperature_ElMonte + </p><p class="snippet">##     Temperature_Sandburg + Visibility</p><p class="snippet">## &lt;environment: 0x000000001a796d18&gt;</p></li>
			</ol>
			<p>We used the chi.squared() function to compute the chi-squared values for each feature in our Ozone dataset. The function outputs the attribute importance based on the chi-squared value. The formula in Step 10 that uses the top five features from the chi-squared statistic could be used for building a supervised learning model.</p>
			<h2 id="_idParaDest-272"><a id="_idTextAnchor275"/>Highly Correlated Variables</h2>
			<p>Generally, two highly correlated variables likely contribute to the prediction ability of the model, which makes one redundant. For example, if we have a dataset with <strong class="inline">age</strong>, <strong class="inline">height</strong>, and <strong class="inline">BMI</strong> as variables, we know that <strong class="inline">BMI</strong> is a function of <strong class="inline">age</strong> and <strong class="inline">height</strong> and it will always be highly correlated with the other two. If it's not, then something is wrong with the BMI calculation. In such cases, one might decide to remove the other two. However, it is always not this straight. In certain cases, a pair of variables might be highly correlated, but it is not easy to interpret why that is the case. In such cases, one can randomly drop one of the two.</p>
			<h3 id="_idParaDest-273"><a id="_idTextAnchor276"/>Exercise 82: Plotting a Correlated Matrix</h3>
			<p>In this exercise, we will compute the correlation between a pair of variables and draw a correlation plot using the <strong class="inline">corrplot</strong> package.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the required libraries using the following command:<p class="snippet">library(mlbench)</p><p class="snippet">library(caret)</p><p>The output is as follows:</p><p class="snippet">## Warning: package 'caret' was built under R version 3.2.5</p><p class="snippet">## Loading required package: lattice</p></li>
				<li>Now, load the data and calculate the correlation matrix:<p class="snippet">correlationMatrix &lt;- cor(OzoneData)</p></li>
				<li>Summarize the correlation matrix:<p class="snippet">print(correlationMatrix)</p><p>The output is as follows:</p><p class="snippet">##                              Month Day_of_month  Day_of_week ozone_reading</p><p class="snippet">## Month                  1.000000000   0.00644330 -0.007345951   0.054521859</p><p class="snippet">## Day_of_month           0.006443300   1.00000000  0.002679760   0.079493243</p><p class="snippet">## Day_of_week           -0.007345951   0.00267976  1.000000000  -0.042135770</p><p class="snippet">## ozone_reading          0.054521859   0.07949324 -0.042135770   1.000000000</p></li>
				<li>Find attributes that are highly correlated (ideally &gt;0.75):<p class="snippet">highlyCorrelated &lt;- findCorrelation(correlationMatrix, cutoff=0.5)</p></li>
				<li>Print the indexes of the highly correlated attributes:<p class="snippet">print(highlyCorrelated)</p><p>The output is as follows:</p><p class="snippet">## [1] 12  9  8  5  4  7</p></li>
				<li>Import the <strong class="inline">corrplot</strong> library:<p class="snippet">library(corrplot)</p></li>
				<li>Plot the correlation matrix:<p class="snippet">corrplot(correlationMatrix)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/C12624_06_07.jpg" alt="Figure 6.7: Plotting correlated matrix&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.7: Plotting correlated matrix</h6>
			<p>Observe in <em class="italics">Figure 6.7</em> that the dark blue circles represent high positive correlation and the dark red circles represent high negative correlation. The range of correlation values is between <strong class="inline">-1</strong> and <strong class="inline">1</strong>. Visually inspecting, we can see the variable <strong class="inline">Inversion_temperature</strong> has high positive correlation with <strong class="inline">pressure_height</strong> and high negative correlation with <strong class="inline">Inversion_base_height</strong>. For example, if <strong class="inline">Inversion_temperature</strong> increases, <strong class="inline">pressure_height</strong> will also increase and vice versa.</p>
			<h4>Note</h4>
			<p class="callout">Figure 6.7 can be found on GitHub: https://github.com/TrainingByPackt/Applied-Supervised-Learning-with-R/blob/master/Lesson06/C12624_06_07.png.</p>
			<h3 id="_idParaDest-274"><a id="_idTextAnchor277"/>Model-Based Feature Importance Ranking</h3>
			<p>A model such as random forest, an ensemble modeling technique where we build several models and combine their results using a simple voting technique (as described in <em class="italics">Chapter 5</em>, <em class="italics">Classification)</em>, has a useful technique to utilize all the variables in a dataset and at the same time not compromise the model performance. The simple idea behind the random forest model is that it randomly selects a subset of data and variables to build many decision trees. The final model prediction happens through not one decision tree but collectively using many decision trees. Majority voting is a commonly used technique for final prediction; in other words, what the majority of the decision tree predicts is the final prediction. </p>
			<p>The technique naturally gives a combination of variables that result in highest accuracy. (Other model evaluation metrics could also be used.)</p>
			<h4>Note</h4>
			<p class="callout">For certain research work in genomics and computational biology<strong class="bold">*</strong>, where potential predictor variables vary in their scale of measurement (input features including both sequence and categorical variables such as folding energy) and their number of categories (for example, when amino acid sequence data show different numbers of categories), random forest importance measures are not reliable.</p>
			<p class="callout"><strong class="bold">*</strong> Bias in random forest variable importance measures: Illustrations, sources and a solution: https://link.springer.com/article/10.1186/1471-2105-8-25.</p>
			<h3 id="_idParaDest-275"><a id="_idTextAnchor278"/>Exercise 83: Exploring RFE Using RF</h3>
			<p>In this exercise, we will explore <strong class="bold">recursive feature elimination</strong> (<strong class="bold">RFE</strong>) using the random forest algorithm. RFE helps in selecting the best features with highest feature importance.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the <strong class="inline">party</strong> package:<p class="snippet">library(party)</p></li>
				<li>Fit the random forest:<p class="snippet">cf1 &lt;- cforest(pm2.5 ~ . , data= na.omit(PM25[,c("month","DEWP","TEMP","PRES","Iws","pm2.5")]), control=cforest_unbiased(mtry=2,ntree=50)) </p></li>
				<li>Calculate the variable importance, based on a mean decrease in MSE. The <strong class="inline">varimp()</strong> function implements the RFE technique:<p class="snippet">varimp(cf1)</p><p>The output is as follows:</p><p class="snippet">##    month     DEWP     TEMP     PRES      Iws </p><p class="snippet">## 3736.679 5844.172 4080.546 1517.037 1388.532</p></li>
			</ol>
			<p>In Step 2, the <strong class="inline">party</strong> package provides the method <strong class="inline">cforest()</strong>, which fits a random forest model using the parameter <strong class="inline">mtry = 2</strong> and <strong class="inline">ntree = 50</strong> and finds the best model where the <strong class="bold">out-of-bag</strong> (<strong class="bold">OOB</strong>) error is the least while training. The OOB error is the mean prediction error on each training sample <strong class="inline">x</strong>, using only the trees that did not have <strong class="inline">x</strong> in their bootstrap sample. The function <strong class="inline">varimp()</strong> returns the variable importance using the permutation principle (with values randomly shuffled) of the mean decrease in MSE. In other words, variable importance is measured as the mean decrease of the MSE over all out-of-bag cross-validated predictions, when a given variable is permuted after training but before prediction.</p>
			<p>As a result of randomly shuffled (permuted) variables, we expect a <em class="italics">bad</em> variable to be created and inclusion of this shuffled variable to increase the MSE compared to when it is not included in the model. Hence, if the mean decrease in the MSE is high, the MSE of the model as a result of shuffling of the variable has got to be high. So, we can conclude the variable has higher importance.</p>
			<h3 id="_idParaDest-276"><a id="_idTextAnchor279"/>Exercise 84: Exploring the Variable Importance using the Random Forest Model</h3>
			<p>In this exercise, we will explore the variable importance using the random forest model. We will again use the Beijing dataset to see which among the five variables (<strong class="inline">month</strong>, <strong class="inline">DEWP</strong>, <strong class="inline">TEMP</strong>, <strong class="inline">PRES</strong>, and <strong class="inline">Iws)</strong> predicts the PM2.5 the best.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the randomForest package using the following command:<p class="snippet">library(randomForest)</p></li>
				<li>Now, create a new object using the following command:<p class="snippet">pm25_model_rf &lt;- randomForest(pm2.5 ~ . , data = na.omit(PM25[,c("month","DEWP","TEMP","PRES","Iws","pm2.5")]), ntree=25,importance=TRUE, nodesize=5)</p></li>
				<li>Print the model:<p class="snippet">pm25_model_rf</p><p>The output is as follows:</p><p class="snippet">## </p><p class="snippet">## Call:</p><p class="snippet">##  randomForest(formula = pm2.5 ~ ., data = na.omit(PM25[, c("month",      "DEWP", "TEMP", "PRES", "Iws", "pm2.5")]), ntree = 25, importance = TRUE,      nodesize = 5) </p><p class="snippet">##                Type of random forest: regression</p><p class="snippet">##                      Number of trees: 25</p><p class="snippet">## No. of variables tried at each split: 1</p><p class="snippet">## </p><p class="snippet">##           Mean of squared residuals: 3864.177</p><p class="snippet">##                     % Var explained: 54.39</p></li>
				<li>Find the R-squared value for each tree:<p class="snippet">pm25_model_rf$rsq</p><p>The output is as follows:</p><p class="snippet">##  [1] 0.2917119 0.3461415 0.3938242 0.4240572 0.4335932 0.4445404 0.4552216</p><p class="snippet">##  [8] 0.4735218 0.4878105 0.4998751 0.5079323 0.5156195 0.5197153 0.5228638</p><p class="snippet">## [15] 0.5286556 0.5305679 0.5312043 0.5341559 0.5374104 0.5397305 0.5421712</p><p class="snippet">## [22] 0.5434857 0.5430657 0.5435383 0.5439461</p></li>
				<li>Next, calculate the variable importance plot:<p class="snippet">varImpPlot(pm25_model_rf)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/C12624_06_08.jpg" alt="Figure 6.8: Percentage increase in MSE and increase in node purity value obtained by fitting the randomForest model on the Beijing PM2.5 data&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.8: Percentage increase in MSE and increase in node purity value obtained by fitting the randomForest model on the Beijing PM2.5 data</h6>
			<p>The previous exercise demonstrates another way to look at the variable importance. Instead of the <strong class="inline">party</strong> package, we have used the <strong class="inline">randomForest</strong> package. <strong class="inline">%IncMSE</strong> is computed as described in the following steps:</p>
			<ol>
				<li value="1">Fit random forest (in our case, it's a regression random forest). Compute OOB-MSE and name this <strong class="inline">MSE_Base</strong>.</li>
				<li>For each variable <strong class="inline">j</strong>: permute values of column <strong class="inline">j</strong>, then predict and compute <strong class="inline">OOB_MSE_j</strong>.</li>
				<li><strong class="inline">%IncMSE</strong> of the <strong class="inline">jth</strong> variable equals <strong class="inline">(OOB_MSE_j - MSE_Base)/ MSE_Base * 100%</strong>.</li>
			</ol>
			<p><em class="italics">Figure 6.8</em> shows that inclusion of the variable <strong class="inline">Iws</strong> in the model increases the MSE by <strong class="inline">22%</strong> compared with the variable DEWP, which increases the MSE only by <strong class="inline">15%</strong>. We know that as a result of the shuffled values of the variable, the MSE is bound to increase, so the higher <strong class="inline">%</strong> implies a good variable. If we see the variable <strong class="inline">TEMP</strong>, the shuffling of values has not impacted the MSE that much compared with <strong class="inline">Iws</strong> and <strong class="inline">DEWP</strong>; hence, relatively, it is less important.</p>
			<p>Node purity computes the value of loss function, which in this model is MSE. It helps in choosing the best split. Decrease in the MSE gives a higher node purity value. DEWP has the highest node purity followed by the feature month. In our dataset, both <strong class="inline">%IncMSE</strong> and <strong class="inline">IncNodePurity</strong> show similar results. However, keep in mind that <strong class="inline">IncNodePurity</strong> is often biased and should always be seen in conjunction with <strong class="inline">%IncMSE</strong>.</p>
			<h2 id="_idParaDest-277"><a id="_idTextAnchor280"/>Feature Reduction</h2>
			<p><strong class="bold">Feature reduction</strong> helps get rid of redundant variables that reduce the model efficiency in the following ways:</p>
			<ul>
				<li>Time to develop/train the model increases.</li>
				<li>Interpretation of the results becomes tedious.</li>
				<li>It inflates the variance of the estimates.</li>
			</ul>
			<p>In this section, we will see three feature reduction techniques that help in improving the model efficiency.</p>
			<h3 id="_idParaDest-278"><a id="_idTextAnchor281"/>Principal Component Analysis (PCA)</h3>
			<p>N. A. Campbell and William R. Atchley in their classic paper, <em class="italics">The Geometry of Canonical Variate Analysis, </em>Systematic Biology, Volume 30, Issue 3, September 1981, Pages 268–280, geometrically defined <em class="italics">a principal component analysis as a rotation of the axes of the original variable coordinate system to new orthogonal axes, called principal axes, such that the new axes coincide with directions of maximum variation of the original observation</em>. This forms the crux of what PCA does. In other words, it represents the original variable with principal components that explain the maximum variation of the original observations or data.</p>
			<p>The paper elegantly presents the geometrical representation of principal components as shown in the following figure, which is a representation of the scatter diagram for two variables, showing the mean for each variable <img src="image/C12624_06_34.png" alt=""/>, <em class="italics">95%</em> concentration ellipse, and principal axes <img src="image/C12624_06_35.png" alt=""/> and <img src="image/C12624_06_36.png" alt=""/>. The points <img src="image/C12624_06_37.png" alt=""/> and <img src="image/C12624_06_38.png" alt=""/> give the principal component scores for the observation <img src="image/C12624_06_39.png" alt=""/> = . The cosine of the angle <img src="image/C12624_06_40.png" alt=""/> between <img src="image/C12624_06_41.png" alt=""/> and <img src="image/C12624_06_42.png" alt=""/> gives the first component <img src="image/C12624_06_43.png" alt=""/> of the eigenvector corresponding to <img src="image/C12624_06_44.png" alt=""/>. </p>
			<p>In linear algebra, an eigenvector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it.</p>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/C12624_06_09.jpg" alt=""/>
				</div>
			</div>
			<h6>Figure 6.9: Shows the representation of the scatter diagram for two variables, showing the mean for each variable (x ̅_1 and x ̅_2), 95% concentration ellipse, and principal axes Y_1 and Y_2</h6>
			<h6>Source: The Geometry of Canonical Variate Analysis, Systematic Biology, Volume 30, Issue 3, September 1981, Pages 268–280</h6>
			<h3 id="_idParaDest-279"><a id="_idTextAnchor282"/>Exercise 85: Performing PCA</h3>
			<p>In this exercise, we will perform PCA, which will help reduce the dimensionality of the feature space. In other words, fewer principal components that are the linear combination of input features will represent the entire dataset.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the <strong class="inline">OzoneData</strong> package:<p class="snippet">dim(OzoneData)</p><p>The output is as follows:</p><p class="snippet">## [1] 366  13</p></li>
				<li>Print the column name using the <strong class="inline">colnames</strong> function:<p class="snippet">colnames(OzoneData)</p><p>The output is as follows:</p><p class="snippet">##  [1] "Month"                 "Day_of_month"         </p><p class="snippet">##  [3] "Day_of_week"           "ozone_reading"        </p><p class="snippet">##  [5] "pressure_height"       "Wind_speed"           </p><p class="snippet">##  [7] "Humidity"              "Temperature_Sandburg" </p><p class="snippet">##  [9] "Temperature_ElMonte"   "Inversion_base_height"</p><p class="snippet">## [11] "Pressure_gradient"     "Inversion_temperature"</p><p class="snippet">## [13] "Visibility"</p><p class="snippet">## [1] 50  4</p></li>
				<li>Find the means for all variables:<p class="snippet">apply(OzoneData,2,mean)</p><p>The output is as follows:</p><p class="snippet">##                 Month          Day_of_month           Day_of_week </p><p class="snippet">##              6.513661             15.756831              4.002732 </p><p class="snippet">##         ozone_reading       pressure_height            Wind_speed </p><p class="snippet">##             11.582020           5752.448016              4.868852 </p><p class="snippet">##              Humidity  Temperature_Sandburg   Temperature_ElMonte </p><p class="snippet">##             58.295691             61.858629             57.219990 </p><p class="snippet">## Inversion_base_height     Pressure_gradient Inversion_temperature </p><p class="snippet">##           2596.265137             17.785440             61.005339 </p><p class="snippet">##            Visibility </p><p class="snippet">##            123.300546</p></li>
				<li>Find the variance of all variables:<p class="snippet">apply(OzoneData,2,var) </p><p>The output is as follows:</p><p class="snippet">##                 Month          Day_of_month           Day_of_week </p><p class="snippet">##          1.194365e+01          7.785578e+01          3.991773e+00 </p><p class="snippet">##         ozone_reading       pressure_height            Wind_speed </p><p class="snippet">##          6.243605e+01          1.092618e+04          4.481383e+00 </p><p class="snippet">##              Humidity  Temperature_Sandburg   Temperature_ElMonte </p><p class="snippet">##          3.861494e+02          2.039533e+02          1.109866e+02 </p><p class="snippet">## Inversion_base_height     Pressure_gradient Inversion_temperature </p><p class="snippet">##          3.115312e+06          1.300448e+03          1.871074e+02 </p><p class="snippet">##            Visibility </p><p class="snippet">##          6.444901e+03</p><p>Significant differences in variance of the variables will control the principal components. <strong class="inline">prcomp()</strong> will standardize the variables (mean <strong class="inline">0</strong> and variance <strong class="inline">1</strong>) before finding out the principal component.</p><p class="snippet">pca.out&lt;-prcomp(OzoneData,scale=TRUE)</p></li>
				<li>Next, find the summary of the PCA:<p class="snippet">summary(pca.out)</p><p>The output is as follows:</p><p class="snippet">## Importance of components:</p><p class="snippet">##                           PC1    PC2     PC3     PC4     PC5     PC6</p><p class="snippet">## Standard deviation     2.2817 1.4288 1.05944 1.01842 1.00160 0.93830</p><p class="snippet">## Proportion of Variance 0.4005 0.1570 0.08634 0.07978 0.07717 0.06772</p><p class="snippet">## Cumulative Proportion  0.4005 0.5575 0.64386 0.72364 0.80081 0.86853</p><p class="snippet">##                            PC7     PC8     PC9    PC10    PC11    PC12</p><p class="snippet">## Standard deviation     0.74291 0.64513 0.54523 0.48134 0.33068 0.25908</p><p class="snippet">## Proportion of Variance 0.04246 0.03202 0.02287 0.01782 0.00841 0.00516</p><p class="snippet">## Cumulative Proportion  0.91099 0.94301 0.96587 0.98369 0.99211 0.99727</p><p class="snippet">##                           PC13</p><p class="snippet">## Standard deviation     0.18840</p><p class="snippet">## Proportion of Variance 0.00273</p><p class="snippet">## Cumulative Proportion  1.00000</p></li>
				<li>Create a biplot using the <strong class="inline">ggbiplot</strong> function:<p class="snippet">library(devtools)</p><p class="snippet">install_github("vqv/ggbiplot", force=TRUE)</p><p class="snippet">library(ggbiplot)</p><p class="snippet">ggbiplot(pca.out)</p><p>The output is as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/C12624_06_10.jpg" alt="Figure 6.10 Scaled biplot of the first two principle components using ggbiplot&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.10 Scaled biplot of the first two principle components using ggbiplot</h6>
			<p>The biplot in the figure shows how <strong class="bold">PC1</strong> and <strong class="bold">PC2</strong> are the linear combination of features in the <strong class="inline">Ozone</strong> dataset. As shown in the output of <strong class="inline">summary(pca.out)</strong>, biplots depict the explained variance by using the various features in the dataset. The axes are seen as arrows originating from the center point. The figure also shows that the variables <strong class="inline">pressure_height</strong> and <strong class="inline">inversion_temperature</strong> contribute to <strong class="bold">PC1</strong>, with higher values in those variables moving the samples to the right on this plot. <strong class="inline">Visibility</strong> and <strong class="inline">day_of_the_week</strong> contribute to <strong class="bold">PC2</strong> with higher values.</p>
			<p>If you find difficulty installing <strong class="inline">ggbiplot</strong>, you could also use the <strong class="inline">biplot()</strong> function from base R, as shown in the following plot. First, let's build a biplot to understand better:</p>
			<p class="snippet">biplot(pca.out,scale = 0, cex=0.65)</p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/C12624_06_11.jpg" alt="Figure 6.11 Scaled biplot of the first principle component&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.11 Scaled biplot of the first principle component</h6>
			<p>Observe that the maximum percentage of variance is explained by PC1 and all PCs are mutually uncorrelated. In particular, around <strong class="inline">40%</strong> of the variance is explained by PC1, and the first principal component (PC1-PC4) explains 70% of the variance. In other words, if we use the first four principal components, we should get a model almost similar to the one we would get when we use all the variables in the dataset. This should not be surprising as the principal component is a linear combination of the variables.</p>
			<h2 id="_idParaDest-280"><a id="_idTextAnchor283"/>Variable Clustering</h2>
			<p><strong class="bold">Variable clustering</strong> is used for measuring collinearity, calculating redundancy, and for separating variables into clusters that can be counted as a single variable, thus resulting in data reduction. Hierarchical cluster analysis on variables uses any one of the following: Hoeffding's D statistics, squared Pearson or Spearman correlations, or uses as a similarity measure the proportion of observations for which two variables are both positive. The idea is to find the cluster of correlated variables that are correlated with themselves and not with variables in another cluster. This reduces a large number of features into a smaller number of features or variable clusters.</p>
			<h3 id="_idParaDest-281"><a id="_idTextAnchor284"/>Exercise 86: Using Variable Clustering</h3>
			<p>In this exercise, we will use feature clustering for identifying a cluster of similar features. From each cluster, we can select one or more features for the model. We will use the hierarchical cluster algorithm from the Hmisc package in R. The similarity measure should be set to "spear," which stands for the Pearson correlation, a robust measure for computing the similarity between two observations.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Install the <strong class="inline">Hmisc</strong> package using the following command:<p class="snippet">install.packages("Hmisc")</p></li>
				<li>Import the <strong class="inline">Hmisc</strong> package and set the seed to <strong class="inline">1</strong>:<p class="snippet">library(Hmisc)</p><p class="snippet">set.seed(1)</p></li>
				<li>Use variable clustering with Spearman correlation as the similarity measure:<p class="snippet">Ozone_var_clust &lt;- varclus(as.matrix(OzoneData), similarity="spear")</p><p class="snippet">Ozone_var_clust</p><p>The output is as follows:</p><p class="snippet">## varclus(x = as.matrix(OzoneData), similarity = "spear")</p><p class="snippet">## </p><p class="snippet">## </p><p class="snippet">## Similarity matrix (Spearman rho^2)</p><p class="snippet">## </p><p class="snippet">##                       Month Day_of_month Day_of_week ozone_reading</p><p class="snippet">## Month                  1.00         0.00        0.00          0.00</p><p class="snippet">## Day_of_month           0.00         1.00        0.00          0.01</p><p class="snippet">## Day_of_week            0.00         0.00        1.00          0.00</p><p class="snippet">## ozone_reading          0.00         0.01        0.00          1.00</p><p class="snippet">## pressure_height        0.12         0.00        0.00          0.36</p><p class="snippet">## Wind_speed             0.04         0.01        0.00          0.00</p><p class="snippet">## Humidity               0.01         0.00        0.00          0.20</p><p class="snippet">## Temperature_Sandburg   0.05         0.01        0.00          0.63</p><p class="snippet">## Temperature_ElMonte    0.07         0.00        0.00          0.59</p><p class="snippet">## Inversion_base_height  0.00         0.01        0.00          0.32</p><p class="snippet">## Pressure_gradient      0.03         0.00        0.00          0.06</p><p class="snippet">## Inversion_temperature  0.04         0.01        0.00          0.54</p><p class="snippet">## Visibility             0.04         0.02        0.01          0.20</p><p class="snippet">##                       pressure_height Wind_speed Humidity</p><p class="snippet">## Month                            0.12       0.04     0.01</p><p class="snippet">## Day_of_month                     0.00       0.01     0.00</p><p class="snippet">## Day_of_week                      0.00       0.00     0.00</p><p class="snippet">## ozone_reading                    0.36       0.00     0.20</p><p class="snippet">## pressure_height                  1.00       0.02     0.03</p><p class="snippet">## Wind_speed                       0.02       1.00     0.03</p><p class="snippet">## Humidity                         0.03       0.03     1.00</p><p class="snippet">&lt;Output Truncated for brevity&gt;</p><p class="snippet">## hclust results (method=complete)</p><p class="snippet">## </p><p class="snippet">## </p><p class="snippet">## Call:</p><p class="snippet">## hclust(d = as.dist(1 - x), method = method)</p><p class="snippet">## </p><p class="snippet">## Cluster method   : complete </p><p class="snippet">## Number of objects: 13</p></li>
				<li>Print the value:<p class="snippet">print(round(Ozone_var_clust$sim,2))</p><p>The output is as follows:</p><p class="snippet">##                       Month Day_of_month Day_of_week ozone_reading</p><p class="snippet">## Month                  1.00         0.00        0.00          0.00</p><p class="snippet">## Day_of_month           0.00         1.00        0.00          0.01</p><p class="snippet">## Day_of_week            0.00         0.00        1.00          0.00</p><p class="snippet">## ozone_reading          0.00         0.01        0.00          1.00</p><p class="snippet">## pressure_height        0.12         0.00        0.00          0.36</p><p class="snippet">## Wind_speed             0.04         0.01        0.00          0.00</p><p class="snippet">## Humidity               0.01         0.00        0.00          0.20</p><p class="snippet">## Temperature_Sandburg   0.05         0.01        0.00          0.63</p><p class="snippet">## Temperature_ElMonte    0.07         0.00        0.00          0.59</p><p class="snippet">## Inversion_base_height  0.00         0.01        0.00          0.32</p><p class="snippet">## Pressure_gradient      0.03         0.00        0.00          0.06</p></li>
			</ol>
			<p>Based on the similarity matrix, the following figure shows the plot of variables in the same cluster. For example, <strong class="inline">Temperature_ElMonte</strong> and <strong class="inline">Inversion_temperature</strong> are both clustered into one cluster with a Spearman correlation score of 0.85. Similarly, <strong class="inline">Humidity</strong> and <strong class="inline">Pressure_gradient</strong> have a Spearman correlation of 0.25. A high similarity would entail the decision of dropping one of them. In addition to the top of the output of the cluster, one should also consider the model metrics before taking the final call of dropping the variable completely:</p>
			<p class="snippet">plot(Ozone_var_clust)</p>
			<div>
				<div id="_idContainer240" class="IMG---Figure">
					<img src="image/C12624_06_12.jpg" alt="Figure 6.12: Hierarchical cluster of variables in Ozone dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.12: Hierarchical cluster of variables in Ozone dataset</h6>
			<h2 id="_idParaDest-282"><a id="_idTextAnchor285"/>Linear Discriminant Analysis for Feature Reduction</h2>
			<p><strong class="bold">Linear discriminant analysis</strong> (<strong class="bold">LDA</strong>) helps in maximizing the class separation by projecting the data into a new feature space: lower dimensional space with good class separability in order to avoid overfitting (<em class="italics">curse of dimensionality</em>). LDA also reduces computational costs, which makes it suitable as a classification algorithm. The idea is to maximize the distance between the mean of each class (or category) and minimize the variability within the class. (This sounds certainly like how the clustering algorithm in unsupervised learning works, but we will not touch that here as it is not in the scope of this book.) Note that LDA assumes that data follows a Gaussian distribution; if it's not, the performance of LDA will be reduced. In this section, we will use LDA as a feature reduction technique rather than as a classifier.</p>
			<p>For the two-class problem, if we have an <em class="italics">m</em>-dimensional dataset <img src="image/C12624_06_45.png" alt=""/> with <em class="italics">N</em> observations, of which <img src="image/C12624_06_46.png" alt=""/> belongs to class <img src="image/C12624_06_47.png" alt=""/> and <img src="image/C12624_06_48.png" alt=""/> belongs to class <img src="image/C12624_06_49.png" alt=""/>. In this case, we can project the data onto a line (with <em class="italics">C=2</em>, project into <em class="italics">C-1</em> space):</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/C12624_06_50.jpg" alt=""/>
				</div>
			</div>
			<p>Such a projection is achieved by projecting the mean of <em class="italics">X</em> onto the mean of <em class="italics">Y</em>. Of all the lines possible, we would like to select the one that maximizes the separability of the scalars. In other words, where the projections of observation from the same class are projected very close to each other and, at the same time, the projected means are as far apart as possible.</p>
			<p>It should be noted that while in LDA we use the class variable more like supervised learning, PCA does not need any class variable to reduce the feature size. That is why, while LDA preserves as much of the class discriminatory information as possible, PCA does not much care about it.</p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/C12624_06_13.jpg" alt="Figure 6.13: Comparing PCA and LDA&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 6.13: Comparing PCA and LDA</h6>
			<h6>Source: https://sebastianraschka.com/Articles/2014_python_lda.html</h6>
			<h3 id="_idParaDest-283"><a id="_idTextAnchor286"/>Exercise 87: Exploring LDA</h3>
			<p>In this exercise, we will perform LDA for feature reduction. We will observe the difference in the model performance with all the features and the reduced features using LDA.</p>
			<p>Perform the following steps:</p>
			<ol>
				<li value="1">Merge the two DataFrames on timestamp to stack other environmental variables along with PM2.5 into one DataFrame:<p class="snippet">PM25_for_LDA &lt;- merge(PM25_three_hour_pm25_avg, PM25[,c("datetime","TEMP","DEWP","PRES","Iws","cbwd","Is","Ir")], by.x = "timestamp",by.y = "datetime")</p><p class="snippet">PM25_for_LDA = PM25_for_LDA[,c("TEMP","PRES","DEWP","Iws","Is","Ir","pollution_level")]</p></li>
				<li>Split the dataset into train and test:<p class="snippet">index = sample(1:nrow(PM25_for_LDA), round(nrow(PM25_for_LDA)*0.6 ), replace = FALSE)</p><p class="snippet">LDA_train = PM25_for_LDA[ index, ]</p><p class="snippet">LDA_test = PM25_for_LDA[ -index, ]</p></li>
				<li>Import the <strong class="inline">MASS</strong> package:<p class="snippet">library(MASS)</p></li>
				<li>Fit the LDA model on the training dataset:<p class="snippet">LDA_model = lda( pollution_level ~ ., data = LDA_train )</p><p class="snippet">projected_data = as.matrix( LDA_train[, 1:6] ) %*%  LDA_model$scaling</p></li>
				<li>Plot 100 randomly selected projected values:<p class="snippet">set.seed(100)</p><p class="snippet">index &lt;- sample(nrow(projected_data),100, replace = FALSE)</p><p class="snippet">plot( projected_data[index], col = LDA_train[,7], pch = 19 )</p><p>The output is as follows:</p><div id="_idContainer248" class="IMG---Figure"><img src="image/C12624_06_14.jpg" alt="Figure 6.14: Plot of randomly selected 100 projected values&#13;&#10;"/></div><h6>Figure 6.14: Plot of randomly selected 100 projected values</h6></li>
				<li>Perform the model testing:<p class="snippet">LDA_test_reduced = LDA_test[, !( names( LDA_test ) %in% c( "pollution_level" ) ) ]  </p><p class="snippet">LDA_model_results = predict( LDA_model, LDA_test_reduced )</p></li>
				<li>Import the <strong class="inline">caret</strong> library and print the confusion matrix:<p class="snippet">library( caret )</p><p class="snippet">c_t = table( LDA_model_results$class, LDA_test$pollution_level )</p><p class="snippet">print( confusionMatrix( c_t ) )</p><p>The output is as follows:</p><p class="snippet">## Confusion Matrix and Statistics</p><p class="snippet">## </p><p class="snippet">##    </p><p class="snippet">##         0     1</p><p class="snippet">##   0  2359   978</p><p class="snippet">##   1  2257 11108</p><p class="snippet">##                                           </p><p class="snippet">##                Accuracy : 0.8063          </p><p class="snippet">##                  95% CI : (0.8002, 0.8123)</p><p class="snippet">##     No Information Rate : 0.7236          </p><p class="snippet">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">##                                           </p><p class="snippet">##                   Kappa : 0.4704          </p><p class="snippet">##  Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">##                                           </p><p class="snippet">##             Sensitivity : 0.5110          </p><p class="snippet">##             Specificity : 0.9191          </p><p class="snippet">##          Pos Pred Value : 0.7069          </p><p class="snippet">##          Neg Pred Value : 0.8311          </p><p class="snippet">##              Prevalence : 0.2764          </p><p class="snippet">##          Detection Rate : 0.1412          </p><p class="snippet">##    Detection Prevalence : 0.1998          </p><p class="snippet">##       Balanced Accuracy : 0.7151          </p><p class="snippet">##                                           </p><p class="snippet">##        'Positive' Class : 0               </p><p class="snippet">## </p></li>
				<li>Find the dimension-reduced dataset:<p class="snippet">new_LDA_train = as.matrix( LDA_train[,1:6] ) %*%</p><p class="snippet">  LDA_model$scaling</p><p class="snippet">new_LDA_train = as.data.frame( new_LDA_train )</p><p class="snippet">new_LDA_train$pollution_level = LDA_train$pollution_level</p></li>
				<li>Test the dataset:<p class="snippet">new_LDA_test = as.matrix( LDA_test[,1:6] ) %*%</p><p class="snippet">  LDA_model$scaling</p><p class="snippet">new_LDA_test = as.data.frame( new_LDA_test )</p><p class="snippet">new_LDA_test$pollution_level = LDA_test$pollution_level</p></li>
				<li>Use the projected data. Let's fit a logistic model. You could use any other classification model as well:<p class="snippet">PM25_logit_model_on_LDA &lt;- glm(pollution_level ~ ., data = new_LDA_train,family=binomial(link='logit'))</p></li>
				<li>Perform the model evaluation on testing data:<p class="snippet">predicted_LDA = predict(PM25_logit_model_on_LDA, newdata = new_LDA_test,type="response")</p></li>
				<li>Predict 1 if probability &gt; 0.5:<p class="snippet">predicted &lt;- ifelse(predicted_LDA&gt;0.5, 1,0)</p><p class="snippet">actual &lt;- new_LDA_test$pollution_level</p></li>
				<li>Find the confusion matrix:<p class="snippet">confusionMatrix(predicted, actual)</p><p>The output is as follows:</p><p class="snippet">## Confusion Matrix and Statistics</p><p class="snippet">## </p><p class="snippet">##           Reference</p><p class="snippet">## Prediction     0     1</p><p class="snippet">##          0  2316   947</p><p class="snippet">##          1  2300 11139</p><p class="snippet">##                                           </p><p class="snippet">##                Accuracy : 0.8056          </p><p class="snippet">##                  95% CI : (0.7995, 0.8116)</p><p class="snippet">##     No Information Rate : 0.7236          </p><p class="snippet">##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </p><p class="snippet">##                                           </p><p class="snippet">##                   Kappa : 0.4655          </p><p class="snippet">##  Mcnemar's Test P-Value : &lt; 2.2e-16       </p><p class="snippet">##                                           </p><p class="snippet">##             Sensitivity : 0.5017          </p><p class="snippet">##             Specificity : 0.9216          </p><p class="snippet">##          Pos Pred Value : 0.7098          </p><p class="snippet">##          Neg Pred Value : 0.8289          </p><p class="snippet">##              Prevalence : 0.2764          </p><p class="snippet">##          Detection Rate : 0.1387          </p><p class="snippet">##    Detection Prevalence : 0.1954          </p><p class="snippet">##       Balanced Accuracy : 0.7117          </p><p class="snippet">##                                           </p><p class="snippet">##        'Positive' Class : 0               </p><p class="snippet">##</p></li>
			</ol>
			<p>Note that the accuracy in <strong class="inline">LDA_test</strong> and the projected <strong class="inline">new_LDA_test</strong> are strikingly similar. This indicates that the projected values in the new lower dimensional space perform equally well compared with the original. It might always not be the case that the new space will result in the same performance as the original. Therefore, a thorough scrutiny is required before reducing the feature space.</p>
			<h2 id="_idParaDest-284"><a id="_idTextAnchor287"/>Summary</h2>
			<p>In this chapter, we saw various feature selection and reduction techniques. The three main topics covered in this chapter were: Feature Engineering, Feature Selection, and Feature Reduction. The latter two have the same purpose of shrinking the number of features; however, the techniques used are completely different. Feature Engineering focuses on transforming variables into a new form that either helps in improving the model performance or makes the variable be in compliance with model assumption. An example is the linearity assumption in the linear regression model, where we typically could square or cube the variables and the skewness in data distribution, which could be addressed using log transformation. Feature Selection and Feature Reduction help in providing the best feature set or the best representation of the feature set, which improves model performance. Most importantly, both techniques shrink the feature space, which drastically improves the model training time without compromising the performance in terms of accuracy, <strong class="bold">RMSE, </strong>or any relevant model evaluation metric.</p>
			<p>We also saw how some of the models themselves, such as random forest and <strong class="bold">LDA, </strong>could directly be used as feature selection and reduction techniques. While random forest works by selecting the best features through a method of random selection, LDA works by finding the best representation of features. Thus, the former is used in feature selection and the latter in reduction.</p>
			<p>In the next chapter, we will explore more about model improvement, where some of the learnings from this chapter will be employed.</p>
		</div>
	</body></html>