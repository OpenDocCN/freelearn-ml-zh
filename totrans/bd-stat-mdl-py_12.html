<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-180"><a id="_idTextAnchor188"/>12</h1>
<h1 id="_idParaDest-181"><a id="_idTextAnchor189"/>Multivariate Time Series</h1>
<p>The models we discussed in the previous chapter only depended on the previous values of the single variable of interest. Those models are appropriate when we only have a single variable in our time series. However, it is common to have multiple variables in time-series data. Often, these other variables in the series can improve forecasting of the variable of interest. We will discuss models for time series with multiple variables in this chapter. We will first discuss the correlation relationship between time-series variables, then discuss how we can model multivariate time series. While there are many models for multivariate time-series data, we will discuss two models that are both powerful and widely used: <strong class="bold">autoregressive integrated moving average with exogenous variables</strong> (<strong class="bold">ARIMAX</strong>) and <strong class="bold">vector autoregressive</strong> (<strong class="bold">VAR</strong>). Understanding these two models <a id="_idIndexMarker919"/>will extend the<a id="_idIndexMarker920"/> reader’s model toolbox and provide building blocks for the reader to learn more about multivariate time-series models.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>Multivariate time series</li>
<li>ARIMAX</li>
<li>VAR modeling</li>
</ul>
<h1 id="_idParaDest-182"><a id="_idTextAnchor190"/>Multivariate time series</h1>
<p>In <a id="_idIndexMarker921"/>the previous chapter, we discussed models for <strong class="bold">univariate time series</strong> or a<a id="_idIndexMarker922"/> time series of one variable. However, in many modeling situations, it is common to have multiple time-varying variables that are measured together. A time series consisting of multiple time-varying variables is called a <strong class="bold">multivariate time series</strong>. Each variable in the time series is<a id="_idIndexMarker923"/> called a <strong class="bold">covariate</strong>. For example, a time series of weather data might include temperature, rain amount, wind speed, and relative humidity. Each of these variables, in the weather dataset, is a univariate time series, and together, a multivariate time series and each pair of variables are covariates.</p>
<p>Mathematically, we typically represent a multivariate time-series as a vector-valued series, as follows:</p>
<p>X = x 0,0 x 0,1 ⋮ , x 1,0 x 1,1 ⋮ , … , x t,0 x t,1 ⋮ </p>
<p>Here, each X instance consists of multiple values at each time step, and there are t steps in the series. The first index in the preceding equation represents the time step (0 through t), and the second index represents the individual series (0 through N variables). For example, if X were a time series of stock prices, at each time step of X, there would be a value of each stock price. In that case, X might look like the following equation if the first two stock symbols in the series were AAPL and GOOGL:</p>
<p>X =  x 0, AAPL x 0, GOOGL ⋮ ,  x 1, AAPL x 1, GOOGL ⋮ , … ,  x t, APPL x t, GOOGL ⋮ </p>
<p>When we have a<a id="_idIndexMarker924"/> multivariate time series, a question arises: <em class="italic">What can we do with this additional </em><em class="italic">information?</em> To answer that question, we need to understand whether these time-series variables are related. We can determine how time series are related by looking at their cross-correlation. Let’s discuss cross-correlation next.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor191"/>Time-series cross-correlation</h2>
<p>We need to understand how time series <a id="_idIndexMarker925"/>can be related in order to determine whether covariate time series may be useful for forecasting a time series of interest. In a previous chapter, we discussed how to understand the relationship between two variables using plots and correlation coefficients; we will take a similar approach for the time-series data. Recall the following <strong class="bold">cross-correlation function</strong> (<strong class="bold">CCF</strong>) discussed<a id="_idIndexMarker926"/> in <a href="B18945_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a><em class="italic">, Introduction to </em><em class="italic">Time Series</em>:</p>
<p> ˆ p  k(X, Y) =  ∑ t=1 n−k (X t −  _ X )(Y t−k −  _ Y )   ______________________   √ ____________  ∑ t=1 n  (X t −  _ X ) 2  √ ____________  ∑ t=1 n  (Y t −  _ Y ) 2  </p>
<p>Here, X is a univariate time series and Y is a univariate time series. This function tells us how X and Y are related at k lags. In this equation, Y is k time steps behind X. In this situation, we say that X is leading Y by k steps or Y is lagging X by k steps. We will use	 this CCF to help us determine whether two time series are related and at which time steps they are related. Notice that if X = Y, then the <a id="_idIndexMarker927"/>CCF reduces to the <strong class="bold">autocorrelation </strong><strong class="bold">function</strong> (<strong class="bold">ACF</strong>).</p>
<p class="callout-heading">Cross-correlation significance</p>
<p class="callout">Just as with the ACF, cross-correlations<a id="_idIndexMarker928"/> have a threshold for statistical significance. We consider any cross-correlation value with an absolute value greater than 1.96 / √ _ N  (where N is the sample size of the time series) to be significantly different from zero.</p>
<p>Let’s take a look at cross-correlations <a id="_idIndexMarker929"/>with some data. This data [1] comes from the <em class="italic">UCI Machine Learning Repository</em> [2]. The data is weather data that was collected as part of a pollution study conducted in China. This data was sampled hourly over several years. We will subset the data to the first 1,000 data points. The weather data contained in this set consists of temperature (<code>TEMP</code>), pressure (<code>PRES</code>), dew point temperature (<code>DEWP</code>), rain precipitation (<code>RAIN</code>), and wind speed (<code>WSPM</code>). We will look at cross-correlations from the perspective of trying to forecast wind speed, meaning we will look at the cross-correlations of wind speed and the other variables. This may be an important problem for someone who wants to forecast power generation from windmills. The wind speed data and the ACF of the wind speed data are plotted in <em class="italic">Figure 12</em><em class="italic">.1</em>:</p>
<div><div><img alt="Figure 12.1 – Plot of wind speed time series and its ACF" height="519" src="img/B18945_12_001.jpg" width="1018"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Plot of wind speed time series and its ACF</p>
<p>We can see from the plot in <em class="italic">Figure 12</em><em class="italic">.1</em> that the wind speed could potentially be modeled by a stationary univariate model, which we discussed in the previous chapter. While there does not<a id="_idIndexMarker930"/> appear to be evidence of <strong class="bold">autoregressive integrated moving average</strong> (<strong class="bold">ARIMA</strong>) behavior, there does appear to be evidence of seasonality at lag 24. Since the data is sampled hourly, the seasonality is likely a daily pattern. We have plotted the time series of the other variables in <em class="italic">Figure 12</em><em class="italic">.2</em>. These are the other variables we may want to use to help predict the value of wind speed:</p>
<div><div><img alt="Figure 12.2 – Plot of wind speed time series and three other weather variables" height="713" src="img/B18945_12_002.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Plot of wind speed time series and three other weather variables</p>
<p>While we have not plotted the ACF for all the time series shown in <em class="italic">Figure 12</em><em class="italic">.2</em>, these time series have characteristics that suggest they may be stationary. However, it is not clear from the time series alone whether these time series are related. This is where the cross-correlation of time series comes in. We can use a cross-correlation plot to determine whether any of the other time series are related to wind speed. We have plotted the cross-correlation of wind speed and the other variables in <em class="italic">Figure 12</em><em class="italic">.3</em>. The <a id="_idIndexMarker931"/>CCFs are plotted up to 48 lags. Since the data was sampled hourly, this gives us 2 days’ worth of lags to see the cross-correlations:</p>
<div><div><img alt="Figure 12.3 – CCF plots of wind speed and three other variables" height="1212" src="img/B18945_12_003.jpg" width="1092"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – CCF plots of wind speed and three other variables</p>
<p>The plots in <em class="italic">Figure 12</em><em class="italic">.3</em> show that there are cross-correlations between wind speed and the other variables. While we are not too interested in the shapes of the cross-correlations, the patterns may provide some depth to how the variables are related. For example, the cross-correlation of wind speed and temperature shows an oscillating pattern with a period of about 12 hours; this may correspond to the effect of sunlight on wind speed. The primary finding is that these other variables share a relationship with wind speed and may help us forecast the values of wind speed. Let’s take a look at how we can use the additional variables for modeling.</p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor192"/>ARIMAX</h1>
<p>In the<a id="_idIndexMarker932"/> previous chapter, we discussed the ARIMA family of models and demonstrated how to model univariate time-series data. However, as we mentioned in the previous section, many time series are multivariate, such as stock data, weather data, or economic data. In this section, we will discuss how we can incorporate information from covariate variables when modeling time-series data.</p>
<p>When we model a multivariate time series, we typically have a variable we are interested in forecasting. This variable is commonly called<a id="_idIndexMarker933"/> the <strong class="bold">endogenous</strong> variable. The other <a id="_idIndexMarker934"/>covariates in the multivariate time series are called <strong class="bold">exogenous</strong> variables. Recall from <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models,</em> the equation representing the ARIMA model:</p>
<p>y′ t = c + ϕ 1 y′ t−1 + … + ϕ p  y ′  t−p + ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q</p>
<p>Here, y′ t is the differenced series, differenced d times until it is stationary. In this model, y t is the variable we are interested in forecasting; it is the endogenous variable. Let us rewrite this equation in a more compact form. We collect the terms related to y′ t into one summation and the terms related to ϵ t into another summation. Let us start with the y′ t terms:</p>
<p>ϕ 1 y′ t−1 + … + ϕ p  y ′  t−p = ∑ i=1 p ϕ i y′ t−i</p>
<p>Then, we will also collect the ϵ t terms into different summations:</p>
<p>ϵ t + θ 1 ϵ t−1 + … + ϕ q ϵ t−q = ϵ t + ∑ j=1 q ϕ j ϵ t−j</p>
<p>Putting these summations together, we get the following more compact version of our ARIMA equation:</p>
<p>y′ t = c + ∑ i=1 p ϕ i y′ t−i + ϵ t + ∑ j=1 q ϕ j ϵ t−j</p>
<p>Now, let’s add the exogenous variables to the model. We will add the exogenous variables by adding a summation term to the model:</p>
<p>y′ t = c + ∑ i=1 p ϕ i y′ t−i + ϵ t + ∑ j=1 q ϕ j ϵ t−j + ∑ k=1 r β k X tk</p>
<p>Here, there are r exogenous variables contained in the vector-valued variable X. Recall from earlier in the chapter, we can represent the vector-valued variable, as shown next, where the first index represents the time step, and the second index represents an individual series within the vector-valued variable:</p>
<p>X =  x 0,0 ⋮ x 0,k,  x 1,0 ⋮ x 1,k, … ,  x t,0 ⋮ x t,k</p>
<p>This is how<a id="_idIndexMarker935"/> we will incorporate the exogenous variables into the ARIMA model. Now, let’s talk about how we would pick the exogenous variables. In <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models</em>, using an ACF plot, we saw that certain lags of a time series are more influential than others for modeling a univariate time series. We will make similar judgments about exogenous variables using CCF plots. Let’s take a look at example CCF plots and discuss how to make these judgments.</p>
<p>The CCF plot for wind speed and temperature is shown in <em class="italic">Figure 12</em><em class="italic">.4</em>. We have marked where the most significant cross-correlations occur within a lag period of 48 hours. Recall this data is sampled hourly; thus, each lagged time step corresponds to a lag of 1 hour. The plot shows that the most significant cross-correlations occur at approximately lag 13, lag 24, and lag 37:</p>
<div><div><img alt="Figure 12.4 – CCF plot of wind speed and temperature variables" height="642" src="img/B18945_12_004.jpg" width="1076"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – CCF plot of wind speed and temperature variables</p>
<p>Based on the<a id="_idIndexMarker936"/> observation of the cross-correlation plot in <em class="italic">Figure 12</em><em class="italic">.4</em>, we would expect that temperature lags at 13, 24, and 37 could provide useful information for the current predicting wind speed. That is, the variables x t−13,temperature, x t−24,temperature, and x t−37,temperature may be useful for predicting y t. We can perform the same CCF analysis with the other two exogenous variables to determine which lags to use in the model. We found that pressure (<code>PRES</code>) had significant cross-correlations at lags 14 and 37, and dew point (<code>DEWP</code>) had significant cross-correlations at lags 2 and 20. Now that we have assessed which lags of the exogenous variables we want to use in the model, let us go ahead and preprocess the data and fit the model.</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor193"/>Preprocessing the exogenous variables</h2>
<p>Before we<a id="_idIndexMarker937"/> fit the model, we need to preprocess the data to make the exogenous variables we mentioned previously. We need to create lagged versions of the time-series variables we described previously. We can lag a time series using the <code>shift()</code> method of a <code>pandas</code> <code>DataFrame</code>. Let’s take a look at a code example.</p>
<p>The<a id="_idIndexMarker938"/> following code sample shows how to use the <code>shift()</code> method to lag the pressure variable by <code>14</code>. The first several lines of code import the library and load the data. The last line in the code sample shifts the <code>PRES</code> variable backward in time by 14 time steps:</p>
<pre class="source-code">
import pandas as pd
# code to download the data
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00501/PRSA2017_Data_20130301-20170228.zip'
file_name = url.split('/')[-1]
r = requests.get(url)
with open(file_name, 'wb') as fout:
    fout.write(r.content)
with zipfile.ZipFile(file_name, 'r') as zip_file:
    zip_file.extractall('./')
df = pd.read_csv(
    './PRSA_Data_20130301-20170228/PRSA_Data_Aotizhongxin_20130301-20170228.csv'
    , sep=','
)
pres_lag_14 = df.PRES.shift(-14)</pre>
<p>The <code>shift()</code> method can be used to shift a series forward or backward in time. In this case, we want our exogenous variables to lag the variable we want to predict. So, we need to provide a negative shift value to move the series backward in time. We will use a similar preprocessing step for the other exogenous variables to create the other lagged variables. This is shown in the following code sample. All of these lagged variables created with the <code>shift()</code> method are collected into a new <code>DataFrame</code> called <code>X</code>, which we will use in the fit step:</p>
<pre class="source-code">
temp_lag_13 = df.TEMP.shift(-13)
temp_lag_24 = df.TEMP.shift(-24)
temp_lag_37 = df.TEMP.shift(-37)
pres_lag_14 = df.PRES.shift(-14)
pres_lag_24 = df.PRES.shift(-37)
dewp_lag_13 = df.DEWP.shift(-2)
dewp_lag_24 = df.DEWP.shift(-20)
X = pd.DataFrame({
    "temp_lag_13": temp_lag_13[:1000],
    "temp_lag_24": temp_lag_24[:1000],
    "temp_lag_37": temp_lag_37[:1000],
    "pres_lag_14": pres_lag_14[:1000],
    "pres_lag_24": pres_lag_24[:1000],
    "dewp_lag_2": dewp_lag_13[:1000],
    "dewp_lag_20": dewp_lag_24[:1000],
})</pre>
<p>In the<a id="_idIndexMarker939"/> preceding code sample, we are taking a subset of the lagged variables (the first <code>1,000</code> data points) to limit the computation time of the model. With the variables collected in the new <code>X</code> <code>DataFrame</code>, the preprocessing is complete, and we can move on to fitting and assessing the model.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor194"/>Fitting the model</h2>
<p>As in the <a id="_idIndexMarker940"/>previous chapter, we will use <code>auto_arima</code> to fit our ARIMA model. The main difference, in this case, is that we will need to provide the <code>auto_arima</code> function with the exogenous variables we created. The code to fit an ARIMA model with exogenous variables is shown in the following code sample:</p>
<pre class="source-code">
y = df.WSPM[:1000]
arima = pm.auto_arima(
    y, X,
    error_action='ignore',
    trace=True,
    suppress_warnings=True,
    maxiter=5,
    seasonal=False,
    test='adf'
)</pre>
<p>This code should look very similar to the code shown in <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models</em>. In fact, the only difference is the addition is the <code>X</code> variable in the <code>auto_arima</code> function. Running this code will produce the following output:</p>
<pre class="source-code">
Performing stepwise search to minimize aic
 ARIMA(2,0,2)(0,0,0)[0]           : AIC=2689.456, Time=0.24 sec
 ARIMA(0,0,0)(0,0,0)[0]           : AIC=8635.103, Time=0.16 sec
 ARIMA(1,0,0)(0,0,0)[0]           : AIC=2742.728, Time=0.29 sec
 ARIMA(0,0,1)(0,0,0)[0]           : AIC=2923.324, Time=0.32 sec
 ARIMA(1,0,2)(0,0,0)[0]           : AIC=2685.581, Time=0.23 sec
 ARIMA(0,0,2)(0,0,0)[0]           : AIC=2810.239, Time=0.37 sec
 ARIMA(1,0,1)(0,0,0)[0]           : AIC=2683.888, Time=0.23 sec
 ARIMA(2,0,1)(0,0,0)[0]           : AIC=2682.988, Time=0.28 sec
 ARIMA(2,0,0)(0,0,0)[0]           : AIC=2681.309, Time=0.26 sec
 ARIMA(3,0,0)(0,0,0)[0]           : AIC=2682.416, Time=0.32 sec
 ARIMA(3,0,1)(0,0,0)[0]           : AIC=2684.608, Time=0.32 sec
 ARIMA(2,0,0)(0,0,0)[0] intercept : AIC=2683.328, Time=0.30 sec
Best model:  ARIMA(2,0,0)(0,0,0)[0]
Total fit time: 3.347 seconds</pre>
<p>Again, this <a id="_idIndexMarker941"/>output should look very similar to the output we saw from <code>auto_arima</code> in <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models</em>. It indicates we should use an AR(2) model based on the fit using <a id="_idIndexMarker942"/>the <code>auto_arima</code> only needs to fit a single coefficient for each exogenous variable in the model. It does not need to fit an <code>summary()</code> method on the object returned from <code>auto_arima</code> to see the coefficients and significance tests for the exogenous variables. Running the <code>summary()</code> method <a id="_idIndexMarker944"/>will output the following <strong class="bold">seasonal autoregressive integrated moving average with eXogenous factors</strong> (<strong class="bold">SARIMAX</strong>) results:</p>
<div><div><img alt="Figure 12.5 – SARIMAX summary for AR(2) model" height="638" src="img/B18945_12_005.jpg" width="683"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – SARIMAX summary for AR(2) model</p>
<p>The <a id="_idIndexMarker945"/>output from the <code>summary()</code> method shown in the preceding snippet shows a lot of information. Let’s focus on the middle section of the output. The middle section shows the coefficients and significance tests for each exogenous variable and each ARIMA coefficient. The coefficients and test p-values are labeled <code>coef</code> and <code>P&gt;|z|</code>, respectively. The significance test can help us determine whether variables should be included in the model.</p>
<p>Recall back in <a href="B18945_07.xhtml#_idTextAnchor118"><em class="italic">Chapter 7</em></a><em class="italic">, Multiple Linear Regression</em>, we discussed the idea of multicollinearity, which essentially means two or more predictor variables are highly correlated. We could encounter a similar situation here. For example, consider the <code>temp_lag_13</code>, <code>temp_lag_24</code>, and <code>temp_lag_37</code> variables. Since the temperature time series exhibits autocorrelation, it is possible, even likely, that these lagged versions of the temperature time series could be highly correlated. When we observe multicollinearity, it is a sign that we can remove variables from the model. We would recommend using one of the feature selection methods described in <a href="B18945_07.xhtml#_idTextAnchor118"><em class="italic">Chapter 7</em></a>, such as recursive feature selection, performance-based selection, or selection by statistical significance.</p>
<p>The <a id="_idIndexMarker946"/>preceding output shows only three of the exogenous variables in the current model, which are highlighted. This means we should consider reducing the variable in this model. We can do this with multiple methods; for this example, we will choose to eliminate features based on p-values. We will iteratively remove the feature with the highest p-value until all features have a p-value below 0.05. The code for this selection process is shown here:</p>
<pre class="source-code">
# initial model
arima = pm.auto_arima(
    y, X,
    error_action='ignore',
    trace=False,
    suppress_warnings=True,
    maxiter=5,
    seasonal=False,
    test='adf'
)
pvalues = arima.pvalues()
iterations = 0
while (pvalues &gt; 0.05).any():
    # get the variable name with the largest p-value
    variable_with_max_pval = pvalues.idxmax()
    # drop that variable from the exogenous variables
    X = X.drop(variable_with_max_pval, axis=1)
    arima = pm.auto_arima(
        y, X,
        error_action='ignore',
        trace=False,
        suppress_warnings=True,
        maxiter=5,
        seasonal=False,
        test='adf'
    )
    pvalues = arima.pvalues()
    print(f"fit iteration {iterations}")
    iterations += 1</pre>
<p>Performing this selection process will result in a model only containing the <code>temp_lag_24</code>, <code>temp_lag_37</code>, <code>pres_lag_24</code>, and <code>dewp_lag_2</code> exogenous variables. The<a id="_idIndexMarker947"/> ARIMA portion of the model is still an AR(2) model. With a model selection taken, let’s assess the model’s performance.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor195"/>Assessing model performance</h2>
<p>We will <a id="_idIndexMarker948"/>assess the performance of the model by plotting the forecast versus the actual data points and calculating an error estimate. Let us start by looking at the forecasts from the ARIMAX model for the next <code>200</code> observations. The forecasts and test data points are plotted in <em class="italic">Figure 12</em><em class="italic">.6</em>:</p>
<div><div><img alt="Figure 12.6 – Forecast of wind speed using an ARIMAX model" height="389" src="img/B18945_12_006.jpg" width="660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6 – Forecast of wind speed using an ARIMAX model</p>
<p>We can see the forecast for wind speed using the ARIMAX model we created in <em class="italic">Figure 12</em><em class="italic">.6</em>. Two observations stand out from the plotted forecast:</p>
<ul>
<li>The forecast captures the oscillation of the <code>windspeed</code> variable</li>
<li>The forecast fails to capture the high spikes in <code>windspeed</code></li>
</ul>
<p>To understand whether the exogenous variables provided material value in the model, we should compare the forecasts of the ARIMAX model, shown in <em class="italic">Figure 12</em><em class="italic">.6</em>, to forecasts from a univariate model. The forecasts from a univariate model are shown i<em class="italic">n </em><em class="italic">Figure 12</em><em class="italic">.7</em>:</p>
<div><div><img alt="Figure 12.7 – Forecast of wind speed using an ARIMA model" height="378" src="img/B18945_12_007.jpg" width="625"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 – Forecast of wind speed using an ARIMA model</p>
<p>The forecasts of the ARIMA model, shown in <em class="italic">Figure 12</em><em class="italic">.7</em>, do not appear to capture much variation in the time series as the forecast length grows. This is actually a property of ARMA models. It should be evident from the two plots that the ARIMAX model provides a better forecast than the ARIMA model. Finally, let us compare the <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) of the<a id="_idIndexMarker949"/> forecasts of the two models. The MSE from the ARIMAX model is approximately 1.5, while the MSE from the ARIMA model is about 1.7. The MSE value <a id="_idIndexMarker950"/>confirms what we saw in the plots—namely, the ARIMAX model provides a better forecast for this multivariate time series.</p>
<p>In this section, we discussed an extension to the ARIMA model that allows us to incorporate information from covariates in a multivariate time series. This is only one of many methods for multivariate time series. In the next section, we will discuss another method for working with multivariate time-series data.</p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor196"/>VAR modeling</h1>
<p>The<a id="_idIndexMarker951"/> AR(p), MA(q), ARMA(p,q), ARIMA(p,d,q)m, and SARIMA(p,d,q) models we looked at in the last chapter form the basis of multivariate VAR modeling. In this chapter, we have discussed ARIMA with exogenous variables (ARIMAX). We will now begin discussion on the VAR model. First, it is important to understand that while <strong class="bold">ARIMAX requires leading (future) values of the exogenous variables</strong>, <strong class="bold">no future values of these variables are required for the VAR model</strong> as they are all autoregressive to each other – hence the name vector autoregressive – and by definition not exogenous. To start, let us consider the two-variable, or bivariate, case. Consider a process y t that is the output of two different input variables, y t1 and y t2. Note that in matrix form, we are discussing the case of an nxm matrix (y n,m) where <em class="italic">n</em> corresponds to the point in <strong class="bold">time</strong> and <em class="italic">m</em> corresponds to the <strong class="bold">variables</strong> involved (variables 1,2, … , m). We exclude the comma from notation going forward, but it is important to understand when we discuss multivariate autoregressive processes that we are discussing data within a multidimensional matrix rather than only the single-dimensional vector form we used for univariate time-series  analysis. We can write our process, y t, in vector notation as follows:</p>
<p>y t = [y t1 y t2].</p>
<p>We can expand on the input variables’ definitions as a VAR(1) process, here, for the two variables in y t shown previously:</p>
<p>y t1 = (1 − ϕ 11) μ 1 − ϕ 12 μ 2 + ϕ 11 y t−11 + ϕ 12 y t−12 + ε t1</p>
<p>y t2 = − ϕ 21 μ 1 + (1 − ϕ 22) μ 2 + ϕ 21 y t−11 + ϕ 22 y t−12 + ε t2.</p>
<p>The<a id="_idIndexMarker952"/> terms (1 − ϕ 11) μ 1 − ϕ 12 μ 2 and − ϕ 21 μ 1 − (1 − ϕ 22) μ 2 are our <strong class="bold">model constants</strong> (<em class="italic">β</em> coefficients).</p>
<p>We can reduce the previous processes (using a zero-mean form) into the matrix-reduced form, like so:</p>
<p>y t = Φ 1 y t−1 + ε t</p>
<p>Here, we have the following:</p>
<p>Φ 1 = [ϕ 11 ϕ 12 ϕ 21 ϕ 22]</p>
<p>Our forecast for a VAR(1) model is shown here:</p>
<p> ˆ y  t 01(l) = (1 − ϕ 11)  _ y  1 − ϕ 12  _ y  2 + ϕ 11  ˆ y  t 01(l − 1) + ϕ 12  ˆ y  t 02(l − 1)</p>
<p> ˆ y  t 02(l) = − ϕ 21  _ y  1 + (1 − ϕ 22)  _ y  2 + ϕ 21  ˆ y  t 01(l − 1) + ϕ 22  ˆ y  t 02(l − 1)</p>
<p>It also includes this:</p>
<p> ˆ y  t 0(l) = [ ˆ y  t 01(l)  ˆ y  t 02(l)]</p>
<p>Here, <em class="italic">l</em> represents the forward lag (t + 1, t + 2, … , t + n),  ˆ y  t 0m represents the forecasted value at time <em class="italic">t</em> for variable <em class="italic">m</em>, and t 0 corresponds to the most recent point in time (often, the length of the data). It is also important to note the mxm covariance matrix is this:</p>
<p> ˆ Γ (k) =   ˆ γ  11(k) …  ˆ γ  1m(k)  ⋮ ⋱ ⋮   ˆ γ  m1(k) …  ˆ γ  mm(k)</p>
<p>Here, each covariance,  ˆ γ  ij(k), with <em class="italic">k</em> corresponding to the lag, is calculated as follows:</p>
<p> ˆ γ  ij(k) =  1 _ n  ∑ t=1 n−k (y ti −  _ y  i)(y t+k,j −  _ y  j).</p>
<p>Our estimated cross-correlation between each variable y ti and y tj is therefore calculated as follows:</p>
<p> ˆ ρ  ij(k) =   ˆ γ  ij(k) _ √ ___________  ˆ γ  ii(0)  ˆ γ  jj(0)  </p>
<p>Understanding the <a id="_idIndexMarker953"/>calculations for cross-correlation and, especially, covariance helps with understanding the relationships between the variables and their impact on model fit. The method for combining the two models to generate one overall process forecast,  ˆ y  t, is through weighting the two input means and their correlations into one response. As we can see in our VAR(1) forecast, shown previously, the correlations between each given variable and the other variables across different points in time are able to be modeled using a VAR model. Notably, there are no truly independent and dependent variables in a VAR model; there is simply an interaction between each variable. It is important to note that all processes modeled with a VAR model should be <strong class="bold">stationary</strong>.</p>
<p>Using the <em class="italic">United States Macroeconomic</em> dataset found at <a href="https://www.statsmodels.org/dev/datasets/generated/macrodata.xhtml">https://www.statsmodels.org/dev/datasets/generated/macrodata.xhtml</a>, we can observe the relationship between real gross private domestic investment (<code>realinv</code>), real personal consumption expenditures (<code>realcons</code>), and real private disposable income (<code>realdpi</code>) with a zero-mean form in the context of VAR modeling, as follows:</p>
<p>realcons(t) = ϕ 11 realcons t−11 + ϕ 12 realinv t−12 + ϕ 13 realdpi t−13 + ε t1</p>
<p>realinv(t) = ϕ 21 realcons t−11 + ϕ 22 realinv t−12 + ϕ 23 realdpi t−13 + ε t2</p>
<p>realdpi(t) = ϕ 31 realcons t−11 + ϕ 32 realinv t−12 + ϕ 33 realdpi t−13 + ε t3</p>
<p>Let us look at an example using this data in the VAR process in Python with the <code>statsmodels</code> VARMAX model. First, let’s load the data:</p>
<pre class="source-code">
import numpy as np
import pandas as pd
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
data=sm.datasets.macrodata.load_pandas().data
data.sort_values(by=['year', 'quarter'], inplace=True)
data['yr_qtr'] = data['year'].astype(str) + data['quarter'].astype(str)
print('{}% of yearly quarters are unique'.format(round(100*(data['yr_qtr'].nunique() / len(data)), 1)))</pre>
<p>We can see here from the output that 100% of our yearly quarters are unique and thus, no timestamp duplication exists:</p>
<p><code>100.0% of yearly quarters </code><code>are unique</code></p>
<p>One of the<a id="_idIndexMarker954"/> first things we always need to do when performing regression-based time-series modeling is to ensure stationarity. In VAR modeling, this means all variables must be stationary. The first step is to visualize the process realizations through line plotting by index. If data points are aggregated at an interval desired to be modeled, then we can leave the index as is, but if a different time indexing is desired, we need to specify this. Here, we are going to build a model to determine how <code>quarter</code> to the length of the data since that is the smallest denomination of time and there is one yearly quarter per index, as we checked previously. We then set the index to <code>quarter</code> (this is why <code>quarter</code> appears twice; once as the index and once as the column name):</p>
<pre class="source-code">
data['quarter'] = range(1, len(data)+1)
data.drop('yr_qtr', axis=1, inplace=True)
data.index = data['quarter']</pre>
<p>Note the updated index and <code>quarter</code> values:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-7">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">quarter</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">year</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">quarter</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">realcons</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">realinv</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">realdpi</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1959</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1707.4</p>
</td>
<td class="No-Table-Style">
<p>286.898</p>
</td>
<td class="No-Table-Style">
<p>1886.9</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>1959</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>1733.7</p>
</td>
<td class="No-Table-Style">
<p>310.859</p>
</td>
<td class="No-Table-Style">
<p>1919.7</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>1959</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>1751.8</p>
</td>
<td class="No-Table-Style">
<p>289.226</p>
</td>
<td class="No-Table-Style">
<p>1916.4</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>1959</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>1753.7</p>
</td>
<td class="No-Table-Style">
<p>299.356</p>
</td>
<td class="No-Table-Style">
<p>1931.3</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>1960</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>1770.5</p>
</td>
<td class="No-Table-Style">
<p>331.722</p>
</td>
<td class="No-Table-Style">
<p>1955.5</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 – First five rows of macrodata data set for VAR modeling</p>
<p>Here, we will walk through six steps to produce a VAR model using the dataset we imported and prepared, shown partially in <em class="italic">Figure 12</em><em class="italic">.8</em>.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor197"/>Step 1 – visual inspection</h2>
<p>In the line<a id="_idIndexMarker955"/> plot of the data in <em class="italic">Figure 12</em><em class="italic">.9</em>, we can see what appears to be a strong linear trend. We need to check the ACF plots to assess this. After, we <a id="_idIndexMarker956"/>will run a <strong class="bold">Dickey-Fuller test</strong> to confirm:</p>
<div><div><img alt="Figure 12.9 – Line plots of realcons, realdpi, and realinv" height="537" src="img/B18945_12_009.jpg" width="809"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 – Line plots of <em class="italic">realcons</em>, <em class="italic">realdpi</em>, and <em class="italic">realinv</em></p>
<p>The ACF plots in <em class="italic">Figure 12</em><em class="italic">.10</em> show a strong indication of a linear trend and at least one unit root, which likely gives us that trend:</p>
<div><div><img alt="Figure 12.10 – ACF plots for VARMAX input variables" height="289" src="img/B18945_12_010.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 – ACF plots for VARMAX input variables</p>
<p>After running a Dickey-Fuller test, we can see that each variable contains a unit root and is thus trended. Therefore, we need to apply a first-order difference to all three variables so that we can work toward stationarizing the data. Recall that the <strong class="bold">null hypothesis for the Dickey-Fuller is that a unit root exists</strong>, and the alternative hypothesis is that one does not. We’ll execute the following code:</p>
<pre class="source-code">
from statsmodels.tsa.stattools import adfuller
for col in ['realcons','realinv','realdpi']:
    adfuller_test = adfuller(data[col], autolag='AIC')
    print('ADF p-value for {}: {}'.format(col, adfuller_test[1]))</pre>
<p>Here we can see the Dickey-Fuller results for each variable. Recall the null hypothesis is the presence of a single unit root. Therefore, since the p-values are not low, we will not reject the null hypothesis. This gives us statistical evidence to assume the presence of trend:</p>
<p><code>ADF p-value for </code><code>realcons: 0.9976992503412904</code></p>
<p><code>ADF p-value for </code><code>realinv: 0.6484956579101141</code></p>
<p><code>ADF p-value for </code><code>realdpi: 1.0</code></p>
<p>Let’s difference the <a id="_idIndexMarker957"/>data here with a loop:</p>
<pre class="source-code">
import numpy as np
data_1d = pd.DataFrame()
for col in ['realcons','realinv','realdpi']:
    data_1d[col] = np.diff(data[col], n=1)</pre>
<h2 id="_idParaDest-190"><a id="_idTextAnchor198"/>Step 2 – selecting the order of AR(p)</h2>
<p>Now, we<a id="_idIndexMarker958"/> can rerun the ACF plots to check whether we still see trending autocorrelation. Because the original line plots did not appear to have seasonality, we will go ahead and plot<a id="_idIndexMarker959"/> the <strong class="bold">partial ACFs</strong> (<strong class="bold">PACFs</strong>) to get an idea of what kind of AR(p) ordering might be useful. Had we suspected seasonality, it might have been useful to check the <strong class="bold">spectral density of the sample autocorrelations to quantify seasonal periodicity</strong>, a process we demonstrated in <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models</em>. When checking the PACF plots, we will use <a id="_idIndexMarker960"/>the <strong class="bold">Yule-Walker method</strong>.</p>
<p>We can see in the ACF plots there is no apparent seasonality. When using a VAR model, all terms are used to estimate all other terms as the estimates for a forecast of any term are developed. In other words, to forecast one, we must forecast all. The reason this matters is that when looking at the PACFs in <em class="italic">Figure 12</em><em class="italic">.11</em>, we can see one variable could arguably be fit with an AR(1) (real investment) model, one with an AR(2) (real disposable income) model, and another with an AR(3) (real consumption) model. Using orders that are significant for each variable is one important aspect of making VAR modeling work.</p>
<p>Another important aspect of selecting ordering in a VAR model is cross-correlation. For example, we may have a model of variables whose highest order is order 3, but if the cross-correlation between the target variable and the input variable is at lag 5, we would most likely need to use lag 5. However, a model using lag 3 or 4 may also be useful. As with any regression model, however, we will always want to check the significance of coefficient <a id="_idIndexMarker961"/>terms following model training:</p>
<div><div><img alt="Figure 12.11 – Differenced VARMAX variables; ACFs and PACFs" height="536" src="img/B18945_12_011.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 – Differenced VARMAX variables; ACFs and PACFs</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor199"/>Step 3 – assessing cross-correlation</h2>
<p>For a VAR<a id="_idIndexMarker962"/> model, cross-correlation is most important between the variables we consider as inputs and the variable(s) we consider our target. This can be done using multiple lags or with the most significant lag if multiple iterations of the same variable using different shifts are a concern for overfitting. We must confirm cross-correlation analysis used in a forecasting model is done with respect to each input variable and the dependent variable. A cross-correlation that shows the input variable as a leading indicator for the target variable is not of major concern; it means historical values of the input are used to predict the target. However, if we have a scenario where the input is a lagging  indicator—meaning the target predicts the input—and we want to use the input to predict the target, we will need to shift the input forward so that the correlation is at lag 0 and trim the data by the amount shifted.</p>
<p>In many cases, there is significant leading and lagging cross-correlation; this may indicate seasonality. In the case of seasonality with a VAR model, an indicator variable should be included with the data that provides a value of 1 associated with the peak in spectral density and a 0 value otherwise. Another special case with cross-correlation is that the input variable has prominent statistical significance but only at a lag far back in time. In this case, it may be appropriate to shift the data forward so that the model does not include terms (autocorrelations) that are not significant at the risk of overfitting. To demonstrate shifting based on the CCF, we will incorporate this into our process, next.</p>
<p>We observe in <em class="italic">Figure 12</em><em class="italic">.12</em> that the strongest correlation between <code>realdpi</code> and <code>realinv</code> is at lag 0, but there are at least two other similarly correlated lags at <em class="italic">x=-27</em> and <em class="italic">x=-37</em> in the plot. As we saw in the <em class="italic">ARIMAX</em> section, we can include multiple shifted versions of the same variables as new variables to improve the predictive power of the model. This would enable us to take advantage of multiple correlations. However, for the purpose of model demonstration, we will not include that as the output from the VAR summary can become large. Also, as we mentioned, including too many lags of the same variable can result in overfitting. It is up to the practitioner to ultimately decide how many lags to use. This <a id="_idIndexMarker963"/>must be done by assessing model fit and bias/variance trade-offs. The function code given here is also in <a href="B18945_10.xhtml#_idTextAnchor160"><em class="italic">Chapter 10</em></a><em class="italic">, Introduction to </em><em class="italic">Time Series</em>:</p>
<pre class="source-code">
# code imported from chapter 10
def plot_ccf(data_a, data_b, lag_lookback, percentile, ax, title=None):
    n = len(data_a)
    ccf = correlate(data_a - np.mean(data_a), data_b - np.mean(data_b), method='direct') / (np.std(data_a) * np.std(data_b) * n)
    _min = (len(ccf)-1)//2 - lag_lookback
    _max = (len(ccf)-1)//2 + (lag_lookback-1)
    zscore_vals={90:1.645,
                 95:1.96,
                 99:2.576}
    markers, stems, baseline = ax.stem(np.arange(-lag_lookback,(lag_lookback-1)), ccf[_min:_max], markerfmt='o', use_line_collection = True)
    z_score_95pct = zscore_vals.get(percentile)/np.sqrt(n) #1.645 for 90%, 1.96 for 95%, and 2.576 for 99%
    ax.set_title(title)
    ax.set_xlabel('Lag')
    ax.set_ylabel('Correlation')
    ax.axhline(y=-z_score_95pct, color='b', ls='--')# Z-statistic for 95% CL LL
    ax.axhline(y=z_score_95pct, color='b', ls='--')# Z-statistic for 95% CL UL
    ax.axvline(x=0, color='black', ls='-');
plot = plot_ccf(data_a=data_1d['realdpi'], data_b=data_1d['realinv'], lag_lookback=50, percentile=95)</pre>
<div><div><img alt="Figure 12.12 – Cross-correlation for realdpi and realinv" height="464" src="img/B18945_12_012.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.12 – Cross-correlation for <em class="italic">realdpi</em> and <em class="italic">realinv</em></p>
<p>In <em class="italic">Figure 12</em><em class="italic">.13</em>, we can see <code>realcons</code> is a leading indicator for <code>realinv</code> with a positive correlation. This means an increase in consumption results in an increase in investment. Nonetheless, we could still use a downshifted version of <code>realinv</code> to predict <code>realcons</code>—if this <a id="_idIndexMarker964"/>were our objective—since both variables are expected to continue indefinitely:</p>
<pre class="source-code">
plot = plot_ccf(data_a=data_1d['realcons'], data_b=data_1d['realinv'], lag_lookback=50, percentile=95)</pre>
<div><div><img alt="Figure 12.13 – Cross-correlation for realcons and realinv" height="467" src="img/B18945_12_013.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.13 – Cross-correlation for <em class="italic">realcons</em> and <em class="italic">realinv</em></p>
<p>To address the one-lag lead <code>realcons</code> has on <code>realinv</code>, let us shift <code>realcons</code> forward one index so that the strongest correlation is at lag 0. This is not required for a VAR model but can be useful in scenarios where the most significant lag is <strong class="bold">much farther back</strong> in time and could result in including an unreasonably high order for our model to include the necessary variable relationships.</p>
<p class="callout-heading">Note on shifting data</p>
<p class="callout">When shifting data, null values will be included in the dataset. Consequently, the dataset must be trimmed by the length of the shift. This may come at the cost of model performance, so it is worth assessing the need to shift by comparing model errors.</p>
<p>After <a id="_idIndexMarker965"/>applying a forward shift to <code>realcons</code> in the following code snippet and rerunning the CCF, we can see in <em class="italic">Figure 12</em><em class="italic">.14</em> the variables’ strongest correlation is now at lag 0:</p>
<pre class="source-code">
data_1d['realcons'] = data_1d['realcons'].shift(1)
data_1d = data_1d.iloc[1:]
plot = plot_ccf(data_a=data_1d['realcons'], data_b=data_1d['realinv'], lag_lookback=50, percentile=95)</pre>
<div><div><img alt="Figure 12.14 – Cross-correlation for realinv and forward-shifted realcons" height="467" src="img/B18945_12_014.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.14 – Cross-correlation for <em class="italic">realinv</em> and forward-shifted <em class="italic">realcons</em></p>
<p>Next, we want to <a id="_idIndexMarker966"/>run a function to append possible values of p and q (AR(p) and MA(q)):</p>
<pre class="source-code">
from statsmodels.tsa.statespace.varmax import VARMAX
import warnings
warnings.simplefilter('error')
results_aic=[]
x_axis_list=[]
for p in range(1,6):
    for q in range(0,6):
        try:
            model = VARMAX(data_1d,
                           order=(p,q),
                           trend='c',
                           enforce_stationarity=True,
                           enforce_invertibility=True)
            results = model.fit()
            results_aic.append(results.aic)
            # x_axis_list.append(p,q,results.aic)
            print('(p,q): ({},{}), AIC: {}'.format(p,q,results.aic))
        except Exception as e:
            # print('(p,q): ({},{}), error: {}'.format(p,q,e))
            Pass</pre>
<p>Here we can see the AIC error for VAR(1,0) and VAR(2,0) models:</p>
<p><code>(p,q): (1,0), </code><code>AIC: 6092.740258665657</code></p>
<p><code>(p,q): (2,0), </code><code>AIC: 6050.444583720871</code></p>
<p>We have two possible<a id="_idIndexMarker967"/> models through the AIC selection method. First, we built a model with <code>p=2</code> and <code>q=0</code>, but after running, we observed that of seven coefficients predicting for <code>realinv</code>, only two were significant (even at the 0.10 level of significance), which is not good. After rerunning using a selection of <code>p=1</code> and <code>q=0</code>, two of the four coefficients were found significant at the 0.05 level of significance and all at the 0.10 level of significance. Therefore, we can conclude the <code>realinv</code>.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor200"/>Step 4 – building the VAR(p,q) model</h2>
<p>We will<a id="_idIndexMarker969"/> skip the model validation steps we walked through in the last chapter as the process would be the same here. What we will do here is re-order the columns so that when we run the <code>get_prediction()</code> function, the results are for the first column. The model will stay the same since all variables are used to predict all other variables. After re-indexing, we will run the <code>VARMAX</code> model with <code>realcons</code> shifted up one, and predict <code>realinv</code>. For our model, we used AR(1) and MA(0). Note that we use <code>trend='c'</code> for <em class="italic">constant</em> as we removed <code>trend</code> when performing differencing; the <code>trend</code> argument is for trend determinism. Trend differencing should be handled outside of the model since not all variables may have unit root trends. Handling trends outside of the model also helps us identify components such as seasonality or significance of the true autocorrelation structure following the removal of <code>trend</code>. It is worthwhile to note that seasonality is not addressed directly with the VAR (and VARMAX) model; indicator (dummy) variables for seasonality should be included.</p>
<p>Regarding<a id="_idIndexMarker970"/> model fit, we can refer to <a href="B18945_11.xhtml#_idTextAnchor174"><em class="italic">Chapter 11</em></a><em class="italic">, ARIMA Models,</em> for information on <code>Dep. Variable</code> list, shown next. Each model variable has listed the coefficients of the other variables’ terms used for predicting each variable. For example, we can observe in <code>Results for equation realcons</code> that <code>realinv</code> does not contribute much predictive capability to <code>realcons</code>, but <code>realdpi</code> does. We can also observe that <code>realcons</code> is significant for predicting <code>realinv</code>. We can also see based on the p-value and confidence interval in<a id="_idIndexMarker975"/> the <code>realinv</code> and <code>realdpi</code>. We can expect to see larger<a id="_idIndexMarker976"/> coefficient values if we have variance in one feature explained by variance in the other:</p>
<pre class="source-code">
model = VARMAX(data_1d.reindex(columns=['realinv','realcons','realdpi']), order=(1,0), trend='c',
 enforce_stationarity=True,enforce_invertibility=True)
results = model.fit()
print(results.summary())</pre>
<div><div><img alt="Figure 12.15 – VAR(1) model output" height="1540" src="img/B18945_12_015.jpg" width="857"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.15 – VAR(1) model output</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor201"/>Step 5 – testing the forecast</h2>
<p>It is <a id="_idIndexMarker977"/>important to note that the prediction for the VAR model is actually quite stable compared to the long-running historical performance. The reason we see such extreme differences at the end of the forecast is that this time period corresponds to <a id="_idIndexMarker978"/>the <strong class="bold">Great Recession</strong>, which lasted from 2007 to 2009. The last five points in our test forecast correspond to the last two quarters of 2008 and the first three quarters of 2009:</p>
<pre class="source-code">
df_pred = results.get_prediction(start=195, end=200).summary_frame(alpha=0.05)
fig, ax = plt.subplots(1,1,figsize=(20,5))
ax.plot(data_1d['realinv'], marker='o', markersize=5)
ax.plot(df_pred['mean'], marker='o', markersize=4)
ax.plot(df_pred['mean_ci_lower'], color='g')
ax.plot(df_pred['mean_ci_upper'], color='g')
ax.fill_between(df_pred.index, df_pred['mean_ci_lower'], df_pred['mean_ci_upper'], color='g', alpha=0.1)
ax.set_title('Test Forecast for VAR(1)')</pre>
<div><div><img alt="Figure 12.16 – Test forecast for VARMAX with p=1" height="345" src="img/B18945_12_016.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.16 – Test forecast for VARMAX with p=1</p>
<p>We can compare our forecast (<em class="italic">mean</em>) to actuals (<code>realinv</code>) for the investment variable, as follows:</p>
<pre class="source-code">
pd.concat([df_pred,data_1d['realinv'].iloc[195:]], axis=1)</pre>
<p>Here in <em class="italic">Figure 12</em><em class="italic">.17</em>, we can <a id="_idIndexMarker979"/>see the VAR (from the <code>VARMAX</code> function) model test forecast:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-5">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><code>mean</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_se</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_ci_lower</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_ci_upper</code></p>
</td>
<td class="No-Table-Style">
<p><code>realinv</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>196</p>
</td>
<td class="No-Table-Style">
<p>-12.4348</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-92.3935</p>
</td>
<td class="No-Table-Style">
<p>67.52398</p>
</td>
<td class="No-Table-Style">
<p>-56.368</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>197</p>
</td>
<td class="No-Table-Style">
<p>-6.26154</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-86.2203</p>
</td>
<td class="No-Table-Style">
<p>73.69722</p>
</td>
<td class="No-Table-Style">
<p>-35.825</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>198</p>
</td>
<td class="No-Table-Style">
<p>-33.5406</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-113.499</p>
</td>
<td class="No-Table-Style">
<p>46.41821</p>
</td>
<td class="No-Table-Style">
<p>-133.032</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>199</p>
</td>
<td class="No-Table-Style">
<p>-53.6481</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-133.607</p>
</td>
<td class="No-Table-Style">
<p>26.31066</p>
</td>
<td class="No-Table-Style">
<p>-299.167</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>200</p>
</td>
<td class="No-Table-Style">
<p>-81.514</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-161.473</p>
</td>
<td class="No-Table-Style">
<p>-1.55526</p>
</td>
<td class="No-Table-Style">
<p>-101.816</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>201</p>
</td>
<td class="No-Table-Style">
<p>-10.1039</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-90.0627</p>
</td>
<td class="No-Table-Style">
<p>69.85483</p>
</td>
<td class="No-Table-Style">
<p>29.72</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.17 – VAR(1) model test forecast</p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor202"/>Step 6 – building the forecast</h2>
<p>We can<a id="_idIndexMarker980"/> observe that because this is a model with an <strong class="bold">autoregressive order p=1</strong>, the forecast <strong class="bold">tends very quickly toward the mean</strong>. Including a higher order would likely result in more model variance, which we could see as a more confident, yet risky forecast. The multivariate AR(1) model produces more bias, which we can observe in <em class="italic">Figure 12</em><em class="italic">.18</em> as the tendency to the mean we mentioned. We can run the following code to generate the data and plot for the forecast:</p>
<pre class="source-code">
df_forecast = results.get_prediction(start=201, end=207).summary_frame(alpha=0.05)
forecast = np.hstack([np.repeat(np.nan, len(data_1d)+1), df_forecast['mean']])
 fig, ax = plt.subplots(1,1,figsize=(20,5))
ax.plot(data_1d['realinv'], marker='o', markersize=5)
ax.plot(forecast, marker='o', markersize=4)
ax.plot(df_forecast['mean_ci_lower'], color='g')
ax.plot(df_forecast['mean_ci_upper'], color='g')
ax.fill_between(df_forecast.index, df_forecast['mean_ci_lower'], df_forecast['mean_ci_upper'], color='g', alpha=0.1)
ax.set_title('Forecast for VAR(1)');</pre>
<div><div><img alt="Figure 12.18 – VAR(1) model forecast, h=7" height="300" src="img/B18945_12_018.jpg" width="1100"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.18 – VAR(1) model forecast, h=7</p>
<p>Here, we can see the<a id="_idIndexMarker981"/> forecasted points and the confidence interval for the forecast:</p>
<pre class="source-code">
df_forecast</pre>
<table class="No-Table-Style _idGenTablePara-1" id="table003-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><code>mean</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_se</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_ci_lower</code></p>
</td>
<td class="No-Table-Style">
<p><code>mean_ci_upper</code></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>202</p>
</td>
<td class="No-Table-Style">
<p>-16.3442</p>
</td>
<td class="No-Table-Style">
<p>40.79603</p>
</td>
<td class="No-Table-Style">
<p>-96.3029</p>
</td>
<td class="No-Table-Style">
<p>63.6146</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>203</p>
</td>
<td class="No-Table-Style">
<p>-10.9571</p>
</td>
<td class="No-Table-Style">
<p>43.69828</p>
</td>
<td class="No-Table-Style">
<p>-96.6041</p>
</td>
<td class="No-Table-Style">
<p>74.68998</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>204</p>
</td>
<td class="No-Table-Style">
<p>-3.24668</p>
</td>
<td class="No-Table-Style">
<p>44.4028</p>
</td>
<td class="No-Table-Style">
<p>-90.2746</p>
</td>
<td class="No-Table-Style">
<p>83.7812</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>205</p>
</td>
<td class="No-Table-Style">
<p>1.301495</p>
</td>
<td class="No-Table-Style">
<p>44.56735</p>
</td>
<td class="No-Table-Style">
<p>-86.0489</p>
</td>
<td class="No-Table-Style">
<p>88.65189</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>206</p>
</td>
<td class="No-Table-Style">
<p>3.576619</p>
</td>
<td class="No-Table-Style">
<p>44.60344</p>
</td>
<td class="No-Table-Style">
<p>-83.8445</p>
</td>
<td class="No-Table-Style">
<p>90.99775</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>207</p>
</td>
<td class="No-Table-Style">
<p>4.650973</p>
</td>
<td class="No-Table-Style">
<p>44.61108</p>
</td>
<td class="No-Table-Style">
<p>-82.7851</p>
</td>
<td class="No-Table-Style">
<p>92.08709</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>208</p>
</td>
<td class="No-Table-Style">
<p>5.147085</p>
</td>
<td class="No-Table-Style">
<p>44.61268</p>
</td>
<td class="No-Table-Style">
<p>-82.2922</p>
</td>
<td class="No-Table-Style">
<p>92.58633</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.19 – VAR(1) model output data</p>
<p>Note that our <a id="_idIndexMarker982"/>results are based on the differenced data. This model tells us the level of statistical significance between variables regarding their respective signals but does not give us forecast values in terms of the original data. To reverse differenced data, we need to estimate a constant of integration and then reverse-difference the data according to that constant or use a different transformation method that allows us to back-transform, such as a logarithmic transformation that can be exponentiated to reverse. Often, the VAR model is used for identifying potential causation between variables identified as highly correlated in a cross-correlation analysis as well as in economic shock analysis.</p>
<p>It can be argued that depending on the level of detail needed in the forecast, differencing and shifting of variables may not be required. However, because this is a fully endogenous model with no exogenous variables, it is important that if we apply differencing to any variables, we must also apply differencing to all variables containing non-stationary behavior. With respect to shifting, it may be more useful to simply apply a higher autoregressive lag order than to apply a shift; the drawback of shifting is we lose samples early on in the process. However, the drawback of using a higher lag order is the inclusion of more variables in the time dimension, which can increase the likelihood of overfitting. Logically, as more variables are included in the model, we must also increase sample size. As with any model, we must apply rigorous cross-validation to ensure performance stability and minimize risk.</p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor203"/>Summary</h1>
<p>In this chapter, we provided an overview of multivariate time-series and how they differ from the univariate case. We then covered the math and intuition behind two popular approaches to solving problems using multivariate time-series models—ARIMAX and the VAR model framework. We walked through examples for each model using a step-by-step approach. This chapter concludes our discussions on time-series analysis and forecasting. At this point, you should be able to identify and assess the statistical properties of time series, transform them as needed, and construct models that are useful for fitting and forecasting both univariate and multivariate cases.</p>
<p>In the next chapter, we will begin our discussion on survival analysis with an introduction to <strong class="bold">time-to-event</strong> (<strong class="bold">TTE</strong>) variables.</p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor204"/>References</h1>
<p>[1] <em class="italic">Liang, X., Zou, T., Guo, B., Li, S., Zhang, H., Zhang, S., Huang, H. and Chen, S. X.</em> (<em class="italic">2015</em>). <em class="italic">Assessing Beijing’s PM2.5 pollution: severity, weather impact, APEC and winter heating</em>. <em class="italic">Proceedings of the Royal Society A, </em><em class="italic">471, 20150257</em>.</p>
<p>[2] <em class="italic">Dua, D. and Graff, C.</em> (<em class="italic">2019</em>). <em class="italic">UCI Machine Learning Repository</em> [<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>]. <em class="italic">Irvine, CA: University of California, School of Information and </em><em class="italic">Computer Science</em>.</p>
</div>
</div>

<div><div><h1 id="_idParaDest-197" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor205"/>Part 5:Survival Analysis</h1>
<p>This part will cover another statistical approach named survival analysis by analyzing a time to event outcome variable. After an introduction of survival analysis and censored data, we will study models with survival responses.</p>
<p>It includes the following chapters:</p>
<ul>
<li><a href="B18945_13.xhtml#_idTextAnchor206"><em class="italic">Chapter 13</em></a>, <em class="italic">Time to Event variables - An introduction</em></li>
<li><a href="B18945_14.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">Survival Models</em></li>
</ul>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
<div><div></div>
</div>
</div></body></html>