- en: Credit Card Fraud Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we built our first anomaly detection model using **Principal
    Component Analysis** (**PCA**) and saw how we can detect cyber attacks using principal
    components. Similar to cyber attack or network intrusion problems, anomaly detection
    models are frequently used for fraud detection. Various organizations in many
    industries, such as financial services, insurance companies, and government agencies,
    often come across fraudulent cases. Especially in financial sectors, frauds are
    directly related to monetary losses and these fraudulent cases can come in many
    different guises, such as stolen credit cards, accounting forgeries, or fake checks.
    Because these events occur relatively rarely, it is difficult and tricky to detect
    these fraudulent cases.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to discuss how we can build an anomaly detection
    model for credit card fraud detection. We are going to use an anonymized credit
    card dataset that contains a large portion of normal credit card transactions
    and relatively fewer fraudulent credit card transactions. We will first look at
    the structure of the dataset, the distribution of the target classes, and the
    distributions of various anonymized features. Then, we are going to start applying
    PCA and building standardized principal components that will be used as features
    for our fraud detection model. In the model building step, we are going to experiment
    with two different approaches to building fraud detection models—the **Principal
    Component Classifier** (**PCC**) that is similar to what we built in [Chapter
    9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber Attack Detection*
    and the one-class **Support Vector Machine** (**SVM**) that learns from normal
    credit card transactions and detects any anomalies. With these models built, we
    are going to evaluate their anomaly detection rates and compare their performances
    for credit card fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for the credit card fraud detection project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for the anonymized credit card dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering and PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The one-class SVM versus the PCC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating anomaly detection models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Credit card fraud is relatively common among other fraudulent events, and can
    happen in our daily lives. There are various ways credit card fraud can happen.
    Credit cards can be lost or stolen and then used by a thief. Another way credit
    card fraud can occur is that your identity might have been exposed to malicious
    persons who then use your identity to open a new credit card account, or even
    take over your existing credit card accounts. Scammers can even use telephone
    phishing for credit card fraud. As there are many ways credit card fraud can happen,
    many credit card holders are exposed to the risk of this type of fraud, and having
    a proper way to prevent them from happening has become essential in our daily
    lives. Many credit card companies have employed various measures to prevent and
    detect these types of fraudulent activities, using various **machine learning**
    (**ML**) and anomaly-detection technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to work on building a credit card fraud detection
    model by using and expanding our knowledge about building anomaly detection models.
    We will be using an anonymized credit card dataset that can be found at the following
    link: [https://www.kaggle.com/mlg-ulb/creditcardfraud/data](https://www.kaggle.com/mlg-ulb/creditcardfraud/data).
    This dataset has about 285,000 credit card transactions, and only about 0.17%
    of those transactions are fraudulent transactions, which reflects a real-life
    situation very well. With this data, we are going to look at how the dataset is
    structured, and then start looking at the distributions of the target and feature
    variables. Then, we will be building features by using PCA, similar to what we
    did in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber
    Attack Detection*. For building credit card fraud detection models, we are going
    to experiment with both the PCC, similar to the one we built in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber
    Attack Detection*, and the one-class SVM, which learns from normal credit card
    transactions and decides whether a new transaction is fraudulent or not. Lastly,
    we are going to look at false alarm and fraud detection rates to evaluate and
    compare the performances of these models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our problem definition for the credit card fraud detection project:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need an anomaly detection model for fraudulent credit
    card transactions that can identify, prevent, and stop potential fraudulent credit
    card activities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? Every credit card holder is exposed to the risks of becoming
    the victim of credit card fraud, and without being properly prepared for such
    malicious attempts, the number of credit card fraud victims is going to increase.
    With a credit card fraud detection model, we can prevent and stop potential fraudulent
    credit card transactions from happening.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem? We are going to use
    anonymized credit card data that is publicly available, and has lots of normal
    credit card transactions and a small number of fraudulent transactions. We are
    going to apply PCA to this data and experiment with the PCC and the one-class
    SVM models for fraud detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? Since any credit card fraud event will result
    in monetary loss, we want a high fraud detection rate. Even if there are some
    false positives or false alarms, it is better to flag any suspicious credit card
    activities to prevent any fraudulent transactions from going through.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for anonymized credit card data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now start looking at the credit card dataset. As mentioned before, we
    are going to use the dataset that is available at the following link: [https://www.kaggle.com/mlg-ulb/creditcardfraud/data](https://www.kaggle.com/mlg-ulb/creditcardfraud/data).
    It is a dataset that contains about 285,000 records of credit card transactions,
    where some of them are fraudulent transactions and the majority of the records
    are normal credit card transactions. Due to confidentiality issues, the feature
    names in the dataset are anonymized. We will be using the `creditcard.csv` file,
    which can be downloaded from the link.
  prefs: []
  type: TYPE_NORMAL
- en: Target variable distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we are going to examine is the distribution of fraudulent and
    non-fraudulent credit card transactions in the dataset. In the dataset, the column
    named `Class` is the target variable that is encoded with `1` for fraudulent credit
    card transactions and `0` for non-fraudulent transactions. You can use the following
    code to first load the data into a Deedle data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset has headers that represent each of the features and the target
    class, so we are loading this data with the `hasHeaders: true` flag. Now that
    we have the data loaded, you can use the following code to analyze the distribution
    of the target classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you might be familiar with this function already, we are using the `AggregateRowsBy`
    function in a Deedle data frame to group rows by the column `Class`, and then
    count the number of records in each target class. Since the column name, `Class`,
    is not a good representative of what our target class is and what it means, we
    renamed it with another name, `is_fraud`. As you can see from this code, you can
    use the `RenameColumns` function with an array of strings for new column names
    to rename the feature names. Lastly, we used the `DataBarBox` class in the Accord.NET
    framework to display a bar plot that visually shows the distributions of the target
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows the distribution of the target classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00163.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this output, there is a large gap between the number of
    fraudulent credit card transactions and non-fraudulent credit card transactions.
    We only have 492 records of frauds and over 284,000 records of non-frauds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a bar plot that the code generates for visually displaying
    the distribution of target classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00164.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected from the previous output, there is a large gap between the number
    of records that belong to the target class, **1**, which represents fraud, and
    the number of records that belong to the target class, **0**, which represents
    non-fraud and normal credit card transactions. This large gap is expected as credit
    card frauds happen relatively rarely, compared to the large number of normal everyday
    credit card transactions. This large class imbalance makes it difficult for most
    ML models to accurately learn how to identify frauds from non-frauds.
  prefs: []
  type: TYPE_NORMAL
- en: Feature distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The features, except for the transactional amounts, we have in this data are
    anonymized due to confidentiality issues. Because we do not know what each feature
    represents and what each feature means, it will be difficult to deduce any intuitive
    insights from the feature analysis. However, it is still helpful to understand
    how each of the features is distributed, how the distribution of each feature
    differs from the others, and whether there is any noticeable pattern we can derive
    from the set of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first take a look at the code. The following code shows how we can compute
    and visualize the distributions of the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we are computing the quartiles. As you might
    recall, quartiles are the points that separate the data into four different sections.
    The first quartile is the middle point between the minimum and the median, the
    second quartile is the median, and the third quartile is the middle point between
    the median and the maximum. You can easily compute the quartiles by using the
    `Accord.Statistics.Measures.Quantiles` function. After we compute the quartiles,
    we build histogram plots for each feature to visualize the distributions, using
    the `HistogramBox` class in the Accord.NET framework. Let's take a look at some
    of the outputs from this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first distribution we are going to look at is for the `V1` feature, and
    the quartiles for `V1` look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems the distribution of the `V1` feature is skewed towards the negative
    direction. Even though the median is about 0, the negative values range from -56.41
    to 0, while the positive values range only from 0 to 2.45\. The following is the
    histogram output from the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the histogram plot shows left skewness in the distribution of the
    feature, `V1`, while the majority of the values are around 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at the distribution of the second feature, `V2`, where the
    output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00167.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The histogram for `V2` looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00168.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It seems the values are centered around 0, although there are some extreme values
    in the negative direction and in the positive direction. The skewness is less
    obvious, compared to the previous feature, `V1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s look at the distribution of the `amount` feature, which can
    tell us the range of transaction amounts. The following are the quartiles for
    the `amount` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00169.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems any credit card transaction can take any positive number that ranges
    between 0 and 25,691.16 as a transaction amount. The following is a histogram
    for the `amount` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00170.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected, we can see there is a long tail to the right. This is somewhat
    expected, as the spending pattern for each individual differs from any other.
    Some people might typically buy moderately priced items, while some others might
    buy very expensive items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s take a brief look at how well the current feature set separates
    fraudulent credit card transactions from non-fraudulent transactions. Let''s take
    a look at the following code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we first convert the Deedle data frame variable,
    `df`, to a two-dimensional array variable, `data`, to build scatter plots. Then,
    we take the first two features and display a scatter plot that shows the distribution
    of the target classes across these first two features. We repeat this process
    twice more for the second, third, and fourth features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatter plot is the distribution of target classes across the
    first and second features in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this scatter plot, it is quite difficult, if not impossible, to separate
    the frauds (encoded as 1) from the non-frauds (encoded as 0). Let''s look at the
    scatter plot between the next two features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00172.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the case of the first two features, there does not seem to be a
    clear line separating frauds from non-frauds. Lastly, the following is a scatter
    plot of the target classes between the third and fourth feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00173.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: From looking at this scatter plot, it will be difficult to draw a clear line
    that separates the two target classes. The fraudulent transactions seem to reside
    more in the bottom-right side of this scatter plot, but the pattern is weak. In
    the following section, we will try to build features that better separate the
    two target classes.
  prefs: []
  type: TYPE_NORMAL
- en: The full code for this data analysis step can be found at the following link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/DataAnalyzer.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have analyzed what the distributions of the target and feature variables
    look like. In this chapter, we are going to focus on building features, using
    PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation for feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to fit the PCA, we will have to prepare our data first. Let''s quickly
    look at the following code to load the credit card fraud data into Deedle''s data
    frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have loaded the data into a variable, named `df`, we are going
    to have to split the data into two sets, one for normal credit card transaction
    data and another for fraudulent transaction data, so that we can fit PCA with
    the normal transactions only. Take a look at the following code for how we can
    separate out the normal transactions from the raw dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you recall from the previous data analysis step, the target variable, `Class`,
    is encoded as 1 for fraudulent transactions and 0 for non-fraudulent transactions.
    As you can see from the code, we created a data frame, `noFraudData`, with only
    normal credit card transaction records. Then, we converted this data frame into
    a two-dimensional double array that will be used to fit the PCA, using the helper
    function, `BuildJaggedArray`. The code for this helper function looks like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code should look familiar, as we have used it in a number of previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we need to do is convert the entire data frame, including both
    non-fraudulent and fraudulent records, into a two-dimensional array. Using the
    trained PCA, we are going to transform this newly created two-dimensional array
    that will later be used for building credit card fraud detection models. Let''s
    take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Fitting a PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to fit a PCA using the non-fraudulent credit card data. Similar
    to what we did in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber
    Attack Detection*, we are going to use the following code to fit a PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, we are using the `PrincipalComponentAnalysis`
    class in the Accord.NET framework to train a PCA. One more thing to note here
    is how we used `PrincipalComponentMethod.Standardize`. Since PCA is sensitive
    to the scales of the features, we are standardizing the feature values first and
    then fitting a PCA. Using this trained PCA, we can transform the whole data that
    contains both fraudulent and non-fraudulent transactions. The code for applying
    PCA transformation to the dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have all the PCA features ready for the following model building step.
    Before we move one, let''s see if we can find any noticeable patterns that can
    separate target classes with the new PCA features. Let''s take a look at the following
    code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Similar to what we did in the data analysis step, we are taking two features
    and creating scatter plots of target classes across the selected features. From
    these plots, we can see if the principal components in the PCA-transformed data
    more effectively separate fraudulent credit card transactions from non-fraudulent
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatter plot applies between the first and second principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00174.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: There is a noticeable cutoff point that separates frauds (red points in the
    scatter plot) from non-frauds (blue points in the scatter plot). From this scatter
    plot, it seems fraudulent samples typically have Y-values (the second principal
    component values) of less than -5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a scatter plot between the second and third principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00175.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The pattern seems to be weaker in this plot, compared to the previous scatter
    plot, but there still seems to be a distinct line that separates many fraud cases
    from non-fraud cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatter plot is between the third and fourth principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00176.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And lastly, the following is a scatter plot between the fourth and fifth principal
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the last two scatter plots, we cannot find a noticeable pattern to separate
    the two target classes from each other. Given that there were some separable lines
    we could find when we looked at the first three principal components and their
    scatter plots, our anomaly detection model for credit card fraud detection will
    be able to learn how to classify frauds, when it learns from this data in a higher
    dimension and multiple principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let''s take a look at the proportion of variance explained by the principal
    components. Take a look at the following code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470),
    *Cyber Attack Detection*, we can use the `Components` property within a `PrincipalComponentAnalysis`
    object to extract the cumulative proportion of variance explained by each component.
    As you can see from the third line in the code, we iterate through the `Components`
    property and extract `CumulativeProportion` values. Then, we display a line chart
    by using the `DataSeriesBox` class. When you run this code, you will see the following
    chart for the cumulative proportion of variance explained by the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this chart, by the twentieth principal component, about
    80% of the variance in the data, is explained. We will use this chart to make
    a decision on how many principal components to use when we build an anomaly detection
    model in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we need to export this data, as we just created a newly PCA-transformed
    dataset in this feature engineering step and we want to use this new data to build
    models. You can export this data by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The full code that was used in this feature engineering step can be found at
    the following link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/FeatureEngineering.cs).
  prefs: []
  type: TYPE_NORMAL
- en: One-class SVM versus PCC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now ready to build anomaly detection models for the credit card fraud
    detection project. In this step, we are going to experiment with two different
    approaches. We are going to build a PCC, similarly to what we did in [Chapter
    9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber Attack Detection*.
    Also, we are going to introduce a new learning algorithm, the one-class SVM, which
    learns from normal credit card transaction data and decides whether a new data
    point is similar to the normal data that it was trained with.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation for model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to load the data that we created in the previous feature-engineering
    step. You can use the following code to load the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If you recall from the previous feature-engineering step, we did not export
    the data with the column names. So, we are loading the data into a Deedle data
    frame, `featuresDF`, with the `hasHeaders` flag set to `false`. Then, we give
    the proper column names for each feature by using the `RenameColumns` method.
    Let''s quickly check the target class distributions within this dataset, using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00179.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As seen previously, in the data analysis step, the majority of the samples belong
    to non-fraudulent transactions and only a small portion of the data is fraudulent
    credit card transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first try to build an anomaly detection model using principal components,
    similar to what we did in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470),
    *Cyber Attack Detection*. For training and testing a PCC model, we wrote a helper
    function, named `BuildPCAClassifier`. The detailed code for this helper function
    can be found at the following repo: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/Modeling.cs).
    Let's take a look at this helper function step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the following lines of code when you look at the code for the
    `BuildPCAClassifier` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: First, we are sub-selecting the first thirteen principal components that explain
    about 50% of the variance. Then, we create a non-fraudulent credit card transaction
    group, `normalDF` and `normalData`, so that we can use this subset to build an
    anomaly detection model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing we do is start computing the **Mahalanobis distance** metric
    to measure the distance between a data point and the distribution of the non-fraudulent
    credit card transactions. If you recall, we used the same distance metric in [Chapter
    9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber Attack Detection*,
    and we recommend you review the *Model building* section in [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber
    Attack Detection*, for a more detailed explanation about this distance metric.
    The code to compute the distances looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should look familiar, as this is the same code we used in [Chapter
    9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber Attack Detection*, to
    build a PCC model for cyber attack detection. Additionally, the following is the
    code for the `ComputeDistances` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code should also look familiar, as we used this same code in [Chapter
    9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber Attack Detection*
    as well. Using these two methods, we computed the mean and the standard deviation
    of the distance measures within the non-fraudulent transaction data. The output
    looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00180.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'With the distance measures within the normal transaction group computed, we
    now compute the distances between the fraudulent transactions and the distribution
    of non-fraudulent transactions. The following is the part of the `BuildPCAClassifier`
    code that computes the distances for frauds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: One-class SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next approach we are going to explore for credit card fraud detection is
    training a one-class SVM. A one-class SVM is a special case of a SVM, where an
    SVM model is first trained with a data and then, when it sees a new data point,
    the SVM model can determine if the new data point is close enough to the data
    that it was trained with. For training a one-class SVM model, we wrote a helper
    function, `BuildOneClassSVM`, and the full code for this function can be found
    at the following repo: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.10/Modeling.cs).
    Let's go through this helper function step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the part of the code that sub-selects non-fraudulent
    credit card transaction data that will be used to train the one-class SVM. The
    code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous PCC model that we built, we are using the first thirteen
    principal components that explain about 50% of the total variance. Next, we are
    going to sub-select records from the non-fraudulent transaction samples and build
    a train set. As you can see from this code, we are randomly selecting 15,000 non-fraudulent
    samples as a train set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a train set to train a one-class SVM model with, let''s take
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using the `OneclassSupportVectorLearning` algorithm in the Accord.NET
    framework to train a one-class SVM model. As you can see, we built an SVM model
    with the `Gaussian` kernel in this chapter, but you can experiment with different
    kernels. Now, the only step left is to test this one-class SVM model that we just
    trained. The following code shows how we built a test set to evaluate this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we are taking all the fraud samples and 5,000
    randomly sub-selected non-fraud samples as a test set. With this test set, we
    are going to evaluate how well this one-class SVM model performs at detecting
    credit card frauds.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to look closer into the evaluation code in the following section,
    but let''s take a quick look at how we can evaluate the performance of the one-class
    SVM model that we just trained. The code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we iterate through different values of thresholds,
    similar to how we set different thresholds for the previous PCC model. As in the
    third line of the code, you can use the `Threshold` property of the model to get
    or set the threshold that determines whether a record is normal or abnormal. Similar
    to how we evaluate the PCC, we are going to look at the fraud detection rate and
    the false-alarm rate for model validations.
  prefs: []
  type: TYPE_NORMAL
- en: The full code we used in the model building step can be found at the following
    link: [https://github.com/yoonhwang/c-sharp-machine-learning/edit/master/ch.10/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/edit/master/ch.10/Modeling.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating anomaly detection models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have trained two anomaly detection models—one using principal components
    and another using a one-class SVM algorithm. In this section, we are going to
    take a closer look at the performance metrics and the codes used to evaluate these
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As briefly mentioned in the previous section, we are going to look at the credit
    card fraud detection rates for each of the target false alarm rates. The code
    for evaluating the PCC model looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Similar to [Chapter 9](part0116.html#3EK180-5ebdf09927b7492888e31e8436526470), *Cyber
    Attack Detection*, we iterate through the target false alarm rates from 5% to
    10% and inspect the detection rates for the given false alarm rates. Using the
    target false alarm rate variable, `targetFalseAlarmRate`, we compute the threshold
    using the `Accord.Statistics.Measures.Quantile` method. With this calculated threshold,
    we flag all records with distances greater than this threshold as fraud, and others
    as non-fraud.  Let's look at the evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the fraud detection rate at the 5% false alarm rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00181.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the fraud detection rate at the 10% false alarm rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00182.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the fraud detection rate at 15% false alarm rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the following is the fraud detection rate at 20% false alarm rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00184.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from these results, as we relax and increase the target false
    alarm rate, the fraud detection rate improves. At the 5% target false alarm rate,
    we could only detect about 59% of the fraudulent transactions. However, at the
    20% target false alarm rate, we can detect over 80% of the fraudulent credit card
    transactions.
  prefs: []
  type: TYPE_NORMAL
- en: One-class SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now take a look at how the one-class SVM model performed on the credit
    card fraud dataset. The code for the model evaluation looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, we iterate through different thresholds from
    -1.0 to 0.0, in increments of 0.1\. You can set the threshold for the model by
    updating the `Threshold` property of the one-class SVM model object. This threshold
    will instruct the model on how to determine which record is fraudulent and which
    is not. When making a decision on the final model, you will need to experiment
    with different values for thresholds to settle on the best threshold that fits
    your requirements. Let's take a look at some of the performance results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the performance metrics for the threshold at -0.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00185.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following shows the performance metrics for the threshold at -0.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following shows the performance metrics for the threshold at -0.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the following shows the performance metrics for the threshold at -0.1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from these results, as we increase the threshold, the false alarm
    rate decreases, but the fraud detection rate decreases as well. It is clear that
    there is a trade-off between higher precision and a higher fraud detection rate.
    At a threshold of -0.4, the model was able to detect about 70% of the fraudulent
    credit card transactions with a roughly 40% false alarm rate. On the other hand,
    at a threshold of -0.1, the model could only detect about 57% of the fraudulent
    credit card transactions, but the false alarm rate was only about 33%.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built another anomaly detection model for credit card fraud
    detection. We started this chapter by looking at the structure of the anonymized
    credit card fraud data, and then started analyzing the distributions of the target
    and feature variables. While we were analyzing the distribution of the target
    classes, we noticed that there was a large class imbalance between the fraud and
    non-fraud classes. This is normal when we face any kind of anomaly detection project,
    where the normal class outweighs by far the positive class. Then, we started analyzing
    the distributions of the anonymized features. Due to the fact that the features
    were anonymized for confidentiality issues, we could not arrive at any intuitions
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: However, we were able to understand the distributions better and how we cannot
    easily separate frauds from non-frauds using raw features. We then applied PCA
    and exported the PCA features for the model building step. We experimented with
    two approaches to building a credit card fraud detection model—the Principal Component
    Classifier and the one-class SVM. We evaluated the performances of these models
    by looking at the fraud detection rates at various false alarm rates. It was clear
    that there are trade-offs between improving the false alarm rates and improving
    the fraud detection rates.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was the last chapter about building ML models in C#. In the next
    chapter, we are going to summarize what we have done so far throughout all the
    chapters, and what additional real-life challenges there are when building ML
    models. Also, we are going to discuss some other software packages, as well as
    some other data-science technologies out there, that can be used for your future
    ML projects.
  prefs: []
  type: TYPE_NORMAL
