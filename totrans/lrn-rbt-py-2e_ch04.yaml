- en: Simulating a Differential Drive Robot Using ROS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at how to model Chefbot. In this chapter,
    we are going to learn how to simulate the robot using the Gazebo simulator in
    ROS. We will learn how to create a simulation model of Chefbot, and we will create
    a hotel-like environment in Gazebo to test our application, which is programmed
    to automatically deliver food to customers. We will look at a detailed explanation
    of each of the steps to test out our application. The following are the important
    topics that we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the Gazebo simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the TurtleBot 2 simulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a simulation of Chefbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URDF tags and plugins for simulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with simultaneous localization and mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing SLAM in a Gazebo environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a map using SLAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with adaptive Monte Carlo localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing AMCL in a Gazebo environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autonomous navigation of Chefbot in a hotel using Gazebo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test the application and codes in this chapter, you need an Ubuntu 16.04
    LTS PC/laptop with ROS Kinetic installed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with the Gazebo simulator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we looked at the basic concepts of the Gazebo simulator
    and its installation procedures. In this chapter, we will learn more about the
    usage of Gazebo and how to simulate a differential drive robot in the Gazebo simulator.
    The first step is to understand the GUI interfaces and its various controls. As
    we have discussed in the first chapter, Gazebo has two main sections. The first
    is the Gazebo server and the second is the Gazebo client. The simulation is done
    on the Gazebo server, which acts as a backend. The GUI is the frontend, which
    acts as the Gazebo client. We will also look at Rviz (ROS Visualizer), which is
    a GUI tool in ROS that is used to visualize different kinds of robot sensor data
    from robot hardware or a simulator, such as Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: We can use Gazebo as an independent simulator to simulate the robot, or we can
    use interfaces with ROS and Python that can be used to program robots in the Gazebo
    simulator. If we are using Gazebo as an independent simulator, the default option
    to simulate the robot is by writing C++-based plugins ([http://gazebosim.org/tutorials/?tut=plugins_hello_world](http://gazebosim.org/tutorials/?tut=plugins_hello_world)).
    We can write C++ plugins for simulating a robot's behavior, creating new sensors,
    creating a new world, and so on. By default, the modeling of robots and environments
    in Gazebo is done using the SDF ([http://sdformat.org/](http://sdformat.org/))
    file. If we are using an ROS interface for Gazebo, we have to create a URDF file
    that contains all the parameters of the robot and has Gazebo-specific tags to
    mention the simulation properties of the robot. When we start the simulation using
    URDF, it will convert to an SDF file using some tools and display the robot in
    Gazebo. The ROS interface of Gazebo is called gazebo-ros-pkgs. It is a set of
    wrappers and plugins that have the ability to model a sensor, robot controller,
    and other simulations in Gazebo and communicate over ROS topics. In this chapter,
    we will be mainly focusing on the ROS-Gazebo interface for simulating Chefbot.
    The advantage of the ROS-Gazebo interface is that we can program the robot by
    making use of the ROS framework. We can program the robot using popular programming
    languages such as C++ and Python using ROS.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not interested in using ROS and want to program the robot using Python,
    you should check out an interface called pygazebo ([https://github.com/jpieper/pygazebo](https://github.com/jpieper/pygazebo)).
    It is a Python binding of Gazebo. In the next section, we will see the GUI of
    Gazebo, along with some of its important controls.
  prefs: []
  type: TYPE_NORMAL
- en: The Gazebo's graphical user interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can start Gazebo in several ways. You have already seen this in *Chapter
    1, Getting Started with Robot Operating System*. In this chapter, we are using
    the following command to start an empty world, meaning that there is no robot
    and no environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will start the Gazebo server and client and load an empty
    world into Gazebo. Here is the view of the empty world in Gazebo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8b1fed3-5755-414e-9712-211259906122.png)'
  prefs: []
  type: TYPE_IMG
- en: Gazebo user interface
  prefs: []
  type: TYPE_NORMAL
- en: 'The Gazebo user interface can be divided into three sections: **Scene**, **Left
    Panel**, and the **Right Panel**.'
  prefs: []
  type: TYPE_NORMAL
- en: The Scene
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Scene is the place where the simulation of the robot takes place. We can
    add various objects to the scene, and we can interact with the robot in the scene
    using the mouse and keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: The Left Panel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can see the left Panel when we launch Gazebo. There are three main tabs
    in the Left Panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '**World**: The **World** tab contains a list of models in the current Gazebo
    Scene. Here, we can modify model parameters, such as the pose, and can also change
    the camera''s pose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insert**: The **Insert** tab allows you to add a new simulation model to
    the Scene. The models are available in the local system, as well as the remote
    server. The `/home/<user_name>/.gazebo/model` folder will keep the local model
    files and models in the remote server in [http://gazebosim.org/models](http://gazebosim.org/models),
    as shown in the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/122b4b47-772b-4175-b19b-87c54b5cc0aa.png)'
  prefs: []
  type: TYPE_IMG
- en: The Insert tab in the left panel of Gazebo
  prefs: []
  type: TYPE_NORMAL
- en: You can see both the local files and remote files in the **Insert** tab that
    is shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: When you start Gazebo for the first time, or when you start a world that has
    models from the remote server, you may see a black screen on Gazebo or a warning
    on the terminal. This is because the model in the remote server is being downloaded
    and Gazebo has to wait a while. The waiting time can vary according to the speed
    of your internet connection. Once the model is downloaded, it will be kept in
    the local model folder, so there will not be any delay the next time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Layers**: Most of the time, we will not use this tab. This tab is for organizing
    the different visualizations available in the simulation. We can hide/unhide the
    models in the simulation by toggling each layer. Most of the time in the simulation,
    this tab will be empty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right Panel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Right panel is hidden by default. We have to drag it in order to view it.
    This panel enables us to interact with the mobile parts of the model. We can see
    the joints of the model if we select the model in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Gazebo toolbars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gazebo has two toolbars. One is above the Scene and one is below it.
  prefs: []
  type: TYPE_NORMAL
- en: Upper toolbar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'he upper toolbar is very useful for interacting with the Gazebo Scene. This
    toolbar is mainly for manipulating the Gazebo Scene. It has functions to select
    the model, scale it, translate and rotate it, and add new shapes to the scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47124ce9-d9b2-49fa-a1aa-44d43dc35a2c.png)'
  prefs: []
  type: TYPE_IMG
- en: Upper toolbar of Gazebo
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows you detailed descriptions of each option:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Select Mode**: If we are in Select Mode, we can select the models in the
    Scene and set their properties, as well as navigate inside the Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translate Mode**: In Translate Mode, we can select a model and translate
    it model by clicking the Left button.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotate Mode**: In Rotate Mode, we can select the model and change its orientation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale Mode**: In Scale Mode, we can select the model and scale it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Undo/Redo**: This enables us to undo or redo actions in the Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simple Shapes**: With this option, we can insert primitive shapes into the
    scene, such as a cylinder, cube, or sphere.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lights**: The Lights option enables us to add different kinds of light sources
    into the Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Copy/Paste**: The Copy and Paste options enable us to copy and paste different
    models and parts of the Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Align**: This enables us to align models to one another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snap**: This snaps one model and moves it inside the Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change view**: This changes the view of the Scene. It mainly uses the perspective
    view and orthogonal view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Screenshot:** This takes a screenshot of the current Scene.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Record Log:** This saves Gazebo''s logs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottom toolbar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bottom toolbar mainly gives us an idea about the simulation. It displays
    the Simulation Time, which refers to the time that is passing within the simulator.
    The simulation can sped up or slowed down. This depends on the computation required
    for the current simulation.
  prefs: []
  type: TYPE_NORMAL
- en: The Real Time display refers to the actual time passing in real life when the
    simulator is running. The **real time factor** (**RTF**) is the ratio between
    simulation time and the speed of real time. If the RTF is one, it means that the
    simulation is happening at a rate identical to the speed of time in reality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state of the world in Gazebo can change with each iteration. Each iteration
    can make changes in Gazebo for a fixed amount of time. That fixed time is called
    the step size. By default, the step size is 1 millisecond. The step size and iteration
    are shown in the tool bar, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e829777b-5166-47cc-8597-0b4bae78a4b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Lower toolbar of Gazebo
  prefs: []
  type: TYPE_NORMAL
- en: We can pause the simulation and see each step using the **Step** button.
  prefs: []
  type: TYPE_NORMAL
- en: You can get more information about the Gazebo GUI from [http://gazebosim.org/tutorials?cat=guided_b&amp;tut=guided_b2](http://gazebosim.org/tutorials?cat=guided_b&tut=guided_b2).
  prefs: []
  type: TYPE_NORMAL
- en: Before going to the next section, you can play with Gazebo and learn more about
    how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a TurtleBot 2 simulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After working with Gazebo, now it's time to run a simulation on it and work
    with some robots. One of the popular robots available for education and research
    is TurtleBot. The TurtleBot software was developed within the ROS framework, and
    there is a good simulation of its operations available in Gazebo. The popular
    versions of TurtleBot are TurtleBot 2 and 3\. We will learn about TurtleBot 2
    in this section because our development of Chefbot was inspired by its design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing TurtleBot 2 simulation packages in Ubuntu 16.04 is straightforward.
    You can use the following command to install TurtleBot 2 simulation packages for
    Gazebo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After installing the packages, we can start running the simulation. There are
    several launch files inside the turtlebot-gazebo packages that have different
    world files. A Gazebo world file (`*.world`) is an SDF file consisting of the
    properties of the models in the environment. When the world file changes, Gazebo
    will load with a different environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will start a world that has a certain set of components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The simulation will take some time to load, and when it loads, you will see
    the following models in the Gazebo Scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92bc12f5-0ac0-42ad-8efd-3fb132a0e954.png)'
  prefs: []
  type: TYPE_IMG
- en: TurtleBot 2 simulation in Gazebo
  prefs: []
  type: TYPE_NORMAL
- en: 'When we load the simulation in Gazebo, it will also load the necessary plugins
    to interact with ROS. TurtleBot 2 has the following important components:'
  prefs: []
  type: TYPE_NORMAL
- en: A mobile base with a differential drive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A depth sensor for creating a map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bumper switch to detect collision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the simulation loads, it will load the ROS-Gazebo plugins to simulate a
    differential drive mobile base, depth sensor (Kinect or Astra), and plugins for
    bumper switches. So, after loading the simulation, if we enter a `$ rostopic list`
    command in the terminal, a selection of topics will appear as shown in the following
    screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, we can see the topics from the differential drive plugin,
    depth sensor, and bumper switches. In addition to these, we can see the topics
    from the ROS-Gazebo plugins that mainly contain the current state of the robot
    and other models in the simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kinect/Astra sensors can give an RGB image and depth image. The differential
    drive plugin can send the odometry data of the robot in the `/odom` (`nav_msgs/Odometry`)
    topic and can publish the robot''s transformation in the `/tf` (`tf2_msgs/TFMessage`)
    topics, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eac279c0-935f-46f8-b44c-d60a905d751c.png)'
  prefs: []
  type: TYPE_IMG
- en: ROS topics from the TurtleBot 2 simulation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the robot model and sensor data in Rviz. There is a TurtleBot
    package dedicated for visualization. You can install the following package to
    visualize the robot data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing this package, we can use the following launch file to visualize
    the robot and its sensor data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following Rviz window with the robot model displayed in it.
    We can then enable the sensor displays to visualize this particular data, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c288263-0b1a-45b7-b09c-593ed6df1ead.png)'
  prefs: []
  type: TYPE_IMG
- en: TurtleBot 2 visualization in Rviz
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to move this robot.
  prefs: []
  type: TYPE_NORMAL
- en: Moving the robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The differential drive plugin of the robot is capable of receiving ROS Twist
    messages (`geometry_msgs/Twist`), which consist of the current linear and angular
    velocities of the robot. The teleoperation of the robot means moving the robot
    manually using a joy stick or keyboard by using ROS Twist messages. We will now
    look at how to move the Turtlebot 2 robot using keyboard teleoperation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to install a package to teleoperate the TurtleBot 2 robot. The following
    command will install the TurtleBot teleoperation package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To start teleoperation, we have to start the Gazebo simulator first, and then
    start the teleoperation node using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the terminal, we can see the key combination for moving the robot. You can
    move it using those keys, and you will see the robot moving in Gazebo and Rviz,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45b86691-5f61-415d-aff2-573de750e345.png)'
  prefs: []
  type: TYPE_IMG
- en: TurtleBot 2 keyboard teleoperation
  prefs: []
  type: TYPE_NORMAL
- en: 'When we press the buttons on the keyboard, it will send a Twist message to
    the differential drive controller, and the controller will move the robot in the
    simulation. The teleop node sends a topic called `/cmd_vel_mux/input/teleop` (`geometry_msgs/Twist`),
    which is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8412f9ba-3fa4-4742-8035-cbcab5be31bb.png)'
  prefs: []
  type: TYPE_IMG
- en: The TurtleBot keyboard teleoperation node
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simulation of Chefbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how the turtlebot simulation works. In this section, we will be
    looking at how to create our own robot simulation using Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start discussing this subject, you should copy the `chefbot_gazebo`
    package to your catkin workspace and enter `catkin_make` to build the package.
    Make sure you have two packages in your workspace, one called `chefbot_description`
    and the other called `chefbot_gazebo`. The `chefbot_gazebo` package contains a
    simulation-related launch file and parameters, and `chefbot_description` has the
    robot's URDF model, along with its simulation parameters, and the launch file
    that is used to view the robot in Rviz and Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin creating our Chefbot model in Gazebo so that you can familiarize
    yourself with the procedure. After that, we will dig deep into the xacro file
    and can look at the simulation parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following launch file will show the robot model in Gazebo with an empty
    world and start all the Gazebo plugins for the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows a screenshot of the Chefbot in Gazebo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b2a0759-4163-4c93-8715-ff1030f0b0de.png)'
  prefs: []
  type: TYPE_IMG
- en: The Chefbot in Gazebo
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how we can add a URDF robot model in Gazebo. You can find the definition
    of the URDF robot model at `chefbot_description/launch/view_robot_gazebo.launch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first section of the code calls the `upload_model.launch` file for creating
    the `robot_description` parameter. If it is successful, then it will start an
    empty world in Gazebo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So how does the robot model in the `robot_description` parameter show in Gazebo?
    The following code snippet in the launch file does that job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The node called `spawn_model` inside the `gazebo_ros` package will read the
    `robot_description` and spawn the model in Gazebo. The `-z 0.1` argument indicates
    the height of the model to be placed in Gazebo. If the height is 0.1, the model
    will be spawned at a height of 0.1\. If gravity is enabled, then the model will
    fall to the ground. We can change this parameter according to our requirement.
    The `-model` argument is the name of the robot model in Gazebo. This node will
    parse all the Gazebo parameters from the `robot_description` and start the simulation
    in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: 'After spawning the model, we can publish the robot transformation (tf) using
    the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We are publishing the ROS tf at 30 Hz.
  prefs: []
  type: TYPE_NORMAL
- en: Depth image to laser scan conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The depth sensor on the robot provides the 3D coordinates of the environment.
    To achieve autonomous navigation, we can use this data to create a 3D map. There
    are different techniques for creating a map of the environment. One of the algorithms
    that we are using for this robot is called gmapping ([http://wiki.ros.org/gmapping](http://wiki.ros.org/gmapping)).
    The gmapping algorithm mainly use a laser scan for creating the map, but in our
    case, we get an entire 3D point cloud from the sensor. We can convert the 3D depth
    data from a laser scan by taking a slice of the depth data. The following nodelet
    ([http://wiki.ros.org/nodelet](http://wiki.ros.org/nodelet)) in this launch file
    is able to receive the depth data and convert it to laser scan data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The nodelet is a special kind of ROS node that has a property called zero copy
    transport, meaning that it doesn't take network bandwidth to subscribe to a topic.
    This will make the conversion from the depth image (`sensor_msgs/Image`) to the
    laser scan (`sensor_msgs/LaserScan`) faster and more efficient. One of the other
    properties of the nodelet is that it can be dynamically loaded as plugins. We
    can set various properties of this nodelet, such as the `range_min`, name of the
    image topic, and the output laser topic.
  prefs: []
  type: TYPE_NORMAL
- en: URDF tags and plugins for Gazebo simulation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the simulated robot in Gazebo. Now, we will look in more detail
    at the simulation-related tags in URDF and the various plugins we have included
    in the URDF model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the Gazebo-specific tags are in the `chefbot_description/gazebo/chefbot.gazebo.xacro`
    file. Also, some of the tags in `chefbot_description/urdf/chefbot.xacro` are used
    in the simulation. Defining the `<collision>` and `<inertial>` tags in chefbot.xacro
    is very important for our simulation. The `<collision>` tag in URDF defines a
    boundary around the robot link, which is mainly used to detect the collision of
    that particular link, whereas the `<inertial>` tag encompasses the mass of the
    link and the moment of inertia. Here is an example of the `<inertial>` tag definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: These parameters are part of the robot's dynamics, so in the simulation these
    values will have an effect on the robot model. Also, in the simulation, it will
    process all the links and joints, as well as its properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at the tags inside the `gazebo/chefbot.gazebo.xacro` file.
    The important Gazebo-specific tag we are using is `<gazebo>`, which is used to
    define the simulation properties of an element in the robot. We can either define
    a property that is applicable to all the links or one that is specific to a link.
    Here is a code snippet inside the xacro file that defines the coefficient of the
    friction of a link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `reference` property is used to specify a link in the robot. So, the preceding
    properties will only be applicable to the `chefbot_wheel_left_link`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows you how to set the color of a robot link.
    We can create custom colors, define the custom colors, or use the default colors
    in Gazebo. You can see that for the `base_link`, we are using the `Gazebo/White`
    color from Gazebo''s default property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Refer to [http://gazebosim.org/tutorials/?tut=ros_urdf](http://gazebosim.org/tutorials/?tut=ros_urdf)
    to see all the tags that are used in the simulation.
  prefs: []
  type: TYPE_NORMAL
- en: That covers the main tags of the simulation. Now we will look at the Gazebo-ROS
    plugins that we have used in this simulation.
  prefs: []
  type: TYPE_NORMAL
- en: Cliff sensor plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cliff sensor is a set of IR sensors that detect cliffs, which helps to avoid
    steps and prevents the robot from falling. This is one of the sensors in the mobile
    base of Turtlebot 2, called Kobuki ([http://kobuki.yujinrobot.com/](http://kobuki.yujinrobot.com/)).
    We're using this plugin in the Turtlebot 2 simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can set the parameters of the sensors, such as the minimum and maximum angle
    of the IR beams, the resolution, and the number of samples per second. We can
    also limit the detection range of the sensor. There are three cliff sensors in
    our simulation model, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Contact sensor plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the code snippet for the contact sensor on our robot. If the base of
    the robot collides with any objects, this plugin will trigger. It is commonly
    attached to the `base_link` of the robot, so whenever the bumper hits any object,
    this sensor will be triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Gyroscope plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The gyroscope plugin is used to measure the angular velocity of the robot.
    Using the angular velocity, we can compute the orientation of the robot. The orientation
    of the robot is used in the robot drive controller for computing the robot''s
    pose, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Differential drive plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The differential drive plugin is the most important plugin of the simulation.
    This plugin simulates the differential drive behavior in the robot. It will move
    the robot model when it receives the command velocity (the linear and angular
    velocity) in the form of ROS Twist messages (`geometry_msgs/Twist`). This plugin
    also computes the odometry of the robot, which gives the local position of the
    robot, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: To compute the robot's odometry, we have to provide the robot's parameters,
    such as the distance between the wheels, wheel diameter, and the torque of the
    motors. According to our design, the wheel separation is 30 cm, the wheel diameter
    is 9 cm, and the torque is 18 N. If we want to publish the transformation of the
    robot, we can set the `publish_tf` as 1\. Each tag inside the plugin is the parameter
    of the corresponding plugin. As you can see, it takes all the inputs from the
    contact sensor, imu, and cliff sensor.
  prefs: []
  type: TYPE_NORMAL
- en: The `libgazebo_ros_kobuki`.so plugin is installed along with Turtlebot 2 simulation
    packages. We are using the same plugin in our robot. We have to make sure that,
    the Turtlebot 2 simulation is installed on your system, prior to running this
    simulation.
  prefs: []
  type: TYPE_NORMAL
- en: Depth camera plugin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The depth camera plugin simulates the characteristics of a depth camera, such
    as Kinect or Astra. The plugin name is `libgazebo_ros_openni_kinect.so`, and it
    helps us to simulate different kinds of depth sensors that have different characteristics.
    The plugin is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The plugin's publishers, the RGB image, depth image, and the point cloud data.
    We can set the camera matrix in the plugin, as well as customize other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to [http://gazebosim.org/tutorials?tut=ros_depth_camera&amp;cat=connect_ros](http://gazebosim.org/tutorials?tut=ros_depth_camera&cat=connect_ros)
    to learn more about the depth camera plugin in Gazebo.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the robot sensor data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we learn how to visualize the sensor data from the simulated
    robot. In the `chefbot_gazebo` package, there are launch files to start the robot
    in an empty world or in a hotel-like environment. The custom environment can be
    built using Gazebo itself. Just create the environment using primitive meshes
    and save as a `*. world` file, which can be the input of the `gazebo_ros` node
    in the launch file. For starting the hotel environment in Gazebo, you can use
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/17b03ab3-7436-498a-b5a4-4665fe13f3e1.png)'
  prefs: []
  type: TYPE_IMG
- en: The Chefbot in Gazebo in the hotel environment
  prefs: []
  type: TYPE_NORMAL
- en: The nine cubes inside the space represent nine tables. The robot can navigate
    to any of the tables to deliver food. We will learn how to do this, but before
    that, we will learn how to visualize the different kinds of sensor data from the
    robot model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/676fd283-5240-4564-a88f-ab5e878c1829.png)'
  prefs: []
  type: TYPE_IMG
- en: The Chefbot in Gazebo in the hotel environment
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will launch the Rviz, which displays the sensor data
    from the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates a visualization of the sensor data, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f66a613-dc3a-4f86-b6ee-3fb46735ed1c.png)'
  prefs: []
  type: TYPE_IMG
- en: The sensor visualization of Chefbot in Rviz
  prefs: []
  type: TYPE_NORMAL
- en: We can enable the Rviz display types to view different kinds of sensor data.
    In the preceding figure, you can see the depth cloud, laser scan, TF, robot model,
    and RGB camera images.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Simultaneous Localization and Mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the requirements of the Chefbot was that it should be able to navigate
    the environment autonomously and deliver food. To achieve this requirement, we
    have to use several algorithms, such as SLAM (Simultaneous Localization and Mapping)
    and AMCL (Adaptive Monte Carlo Localization). There are different approaches to
    solving the autonomous navigation problem. In this book, we are mainly sticking
    with these algorithms. The SLAM algorithms are used for mapping an environment
    at the same time as localizing the robot on the same map. It's seems like a chicken-and-egg
    problem, but now there are different algorithms to solve it. The AMCL algorithm
    is used to localize the robot in an existing map. The algorithm that we use in
    this book is called Gmapping ([http://www.openslam.org/gmapping.html](http://www.openslam.org/gmapping.html)),
    which implements Fast SLAM 2.0 ([http://robots.stanford.edu/papers/Montemerlo03a.html](http://robots.stanford.edu/papers/Montemerlo03a.html)).
    The standard gmapping library is wrapped in an ROS package called ROS Gmapping
    ([http://wiki.ros.org/gmapping](http://wiki.ros.org/gmapping)), which can be used
    in our application.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of the SLAM node is that as we move the robot around the environment,
    it will create a map of the environment using the laser scan data and the odometry
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the ROS Gmapping wiki page at [http://wiki.ros.org/gmapping](http://wiki.ros.org/gmapping) for
    more details. [](http://wiki.ros.org/gmapping)
  prefs: []
  type: TYPE_NORMAL
- en: Implementing SLAM in the Gazebo environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to implement SLAM and apply it to the simulation
    that we built. You can check the code at `chefbot_gazebo/launch/gmapping_demo.launch`
    and `launch/includes/ gmapping.launch.xml`. Basically, we are using a node from
    the gmapping package and configuring it with the proper parameters. The `gmapping.launch.xml`
    code fragment has the complete definition of this node. The following is the code
    snippet of this node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The name of the node that we are using is `slam_gmapping` and the package is
    `gmapping`. We have to provide a few parameters to this node, which can be found
    in the Gmapping wiki page.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a map using SLAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to create a map of our environment using
    SLAM. First, however, there are several commands that we have to use to start
    mapping. You should execute each command in each Linux terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to start our simulation using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to start the keyboard teleoperation node in a new terminal. This
    will help us move the robot manually using the keyboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The next command starts the SLAM in a new terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the mapping will begin. To visualize the mapping process, we can start
    Rviz with the help of **Navigation** settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can see the map created in Rviz, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69957574-a44c-4995-8cdb-d6be0651b919.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a map in Rviz using Gmapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can use the teleop node to move the robot, and you can see that a map
    is being created in Rviz. To create a good map of the environment, you have to
    move the robot slowly, and often you have to rotate the robot. When we move the
    robot in the environment and build the map, you can save the current map using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The map will be saved as `*.pgm` and `*.yaml`, where the `pgm` file is the map
    and the `yaml` file is the configuration of the map. You can see the saved map
    in your desktop.
  prefs: []
  type: TYPE_NORMAL
- en: 'After moving the robot around the environment, you may get a complete map,
    such as the one shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38006876-9c8f-4f46-9196-0b3680f204e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Final map using Gmapping.
  prefs: []
  type: TYPE_NORMAL
- en: The map can be saved at any time, but make sure that the robot covers the entire
    area of the environment and has mapped all of its space, as shown in the preceding
    screenshot*.* Once we are sure that the map is completely built, enter the `map_saver`
    command again and close the terminals. If you aren't able to map the environment,
    you can check the existing map from `chefbot_gazebo/maps/hotel`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Adaptive Monte Carlo Localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have successfully built the map of the environment. Now we have to navigate
    autonomously from the current robot position to target position. The first step
    before starting autonomous navigation is localizing the robot in the current map.
    The algorithm we are using to localize on the map is called AMCL. The AMCL uses
    a particle filter to track the robot's position with respect to the map. We are
    using an ROS package to implement AMCL in our robot ([http://wiki.ros.org/amcl](http://wiki.ros.org/amcl)).
    Similar to Gmapping, there are a lot of parameters to configure for the `amcl`
    node, which is inside the `amcl` package. You can find all the parameters of amcl
    on the ROS wiki page itself.
  prefs: []
  type: TYPE_NORMAL
- en: So how we can start AMCL for our robot? There is a launch file for doing that,
    which is placed in `chefbot_gazebo/amcl_demo.launch` and `chefbot_gazebo/includes/amcl.launch.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the definition of `amcl_demo.launch`. The following code shows the
    definition of this launch file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The first node in this launch file starts `map_server` from the `map_server`
    package. The `map_server` node loads a static map that we have already saved and
    publishes it into a topic called `map` (`nav_msgs/OccupancyGrid`). We can mention
    the map file as an argument of the `amcl_demo.launch` file, and if there is a
    map file, the `map_server` node will load that; otherwise it will load the default
    map, which is located in the `chefbot_gazeob/maps/hotel.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After loading the map, we start the `amcl` node and move the base node. The
    AMCL node helps to localize the robot on the current `map` and the `move_base`
    node inside the ROS navigation stack, which helps in navigating the robot from
    the start to the target position. We will learn more about the `move_base` node
    in the upcoming chapters. The `move_base` node also needs to be configured with
    parameters. The parameter files are kept inside the `chefbot_gazebo/param` folder,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: You can refer more about ROS navigation stack from following link
  prefs: []
  type: TYPE_NORMAL
- en: '[http://wiki.ros.org/navigation/Tutorials/RobotSetup](http://wiki.ros.org/navigation/Tutorials/RobotSetup)'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AMCL in the Gazebo environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to implement AMCL in our Chefbot. We will
    use the following procedures to incorporate AMCL within the simulation. Each command
    should be executed in each terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first command starts the Gazebo simulator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can start the AMCL launch file, with or without the map file as an argument.
    If you want to use a custom map that you have built, then use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use the default map, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After starting AMCL, we can start Rviz to visualize the map and robot. We will
    see a view in Rviz as shown in the following screenshot. You can see a map and
    a robot surrounded by green particles. The green particles are called `amcl` particles.
    They indicate the uncertainty of the robot''s position. If there are more particles
    around the robot, then this means that the uncertainty of the robot''s position
    is higher. When it starts moving, the particle count will reduce and its position
    will be more certain. If the robot isn''t able to localize the position of the
    map, we can use the *2D Pose Estimate* button in Rviz (on the toolbar) to manually
    set the initial position of the robot on the map. You can see the button in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6521ef37-a6a5-4f66-9b4e-b3c36db0db91.png)'
  prefs: []
  type: TYPE_IMG
- en: Starting AMCL on the hotel map.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you zoom into the robot''s position in Rviz, you can see the particles,
    as shown in the preceding screenshot. We can also see the obstacles around the
    robot in different colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a7f6680-0766-4bb0-b0cf-9a17ec0fa8e0.png)'
  prefs: []
  type: TYPE_IMG
- en: AMCL cloud around the robot.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to program the Chefbot to autonomously
    navigate this map. You don't need to close the current terminals; we can navigate
    the robot autonomously in the Rviz itself.
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous navigation of Chefbot in the hotel using Gazebo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start the robot''s autonomous navigation, we just need to command the target
    robot position on the map. There is a button in Rviz called 2D Nav Goal. We can
    click that button and click on a point on the map. You can now see an arrow indicating
    the position of the robot. When you give the target position in the map, you can
    see that the robot is planning a path from its current position to the target
    position. It will slowly move from its current position to the target position,
    avoiding all obstacles. The following screenshot shows the path planning and navigation
    of the robot to the target position. The color grid around the robot shows the
    local cost map of the robot, as well as the local planner path and the obstacles
    around the robot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cadf1a03-9f08-4649-91d6-23ec2e051d5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Autonomous navigation of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, if we command a position inside the map that is nearer to a table,
    the robot can go to that table and serve the food and then return back to its
    home position. Instead of commanding it from Rviz, we can write an ROS node to
    do the same. This will be explained in the last few chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to simulate our own robot, called Chefbot. We
    looked at the design of the Chefbot in the previous chapter. We started the chapter
    by learning about the Gazebo simulator and its different features and capabilities.
    After that, we looked at how the ROS framework and Gazebo simulator are used to
    perform a robot simulation. We installed the TurtleBot 2 package and tested the
    Turtlebot 2 simulation in Gazebo. After that, we created the Chefbot simulation
    and used Gmapping, AMCL, and autonomous navigation in a hotel environment. We
    learned that the accuracy of the simulation depends on the map, and that the robot
    will work better in a simulation if the generated map is perfect.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to design the robot's hardware and electronic
    circuit.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How we can model a sensor in Gazebo?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is ROS interfaced with Gazebo?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the important URDF tags for simulation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Gmapping, and how we can implement it in ROS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the function of the `move_base` node in ROS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is AMCL, and how we can implement it in ROS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about URDF, Xacro, and Gazebo, you can refer to the book *Mastering
    ROS for Robotics Programming - Second Edition* ([https://www.packtpub.com/hardware-and-creative/mastering-ros-robotics-programming-second-edition](https://www.packtpub.com/hardware-and-creative/mastering-ros-robotics-programming-second-edition)).
  prefs: []
  type: TYPE_NORMAL
