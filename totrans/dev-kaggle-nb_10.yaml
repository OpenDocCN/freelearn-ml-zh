- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Unleash the Power of Generative AI with Kaggle Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Kaggle模型释放生成式人工智能的力量
- en: In the previous chapters, our primary focus was on mastering the analysis of
    diverse data types and developing strategies to tackle a variety of problems.
    We delved into an array of tools and methodologies for data exploration and visualization,
    enriching our skill set in these areas. A few of the earlier chapters were dedicated
    to constructing baseline models, notably for participation in competitive scenarios.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们主要关注掌握分析不同数据类型和制定解决各种问题的策略。我们深入研究了数据探索和可视化的各种工具和方法，丰富了我们在这些领域的技能。其中一些早期章节专门用于构建基线模型，特别是在竞争场景中的参与。
- en: Now, in this current chapter, we will pivot our attention toward leveraging
    Kaggle Models. Our objective is to integrate these models into Kaggle applications,
    in order to prototype the use of the newest Generative AI technologies in practical
    applications. A few examples of such real-world applications are personalized
    marketing, chatbots, content creation, targeted advertising, answering customers’
    inquiries, fraud detection, medical diagnosis, patient monitoring, drug discovery,
    personalized medicine, financial analysis, risk evaluation, trading, document
    drafting, litigation support, legal analysis, personalized recommendations, and
    synthetic data generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在本章中，我们将把注意力转向利用Kaggle模型。我们的目标是将这些模型集成到Kaggle应用中，以便原型化在实用应用中使用最新的生成式人工智能技术。这类现实世界应用的例子包括个性化营销、聊天机器人、内容创作、定向广告、回答客户咨询、欺诈检测、医学诊断、患者监测、药物发现、个性化医疗、金融分析、风险评估、交易、文件起草、诉讼支持、法律分析、个性化推荐和合成数据生成。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An introduction to Kaggle Models – how to access and use them
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle模型简介 – 如何访问和使用它们
- en: Prompting a **Large Language Model** (**LLM**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活一个**大型语言模型**（**LLM**）
- en: Using LLMs together with task-chaining solutions like Langchain to create a
    sequence (or chains) of prompts for LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将LLM与任务链解决方案（如Langchain）结合使用，为LLM创建一系列（或链）提示
- en: Building a **Retrieval Augmented Generation** (**RAG**) system using LangChain,
    LLMs, and a vector database
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LangChain、LLM和向量数据库构建**检索增强生成**（**RAG**）系统
- en: Introducing Kaggle Models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Kaggle模型
- en: Kaggle Models represent one of the latest innovations on the Kaggle platform.
    This feature gained prominence in particular after the introduction of code competitions,
    where participants often train models either on their local hardware or in the
    cloud. Post-training, they upload these models to Kaggle as a dataset. This practice
    allows Kagglers to utilize these pre-trained models in their inference notebooks,
    streamlining the process for code competition submissions. This method significantly
    reduces the runtime of the inference notebooks, fitting within the stringent time
    and memory constraints of the competition. Kaggle’s endorsement of this approach
    aligns well with real-world production systems, where model training and inference
    typically occur in separate pipelines.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle模型代表了Kaggle平台上的最新创新之一。这一功能在代码竞赛的引入后尤其受到关注，在竞赛中，参与者通常在本地硬件或云中训练模型。训练完成后，他们将模型作为数据集上传到Kaggle。这种做法允许Kagglers在他们推理笔记本中使用这些预训练模型，简化了代码竞赛提交的过程。这种方法显著减少了推理笔记本的运行时间，符合竞赛严格的时间和内存限制。Kaggle对这种方法的认可与现实世界的生产系统相吻合，在现实世界的生产系统中，模型训练和推理通常在独立的管道中发生。
- en: This strategy becomes indispensable with large-scale models, such as those based
    on Transformer architectures, considering the immense computational resources
    required for fine-tuning. Platforms like HuggingFace have further democratized
    access to large models, offering options to either utilize online or download
    collaboratively developed models. Kaggle’s introduction of the Models feature,
    which can be added to notebooks just like datasets, has been a significant advancement.
    These models can be directly used within a notebook for tasks like transfer learning
    or further fine-tuning. At the time of writing, however, Kaggle does not permit
    users to upload their models in the same manner as datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略对于基于Transformer架构的大型模型至关重要，因为这些模型在微调时需要巨大的计算资源。像HuggingFace这样的平台进一步民主化了大型模型的访问，提供了在线使用或下载协作开发模型的选择。Kaggle引入的模型功能，可以像数据集一样添加到笔记本中，是一项重大进步。这些模型可以直接在笔记本中用于迁移学习或进一步微调等任务。然而，在撰写本文时，Kaggle不允许用户以与数据集相同的方式上传模型。
- en: Kaggle’s model library offers a browsing and search functionality, allowing
    users to find models based on various criteria like name, metadata, task, data
    type, and more. At the time of writing, the library boasted 269 models with 1,997
    variations, published by prominent organizations, including Google, TensorFlow,
    Kaggle, DeepMind, Meta, and Mistral.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle的模型库提供了浏览和搜索功能，使用户可以根据名称、元数据、任务、数据类型等多种标准找到模型。在撰写本文时，该库拥有由Google、TensorFlow、Kaggle、DeepMind、Meta和Mistral等知名组织发布的269个模型和1,997个变体。
- en: The field of Generative AI has seen a surge in interest following the introduction
    of models like GPT-3, ChatGPT, GPT-4, and various other **LLMs** or Foundation
    Models. Kaggle provides access to several powerful LLMs, such as Llama, Alpaca,
    and Llama 2\. The platform’s integrated ecosystem allows users to swiftly test
    new models as they emerge. For instance, Meta’s Llama 2, available since July
    18, 2023, is a series of generative text models, with variants ranging from 7
    billion to 70 billion parameters. These models, including specialized versions
    for chat applications, are accessible on Kaggle with relative ease compared to
    other platforms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GPT-3、ChatGPT、GPT-4等模型的出现，生成式AI领域引起了极大的兴趣。Kaggle提供了访问多个强大的LLM（大型语言模型）或基础模型的机会，例如Llama、Alpaca和Llama
    2。该平台的集成生态系统使用户能够迅速测试新出现的模型。例如，Meta的Llama 2自2023年7月18日起可用，是一系列生成文本模型，参数量从70亿到700亿不等。这些模型，包括适用于聊天应用的专用版本，与其他平台相比，在Kaggle上相对容易访问。
- en: Kaggle further simplifies the process by allowing users to start a notebook
    directly from the model page, akin to initiating a notebook from a competition
    or dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle通过允许用户直接从模型页面启动笔记本，类似于从比赛或数据集启动笔记本，进一步简化了流程。
- en: This streamlined approach, as illustrated in the following screenshot, enhances
    the user experience and fosters a more efficient workflow in model experimentation
    and application.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简化的方法，如以下截图所示，增强了用户体验，并促进了模型实验和应用中更高效的流程。
- en: '![](img/B20963_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/B20963_10_01.png)'
- en: 'Figure 10.1: Main page of the Mistral Model, with the button to Add a Notebook
    in the top-right corner'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：Mistral模型的主页，右上角有添加笔记本的按钮
- en: 'Once the notebook is open in the editor, the model is already added to it.
    One more step is needed in the case of models, and this is because a model has
    also variations, versions, and frameworks. In the right-hand panel of the notebook
    edit window, you can set these options. After these options are set, we are ready
    to use the model within the notebook. the following screenshot, we show the options
    for one model, Mistral, from Mistral AI (see *Reference 2*), after everything
    was selected in the menu:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在编辑器中打开笔记本，模型就已经添加进去了。对于模型来说，还需要额外一步，这是因为模型也有变体、版本和框架。在笔记本编辑窗口的右侧面板中，您可以设置这些选项。设置完这些选项后，我们就可以在笔记本中使用模型了。以下截图显示了Mistral
    AI（见*参考文献2*）的一个模型Mistral的选项，在菜单中选择了所有内容：
- en: '![](img/B20963_10_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/B20963_10_02.png)'
- en: 'Figure 10.2: Model Mistral from Mistral AI is added to a notebook, and all
    options are selected'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：Mistral AI的Mistral模型添加到笔记本中，并选择了所有选项
- en: Prompting a foundation model
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活基础模型
- en: LLMs can be used directly, for example, for such tasks as summarization, question
    answering, and reasoning. Due to the very large amounts of data on which they
    were trained, they can answer very well to a variety of questions on many subjects,
    since they have the context available in that training dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 可以直接用于诸如摘要、问答和推理等任务。由于它们是在非常大的数据集上训练的，因此它们可以很好地回答许多主题的多种问题，因为它们在训练数据集中有可用的上下文。
- en: In many practical cases, such LLMs can correctly answer our questions on the
    first attempt. In other cases, we will need to provide a few clarifications or
    examples. The quality of the answers in these zero-shot or few-shot approaches
    highly depends on the ability of the user to craft the prompts for LLM. In this
    section, we will show the simplest way to interact with one LLM on Kaggle, using
    prompts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际情况下，这样的LLM可以在第一次尝试中正确回答我们的问题。在其他情况下，我们需要提供一些澄清或示例。这些零样本或少样本方法中答案的质量高度依赖于用户为LLM编写的提示能力。在本节中，我们将展示在Kaggle上与一个LLM交互的最简单方法，使用提示。
- en: Model evaluation and testing
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估和测试
- en: 'Before starting to use an LLM on Kaggle, we will need to perform a few preparation
    steps. We begin by loading the model and then defining a tokenizer. Next, we create
    a model pipeline. In our first code example, we will use AutoTokenizer from transformers
    as a tokenizer and create a pipeline, also using the transformers pipeline. The
    following code (excerpts from the notebook in *Reference 3*) illustrates these
    steps described:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始在Kaggle上使用LLM之前，我们需要进行一些准备工作。我们首先加载模型，然后定义一个分词器。接下来，我们创建一个模型管道。在我们的第一个代码示例中，我们将使用transformers中的AutoTokenizer作为分词器并创建一个管道，也使用transformers
    pipeline。以下代码（来自*参考3*中的笔记本摘录）说明了这些步骤：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code returns the tokenizer and the pipeline. We then implement
    a function to test the model. The function receives as parameters the tokenizer,
    the pipeline, and the prompt with which we would like to test the model. See the
    following code for the test function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回了分词器和管道。然后我们实现了一个测试模型的功能。该函数接收分词器、管道以及我们想要测试模型的提示。以下代码是测试函数：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we are ready to prompt the model. The model we are using has the following
    characteristics: a Llama 2 model (7b), a chat version from HuggingFace (version
    1), and thePyTorch framework. We will prompt the model with math questions. In
    the next code extract, we initialize the tokenizer and pipeline and then prompt
    the model with a simple arithmetic problem, formulated in plain language:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备提示模型。我们使用的模型具有以下特点：Llama 2模型（7b）、来自HuggingFace的聊天版本（版本1）以及PyTorch框架。我们将用数学问题提示模型。在下一个代码摘录中，我们初始化分词器和管道，然后用一个简单的算术问题提示模型，这个问题是用普通语言表述的：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s see how the model reasons. the following screenshot, we plot the time
    for inference, the prompt, and the answer:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型是如何推理的。以下截图，我们绘制了推理时间、提示和答案：
- en: '![](img/B20963_10_03.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_10_03.png)'
- en: 'Figure 10.3: Prompt, answer, and inference time for a math question with the
    Llama 2 model'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：使用Llama 2模型对数学问题的提示、答案和推理时间
- en: 'For this simple math question, the reasoning of the model seems accurate. Let’s
    try again with a different question. In the following code excerpt, we ask a geometry
    question:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单的数学问题，模型的推理似乎很准确。让我们尝试一个不同的问题。在以下代码摘录中，我们提出了一个几何问题：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*The following screenshot shows* the result of prompting the model with the
    preceding geometry question:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下截图显示了*使用前面的几何问题提示模型的成果：'
- en: '![](img/B20963_10_04.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_10_04.png)'
- en: 'Figure 10.4: Llama 2 model answer to a basic geometry question'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：Llama 2模型对一个基本几何问题的回答
- en: 'The response to simple mathematical questions is not correct all of the time.
    In the following example, we prompted the model with a variation of the first
    algebraic problem. You can see that, in this case, the model took a convoluted
    and wrong path to reach an incorrect solution:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于简单的数学问题，模型的回答并不总是正确的。在以下示例中，我们使用第一个代数问题的变体提示了模型。你可以看到，在这种情况下，模型采取了一条复杂且错误的路径来得出错误的解决方案：
- en: '![](img/B20963_10_05.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_10_05.png)'
- en: 'Figure 10.5: Llama 2 model solution is wrong for an algebra problem'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：Llama 2模型在代数问题上的解决方案是错误的
- en: Model quantization
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型量化
- en: In the preceding experiments, we engaged a model with a series of straightforward
    questions. This process underscored the crucial role of crafting clear, well-structured
    prompts to elicit accurate and relevant responses. While Kaggle generously offers
    substantial computational resources at no cost, the sheer size of LLMs presents
    a challenge. These models demand significant RAM and CPU/GPU power for loading
    and inference.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的实验中，我们用一系列简单的问题测试了模型。这个过程强调了精心设计、结构良好的提示在引发准确和相关信息中的关键作用。虽然 Kaggle 慷慨地提供了大量的免费计算资源，但
    LLMs 的规模本身就是一个挑战。这些模型需要大量的 RAM 和 CPU/GPU 功率来加载和推理。
- en: To mitigate these demands, we can employ a technique known as model quantization.
    This method effectively reduces the memory and computational requirements of a
    model. It achieves this by representing the model’s weights and activation functions
    using low-precision data types, such as 8-bit or 4-bit integers, instead of the
    standard 32-bit floating-point format. This approach not only conserves resources
    but also maintains a balance between efficiency and performance (see *Reference
    4*).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些需求，我们可以采用一种称为模型量化的技术。这种方法有效地减少了模型的内存和计算需求。它通过使用低精度数据类型（如 8 位或 4 位整数）来表示模型的权重和激活函数，而不是标准的
    32 位浮点格式，来实现这一点。这种方法不仅节省了资源，而且在效率和性能之间保持了平衡（见*参考文献 4*）。
- en: In our upcoming example, we’ll demonstrate how to quantize a model from Kaggle
    using one of the available techniques, the `llama.cpp` library. We’ve chosen the
    Llama 2 model for this purpose. Llama 2 is, at the time of writing, one of the
    most successful LLMs that you can download (with Meta approval) and use freely.
    It also has demonstrable accuracy for a variety of tasks, on par with many other
    available models. The quantization will be executed using the `llama.cpp` library.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们即将提供的示例中，我们将演示如何使用可用的技术之一，即 `llama.cpp` 库，量化 Kaggle 上的模型。我们选择了 Llama 2 模型来完成这个目的。截至写作时，Llama
    2 是你可以下载（经 Meta 批准）并免费使用的最成功的 LLM 之一。它也在各种任务上表现出可观的准确性，与其他许多可用模型相当。量化将使用 `llama.cpp`
    库执行。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s look at a few examples of testing the quantized model. We will start
    by prompting it with a geography question:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个测试量化模型的例子。我们将首先用地理问题提示它：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result of the prompt is:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提示的结果如下：
- en: '![](img/B20963_10_06.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_06.png)'
- en: 'Figure 10.6: Result of prompting the quantized Llama 2 model with a geography
    question'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：使用地理问题提示量化 Llama 2 模型的结果
- en: 'In the next screenshot, we show the answer of the model to a simple geometry
    question. The answer is quite straightforward and clearly formulated. The code
    to prompt the model and the one to print the result is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个屏幕截图中，我们展示了模型对一个简单几何问题的回答。答案非常直接，表述清晰。提示模型和打印结果的代码如下：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B20963_10_07.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_07.png)'
- en: 'Figure 10.7: Result of prompting the quantized Llama 2 model with a geometry
    question'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：使用几何问题提示量化 Llama 2 模型的结果
- en: The notebook to illustrate the first method to quantize a Llama 2 model, from
    which we extracted the code and results, is given in *Reference 5*. This notebook
    was run on the GPU. In another notebook given in *Reference 6*, we run the same
    model but on the CPU. It is quite interesting to notice that the time to execute
    the inference on the CPU with the quantized model is much smaller than on the
    GPU (with the same quantized model). See the notebooks in *Reference 5* and *6*
    for more details.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 展示第一个量化 Llama 2 模型方法的笔记本，我们从其中提取了代码和结果，详见*参考文献 5*。该笔记本是在 GPU 上运行的。在*参考文献 6*中给出的另一个笔记本中，我们运行了相同的模型，但是在
    CPU 上。值得注意的是，使用量化模型在 CPU 上执行推理的时间远小于在 GPU 上（使用相同的量化模型）。有关更多详细信息，请参阅*参考文献 5*和*6*。
- en: 'We can also use alternative approaches for model quantization. For example,
    in *Reference 7*, we used the `bitsandbytes` library for model quantization. In
    order to use this quantization option, we need to install the accelerate library
    and the latest version of `bitsandbytes`. The following code excerpt shows how
    to initialize the model configuration for quantization and load the model with
    this configuration:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用其他模型量化的方法。例如，在*参考文献 7*中，我们使用了 `bitsandbytes` 库进行模型量化。为了使用这种量化选项，我们需要安装
    accelerate 库和 `bitsandbytes` 的最新版本。以下代码片段展示了如何初始化量化模型配置并使用此配置加载模型：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We also define a pipeline:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个管道：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can test the model with a simple prompt:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的提示来测试模型：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The answer seems to be correct:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 答案看起来似乎是正确的：
- en: '![](img/B20963_10_08.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_08.png)'
- en: 'Figure 10.8: Answer to a simple geography question (Llama 2 quantized with
    the bitsandbytes library)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：对简单地理问题的回答（使用bitsandbytes库量化的Llama 2）
- en: So far, we have experimented with prompting models. We directly used the models
    from Kaggle Models, or after quantization. We performed the quantization with
    two different methods. In the next section, however, we will see how, using a
    task-chaining framework such as Langchain, we can extend the power of LLMs and
    create sequences of operations, where the answer of an initial query for the LLM
    is fed as input to the next task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经尝试了提示模型。我们直接使用了Kaggle Models中的模型，或者经过量化处理。我们使用了两种不同的方法进行量化。然而，在下一节中，我们将看到如何使用Langchain这样的任务链框架来扩展LLM（大型语言模型）的能力，并创建一系列操作，其中LLM的初始查询答案作为下一个任务的输入。
- en: Building a multi-task application with Langchain
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Langchain构建多任务应用
- en: Langchain is the most popular task-chaining framework (*Reference 8*). Task
    chaining is an extension of the prompt engineering concept that we illustrated
    in the previous section. Chains serve as predetermined sequences of operations,
    designed to organize intricate processes in a format that is both more manageable
    and easier to understand. These chains follow a distinct order of actions. They
    are well suited for workflows characterized by a consistent number of steps. With
    task chaining, you can create sequences of prompts, where the output of the previous
    task executed by the framework is fed as input for the next task.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Langchain是最受欢迎的任务链框架（*参考文献8*）。任务链是我们在上一节中阐述的提示工程概念的扩展。链是一系列预定的操作，旨在以更易于管理和理解的方式组织复杂的过程。这些链遵循特定的动作顺序。它们非常适合具有固定步骤数量的工作流程。使用任务链，您可以创建一系列提示，其中框架执行的前一个任务的输出作为下一个任务的输入。
- en: Besides Langchain, there are now several other options available for task chaining,
    like LlamaIndex or Semantic Kernel (from Microsoft). Langchain provides multiple
    functionalities, including specialized tools to ingest data or output results,
    intelligent agents, as well as the possibility to extend it by defining your own
    tasks, tools, or agents. An agent will select and execute tasks based on the perceived
    context, in order to achieve its objective. In order to execute tasks, it will
    use generic or custom tools.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Langchain之外，现在还有其他几种任务链选项可用，如LlamaIndex或来自微软的Semantic Kernel。Langchain提供了多种功能，包括专门的数据摄取或结果输出工具、智能代理，以及通过定义自己的任务、工具或代理来扩展它的可能性。代理将根据感知到的上下文选择并执行任务，以实现其目标。为了执行任务，它将使用通用或定制的工具。
- en: 'Let’s start working with Langchain by defining a two-step sequence. We will
    define the sequence in a custom function that will receive a parameter and formulate
    an initial prompt, parameterized with the input parameter. Based on the answer
    to the first prompt, we assemble the prompt for the next task. This way, we can
    create the dynamic behavior of our mini-application. The code for defining this
    function is as follows (*Reference 7*):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个两步序列开始使用Langchain。我们将在一个自定义函数中定义这个序列，该函数将接收一个参数并形成一个参数化的初始提示，该提示以输入参数为参数。根据第一个提示的答案，我们组装下一个任务的提示。这样，我们可以创建我们迷你应用的动态行为。定义此函数的代码如下（*参考文献7*）：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The expected input parameter is the name of a country. The first prompt will
    get the most popular food in that country. The next prompt will use the answer
    to the first question to build the second question, which is about the top three
    ingredients for that food.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输入参数是一个国家的名称。第一个提示将获取那个国家最受欢迎的食物。下一个提示将使用第一个问题的答案来构建第二个问题，这个问题是关于那种食物的前三种成分。
- en: 'Let’s check the functionality of the code with two examples. First, let’s try
    with the `France` parameter:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用两个例子来检查代码的功能。首先，让我们尝试使用`France`参数：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/B20963_10_09.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_09.png)'
- en: 'Figure 10.9: Two-step sequential chain execution (ingredients of the most famous
    food in France)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：两步顺序链执行（法国最著名食物的成分）
- en: 'The answer looks quite convincing. Indeed, tourists in France prefer snails,
    and yes, the top three ingredients for this tasty food are listed correctly. Let’s
    double-check, with another country famous for its delicious food, `Italy`. The
    prompt will be:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 答案看起来相当令人信服。确实，法国的游客更喜欢蜗牛，而且，这种美味食物的前三种成分确实列得正确。让我们再检查一次，用另一个以其美味食物而闻名的国家`意大利`。提示将是：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Consequently, the result will be:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结果将是：
- en: '![](img/B20963_10_10.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_10.png)'
- en: 'Figure 10.10: Most popular food in Italy and its ingredients'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：意大利最受欢迎的食物及其成分
- en: We illustrated with an intuitive example how we can use LangChain together with
    an LLM to chain multiple prompts and extend the power of LLMs, for example, in
    the automatization of business processes. In the next section, we will see how
    we can use LLMs for another important task, the automatization of code generation,
    to increase the productivity in the coding process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个直观的例子说明了如何使用LangChain与LLM结合，通过链式多个提示来扩展LLMs的能力，例如，在业务流程自动化的自动化中。在下一节中，我们将看到如何使用LLMs来完成另一个重要任务，即代码生成自动化，以提高编码过程中的生产力。
- en: Code generation with Kaggle Models
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kaggle Models进行代码生成
- en: 'For code generation, we will experiment with the Code Llama model, the 13b
    version. From the LLMs available on the Kaggle platform at the time of writing,
    this model was the most appropriate, in regards to its purpose (it is a model
    specialized for code generation) and size (i.e., we can use it with Kaggle Notebooks),
    for the task of code generation. The notebook used to demonstrate the code generation
    is given in *Reference 9*. The model is loaded, quantized using `bitsandbytes`,
    and has a tokenizer initialized in the same way, as demonstrated in *Reference
    7*. We define a prompt and a pipeline (using the transformers function) with the
    following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代码生成，我们将实验Code Llama模型，13b版本。在撰写本文时，在Kaggle平台上可用的LLMs中，这个模型在目的（它是一个专门用于代码生成的模型）和大小（即我们可以使用它与Kaggle
    Notebooks）方面对于代码生成任务来说是最合适的。用于演示代码生成的笔记本在*参考9*中给出。模型被加载，使用`bitsandbytes`量化，并且以与*参考7*中相同的方式初始化了tokenizer。我们使用以下代码定义了一个提示和一个管道（使用transformers函数）：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result of executing the preceding code is given in the following screenshot.
    The code looks functional, but the answer contains more information than expected.
    We obtained this by printing all the sequences outputted. If we just select the
    first one, the answer will be correct (only the code for the circle area).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面代码的结果如下所示。代码看起来功能正常，但答案包含比预期更多的信息。我们通过打印所有输出的序列获得了这些信息。如果我们只选择第一个，答案将是正确的（只有圆面积的计算代码）。
- en: '![](img/B20963_10_11.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_11.png)'
- en: 'Figure 10.11: Code generation: a function to compute the area of a circle'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：代码生成：计算圆面积的函数
- en: In the notebook from *Reference 9*, there are more examples; we will not give
    all the details here. You can modify the notebook and generate more answers, by
    changing the prompt.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在*参考9*的笔记本中，有更多的例子；我们这里不会给出所有细节。你可以通过更改提示来修改笔记本并生成更多答案。
- en: In the next section, let’s see how we can further extend the functionality of
    LLMs, by creating a system that retrieves information stored in a special database
    (a vector database), assembling a prompt by combining the initial query with the
    retrieved information (context), and prompting the LLM to answer to the initial
    query by only using the context result from the retrieval step. Such a system
    is called **Retrieval Augmented Generation** (**RAG**).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们看看如何通过创建一个系统来进一步扩展LLMs的功能，该系统可以检索存储在特殊数据库（向量数据库）中的信息，通过将初始查询与检索到的信息（上下文）结合来组装提示，并通过仅使用检索步骤的结果来提示LLM回答初始查询。这样的系统被称为**检索增强生成**（**RAG**）。
- en: Creating a RAG system
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个RAG系统
- en: In the previous sections, we explored various approaches to interact with Foundation
    Models – more precisely, available LLMs from Kaggle Models. First, we experimented
    with prompting, directly using the models. Then, we quantized the models with
    two different approaches. We also showed that we can use models to generate code.
    A more complex application included a combination of `LangChain` with an LLM to
    create sequences of connected operations, or task sequences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了与基础模型交互的各种方法——更确切地说，是来自Kaggle Models的可用LLMs。首先，我们通过提示直接使用模型进行了实验。然后，我们用两种不同的方法量化了模型。我们还展示了我们可以使用模型来生成代码。一个更复杂的应用包括将`LangChain`与LLM结合以创建一系列连接的操作，或任务序列。
- en: In all these cases, the answers of the LLM are based on the information already
    available with the model at the time of training it. If we would like to have
    the LLM answer queries about information that was never presented to the LLM,
    the model might provide a deceiving answer by hallucinating. To counter this tendency
    of models to hallucinate when they do not have the right information, we can fine-tune
    models with our own data. The disadvantage to this is that it is costly, since
    the computational resources needed to fine-tune a large model are very large.
    It also doesn’t necessarily totally eliminate hallucination.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些情况下，LLM的答案都是基于在训练模型时模型已经拥有的信息。如果我们希望LLM回答关于从未向LLM展示过的信息的查询，模型可能会通过虚构来提供误导性的答案。为了对抗模型在没有正确信息时虚构的倾向，我们可以使用自己的数据微调模型。这种方法的缺点是成本高昂，因为微调大型模型所需的计算资源非常大。它也不一定能完全消除虚构。
- en: 'An alternative to this approach is to combine vector databases, task-chaining
    frameworks, and LLMs to create a RAG system (see *Reference 10*). In the following
    figure, we illustrate the functionality of such a system:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 与此方法不同的选择是将向量数据库、任务链框架和LLM（大型语言模型）结合起来创建一个RAG系统（参见*参考文献10*）。在下面的图中，我们展示了这样一个系统的功能：
- en: '![](img/B20963_10_12.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_12.png)'
- en: 'Figure 10.12: RAG system explained'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：RAG系统解释
- en: Before using the RAG system, we will have to ingest the documents into the vector
    database (*Step 1* in *Figure 10.12*). The documents can be in any format, including
    Word, PowerPoint, text, Excel, images, video, email, etc. We first transform each
    modality in the text format (for example, using Tesseract to extract text from
    images, or OpenAI Whisper to convert video into text). After we transform all
    the formats/modalities into text, we will have to split the larger texts into
    fixed-size chunks (partially superposed, to not lose context that might be distributed
    across multiple chunks).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用RAG系统之前，我们必须将文档导入向量数据库（*图10.12中的步骤1*）。文档可以是任何格式，包括Word、PowerPoint、文本、Excel、图片、视频、电子邮件等。我们首先将文本格式的每种模态转换（例如，使用Tesseract从图片中提取文本，或使用OpenAI
    Whisper将视频转换为文本）。在我们将所有格式/模态转换为文本之后，我们必须将较大的文本分割成固定大小的块（部分重叠，以避免丢失可能分布在多个块中的上下文）。
- en: Then, we use one of the options to encode the information before adding the
    pre-processed documents to the vector database. The vector database stores the
    data encoded using text embeddings, and it also uses very efficient indexing for
    such an encoding type, which will allow us to perform a fast search and retrieval
    of information, based on a similarity search. We have multiple options for vector
    databases, like ChromaDB, Weaviate, Pinecone, and FAISS. In our application on
    Kaggle, we used ChromaDB, which has a simple interface, plugins with Langchain,
    is easy to integrate, has options to be used in memory as well as persistent storage.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在将预处理过的文档添加到向量数据库之前，使用其中一种选项对信息进行编码。向量数据库存储使用文本嵌入编码的数据，并且它还使用非常高效的索引来支持这种编码类型，这将使我们能够根据相似性搜索快速搜索和检索信息。我们有多个向量数据库选项，如ChromaDB、Weaviate、Pinecone和FAISS。在我们的Kaggle应用程序中，我们使用了ChromaDB，它有一个简单的界面，与Langchain插件兼容，易于集成，有选项用于内存以及持久存储。
- en: Once the data is transformed, chunked, encoded, and indexed in the vector database,
    we can start to query our system. Queries are passed through a Langchain specialized
    task – a question and answering retrieval (*Step 2* in *Figure 10.12*). The query
    is used to perform similarity search in the vector database. The retrieved documents
    are used together with the query (*Step 3* in *Figure 10.12*) to compose the prompt
    for LLM (*Step 4* in *Figure 10.12*). The LLM will provide its answer to the query
    by referring only to the context we provided – context from the data stored in
    the vector database.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据在向量数据库中转换为、分割、编码和索引，我们就可以开始查询我们的系统。查询通过Langchain的专业任务传递——问答检索（*图10.12中的步骤2*）。查询用于在向量数据库中执行相似性搜索。检索到的文档与查询一起使用（*图10.12中的步骤3*）来组成LLM的提示（*图10.12中的步骤4*）。LLM将仅根据我们提供的上下文来回答查询——来自存储在向量数据库中的数据的上下文。
- en: 'The code to implement the RAG system is given in *Reference 11*. We will use
    as documents the text of the State of the Union 2023 (from Kaggle datasets). Let’s
    first use the LLM directly by prompting to answer a question about the State of
    the Union in general:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 实现RAG系统的代码在*参考文献11*中给出。我们将使用2023年国情咨文的文本（来自Kaggle数据集）作为文档。让我们首先直接使用LLM通过提示来回答关于国情咨文的一般问题：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The answer is given in the following screenshot. We can observe that the LLM
    has the relevant information, and the answer is correct. Of course, if we had
    asked about recent information, the answer might have been wrong.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在以下屏幕截图中给出。我们可以观察到LLM具有相关信息，并且答案是正确的。当然，如果我们询问的是最近的信息，答案可能就是错误的。
- en: '![](img/B20963_10_13.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_13.png)'
- en: 'Figure 10.13: Result of the prompt (a general question, without context)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：提示的结果（一个不带背景的一般问题）
- en: Let’s see now some answers to questions about the information we ingested in
    the vector database.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一些关于我们摄入到向量数据库中的信息的问题的答案。
- en: 'Data transformation, chunking, and encoding are done with the following code.
    Since the data we ingest is plain text, we will use `TextLoader` from Langchain.
    We will use `ChromaDB` as the vector database, and Sentence Transformer for embeddings:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换、分块和编码使用以下代码完成。由于我们摄入的数据是纯文本，我们将使用Langchain的`TextLoader`。我们将使用`ChromaDB`作为向量数据库，并使用Sentence
    Transformer进行嵌入：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We define the question and answering retrieval chain:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了问题和答案检索链：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We also define a function to test the preceding chain:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数来测试前面的链：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s test the functionality of this system. We will formulate queries about
    the subject – in this case, the State of the Union 2023:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试这个系统的功能。我们将针对主题制定查询 – 在这个例子中，是2023年国情咨文：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The result of running the preceding query will be:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述查询的结果将是：
- en: '![](img/B20963_10_14.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_14.png)'
- en: 'Figure 10.14: Query and answer using the RAG system (example 1)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：使用RAG系统进行查询和答案（示例1）
- en: 'Next, we show the answer to a different query on the same content (the query
    included in the printout):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示同一内容上的不同查询的答案（包含在打印输出中的查询）：
- en: '![](img/B20963_10_15.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_10_15.png)'
- en: 'Figure 10.15: Query and answer using the RAG system (example 2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：使用RAG系统进行查询和答案（示例2）
- en: 'We can also retrieve the documents that were used to create the context for
    the answer. The following code does just that:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检索用于创建答案背景的文档。以下代码正是如此：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: RAG is a powerful method to leverage the capability of LLMs for reasoning, while
    controlling what the source of information is. The answer given by the LLM is
    only from the context extracted by similarity search (in the first step of a question-and-answer
    retrieval chain), from the vector database where we stored the information.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一种强大的方法，可以发挥LLM推理能力，同时控制信息来源。LLM给出的答案仅来自通过相似性搜索提取的上下文（问答检索链的第一步），以及我们存储信息的向量数据库。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored how we can leverage the potential of Generative
    AI, using LLMs from Kaggle Models. We started by focusing on the simplest way
    to use such Foundation Models – by directly prompting them. We learned that crafting
    a prompt is important and experimented with simple math questions. We used the
    models that were available in Kaggle Models as well as quantized ones and quantized
    models with two approaches: using `Llama.cpp` and the `bitsandbytes` library.
    We then combined Langchain with a LLM to create sequences of chained tasks, where
    the output of one task is used to craft (by the framework) the input (or prompt)
    for the next task. Using the Code Llama 2 model, we tested the feasibility of
    code generation on Kaggle. The results were less than perfect, with multiple sequences
    generated besides the expected one. Finally, we learned how to create a RAG system
    that combines the speed, versatility, and ease of using vector databases with
    the chaining functions of Langchain and the reasoning capabilities of LLMs.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which is also the last chapter of our book, you will learn
    a few useful recipes that will help you to make your high-quality work on the
    platform more visible and appreciated.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Llama 2, Kaggle Models: [https://www.kaggle.com/models/metaresearch/llama-2](https://www.kaggle.com/models/metaresearch/llama-2)'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mistral, Kaggle Models: [https://www.kaggle.com/models/mistral-ai/mistral/](https://www.kaggle.com/models/mistral-ai/mistral/)'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test Llama v2 with math, Kaggle Notebooks: [https://www.kaggle.com/code/gpreda/test-llama-v2-with-math](https://www.kaggle.com/code/gpreda/test-llama-v2-with-math)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model Quantization, HuggingFace: [https://huggingface.co/docs/optimum/concept_guides/quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test Llama 2 quantized with Llama.cpp, Kaggle Notebooks: [https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp](https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test of Llama 2 quantized with llama.cpp (on CPU), Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu](https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Simple sequential chain with Llama 2 and Langchain, Kaggle
    Notebooks: [https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/](https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Langchain, Wikipedia page: [https://en.wikipedia.org/wiki/LangChain](https://en.wikipedia.org/wiki/LangChain)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Use Code Llama to generate Python code (13b), Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b](https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Retrieval Augmented Generation, Combining LLMs, Task-Chaining
    and Vector Databases, Endava Blog: [https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases](https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加布里埃尔·普雷达 – 检索增强生成，结合 LLMs、任务链和向量数据库，Endava 博客：[https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases](https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases)
- en: 'Gabriel Preda – RAG using Llama 2, Langchain and ChromaDB, Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb](https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加布里埃尔·普雷达 – 使用 Llama 2、Langchain 和 ChromaDB 进行 RAG，Kaggle 笔记本：[https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb](https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb)
- en: Join our book’s Discord space
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并和超过 5000 名成员一起学习，详情请见：
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/kaggle](https://packt.link/kaggle)'
- en: '![](img/QR_Code9220780366773140.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code9220780366773140.png)'
