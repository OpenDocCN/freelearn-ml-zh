- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unleash the Power of Generative AI with Kaggle Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, our primary focus was on mastering the analysis of
    diverse data types and developing strategies to tackle a variety of problems.
    We delved into an array of tools and methodologies for data exploration and visualization,
    enriching our skill set in these areas. A few of the earlier chapters were dedicated
    to constructing baseline models, notably for participation in competitive scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in this current chapter, we will pivot our attention toward leveraging
    Kaggle Models. Our objective is to integrate these models into Kaggle applications,
    in order to prototype the use of the newest Generative AI technologies in practical
    applications. A few examples of such real-world applications are personalized
    marketing, chatbots, content creation, targeted advertising, answering customers’
    inquiries, fraud detection, medical diagnosis, patient monitoring, drug discovery,
    personalized medicine, financial analysis, risk evaluation, trading, document
    drafting, litigation support, legal analysis, personalized recommendations, and
    synthetic data generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to Kaggle Models – how to access and use them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting a **Large Language Model** (**LLM**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLMs together with task-chaining solutions like Langchain to create a
    sequence (or chains) of prompts for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a **Retrieval Augmented Generation** (**RAG**) system using LangChain,
    LLMs, and a vector database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Kaggle Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kaggle Models represent one of the latest innovations on the Kaggle platform.
    This feature gained prominence in particular after the introduction of code competitions,
    where participants often train models either on their local hardware or in the
    cloud. Post-training, they upload these models to Kaggle as a dataset. This practice
    allows Kagglers to utilize these pre-trained models in their inference notebooks,
    streamlining the process for code competition submissions. This method significantly
    reduces the runtime of the inference notebooks, fitting within the stringent time
    and memory constraints of the competition. Kaggle’s endorsement of this approach
    aligns well with real-world production systems, where model training and inference
    typically occur in separate pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy becomes indispensable with large-scale models, such as those based
    on Transformer architectures, considering the immense computational resources
    required for fine-tuning. Platforms like HuggingFace have further democratized
    access to large models, offering options to either utilize online or download
    collaboratively developed models. Kaggle’s introduction of the Models feature,
    which can be added to notebooks just like datasets, has been a significant advancement.
    These models can be directly used within a notebook for tasks like transfer learning
    or further fine-tuning. At the time of writing, however, Kaggle does not permit
    users to upload their models in the same manner as datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle’s model library offers a browsing and search functionality, allowing
    users to find models based on various criteria like name, metadata, task, data
    type, and more. At the time of writing, the library boasted 269 models with 1,997
    variations, published by prominent organizations, including Google, TensorFlow,
    Kaggle, DeepMind, Meta, and Mistral.
  prefs: []
  type: TYPE_NORMAL
- en: The field of Generative AI has seen a surge in interest following the introduction
    of models like GPT-3, ChatGPT, GPT-4, and various other **LLMs** or Foundation
    Models. Kaggle provides access to several powerful LLMs, such as Llama, Alpaca,
    and Llama 2\. The platform’s integrated ecosystem allows users to swiftly test
    new models as they emerge. For instance, Meta’s Llama 2, available since July
    18, 2023, is a series of generative text models, with variants ranging from 7
    billion to 70 billion parameters. These models, including specialized versions
    for chat applications, are accessible on Kaggle with relative ease compared to
    other platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle further simplifies the process by allowing users to start a notebook
    directly from the model page, akin to initiating a notebook from a competition
    or dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This streamlined approach, as illustrated in the following screenshot, enhances
    the user experience and fosters a more efficient workflow in model experimentation
    and application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Main page of the Mistral Model, with the button to Add a Notebook
    in the top-right corner'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the notebook is open in the editor, the model is already added to it.
    One more step is needed in the case of models, and this is because a model has
    also variations, versions, and frameworks. In the right-hand panel of the notebook
    edit window, you can set these options. After these options are set, we are ready
    to use the model within the notebook. the following screenshot, we show the options
    for one model, Mistral, from Mistral AI (see *Reference 2*), after everything
    was selected in the menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Model Mistral from Mistral AI is added to a notebook, and all
    options are selected'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting a foundation model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs can be used directly, for example, for such tasks as summarization, question
    answering, and reasoning. Due to the very large amounts of data on which they
    were trained, they can answer very well to a variety of questions on many subjects,
    since they have the context available in that training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In many practical cases, such LLMs can correctly answer our questions on the
    first attempt. In other cases, we will need to provide a few clarifications or
    examples. The quality of the answers in these zero-shot or few-shot approaches
    highly depends on the ability of the user to craft the prompts for LLM. In this
    section, we will show the simplest way to interact with one LLM on Kaggle, using
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting to use an LLM on Kaggle, we will need to perform a few preparation
    steps. We begin by loading the model and then defining a tokenizer. Next, we create
    a model pipeline. In our first code example, we will use AutoTokenizer from transformers
    as a tokenizer and create a pipeline, also using the transformers pipeline. The
    following code (excerpts from the notebook in *Reference 3*) illustrates these
    steps described:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the tokenizer and the pipeline. We then implement
    a function to test the model. The function receives as parameters the tokenizer,
    the pipeline, and the prompt with which we would like to test the model. See the
    following code for the test function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to prompt the model. The model we are using has the following
    characteristics: a Llama 2 model (7b), a chat version from HuggingFace (version
    1), and thePyTorch framework. We will prompt the model with math questions. In
    the next code extract, we initialize the tokenizer and pipeline and then prompt
    the model with a simple arithmetic problem, formulated in plain language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how the model reasons. the following screenshot, we plot the time
    for inference, the prompt, and the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Prompt, answer, and inference time for a math question with the
    Llama 2 model'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this simple math question, the reasoning of the model seems accurate. Let’s
    try again with a different question. In the following code excerpt, we ask a geometry
    question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*The following screenshot shows* the result of prompting the model with the
    preceding geometry question:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Llama 2 model answer to a basic geometry question'
  prefs: []
  type: TYPE_NORMAL
- en: 'The response to simple mathematical questions is not correct all of the time.
    In the following example, we prompted the model with a variation of the first
    algebraic problem. You can see that, in this case, the model took a convoluted
    and wrong path to reach an incorrect solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Llama 2 model solution is wrong for an algebra problem'
  prefs: []
  type: TYPE_NORMAL
- en: Model quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding experiments, we engaged a model with a series of straightforward
    questions. This process underscored the crucial role of crafting clear, well-structured
    prompts to elicit accurate and relevant responses. While Kaggle generously offers
    substantial computational resources at no cost, the sheer size of LLMs presents
    a challenge. These models demand significant RAM and CPU/GPU power for loading
    and inference.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate these demands, we can employ a technique known as model quantization.
    This method effectively reduces the memory and computational requirements of a
    model. It achieves this by representing the model’s weights and activation functions
    using low-precision data types, such as 8-bit or 4-bit integers, instead of the
    standard 32-bit floating-point format. This approach not only conserves resources
    but also maintains a balance between efficiency and performance (see *Reference
    4*).
  prefs: []
  type: TYPE_NORMAL
- en: In our upcoming example, we’ll demonstrate how to quantize a model from Kaggle
    using one of the available techniques, the `llama.cpp` library. We’ve chosen the
    Llama 2 model for this purpose. Llama 2 is, at the time of writing, one of the
    most successful LLMs that you can download (with Meta approval) and use freely.
    It also has demonstrable accuracy for a variety of tasks, on par with many other
    available models. The quantization will be executed using the `llama.cpp` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at a few examples of testing the quantized model. We will start
    by prompting it with a geography question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the prompt is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Result of prompting the quantized Llama 2 model with a geography
    question'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next screenshot, we show the answer of the model to a simple geometry
    question. The answer is quite straightforward and clearly formulated. The code
    to prompt the model and the one to print the result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B20963_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Result of prompting the quantized Llama 2 model with a geometry
    question'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook to illustrate the first method to quantize a Llama 2 model, from
    which we extracted the code and results, is given in *Reference 5*. This notebook
    was run on the GPU. In another notebook given in *Reference 6*, we run the same
    model but on the CPU. It is quite interesting to notice that the time to execute
    the inference on the CPU with the quantized model is much smaller than on the
    GPU (with the same quantized model). See the notebooks in *Reference 5* and *6*
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use alternative approaches for model quantization. For example,
    in *Reference 7*, we used the `bitsandbytes` library for model quantization. In
    order to use this quantization option, we need to install the accelerate library
    and the latest version of `bitsandbytes`. The following code excerpt shows how
    to initialize the model configuration for quantization and load the model with
    this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test the model with a simple prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The answer seems to be correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Answer to a simple geography question (Llama 2 quantized with
    the bitsandbytes library)'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have experimented with prompting models. We directly used the models
    from Kaggle Models, or after quantization. We performed the quantization with
    two different methods. In the next section, however, we will see how, using a
    task-chaining framework such as Langchain, we can extend the power of LLMs and
    create sequences of operations, where the answer of an initial query for the LLM
    is fed as input to the next task.
  prefs: []
  type: TYPE_NORMAL
- en: Building a multi-task application with Langchain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Langchain is the most popular task-chaining framework (*Reference 8*). Task
    chaining is an extension of the prompt engineering concept that we illustrated
    in the previous section. Chains serve as predetermined sequences of operations,
    designed to organize intricate processes in a format that is both more manageable
    and easier to understand. These chains follow a distinct order of actions. They
    are well suited for workflows characterized by a consistent number of steps. With
    task chaining, you can create sequences of prompts, where the output of the previous
    task executed by the framework is fed as input for the next task.
  prefs: []
  type: TYPE_NORMAL
- en: Besides Langchain, there are now several other options available for task chaining,
    like LlamaIndex or Semantic Kernel (from Microsoft). Langchain provides multiple
    functionalities, including specialized tools to ingest data or output results,
    intelligent agents, as well as the possibility to extend it by defining your own
    tasks, tools, or agents. An agent will select and execute tasks based on the perceived
    context, in order to achieve its objective. In order to execute tasks, it will
    use generic or custom tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start working with Langchain by defining a two-step sequence. We will
    define the sequence in a custom function that will receive a parameter and formulate
    an initial prompt, parameterized with the input parameter. Based on the answer
    to the first prompt, we assemble the prompt for the next task. This way, we can
    create the dynamic behavior of our mini-application. The code for defining this
    function is as follows (*Reference 7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The expected input parameter is the name of a country. The first prompt will
    get the most popular food in that country. The next prompt will use the answer
    to the first question to build the second question, which is about the top three
    ingredients for that food.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the functionality of the code with two examples. First, let’s try
    with the `France` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B20963_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Two-step sequential chain execution (ingredients of the most famous
    food in France)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer looks quite convincing. Indeed, tourists in France prefer snails,
    and yes, the top three ingredients for this tasty food are listed correctly. Let’s
    double-check, with another country famous for its delicious food, `Italy`. The
    prompt will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Consequently, the result will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Most popular food in Italy and its ingredients'
  prefs: []
  type: TYPE_NORMAL
- en: We illustrated with an intuitive example how we can use LangChain together with
    an LLM to chain multiple prompts and extend the power of LLMs, for example, in
    the automatization of business processes. In the next section, we will see how
    we can use LLMs for another important task, the automatization of code generation,
    to increase the productivity in the coding process.
  prefs: []
  type: TYPE_NORMAL
- en: Code generation with Kaggle Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For code generation, we will experiment with the Code Llama model, the 13b
    version. From the LLMs available on the Kaggle platform at the time of writing,
    this model was the most appropriate, in regards to its purpose (it is a model
    specialized for code generation) and size (i.e., we can use it with Kaggle Notebooks),
    for the task of code generation. The notebook used to demonstrate the code generation
    is given in *Reference 9*. The model is loaded, quantized using `bitsandbytes`,
    and has a tokenizer initialized in the same way, as demonstrated in *Reference
    7*. We define a prompt and a pipeline (using the transformers function) with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The result of executing the preceding code is given in the following screenshot.
    The code looks functional, but the answer contains more information than expected.
    We obtained this by printing all the sequences outputted. If we just select the
    first one, the answer will be correct (only the code for the circle area).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Code generation: a function to compute the area of a circle'
  prefs: []
  type: TYPE_NORMAL
- en: In the notebook from *Reference 9*, there are more examples; we will not give
    all the details here. You can modify the notebook and generate more answers, by
    changing the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s see how we can further extend the functionality of
    LLMs, by creating a system that retrieves information stored in a special database
    (a vector database), assembling a prompt by combining the initial query with the
    retrieved information (context), and prompting the LLM to answer to the initial
    query by only using the context result from the retrieval step. Such a system
    is called **Retrieval Augmented Generation** (**RAG**).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a RAG system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we explored various approaches to interact with Foundation
    Models – more precisely, available LLMs from Kaggle Models. First, we experimented
    with prompting, directly using the models. Then, we quantized the models with
    two different approaches. We also showed that we can use models to generate code.
    A more complex application included a combination of `LangChain` with an LLM to
    create sequences of connected operations, or task sequences.
  prefs: []
  type: TYPE_NORMAL
- en: In all these cases, the answers of the LLM are based on the information already
    available with the model at the time of training it. If we would like to have
    the LLM answer queries about information that was never presented to the LLM,
    the model might provide a deceiving answer by hallucinating. To counter this tendency
    of models to hallucinate when they do not have the right information, we can fine-tune
    models with our own data. The disadvantage to this is that it is costly, since
    the computational resources needed to fine-tune a large model are very large.
    It also doesn’t necessarily totally eliminate hallucination.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative to this approach is to combine vector databases, task-chaining
    frameworks, and LLMs to create a RAG system (see *Reference 10*). In the following
    figure, we illustrate the functionality of such a system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: RAG system explained'
  prefs: []
  type: TYPE_NORMAL
- en: Before using the RAG system, we will have to ingest the documents into the vector
    database (*Step 1* in *Figure 10.12*). The documents can be in any format, including
    Word, PowerPoint, text, Excel, images, video, email, etc. We first transform each
    modality in the text format (for example, using Tesseract to extract text from
    images, or OpenAI Whisper to convert video into text). After we transform all
    the formats/modalities into text, we will have to split the larger texts into
    fixed-size chunks (partially superposed, to not lose context that might be distributed
    across multiple chunks).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we use one of the options to encode the information before adding the
    pre-processed documents to the vector database. The vector database stores the
    data encoded using text embeddings, and it also uses very efficient indexing for
    such an encoding type, which will allow us to perform a fast search and retrieval
    of information, based on a similarity search. We have multiple options for vector
    databases, like ChromaDB, Weaviate, Pinecone, and FAISS. In our application on
    Kaggle, we used ChromaDB, which has a simple interface, plugins with Langchain,
    is easy to integrate, has options to be used in memory as well as persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is transformed, chunked, encoded, and indexed in the vector database,
    we can start to query our system. Queries are passed through a Langchain specialized
    task – a question and answering retrieval (*Step 2* in *Figure 10.12*). The query
    is used to perform similarity search in the vector database. The retrieved documents
    are used together with the query (*Step 3* in *Figure 10.12*) to compose the prompt
    for LLM (*Step 4* in *Figure 10.12*). The LLM will provide its answer to the query
    by referring only to the context we provided – context from the data stored in
    the vector database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to implement the RAG system is given in *Reference 11*. We will use
    as documents the text of the State of the Union 2023 (from Kaggle datasets). Let’s
    first use the LLM directly by prompting to answer a question about the State of
    the Union in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The answer is given in the following screenshot. We can observe that the LLM
    has the relevant information, and the answer is correct. Of course, if we had
    asked about recent information, the answer might have been wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: Result of the prompt (a general question, without context)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see now some answers to questions about the information we ingested in
    the vector database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data transformation, chunking, and encoding are done with the following code.
    Since the data we ingest is plain text, we will use `TextLoader` from Langchain.
    We will use `ChromaDB` as the vector database, and Sentence Transformer for embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the question and answering retrieval chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define a function to test the preceding chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test the functionality of this system. We will formulate queries about
    the subject – in this case, the State of the Union 2023:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running the preceding query will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Query and answer using the RAG system (example 1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we show the answer to a different query on the same content (the query
    included in the printout):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B20963_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Query and answer using the RAG system (example 2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also retrieve the documents that were used to create the context for
    the answer. The following code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: RAG is a powerful method to leverage the capability of LLMs for reasoning, while
    controlling what the source of information is. The answer given by the LLM is
    only from the context extracted by similarity search (in the first step of a question-and-answer
    retrieval chain), from the vector database where we stored the information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored how we can leverage the potential of Generative
    AI, using LLMs from Kaggle Models. We started by focusing on the simplest way
    to use such Foundation Models – by directly prompting them. We learned that crafting
    a prompt is important and experimented with simple math questions. We used the
    models that were available in Kaggle Models as well as quantized ones and quantized
    models with two approaches: using `Llama.cpp` and the `bitsandbytes` library.
    We then combined Langchain with a LLM to create sequences of chained tasks, where
    the output of one task is used to craft (by the framework) the input (or prompt)
    for the next task. Using the Code Llama 2 model, we tested the feasibility of
    code generation on Kaggle. The results were less than perfect, with multiple sequences
    generated besides the expected one. Finally, we learned how to create a RAG system
    that combines the speed, versatility, and ease of using vector databases with
    the chaining functions of Langchain and the reasoning capabilities of LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, which is also the last chapter of our book, you will learn
    a few useful recipes that will help you to make your high-quality work on the
    platform more visible and appreciated.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Llama 2, Kaggle Models: [https://www.kaggle.com/models/metaresearch/llama-2](https://www.kaggle.com/models/metaresearch/llama-2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mistral, Kaggle Models: [https://www.kaggle.com/models/mistral-ai/mistral/](https://www.kaggle.com/models/mistral-ai/mistral/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test Llama v2 with math, Kaggle Notebooks: [https://www.kaggle.com/code/gpreda/test-llama-v2-with-math](https://www.kaggle.com/code/gpreda/test-llama-v2-with-math)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model Quantization, HuggingFace: [https://huggingface.co/docs/optimum/concept_guides/quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test Llama 2 quantized with Llama.cpp, Kaggle Notebooks: [https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp](https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Test of Llama 2 quantized with llama.cpp (on CPU), Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu](https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Simple sequential chain with Llama 2 and Langchain, Kaggle
    Notebooks: [https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/](https://www.kaggle.com/code/gpreda/simple-sequential-chain-with-llama-2-and-langchain/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Langchain, Wikipedia page: [https://en.wikipedia.org/wiki/LangChain](https://en.wikipedia.org/wiki/LangChain)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Use Code Llama to generate Python code (13b), Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b](https://www.kaggle.com/code/gpreda/use-code-llama-to-generate-python-code-13b)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – Retrieval Augmented Generation, Combining LLMs, Task-Chaining
    and Vector Databases, Endava Blog: [https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases](https://www.endava.com/en/blog/engineering/2023/retrieval-augmented-generation-combining-llms-task-chaining-and-vector-databases)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gabriel Preda – RAG using Llama 2, Langchain and ChromaDB, Kaggle Notebooks:
    [https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb](https://www.kaggle.com/code/gpreda/rag-using-llama-2-langchain-and-chromadb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code9220780366773140.png)'
  prefs: []
  type: TYPE_IMG
