["```py\n    optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n        l2_norm_clip=l2_norm_clip,\n        noise_multiplier=noise_multiplier,\n        num_microbatches=num_microbatches,\n        learning_rate=learning_rate) \n    ```", "```py\n    loss = tf.keras.losses.CategoricalCrossentropy(\n        from_logits=True, reduction=tf.losses.Reduction.NONE) \n    ```", "```py\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) \n    ```", "```py\nfrom opacus import PrivacyEngine\noptimizer= torch.optim.SGD(model.parameters(), lr=learning_rate)\nprivacy_engine = PrivacyEngine(\n    model,\n    sample_rate=sample_rate,\n    max_grad_norm=max_per_sample_grad_norm,\n    noise_multiplier = noise_multiplier \n)\nprivacy_engine.attach(optimizer) \n```", "```py\n    from sagemaker import Session \n    session = Session()\n    bucket = session.default_bucket()\n    prefix = \"sagemaker/bias_explain\"\n    region = session.boto_region_name\n    # Define IAM role\n    from sagemaker import get_execution_role\n    import pandas as pd\n    import numpy as np\n    import os\n    import boto3 \n    role = get_execution_role()\n    s3_client = boto3.client(\"s3\") \n    ```", "```py\n    training_data = pd.read_csv(\"data/churn.csv\").dropna()\n    training_data.head() \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    churn_train, churn_test = train_test_split (training_data, test_size=0.2)\n    Create encoding function to encode catagorical features into numerics\n    from sklearn import preprocessing\n    def number_encode_features(df):\n        result = df.copy()\n        encoders = {}\n        for column in result.columns:\n            if result.dtypes[column] == object:\n                encoders[column] = preprocessing.LabelEncoder()\n                result[column] = encoders[column].fit_transform(result[column].fillna(\"None\"))\n        return result, encoders \n    ```", "```py\n    churn_train = pd.concat([churn_train[\"Exited\"], churn_train.drop([\"Exited\"], axis=1)], axis=1)\n    churn_train, _ = number_encode_features(churn_train)\n    churn_train.to_csv(\"data/train_churn.csv\", index=False, header=False)\n    churn_test, _ = number_encode_features(churn_test)\n    churn_features = churn_test.drop([\"Exited\"], axis=1)\n    churn_target = churn_test[\"Exited\"]\n    churn_features.to_csv(\"data/test_churn.csv\", index=False, header=False)\n    Upload the newly created training and test files to S3 to prepare for model training\n    from sagemaker.s3 import S3Uploader\n    from sagemaker.inputs import TrainingInput\n    train_uri = S3Uploader.upload(\"data/train_churn.csv\", \"s3://{}/{}\".format(bucket, prefix))\n    train_input = TrainingInput(train_uri, content_type=\"csv\") \n    ```", "```py\n    from sagemaker.image_uris import retrieve\n    from sagemaker.estimator import Estimator\n    container = retrieve(\"xgboost\", region, version=\"1.2-1\")\n    xgb = Estimator(container,role, instance_count=1,instance_type=\"ml.m5.xlarge\", disable_profiler=True,sagemaker_session=session,)\n    xgb.set_hyperparameters(max_depth=5, eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,objective=\"binary:logistic\",num_round=800,)\n    xgb.fit({\"train\": train_input}, logs=False) \n    ```", "```py\n    model_name = \"churn-clarify-model\"\n    model = xgb.create_model(name=model_name)\n    container_def = model.prepare_container_def()\n    session.create_model(model_name, role, container_def) \n    ```", "```py\n    from sagemaker import clarify\n    clarify_processor = clarify.SageMakerClarifyProcessor(\n      role=role, instance_count=1, instance_type=\"ml.m5.xlarge\", sagemaker_session=session) \n    ```", "```py\n    bias_report_output_path = \"s3://{}/{}/clarify-bias\".format(bucket, prefix)\n    bias_data_config = clarify.DataConfig(\n        s3_data_input_path=train_uri,\n        s3_output_path=bias_report_output_path,\n        label=\"Exited\",\n        headers=churn_train.columns.to_list(),\n        dataset_type=\"text/csv\") \n    ```", "```py\n    model_config = clarify.ModelConfig(\n        model_name=model_name, instance_type=\"ml.m5.xlarge\",\n        instance_count=1,accept_type=\"text/csv\",\n    content_type=\"text/csv\",) \n    ```", "```py\n    predictions_config = clarify.ModelPredictedLabelConfig(probability_threshold=0.8) \n    ```", "```py\n    bias_config = clarify.BiasConfig( \n        label_values_or_threshold=[1], facet_name=\"Gender\", facet_values_or_threshold=[0]) \n    ```", "```py\n    clarify_processor.run_bias(\n        data_config=bias_data_config,\n        bias_config=bias_config,\n        model_config=model_config,\n        model_predicted_label_config=predictions_config,\n        pre_training_methods=\"all\",\n        post_training_methods=\"all\") \n    ```", "```py\n    shap_config = clarify.SHAPConfig(\n        baseline=[churn_features.iloc[0].values.tolist()],\n        num_samples=15,\n        agg_method=\"mean_abs\",\n        save_local_shap_values=True,) \n    ```", "```py\n    explainability_output_path = \"s3://{}/{}/clarify-explainability\".format(bucket, prefix)\n    explainability_data_config = clarify.DataConfig(\n        s3_data_input_path=train_uri,\n        s3_output_path=explainability_output_path,\n        label=\"Exited\",\n        headers=churn_train.columns.to_list(),\n        dataset_type=\"text/csv\") \n    ```", "```py\n    clarify_processor.run_explainability(\n        data_config=explainability_data_config,\n        model_config=model_config,\n        explainability_config=shap_config,) \n    ```", "```py\n    Max_per_sample_grad_norm = 1.5\n    sample_rate = batch_size/len(train_ds)\n    noise_multiplier = 0.8 \n    ```", "```py\n    from opacus import PrivacyEngine\n    net = get_CHURN_model()\n    optimizer = optim.Adam(net.parameters(), weight_decay=0.0001, lr=0.003)\n    privacy_engine = PrivacyEngine()\n    model, optimizer, dataloader = privacy_engine.make_private(\n        module=net,\n        noise_multiplier=noise_multiplier,\n        max_grad_norm=max_per_sample_grad_norm,\n        optimizer = optimizer,\n        data_loader = trainloader)\n    model = train(dataloader, model, optimizer, batch_size) \n    ```", "```py\n    epsilon = privacy_engine.accountant.get_epsilon(delta=1e-5)\n    print (f\" Îµ = {epsilon:.2f}\") \n    ```"]