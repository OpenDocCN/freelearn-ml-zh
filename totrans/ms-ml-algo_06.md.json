["```py\nimport numpy as np\n\nw = np.array([1.0, 0.2])\nx = np.array([0.1, 0.5])\nalpha = 0.0\n\nfor i in range(50):\n    y = np.dot(w, x.T)\n    w += x*y\n    alpha = np.arccos(np.dot(w, x.T) / (np.linalg.norm(w) * np.linalg.norm(x)))\n\nprint(w)\n[  8028.48942243  40137.64711215]\n\nprint(alpha * 180.0 / np.pi)\n0.00131766983584\n```", "```py\nw = np.array([1.0, -1.0])\n\n...\n\nprint(w)\n[-16053.97884486 -80275.89422431]\n\nprint(alpha * 180.0 / np.pi)\n179.999176456\n```", "```py\nimport numpy as np\n\nrs = np.random.RandomState(1000)\nX = rs.normal(loc=1.0, scale=(20.0, 1.0), size=(1000, 2))\n\nw = np.array([30.0, 3.0])\n\nS = np.cov(X.T)\n\nfor i in range(10):\n    w += np.dot(S, w)\n    w /= np.linalg.norm(w)\n\nw *= 50.0\n\nprint(np.round(w, 1))\n[ 50\\.  -0.]\n```", "```py\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\n\nX, _ = make_blobs(n_samples=500, centers=2, cluster_std=5.0, random_state=1000)\nXs = zero_center(X)\n\nQ = np.cov(Xs.T)\neigu, eigv = np.linalg.eig(Q)\n\nprint(eigu)\n[ 24.5106037   48.99234467]\n\nprint(eigv)\n[[-0.75750566 -0.6528286 ]\n [ 0.6528286  -0.75750566]]\n\nn_components = 2\n\nW_sanger = np.random.normal(scale=0.5, size=(n_components, Xs.shape[1]))\nW_sanger /= np.linalg.norm(W_sanger, axis=1).reshape((n_components, 1))\n```", "```py\nlearning_rate = 0.01\nnb_iterations = 5000\nt = 0.0\n\nfor i in range(nb_iterations):\n    dw = np.zeros((n_components, Xs.shape[1]))\n    t += 1.0\n\n    for j in range(Xs.shape[0]):\n        Ysj = np.dot(W_sanger, Xs[j]).reshape((n_components, 1))\n        QYd = np.tril(np.dot(Ysj, Ysj.T))\n        dw += np.dot(Ysj, Xs[j].reshape((1, X.shape[1]))) - np.dot(QYd, W_sanger)\n\n    W_sanger += (learning_rate / t) * dw\n    W_sanger /= np.linalg.norm(W_sanger, axis=1).reshape((n_components, 1))\n```", "```py\nprint(W_sanger.T)\n[[-0.6528286  -0.75750566]\n [-0.75750566  0.6528286 ]]\n```", "```py\nimport numpy as np\n\nn_components = 2\nlearning_rate = 0.0001\nmax_iterations = 1000\nstabilization_cycles = 5\nthreshold = 0.00001\n\nW = np.random.normal(0.0, 0.5, size=(Xs.shape[1], n_components))\nV = np.tril(np.random.normal(0.0, 0.01, size=(n_components, n_components)))\nnp.fill_diagonal(V, 0.0)\n\nprev_W = np.zeros((Xs.shape[1], n_components))\nt = 0\n```", "```py\nwhile(np.linalg.norm(W - prev_W, ord='fro') > threshold and t < max_iterations):\n    prev_W = W.copy()\n    t += 1\n\n    for i in range(Xs.shape[0]):\n        y_p = np.zeros((n_components, 1))\n        xi = np.expand_dims(Xs[i], 1)\n        y = None\n\n        for _ in range(stabilization_cycles):\n            y = np.dot(W.T, xi) + np.dot(V, y_p)\n            y_p = y.copy()\n\n        dW = np.zeros((Xs.shape[1], n_components))\n        dV = np.zeros((n_components, n_components))\n\n        for t in range(n_components):\n            y2 = np.power(y[t], 2)\n            dW[:, t] = np.squeeze((y[t] * xi) + (y2 * np.expand_dims(W[:, t], 1)))\n            dV[t, :] = -np.squeeze((y[t] * y) + (y2 * np.expand_dims(V[t, :], 1)))\n\n        W += (learning_rate * dW)\n        V += (learning_rate * dV)\n\n        V = np.tril(V)\n        np.fill_diagonal(V, 0.0)\n\n        W /= np.linalg.norm(W, axis=0).reshape((1, n_components))\n```", "```py\nprint(W)\n[[-0.65992841  0.75897537]\n [-0.75132849 -0.65111933]]\n\nY_comp = np.zeros((Xs.shape[0], n_components))\n\nfor i in range(Xs.shape[0]):\n        y_p = np.zeros((n_components, 1))\n        xi = np.expand_dims(Xs[i], 1)\n\n        for _ in range(stabilization_cycles):\n            Y_comp[i] = np.squeeze(np.dot(W.T, xi) + np.dot(V.T, y_p))\n            y_p = y.copy()\n\nprint(np.cov(Y_comp.T))\n[[ 48.9901765   -0.34109965]\n [ -0.34109965  24.51072811]]\n```", "```py\nimport numpy as np\n\nfrom sklearn.datasets import fetch_olivetti_faces\n\nfaces = fetch_olivetti_faces(shuffle=True)\n\nXcomplete = faces['data'].astype(np.float64) / np.max(faces['data'])\nnp.random.shuffle(Xcomplete)\n\nnb_iterations = 5000\nnb_startup_iterations = 500\npattern_length = 64 * 64\npattern_width = pattern_height = 64\neta0 = 1.0\nsigma0 = 3.0\ntau = 100.0\n\nX = Xcomplete[0:100]\nmatrix_side = 5\n```", "```py\nW = np.random.normal(0, 0.1, size=(matrix_side, matrix_side, pattern_length))\n```", "```py\ndef winning_unit(xt):\n    distances = np.linalg.norm(W - xt, ord=2, axis=2)\n    max_activation_unit = np.argmax(distances)\n    return int(np.floor(max_activation_unit / matrix_side)), max_activation_unit % matrix_side\n```", "```py\ndef eta(t):\n    return eta0 * np.exp(-float(t) / tau)\n\ndef sigma(t):\n    return float(sigma0) * np.exp(-float(t) / tau)\n```", "```py\nprecomputed_distances = np.zeros((matrix_side, matrix_side, matrix_side, matrix_side))\n\nfor i in range(matrix_side):\n    for j in range(matrix_side):\n        for k in range(matrix_side):\n            for t in range(matrix_side):\n                precomputed_distances[i, j, k, t] = \\\n                    np.power(float(i) - float(k), 2) + np.power(float(j) - float(t), 2)\n\ndef distance_matrix(xt, yt, sigmat):\n    dm = precomputed_distances[xt, yt, :, :]\n    de = 2.0 * np.power(sigmat, 2)\n    return np.exp(-dm / de)\n```", "```py\nsequence = np.arange(0, X.shape[0])\nt = 0\n\nfor e in range(nb_iterations):\n    np.random.shuffle(sequence)\n    t += 1\n\n    if e < nb_startup_iterations:\n        etat = eta(t)\n        sigmat = sigma(t)\n    else:\n        etat = 0.2\n        sigmat = 1.0\n\n    for n in sequence:\n        x_sample = X[n]\n\n        xw, yw = winning_unit(x_sample)\n        dm = distance_matrix(xw, yw, sigmat)\n\n        dW = etat * np.expand_dims(dm, axis=2) * (x_sample - W)\n        W += dW\n\n    W /= np.linalg.norm(W, axis=2).reshape((matrix_side, matrix_side, 1))\n```"]