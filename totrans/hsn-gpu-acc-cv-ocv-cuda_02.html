<html><head></head><body><div><div><h1 class="header-title">Parallel Programming using CUDA C</h1>
                
            
            
                
<p>In the last chapter, we saw how easy it is to install CUDA and write a program using it. Though the example was not impressive, it was shown to convince you that it is very easy to get started with CUDA. In this chapter, we will build upon this concept. It teaches you to write advance programs using CUDA for GPUs in detail. It starts with a variable addition program and then incrementally builds towards complex vector manipulation examples in CUDA C. It also covers how the kernel works and how to use device properties in CUDA programs. The chapter discusses how vectors are operated upon in CUDA programs and how CUDA can accelerate vector operations compared to CPU processing. It also discusses terminologies associated with CUDA programming.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The concept of the kernel call</li>
<li>Creating kernel functions and passing parameters to it in CUDA</li>
<li>Configuring kernel parameters and memory allocation for CUDA programs</li>
<li>Thread execution in CUDA programs</li>
<li>Accessing GPU device properties from CUDA programs</li>
<li>Working with vectors in CUDA programs</li>
<li>Parallel communication patterns</li>
</ul>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>This chapter requires familiarity with the basic C or C++ programming language, particularly dynamic memory allocation functions. All the code used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Accelerated-Computer-Vision-with-OpenCV-and-CUDA</a>. The code can be executed on any operating system, though it is only tested on Windows 10 and Ubuntu 16.04. </p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2PQmu4O">http://bit.ly/2PQmu4O</a></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">CUDA program structure</h1>
                
            
            
                
<p>We have seen a very simple <kbd>Hello, CUDA!</kbd> program earlier, that showcased some important concepts related to CUDA programs. A CUDA program is a combination of functions that are executed either on the host or on the GPU device. The functions that do not exhibit parallelism are executed on the CPU, and the functions that exhibit data parallelism are executed on the GPU. The GPU compiler segregates these functions during compilation. As seen in the previous chapter, functions meant for execution on the device are defined using the <kbd>__global__</kbd> keyword and compiled by the NVCC compiler, while normal C host code is compiled by the C compiler. A CUDA code is basically the same ANSI C code with the addition of some keywords needed for exploiting data parallelism.</p>
<p>So, in this section, a simple two-variable addition program is taken to explain important concepts related to CUDA programming, such as kernel calls, passing parameters to kernel functions from host to device, the configuration of kernel parameters, CUDA APIs needed to exploit data parallelism, and how memory allocation takes place on the host and the device. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Two-variable addition program in CUDA C</h1>
                
            
            
                
<p>In the simple <kbd>Hello, CUDA!</kbd> code seen in <a href="26a373fe-8ec0-40bf-afb7-7db6a1d414c9.xhtml">Chapter 1</a>, <em>Introducing Cuda and Getting Started with Cuda,</em> the device function was empty. It had nothing to do. This section explains a simple addition program that performs addition of two variables on the device. Though it is not exploiting any data parallelism of the device, it is very useful for demonstrating important programming concepts of CUDA C. First, we will see how to write a kernel function for adding two variables.</p>
<p>The code for the kernel function is shown here:</p>
<pre>include &lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>//Definition of kernel function to add two variables<br/>__global__ void gpuAdd(int d_a, int d_b, int *d_c) <br/>{<br/>   *d_c = d_a + d_b;<br/>}</pre>
<p><br/>
The <kbd>gpuAdd</kbd> function looks very similar to a normal <kbd>add</kbd> function implemented in ANSI C. It takes two integer variables <kbd>d_a</kbd> and <kbd>d_b</kbd> as inputs and stores the addition at the memory location indicated by the third integer pointer <kbd>d_c</kbd>. The return value for the device function is void because it is storing the answer in the memory location pointed to by the device pointer and not explicitly returning any value. Now we will see how to write the main function for this code. The code for the main function is shown here:</p>
<pre><br/> int main(void) <br/>{<br/> //Defining host variable to store answer<br/>   int h_c;<br/> //Defining device pointer<br/>   int *d_c;<br/> //Allocating memory for device pointer<br/>   cudaMalloc((void**)&amp;d_c, sizeof(int));<br/> //Kernel call by passing 1 and 4 as inputs and storing answer in d_c<br/> //&lt;&lt; &lt;1,1&gt; &gt;&gt; means 1 block is executed with 1 thread per block<br/>   gpuAdd &lt;&lt; &lt;1, 1 &gt;&gt; &gt; (1, 4, d_c);<br/> //Copy result from device memory to host memory<br/>   cudaMemcpy(&amp;h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);<br/>   printf("1 + 4 = %d\n", h_c);<br/> //Free up memory<br/>   cudaFree(d_c);<br/>   return 0;<br/>}</pre>
<p>In the <kbd>main</kbd> function, the first two lines define variables for host and device. The third line allocates memory of the <kbd>d_c</kbd> variable on the device using the <kbd>cudaMalloc</kbd> function. The <kbd>cudaMalloc</kbd> function is similar to the <kbd>malloc</kbd> function in C. In the fourth line of the main function, <kbd>gpuAdd</kbd> is called with <kbd>1</kbd> and <kbd>4</kbd> as two input variables and <kbd>d_c</kbd>, which is a device memory pointer as an output pointer variable. The weird syntax of the <kbd>gpuAdd</kbd> function, which is also called a kernel call, is explained in the next section. If the answer of <kbd>gpuAdd</kbd> needs to be used on the host, then it must be copied from the device's memory to the host's memory, which is done by the <kbd>cudaMemcpy</kbd> function. Then, this answer is printed using the <kbd>printf</kbd> function. The penultimate line frees the memory used on the device by using the <kbd>cudafree</kbd> function. It is very important to free up all the memory used on the device explicitly from the program; otherwise, you might run out of memory at some point. The lines that start with <kbd>//</kbd> are comments for more code readability, and these lines are ignored by compilers.</p>
<p>The two-variable addition program has two functions, <kbd>main</kbd> and <kbd>gpuAdd</kbd>. As you can see, <kbd>gpuAdd</kbd> is defined by using the <kbd>__global__ </kbd> keyword, and hence it is meant for execution on the device, while the main function will be executed on the host. The program adds two variables on the device and prints the output on the command line, as shown here:</p>
<div><img class="alignnone size-full wp-image-158 image-border" src="img/9869b1ed-27f0-432f-ab68-0b41b03af8e1.png" style="" width="263" height="67"/></div>
<p>We will use a convention in this book that host variables will be prefixed with <kbd>h_</kbd> and device variables will be prefixed with <kbd>d_</kbd>.  This is not compulsory; it is just done so that readers can understand the concepts easily without any confusion between host and device.</p>
<p>All CUDA APIs such as <kbd>cudaMalloc</kbd>, <kbd>cudaMemcpy</kbd>, and <kbd>cudaFree</kbd>, along with other important CUDA programming concepts such as kernel call, passing parameters to kernels, and memory allocation issues are discussed in upcoming sections.   </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">A kernel call</h1>
                
            
            
                
<p>The device code that is written using ANSI C keywords along with CUDA extension keywords is called a <strong>kernel</strong>. It is launched from the host code by a method called <strong>kernel call</strong>. Basically, the meaning of kernel call is that we are launching device code from the host code. A kernel call typically generates a large number of blocks and threads to exploit data parallelism on the GPU. Kernel code is very similar to normal C functions; it is just that this code is executed by several threads in parallel. It has a very weird syntax, which is as follows:</p>
<pre>kernel &lt;&lt; &lt;number of blocks, number of threads per block, size of shared memory &gt; &gt;&gt; (parameters for kernel)</pre>
<p>It starts with the name of the kernel that we want to launch. You should make sure that this kernel is defined using the <kbd>__global__</kbd>  keyword. Then, it has the <kbd>&lt;&lt; &lt; &gt; &gt;&gt;</kbd> kernel launch operator that contains configuration parameters for kernel. It can include three parameters separated by a comma. The first parameter indicates the number of blocks you want to execute, and the second parameter indicates the number of threads each block will have. So, the total number of threads started by a kernel launch will be the product of these two numbers. The third parameter, which specifies the size of shared memory used by the kernel, is optional. In the program for variable addition, the kernel launch syntax is as follows:</p>
<pre>gpuAdd &lt;&lt; &lt;1,1&gt; &gt;&gt; (1 , 4, d_c)</pre>
<p>Here, <kbd>gpuAdd</kbd> is the name of a kernel that we want to launch, and <kbd>&lt;&lt;&lt;1,1&gt;&gt;&gt;</kbd> indicates we want to start one block with one thread per block, which means that we are starting only one thread. Three arguments in round brackets are the parameters that are passed to the kernel. Here, we are passing two constants,  <kbd>1</kbd> and <kbd>4</kbd>. The third parameter is a pointer to device memory <kbd>d_c</kbd>.  It points at the location on device memory where the kernel will store its answer after addition. One thing that the programmer has to keep in mind is that pointers passed as parameters to kernel should only point to device memory. If it is pointing to host memory, it can crash your program. After kernel execution is completed, the result pointed by the device pointer can be copied back to host memory for further use. Starting only one thread for execution on the device is not the optimal use of device resources. Suppose you want to start multiple threads in parallel; what is the modification that you have to make in the syntax of the kernel call? This is addressed in the next section and is termed "configuring kernel parameters".</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Configuring kernel parameters</h1>
                
            
            
                
<p>For starting multiple threads on the device in parallel, we have to configure parameters in the kernel call, which are written inside the kernel launch operator. They specify the number of blocks and the number of threads per block. We can launch many blocks in parallel with many threads in each block. Normally, there is a limit of 512 or 1,024 threads per block. Each block runs on the streaming multiprocessor, and threads in one block can communicate with one another via shared memory. The programmer can't choose which multiprocessor will execute a particular block and in which order blocks or threads will execute. </p>
<p>Suppose you want to start 500 threads in parallel; what is the modification that you can make to the kernel launch syntax that was shown previously? One option is to start one block of 500 threads via the following syntax:</p>
<pre>gpuAdd&lt;&lt; &lt;1,500&gt; &gt;&gt; (1,4, d_c)</pre>
<p>We can also start 500 blocks of one thread each or two blocks of 250 threads each. Accordingly, you have to modify values in the kernel launch operator. The programmer has to be careful that the number of threads per block does not go beyond the maximum supported limit of your GPU device. In this book, we are targeting computer vision applications where we need to work on two-and three-dimensional images. Here, it would be great if blocks and threads are not one-dimensional but more than that for better processing and visualization. </p>
<p>GPU supports a three-dimensional grids of blocks and three-dimensional blocks of threads. It has the following syntax:</p>
<pre>mykernel&lt;&lt; &lt;dim3(Nbx, Nby,Nbz), dim3(Ntx, Nty,Ntz) &gt; &gt;&gt; ()  </pre>
<p>Here <kbd>N<sub>bx</sub></kbd>, <kbd>N<sub>by</sub></kbd>, and <kbd>N<sub>bz</sub></kbd><sub>  </sub>indicate the number of blocks in a grid in the direction of the <em>x</em>, <em>y</em>, and <em>z</em> axes, respectively. Similarly, <kbd>N<sub>t</sub><sub>x</sub></kbd>, <kbd>N<sub>ty</sub></kbd>, and <kbd>N<sub>tz</sub></kbd> indicate the number of threads in a block in the direction of the <em>x</em>, <em>y</em>, and z axes. If the <em>y</em> and z dimensions are not specified, they are taken as <kbd>1</kbd> by default. So, for example, to process an image, you can start a 16 x 16 grid of blocks, all containing 16 x 16 threads. The syntax will be as follows:</p>
<pre>mykernel &lt;&lt; &lt;dim3(16,16),dim3(16,16)&gt; &gt;&gt; ()</pre>
<p>To summarize, the configuration of the number of blocks and the number of threads is very important while launching the kernel. It should be chosen with proper care depending on the application that we are working on and the GPU resources. The next section will explain some important CUDA functions added over regular ANSI C functions.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">CUDA API functions</h1>
                
            
            
                
<p>In the variable addition program, we have encountered some functions or keywords that are not familiar to regular C or C++ programmers. These keywords and functions include <kbd>__global__</kbd>  , <kbd>cudaMalloc</kbd>, <kbd>cudaMemcpy</kbd>, and <kbd>cudaFree</kbd>. So, in this section, these functions are explained in detail one by one:</p>
<ul>
<li><strong>__global__</strong> :<strong> </strong>It is one of three qualifier keywords, along with <kbd>__device__</kbd>  and <kbd>__host__</kbd> . This keyword indicates that a function is declared as a device function and will execute on the device when called from the host. It should be kept in mind that this function can only be called from the host. If you want your function to execute on the device and called from the device function, then you have to use the <kbd>__device__</kbd> keyword. The <kbd>__host__</kbd>  keyword is used to define host functions that can only be called from other host functions. This is similar to normal C functions. By default, all functions in a program are host functions. Both <kbd>__host__</kbd> and <kbd>__device__</kbd> can be simultaneously used to define any function. It generates two copies of the same function. One will execute on the host, and the other will execute on the device.</li>
</ul>
<p> </p>
<ul>
<li><strong>cudaMalloc</strong>:<strong> </strong>It is similar to the <kbd>Malloc</kbd> function used in C for dynamic memory allocation. This function is used to allocate a memory block of a specific size on the device. The syntax of <kbd>cudaMalloc</kbd> with an example is as follows:</li>
</ul>
<pre style="padding-left: 60px">cudaMalloc(void ** d_pointer, size_t size)<br/>Example: cudaMalloc((void**)&amp;d_c, sizeof(int));</pre>
<p style="padding-left: 60px">As shown in the preceding example code, it allocates a memory block of size equal to the size of one integer variable and returns the pointer <kbd>d_c</kbd>, which points to this memory location. </p>
<ul>
<li><strong>cudaMemcpy</strong>:<strong> </strong>This function is similar to the <kbd>Memcpy</kbd> function in C. It is used to copy one block of memory to other blocks on a host or a device. It has the following syntax:</li>
</ul>
<pre style="padding-left: 60px">cudaMemcpy ( void * dst_ptr, const void * src_ptr, size_t size, enum cudaMemcpyKind kind )<br/>Example: cudaMemcpy(&amp;h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);</pre>
<p style="padding-left: 60px">This function has four arguments. The first and second arguments are the destination pointer and the source pointer, which point to the host or device memory location. The third argument indicates the size of the copy and the last argument indicates the direction of  the copy. It can be from host to device, device to device, host to host, or device to host. But be careful, as you have to match this direction with the appropriate pointers as the first two arguments. As shown in the example, we are copying a block of one integer variable from the device to the host by specifying the device pointer <kbd>d_c</kbd> as the source, and the host pointer <kbd>h_c</kbd> as a destination.</p>
<ul>
<li> <strong>cudaFree</strong>: It is similar to the free function available in C. The syntax of <kbd>cudaFree</kbd> is as follows:</li>
</ul>
<pre style="padding-left: 60px">cudaFree ( void * d_ptr )<br/>Example: cudaFree(d_c)</pre>
<p style="padding-left: 60px">It frees the memory space pointed to by <kbd>d_ptr</kbd>. In the example code, it frees the memory location pointed to by <kbd>d_c</kbd>. Please make sure that <kbd>d_c</kbd> is allocated memory, using  <kbd>cudaMalloc</kbd> to free it using <kbd>cudaFree</kbd>.</p>
<p style="padding-left: 60px">There are many other keywords and functions available in CUDA over and above existing ANSI C functions. We will be frequently using only these three functions, and hence they are discussed in this section. For more details, you can always visit the CUDA programming guide.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Passing parameters to CUDA functions</h1>
                
            
            
                
<p>The <kbd>gpuAdd</kbd> kernel function of the variable addition program is very similar to the normal C function. So, like normal C functions, the kernel functions can also be passed parameters by value or by reference. Hence, in this section, we will see both the methods to pass parameters for CUDA kernels.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Passing parameters by value</h1>
                
            
            
                
<p>If you recall, in the <kbd>gpuAdd</kbd> program, the syntax for calling the kernel was as follows:</p>
<pre>gpuAdd &lt;&lt; &lt;1,1&gt; &gt;&gt;(1,4,d_c)</pre>
<p>On the other hand, the signature of the <kbd>gpuAdd</kbd> function in definition was as follows:</p>
<pre>__global__  gpuAdd(int d_a, int d_b, int *d_c) </pre>
<p>So, you can see that we are passing values of <kbd>d_a</kbd> and <kbd>d_b</kbd> while calling the kernel. First, parameter <kbd>1</kbd> will be copied to <kbd>d_a</kbd> and then parameter <kbd>4</kbd> will be copied to <kbd>d_b</kbd> while calling the kernal. The answer after addition will be stored at the address pointed by <kbd>d_c</kbd> on device memory. Instead of directly passing values <kbd>1</kbd> and <kbd>4</kbd> as inputs to the kernel, we can also write the following:</p>
<pre>gpuAdd &lt;&lt; &lt;1,1&gt; &gt;&gt;(a,b,d_c)</pre>
<p>Here, <kbd>a</kbd> and <kbd>b</kbd> are integer variables that can contain any integer values. Passing parameters by values is not recommended, as it creates unnecessary confusion and complications in programs. It is better to pass parameters by reference.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Passing parameters by reference</h1>
                
            
            
                
<p>Now we will see how to write the same program by passing parameters by reference. For that, we have to first modify the kernel function for addition of two variables. The modified kernel for passing parameters by reference is shown here:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>//Kernel function to add two variables, parameters are passed by reference<br/> __global__ void gpuAdd(int *d_a, int *d_b, int *d_c) <br/>{<br/>  *d_c = *d_a + *d_b;<br/>}</pre>
<p>Instead of using integer variables <kbd>d_a</kbd> and <kbd>d_b</kbd> as inputs to the kernel, the pointers to these variables on device <kbd>*d_a</kbd> and <kbd>*d_b</kbd> are taken as inputs. The answer which will be obtained after the addition is stored at the memory location pointed by third integer pointer <kbd>d_c</kbd>. The pointers passed as a reference to this device function should be allocated memory with the <kbd>cudaMalloc</kbd> function. The main function for this code is shown here:  </p>
<pre>int main(void) <br/>{<br/>  //Defining host and variables<br/>  int h_a,h_b, h_c;<br/>  int *d_a,*d_b,*d_c;<br/>  //Initializing host variables<br/>  h_a = 1;<br/>  h_b = 4;<br/>  //Allocating memory for Device Pointers<br/>  cudaMalloc((void**)&amp;d_a, sizeof(int));<br/>  cudaMalloc((void**)&amp;d_b, sizeof(int));<br/>  cudaMalloc((void**)&amp;d_c, sizeof(int));<br/>  //Coping value of host variables in device memory<br/>  cudaMemcpy(d_a, &amp;h_a, sizeof(int), cudaMemcpyHostToDevice);<br/>  cudaMemcpy(d_b, &amp;h_b, sizeof(int), cudaMemcpyHostToDevice);<br/>  //Calling kernel with one thread and one block with parameters passed by reference<br/>  gpuAdd &lt;&lt; &lt;1, 1 &gt;&gt; &gt; (d_a, d_b, d_c);<br/>  //Coping result from device memory to host<br/>  cudaMemcpy(&amp;h_c, d_c, sizeof(int), cudaMemcpyDeviceToHost);<br/>  printf("Passing Parameter by Reference Output: %d + %d = %d\n", h_a, h_b, h_c);<br/>  //Free up memory<br/>  cudaFree(d_a);<br/>  cudaFree(d_b);<br/>  cudaFree(d_c);<br/>  return 0;<br/> }</pre>
<p><kbd>h_a</kbd>, <kbd>h_b</kbd>, and <kbd>h_c</kbd> are variables in the host memory. They are defined like normal C code. On the other hand, <kbd>d_a</kbd>, <kbd>d_b</kbd>, and <kbd>d_c</kbd> are pointers residing on host memory, and they point to the device memory. They are allocated memory from the host by using the <kbd>cudaMalloc</kbd> function. The values of <kbd>h_a</kbd> and <kbd>h_b</kbd> are copied to the device memory pointed to by <kbd>d_a</kbd> and <kbd>d_b</kbd> by using the <kbd>cudaMemcpy</kbd> function, and the direction of data transfer is from the host to the device. Then, in kernel call, these three device pointers are passed to the kernel as parameters. The kernel computes addition and stores the result at the memory location pointed by <kbd>d_c</kbd>. The result is copied back to the host memory by using <kbd>cudaMemcpy</kbd> again, but this time with the direction of data transfer as the device to host. The output of the program is as follows:</p>
<div><img class="alignnone size-full wp-image-159 image-border" src="img/9097c292-8aee-4dbc-964a-31ec2a203559.png" style="" width="397" height="69"/></div>
<p>The memory used by three device pointers is freed by using the <kbd>cudaFree</kbd> at the end of the program. The sample memory map on the host and the device will look similar to the following:</p>
<div><table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 49%" colspan="2"><strong>Host Memory (CPU)</strong></td>
<td style="width: 43%" colspan="2"><strong>Device Memory (GPU)</strong></td>
</tr>
<tr>
<td style="width: 23.9895%">Address</td>
<td style="width: 25.0105%">Value</td>
<td style="width: 21%">Address</td>
<td style="width: 22%">Value</td>
</tr>
<tr>
<td style="width: 23.9895%">#01</td>
<td style="width: 25.0105%">h_a=1</td>
<td style="width: 21%">#01</td>
<td style="width: 22%">1</td>
</tr>
<tr>
<td style="width: 23.9895%">#02</td>
<td style="width: 25.0105%">h_b=4</td>
<td style="width: 21%">#02</td>
<td style="width: 22%">4</td>
</tr>
<tr>
<td style="width: 23.9895%">#03</td>
<td style="width: 25.0105%">h_c=5</td>
<td style="width: 21%">#03</td>
<td style="width: 22%">5</td>
</tr>
<tr>
<td style="width: 23.9895%">#04</td>
<td style="width: 25.0105%">d_a=#01</td>
<td style="width: 21%">#04</td>
<td style="width: 22%"/>
</tr>
<tr>
<td style="width: 23.9895%">#05</td>
<td style="width: 25.0105%">d_b=#02</td>
<td style="width: 21%">#05</td>
<td style="width: 22%"/>
</tr>
<tr>
<td style="width: 23.9895%">#06</td>
<td style="width: 25.0105%">d_c=#03</td>
<td style="width: 21%">#06</td>
<td style="width: 22%"/>
</tr>
</tbody>
</table>
</div>
<p>As you can see from the table, <kbd>d_a</kbd>, <kbd>d_b</kbd>, and <kbd>d_c</kbd> are residing on the host and pointing to values on the device memory. While passing parameters by reference to kernels, you should take care that all pointers are pointing to the device memory only. If it is not the case, then the program may crash. </p>
<p>While using device pointers and passing them to kernels, there are some restrictions that have to be followed by the programmer. The device pointers that are allocated memory using <kbd>cudaMalloc</kbd> can only be used to read or write from the device memory. They can be passed as parameters to the device function, and they should not be used to read and write memory from the host functions. To simplify, device pointers should be used to read and write device memory from the device function, and host pointers should be used to read and write host memory from host functions. So, in this book, you will always find the device pointer prefixed by <kbd>d_</kbd> in kernel functions.</p>
<p>To summarize, in this section, concepts related to CUDA programming were explained in detail by taking two-variable additional programs as an example. After this section, you should be familiar with basic CUDA programming concepts and the terminology associated with CUDA programs. In the next section, you will learn how threads are executed on the device. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Executing threads on a device</h1>
                
            
            
                
<p>We have seen that, while configuring kernel parameters, we can start multiple blocks and multiple threads in parallel. So, in which order do these blocks and threads start and finish their execution? It is important to know this if we want to use the output of one thread in other threads. To understand this, we have modified the kernel in the <kbd>hello,CUDA!</kbd> program we saw in the first chapter, by including a print statement in the kernel call, which prints the block number. The modified code is as follows:</p>
<pre>#include &lt;iostream&gt;<br/>#include &lt;stdio.h&gt;<br/>__global__ void myfirstkernel(void) <br/>{<br/>  //blockIdx.x gives the block number of current kernel<br/>   printf("Hello!!!I'm thread in block: %d\n", blockIdx.x);<br/>}<br/>int main(void) <br/>{<br/>   //A kernel call with 16 blocks and 1 thread per block<br/>   myfirstkernel &lt;&lt; &lt;16,1&gt;&gt; &gt;();<br/> <br/>   //Function used for waiting for all kernels to finish<br/>   cudaDeviceSynchronize();<br/><br/>   printf("All threads are finished!\n");<br/>   return 0;<br/>}</pre>
<p>As can be seen from the code, we are launching a kernel with 16 blocks in parallel with each block having a single thread. In the kernel code, we are printing the block ID of the kernel execution. We can think that 16 copies of the same <kbd>myfirstkernel</kbd> start execution in parallel. Each of these copies will have a unique block ID, which can be accessed by the <kbd>blockIdx.x CUDA</kbd> directive, and a unique thread ID, which can be accessed by <kbd>threadIdx.x</kbd>. These IDs will tell us which block and thread are executing the kernel. When you run the program many times, you will find that, each time, blocks execute in a different order. One sample output can be shown as follows:  </p>
<div><img class="alignnone size-full wp-image-160 image-border" src="img/50a8685b-96c7-4229-8ac2-2562c57a9a02.png" style="" width="257" height="321"/></div>
<p>One question you should ask is how many different output patterns will the previous program produce? The correct answer is 16! It will produce <em>n</em> factorial number of outputs, where <em>n</em> indicates the number of blocks started in parallel. So, whenever you are writing the program in CUDA, you should be careful that the blocks execute in random order.</p>
<p class="mce-root">This program also contains one more CUDA directive: <kbd>cudaDeviceSynchronize()</kbd>. Why is it used? It is used because a  kernel launch is an asynchronous process, which means it returns control to the CPU thread immediately after starting up the GPU process before the kernel has finished executing. In the previous code, the next line in CPU thread is <kbd>print</kbd> and application exit will terminate console before the kernel has finished execution. So, if we do not include this directive, you will not see any print statements of the kernel execution. The output that is generated later by the kernel has nowhere to go, and you won't see it. To see the outputs generated by the kernel, we will include this directive, which ensures that the kernel finishes before the application is allowed to exit, and the output from the kernel will find a <strong>waiting standard output queue</strong>.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Accessing GPU device properties from CUDA programs</h1>
                
            
            
                
<p class="mce-root">CUDA provides a simple interface to find the information such as determining which CUDA-enabled GPU devices (if any) are present and what capabilities each device supports. First, it is important to get a count of how many CUDA-enabled devices are present on the system, as a system may contain more than one GPU-enabled device. This count can be determined by the CUDA API <kbd>cudaGetDeviceCount()</kbd>. The program for getting a number of CUDA enabled devices on the system is shown here:</p>
<pre>#include &lt;memory&gt;<br/>#include &lt;iostream&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>// Main Program <br/>int main(void)<br/>{<br/>  int device_Count = 0;<br/>  cudaGetDeviceCount(&amp;device_Count);<br/>  // This function returns count of number of CUDA enable devices and 0 if there are no CUDA capable devices.<br/>  if (device_Count == 0)<br/>  {<br/>     printf("There are no available device(s) that support CUDA\n");<br/>  }<br/>  else<br/>  {<br/>     printf("Detected %d CUDA Capable device(s)\n", device_Count);<br/>  }<br/>}</pre>
<p class="mce-root">The relevant information about each device can be found by querying the <kbd>cudaDeviceProp</kbd> structure, which returns all the device properties. If you have more than one CUDA-capable device, then you can start a for loop to iterate over all device properties. The following section contains the list of device properties divided into different sets and small code snippets used to access them from CUDA programs. These properties are provided by the <kbd>cudaDeviceProp</kbd> structure in CUDA 9 runtime. </p>
<p>For more details about properties in the different versions of CUDA, you can check the programming guide for a particular version.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">General device properties</h1>
                
            
            
                
<p><kbd>cudaDeviceProp</kbd> provides several properties that can be used to identify the device and the versions being used. It provides the <kbd>name</kbd> property that returns the name of the device as a string. We can also get a version of the driver and the runtime engine the device is using by querying <kbd>cudaDriverGetVersion</kbd> and <kbd>cudaRuntimeGetVersion</kbd> properties. Sometimes, if you have more than one device, you want to use the device that has more multiprocessors. The <kbd>multiProcessorCount</kbd> property returns the count of the number of multiprocessors on the device. The speed of the GPU in terms of clock rate can be fetched by using the <kbd>clockRate</kbd> property. It returns clock rate in Khz. The following code snippet shows how to use these properties from the CUDA program:</p>
<pre>cudaDeviceProp device_Property;<br/>cudaGetDeviceProperties(&amp;device_Property, device);<br/>printf("\nDevice %d: \"%s\"\n", device, device_Property.name);<br/>cudaDriverGetVersion(&amp;driver_Version);<br/>cudaRuntimeGetVersion(&amp;runtime_Version);<br/>printf(" CUDA Driver Version / Runtime Version %d.%d / %d.%d\n", driver_Version / 1000, (driver_Version % 100) / 10, runtime_Version / 1000, (runtime_Version % 100) / 10);<br/>printf( " Total amount of global memory: %.0f MBytes (%llu bytes)\n",<br/> (float)device_Property.totalGlobalMem / 1048576.0f, (unsigned long long) device_Property.totalGlobalMem);<br/> printf(" (%2d) Multiprocessors", device_Property.multiProcessorCount );<br/>printf("  GPU Max Clock rate: %.0f MHz (%0.2f GHz)\n", device_Property.clockRate * 1e-3f, device_Property.clockRate * 1e-6f);</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Memory-related properties</h1>
                
            
            
                
<p>Memory on the GPU has a hierarchical architecture. It can be divided in terms of L1 cache, L2 cache, global memory, texture memory, and shared memory. The <kbd>cudaDeviceProp</kbd> provides many properties that help in identifying memory available with the device. <kbd>memoryClockRate</kbd> and <kbd>memoryBusWidth</kbd> provide clock rate and bus width of the memory respectively. The speed of the memory is very important. It affects the overall speed of your program. <kbd>totalGlobalMem</kbd> returns the size of global memory available with the device. <kbd>totalConstMem</kbd> returns the total constant memory available with the device. <kbd>sharedMemPerBlock</kbd> returns the total shared memory that can be used in tne device. The total number of registers available per block can be identified by using <kbd>regsPerBlock</kbd>. Size of L2 cache can be identified using the <kbd>l2CacheSize</kbd> property. The following code snippet shows how to use memory-related properties from the CUDA program:</p>
<pre>printf( " Total amount of global memory: %.0f MBytes (%llu bytes)\n",<br/>(float)device_Property.totalGlobalMem / 1048576.0f, (unsigned long long) device_Property.totalGlobalMem);<br/>printf(" Memory Clock rate: %.0f Mhz\n", device_Property.memoryClockRate * 1e-3f);<br/>printf(" Memory Bus Width: %d-bit\n", device_Property.memoryBusWidth);<br/>if (device_Property.l2CacheSize)<br/>{<br/>    printf(" L2 Cache Size: %d bytes\n", device_Property.l2CacheSize);<br/>}<br/>printf(" Total amount of constant memory: %lu bytes\n",         device_Property.totalConstMem);<br/>printf(" Total amount of shared memory per block: %lu bytes\n", device_Property.sharedMemPerBlock);<br/>printf(" Total number of registers available per block: %d\n", device_Property.regsPerBlock);</pre>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Thread-related properties</h1>
                
            
            
                
<p>As seen in earlier sections, blocks and threads can be multidimensional. So, it would be nice to know how many threads and blocks can be launched in parallel in each dimension. There is also a limit on the number of threads per multiprocessor and the number of threads per block. This number can be found by using the <kbd>maxThreadsPerMultiProcessor</kbd> and the <kbd>maxThreadsPerBlock</kbd>. It is very important in the configuration of kernel parameters. If you launch more threads per block than the maximum threads possible per block, your program can crash. The maximum threads per block in each dimension can be identified by the <kbd>maxThreadsDim</kbd>. In the same way, the maximum blocks per grid in each dimension can be identified by using the <kbd>maxGridSize</kbd>. Both of them return an array with three values, which shows the maximum value in the <em>x</em>, <em>y,</em> and <em>z</em> dimensions respectively. The following code snippet shows how to use thread-related properties from the CUDA code:</p>
<pre>printf(" Maximum number of threads per multiprocessor: %d\n",              device_Property.maxThreadsPerMultiProcessor);<br/>printf(" Maximum number of threads per block: %d\n",         device_Property.maxThreadsPerBlock);<br/>printf(" Max dimension size of a thread block (x,y,z): (%d, %d, %d)\n",<br/>    device_Property.maxThreadsDim[0],<br/>    device_Property.maxThreadsDim[1],<br/>    device_Property.maxThreadsDim[2]);<br/>printf(" Max dimension size of a grid size (x,y,z): (%d, %d, %d)\n",<br/>    device_Property.maxGridSize[0],<br/>    device_Property.maxGridSize[1],<br/>    device_Property.maxGridSize[2]);</pre>
<p>There are many other properties available in the <kbd> cudaDeviceProp</kbd> structure. You can check the CUDA programming guide for details of other properties. The output from all preceding code sections combined and executed on the NVIDIA Geforce 940MX GPU and CUDA 9.0 is as follows: </p>
<p class="mce-root"/>
<div><img class="alignnone size-full wp-image-161 image-border" src="img/71c144c7-d4f6-46e1-bd68-3558fd3778f1.png" style="" width="823" height="621"/></div>
<p>One question you might ask is why you should be interested in knowing the device properties. The answer is that this will help you in choosing a GPU device with more multiprocessors, if multiple GPU devices are present. If in your application the kernel needs close interaction with the CPU, then you might want your kernel to run on an integrated GPU that shares system memory with the CPU. These properties will also help you in finding the number of blocks and number of threads per block available on your device. This will help you with the configuration of kernel parameters. To show you one use of device properties, suppose you have an application that requires double precision for floating-point operation. Not all GPU devices support this operation. To know whether your device supports double precision floating-point operation and set that device for your application, the following code can be used:</p>
<pre>#include &lt;memory&gt;<br/>#include &lt;iostream&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/>// Main Program<br/>int main(void)<br/>{<br/>int device;<br/>cudaDeviceProp device_property;<br/>cudaGetDevice(&amp;device);<br/>printf("ID of device: %d\n", device);<br/>memset(&amp;device_property, 0, sizeof(cudaDeviceProp));<br/>device_property.major = 1;<br/>device_property.minor = 3;<br/>cudaChooseDevice(&amp;device, &amp;device_property);<br/>printf("ID of device which supports double precision is: %d\n", device);<br/>cudaSetDevice(device);<br/>}</pre>
<p>This code uses two properties available in the <kbd>cudaDeviceprop</kbd> structure that help in identifying whether the device supports double precision operations. These two properties are major and minor. CUDA documentation says us that if major is greater than 1 and minor is greater than 3, then that device will support double precision operations. So, the program's <kbd>device_property</kbd> structure is filled with these two values. CUDA also provides the <kbd>cudaChooseDevice</kbd> API that helps in choosing a device with particular properties. This API is used on the current device to identify whether it contains these two properties. If it contains properties, then that device is selected for your application using the <kbd>cudaSetDevice</kbd> API. If more than one device is present in the system, this code should be written inside a for a loop to iterate over all devices.</p>
<p>Though trivial, this section is very important for you in finding out which applications can be supported by your GPU device and which cannot.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Vector operations in CUDA </h1>
                
            
            
                
<p>Until now, the programs that we have seen were not leveraging any advantages of the parallel-processing capabilities of GPU devices. They were just written to get you familiar with the programming concepts in CUDA. From this section, we will start utilizing the parallel-processing capabilities of the GPU by performing vector or array operations on it. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Two-vector addition program</h1>
                
            
            
                
<p>To understand vector operation on the GPU, we will start by writing a vector addition program on the CPU and then modify it to utilize the parallel structure of GPU. We will take two arrays of some numbers and store the answer of element-wise addition in the third array. The vector addition function on CPU is shown here:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/> //Defining Number of elements in Array<br/>#define N 5<br/> //Defining vector addition function for CPU<br/>void cpuAdd(int *h_a, int *h_b, int *h_c) <br/>{<br/>     int tid = 0;<br/>     while (tid &lt; N)<br/>     {<br/>         h_c[tid] = h_a[tid] + h_b[tid];<br/>         tid += 1;<br/>     }<br/> }</pre>
<p>The  <kbd>cpuAdd</kbd> should be very simple to understand. One thing you might find difficult to understand is the use of <kbd>tid</kbd>. It is included to make the program similar to the GPU program, in which <kbd>tid</kbd> indicated a particular thread ID. Here, also, if you have a multicore CPU, then you can initialize <kbd>tid</kbd> equal to 0 and 1 for each of them and then add 2 to it in the loop so that one CPU will perform a sum on even elements and one CPU will perform addition on odd elements. The main function for the code is shown here:</p>
<pre>int main(void) <br/>{<br/>   int h_a[N], h_b[N], h_c[N];<br/>   //Initializing two arrays for addition<br/>   for (int i = 0; i &lt; N; i++) <br/>   {<br/>     h_a[i] = 2 * i*i;<br/>     h_b[i] = i;<br/>     }<br/>   //Calling CPU function for vector addition<br/>   cpuAdd (h_a, h_b, h_c);<br/>   //Printing Answer<br/>   printf("Vector addition on CPU\n");<br/>   for (int i = 0; i &lt; N; i++) <br/>   {<br/>     printf("The sum of %d element is %d + %d = %d\n", i, h_a[i], h_b[i],             h_c[i]);<br/>   }<br/>   return 0;<br/> }</pre>
<p class="mce-root">There are two functions in the program: <kbd>main</kbd> and <kbd>cpuAdd</kbd>. In the main function, we start by defining two arrays to hold inputs and initialize it to some random numbers. Then, we pass these two arrays as input to the <kbd>cpuAdd</kbd> function. The <kbd>cpuAdd</kbd> function stores the answer in the third array. Then, we print this answer on the console, which is shown here:</p>
<div><img class="alignnone size-full wp-image-191 image-border" src="img/783fe03c-7410-4177-9649-d6aa378983ea.png" style="" width="294" height="148"/></div>
<p>This explanation of using the <kbd>tid in cpuadd</kbd> function may give you an idea of how to write the same function for the GPU execution, which can have many cores in parallel. If we initialize this add function with the ID of that core, then we can do the addition of all the elements in parallel. So, the modified kernel function for addition on the GPU is shown here:</p>
<pre class="mce-root">#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/> //Defining number of elements in Array<br/>#define N 5<br/> //Defining Kernel function for vector addition<br/>__global__ void gpuAdd(int *d_a, int *d_b, int *d_c) <br/>{<br/> //Getting block index of current kernel<br/>     int tid = blockIdx.x; // handle the data at this index<br/>     if (tid &lt; N)<br/>     d_c[tid] = d_a[tid] + d_b[tid];<br/> }</pre>
<p>In the <kbd>gpuAdd</kbd> kernel function, <kbd>tid</kbd> is initialized with the block ID of the current block in which the kernel is executing. All kernels will add an array element indexed by this block ID. If the number of blocks are equal to the number of elements in an array, then all the addition operations will be done in parallel. How this kernel is called from the main function is explained next. The code for the main function is as follows:</p>
<pre class="mce-root">int main(void) <br/>{<br/> //Defining host arrays<br/> int h_a[N], h_b[N], h_c[N];<br/> //Defining device pointers<br/> int *d_a, *d_b, *d_c;<br/> // allocate the memory<br/> cudaMalloc((void**)&amp;d_a, N * sizeof(int));<br/> cudaMalloc((void**)&amp;d_b, N * sizeof(int));<br/> cudaMalloc((void**)&amp;d_c, N * sizeof(int));<br/> //Initializing Arrays<br/> for (int i = 0; i &lt; N; i++) <br/>    {<br/>     h_a[i] = 2*i*i;<br/>     h_b[i] = i ;<br/>     }<br/> <br/>// Copy input arrays from host to device memory<br/> cudaMemcpy(d_a, h_a, N * sizeof(int), cudaMemcpyHostToDevice);<br/> cudaMemcpy(d_b, h_b, N * sizeof(int), cudaMemcpyHostToDevice);<br/><br/>//Calling kernels with N blocks and one thread per block, passing device pointers as parameters<br/>gpuAdd &lt;&lt; &lt;N, 1 &gt;&gt; &gt;(d_a, d_b, d_c);<br/> //Copy result back to host memory from device memory<br/>cudaMemcpy(h_c, d_c, N * sizeof(int), cudaMemcpyDeviceToHost);<br/>printf("Vector addition on GPU \n");<br/> //Printing result on console<br/>for (int i = 0; i &lt; N; i++) <br/>{<br/>     printf("The sum of %d element is %d + %d = %d\n", i, h_a[i], h_b[i],             h_c[i]);<br/>}<br/> //Free up memory<br/> cudaFree(d_a);<br/> cudaFree(d_b);<br/> cudaFree(d_c);<br/> return 0;<br/>}</pre>
<p>The GPU main function has the known structure as explained in the first section of this chapter:</p>
<ul>
<li>It starts with defining arrays and pointers for host and device. The device pointers are allocated memory using the <kbd>cudaMalloc</kbd> function.</li>
<li>The arrays, which are to be passed to the kernel, are copied from the host memory to the device memory by using the <kbd>cudaMemcpy</kbd> function.</li>
<li>The kernel is launched by passing the device pointers as parameters to it. If you see the values inside the kernel launch operator, they are <em>N</em> and <em>1</em>, which indicate we are launching <em>N</em> blocks with one thread per each block.</li>
<li>The answer stored by the kernel on the device memory is copied back to the host memory by again using the <kbd>cudaMemcpy</kbd>, but this time with the direction of data transfer from the device to the host.</li>
<li>And, finally, memory allocated to three device pointers is freed up by using the <kbd>cudaFree</kbd> function. The output of the program is as follows:</li>
</ul>
<div><img class="alignnone size-full wp-image-192 image-border" src="img/2e6cda1b-587e-4e29-b716-4459ad2ddfe2.png" style="" width="300" height="152"/></div>
<p>All CUDA programs follow the same pattern as shown before. We are launching N blocks in parallel. The meaning of this is that we are launching N copies of the same kernel simultaneously. You can understand this by taking a real-life example: Suppose you want to transfer five big boxes from one place to another. In the first method, you can perform this task by hiring one person who takes one block from one place to the other and repeat this five times. This option will take time, and it is similar to how vectors are added to the CPU. Now, suppose you hire five people and each of them carries one box. Each of them also knows the ID of the box they are carrying. This option will be much faster than the previous one. Each one of them just needs to be told that they have to carry one box with a particular ID from one place to the other.</p>
<p>This is exactly how kernels are defined and executed on the device. Each kernel copy knows the ID of it. This can be known by the <kbd>blockIdx.x</kbd> command. Each copy works on an array element indexed by its ID. All copies add all elements in parallel, which significantly reduces the processing time for the entire array. So, in a way, we are improving the throughput by doing operations in parallel over CPU sequential execution. The comparison of throughput between the CPU code and the GPU code is explained in the next section.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Comparing latency between the CPU and the GPU code </h1>
                
            
            
                
<p>The programs for CPU and the GPU addition are written in a modular way so you can play around with the value of N. If N is small, then you will not notice any significant time difference between the CPU and the GPU code. But if you N is sufficiently large, then you will notice the significant difference in the CPU execution time and the GPU execution time for the same-vector addition. The time taken for the execution of a particular block can be measured by adding the following lines to the existing code:</p>
<pre>clock_t start_d = clock();<br/>printf("Doing GPU Vector add\n");<br/>gpuAdd &lt;&lt; &lt;N, 1 &gt;&gt; &gt;(d_a, d_b, d_c);<br/>cudaThreadSynchronize();<br/>clock_t end_d = clock();<br/>double time_d = (double)(end_d - start_d) / CLOCKS_PER_SEC;<br/>printf("No of Elements in Array:%d \n Device time %f seconds \n host time %f Seconds\n", N, time_d, time_h);</pre>
<p>Time is measured by calculating the total number of clock cycles taken to perform a particular operation. This can be done by taking the difference of starting and ending the clock tick count, measured using the clock() function. This is divided by the number of clock cycles per second, to get the execution time. When N is taken as 10,000,000 in the previous vector addition programs of the CPU and the GPU and executed simultaneously, the output is as follows:</p>
<div><img class="alignnone size-full wp-image-164 image-border" src="img/085e74b0-e73e-485d-bf4d-1a3dc7f3ffc9.png" style="" width="274" height="81"/></div>
<p>As can be seen from the output, the execution time or throughput is improved from 25 milliseconds to almost 1 millisecond when the same function is implemented on GPU. This proves what we have seen in theory earlier that executing code in parallel on GPU helps in the improvement of throughput. CUDA provides an efficient and accurate method for measuring the performance of CUDA programs, using CUDA events, which will be explained in the later chapters.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Elementwise squaring of vectors in CUDA</h1>
                
            
            
                
<p>Now, one question you can ask is, now that we are launching N blocks in parallel with one thread in each block, can we work in a reverse way? The answer is <em>yes</em>. We can launch only one block with N threads in parallel. To show that and make you more familiar with working around vectors in CUDA, we take the second example of the element-wise squaring of numbers in an array. We take one array of numbers and return an array that contains the square of these numbers. The kernel function to find the element-wise square is shown here:</p>
<pre>#include "stdio.h"<br/>#include&lt;iostream&gt;<br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/> //Defining number of elements in Array<br/>#define N 5<br/>//Kernel function for squaring number<br/>__global__ void gpuSquare(float *d_in, float *d_out) <br/>{<br/>     //Getting thread index for current kernel<br/>     int tid = threadIdx.x; // handle the data at this index<br/>     float temp = d_in[tid];<br/>     d_out[tid] = temp*temp;<br/> }</pre>
<p>The <kbd>gpuSquare</kbd> kernel function has pointers to two arrays as arguments. The first pointer <kbd>d_in</kbd> points to the memory location where the input array is stored, while the second pointer <kbd>d_out</kbd> points to the memory location where output will be stored. In this program, instead of launching multiple blocks in parallel, we want to launch multiple threads in parallel, so <kbd>tid</kbd> is initialized with a particular thread ID using <kbd>threadIdx.x</kbd>. The main function for this program is as follows:</p>
<pre>int main(void) <br/>{<br/> //Defining Arrays for host<br/>     float h_in[N], h_out[N];<br/>     float *d_in, *d_out;<br/>// allocate the memory on the cpu<br/>     cudaMalloc((void**)&amp;d_in, N * sizeof(float));<br/>     cudaMalloc((void**)&amp;d_out, N * sizeof(float));<br/> //Initializing Array<br/>     for (int i = 0; i &lt; N; i++) <br/>    {<br/>         h_in[i] = i;<br/>     }<br/> //Copy Array from host to device<br/>     cudaMemcpy(d_in, h_in, N * sizeof(float), cudaMemcpyHostToDevice);<br/> //Calling square kernel with one block and N threads per block<br/>     gpuSquare &lt;&lt; &lt;1, N &gt;&gt; &gt;(d_in, d_out);<br/> //Coping result back to host from device memory<br/>     cudaMemcpy(h_out, d_out, N * sizeof(float), cudaMemcpyDeviceToHost);<br/> //Printing result on console<br/>     printf("Square of Number on GPU \n");<br/>     for (int i = 0; i &lt; N; i++) <br/>     {<br/>         printf("The square of %f is %f\n", h_in[i], h_out[i]);<br/>     }<br/> //Free up memory<br/>     cudaFree(d_in);<br/>     cudaFree(d_out);<br/>     return 0;<br/> }</pre>
<p>This main function follows a similar structure to the vector addition program. One difference that you will see here from the vector addition program is that we are launching a single block with N threads in parallel. The output of the program is as follows:</p>
<div><img class="alignnone size-full wp-image-193 image-border" src="img/f31562a9-874e-4188-9432-a6b379875f16.png" style="" width="299" height="152"/></div>
<p>Whenever you are using this way of launching N threads in parallel, you should take care that the maximum threads per block are limited to 512 or 1,024. So, the value of N should be less than this value. If N is 2,000 and the maximum number of threads per block for your device is 512, then you can't write <kbd>&lt;&lt; &lt;1,2000 &gt; &gt;&gt;</kbd>. Instead, you should use something such as <kbd>&lt;&lt; &lt;4,500&gt; &gt;&gt;</kbd>. The choice of a number of blocks and the number of threads per block should be made judiciously.</p>
<p class="mce-root"/>
<p>To summarize, we have learned how to work with vectors and how we can launch multiple blocks and multiple threads in parallel. We have also seen that by doing vector operations on GPU, it improves throughput, compared to the same operation on the CPU. In the last section of this chapter, we discuss the various parallel communication patterns that are followed by threads executing in parallel.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Parallel communication patterns</h1>
                
            
            
                
<p>When several thread is executed in parallel, they follow a certain communication pattern that indicates where it is taking inputs and where it is writing its output in memory. We will discuss each communication pattern one by one. It will help you to identify communication patterns related to your application and how to write code for that.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Map</h1>
                
            
            
                
<p>In this communication pattern, each thread or task takes a single input and produces a single output. Basically, it is a one-to-one operation. The vector addition program and element-wise squaring program, seen in the previous sections, are examples of the map pattern. The code of the map pattern will look as follows:</p>
<pre>d_out[i] = d_in[i] * 2</pre>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Gather</h1>
                
            
            
                
<p>In this pattern, each thread or task has multiple inputs, and it produces a single output to be written at a single location in memory. Suppose you want to write a program that finds a moving average of three numbers; this is an example of a gather operation. It takes three inputs from memory and writes single output to memory. So, there is data reuse on the input side. It is basically a many-to-one operation. The code for gather pattern will look as follows:</p>
<pre>out[i] = (in [i-1] + in[i] + in[i+1])/3</pre>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Scatter</h1>
                
            
            
                
<p>In a scatter pattern, a thread or a task takes a single input and computes where in the memory it should write the output. Array sorting is an example of a scatter operation. It can also be one-to-many operations. The code for scatter pattern will look as follows:</p>
<pre>out[i-1] += 2 * in[i] and out[i+1] += 3*in[i]  </pre>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Stencil</h1>
                
            
            
                
<p>When threads or tasks read input from a fixed set of a neighborhood in an array, then this is called a <strong>stencil</strong> <strong>communication pattern</strong>. It is very useful in image-processing examples where we work on 3x3 or 5x5 neighborhood windows. It is a special form of a gather operation, so code syntax is similar to it. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Transpose </h1>
                
            
            
                
<p>When the input is in the form of a row-major matrix, and we want the output to be in column-major form, we have to use this transpose communication pattern. It is particularly useful if you have a structure of arrays and you want to convert it in the form of an array of structures. It is also a one-to-one operation. The code for the transpose pattern will look as follows:</p>
<pre>out[i+j*128] = in [j +i*128]</pre>
<p>In this section, various communication patterns that CUDA programming follows is discussed. It is useful to find a communication pattern related to your application and use the code syntax of that pattern shown as an example.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>To summarize, in this chapter, you were introduced to programming concepts in CUDA C and how parallel computing can be done using CUDA. It was shown that CUDA programs can run on any NVIDIA GPU hardware efficiently and in parallel. So, CUDA is both efficient and scalable. The CUDA API functions over and above existing ANSI C functions needed for parallel data computations were discussed in detail. How to call device code from the host code via a kernel call, configuring of kernel parameters, and a passing of parameters to the kernel were also discussed by taking a simple two-variable addition example. It was also shown that CUDA does not guarantee the order in which the blocks or thread will run and which block is assigned to which multi-processor in hardware. Moreover, vector operations, which take advantage of parallel-processing capabilities of GPU and CUDA, were discussed. It can be seen that, by performing vector operations on the GPU, it can improve the throughput drastically, compared to the CPU. In the last section, various common communication patterns followed in parallel programming were discussed in detail. Still, we have not discussed memory architecture and how threads can communicate with one another in CUDA. If one thread needs data of the other thread, then what can be done is also not discussed. So, in the next chapter, we will discuss memory architecture and thread synchronization in detail.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Write a CUDA program to subtract two numbers. Pass parameters by value in the kernel function.</li>
<li>Write a CUDA program to multiply two numbers. Pass parameters by reference in the kernel function.</li>
<li>Suppose you want to launch 5,000 threads in parallel. Configure kernel parameters in three different ways to accomplish this. Maximum 512 threads are possible per block.</li>
<li>True or false: The programmer can decide in which order blocks will execute on the device, and blocks will be assigned to which streaming multiprocessor?</li>
<li>Write a CUDA program to find out that your system contains a GPU device that has a major-minor version of 5.0 or greater.</li>
</ol>
<ol start="6">
<li>Write a CUDA program to find a cube of a vector that contains numbers from 0 to 49.</li>
<li>For the following applications, which communication pattern is useful?
<ol>
<li>Image processing</li>
<li>Moving average</li>
<li>Sorting array in ascending order</li>
<li>Finding cube of numbers in array</li>
</ol>
</li>
</ol>


            

            
        
    </div></div></body></html>