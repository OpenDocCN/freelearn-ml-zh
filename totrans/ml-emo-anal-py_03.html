<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer056">
<h1 class="chapter-number" id="_idParaDest-54"><a id="_idTextAnchor077"/>3</h1>
<h1 id="_idParaDest-55"><a id="_idTextAnchor078"/>Labeling Data</h1>
<p><strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) models are only as good as the data they are trained with. Hence good, high-quality<a id="_idIndexMarker316"/> data is <span class="No-Break">vitally important.</span></p>
<p>AI algorithms generally start in a basic, simplified,<a id="_idIndexMarker317"/> form. In supervised learning, accurately labeling (also known as annotating) data is a vitally important step to train an algorithm, improve its predictions, and ensure that what it learns is right. Numerous studies, reports, and surveys show that data scientists spend anywhere between 50-80% of their time doing data preparation and preprocessing (see <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>) – and data labeling is usually a huge part <span class="No-Break">of this.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<img alt="Figure 3.1 – Distribution of time allocated to machine learning tasks" height="360" src="image/B18714_03_01.jpg" width="365"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Distribution of time allocated to machine learning tasks</p>
<p>In this chapter, you will learn why it is important to ensure that data is labeled correctly; how this can be achieved; how to assess whether it has indeed been achieved; and in particular, how to identify annotators who have not carried out the task to the <span class="No-Break">required standard.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>Why labeling must be <span class="No-Break">high quality</span></li>
<li>The <span class="No-Break">labeling process</span></li>
<li><span class="No-Break">Best practices</span></li>
<li>How to <span class="No-Break">label data</span></li>
<li>Determining what the best labeling option is <span class="No-Break">for you</span></li>
<li><span class="No-Break">Gold tweets</span></li>
<li><span class="No-Break">Competency tasks</span></li>
<li><span class="No-Break">Annotation</span></li>
<li>Deciding whether to buy or build an <span class="No-Break">annotation solution</span></li>
<li>Results from the <span class="No-Break">example scenario</span></li>
<li><span class="No-Break">Inter-annotator reliability</span></li>
<li><span class="No-Break">Krippendorff’s alpha</span></li>
<li>Debriefing <span class="No-Break">after annotation</span></li>
</ul>
<p>Before we investigate in depth how to do labeling, let us think about why maximum effort must be put into ensuring that labels are <span class="No-Break">high quality.</span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor079"/>Why labeling must be high quality</h1>
<p>The process of labeling data<a id="_idIndexMarker318"/> is the process of preparing a dataset such that algorithms can learn to recognize patterns that repeat in the data. The idea is that once the algorithm has seen enough data labeled in a certain way, it should be able to identify such patterns in data<a id="_idIndexMarker319"/> that it has not seen previously (known as <span class="No-Break">unlabeled data).</span></p>
<p>Remember, data labeling facilitates the algorithms that power AI to build accurate models. With more and more businesses getting interested in AI to help gain insights and make predictions, it is no surprise that data labeling is a huge market worth billions. It is no exaggeration to say that without data labeling, it is impossible to build <span class="No-Break">AI models.</span></p>
<p>Nowadays, <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) is more important than ever. Stakeholders<a id="_idIndexMarker320"/> want to know why a model came to the decision that it did. In turn, this also helps build confidence in the model. Part of this is the data labeling stage, hence it is vital to ensure that this process is subject to the same rigorous checks and balances as other parts of the process; even minor errors can have significant consequences upstream<a id="_idIndexMarker321"/> that can be problematic for <span class="No-Break">model accuracy.</span></p>
<p>It is also important to understand that data labeling is not a one-off process. Instead, it is a continuous process of building <span class="No-Break">and refinement.</span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor080"/>The labeling process</h1>
<p>The process of data labeling<a id="_idIndexMarker322"/> usually begins by getting humans to use their domain expertise or knowledge, intelligence, sense, and perception to make a decision about data that <span class="No-Break">is unlabeled.</span></p>
<p>Generally, labeled data consists of data that is unlabeled along with a tag, label, or name corresponding to some<a id="_idIndexMarker323"/> feature found in the data. In <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), annotators (also known as labelers) will mentally identify important aspects of the text and use these to create labels. For example, to ascertain the emotion of a text, annotators would perhaps combine an approach that looked for key indicators of emotion (e.g., <em class="italic">happy</em>, <em class="italic">amazed</em>) with an understanding of the context and an exploration of the emotional undertones of the <span class="No-Break">words themselves.</span></p>
<p>Other types of data labeling include images and audio. When labeling a dataset containing images, each image might be labeled as a fruit (e.g., <strong class="source-inline">apple</strong>, <strong class="source-inline">banana</strong>, <strong class="source-inline">pear</strong>), or the labeling process may be as simple as asking the annotator to tag images as <strong class="source-inline">true</strong> if they contain an apple, or <strong class="source-inline">false</strong> otherwise. When annotators are required to consider audio (e.g., speech or other sounds), the annotation process can be quite complex, since the labels may have to be attached to segments of the input or otherwise time-stamped, which may require sophisticated <span class="No-Break">annotation tools.</span></p>
<p>Although data labeling<a id="_idIndexMarker324"/> can be performed by anyone with suitable training, there are specialists who are experts in the art of data labeling, ensuring that labeling is standardized, robust, <span class="No-Break">and accurate.</span></p>
<p>When labeling data, it is a good idea<a id="_idIndexMarker325"/> to consider <span class="No-Break">the following:</span></p>
<ul>
<li>The goal is high-quality data labels. This may entail navigating problems such as poorly trained or poorly equipped personnel, inadequate methods and processes, or unclear instructions <span class="No-Break">being provided.</span></li>
<li>For most AI problems that involve training a model from scratch, non-trivial amounts of training data are required. Furthermore, this may not be a one-time process. Consequently, keeping scaling in mind when putting together processes, training, and other requirements is essential. This then allows the labeling capacity to be increased with minimal effort, in a streamlined way, and with <span class="No-Break">minimum disruption.</span></li>
<li>Cost may also be a factor as labeling can end up being a costly process, and to make things worse, this may not always be apparent. For example, paying highly-qualified staff to preprocess or clean data using laborious, tedious, and repetitive processes is clearly not an efficient use of resources. It might be beneficial to employ data labeling specialists under <span class="No-Break">such circumstances.</span></li>
<li><strong class="bold">Quality assurance</strong> (<strong class="bold">QA</strong>) must also be considered. The quality and accuracy of labeling<a id="_idIndexMarker326"/> are directly proportional to the accuracy and care taken when labeling. There are various algorithms and techniques that can be used to obtain a measure of annotation accuracy. For example, when multiple annotators are used, Fleiss’ kappa is a statistical measure that can be used to determine the level of agreement between <span class="No-Break">the annotators.</span></li>
</ul>
<p>Typically, the data labeling process consists of the <span class="No-Break">following steps:</span></p>
<ol>
<li><strong class="bold">Collection</strong>: Data is collected (e.g., tweets, images, files, etc.), and although there is no perfect<a id="_idIndexMarker327"/> amount needed, typically, this is a large amount as it guarantees to cover the distribution of data better and give better results. Note that for fine-tuning the model, smaller amounts may <span class="No-Break">be sufficient.</span></li>
<li><strong class="bold">Tagging</strong>: This involves humans identifying objects<a id="_idIndexMarker328"/> or aspects of the data, typically using a data <span class="No-Break">labeling tool.</span></li>
<li><strong class="bold">QA</strong>: There is little point in laboriously<a id="_idIndexMarker329"/> labeling data if it is not accurate and does not inform the model as intended. Consequently, QA processes must be in place to assess the accuracy of the labeled data. Without these, it is likely the model will not <span class="No-Break">perform adequately.</span></li>
</ol>
<p>Before we finish this section, a word on what to do if there isn’t enough data. There are a number of methods that can <span class="No-Break">help here:</span></p>
<ul>
<li><strong class="bold">Augmentation</strong>: This involves creating new data based<a id="_idIndexMarker330"/> on the existing data. For example, for images, techniques such as horizontal and vertical shifting/flipping, random rotation, and random zoom can be used to generate <em class="italic">new</em> images with different resolutions that are cropped, rotated, or zoomed. As an added benefit, this also exposes the model to this sort of image and helps it to become <span class="No-Break">more robust.</span></li>
<li><strong class="bold">Synthesis</strong>: This involves creating new instances using techniques such as <strong class="bold">Synthetic Minority Oversampling TEchnique</strong> (<strong class="bold">SMOTE</strong>) or <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>) and is typically used when there are not enough examples of a minority class<a id="_idIndexMarker331"/> for a model<a id="_idIndexMarker332"/> to be able to learn the decision boundary. SMOTE<a id="_idIndexMarker333"/> works by selecting examples that are near to each other in the feature space, drawing a line between these, and then creating new samples at points along the line. GANs are deep learning models that generate data that impersonates the real data by using a method in which two neural networks work against each other: a generator, to generate the synthetic data, and a discriminator, which attempts to distinguish between real and synthetic data. Both these techniques are risky since they rely on assumptions about the data that may not be true, and hence they should only be used when very little data is available. Furthermore, any biases present in GAN datasets<a id="_idIndexMarker334"/> will also be inherited by the model when it <span class="No-Break">generates data.</span></li>
<li><strong class="bold">Regularization</strong>: This involves penalizing data points<a id="_idIndexMarker335"/> that are somehow deemed less important, hence giving more weight to the more important <span class="No-Break">data points.</span></li>
</ul>
<p>It is important to note that even with these methods, it is always preferable to use well-curated data that covers all cases and <span class="No-Break">is well-distributed.</span></p>
<p>Next, let’s look at some best practices that can make the process run more smoothly and with <span class="No-Break">fewer errors.</span></p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor081"/>Best practices</h2>
<p>Data labeling<a id="_idIndexMarker336"/> is a tedious, but necessary, process. The last thing you want to do is have to repeat the process due to some misunderstanding. That is why following good practice can improve the effectiveness and accuracy of the process. The following are some of the things you can do to ensure that your data is labeled efficiently <span class="No-Break">and correctly:</span></p>
<ul>
<li><strong class="bold">Use the right software</strong>: You could just use a spreadsheet, but why not make things easier for yourself? There are lots of commercial products available that will speed up the process. If you have the right programming skills, it’s also worth considering fashioning your own solution. It doesn’t need to be commercial -grade – just good enough to do the job. Some of the things you should consider are as follows (we talk about these in depth later in <span class="No-Break">this chapter):</span><ul><li>Provide logins for annotators, so they can log back in later and carry on where they left off. Labeling is a tedious, mundane task. Logins allow users to log off, take a break, and come back (e.g., the next <span class="No-Break">day) refreshed.</span></li><li>Annotators are typically paid to do the task. It is a good idea to intersperse <strong class="bold">gold standard</strong> data points throughout the task<a id="_idIndexMarker337"/> to check that the annotator isn’t just randomly selecting an option. If an annotator provides labels that fail to match the gold standard points, it is likely that they are not paying proper attention to the task. In that case, it may be worth looking in more detail at a larger sample of their annotations and, if necessary, eliminating <span class="No-Break">their contributions.</span></li><li>If your annotators are volunteers, they may get fed up long before the end. In a dataset of 200 items (for example), what you don’t want is that 5 annotators (for example) all label data items from 1-100, and no one managed to get to 101-200. To mitigate this probability, you should develop your solution so that annotators work on different parts of the dataset. This also helps in the learning process because the models won’t learn anything from seeing the same example <span class="No-Break">numerous times.</span></li></ul></li>
<li><strong class="bold">Measure model performance</strong>: Research has shown that beyond a certain point, throwing more data at a model yields little benefit. Labeling is a time-consuming and costly exercise, hence it makes sense to know when to stop, or at least consider how to extract further improvements from <span class="No-Break">the model.</span></li>
<li><strong class="bold">Organize</strong>: Sometimes, you might even<a id="_idIndexMarker338"/> want multiple annotators working on the same data. It stands to reason that the more annotators you have, the quicker you can get the job done. However, you should organize the task so that different sections of the dataset are labeled but there should also be some overlap so that annotator agreement can <span class="No-Break">be measured.</span></li>
<li><strong class="bold">Provide clear instructions</strong>: Producing a document or a web page or some other written means of getting your instructions across to your annotators in a clear, consistent way will help in improving the accuracy <span class="No-Break">of labeling.</span></li>
<li><strong class="bold">Measure agreement</strong>: Ideally, to ensure high-quality labeling, there should be a sensible level of agreement between annotators. This can be ascertained by making use of one of the inter-annotator <span class="No-Break">reliability metrics.</span></li>
</ul>
<p>Following these practices will ensure a streamlined, error-free process that provides good-quality <span class="No-Break">labeled data.</span></p>
<p>Where multiple annotators consistently disagree on the label, it is very likely that the data point is not suitable for training and should be discarded from the dataset. Furthermore, not all data points gathered need to be labeled if the requirement is high-quality labels. For example, a noisy or blurry image might lead different annotators to see the <span class="No-Break">image differently.</span></p>
<p>Now that we have seen<a id="_idIndexMarker339"/> some best practices, we are ready to think about how to actually do <span class="No-Break">the labeling.</span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor082"/>Labeling the data</h1>
<p>There are many ways to get data labeled, each with its own pros <span class="No-Break">and cons:</span></p>
<ul>
<li><strong class="bold">Internal (in-house) labeling</strong>: This is when experts from within an organization are used<a id="_idIndexMarker340"/> to label data. These are usually<a id="_idIndexMarker341"/> people who are domain experts and hence are very familiar with the process and requirements. Consequently, this leads to better quality control and high-quality labeling. Furthermore, as the data doesn’t need to leave the building, there are fewer associated security risks. However, internal labeling is not always possible (e.g., the company size is small or there is a lot of data to label). Furthermore, domain experts are expensive people so asking them to spend inordinate amounts of time on menial annotation tasks is probably not the best use <span class="No-Break">of resources!</span></li>
<li><strong class="bold">External (outsourced) labeling</strong>: As the name suggests, this is when the job is outsourced<a id="_idIndexMarker342"/> to companies that specialize<a id="_idIndexMarker343"/> in data labeling. These companies are experts at data labeling, and consequently, the process is much smoother, quicker, and often, much cheaper as well. However, the danger here is that although they may be experts at data labeling, they likely will not be experts in the specific domain, and this increases the risk of error. Furthermore, there are also increased security<a id="_idIndexMarker344"/> risks as data will have to be made<a id="_idIndexMarker345"/> available <span class="No-Break">to them.</span></li>
<li><strong class="bold">Crowdsourcing</strong>: This is the process of turning to a group<a id="_idIndexMarker346"/> of people, sourced online, to help with any task. Typically, these<a id="_idIndexMarker347"/> people are freelancers and can be paid or unpaid. Many of the world’s biggest companies (e.g., McDonald's) have used crowdsourcing for tasks such as research, logo design, and more. It is a good option for organizations that are unable to implement internal labeling and cannot find a suitable<a id="_idIndexMarker348"/> external labeling partner. One of the most popular ways to approach crowdsourcing is by using Amazon’s <strong class="bold">Mechanical Turk</strong> (<strong class="bold">MTurk</strong>). <strong class="bold">MTurk</strong> is a crowdsourcing marketplace that makes it easier for individuals and businesses to connect and hence allows businesses to outsource tasks to a workforce that then performs these tasks virtually. This workforce can work in collaboration with an in-house team or even alone. It is an extremely efficient way to get the job done but does suffer from the fact that quality is not guaranteed, there are associated data security risks, and there is a bigger associated <span class="No-Break">management overhead.</span></li>
</ul>
<p>Clearly, internal labeling is the go-to option, if it is viable. The problem here is that some companies just don’t have the resources to do this. Consequently, any of the other options can be considered but each has its pros and cons – for example, crowdsourcing requires much more quality control than the other methods. All things considered, it is probably sensible to either crowdsource (and put up with the extra management that will be required) or employ an external labeling service, which, in any case, is not as expensive as you <span class="No-Break">might think.</span></p>
<p>The next sections consider and describe how, for a previous task, we manually labeled a dataset by sourcing annotators, building a UI, and including checks and balances to ensure that the labeling was fit for purpose and as accurate <span class="No-Break">as possible.</span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor083"/>Gold tweets</h2>
<p>It is a sensible idea to intersperse<a id="_idIndexMarker349"/> a number of special tweets within the dataset<a id="_idIndexMarker350"/> that is to be labeled. These should be a mixture of different types of tweets. These tweets, referred to as <strong class="bold">gold tweets</strong>, were annotated internally beforehand (by the authors) and were interspersed within the dataset. They are important as they can be used as a mechanism to avoid malicious or incompetent annotations. For example, if an annotator’s gold tweet results are below some threshold (e.g., 70%), it can be assumed that they either didn’t understand the task, or were demotivated to utilize the deeper thinking required on such tasks, and hence their annotations should probably be discarded. Essentially, this is a mechanism that avoids malicious annotations. Furthermore, to ensure that the results are valid, a metric such as Fleiss’ kappa or Krippendorff’s alpha (we will explain these later in this chapter) should be used to ensure that there are no problems with <span class="No-Break">rater agreement.</span></p>
<p>Let’s next look at another way we can determine whether an individual is of the required level to undertake our <span class="No-Break">annotation task.</span></p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor084"/>The competency task</h2>
<p>When opting<a id="_idIndexMarker351"/> for internal, external, or crowdsource approaches, it is always<a id="_idIndexMarker352"/> a good idea to implement some initial competency tasks, or tests, to assess the suitability of the annotators. For example, if you are asking an annotator to label Arabic tweets, not only must they understand Arabic but they must also be familiar with the nuances of the language and the intricacies of the language used on X (formerly known <span class="No-Break">as Twitter).</span></p>
<p>Furthermore, the labeling exercise should be manageable for the annotators, and the level of annotator attention kept high, hence the dataset should be constructed with (for example) a mixture of challenging and <span class="No-Break">easy tasks.</span></p>
<p>We will now describe a robust scenario for labeling. In this example, the objective is to label tweets; however, the ideas and methodologies can be adapted for any type of data (e.g., images). Consider the following scenario. The task is to label 1,000 Arabic tweets, hence a request for native Arabic speakers is placed on a crowdsourcing website to examine 1,000 tweets and classify them <span class="No-Break">for emotion.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Although the task itself was to label Arabic tweets, for ease of reading, the screenshots presented in this section have been translated <span class="No-Break">into English.</span></p>
<p>The task itself could be done<a id="_idIndexMarker353"/> using Excel, an off-the-shelf software solution, or even a custom<a id="_idIndexMarker354"/> solution developed in-house. A payment of $100 is offered to annotators, but it is made clear that this payment will only be made after the annotations are completed. Each of the annotators is asked to perform the same task: namely, annotate 1,000 tweets for the same number <span class="No-Break">of emotions.</span></p>
<p class="callout-heading">Multi-class or multi-label?</p>
<p class="callout">The terms <em class="italic">multi-class</em> classification<a id="_idIndexMarker355"/> and <em class="italic">multi-label</em> classification describe<a id="_idIndexMarker356"/> two <span class="No-Break">different problems.</span></p>
<p class="callout">When instances can only be labeled with one from three or more classes, this is known as multi-class classification. For example, when classifying images of fruits into <strong class="source-inline">[Apple, Banana, Orange]</strong>, each image can belong to one, and only <span class="No-Break">one, category.</span></p>
<p class="callout">However, where multiple labels can be predicted for each instance, this is known as multi-label classification. For example, when labeling a text with one of <strong class="source-inline">[Education, Politics, Religion, Sports]</strong>, the text might be about all of these or none <span class="No-Break">of these.</span></p>
<p class="callout">The scenario presented in this chapter addresses the problem of <span class="No-Break">multi-label classification.</span></p>
<p>One tweet at a time was presented to the annotators and two questions were asked. The first was a single-answer multiple- <span class="No-Break">choice question:</span></p>
<p><em class="italic">Q1. Which of the following options best describes the emotional state of </em><span class="No-Break"><em class="italic">the tweeter?</em></span></p>
<ul>
<li>anger (including <span class="No-Break">annoyance, rage)</span></li>
<li>anticipation (including <span class="No-Break">interest, vigilance)</span></li>
<li>disgust (including disinterest, <span class="No-Break">dislike, loathing)</span></li>
<li>fear (including apprehension, <span class="No-Break">anxiety, terror)</span></li>
<li>joy (including <span class="No-Break">serenity, ecstasy)</span></li>
<li>love (<span class="No-Break">including affection)</span></li>
<li>optimism (including <span class="No-Break">hopefulness, confidence)</span></li>
<li>pessimism (including cynicism, <span class="No-Break">no confidence)</span></li>
<li>sadness (including <span class="No-Break">pensiveness, grief)</span></li>
<li>surprise (including <span class="No-Break">distraction, amazement)</span></li>
<li>trust (including acceptance, <span class="No-Break">liking, admiration)</span></li>
<li>neutral or <span class="No-Break">no emotion</span></li>
</ul>
<p>The second question was a checkbox question, where more than one answer could <span class="No-Break">be selected:</span></p>
<p><em class="italic">Q2. In addition to your response to Q1, which of the following options further describe the emotional state of </em><span class="No-Break"><em class="italic">the tweeter?</em></span></p>
<p>This question included the same choices as Q1, but <em class="italic">neutral or no emotion</em> was replaced with <em class="italic">none of </em><span class="No-Break"><em class="italic">the above</em></span><span class="No-Break">.</span></p>
<p>In this scenario, it is not unusual to receive<a id="_idIndexMarker357"/> a huge number<a id="_idIndexMarker358"/> of responses. How are you to decide whom to select for the task? It is a good idea to gauge the competence levels of the applicants who expressed an interest by asking them to complete a short, online, emotion classification exercise consisting of a small number of tweets that are representative of the overall task, and ideally sourced from the same dataset as the task tweets. To perform the test fairly and consistently, applicants should be provided with clear instructions, as seen in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<img alt="Figure 3.2 – Instructions for tweet annotators (source: Classification of tweets using multiple thresholds with self-correction and weighted conditional probabilities, Ahmad and Ramsay, 2020)" height="811" src="image/B18714_03_02.jpg" width="793"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Instructions for tweet annotators (source: Classification of tweets using multiple thresholds with self-correction and weighted conditional probabilities, Ahmad and Ramsay, 2020)</p>
<p>It is also sensible<a id="_idIndexMarker359"/> to provide<a id="_idIndexMarker360"/> the annotators with examples, as seen in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<img alt="Figure 3.3 – Example annotations for annotators (source: Classification of tweets using multiple thresholds with self-correction and weighted conditional probabilities, Ahmad and Ramsay, 2020)" height="967" src="image/B18714_03_03.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Example annotations for annotators (source: Classification of tweets using multiple thresholds with self-correction and weighted conditional probabilities, Ahmad and Ramsay, 2020)</p>
<p>In this scenario, applicants<a id="_idIndexMarker361"/> are asked to examine each tweet and select one primary emotion<a id="_idIndexMarker362"/> and as many secondary emotions as appropriate (we’ll explain why we use this setup shortly). It is important to understand that the annotators do not possess the same domain knowledge as you; consequently, the instructions need to be clear, concise, unambiguous, and easily understood. The instructions should clearly describe what you want the annotators to do and contain ample examples. For example, in the emotion annotation task, it is a good idea to include the different variations of an emotion (e.g., for anger: annoyance and rage). It is also usually a good idea to conduct an iterative review of the instructions to ensure that any issues are identified before publication. Although instructions can be published in any format (Word, PDF, etc.), HTML is a good choice because it allows changes to be made without having the overhead of having to redistribute copies of the instructions. However, it is a good idea to use version numbering and source code management (e.g., Git). Indeed, it may also be sensible to link instruction versions with versions of <span class="No-Break">your build.</span></p>
<p>Instructions are also required because labeling tweets is generally a subjective task, open to the labeler’s understanding, experiences, background, and interpretation of the text with no single point of truth. Indeed, the same annotator may give different answers if asked to reannotate the tweet in the future. The following tweet is an example where there is no, one, <span class="No-Break">definitive answer:</span></p>
<p><em class="italic">This is good airline service but comes with high tariff. They do not provide good service. Best part of this airline is they are </em><span class="No-Break"><em class="italic">always available</em></span><span class="No-Break">.</span></p>
<p>In this situation, the instructions need to provide clear guidance as to how to understand the item, and how to <span class="No-Break">label it.</span></p>
<p>The job of the annotator<a id="_idIndexMarker363"/> is not easy. The annotator<a id="_idIndexMarker364"/> has to reflect, understand, and then select the label, or labels, that will ultimately form the inputs<a id="_idIndexMarker365"/> to the machine learning model. On the face of it, this is not a difficult task, with no specific training or knowledge required, and it can be tackled increasingly as an independent resource working from home. However, having to do the same thing repeatedly, while at the same time maintaining a level of accuracy and consistency, is not straightforward. However, there is no magic wand that can be waved, and the expertise of the annotators is as important as any other part of the solution. Some of the skills that an annotator<a id="_idIndexMarker366"/> is expected to have include <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Concentration</strong>: An annotator should have the ability to concentrate on what’s on the screen for long periods of time, without becoming side-tracked, and without <span class="No-Break">making mistakes</span></li>
<li><strong class="bold">Paying attention to detail</strong>: Getting annotations wrong repeatedly could end up in a biased or an underperforming model, which could have <span class="No-Break">other consequences</span></li>
<li><strong class="bold">Working alone</strong>: Annotating can involve long periods of working alone, which might be feasible for some, but not <span class="No-Break">for others</span></li>
<li><strong class="bold">Understanding the language</strong>: Things such as sarcasm and humor can be hard to detect, hence these require a proper level of understanding of the language and an advanced level of <span class="No-Break">cognitive process</span></li>
</ul>
<p>In summary, the annotator will have to achieve a consistent level of commitment, concentration, and care in order to do a good job. However, it must also be understood that it is impossible to entirely remove human error, no matter how good the procedures and instructions are. It should also be noted that the task is intrinsically subjective, and two annotators can sometimes assign different labels to the same tweet without either of them <span class="No-Break">being wrong.</span></p>
<p class="IMG---Figure"/>
<div>
<div class="IMG---Figure" id="_idContainer046">
<img alt="Figure 3.4 – Tweet annotation page" height="578" src="image/B18714_03_04.jpg" width="1350"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Tweet annotation page</p>
<p>Both the tweets and the emotion<a id="_idIndexMarker367"/> answers were sourced from the complete dataset that we wish to label. Since<a id="_idIndexMarker368"/> these tweets are carefully selected specifically for the purpose of the test, the answers are clear, well-defined, and known. Hence it is relatively straightforward to select whatever number of annotators is required, and who scored <span class="No-Break">the highest.</span></p>
<p>Having selected the annotators to move forward with, it is now time to move on to the task of labeling the proper <span class="No-Break">dataset itself.</span></p>
<h1 id="_idParaDest-62"><a id="_idTextAnchor085"/>The annotation task</h1>
<p>The same annotation<a id="_idIndexMarker369"/> platform as used for the competence task can also be repurposed for the annotation task. However, in the competency task, the tweets were presented in sequence, whereas in the annotation task, some clever logic is used. Without going into too much detail, it is a good idea to use a database such as Access or MySQL. This allows questions to be partitioned, answers to be tracked, and the results to be analyzed. This also enables the ability to determine how much of the dataset has been labeled, which tweets have been labeled, and, importantly, the distribution of those answers (we’ll explain why this is <span class="No-Break">important shortly).</span></p>
<p>A schema describes how data will be organized and connected and includes tables, relationships, and other elements. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> shows a simple sample <span class="No-Break">database schema.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 3.5 – Database schema diagram" height="1036" src="image/B18714_03_05.jpg" width="1364"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Database schema diagram</p>
<p>The <strong class="source-inline">USERS</strong> table contains a record for each person who will be labeling. Note that the <strong class="source-inline">disabled</strong> column can be used to bar access to the system – for example, if a user has failed the competency test. Note also the <strong class="source-inline">LOGINS</strong> table. This simply logs the date and time each time a user<a id="_idIndexMarker370"/> successfully logs in to the system. This can be used to obtain useful information such as the time the user did the labeling, and how many sittings were required to do the labeling. In turn, this could be useful to pinpoint why (for example) they didn’t do a good job. For example, if the <strong class="source-inline">LOGINS</strong> table showed that they were logging in daily late in the evening when people are generally tired, this might be a reason for a poor labeling score (this is entirely speculative, of course, as in today’s modern world, many people prefer working in the afternoons <span class="No-Break">or evenings).</span></p>
<p>The <strong class="source-inline">DATA_SOURCES</strong> table can be used to group tweets by function (e.g., <strong class="source-inline">Competency</strong>, <strong class="source-inline">Live</strong>, and <strong class="source-inline">Test</strong>). The <strong class="source-inline">ITEMS</strong> table then contains items that require labeling with a foreign key back to the data source they belong to. Finally, the <strong class="source-inline">ANSWERS</strong> table is self-explanatory, linking each item to a user and the answers that <span class="No-Break">were supplied.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">For the purposes of labeling 11 emotions, we simply created 11 columns – one for each emotion. However, for a truly generic, customizable solution, the labels should be stored in a separate table and another table used to link them to the items. Furthermore, the number and type of answer expected (single, multiple, etc.) should also <span class="No-Break">be configurable.</span></p>
<p>Although there was payment<a id="_idIndexMarker371"/> involved, respondents were free to annotate as many tweets as they wished. However, the ideal scenario was that even if each of the participants only did some of the task, the overall labeled dataset would be distributed across the dataset. In other words, if each of the 5 annotators only labeled 200 tweets each, the totality should be across<a id="_idIndexMarker372"/> the whole dataset (see <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 3.6 – Distributed tweet labeling" height="182" src="image/B18714_03_06.jpg" width="1371"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Distributed tweet labeling</p>
<p>We can ensure this by making use of some clever programming. Each tweet in the database has a unique ID (<strong class="source-inline">item_id</strong>). This is also stored in the <strong class="source-inline">ANSWERS</strong> table when an answer is submitted, along with the users ID. In this way, we can keep track of which questions have been answered and by whom. Recall that each question has to be answered by every annotator. It is then a simple task to fashion some SQL, as follows, that finds the question that has been answered the least (i.e., has the least number of rows in the <span class="No-Break"><strong class="source-inline">ANSWERS</strong></span><span class="No-Break"> table):</span></p>
<pre class="source-code">
select count(answers.item_id) as 'count', items.item_idfrom items
left outer join
answers
on items.item_id = answers.item_id
group by (items.item_id)
order by count(answers.item_id) asc , items.item_id asc limit 1</pre>
<p>This is done at the beginning<a id="_idIndexMarker373"/> when the web page is loading and the system is deciding which question to display. Although there are situations where this might be useful, this mechanism does not guarantee that each annotator will see every tweet; however, it does ensure that by the time every annotator reaches the end of the dataset, every item in the dataset will have been annotated. Clearly, this is not what is required as it is unhelpful in obtaining measurements on annotator agreement. A more useful modification to this technique would be to look for the next tweet that needs to be annotated by the user. This then allows users to log off and carry on where they left off when they next <span class="No-Break">log in:</span></p>
<pre class="source-code">
select min(items.item_id) as 'item_id'from items
where items.item_id NOT IN
(
  select answers.item_id
  from answers
  where answers.user_id = [USERS_ID]
)</pre>
<p>Validation mechanisms<a id="_idIndexMarker374"/> were added to the web page to prevent <em class="italic">lazy</em> annotation, so that it was not possible to move to the next tweet until an answer had been selected for <em class="italic">Q1</em> and at least one answer was selected for <em class="italic">Q2</em>. In conjunction with the <em class="italic">neutral</em> and <em class="italic">none</em> options, the idea was that, in theory, this would prevent users from simply clicking the <strong class="bold">Submit</strong> button without considering the tweet (i.e., even if the user didn’t think any of the emotions applied, they were still required to register that fact). These options forced the user to proactively indicate that there were no emotions in the tweet, the idea being that if they were forced to select an option to proceed to the next tweet, then they may as well select something relevant as opposed to choosing at random. Users were also given four weeks to complete the task, hence there was no requirement to rush to complete the task. To facilitate this, the system allowed them to stop and come back later to resume from where they <span class="No-Break">left off.</span></p>
<p>The same interface as the one used for the competency task was used to annotate 1,000 tweets. One tweet was displayed at a time, as can be seen in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em>. Following the guidelines from Mohammad and Kiritchenko, the annotator was asked to answer two questions. The first was a single-answer multiple- choice question that asked the user to select one option that best describes the emotional state of the tweeter from the emotions. The second was a checkbox question, where multiple answers could <span class="No-Break">be selected.</span></p>
<p>Earlier, we described <a id="_idIndexMarker375"/>how the applicants were asked to examine each tweet and select one primary emotion and as many secondary emotions as appropriate. This is the technique described by Mohammad and Kiritchenko, reasoning that they wanted to include all emotions, however subtle, that applied in the answer, not just the primary emotion. They argue that one of the criticisms of natural language annotation is that only the instances with high agreement are kept, and low agreement instances are discarded. The high agreement instances typically tend to be simple examples of the emotion class, and hence are easier to model. However, in the real world, there are hugely more complex and complicated examples and usages of language to elicit emotion. If the model has been trained largely on high agreement instances, it then performs sub-optimally when it has to process instances that it has not seen <span class="No-Break">during training.</span></p>
<p>Given the amalgamated results from <em class="italic">Q1</em> and <em class="italic">Q2</em>, a primary emotion was established by a majority vote. Where there was a tie, all tied emotions were deemed as primary emotions. The aggregated responses from <em class="italic">Q1</em> and <em class="italic">Q2</em> were used to obtain a full set of labels for <span class="No-Break">a tweet.</span></p>
<p>Two criteria were applied to these <span class="No-Break">aggregated responses:</span></p>
<ul>
<li>If at least half of the annotators indicated that a certain emotion applied, then that label <span class="No-Break">was chosen</span></li>
<li>If no emotion was indicated by at least half of the annotators and more than half of the responses indicated that the tweet was neutral, then the tweet was marked <span class="No-Break">as neutral</span></li>
</ul>
<p>A typical trend<a id="_idIndexMarker376"/> is that the highest-scoring secondary emotions are primary emotions such as anger and fear. These usually correspond well with the highest-scoring primary emotions. In general, the highest-scoring primary emotions are also the highest-scoring secondary emotions, and the lowest-scoring primary emotions are the lowest-scoring secondary emotions. However, there may be anomalies in emotions such as trust, where it may be a high-ranking primary emotion but a low-ranking secondary emotion, indicating that in the presence of other emotions, the usefulness of such emotions <span class="No-Break">is limited.</span></p>
<p>Clearly, there are some obvious limitations<a id="_idIndexMarker377"/> to this method. The participants were self-selected, and no information was available regarding the conditions (e.g., whether the participant completed the survey under strict conditions conducive to clear thinking) under which the exercise was completed. It is also possible that some annotators perhaps did not fully understand the instructions, but declined to say so. Hence, even after the competency test, it is likely that there may still be invalid annotations. Annotating tweets in this manner is clearly a monotonous, time-consuming activity. However, the annotators were paid, hence they should be highly motivated to annotate to a high standard. Furthermore, a key advantage<a id="_idIndexMarker378"/> of datasets constructed as this one was, is that the tweets were not gathered by making use of emotion-bearing keywords; hence the dataset should be more representative of the types of tweets seen on X (formerly known <span class="No-Break">as Twitter).</span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor086"/>Buy or build?</h2>
<p>For this section, you will need<a id="_idIndexMarker379"/> the <span class="No-Break">following prerequisites:</span></p>
<ul>
<li>Software development tools such as <span class="No-Break">VS Code</span></li>
<li>Database tools such as MySQL Workbench or <span class="No-Break">SQL Server</span></li>
<li>Access to a <span class="No-Break">web browser</span></li>
<li>A version control system such as Git <span class="No-Break">or GitHub</span></li>
</ul>
<p>There is certainly a debate around whether you should develop your own interface, or buy<a id="_idIndexMarker380"/> a ready-made one off the shelf. Naturally, being programmers at heart, our initial inclination is always to go for the former! However, this may not always be possible with company size, work priorities, and cost among the factors having to be considered. Clearly, for certain situations, it is preferable to use one of the ready-made tools. Here are <span class="No-Break">some examples:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Tool</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Data Type</strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Amazon SageMaker <span class="No-Break">Ground Truth</span></p>
</td>
<td class="No-Table-Style">
<p>Text, image, <span class="No-Break">and video</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Label Studio</span></p>
</td>
<td class="No-Table-Style">
<p>Text, image, video, audio, <span class="No-Break">and more</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Sloth</span></p>
</td>
<td class="No-Table-Style">
<p>Image <span class="No-Break">and video</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Dataturk</span></p>
</td>
<td class="No-Table-Style">
<p>Text, image, <span class="No-Break">and video</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Superannotate</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Image</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">Audio-annotator</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Audio</span></p>
</td>
</tr>
</tbody>
</table>
<p>Building<a id="_idIndexMarker381"/> an annotation interface is not a straightforward task; it requires a number of different features, but with a little planning, it can be a smooth process. The following are some of the features<a id="_idIndexMarker382"/> that should <span class="No-Break">be considered:</span></p>
<ul>
<li>The option to view any associated metadata of the item (e.g., for images, the dimensions or the date and time it was taken), as well as the <span class="No-Break">item itself</span></li>
<li>A mobile-friendly interface because not everyone will access it from a desktop or <span class="No-Break">a laptop</span></li>
<li>Administrator login to access stats and view <span class="No-Break">current progress</span></li>
<li>Standard reports, and the ability to create <span class="No-Break">custom reports</span></li>
<li>The ability to easily add new sets of items <span class="No-Break">for labeling</span></li>
<li>The ability to finalize item sets so that no more annotations <span class="No-Break">are allowed</span></li>
<li>The ability to add <span class="No-Break">gold tweets</span></li>
<li>The ability to import items <span class="No-Break">for labeling</span></li>
<li>The ability<a id="_idIndexMarker383"/> to export results so they can <span class="No-Break">be shared</span></li>
</ul>
<p>Typically, the data that requires labeling<a id="_idIndexMarker384"/> will be collected on a different platform and also processed via machine learning algorithms on a different platform. Consequently, the data will need to be imported and exported from the labeling platform in an appropriate format. Ideally, these processes should be part of an automated pipeline, so that manual bottlenecks are not created. It is important to note that these non-labeling processes will remain the same, regardless of whether an in-house or an external solution is used. There is also a possibility, if an external solution is used, that carefully-crafted interfaces that work well in a pipeline may suddenly stop working and require altering if the external supplier changes the way their solution interfaces with the outside world. Furthermore, although many external solutions are feature-rich, there may be something niche that isn’t available, and although it can be requested, there is no guarantee when it will be implemented, if at all. As a final benefit to convince you of the merits of developing an in-house solution, if the necessary engineers and time are available, bugs can also be fixed, tested, and <span class="No-Break">deployed quicker.</span></p>
<p>It may still be unclear whether to choose from the abundance of annotation tools available or to embark on building a solution from scratch. We'll, therefore, finalize this discussion with a summary table<a id="_idIndexMarker385"/> to help you decide which route <span class="No-Break">to take:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002">
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Situation</strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Decision</strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If this is a one-off task, and very unlikely to be <span class="No-Break">repeated again</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Buy</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If you need a <span class="No-Break">quick turnaround</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Buy</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If cost is not <span class="No-Break">a concern</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Buy</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If you have tight security requirements that dictate, for example, that the data should not leave <span class="No-Break">the premises</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Build/Buy</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If you know that this is a process that will be repeated often and especially if costs adding up are <span class="No-Break">a concern</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Build/Buy</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If you have a niche requirement that may be useful to others, and hence your platform could be sold as a SaaS model, <span class="No-Break">for example</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Build</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>If your requirements are niche or bespoke and unlikely to be catered for by one of the <span class="No-Break">commercial solutions</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Build</span></p>
</td>
</tr>
</tbody>
</table>
<h2 id="_idParaDest-64"><a id="_idTextAnchor087"/>Results</h2>
<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em> shows the overall results <a id="_idIndexMarker386"/>screen. This can be used to get an update on the current status of the labeling and how many each annotator has done, and can also be used to access the detailed analysis screen. These details were enough for our work, but this screen could be further enhanced with useful information such as <span class="No-Break">the following:</span></p>
<ul>
<li>How many times the annotator <span class="No-Break">logged in</span></li>
<li>Average <span class="No-Break">response time</span></li>
<li>When the annotator last <span class="No-Break">logged in</span></li>
</ul>
<p>Clearly, analyzing such information can lead to further insights that may ultimately lead to better annotation and, hence, a <span class="No-Break">better-performing model.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<img alt="Figure 3.7 – Labeling status screen" height="962" src="image/B18714_03_07.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Labeling status screen</p>
<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em> shows the results summary <a id="_idIndexMarker387"/>screen for an <span class="No-Break">individual annotator.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="Figure 3.8 – Labeling results" height="915" src="image/B18714_03_08.jpg" width="1620"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Labeling results</p>
<p>Each tweet is listed along with the selections made for the tweet and how many the annotator got correct. This allows administrators to examine, in detail, each tweet and the answers that the annotator supplied. When this screen is used to display the competency results (where the actual answers are known), the <strong class="bold">Actual answersUser answers</strong> column shows the actual answers and the responses, so that quick comparisons can be made, and the final <strong class="bold">%Correct</strong> column shows how many the annotator <span class="No-Break">got correct.</span></p>
<p>Having collected the labeling<a id="_idIndexMarker388"/> results, it is a good idea to understand whether the answers are reliable. We look at this in the <span class="No-Break">next section.</span></p>
<h1 id="_idParaDest-65"><a id="_idTextAnchor088"/>Inter-annotator reliability</h1>
<p>Inter-annotator reliability<a id="_idIndexMarker389"/> is the widely used term to describe “<em class="italic">the extent to which independent coders evaluate the characteristic of a message, or artefact, and reach the same conclusion</em>” (Tinsley and D. J. Weiss). It is an important metric because it determines whether the data can be considered valid and is an indication of the trustworthiness of the data. Without this reliability, any content analysis is useless. From a practical point of view, establishing a high level of reliability also has the benefit of allowing the work to be divided among multiple annotators. Measuring reliability<a id="_idIndexMarker390"/> is actually measuring reproducibility, that is, <em class="italic">"</em>the<em class="italic"> likelihood that different coders who receive the same training and textual guidance will assign the same value to the same piece of </em><span class="No-Break"><em class="italic">content</em></span><span class="No-Break">” (Joyce).</span></p>
<p>There are many ways to measure reliability – two of the most common are Fleiss’ kappa and <span class="No-Break">Krippendorff’s alpha:</span></p>
<ul>
<li><strong class="bold">Fleiss’ kappa:</strong> This is a statistical measure for assessing<a id="_idIndexMarker391"/> the reliability of agreement between a fixed number of raters<a id="_idIndexMarker392"/> when assigning categorical ratings to a number of items or classifying items (i.e., it is a measurement of agreement). It can be interpreted as expressing the extent to which the observed amount of agreement among raters exceeds what would be expected if all raters made their ratings completely randomly. Many kappas only work when assessing the agreement between two raters. However, Fleiss’ kappa does not suffer from this limitation as it measures the overall agreement between all the raters. The measure calculates the degree of agreement in classification over that which would be expected by chance. Perfect agreement would equate to a kappa of <strong class="source-inline">1</strong> and chance agreement would equate to <strong class="source-inline">0</strong>. There is no generally agreed-upon measure of significance, although guidelines have been given; the commonly cited scale as described can be seen in <em class="italic">Table 3.1</em>. Fleiss’ kappa can only be used with binary (yes/no) or nominal-scale (gender, nationality, <span class="No-Break">etc.) ratings.</span></li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
</colgroup>
<thead>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">Kappa</strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Strength </strong><span class="No-Break"><strong class="bold">of agreement</strong></span></p>
</td>
</tr>
</thead>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>&lt;0</p>
</td>
<td class="No-Table-Style">
<p>Less than <span class="No-Break">chance agreement</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">0.01–0.20</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Slight agreement</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">0.21– 0.40</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Fair agreement</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">0.41–0.60</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Moderate agreement</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">0.61–0.80</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">Substantial agreement</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break">0.81–0.99</span></p>
</td>
<td class="No-Table-Style">
<p>Almost <span class="No-Break">perfect agreement</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1 – Interpretation of Fleiss’  kappa</p>
<p>The formula to calculate Fleiss’ kappa is <span class="No-Break">as follows:</span></p>
<p><img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/math&gt;" height="83" src="image/9.png" style="vertical-align:-0.738em;height:2.003em;width:2.311em" width="96"/></p>
<p>Here, P<span class="subscript">0</span> is the relative<a id="_idIndexMarker393"/> observed agreement among raters and P<span class="subscript">e</span> is the hypothetical probability<a id="_idIndexMarker394"/> of <span class="No-Break">chance agreement.</span></p>
<ul>
<li><strong class="bold">Krippendorff’s alpha:</strong> The problem with Fleiss’ kappa<a id="_idIndexMarker395"/> is that it requires the total number<a id="_idIndexMarker396"/> of answers for each item (e.g., a tweet) to be equal. This is clearly not possible when annotating tweets with multiple emotions because the number of emotions chosen for each tweet can vary. In these situations, Krippendorff’s alpha is used because it can cope with various sample sizes, categories, and numbers of annotators. It is also able to handle cases where data is missing – for example, when an annotator has left an answer blank. Krippendorff’s alpha is also more reliable than other measures because, in contrast to other measures that are based on agreement, Krippendorff’s alpha is a ratio that is based on the observed and the expected <em class="italic">disagreement</em>. This is one reason why it is believed to be more reliable. It has been reported that, in practice, the results from both Krippendorff’s alpha and Fleiss’ kappa are similar (Gwet). This is unsurprising since in cases where exactly one label is assigned, they compute the same thing, but only Krippendorff’s alpha works when a data point can have zero, one, or <span class="No-Break">more labels.</span></li>
</ul>
<p>The interpretation of Krippendorff’s alpha is also more straightforward than that of Fleiss’ kappa. The alpha value is a number ranging from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, where <strong class="source-inline">0</strong> is perfect disagreement and <strong class="source-inline">1</strong> is perfect agreement. Krippendorff suggests that, ideally, the value should be greater than or equal to <strong class="source-inline">0.8</strong>, but tentative conclusions are still acceptable when the value is greater than or equal to <strong class="source-inline">0.667</strong> (Klaus). However, this would be the lowest acceptable value. It should be noted that while Krippendorff’s alpha can measure the overall reliability of the annotation process, it does not inform on which annotators or which instances are problematic if the alpha is low. For that, further analysis may be conducted – for example, comparing the annotations of each pair of annotators, or looking at the instances where <span class="No-Break">disagreement occurred.</span></p>
<p>The formula to calculate Krippendorff’s alpha is <span class="No-Break">as follows:</span></p>
<p><img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="83" src="image/10.png" style="vertical-align:-0.738em;height:2.003em;width:2.459em" width="102"/></p>
<p>Here, D<span class="subscript">0</span> is the disagreement<a id="_idIndexMarker397"/> observed and D<span class="subscript">e</span> is the disagreement expected<a id="_idIndexMarker398"/> <span class="No-Break">by chance.</span></p>
<p>That’s enough theory. Let’s work through an example using <span class="No-Break">Krippendorff’s alpha.</span></p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor089"/>Calculating Krippendorff’s alpha</h2>
<p>Recall that Fleiss’ kappa<a id="_idIndexMarker399"/> does not support multi-label input, hence Krippendorff’s alpha is computed for multi-label classification. In this section, a fictitious example, heavily based on Krippendorff’s paper, is set up to work through a simple Krippendorf’s <span class="No-Break">alpha calculation.</span></p>
<p>Consider the scenario<a id="_idIndexMarker400"/> where two annotators, A1 and A2, are asked to provide 10 (N) binary (yes/no) labels to the same set of two data points. The <em class="italic">reliability data matrix</em> then shows the combined responses from each annotator across both data points, where the rows are the annotators, and the columns are <span class="No-Break">their labels:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table004">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>3</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
<td class="No-Table-Style">
<p>7</p>
</td>
<td class="No-Table-Style">
<p>8</p>
</td>
<td class="No-Table-Style">
<p>9</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">A1</strong></span></p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold">A2</strong></span></p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p>Using this matrix, a <em class="italic">coincidence matrix</em> is created that accounts for all values in the reliability <span class="No-Break">data matrix.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">These matrices are slightly different from the typical coincidence matrices found in AI and ML in that they are used to record and analyze relationships between two or more categorical variables and show items that appear together. The typical coincidence matrix used in ML presents a table showing the different outcomes of the prediction and results of a <span class="No-Break">classification problem.</span></p>
<p>The reliability data matrix is hence used to create a 2X2 coincidence matrix using the <span class="No-Break">following template:</span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table005">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0</span><span class="No-Break"><span class="subscript">00</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0</span><span class="No-Break"><span class="subscript">01</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">n</span><span class="No-Break"><span class="subscript">0</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0</span><span class="No-Break"><span class="subscript">10</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">0</span><span class="No-Break"><span class="subscript">11</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">n</span><span class="No-Break"><span class="subscript">1</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">n</span><span class="No-Break"><span class="subscript">0</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">n</span><span class="No-Break"><span class="subscript">1</span></span></p>
</td>
<td class="No-Table-Style">
<p>2 <span class="No-Break">x N</span></p>
</td>
</tr>
</tbody>
</table>
<p>In the coincidence matrix, the values<a id="_idIndexMarker401"/> are entered twice – for example, once as (<strong class="source-inline">0,1</strong>) and once as (<strong class="source-inline">1,0</strong>). Hence, in the example, <strong class="source-inline">1</strong> is entered as a <strong class="source-inline">0-1</strong> pair of values and also as a <strong class="source-inline">1-0</strong> pair <span class="No-Break">of values:</span></p>
<table class="No-Table-Style" id="table006">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style"/>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>0</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">10</span></p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">14</span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>1</p>
</td>
<td class="No-Table-Style">
<p>4</p>
</td>
<td class="No-Table-Style">
<p>2</p>
</td>
<td class="No-Table-Style">
<p>6</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break">14</span></p>
</td>
<td class="No-Table-Style">
<p>5</p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break">20</span></p>
</td>
</tr>
</tbody>
</table>
<p>These values are derived <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">0</strong><span class="subscript">00</span><strong class="bold"> (10)</strong>: The reliability data matrix shows that there are five <strong class="source-inline">0-0</strong> pairs. However, each pair is represented twice, <span class="No-Break">hence </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">.</span></li>
<li><strong class="bold">0</strong><span class="subscript">01</span><strong class="bold"> (4)</strong>: There are four <span class="No-Break"><strong class="source-inline">0-1</strong></span><span class="No-Break"> pairs.</span></li>
<li><strong class="bold">0</strong><span class="subscript">10</span><strong class="bold"> (4)</strong>: The four <strong class="source-inline">1-0</strong> pairs are the same as the four <span class="No-Break"><strong class="source-inline">0-1</strong></span><span class="No-Break"> pairs.</span></li>
<li><strong class="bold">0</strong><span class="subscript">11</span><strong class="bold"> (2)</strong>: There is one <strong class="source-inline">1-1</strong> pair, <span class="No-Break">hence </span><span class="No-Break"><strong class="source-inline">2</strong></span><span class="No-Break">.</span></li>
</ul>
<p>Now, it is a simple matter of using the formula and substituting the values to calculate <span class="No-Break">the alpha:</span></p>
<p><img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/msub&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="83" src="image/11.png" style="vertical-align:-0.738em;height:2.003em;width:4.534em" width="189"/></p>
<p><img alt="&lt;math  display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mfrac&gt;&lt;msub&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mn&gt;01&lt;/mn&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" height="77" src="image/12.png" style="vertical-align:-0.598em;height:1.851em;width:9.571em" width="399"/></p>
<p><img alt="&lt;mml:math   display=&quot;block&quot;&gt;&lt;mml:mi&gt;α&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;20&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;4&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;14&lt;/mml:mn&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mn&gt;6&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.095&lt;/mml:mn&gt;&lt;/mml:math&gt;" height="59" src="image/13.png" style="vertical-align:-0.452em;height:1.414em;width:12.891em" width="537"/></p>
<p>Luckily, we do not need to code<a id="_idIndexMarker402"/> this ourselves since the NLTK metrics package has functions to calculate inter-annotator agreement values. The <strong class="source-inline">nltk.metrics.agreement</strong> module requires its input to be in the form of a list of triples, where each triple contains a label to identify the annotator (e.g., <strong class="source-inline">A1</strong> and <strong class="source-inline">A2</strong>), an item to indicate the label (e.g., <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">3</strong>, etc.) and the annotator label (e.g., <span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">, </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">).</span></p>
<p>The corresponding Python code for the preceding simple example is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
import nltkdata = [
    ['A1', 1, 0],
    ['A1', 2, 1],
    ['A1', 3, 0],
    ['A1', 4, 0],
    ['A1', 5, 0],
    ['A1', 6, 0],
    ['A1', 7, 0],
    ['A1', 8, 0],
    ['A1', 9, 1],
    ['A1', 10, 0],
    ['A2', 1, 1],
    ['A2', 2, 1],
    ['A2', 3, 1],
    ['A2', 4, 0],
    ['A2', 5, 0],
    ['A2', 6, 1],
    ['A2', 7, 0],
    ['A2', 8, 0],
    ['A2', 9, 0],
    ['A2', 10, 0]
    ]
task = nltk.metrics.agreement.AnnotationTask(data=data)
print (task.alpha())</pre>
<p>This generates the same result<a id="_idIndexMarker403"/> as in the <span class="No-Break">worked example:</span></p>
<pre class="source-code">
0.0952380952381</pre> <h1 id="_idParaDest-67"><a id="_idTextAnchor090"/>Debrief</h1>
<p>It is a good idea<a id="_idIndexMarker404"/> to follow up with your annotators after the task to ascertain their views about the overall task and whether they experienced any particular difficulties with any emotions. For example, tweets may contain several different emotions and hence it may be difficult to pinpoint the prevalent ones. There may also have been occasions where emotive words were seen but the annotator did not feel that this led them to strongly lean toward any particular emotion. It is sensible to obtain an understanding of these types of situations as early as possible as changes to the procedures for collecting data are likely to change the nature of what is collected. Typically, annotators<a id="_idIndexMarker405"/> refer to tweets being short and informal as a primary reason for being unable to determine a definitive emotion for a tweet. It is, therefore, ironic that when tweets are overly lengthy, annotators mention that it is hard to restrict themselves to selecting a sensible set of emotions, as in <span class="No-Break">this example:</span></p>
<p><em class="italic">Who is responsible for this tampering? !!! At the University of Nora !! Meeting with His Excellency </em><span class="No-Break"><em class="italic">the #MinisterofEducation</em></span></p>
<h1 id="_idParaDest-68"><a id="_idTextAnchor091"/>Summary</h1>
<p>There is no doubt that data annotation is a challenge, but with the right tools and techniques, these problems can be minimized and the process streamlined, resulting in a well-labeled dataset that is fit <span class="No-Break">for purpose.</span></p>
<p>In this chapter, we started by understanding why labeling must be high quality, and what the consequences are of even minor errors. The data labeling process usually begins by getting humans to use their domain expertise, intelligence, sense, and perception to make a decision about data that is unlabeled. We explored the process and key considerations and discussed the options when there is not enough data available. Data labeling is a tedious but necessary process and is prone to errors by the annotators. It is thus important to improve its effectiveness and accuracy by identifying and then following good practices. We then discussed the various ways to label data and their pros and cons. A common technique is to crowdsource, hence we introduced techniques such as gold tweets and competency tasks to improve outcomes when following this technique. We also presented a scenario for labeling, discussed the labels that might be used and the skills an annotator needs, and presented a simple architecture and UI for the data labeling task. Finally, we presented the criteria for building or buying a labeling tool, worked through the theory behind inter-annotator reliability, and presented the corresponding <span class="No-Break">Python code.</span></p>
<p>In the next chapter, we will look at competition-based research and evaluation strategies, discover how it is not always necessary to create your own dataset, and explore how existing datasets can be transformed in ways to make them useful for the emotion <span class="No-Break">analysis task.</span></p>
<h1 id="_idParaDest-69"><a id="_idTextAnchor092"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
<ul>
<li>T.N. Ahmad and A. Ramsay. <em class="italic">Classification of Tweets using Multiple Thresholds with Self-Correction and Weighted Conditional </em><span class="No-Break"><em class="italic">Probabilities</em></span><span class="No-Break">. 2020.</span></li>
<li>S. M. Mohammad and S. Kiritchenko. <em class="italic">Understanding Emotions: A Dataset of Tweets to Study Interactions between Affect Categories</em>. In Proceedings of the 11th Edition of the Language Resources and Evaluation Conference, Miyazaki, <span class="No-Break">Japan, 2018.</span></li>
<li>H. E. Tinsley and D. J. Weiss. <em class="italic">Interrater reliability and agreement. In Handbook of applied multivariate statistics and mathematical modeling</em>, pages 95–124. <span class="No-Break">Elsevier, 2000.</span></li>
<li>M. Joyce. <em class="italic">Picking the best intercoder reliability statistic for your digital activism content analysis</em>. In <em class="italic">Digital Activism Research Project: Investigating the Global Impact of Comment Forum Speech as a Mirror of Mainstream Discourse</em>, volume <span class="No-Break">243, 2013.</span></li>
<li>Gwet, Kilem. (2015). <em class="italic">On Krippendorff’s </em><span class="No-Break"><em class="italic">Alpha Coefficient</em></span><span class="No-Break">.</span></li>
<li>K. Klaus. <em class="italic">Content analysis: An introduction to its </em><span class="No-Break"><em class="italic">methodology</em></span><span class="No-Break">, 1980.</span></li>
<li>K. Krippendorff. <em class="italic">Computing Krippendorff’s </em><span class="No-Break"><em class="italic">alpha-reliability</em></span><span class="No-Break">. 2011.</span></li>
</ul>
</div>
</div></body></html>