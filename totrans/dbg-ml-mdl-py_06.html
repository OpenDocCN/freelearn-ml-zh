<html><head></head><body>
<div id="_idContainer077">
<h1 class="chapter-number" id="_idParaDest-121"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-122"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.2.1">Interpretability and Explainability in Machine Learning Modeling</span></h1>
<p><span class="koboSpan" id="kobo.3.1">The majority of the machine learning models we use or develop are complex and require the use of explainability techniques to identify opportunities for improving their performance, reducing their bias, and increasing </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">their reliability.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">We will look at the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">Interpretable versus black-box </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">machine learning</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">Explainability methods in </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">machine learning</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Practicing machine learning explainability </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">in Python</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Reviewing why having explainability is </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">not enough</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.15.1">By the end of this chapter, you will have learned about the importance of explainability in machine learning modeling and practiced using some of the explainability techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">in Python.</span></span></p>
<h1 id="_idParaDest-123"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">The following requirements should be considered for this chapter as they help you better understand the mentioned concepts, use them in your projects, and practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Python </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">library requirements:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">sklearn</span></strong><span class="koboSpan" id="kobo.23.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.25.1">numpy</span></strong><span class="koboSpan" id="kobo.26.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">1.22.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">matplotlib</span></strong><span class="koboSpan" id="kobo.29.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">3.7.1</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.31.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06"><span class="No-Break"><span class="koboSpan" id="kobo.33.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.34.1">.</span></span></p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.35.1">Interpretable versus black-box machine learning</span></h1>
<p><span class="koboSpan" id="kobo.36.1">Interpretable </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.37.1">and simple models such as linear regression make it easy to assess the possibility of improving </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.38.1">them, finding issues with them such as biases that need to be detected and removed, and building trust in using such models. </span><span class="koboSpan" id="kobo.38.2">However, to achieve higher performance, we usually don’t stop with these simple models and rely on complex or so-called black-box models. </span><span class="koboSpan" id="kobo.38.3">In this section, we will review some of the interpretable models and then introduce techniques you can use to explain your </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">black-box models.</span></span></p>
<h2 id="_idParaDest-125"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.40.1">Interpretable machine learning models</span></h2>
<p><span class="koboSpan" id="kobo.41.1">Linear models </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.42.1">such as linear and logistic regression, shallow decision trees, and Naive Bayes classifiers are examples of simple and interpretable methods (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.43.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.44.1">.1</span></em><span class="koboSpan" id="kobo.45.1">). </span><span class="koboSpan" id="kobo.45.2">We can easily extract the contribution of features in predictions of outputs for these models and identify opportunities for improving their performance, such as by adding or removing features or changing feature normalization. </span><span class="koboSpan" id="kobo.45.3">We can also easily identify if there are biases in our models – for example, for a specific race or gender group. </span><span class="koboSpan" id="kobo.45.4">However, these models are very simple, and having access to large datasets of thousands or millions of samples allows us to train high-performance but </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">complex models:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.47.1"><img alt="Figure 6.1 – Examples of interpretable classification methods" src="image/B16369_06_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.48.1">Figure 6.1 – Examples of interpretable classification methods</span></p>
<p><span class="koboSpan" id="kobo.49.1">Complex models, such as </span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.50.1">random forest models with many deep decision trees or deep neural networks, help us in achieving higher performance, although they work almost like black-box systems. </span><span class="koboSpan" id="kobo.50.2">To be able to understand these models and explain how they come up with their predictions, and to build trust in their utility, we can use machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">explainability techniques.</span></span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.52.1">Explainability for complex models</span></h2>
<p><span class="koboSpan" id="kobo.53.1">Explainability techniques </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.54.1">work like bridges between complex machine learning models and users. </span><span class="koboSpan" id="kobo.54.2">They are supposed to provide explainabilities that are faithful to how the models work. </span><span class="koboSpan" id="kobo.54.3">And on the other side, they are supposed to provide explanations that are useful and understandable for the users. </span><span class="koboSpan" id="kobo.54.4">These explanations can be used to identify opportunities for improving model performance, reducing the sensitivity of models to small feature value changes, increasing data efficiency in model training, trying to help in proper reasoning in the model and avoid spurious correlations, and helping in achieving fairness (Weber et al., 2022; </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.55.1">Figure 6</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.58.1"><img alt="Figure 6.2 – Effects of using explainability on machine learning models" src="image/B16369_06_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.59.1">Figure 6.2 – Effects of using explainability on machine learning models</span></p>
<p><span class="koboSpan" id="kobo.60.1">Now that you have a better </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.61.1">understanding of the importance of explainability in machine learning modeling, we are ready to get into the details of </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">explainability techniques.</span></span></p>
<h1 id="_idParaDest-127"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.63.1">Explainability methods in machine learning</span></h1>
<p><span class="koboSpan" id="kobo.64.1">We need to keep the following considerations in mind when using or developing explainability techniques for machine learning modeling (Ribeiro et </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">al., 2016):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Interpretability</span></strong><span class="koboSpan" id="kobo.67.1">: The</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.68.1"> explanations need to be understandable to users. </span><span class="koboSpan" id="kobo.68.2">One of the main objectives of machine learning explanation is to make complex models understandable for users and, if possible, provide </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">actionable information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.70.1">Local fidelity (faithfulness)</span></strong><span class="koboSpan" id="kobo.71.1">: Capturing the complexity of models so that they are </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.72.1">completely faithful and meet global faithfulness criteria can’t be achieved by all techniques. </span><span class="koboSpan" id="kobo.72.2">However, an explanation should be at least locally faithful to the model. </span><span class="koboSpan" id="kobo.72.3">In other words, an explanation needs to properly explain how the model behaves in the close neighborhood of the data point </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">under investigation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Being model-agnostic</span></strong><span class="koboSpan" id="kobo.75.1">: Although</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.76.1"> there are techniques that are designed for specific machine learning methods, such as random forest, they are supposed to be agnostic to models that are built with different hyperparameters or for different datasets. </span><span class="koboSpan" id="kobo.76.2">An explainability technique needs to consider the model as a black box and provide explanations for the model either globally </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">or locally.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.78.1">Explainability techniques can be categorized into </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">local explainability</span></strong><span class="koboSpan" id="kobo.80.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.81.1">global explainability</span></strong><span class="koboSpan" id="kobo.82.1"> methods. </span><span class="koboSpan" id="kobo.82.2">Local explainability methods aim to meet the previously listed criteria, while global explainability techniques try to go beyond local explainability and provide global explanations to </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">the models.</span></span></p>
<h2 id="_idParaDest-128"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.84.1">Local explainability techniques</span></h2>
<p><span class="koboSpan" id="kobo.85.1">Local explainability helps</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.86.1"> us </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.87.1">understand the behavior of a model close to a data point in a feature space. </span><span class="koboSpan" id="kobo.87.2">Although these models meet local fidelity criteria, features identified to be locally important might not be globally important, and vice versa (Ribeiro et al., 2016). </span><span class="koboSpan" id="kobo.87.3">This means that we cannot infer local explanations from global explanations, and vice versa, easily. </span><span class="koboSpan" id="kobo.87.4">In this section, we will discuss five local </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">explanation techniques:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.89.1">Feature importance</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.90.1">Counterfactuals</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.91.1">Sample-based explainability</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.92.1">Rule-based explainability</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.93.1">Saliency maps</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.94.1">We will also go through a few global explainability </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">techniques after.</span></span></p>
<h3><span class="koboSpan" id="kobo.96.1">Feature importance</span></h3>
<p><span class="koboSpan" id="kobo.97.1">One of the </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.98.1">primary approaches to local explainability is explaining the contribution of each feature </span><em class="italic"><span class="koboSpan" id="kobo.99.1">locally</span></em><span class="koboSpan" id="kobo.100.1"> in predicting the outcome of the target data points in a neighborhood. </span><span class="koboSpan" id="kobo.100.2">Widely used examples of such methods</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.101.1"> include </span><strong class="bold"><span class="koboSpan" id="kobo.102.1">SHapley Additive exPlanations</span></strong><span class="koboSpan" id="kobo.103.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.104.1">SHAP</span></strong><span class="koboSpan" id="kobo.105.1">) (Lundberg et al., 2017) and </span><strong class="bold"><span class="koboSpan" id="kobo.106.1">Local Interpretable Model-agnostic Explanations</span></strong><span class="koboSpan" id="kobo.107.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.108.1">LIME</span></strong><span class="koboSpan" id="kobo.109.1">) (Ribeiro et al., 2016). </span><span class="koboSpan" id="kobo.109.2">Let’s briefly discuss</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.110.1"> the theory behind these two methods and practice them </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">in Python.</span></span></p>
<h4><span class="koboSpan" id="kobo.112.1">Local explanation using SHAP</span></h4>
<p><span class="koboSpan" id="kobo.113.1">SHAP is a</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.114.1"> Python framework that was introduced by Scott Lundberg and Su-In Lee (Lundberg and Lee, 2017). </span><span class="koboSpan" id="kobo.114.2">The idea of this framework </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.115.1">is based on using the Shapely value, a known concept named after Lloyd Shapley, an American game theorist and Nobel Prize winner (Winter, 2022). </span><span class="koboSpan" id="kobo.115.2">SHAP can determine the contribution of each feature to a model’s prediction. </span><span class="koboSpan" id="kobo.115.3">As features work cooperatively in determining the decision boundaries of classification models and eventually affecting model predictions, SHAP tries to first identify the marginal contribution of each feature and then provide Shapely values as an estimate of the contribution of each feature in cooperation with the whole feature set regarding the predictions of a model. </span><span class="koboSpan" id="kobo.115.4">From a theoretical perspective, these marginal contributions can be calculated by removing features individually and in different combinations, calculating the effect of each feature set removal, and then normalizing the contributions. </span><span class="koboSpan" id="kobo.115.5">This process can’t be repeated for all possible feature combinations as the number of possible combinations could grow exponentially to billions, even for a model with 40 features. </span><span class="koboSpan" id="kobo.115.6">Instead, this process is used a limited number of times to come up with an approximation of Shapely values. </span><span class="koboSpan" id="kobo.115.7">Also, since removing features is not possible in most machine learning models, feature values get replaced by alternative values either from a random distribution or from a background set of meaningful and possible values for each feature. </span><span class="koboSpan" id="kobo.115.8">We don’t want to get into</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.116.1"> the theoretical details of this process but we will practice using this approach in the </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">next section.</span></span></p>
<h4><span class="koboSpan" id="kobo.118.1">Local explanation using LIME</span></h4>
<p><span class="koboSpan" id="kobo.119.1">LIME is an</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.120.1"> alternative to SHAP for local explainability that </span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.121.1">explains the predictions of any classifier or regressor, in a model-agnostic way, by approximating a model locally with an interpretable model (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.122.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.123.1">.3</span></em><span class="koboSpan" id="kobo.124.1">; Ribeiro et </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">al., 2016):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.126.1"><img alt="Figure 6.3 – Schematic representation of local interpretable modeling in LIME" src="image/B16369_06_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.127.1">Figure 6.3 – Schematic representation of local interpretable modeling in LIME</span></p>
<p><span class="koboSpan" id="kobo.128.1">Some of the advantages of this technique, which were also mentioned in the original paper by Ribeiro et al., 2016, include </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.130.1">The theory and the provided explanations are intuitive and easy </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">to understand</span></span></li>
<li><span class="koboSpan" id="kobo.132.1">Sparse explanations are provided to </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">increase interpretability</span></span></li>
<li><span class="koboSpan" id="kobo.134.1">Works with different types of structured and unstructured data, such as texts </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">and images</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.136.1">Counterfactuals</span></h3>
<p><span class="koboSpan" id="kobo.137.1">Counterfactual </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.138.1">examples, or explanations, help us identify what needs to be changed in an instance to change the outcome of a classification model. </span><span class="koboSpan" id="kobo.138.2">These counterfactuals could help in identifying actionable paths in many applications, such as finance, retail, marketing, recruiting, and healthcare. </span><span class="koboSpan" id="kobo.138.3">One example is when suggesting to a bank customer how they can change the rejection to their loan application (Guidotti, 2022). </span><span class="koboSpan" id="kobo.138.4">Counterfactuals could also help in identifying biases in models that help us improve model performance or eliminate fairness issues in our models. </span><span class="koboSpan" id="kobo.138.5">We need to keep the following considerations in mind while </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.139.1">generating </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.140.1">and using counterfactual explanations (</span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">Guidotti, 2022):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Validity</span></strong><span class="koboSpan" id="kobo.143.1">: A </span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.144.1">counterfactual example is valid if and only if its classification outcome would be different from the </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">original sample.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.146.1">Similarity</span></strong><span class="koboSpan" id="kobo.147.1">: A </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.148.1">counterfactual example should be as similar as possible to the original </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">data point.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Diversity</span></strong><span class="koboSpan" id="kobo.151.1">: Although </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.152.1">counterfactual examples should be similar to the original samples they are derived from, they need to be diverse among each other to provide different options (that is, different possible </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">feature changes).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.154.1">Actionability</span></strong><span class="koboSpan" id="kobo.155.1">: Not all </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.156.1">the feature value changes are actionable. </span><span class="koboSpan" id="kobo.156.2">The actionability of the counterfactuals that are suggested by a counterfactual method is an important factor in benefitting from them </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">in practice.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.158.1">Plausibility</span></strong><span class="koboSpan" id="kobo.159.1">: The</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.160.1"> feature values of a counterfactual example should be plausible. </span><span class="koboSpan" id="kobo.160.2">The plausibility of the counterfactuals increases trust in deriving explanations </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">from them.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.162.1">We also have to note that counterfactual explainers need to be efficient and fast enough in generating the counterfactuals and stable in generating counterfactuals for similar data points (</span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">Guidotti, 2022).</span></span></p>
<h3><span class="koboSpan" id="kobo.164.1">Sample-based explainability</span></h3>
<p><span class="koboSpan" id="kobo.165.1">Another </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.166.1">approach to explainability is to rely on the feature values and results of real or synthetic data points to help in local model </span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.167.1">explainability. </span><span class="koboSpan" id="kobo.167.2">In this category of explainability techniques, we aim to find out which samples are misclassified and what feature sets result in an increasing chance of misclassification to help us explain our models. </span><span class="koboSpan" id="kobo.167.3">We can also assess which training data points result in a change in the decision boundary so that we can predict the output of test or production data points. </span><span class="koboSpan" id="kobo.167.4">There are statistical methods such </span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.168.1">as the </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">Influence function</span></strong><span class="koboSpan" id="kobo.170.1"> (Koh and Liang 2017), a classical approach for assessing the influence of samples on model parameters, that we can use to identify the sample’s contribution to the decision-making process </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">of models.</span></span></p>
<h3><span class="koboSpan" id="kobo.172.1">Rule-based explainability</span></h3>
<p><span class="koboSpan" id="kobo.173.1">Rule-based methods </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.174.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.175.1">Anchor explanations</span></strong><span class="koboSpan" id="kobo.176.1"> aim </span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.177.1">to find the conditions of feature values that </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.178.1">result in a high probability of getting the same output (Ribeiro et al., 2018). </span><span class="koboSpan" id="kobo.178.2">For example, in the case of predicting the salary of individuals in a dataset to be less than or equal to 50k or above 50k, “Education &lt;= high school to result in &lt;=50k salary” could be considered a rule in rule-based explanation. </span><span class="koboSpan" id="kobo.178.3">These explanations need to be </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">locally faithful.</span></span></p>
<h3><span class="koboSpan" id="kobo.180.1">Saliency maps</span></h3>
<p><span class="koboSpan" id="kobo.181.1">The objective of</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.182.1"> saliency maps is to explain </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.183.1">which features contribute more or less to the predicted outputs for a data point. </span><span class="koboSpan" id="kobo.183.2">These methods are commonly used in machine learning or deep learning models that have been trained on image data (Simonyan et al., 2013). </span><span class="koboSpan" id="kobo.183.3">For example, we can use saliency maps to figure out if a classification model uses a background forest to identify if it is an image of a bear rather than a teddy bear or uses the components of the bear’s body </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">for it.</span></span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor209"/><span class="koboSpan" id="kobo.185.1">Global explanation</span></h2>
<p><span class="koboSpan" id="kobo.186.1">Despite the</span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.187.1"> difficulty</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.188.1"> in achieving a reliable global explanation for machine learning models, it could increase trust in them (Ribeiro et al., 2016). </span><span class="koboSpan" id="kobo.188.2">Performance is not the only aspect of building trust when developing and deploying machine learning models. </span><span class="koboSpan" id="kobo.188.3">And local explanations, although very helpful in </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.189.1">investigating individual samples </span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.190.1">and providing actionable information, might not be enough for this trust building. </span><span class="koboSpan" id="kobo.190.2">Here, we will discuss three approaches for going beyond local explanation, including collecting local explanations, knowledge distillation, and summaries </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">of counterfactuals.</span></span></p>
<h3><span class="koboSpan" id="kobo.192.1">Collecting local explanations</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.193.1">Submodular pick LIME</span></strong><span class="koboSpan" id="kobo.194.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.195.1">SP-LIME</span></strong><span class="koboSpan" id="kobo.196.1">) is a</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.197.1"> global </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.198.1">explanation technique that uses local explanations of LIME to come up with a global perspective of a model’s behavior (Riberio et al., 2016). </span><span class="koboSpan" id="kobo.198.2">As it might not be feasible to use the local explanations of all data points, SP-LIME picks a representative diverse set of samples capable of representing the global behavior of </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">the model.</span></span></p>
<h3><span class="koboSpan" id="kobo.200.1">Knowledge distillation</span></h3>
<p><span class="koboSpan" id="kobo.201.1">The idea of </span><strong class="bold"><span class="koboSpan" id="kobo.202.1">knowledge distillation</span></strong><span class="koboSpan" id="kobo.203.1"> is</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.204.1"> to </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.205.1">approximate the behavior of complex models, which was initially proposed for neural network models, using simpler interpretable models such as decision trees (Hinton et al., 2015; Frosst and Hinton, 2017). </span><span class="koboSpan" id="kobo.205.2">In other words, we aim to build simpler models, such as decision trees, that approximate the predictions of complex models for a given set </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">of samples.</span></span></p>
<h3><span class="koboSpan" id="kobo.207.1">Summaries of counterfactuals</span></h3>
<p><span class="koboSpan" id="kobo.208.1">We can use a</span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.209.1"> summary of counterfactuals that’s been generated for multiple data points with correct and incorrect predicted outcomes to figure out the contribution of features in output prediction and the sensitivity of a prediction to feature perturbation. </span><span class="koboSpan" id="kobo.209.2">We will practice using counterfactuals later in this chapter, where you will see that not all counterfactuals are acceptable and they need to be chosen according to the meaning behind features and </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">their values.</span></span></p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.211.1">Practicing machine learning explainability in Python</span></h1>
<p><span class="koboSpan" id="kobo.212.1">There are</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.213.1"> several Python libraries you can use to extract local and global explanations for your machine learning models (</span><em class="italic"><span class="koboSpan" id="kobo.214.1">Table 6.1</span></em><span class="koboSpan" id="kobo.215.1">). </span><span class="koboSpan" id="kobo.215.2">Here, we want to practice with a few of the ones that focus on local </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">model explainability:</span></span></p>
<table class="No-Table-Style" id="table001-5">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.217.1">Library</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.218.1">Library Name for Importing </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.219.1">and Installation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.220.1">URL</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.221.1">SHAP</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">Shap</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/shap/"><span class="No-Break"><span class="koboSpan" id="kobo.223.1">https://pypi.org/project/shap/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.224.1">LIME</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">Lime</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/lime/"><span class="No-Break"><span class="koboSpan" id="kobo.226.1">https://pypi.org/project/lime/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.227.1">Shapash</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">shapash</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/shapash/"><span class="No-Break"><span class="koboSpan" id="kobo.229.1">https://pypi.org/project/shapash/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.230.1">ELI5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">eli5</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/eli5/"><span class="No-Break"><span class="koboSpan" id="kobo.232.1">https://pypi.org/project/eli5/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.233.1">Explainer dashboard</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">explainer dashboard</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/explainerdashboard/"><span class="No-Break"><span class="koboSpan" id="kobo.235.1">https://pypi.org/project/explainerdashboard/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.236.1">Dalex</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">dalex</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/dalex/"><span class="No-Break"><span class="koboSpan" id="kobo.238.1">https://pypi.org/project/dalex/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.239.1">OmniXAI</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.240.1">omnixai</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/omnixai/"><span class="No-Break"><span class="koboSpan" id="kobo.241.1">https://pypi.org/project/omnixai/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.242.1">CARLA</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">carla</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/"><span class="No-Break"><span class="koboSpan" id="kobo.244.1">https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.245.1">Diverse Counterfactual </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.246.1">Explanations</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.247.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.248.1">DiCE</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">dice-ml</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/dice-ml/"><span class="No-Break"><span class="koboSpan" id="kobo.251.1">https://pypi.org/project/dice-ml/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.252.1">Machine Learning </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">Library Extensions</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">mlxtend</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://pypi.org/project/mlxtend/"><span class="No-Break"><span class="koboSpan" id="kobo.255.1">https://pypi.org/project/mlxtend/</span></span></a></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.256.1">Anchor</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">anchor</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><a href="https://github.com/marcotcr/anchor"><span class="No-Break"><span class="koboSpan" id="kobo.258.1">https://github.com/marcotcr/anchor</span></span></a></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.259.1">Table 6.1 – Python libraries or repositories with available functionalities for machine learning model explainability</span></p>
<p><span class="koboSpan" id="kobo.260.1">First, we will practice </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.261.1">with SHAP, a widely used technique for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">learning explainability.</span></span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.263.1">Explanations in SHAP</span></h2>
<p><span class="koboSpan" id="kobo.264.1">We’ll first look </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.265.1">at performing local explanation with SHAP, followed by global </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">explanation later.</span></span></p>
<h3><span class="koboSpan" id="kobo.267.1">Local explanation</span></h3>
<p><span class="koboSpan" id="kobo.268.1">In this section, we </span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.269.1">will practice with SHAP to extract feature importance from our machine learning models. </span><span class="koboSpan" id="kobo.269.2">We will use the </span><strong class="bold"><span class="koboSpan" id="kobo.270.1">University of California Irvine</span></strong><span class="koboSpan" id="kobo.271.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.272.1">UCI</span></strong><span class="koboSpan" id="kobo.273.1">) adult</span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.274.1"> dataset to predict if people made over $50k in the 90s; this is also available as the adult income dataset as part of the SHAP library. </span><span class="koboSpan" id="kobo.274.2">You can read about the definition of the features and other information about this dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">at </span></span><a href="https://archive.ics.uci.edu/ml/datasets/adult"><span class="No-Break"><span class="koboSpan" id="kobo.276.1">https://archive.ics.uci.edu/ml/datasets/adult</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.277.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">First, we need to build a supervised machine learning model using this dataset before using any explainability method. </span><span class="koboSpan" id="kobo.278.2">We will </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.279.1">use </span><strong class="bold"><span class="koboSpan" id="kobo.280.1">XGBoost</span></strong><span class="koboSpan" id="kobo.281.1"> as a high-performance machine learning method for tabular data to practice </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">with SHAP:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.283.1">
# loading UCI adult income dataset# classification task to predict if people made over $50k in the 90s or not
X,y = shap.datasets.adult()
# split the data to train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size = 0.3, random_state=10)
# initializing a XGboost model
xgb_model = xgboost.XGBClassifier(random_state=42)
# fitting the XGboost model with training data
xgb_model.fit(X_train, y_train)
# generating predictions for the test set
y_pred = xgb_model.predict(X_test)
# identifying misclassified datapoints in the test set
misclassified_index = np.where(y_test != y_pred)[0]
# calculating roc-auc of predictions
print("ROC-AUC of predictions: {}".format(
    roc_auc_score(y_test, xgb_model.predict_proba(
        X_test)[:, 1])))
print("First 5 misclassified test set datapoints:
    {}".format(misclassified_index[0:5]))</span></pre>
<p><span class="koboSpan" id="kobo.284.1">There are </span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.285.1">different methods to approximate feature importance that are available in the SHAP library, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.286.1">shap.LinearExplainer()</span></strong><span class="koboSpan" id="kobo.287.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">shap.KernelExplainer()</span></strong><span class="koboSpan" id="kobo.289.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.290.1">shap.TreeExplainer()</span></strong><span class="koboSpan" id="kobo.291.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">shap.DeepExplainer()</span></strong><span class="koboSpan" id="kobo.293.1">. </span><span class="koboSpan" id="kobo.293.2">You can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">shap.TreeExplainer()</span></strong><span class="koboSpan" id="kobo.295.1"> in the case of tree-based methods such as random forest and XGBoost. </span><span class="koboSpan" id="kobo.295.2">Let’s build an explainer object using the trained model and then extract </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">Shapely values:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.297.1">
# generate the Tree explainerexplainer = shap.TreeExplainer(xgb_model)
# extract SHAP values from the explainer object
shap_values = explainer.shap_values(X_test)</span></pre>
<p><span class="koboSpan" id="kobo.298.1">There are </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.299.1">multiple plotting functions in the SHAP library to provide us with visual illustrations of feature importance using Shapely values. </span><span class="koboSpan" id="kobo.299.2">For example, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">shap.dependence_plot()</span></strong><span class="koboSpan" id="kobo.301.1"> to identify the Shapely value for the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.302.1">Education-Num</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.303.1"> feature:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.304.1">
# If interaction_index of "auto" is chosen then# the strongest interaction is used to color the dots.
</span><span class="koboSpan" id="kobo.304.2">shap.dependence_plot("Education-Num", shap_values, X_test)</span></pre>
<p><span class="koboSpan" id="kobo.305.1">The following dependence plot clearly shows that a higher </span><em class="italic"><span class="koboSpan" id="kobo.306.1">Education-Num</span></em><span class="koboSpan" id="kobo.307.1"> value results in a higher Shapely value or a greater contribution in predicting a positive outcome (that is, &gt;</span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">50k salary):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.309.1"><img alt="Figure 6.4 – SHAP values for the Education-Num feature in the test set of the adult income dataset" src="image/B16369_06_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.310.1">Figure 6.4 – SHAP values for the Education-Num feature in the test set of the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.311.1">We can repeat</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.312.1"> this process with other features, such as </span><em class="italic"><span class="koboSpan" id="kobo.313.1">Age</span></em><span class="koboSpan" id="kobo.314.1">, which results in a similar explanation as </span><em class="italic"><span class="koboSpan" id="kobo.315.1">Education-Num</span></em><span class="koboSpan" id="kobo.316.1">. </span><span class="koboSpan" id="kobo.316.2">The only difference in using </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">shap.dependence_plot()</span></strong><span class="koboSpan" id="kobo.318.1"> for </span><em class="italic"><span class="koboSpan" id="kobo.319.1">Education-Num</span></em><span class="koboSpan" id="kobo.320.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.321.1">Age</span></em><span class="koboSpan" id="kobo.322.1"> is </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">interaction_index</span></strong><span class="koboSpan" id="kobo.324.1">, which is specified as </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">None</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.326.1">for </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.327.1">Age</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.329.1">
# generate dependence plot for "Age" featureshap.dependence_plot("Age", shap_values, X_test,
    interaction_index=None)</span></pre>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.330.1"><img alt="Figure 6.5 – SHAP values for the Age feature in the test set of the adult income dataset" src="image/B16369_06_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.331.1">Figure 6.5 – SHAP values for the Age feature in the test set of the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.332.1">If we need to </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.333.1">extract an explanation of our model on a specific subset of our dataset, we can use the same functions but use the subset of data we want to investigate instead of the whole dataset. </span><span class="koboSpan" id="kobo.333.2">We can also use training and test sets to identify explanations in the data that’s used for model training and unseen data we want to use to evaluate the performance of our model. </span><span class="koboSpan" id="kobo.333.3">To showcase this, we will investigate the importance of </span><em class="italic"><span class="koboSpan" id="kobo.334.1">Age</span></em><span class="koboSpan" id="kobo.335.1"> on a subset of the test set that’s been misclassified using the </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.337.1">
# generate dependence plot for "Age" featureshap.dependence_plot("Age",
    shap_values[misclassified_index],
    X_test.iloc[misclassified_index,:],
    interaction_index=None)</span></pre>
<p><span class="koboSpan" id="kobo.338.1">As you can see, the SHAP values have similar trends for misclassified data points (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.339.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.340.1">.6</span></em><span class="koboSpan" id="kobo.341.1">) and the whole dataset (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.342.1">Figure 6</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.343.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.345.1"><img alt="Figure 6.6 – SHAP values for the Age feature for the misclassified data points in the test set of the adult income dataset" src="image/B16369_06_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.346.1">Figure 6.6 – SHAP values for the Age feature for the misclassified data points in the test set of the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.347.1">In addition to</span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.348.1"> extracting Shapely values across a series of data points, we also need to investigate how features contributed to the correct or wrong prediction for a data point. </span><span class="koboSpan" id="kobo.348.2">Here, we chose two samples: </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">sample 12</span></strong><span class="koboSpan" id="kobo.350.1">, with the actual label being False or 0 (that is, low income) and the predicted label being True or 1 (that is, high income), and </span><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">sample 24</span></strong><span class="koboSpan" id="kobo.352.1">, with the actual and predicted labels of True and False, respectively. </span><span class="koboSpan" id="kobo.352.2">Here, we can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">shap.plots._waterfall.waterfall_legacy()</span></strong><span class="koboSpan" id="kobo.354.1"> and extract the expected values of the input features, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.355.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.356.1">.7</span></em><span class="koboSpan" id="kobo.357.1">. </span><span class="koboSpan" id="kobo.357.2">In this kind of plotting in SHAP, for each feature, </span><em class="italic"><span class="koboSpan" id="kobo.358.1">X</span></em><span class="koboSpan" id="kobo.359.1">, </span><em class="italic"><span class="koboSpan" id="kobo.360.1">f(X)</span></em><span class="koboSpan" id="kobo.361.1"> is the predicted value given </span><em class="italic"><span class="koboSpan" id="kobo.362.1">X</span></em><span class="koboSpan" id="kobo.363.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.364.1">E[f(X)]</span></em><span class="koboSpan" id="kobo.365.1"> is the expected value of the target variable (that is, the mean of all predictions, </span><em class="italic"><span class="koboSpan" id="kobo.366.1">mean(model.predict(X))</span></em><span class="koboSpan" id="kobo.367.1">). </span><span class="koboSpan" id="kobo.367.2">This plot shows us how much a single feature affected </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">the prediction:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.369.1">
# extracting expected valuesexpected_value = explainer.expected_value
# generate waterfall plot for observation 12
shap.plots._waterfall.waterfall_legacy(expected_value,
    shap_values[12], features=X_test.iloc[12,:],
    feature_names=X.columns, max_display=15, show=True)
# generate waterfall plot for observation 24
shap.plots._waterfall.waterfall_legacy(expected_value,
    shap_values[24],features=X_test.iloc[24,:],
    feature_names=X.columns,max_display=15, show=True)</span></pre>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.370.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.371.1">.7</span></em><span class="koboSpan" id="kobo.372.1">, which is for </span><strong class="source-inline"><span class="koboSpan" id="kobo.373.1">sample 12</span></strong><span class="koboSpan" id="kobo.374.1">, shows us that </span><em class="italic"><span class="koboSpan" id="kobo.375.1">Relationship</span></em><span class="koboSpan" id="kobo.376.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.377.1">Education-Num</span></em><span class="koboSpan" id="kobo.378.1"> are the features</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.379.1"> with the most effect, and </span><em class="italic"><span class="koboSpan" id="kobo.380.1">Race</span></em><span class="koboSpan" id="kobo.381.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.382.1">Country</span></em><span class="koboSpan" id="kobo.383.1"> are the ones with the least effect on the outcome of </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">this sample:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.385.1"><img alt="Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset" src="image/B16369_06_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.386.1">Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset</span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.387.1">Relationship</span></em><span class="koboSpan" id="kobo.388.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.389.1">Education-Num</span></em><span class="koboSpan" id="kobo.390.1"> are also the features with the most effect for </span><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">sample 24</span></strong><span class="koboSpan" id="kobo.392.1"> (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.393.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.394.1">.8</span></em><span class="koboSpan" id="kobo.395.1">). </span><span class="koboSpan" id="kobo.395.2">However, the third most contribution in </span><strong class="source-inline"><span class="koboSpan" id="kobo.396.1">sample 12</span></strong><span class="koboSpan" id="kobo.397.1"> is from </span><em class="italic"><span class="koboSpan" id="kobo.398.1">Hours per week</span></em><span class="koboSpan" id="kobo.399.1">, which has a low effect on the outcome of </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">sample 24</span></strong><span class="koboSpan" id="kobo.401.1">. </span><span class="koboSpan" id="kobo.401.2">This is the type of analysis we </span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.402.1">can do to compare some of the incorrect predictions and identify potentially actionable suggestions for improving model performance. </span><span class="koboSpan" id="kobo.402.2">Alternatively, we can extract actionable suggestions to improve the future income of individuals in </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">this dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.404.1"><img alt="Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset" src="image/B16369_06_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.405.1">Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.406.1">Despite the easy-to-understand insights provided by SHAP, we need to make sure feature dependencies in our models don’t lead to confusion when we interpret </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">Shapely values.</span></span></p>
<h3><span class="koboSpan" id="kobo.408.1">Global explanation</span></h3>
<p><span class="koboSpan" id="kobo.409.1">Although </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">shap.dependence_plot()</span></strong><span class="koboSpan" id="kobo.411.1"> might seem to provide a global explanation, as it shows the </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.412.1">effect of a feature across all or a subset of data points, we need explanations across model features and data points to build trust for our models. </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">shap.summary_plot()</span></strong><span class="koboSpan" id="kobo.414.1"> is an example of such a global explanation that summarizes the Shapely values of features across the specified set of data points. </span><span class="koboSpan" id="kobo.414.2">These kinds of summary plots and results are important for identifying the most effective features and understanding if there are biases, such as concerning race or sex, in our model. </span><span class="koboSpan" id="kobo.414.3">With the following summary plot (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.415.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.416.1">.9</span></em><span class="koboSpan" id="kobo.417.1">), we can easily see that </span><em class="italic"><span class="koboSpan" id="kobo.418.1">Sex</span></em><span class="koboSpan" id="kobo.419.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.420.1">Race</span></em><span class="koboSpan" id="kobo.421.1"> are not among the features with the most effect, although their effect is not necessarily negligible and might need further investigation. </span><span class="koboSpan" id="kobo.421.2">We will talk about model bias and fairness in the </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">next chapter:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<span class="koboSpan" id="kobo.423.1"><img alt="Figure 6.9 – SHAP summary plot for the adult income dataset" src="image/B16369_06_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.424.1">Figure 6.9 – SHAP summary plot for the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.425.1">Here is the code to generate the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">summary plot:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.427.1">
# create a SHAP beeswarm plot (i.e. </span><span class="koboSpan" id="kobo.427.2">SHAP summary plot)shap.summary_plot(shap_values, X_test,plot_type="bar")</span></pre>
<h2 id="_idParaDest-132"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.428.1">Explanations using LIME</span></h2>
<p><span class="koboSpan" id="kobo.429.1">Having </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.430.1">learned how to perform explanations using SHAP, we will now turn our attention to LIME. </span><span class="koboSpan" id="kobo.430.2">We will start with local </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">explanation first.</span></span></p>
<h3><span class="koboSpan" id="kobo.432.1">Local explanation</span></h3>
<p><span class="koboSpan" id="kobo.433.1">LIME is another </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.434.1">way to get an easy-to-understand local explanation for individual data points. </span><span class="koboSpan" id="kobo.434.2">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">lime</span></strong><span class="koboSpan" id="kobo.436.1"> Python library to build an explainer object and then use it to identify local explanations for samples of interest. </span><span class="koboSpan" id="kobo.436.2">Here, once again, we will use the XGBoost model we trained for SHAP and generate explanations for </span><strong class="source-inline"><span class="koboSpan" id="kobo.437.1">sample 12</span></strong><span class="koboSpan" id="kobo.438.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">sample 24</span></strong><span class="koboSpan" id="kobo.440.1"> to show that their outcomes were </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">incorrectly predicted.</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">By default, </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">lime</span></strong><span class="koboSpan" id="kobo.444.1"> uses </span><em class="italic"><span class="koboSpan" id="kobo.445.1">ridge regression</span></em><span class="koboSpan" id="kobo.446.1"> as the interpretable model for generating local explanations. </span><span class="koboSpan" id="kobo.446.2">We can change this method in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">lime.lime_tabular.LimeTabularExplainer()</span></strong><span class="koboSpan" id="kobo.448.1"> class by changing </span><strong class="source-inline"><span class="koboSpan" id="kobo.449.1">feature_selection</span></strong><span class="koboSpan" id="kobo.450.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">none</span></strong><span class="koboSpan" id="kobo.452.1"> for linear modeling without any feature selection, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">lasso_path</span></strong><span class="koboSpan" id="kobo.454.1">, which uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.455.1">lasso_path()</span></strong><span class="koboSpan" id="kobo.456.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.457.1">scikit-learn</span></strong><span class="koboSpan" id="kobo.458.1">, as another form of supervised linear modeling </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">with regularization.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.460.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.461.1">The mode fitting line for fitting the XGBoost model for the UCI adult dataset, which was presented in a previous code snippet, needs to be changed since </span><strong class="source-inline"><span class="koboSpan" id="kobo.462.1">xgb_model.fit(np.array(X_train), y_train)</span></strong><span class="koboSpan" id="kobo.463.1"> makes the model usable for the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">lime</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.465.1"> library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.466.1">
# create explainerexplainer = lime.lime_tabular.LimeTabularExplainer(
    np.array(X_train), feature_names=X_train.columns,
    #X_train.to_numpy()
    class_names=['Lower income','Higher income'],
    verbose=True)
# visualizing explanation by LIME
print('actual label of sample 12: {}'.format(y_test[12]))
print('prediction for sample 12: {}'.format(y_pred[12]))
exp = explainer.explain_instance(
    data_row = X_test.iloc[12],
    predict_fn = xgb_model.predict_proba)
exp.show_in_notebook(show_table=True)</span></pre>
<p><span class="koboSpan" id="kobo.467.1">You can</span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.468.1"> interpret the middle plot in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.469.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.470.1">.10</span></em><span class="koboSpan" id="kobo.471.1"> for </span><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">sample 12</span></strong><span class="koboSpan" id="kobo.473.1"> as the local contribution of features in predicting the outcome as </span><em class="italic"><span class="koboSpan" id="kobo.474.1">Higher income</span></em><span class="koboSpan" id="kobo.475.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.476.1">Lower income</span></em><span class="koboSpan" id="kobo.477.1">. </span><span class="koboSpan" id="kobo.477.2">Similar to SHAP, the </span><em class="italic"><span class="koboSpan" id="kobo.478.1">Education-Num</span></em><span class="koboSpan" id="kobo.479.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.480.1">Relationship</span></em><span class="koboSpan" id="kobo.481.1"> features contribute the most to the sample being incorrectly predicted as </span><em class="italic"><span class="koboSpan" id="kobo.482.1">Higher income</span></em><span class="koboSpan" id="kobo.483.1">. </span><span class="koboSpan" id="kobo.483.2">On the other hand, </span><em class="italic"><span class="koboSpan" id="kobo.484.1">Capital Gain</span></em><span class="koboSpan" id="kobo.485.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.486.1">Capital Loss</span></em><span class="koboSpan" id="kobo.487.1"> have the maximum contribution in pushing the prediction of the sample’s output as the other class. </span><span class="koboSpan" id="kobo.487.2">But we also have to pay attention to feature values as both </span><em class="italic"><span class="koboSpan" id="kobo.488.1">Capital Gain</span></em><span class="koboSpan" id="kobo.489.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.490.1">Capital Loss</span></em><span class="koboSpan" id="kobo.491.1"> are zero for </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">this sample:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.493.1"><img alt="Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset" src="image/B16369_06_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.494.1">Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.495.1">Similarly, we </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.496.1">can investigate the result of LIME for </span><strong class="source-inline"><span class="koboSpan" id="kobo.497.1">sample 24</span></strong><span class="koboSpan" id="kobo.498.1">, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.499.1">Figure 6</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.500.1">.11</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.502.1"><img alt="Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset" src="image/B16369_06_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.503.1">Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset</span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.504.1">Capital Gain</span></em><span class="koboSpan" id="kobo.505.1">, </span><em class="italic"><span class="koboSpan" id="kobo.506.1">Education-Num</span></em><span class="koboSpan" id="kobo.507.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.508.1">Hours per week</span></em><span class="koboSpan" id="kobo.509.1"> contribute the most to predicting the output in positive or negative directions. </span><span class="koboSpan" id="kobo.509.2">However, </span><em class="italic"><span class="koboSpan" id="kobo.510.1">Capital Gain</span></em><span class="koboSpan" id="kobo.511.1"> doesn’t affect this specific data </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.512.1">point as its value </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">is zero.</span></span></p>
<h3><span class="koboSpan" id="kobo.514.1">Global explanation</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.515.1">Submodular pick LIME</span></strong><span class="koboSpan" id="kobo.516.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.517.1">SP_LIME</span></strong><span class="koboSpan" id="kobo.518.1">) is a </span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.519.1">global </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.520.1">explanation method in which a subset of samples get selected as candidates, and then we can use the explanation of these candidates via LIME so that they’re representative of the global explanation of the model. </span><span class="koboSpan" id="kobo.520.2">We can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">lime.submodular_pick.SubmodularPick()</span></strong><span class="koboSpan" id="kobo.522.1"> to pick these samples. </span><span class="koboSpan" id="kobo.522.2">Here are the parameters of this class that could help you explain your global regression or </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">classification models:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">predict_fn</span></strong><span class="koboSpan" id="kobo.525.1"> (prediction function): For </span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">ScikitClassifiers</span></strong><span class="koboSpan" id="kobo.527.1">, this is </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">classifier.predict_proba()</span></strong><span class="koboSpan" id="kobo.529.1">, while for </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">ScikitRegressors</span></strong><span class="koboSpan" id="kobo.531.1">, this </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">is </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">regressor.predict()</span></strong></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">sample_size</span></strong><span class="koboSpan" id="kobo.535.1">: The number of data points to explain if </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">method == 'sample'</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.537.1">is chosen</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">num_exps_desired</span></strong><span class="koboSpan" id="kobo.539.1">: The number of explanation </span><span class="No-Break"><span class="koboSpan" id="kobo.540.1">objects returned</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.541.1">num_features</span></strong><span class="koboSpan" id="kobo.542.1">: The maximum number of features present in </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">the explanation:</span></span></li>
</ul>
<pre class="source-code"><span class="koboSpan" id="kobo.544.1">
sp_obj = submodular_pick.SubmodularPick(explainer,    np.array(X_train), xgb_model.predict_proba,
    method='sample', sample_size=3, num_features=8,
    num_exps_desired=5)
# showing explanation for the picked instances for explanation if you are using Jupyter or Colab notebook
[exp.show_in_notebook() for exp in sp_obj.explanations]</span></pre>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.545.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.546.1">.12</span></em><span class="koboSpan" id="kobo.547.1"> shows the three data points that were picked </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">by SP-LIME:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.549.1"><img alt="Figure 6.12 – Data points selected by SPI-LIME for global explainability" src="image/B16369_06_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.550.1">Figure 6.12 – Data points selected by SPI-LIME for global explainability</span></p>
<p><span class="koboSpan" id="kobo.551.1">But instead of </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.552.1">visualizing the picked instances, you can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">as_map()</span></strong><span class="koboSpan" id="kobo.554.1"> parameter instead of </span><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">show_in_notebook()</span></strong><span class="koboSpan" id="kobo.556.1"> for each explanation object, as part of the explanation objects in </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">sp_obj.explanations</span></strong><span class="koboSpan" id="kobo.558.1">, and then summarize the information for a bigger set of data points instead of investigating a handful of samples. </span><span class="koboSpan" id="kobo.558.2">For such analysis, you can use a small percentage of data points, such as 1% or lower in the case of very large datasets with tens of thousands of data points </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">or more.</span></span></p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.560.1">Counterfactual generation using Diverse Counterfactual Explanations (DiCE)</span></h2>
<p><span class="koboSpan" id="kobo.561.1">You</span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.562.1"> can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">dice_ml</span></strong><span class="koboSpan" id="kobo.564.1"> Python library (Mothilal et al., 2020) to generate counterfactuals and understand </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.565.1">how a </span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.566.1">model can switch from one prediction to another, as explained earlier in this chapter. </span><span class="koboSpan" id="kobo.566.2">First, we must train a model and then make an explanation object using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.567.1">dice_ml.Dice()</span></strong><span class="koboSpan" id="kobo.568.1"> Python class, after installing and importing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.569.1">dice_ml</span></strong><span class="koboSpan" id="kobo.570.1"> library, </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.572.1">
### This example is taken from https://github.com/interpretml/DiCE ###dataset = helpers.load_adult_income_dataset()
target = dataset["income"] # outcome variable
train_dataset, test_dataset, _, _ = train_test_split(
    dataset,target,test_size=0.2,random_state=0,
    stratify=target)
# Dataset for training an ML model
d = dice_ml.Data(dataframe=train_dataset,
    continuous_features=['age','hours_per_week'],
    outcome_name='income')
# Pre-trained ML model
m = dice_ml.Model(
    model_path=dice_ml.utils.helpers.get_adult_income_modelpath(),
    backend='TF2', func="ohe-min-max")
# DiCE explanation instance
exp = dice_ml.Dice(d,m)</span></pre>
<p><span class="koboSpan" id="kobo.573.1">Then, we can use the generated explanation object to generate counterfactuals for one or multiple </span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.574.1">samples. </span><span class="koboSpan" id="kobo.574.2">Here, we</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.575.1"> are generating 10 counterfactuals for </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.576.1">sample 1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.578.1">
query_instance = test_dataset.drop(columns="income")[0:1]dice_exp = exp.generate_counterfactuals(query_instance,
    total_CFs=10, desired_class="opposite",
    random_seed = 42)
# Visualize counterfactual explanation
dice_exp.visualize_as_dataframe()</span></pre>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.579.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.580.1">.13</span></em><span class="koboSpan" id="kobo.581.1"> shows </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.582.1">both the feature values of the target sample and 10 </span><span class="No-Break"><span class="koboSpan" id="kobo.583.1">corresponding counterfactuals:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.584.1"><img alt="Figure 6.13 – A selected data point and the generated counterfactuals from the adult income dataset" src="image/B16369_06_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.585.1">Figure 6.13 – A selected data point and the generated counterfactuals from the adult income dataset</span></p>
<p><span class="koboSpan" id="kobo.586.1">Although all counterfactuals meet the objective of switching the outcome of the target sample (that is, </span><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">sample 1</span></strong><span class="koboSpan" id="kobo.588.1">), not all counterfactuals are feasible according to the definition and meaning of each feature. </span><span class="koboSpan" id="kobo.588.2">For example, if we want to suggest to a 29-year-old individual that they change their outcome from low to high salary, suggesting that they will </span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.589.1">earn a high </span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.590.1">salary </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.591.1">when they are 80 years old is not an effective and actionable suggestion. </span><span class="koboSpan" id="kobo.591.2">Also, suggesting a change of </span><em class="italic"><span class="koboSpan" id="kobo.592.1">hours_per_week</span></em><span class="koboSpan" id="kobo.593.1"> of work from 38 to &gt;90 is not feasible. </span><span class="koboSpan" id="kobo.593.2">You need to use such considerations in rejecting counterfactuals so that you can identify opportunities for model performance and provide actionable suggestions to users. </span><span class="koboSpan" id="kobo.593.3">Also, you can switch between different techniques to generate more meaningful counterfactuals for your models </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">and applications.</span></span></p>
<p><span class="koboSpan" id="kobo.595.1">There are more recent Python libraries such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">Dalex</span></strong><span class="koboSpan" id="kobo.597.1"> (Baniecki et al., 2021) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">OmniXA</span></strong><span class="koboSpan" id="kobo.599.1"> (Yang et al., 2022) that you can use for model explainability. </span><span class="koboSpan" id="kobo.599.2">We will also discuss how these methods and Python libraries can be used to decrease bias and help us move toward fairness in developing new or revising our already trained machine </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">learning models.</span></span></p>
<h1 id="_idParaDest-134"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.601.1">Reviewing why having explainability is not enough</span></h1>
<p><span class="koboSpan" id="kobo.602.1">Explainability</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.603.1"> helps us build trust for the users of our models. </span><span class="koboSpan" id="kobo.603.2">As you learned in this chapter, you can use explainability techniques to understand how your models generate the outputs for one or multiple instances in a dataset. </span><span class="koboSpan" id="kobo.603.3">These explanations could help in improving our models from a performance and fairness perspective. </span><span class="koboSpan" id="kobo.603.4">However, we cannot achieve such improvements by simply using these techniques blindly and generating some results in Python. </span><span class="koboSpan" id="kobo.603.5">For example, as we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.604.1">Counterfactual generation using Diverse Counterfactual Explanations (DiCE)</span></em><span class="koboSpan" id="kobo.605.1"> section, some of the generated counterfactuals might not be reasonable and meaningful and we cannot rely on them. </span><span class="koboSpan" id="kobo.605.2">Or, when generating local explanations for one or multiple data points using SHAP or LIME, we need to pay attention to the meaning of features, the range of values for each feature and the meaning behind them, and the characteristics of each data point we investigate. </span><span class="koboSpan" id="kobo.605.3">One aspect of decision-making using explainability is to distinguish the issues with the model and the specific data points in training, testing, or production that we are investigating. </span><span class="koboSpan" id="kobo.605.4">A data point could be an outlier that makes our model less reliable for it but doesn’t necessarily make our model less reliable as a whole. </span><span class="koboSpan" id="kobo.605.5">In the next chapter, </span><a href="B16369_07.xhtml#_idTextAnchor218"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.606.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.607.1">, </span><em class="italic"><span class="koboSpan" id="kobo.608.1">Decreasing Bias and Achieving Fairness</span></em><span class="koboSpan" id="kobo.609.1">, we will discuss that bias detection is not simply about identifying if there are features such as </span><em class="italic"><span class="koboSpan" id="kobo.610.1">age</span></em><span class="koboSpan" id="kobo.611.1">, </span><em class="italic"><span class="koboSpan" id="kobo.612.1">race</span></em><span class="koboSpan" id="kobo.613.1">, or </span><em class="italic"><span class="koboSpan" id="kobo.614.1">skin color</span></em><span class="koboSpan" id="kobo.615.1"> that our models </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">rely on.</span></span></p>
<p><span class="koboSpan" id="kobo.617.1">Altogether, these </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.618.1">considerations tell us that running a few Python classes to use explainability for our models is not enough to achieve trust and generate meaningful explanations. </span><span class="koboSpan" id="kobo.618.2">There is more </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">to it.</span></span></p>
<h1 id="_idParaDest-135"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.620.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.621.1">In this chapter, you learned about interpretable machine learning models and how explainability techniques could help you in improving the performance and reliability of your models. </span><span class="koboSpan" id="kobo.621.2">You learned about different local and global explainability techniques, such as SHAP and LIME, and practiced with them in Python. </span><span class="koboSpan" id="kobo.621.3">You also had the chance to practice with the provided Python code to learn how to use machine learning explainability techniques in </span><span class="No-Break"><span class="koboSpan" id="kobo.622.1">your projects.</span></span></p>
<p><span class="koboSpan" id="kobo.623.1">In the next chapter, you will learn about the approaches to detect and decrease biases in your models and how you can use the available functionalities in Python to meet the necessary fairness criteria when developing machine </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">learning models.</span></span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.625.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.626.1">How could explainability help you improve your </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">model’s performance?</span></span></li>
<li><span class="koboSpan" id="kobo.628.1">What is the difference between local and </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">global explainability?</span></span></li>
<li><span class="koboSpan" id="kobo.630.1">Is it better to use linear models because of </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">their interpretability?</span></span></li>
<li><span class="koboSpan" id="kobo.632.1">Does explainability analysis make a machine learning model </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">more reliable?</span></span></li>
<li><span class="koboSpan" id="kobo.634.1">Could you explain the difference between SHAP and LIME for machine </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">learning explainability?</span></span></li>
<li><span class="koboSpan" id="kobo.636.1">How could you benefit from counterfactuals in developing machine </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">learning models?</span></span></li>
<li><span class="koboSpan" id="kobo.638.1">Assume a machine learning model is used for loan approval in a bank. </span><span class="koboSpan" id="kobo.638.2">Are all suggested counterfactuals useful in suggesting ways a person could improve their chance of </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">getting approval?</span></span></li>
</ol>
<h1 id="_idParaDest-137"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.640.1">References</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.641.1">Weber, Leander, et al. </span><em class="italic"><span class="koboSpan" id="kobo.642.1">Beyond explaining: Opportunities and challenges of XAI-based model improvement</span></em><span class="koboSpan" id="kobo.643.1">. </span><span class="koboSpan" id="kobo.643.2">Information </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">Fusion (2022).</span></span></li>
<li><span class="koboSpan" id="kobo.645.1">Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. </span><em class="italic"><span class="koboSpan" id="kobo.646.1">Explainable AI: A review of machine learning interpretability methods</span></em><span class="koboSpan" id="kobo.647.1">. </span><span class="koboSpan" id="kobo.647.2">Entropy 23.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">2020): 18.</span></span></li>
<li><span class="koboSpan" id="kobo.649.1">Gilpin, Leilani H., et al. </span><em class="italic"><span class="koboSpan" id="kobo.650.1">Explaining explanations: An overview of interpretability of machine learning</span></em><span class="koboSpan" id="kobo.651.1">. </span><span class="koboSpan" id="kobo.651.2">2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">IEEE, 2018.</span></span></li>
<li><span class="koboSpan" id="kobo.653.1">Carvalho, Diogo V., Eduardo M. </span><span class="koboSpan" id="kobo.653.2">Pereira, and Jaime S. </span><span class="koboSpan" id="kobo.653.3">Cardoso. </span><em class="italic"><span class="koboSpan" id="kobo.654.1">Machine learning interpretability: A survey on methods and metrics</span></em><span class="koboSpan" id="kobo.655.1">. </span><span class="koboSpan" id="kobo.655.2">Electronics 8.8 (</span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">2019): 832.</span></span></li>
<li><span class="koboSpan" id="kobo.657.1">Winter, Eyal. </span><em class="italic"><span class="koboSpan" id="kobo.658.1">The Shapley value</span></em><span class="koboSpan" id="kobo.659.1">. </span><span class="koboSpan" id="kobo.659.2">Handbook of game theory with economic applications 3 (</span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">2002): 2025-2054.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.661.1">A Guide to Explainable AI Using </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.662.1">Python</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">: </span></span><a href="https://www.thepythoncode.com/article/explainable-ai-model-python"><span class="No-Break"><span class="koboSpan" id="kobo.664.1">https://www.thepythoncode.com/article/explainable-ai-model-python</span></span></a></li>
<li><span class="koboSpan" id="kobo.665.1">Burkart, Nadia, and Marco F. </span><span class="koboSpan" id="kobo.665.2">Huber. </span><em class="italic"><span class="koboSpan" id="kobo.666.1">A survey on the explainability of supervised machine learning</span></em><span class="koboSpan" id="kobo.667.1">. </span><span class="koboSpan" id="kobo.667.2">Journal of Artificial Intelligence Research 70 (</span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">2021): 245-317.</span></span></li>
<li><span class="koboSpan" id="kobo.669.1">Guidotti, Riccardo. </span><em class="italic"><span class="koboSpan" id="kobo.670.1">Counterfactual explanations and how to find them: literature review and benchmarking</span></em><span class="koboSpan" id="kobo.671.1">. </span><span class="koboSpan" id="kobo.671.2">Data Mining and Knowledge Discovery (</span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">2022): 1-55.</span></span></li>
<li><span class="koboSpan" id="kobo.673.1">Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. </span><em class="italic"><span class="koboSpan" id="kobo.674.1">Anchors: High-precision model-agnostic explanations</span></em><span class="koboSpan" id="kobo.675.1">. </span><span class="koboSpan" id="kobo.675.2">Proceedings of the AAAI conference on artificial intelligence. </span><span class="koboSpan" id="kobo.675.3">Vol. </span><span class="koboSpan" id="kobo.675.4">32. </span><span class="koboSpan" id="kobo.675.5">No. </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">1. </span><span class="koboSpan" id="kobo.676.2">2018.</span></span></li>
<li><span class="koboSpan" id="kobo.677.1">Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. </span><em class="italic"><span class="koboSpan" id="kobo.678.1">Distilling the knowledge in a neural network</span></em><span class="koboSpan" id="kobo.679.1">. </span><span class="koboSpan" id="kobo.679.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">arXiv:1503.02531 (2015).</span></span></li>
<li><span class="koboSpan" id="kobo.681.1">Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. </span><em class="italic"><span class="koboSpan" id="kobo.682.1">Deep inside convolutional networks: Visualising image classification models and saliency maps</span></em><span class="koboSpan" id="kobo.683.1">. </span><span class="koboSpan" id="kobo.683.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">arXiv:1312.6034 (2013).</span></span></li>
<li><span class="koboSpan" id="kobo.685.1">Frosst, Nicholas, and Geoffrey Hinton. </span><em class="italic"><span class="koboSpan" id="kobo.686.1">Distilling a neural network into a soft decision tree</span></em><span class="koboSpan" id="kobo.687.1">. </span><span class="koboSpan" id="kobo.687.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">arXiv:1711.09784 (2017).</span></span></li>
<li><span class="koboSpan" id="kobo.689.1">Lundberg, Scott M., and Su-In Lee. </span><em class="italic"><span class="koboSpan" id="kobo.690.1">A unified approach to interpreting model predictions</span></em><span class="koboSpan" id="kobo.691.1">. </span><span class="koboSpan" id="kobo.691.2">Advances in neural information processing systems </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">30 (2017).</span></span></li>
<li><span class="koboSpan" id="kobo.693.1">Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. </span><em class="italic"><span class="koboSpan" id="kobo.694.1">“Why should I trust you?” </span><span class="koboSpan" id="kobo.694.2">Explaining the predictions of any classifier</span></em><span class="koboSpan" id="kobo.695.1">. </span><span class="koboSpan" id="kobo.695.2">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">mining. </span><span class="koboSpan" id="kobo.696.2">2016.</span></span></li>
<li><span class="koboSpan" id="kobo.697.1">Baniecki, Hubert, et al. </span><em class="italic"><span class="koboSpan" id="kobo.698.1">Dalex: responsible machine learning with interactive explainability and fairness in Python</span></em><span class="koboSpan" id="kobo.699.1">. </span><span class="koboSpan" id="kobo.699.2">The Journal of Machine Learning Research 22.1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">2021): 9759-9765.</span></span></li>
<li><span class="koboSpan" id="kobo.701.1">Yang, Wenzhuo, et al. </span><em class="italic"><span class="koboSpan" id="kobo.702.1">OmniXAI: A Library for Explainable AI</span></em><span class="koboSpan" id="kobo.703.1">. </span><span class="koboSpan" id="kobo.703.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">arXiv:2206.01612 (2022).</span></span></li>
<li><span class="koboSpan" id="kobo.705.1">Hima Lakkaraju, Julius Adebayo, Sameer Singh, </span><em class="italic"><span class="koboSpan" id="kobo.706.1">AAAI 2021 Tutorial on Explaining Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.707.1">Learning Predictions</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.709.1">Mothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. </span><em class="italic"><span class="koboSpan" id="kobo.710.1">Explaining machine learning classifiers through diverse counterfactual explanations</span></em><span class="koboSpan" id="kobo.711.1">. </span><span class="koboSpan" id="kobo.711.2">Proceedings of the 2020 conference on fairness, accountability, and </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">transparency. </span><span class="koboSpan" id="kobo.712.2">2020.</span></span></li>
</ul>
</div>
</body></html>