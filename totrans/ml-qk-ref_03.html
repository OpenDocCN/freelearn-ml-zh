<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performance in Ensemble Learning</h1>
                </header>
            
            <article>
                
<p>So far, we have learned that no two models will give the same result. In other words, different combinations of data or algorithms will result in a different outcome. This outcome can be good for a particular combination and not so good for another combination. What if we have a model that tries to take these combinations into account and comes up with a generalized and better result? This is called an <strong>ensemble model</strong>.</p>
<p>In this chapter, we will be learning about a number of concepts in regard to ensemble modeling, which are as follows:</p>
<ul>
<li>Bagging</li>
<li>Random forest</li>
<li>Boosting</li>
<li>Gradient boosting</li>
<li>Optimization of parameters </li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is ensemble learning?</h1>
                </header>
            
            <article>
                
<p>Sometimes, one machine learning model is not good enough for a certain scenario or use case as it might not give you the desired accuracy, recall, and precision. Hence, multiple learning models—or an ensemble of models captures the pattern of the data and gives better output.</p>
<p>As an example, let's say we are trying to decide on a place where we would like to go in the summer. Typically, if we are planning for a trip, the suggestions for the place pours in from all corners. That is, these suggestions might come from our family, websites, friends, and travel agencies, and then we have to decide on the basis of a good experience that we had in the past:</p>
<ul>
<li><strong>Family</strong>: Let's say that whenever we have consulted a family member and listened to them, there has been a 60% chance that they were proven right and we ended up having a good experience on the trip.</li>
<li><strong>Friends</strong>: Similarly, if we listen to our friends, they suggest places where we might have a good experience. In these instances, a good experience occurred in 50% of cases.</li>
<li><strong>Travel websites</strong>: Travel websites are another source where we can get loads of information regarding where to visit. If we choose to take their advice, there's a 35% chance that they were right and we had a good experience.</li>
<li><strong>Travel agencies</strong>: Another piece of advice and information might flow from travel agencies if we go and check with them first. Based on our past experiences, we saw that they were right in 45% of cases.</li>
</ul>
<p>However, we have to accumulate all of the preceding inputs and make a decision since no source has been 100% correct so far. If we combine these results, the accuracy scenario will be as follows:</p>
<pre>1 - (60% * 50% * 35% * 45%)                                                                                                                                                                              1- 0.04725 = 0.95275<br/><br/># Accuracy is close to 95%.</pre>
<p>From this, we are able to see the impact of ensemble modeling.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble methods </h1>
                </header>
            
            <article>
                
<p>Primarily, there are three methods of building an ensemble model, that is, <strong>Bagging</strong>, <strong>Boosting</strong>, and <strong>Stacking</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-837 image-border" src="assets/f2ec24a4-2d97-43aa-8d0a-71c5a58a59ff.png" style="width:93.08em;height:41.33em;"/></p>
<p>We will discuss each method one by one. However, before we get into this, we need to understand what bootstrapping is, which sets the basis for <strong>Bagging</strong> and <strong>Boosting</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bootstrapping</h1>
                </header>
            
            <article>
                
<p>Bootstrapping is a statistical technique that's used to draw an inference about the parameters of population based on the samples drawn from it with replacement and averaging these results out. In the event of sampling with replacement, samples are drawn one after another, and once one sample is drawn from the population, the population is replenished with the sampled data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-828 image-border" src="assets/bd6e63b8-5481-4f46-8fe6-c3ecc23016dd.png" style="width:67.67em;height:43.75em;"/></p>
<p>In the preceding diagram, there is a dataset that has got multiples components (<strong>A</strong>, <strong>B</strong>, <strong>C</strong>, <strong>D</strong>, <strong>E</strong>, <strong>F</strong>, <strong>G</strong>, <strong>H</strong>, and <strong>I</strong>). To start, we need to draw three samples of the same size. Let's draw <strong>Sample 1</strong> randomly and say that the first element turned out to be <strong>A</strong>. However, before we draw the second element of <strong>Sample 1</strong>, <strong>A</strong> is returned to the dataset. A similar process takes place for the entire draw. This is called <strong>Sampling with Replacement</strong>. Hence, we have a chance of selecting the same item multiple times in a set. By following this process, we have drawn three samples, that is, <strong>Sample 1</strong>, <strong>Sample 2</strong>, and <strong>Sample 3</strong>.</p>
<p class="mce-root"/>
<p>When we take a step further down, which is determining the statistics (various metrics) on <strong>Sample 1</strong>, <strong>Sample 2</strong>, and <strong>Sample 3</strong>, we find out a mean or an average of all the statistics to infer something about the dataset (population). This entire process is called <strong>bootstrapping</strong> and the drawn samples are termed bootstrapped samples. This can be defined with the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><em>Inference about the Dataset(Population) = Average(sample 1,sample 2,............,sample N)</em></p>
<p>If you look at the preceding diagram carefully, there might be a scenario wherein a few elements of the dataset haven't been picked or are not part of those three samples:</p>
<ul>
<li><strong>Sample 1</strong>: (<strong>A</strong>, <strong>E</strong>, <strong>H</strong>, <strong>C</strong>)</li>
<li><strong>Sample 2</strong>: (<strong>F</strong>, <strong>G</strong>, <strong>A</strong>, <strong>C</strong>)</li>
<li><strong>Sample 3</strong>: (<strong>E</strong>, <strong>H</strong>, <strong>G</strong>, <strong>F</strong>)</li>
</ul>
<p>Therefore, the elements that haven't been picked are <strong>B</strong>, <strong>D</strong>, and <strong>I</strong>. The samples that were not part of the drawn samples are called <strong>out-of-bag</strong> (<strong><span>OOB</span></strong>) samples.</p>
<p>Let's do a simple coding exercise to see how this can be done in Python:</p>
<ol>
<li>Here, we will be using the <kbd>sklearn</kbd> and <kbd>resample</kbd> functions. Let's import the necessary libraries:</li>
</ol>
<pre style="padding-left: 60px">#importing Libraries<br/>from sklearn.utils import resample</pre>
<ol start="2">
<li>Next, create a dataset that we will need to sample:</li>
</ol>
<pre style="padding-left: 60px">dataset=[10,2</pre>
<ol start="3" style="font-size: 16px">
<li>Now, we will extract a bootstrap sample with the help of the <kbd>resample</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">0,30,40,50,60,70,80,90,100]</pre>
<pre style="padding-left: 60px">#using "resample" function generate a bootstrap sample<br/>boot_samp = resample(dataset, replace=True, n_samples=5, random_state=1)</pre>
<ol start="4">
<li>We will use list comprehension to extract an OOB sample:</li>
</ol>
<pre style="padding-left: 60px">#extracting OOB sample<br/>OOB=[x for x in dataset if x not in boot_samp]</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Now, let's print it:</li>
</ol>
<pre style="padding-left: 60px">print(boot_samp)</pre>
<p style="padding-left: 60px">We get the following output:</p>
<pre style="padding-left: 60px">[60, 90, 100, 60, 10]</pre>
<p style="padding-left: 60px">We can see that there is a repetition of 60 in the sampling. This is due to sampling with replacement.</p>
<ol start="6">
<li>Next, we need to print the following code:</li>
</ol>
<pre style="padding-left: 60px">print(OOB)</pre>
<p style="padding-left: 60px">We get the following output:</p>
<pre style="padding-left: 60px">[20, 30, 40, 50, 70, 80]</pre>
<p>By this end of this, we want to have a result that's as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>OOB = Dataset - Boot_Sample </em></p>
<p class="CDPAlignCenter CDPAlign"><em>=[10,20,30,40,50,60,70,80,90,100] - [60,90,100,60,10]</em></p>
<p class="CDPAlignCenter CDPAlign"><em>=[20,30,40,50,70,80]</em></p>
<p>This is the same result we have got from the code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bagging</h1>
                </header>
            
            <article>
                
<p>Bagging stands for <span>bootstrap</span><span> </span><strong>aggregation</strong><span>. Hence, it's clear that the bagging</span> concept <span>stems from bootstrapping. It implies that bagging has got the elements of bootstrapping. It is a bootstrap ensemble method wherein multiple classifiers (typically from the same algorithm) are trained on the samples that are drawn randomly with replacements (bootstrap samples) from the training set/population. Aggregation of all the classifiers takes place in the form of average or by voting. It tries to reduce the affect of the overfitting issue in the model as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-816 image-border" src="assets/a942be0b-22b5-45c0-9fbf-007ec88a7e7e.png" style="width:42.67em;height:39.92em;"/></p>
<p>There are three stages of bagging:</p>
<ul>
<li><strong>Bootstrapping</strong>:<strong> </strong><span>This is a statistical technique that's used to generate random samples or bootstrap samples with replacement.</span></li>
<li><strong>Model fitting</strong>:<strong> </strong><span>In this stage, we build models on bootstrap samples. Typically, the same algorithm is used for building the models. However, there is no restriction on using different algorithms.</span></li>
<li><strong>Combining models</strong>:<strong> </strong><span>This step involves combining all the models and taking an average. For example, if we have applied a decision tree classifier, then the probability that's coming out of every classifier is averaged.</span></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree</h1>
                </header>
            
            <article>
                
<p>A decision tree is a supervised learning technique that works on the divide-and-conquer approach. It can be used to address both classification and regression. The population undergoes a split into two or more homogeneous samples based on the most significant feature.</p>
<p>For example, let's say we have got a sample of people who applied for a loan from the bank. For this example, we will take the count as 50. Here, we have got three attributes, that is, gender, income, and the number of other loans held by the person, to predict whether to give them a loan or not.</p>
<p>We need to segment the people based on gender, income, and the number of other loans they hold and find out the most significant factor. This tends to create the most homogeneous set.</p>
<p>Let's take income first and try to create the segment based on it. The total number of people who applied for the loan is 50. Out of 50, the loan was awarded to 20 people. However, if we break this up by income, we can see that the breakup has been done by income &lt;100,000 and &gt;=100,000. This doesn't generate a homogeneous group. We can see that 40% of applicants (20) have been given a loan. Of the people whose income was less than 100,000, 30% of them managed to get the loan. Similarly, 46.67 % of people whose income was greater than or equal to 100,000 managed to get the loan. The following diagram shows the tree splitting on the basis of income:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-826 image-border" src="assets/92c46e1e-2cb7-4b7e-8f24-a15ef8a9a299.png" style="width:31.58em;height:19.75em;"/></p>
<p>Let's take up the number of loans now. Even this time around, we are not able to see the creation of a homogeneous group. The following diagram shows the tree splitting on the basis of the number of loans:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-825 image-border" src="assets/6d36994c-4d82-4617-a5d3-4e8c4f27e2fc.png" style="width:30.92em;height:19.92em;"/></p>
<p>Let's get on with gender and see how it fares in terms of creating a homogeneous group. This turns out to be the homogeneous group. There were 15 who were female, out of which 53.3% got the loan. 34.3% of male also ended up getting the loan. The following diagram shows the tree splitting based on gender:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-824 image-border" src="assets/48b51511-931c-4a3f-a32a-9be7d3edf359.png" style="width:30.83em;height:21.00em;"/></p>
<p>With the help of this, the most significant variable has been found. Now, we will dwell on how significant the variables are.</p>
<p>Before we do that, it's imperative for us to understand the terminology and nomenclature associated with the decision tree:</p>
<ul>
<li><strong>Root Node</strong>: This stands for the whole population or dataset that undergoes a split into two or more homogeneous groups</li>
<li><strong>Decision Node</strong>: This is created when a node is divided into further subnodes</li>
<li><strong>Leaf Node</strong>: When there is no possibility of nodes splitting any further, that node is termed a leaf node or terminal node</li>
<li><strong>Branch</strong>: A subsection of the entire tree is called a <strong>branch</strong> or a <strong>Sub-tree</strong>:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-832 image-border" src="assets/83181071-c958-46b3-86ae-372d6b81f921.png" style="width:41.67em;height:23.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tree splitting</h1>
                </header>
            
            <article>
                
<p>There are various algorithms that help when it comes to tree splitting, all of which take us to the leaf node. The decision tree takes all of the features (variables) that are available into account and selects the feature that would result in the most pure or most homogeneous split. The algorithm that's used to split the tree also depends on the target variable. Let's go through this, step by step:</p>
<ol>
<li><strong>Gini index</strong>: This says that if we select two items at random from a population, they must be from the same class. The probability for this event would turn out to be 1 if the population is totally pure. It only performs binary splits. <strong>Classification and regression trees</strong> (<strong>CARTs</strong>) make use of this kind of split.</li>
</ol>
<p style="padding-left: 60px">The following formula is how you calculate the Gini index:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/adcdc6ab-e709-4f65-a616-2d00813d3e13.png" style="width:5.75em;height:4.17em;"/> </p>
<p style="padding-left: 60px">Here, <em>p(t)</em> is the p<span>roportion of observations with a target variable with a value of <em>t.</em></span></p>
<p class="graf graf--p graf-after--p" style="padding-left: 60px">For the binary target variable, <em>t=1</em>, the max Gini index value is as follows:</p>
<p class="graf graf--p graf-after--p CDPAlignCenter CDPAlign"><em>= 1 — (1/2)^2— (1/2)^2</em><br/>
<em>= 1–2*(1/2)^2</em><br/>
<em>= 1- 2*(1/4)</em><br/>
<em>= 1–0.5</em><br/>
<em>= 0.5</em></p>
<p style="padding-left: 60px"><span>A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups that were created the by the split. A perfect separation results in a Gini score of 0, whereas the worst case split results in 50/50 classes.</span></p>
<p style="padding-left: 60px"><span>For a nominal vari</span><span>able with <em>k</em> level, the maximum value of the Gini index is <em>(1- 1/k).</em></span></p>
<ol start="2">
<li><strong>Information gain</strong>: Let's delve into this and find out what it is. If we happened to have three scenarios, as shown in the following diagram, which can be described easily? </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-820 image-border" src="assets/2ecceb2f-c502-4b47-9276-be973ecc1baf.png" style="width:34.42em;height:10.08em;"/></p>
<p style="padding-left: 60px">Since <strong>Z</strong> seem to be quite homogeneous and all of the values of it are similar, it is called a <strong>pure set</strong>. Hence, it requires less effort to explain it. However, <strong>Y</strong> would need more information to explain as it's not pure. <strong>X</strong> turns out to be the impurest of them all. What it tries to convey is that randomness and disorganization adds to complexity and so it needs more information to explain. This degree of randomness is known as <strong>entropy</strong>. If the sample is completely homogeneous, then the entropy is <em>0</em>. If the sample is equally divided, its entropy will be <em>1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>Entropy = -p log<sub>2</sub>p - q log<sub>2</sub>q</em></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Here, <em>p</em> means<span> the probability of success and </span><em>q</em> means the<span> probability of failure.</span></p>
<p style="padding-left: 60px">Entropy is also used with a categorical target variable. It picks the split that has the lowest entropy compared to the parent node.</p>
<p style="padding-left: 60px">Here, we must calculate the entropy of parent node first. Then, we need to calculate entropy of each individual node that's been split and post that, including the weighted average of all subnodes.</p>
<ol start="3">
<li><strong>Reduction in variance</strong>: When it comes to the continuous target variable, reduction in variance is used. Here, we are using variance to decide the best split. The split with the lowest variance is picked as the criteria to split:</li>
</ol>
<p class="CDPAlignCenter CDPAlign">Variance = <img class="fm-editor-equation" src="assets/b83a3621-9aeb-480a-9a06-8df93b13d3d0.png" style="width:4.67em;height:1.08em;"/></p>
<p style="padding-left: 60px">Here, <img class="fm-editor-equation" src="assets/41238de0-7910-4753-b06d-e0aa03b19ed2.png" style="font-size: 1em;text-align: center;width:1.00em;height:1.08em;"/><span> is the mean of all the values, </span><em>X,</em> is the real values, and <em>n</em> is the number of values.</p>
<p>The calculation of variance for each node is done first and then the weighted average of each node's variance makes us select the best node.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameters of tree splitting</h1>
                </header>
            
            <article>
                
<p>There are a number of parameters that we need to tune or be aware of:</p>
<ul>
<li><kbd>Max_depth</kbd>: One of the most important parameters is <kbd>max_depth</kbd>. It captures the essence of how deep the tree can get. More depth in the tree means that it is able to extract more information from the features. However, sometimes, excessive depth might be a cause of worry as it tends to bring along overfitting as well.</li>
<li><kbd>min_samples_split</kbd>: This represents the minimum number of samples required to split an internal node. <span>This can vary between considering at least one sample at each node to considering all of the samples at each node. When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node. An increase in the value of <kbd>min_samples_split</kbd> tends to be underfitted.</span></li>
<li><kbd>min_samples_leaf</kbd>: This is the minimum number of samples required to be at a leaf node. Increasing this value to the maximum might cause underfitting.</li>
<li><kbd>max_features</kbd>: This is maximum number of features to be considered for the best split. It might cause overfitting when there is an increase in the max number of features.</li>
</ul>
<p>Now, we are well equipped to understand the random forest algorithm. We're going to talk about that next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest algorithm</h1>
                </header>
            
            <article>
                
<p>The random forest algorithm works with the bagging technique. The number of trees are planted and grown in the following manner:</p>
<ul>
<li>There are <em>N </em>observations in the training set. Samples out of <em>N</em> observations are taken at random and with replacement. These samples will act as a training set for different trees.</li>
<li>If there are <em>M</em> input features (variables), <em>m</em> features are drawn as a subset out of <em>M</em> and of course <em>m &lt; M</em>. What this does is select <em>m</em> features at random at each node of the tree.</li>
<li>Every tree is grown to the largest extent possible.</li>
</ul>
<p class="mce-root"/>
<ul>
<li>Prediction takes place based on the aggregation of the results coming out of all the trees. In the case of classification, the method of aggregation is voting, whereas it is an average of all the results in the case of regression:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-834 image-border" src="assets/09fecda6-8be8-4b47-9b9c-51591cbcf909.png" style="width:31.75em;height:28.75em;"/></p>
<p>Let's work on a case study, since that will help us understand this concept more in detail. Let's work on breast cancer data.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study</h1>
                </header>
            
            <article>
                
<p><span>The data that is given in this case study is about patients who were detected with two kinds of breast cancer:</span></p>
<ul>
<li><span>Malignant</span></li>
<li><span>Benign</span></li>
</ul>
<p><span>A number of features are given here that have characteristics in regard to the cell nuclei that have been computed from the <strong>fine-needle aspiration</strong> (<strong>FNA</strong>) of a breast mass. Based on these features, we need to predict whether the cancer is malignant or b</span><span>enign. Follow these steps to get started:</span></p>
<ol>
<li>Import all the required libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from sklearn import preprocessing<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix<br/>#importing our parameter tuning dependencies<br/>from sklearn.model_selection import (cross_val_score, GridSearchCV,StratifiedKFold, ShuffleSplit )<br/>#importing our dependencies for Feature Selection<br/>from sklearn.feature_selection import (SelectKBest, RFE, RFECV)<br/>from sklearn.ensemble import ExtraTreesClassifier<br/>from sklearn.cross_validation import ShuffleSplit<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import f1_score<br/>from collections import defaultdict<br/># Importing our sklearn dependencies for the modeling<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.cross_validation import KFold<br/>from sklearn import metrics<br/>from sklearn.metrics import (accuracy_score, confusion_matrix, <br/> classification_report, roc_curve, auc)</pre>
<ol start="2">
<li>Load the breast cancer data:</li>
</ol>
<pre style="padding-left: 60px">data= pd.read_csv("breastcancer.csv")</pre>
<ol start="3">
<li>Let's understand the data:</li>
</ol>
<pre style="padding-left: 60px">data.info()</pre>
<p style="padding-left: 60px">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-815 image-border" src="assets/f9e6557f-a0cb-4eaa-9312-442ffb09c069.png" style="width:26.67em;height:44.58em;"/></p>
<ol start="4">
<li>Let's consider <kbd>data.head()</kbd> here:</li>
</ol>
<pre style="padding-left: 60px">data.head()</pre>
<p style="padding-left: 60px">From this, we get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="assets/819947a5-3c40-47d5-8342-71e05f58cd5f.png" style="width:104.58em;height:19.83em;"/></p>
<ol start="5">
<li>We get the data diagnosis from the following code:</li>
</ol>
<pre style="padding-left: 60px">data.diagnosis.unique()</pre>
<p style="padding-left: 60px">The following is the output for the preceding code: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e288080-bf9d-41ea-b5bc-b43963d466ed.png" style="width:20.58em;height:2.17em;"/></p>
<ol start="6">
<li>The data is described as follows:</li>
</ol>
<pre style="padding-left: 60px">data.describe()</pre>
<p style="padding-left: 60px">We get this output from the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-830 image-border" src="assets/b619a949-107e-455d-840b-3027dbbf095e.png" style="width:99.25em;height:28.33em;"/></p>
<pre>data['diagnosis'] = data['diagnosis'].map({'M':1,'B':0})<br/><br/>datas = pd.DataFrame(preprocessing.scale(data.iloc[:,1:32]))<br/>datas.columns = list(data.iloc[:,1:32].columns)<br/>datas['diagnosis'] = data['diagnosis']<br/><br/>datas.diagnosis.value_counts().plot(kind='bar', alpha = 0.5, facecolor = 'b', figsize=(12,6))<br/>plt.title("Diagnosis (M=1, B=0)", fontsize = '18')<br/>plt.ylabel("Total Number of Patients")<br/>plt.grid(b=True)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-823 image-border" src="assets/4bd0bc77-162c-441f-b7cd-428b83e5e6f4.png" style="width:76.17em;height:39.25em;"/></p>
<pre>data_mean = data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']]<br/><br/>plt.figure(figsize=(10,10))<br/>foo = sns.heatmap(data_mean.corr(), vmax=1, square=True, annot=True)</pre>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-817 image-border" src="assets/83f6dc50-a460-45af-a92d-2b8ffc6995c3.png" style="width:44.42em;height:41.83em;"/></p>
<pre>from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict<br/>from sklearn import metrics<br/>predictors = data_mean.columns[2:11]<br/>target = "diagnosis"<br/>X = data_mean.loc[:,predictors]<br/>y = np.ravel(data.loc[:,[target]])<br/># Split the dataset in train and test:<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)<br/>print ('Shape of training set : %i &amp; Shape of test set : %i' % (X_train.shape[0],X_test.shape[0]) )<br/>print ('There are very few data points so 10-fold cross validation should give us a better estimate')</pre>
<p>The preceding input gives us the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-842 image-border" src="assets/6d121f10-24e4-49d3-8832-e24d16f80548.png" style="width:55.42em;height:3.25em;"/></p>
<pre>param_grid = {<br/> 'n_estimators': [ 25, 50, 100, 150, 300, 500], <br/> "max_depth": [ 5, 8, 15, 25],<br/> "max_features": ['auto', 'sqrt', 'log2'] <br/> } <br/>#use OOB samples ("oob_score= True") to estimate the generalization accuracy.<br/>rfc = RandomForestClassifier(bootstrap= True, n_jobs= 1, oob_score= True)<br/>#let's use cv=10 in the GridSearchCV call<br/>#performance estimation<br/>#initiate the grid <br/>grid = GridSearchCV(rfc, param_grid = param_grid, cv=10, scoring ='accuracy')<br/>#fit your data before you can get the best parameter combination.<br/>grid.fit(X,y)<br/>grid.cv_results_</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1004 image-border" src="assets/f63c06af-363d-4a1b-9699-bd534ae987c2.png" style="width:73.42em;height:27.92em;"/></p>
<pre># Let's find out the best scores, parameter and the estimator from the gridsearchCV<br/>print("GridSearhCV best model:\n ")<br/>print('The best score: ', grid.best_score_)<br/>print('The best parameter:', grid.best_params_)<br/>print('The best model estimator:', grid.best_estimator_)</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-838 image-border" src="assets/a1c90824-2c4a-4261-85a7-1fc5076b3613.png" style="width:81.75em;height:17.92em;"/></p>
<pre># model = RandomForestClassifier() with optimal values<br/>model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',<br/> max_depth=8, max_features='sqrt', max_leaf_nodes=None,<br/> min_impurity_decrease=0.0, min_impurity_split=None,<br/> min_samples_leaf=1, min_samples_split=2,<br/> min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,<br/> oob_score=True, random_state=None, verbose=0, warm_start=False)<br/>model.fit(X_train, y_train)</pre>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d7aa01a1-4ef0-4fe8-8c92-90eed38bba16.png"/></p>
<pre>print("Performance Accuracy on the Testing data:", round(model.score(X_test, y_test) *100))</pre>
<p>From this, we can see that the performance accuracy on the testing data is <kbd>95.0</kbd>:</p>
<pre>#Getting the predictions for X<br/>y_pred = model.predict(X_test)<br/>print('Total Predictions {}'.format(len(y_pred)))</pre>
<p>Here, the total predictions is <kbd>114</kbd>:</p>
<pre>truth = pd.DataFrame(y_test, columns= ['Truth'])<br/>predictions = pd.DataFrame(y_pred, columns= ['Predictions'])<br/>frames = [truth, predictions]<br/>_result = pd.concat(frames, axis=1)<br/>print(_result.shape)<br/>_result.head()</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-827 image-border" src="assets/e256f6d9-d551-4096-a773-babd33156f3a.png" style="width:8.50em;height:11.17em;"/></p>
<pre># 10 fold cross-validation with a Tree classifier on the training dataset# 10 fold <br/>#splitting the data, fitting a model and computing the score 10 consecutive times<br/>cv_scores = []<br/>scores = cross_val_score(rfc, X_train, y_train, cv=10, scoring='accuracy')<br/>cv_scores.append(scores.mean())<br/>cv_scores.append(scores.std())<br/><br/>#cross validation mean score<br/>print("10 k-fold cross validation mean score: ", scores.mean() *100)</pre>
<p>From this, we can see that the 10 k-fold cross validation mean score is <kbd>94.9661835749</kbd>:</p>
<pre># printing classification accuracy score rounded<br/>print("Classification accuracy: ", round(accuracy_score(y_test, y_pred, normalize=True) * 100))</pre>
<p>Here, we can see that the classification accuracy is <kbd>95.0</kbd>:</p>
<pre># Making the Confusion Matrix<br/>cm = confusion_matrix(y_test, y_pred) <br/>plt.figure(figsize=(12,6))<br/>ax = plt.axes()<br/>ax.set_title('Confusion Matrix for both classes\n', size=21)<br/>sns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap<br/>plt.show()</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="assets/df04808f-0b0b-4ae4-ae11-5eaaea1d3c6f.png" style="width:35.58em;height:22.42em;"/></p>
<pre># The classification Report<br/>target_names = ['Benign [Class 0]', 'Malignant[Class 1]']<br/>print(classification_report(y_test, y_pred, target_names=target_names))</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-822 image-border" src="assets/ab332a99-7d5f-4df2-9ed3-4d8a540728df.png" style="width:30.58em;height:7.17em;"/></p>
<pre>y_pred_proba = model.predict_proba(X_test)[::,1]<br/>fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)<br/>auc = metrics.roc_auc_score(y_test, y_pred_proba)<br/>plt.plot(fpr,tpr,label="curve, auc="+str(auc))<br/>plt.legend(loc=4)<br/>plt.show()</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-831 image-border" src="assets/959ed7fc-e995-4594-8772-215c3369f34e.png" style="width:26.83em;height:18.17em;"/></p>
<p>The preceding graph is a <strong>receiver operating characteristic</strong> (<strong>ROC</strong>) metric, which is used to evaluate classifier output quality using cross-validation.</p>
<p>The preceding plot shows the ROC response to our chosen features (<kbd>['compactness_mean', 'perimeter_mean', 'radius_mean', 'texture_mean', 'concavity_mean', 'smoothness_mean']</kbd>) and the diagnosis-dependent variable that was created from k-fold cross-validation.</p>
<p>A ROC area of <kbd>0.99</kbd> is quite good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boosting</h1>
                </header>
            
            <article>
                
<p>When it comes to bagging, it can be applied to both classification and regression. However, there is another technique that is also part of the ensemble family: boosting. However, the underlying principle of these two are quite different. In bagging, each of the models runs independently and then the results are aggregated at the end. This is a parallel operation. Boosting acts in a different way, since it flows sequentially. Each model here runs and passes on the significant features to another model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-835 image-border" src="assets/d4f677a3-842b-47dc-be84-527d26d94f49.png" style="width:39.75em;height:15.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient boosting</h1>
                </header>
            
            <article>
                
<p>To explain gradient boosting, we will take the route of Ben Gorman, a great data scientist. He has been able to explain it in a mathematical yet simple way. Let's say that we have got nine training examples wherein we are required to predict the age of a person based on three features, such as whether they like gardening, playing video games, or surfing the internet. The data for this is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-821 image-border" src="assets/0bf3e5de-9aff-46ba-aaf6-830f589b6200.png" style="width:29.83em;height:13.25em;"/></p>
<p>To build this model, the objective is to minimize the mean squared error.</p>
<p>Now, we will build the model with a regression tree. To start with, if we want to have at least three samples at the training nodes, the first split of the tree might look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-818 image-border" src="assets/928c8ffa-6ef3-4e38-8c74-2e1a1fbde65f.png" style="width:23.25em;height:18.00em;"/></p>
<p>This seems to be fine, but it's not including information such as whether they play video games or browse the internet. What if we plan to have two samples at the training nodes?</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-829 image-border" src="assets/a16df436-cb99-466d-a364-c33c32f41c14.png" style="width:40.75em;height:17.00em;"/></p>
<p>Through the preceding tree, we are able to get certain information from features, such as <strong>SurfInternet</strong> and <strong>PlaysVideoGames</strong>. Let's figure out how residuals/errors come along:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1006 image-border" src="assets/53f43655-8f92-48e9-9912-97466e94b504.png" style="width:23.83em;height:14.17em;"/></p>
<p class="CDPAlignCenter CDPAlign"/>
<p>Now, we will work on the residuals of the first model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-840 image-border" src="assets/aa17a869-d555-461e-be73-e76c2dc09eaf.png" style="width:22.75em;height:20.92em;"/></p>
<p class="mce-root"/>
<p>Once we have built the model on residuals, we have to combine the previous model with the current one, as shown in the following table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1008 image-border" src="assets/948cc81a-328c-4744-9dde-1a5ba19ab78f.png" style="width:45.75em;height:15.00em;"/></p>
<p>We can see that the residuals have come down and that the model is getting better.</p>
<p>Let's try to formulate what we have done up until this point:</p>
<ol>
<li>First, we built a model on the data <em>f<sub>1</sub>(x) = y.</em></li>
<li>The next thing we did was calculate the residuals and build the model on residuals: </li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em>h <sub>1</sub>(x)=y- <span>f</span><sub>1</sub><span>(x)</span></em></p>
<ol start="3">
<li>The next step is to combine the model, that is, <em>f<sub>2</sub>(x)= f<sub>1</sub>(x) + <span>h </span><sub>1</sub><span>(x)</span>.</em></li>
</ol>
<p style="padding-left: 60px">Adding more models can correct the errors of the previous models. The preceding equation will turn out to be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>f<span>3</span>(x)= f<span>2</span>(x) + h<span>2</span>(x) </em></p>
<p style="padding-left: 60px">The equation will finally look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>f<sub>m</sub>(x)= f<sub>m</sub><sub>-1</sub>(x) + h<sub>m</sub><sub>-1</sub>(x)</em></p>
<p style="padding-left: 60px">Alternatively, we can write the following:</p>
<p class="CDPAlignCenter CDPAlign"><em> h<sub>m</sub>(x)= y- f<sub>m</sub>(x)</em></p>
<p class="mce-root"/>
<ol start="4">
<li>Since our task is to minimize the squared error, <em>f</em> will be initialized with the mean of the training target values:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fa749feb-9490-4a4c-9790-fac085bc18f0.png" style="width:31.33em;height:3.67em;"/></p>
<ol start="5">
<li>Then, we can find out <em>f</em><sub><em>m+1</em>, </sub>just like before:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><em><span>f</span><sub>m</sub><span>(x)= f</span><sub>m</sub><sub>-1</sub><span>(x) + h</span><sub>m</sub><sub>-1</sub><span>(x)</span></em></p>
<p>Now, we can use gradient descent for our gradient boosting model. The objective function we want to minimize is<span> <em>L</em></span>. Our starting point is<span> <em>f<sub>o</sub>(x)</em></span>. For iteration <em>m=1</em>, we compute the gradient of<span><em> L</em> </span>with respect to<em><span> f<sub>o</sub>(x)</span></em>. Then, we fit a weak learner to the gradient components. In the case of a regression tree, leaf nodes produce an<span> </span><strong>average gradient</strong><span> </span>among samples with similar features. For each leaf, we step in the direction of the average gradient. The result is <em>f<sub>1</sub></em><span><em> </em>and this</span> can be repeated until we have <em>f<sub>m</sub></em>.</p>
<p>We modified our gradient boosting algorithm so that it works with any differentiable loss function. Let's clean up the preceding ideas and reformulate our gradient boosting model once again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameters of gradient boosting</h1>
                </header>
            
            <article>
                
<p>There are different parameters to consider before <span>applying gradient boosting for the breast cancer use case:</span></p>
<ul>
<li><kbd>Min_samples_split</kbd>: T<span>he minimum number of samples required in a node to be considered for splitting is termed <kbd>min_samples_split</kbd>.</span></li>
<li><kbd>Min_samples_leaf</kbd>: The minimum number of samples required at the terminal or leaf node is termed <kbd>min_samples_leaf</kbd>.</li>
<li><kbd>Max_depth</kbd>: This <span>is the maximum number of nodes allowed from the root to the farthest leaf of a tree. Deeper trees can model more complex relationships, however, causing the model to overfit.</span></li>
<li><kbd>Max_leaf_nodes</kbd>: The maximum number of nodes at the leaves in a tree. Since binary trees are created, a depth of <kbd>n</kbd> would produce a maximum of <em>2<sup>n </sup></em> leaves. Hence, either <kbd>max_depth</kbd> or <kbd>max_leaf_nodes</kbd> can be defined.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we will apply gradient boosting for the breast cancer use case. Here, we are loading the libraries that are required to build the model:</p>
<pre>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc</pre>
<p>We are now done with the various steps of data cleaning and exploration while performing random forest. Now, we will jump right into building the model.</p>
<p>Here, we will perform a grid search to find out the optimal parameters for the gradient boosting algorithm:</p>
<pre>param_grid = {<br/> 'n_estimators': [ 25, 50, 100, 150, 300, 500], # the more parameters, the more computational expensive<br/> "max_depth": [ 5, 8, 15, 25],<br/> "max_features": ['auto', 'sqrt', 'log2'] <br/> }<br/>gbm = GradientBoostingClassifier(learning_rate=0.1,random_state=10,subsample=0.8)<br/>#performance estimation<br/>#initiate the grid <br/>grid = GridSearchCV(gbm, param_grid = param_grid, cv=10, scoring ='accuracy')<br/>#fit your data before you can get the best parameter combination.<br/>grid.fit(X,y)<br/>grid.cv_results_     </pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1009 image-border" src="assets/f0781efe-1af7-400d-8397-3a9e5d860876.png" style="width:75.83em;height:31.67em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, let's find out the optimal parameters:</p>
<pre>#Let's find out the best scores, parameter and the estimator from the gridsearchCV<br/>print("GridSearhCV best model:\n ")<br/>print('The best score: ', grid.best_score_)<br/>print('The best parameter:', grid.best_params_)<br/>print('The best model estimator:', grid.best_estimator_)</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-833 image-border" src="assets/588de5cb-de5b-4e24-ba89-04eb0c654304.png" style="width:73.58em;height:21.33em;"/></p>
<p>Now, we will build the model:</p>
<pre>model2 = GradientBoostingClassifier(criterion='friedman_mse', init=None,<br/> learning_rate=0.1, loss='deviance', max_depth=5,<br/> max_features='sqrt', max_leaf_nodes=None,<br/> min_impurity_decrease=0.0, min_impurity_split=None,<br/> min_samples_leaf=1, min_samples_split=2,<br/> min_weight_fraction_leaf=0.0, n_estimators=150,<br/> presort='auto', random_state=10, subsample=0.8, verbose=0,<br/> warm_start=False)<br/>model2.fit(X_train, y_train) <br/><br/>print("Performance Accuracy on the Testing data:", round(model2.score(X_test, y_test) *100))</pre>
<p>The performance accuracy on the testing data is <kbd>96.0</kbd>:</p>
<pre>#getting the predictions for X<br/>y_pred2 = model2.predict(X_test)<br/>print('Total Predictions {}'.format(len(y_pred2)))</pre>
<p>The total number of predictions is <kbd>114</kbd>:</p>
<pre>truth = pd.DataFrame(y_test, columns= ['Truth'])<br/>predictions = pd.DataFrame(y_pred, columns= ['Predictions'])<br/>frames = [truth, predictions]<br/>_result = pd.concat(frames, axis=1)<br/>print(_result.shape)<br/>_result.head()</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-836 image-border" src="assets/ee427abe-70f1-4061-95f2-045ab887cb9e.png" style="width:9.33em;height:12.42em;"/></p>
<p>Let's perform cross-validation:</p>
<pre>cv_scores = []<br/><br/>scores2 = cross_val_score(gbm, X_train, y_train, cv=10, scoring='accuracy')<br/>cv_scores.append(scores2.mean())<br/>cv_scores.append(scores2.std())<br/><br/>#cross validation mean score <br/>print("10 k-fold cross validation mean score: ", scores2.mean() *100)</pre>
<p>The 10 k-fold cross-validation mean score is <kbd>94.9420289855</kbd>:</p>
<pre>#printing classification accuracy score rounded<br/>print("Classification accuracy: ", round(accuracy_score(y_test, y_pred2, normalize=True) * 100))</pre>
<p>The classification accuracy is <kbd>96.0</kbd>:</p>
<pre># Making the Confusion Matrix<br/>cm = confusion_matrix(y_test, y_pred2)<br/>plt.figure(figsize=(12,6))<br/>ax = plt.axes()<br/>ax.set_title('Confusion Matrix for both classes\n', size=21)<br/>sns.heatmap(cm, cmap= 'plasma',annot=True, fmt='g') # cmap<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>By looking at the confusion matrix, we can see that this model is better than the previous one:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3bc63967-fbe6-41e9-b577-ad52bb8bcdf9.png" style="width:42.58em;height:24.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we studied ensemble learning and its different methods, namely bagging, boosting, and stacking. We even saw what is bootstrapping which is the root for ensemble learning methods such as bagging and boosting. We also learned about decision trees and its approach of divide and rule with example of people applying for loan. Then we covered tree splitting and the parameters to split a decision tree, moving on to the random forest algorithm. We worked on a case study of breast cancer using the concepts covered. We also discovered the difference between bagging and boosting and gradient boosting. We also discussed on parameters of gradient boosting to use it our example of breast cancer.</p>
<p>In the next chapter, we will learn about training neural networks.</p>


            </article>

            
        </section>
    </body></html>