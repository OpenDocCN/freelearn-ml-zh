<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods"><div class="book" id="23MNU2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Ensemble Methods</h1></div></div></div><p class="calibre8">In this chapter, we take a step back from learning new models and instead think about how several trained models can work together as an ensemble, in order to produce a single model that is more powerful than the involved models, individually.</p><p class="calibre8">The first type of ensemble that we will study uses different samples of the same dataset in order to train multiple versions of the same model. These models then vote on the correct answer for a new observation and an average or majority decision is made, depending on the type of problem. This process is known as bagging, which is short for bootstrap aggregation. Another approach to combine models is boosting. This essentially involves training a chain of models and assigning weights to observations that were incorrectly classified or fell far from their predicted value so that successive models are forced to prioritize them.</p><p class="calibre8">As methods, bagging and boosting are fairly general and have been applied with a number of different types of models. Decision trees, studied in <a class="calibre1" title="Chapter 6. Support Vector Machines" href="part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7">Chapter 6</a>, <span class="strong"><em class="calibre9">Tree-Based Methods</em></span>, are particularly suited to ensemble methods. So much so, that a particular type of tree-based ensemble model has its own name--the random forest. Random forests offer significant improvements over the single decision tree and are generally considered to be very powerful and flexible models, as we shall soon discover. In this chapter, we'll revisit some of the datasets we analyzed in previous chapters and see if we can improve performance by applying some of the principles we learn here.</p></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods">
<div class="book" title="Bagging"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec64" class="calibre1"/>Bagging</h1></div></div></div><p class="calibre8">The focus <a id="id667" class="calibre1"/>of this chapter is on combining the results from different models in order to produce a single model that will outperform individual models on their own. <span class="strong"><strong class="calibre2">Bagging</strong></span> is essentially an intuitive procedure for combining multiple models trained on the same dataset, by using majority voting for classification models and average value for regression models. We'll present this procedure for the classification case, and later show how this is easily extended to handle regression models.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note33" class="calibre1"/>Note</h3><p class="calibre8">
<span class="strong"><strong class="calibre2">Bagging procedure for binary classification</strong></span>
</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Inputs</strong></span>:</p><p class="calibre8">
<span class="strong"><em class="calibre9">data</em></span>: The input data frame containing the input features and a column with the binary output label.</p><p class="calibre8">
<span class="strong"><em class="calibre9">M</em></span>: An integer, representing the number of models that we want to train.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Output</strong></span>:</p><p class="calibre8">
<span class="strong"><em class="calibre9">models</em></span>: A set of Μ trained binary classifier models.</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Method</strong></span>:</p><p class="calibre8">1. Create a random sample of size <span class="strong"><em class="calibre9">n</em></span>, where <span class="strong"><em class="calibre9">n</em></span> is the number of observations in the original dataset, with replacement. This means that some of the observations from the original training set will be repeated and some will not be chosen at all. This process is known as <span class="strong"><strong class="calibre2">bootstrapping</strong></span>, <span class="strong"><strong class="calibre2">bootstrap sampling</strong></span>, or <span class="strong"><strong class="calibre2">bootstrap resampling</strong></span>.</p><p class="calibre8">2. Train a classification model using this sampled dataset. Typically, we opt not to use regularization or shrinkage methods designed to reduce overfitting in this step, because the aggregating process used at the end will be used to smooth out overfitting.</p><p class="calibre8">3. For each observation in the sampled dataset, record the class assigned by the model.</p><p class="calibre8">4. Repeat this process <span class="strong"><em class="calibre9">M</em></span> times in order to train <span class="strong"><em class="calibre9">M</em></span> models.</p><p class="calibre8">5. For every observation in the original training dataset, compute the predicted class via a majority vote across the different models. For example, suppose M = 61 and through bootstrap sampling, a particular observation appears in the training data for 50 of the models. If 37 of these predict class 1 for this observation and 13 predict class -1, by majority vote the overall prediction will be class 1.</p><p class="calibre8">6. Compute the model's accuracy using the labels provided by the training set.</p></div><p class="calibre8">In a nutshell, all we are effectively doing is training the same model on <span class="strong"><em class="calibre9">M</em></span> different versions of the input training set (created through sampling with replacement) and averaging the result.</p><p class="calibre8">A legitimate question to ask would be, how many distinct observations do we get each time we sample <a id="id668" class="calibre1"/>with replacement? On average, we end up with 63 percent of the distinct observations in every sample that we make. To understand where this comes from, consider that because we are sampling with replacement, the probability of not picking out a particular observation, <span class="strong"><em class="calibre9">x<sub class="calibre14">1</sub></em></span>, during sampling is just the result of <span class="strong"><em class="calibre9">n</em></span> failed Bernoulli trials.</p><div class="mediaobject"><img src="../images/00158.jpeg" alt="Bagging" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This number also happens to be the average proportion of observations that are not selected across the entire training dataset because we multiply and divide the previous expression by <span class="strong"><em class="calibre9">n</em></span> to compute this quantity. The numerical result of this expression can be approximated by <span class="strong"><em class="calibre9">e-1</em></span>, which is roughly 37 percent. Consequently, the average proportion of observations that are selected is around 63 percent. This number is just an average, of course, and is more accurate for larger values of <span class="strong"><em class="calibre9">n</em></span>.</p></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods">
<div class="book" title="Bagging">
<div class="book" title="Margins and out-of-bag observations"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec80" class="calibre1"/>Margins and out-of-bag observations</h2></div></div></div><p class="calibre8">Let's imagine <a id="id669" class="calibre1"/>that for a particular observation, <span class="strong"><em class="calibre9">x<sub class="calibre14">1</sub></em></span>, 85 percent of our models predict the correct class and the remaining 15 percent predict the incorrect <a id="id670" class="calibre1"/>class. Let's also imagine that we have <a id="id671" class="calibre1"/>another observation, <span class="strong"><em class="calibre9">x<sub class="calibre14">2</sub></em></span>, for which the analogous percentages are 53 percent and 47 percent. Clearly, our intuition suggests that we should be more confident about the classification of the former observation compared to the latter observation. Put differently, the difference between <a id="id672" class="calibre1"/>the classification proportions, also known as the <span class="strong"><strong class="calibre2">margin</strong></span> (similar to but not to be confused with the margin used for support vector machines) is a good indicator of the confidence of our classification.</p><p class="calibre8">The 70 percent margin of observation <span class="strong"><em class="calibre9">x<sub class="calibre14">1</sub></em></span> is much larger than the 6 percent margin of observation <span class="strong"><em class="calibre9">x<sub class="calibre14">2</sub></em></span> and thus, we believe more strongly in our ability to correctly classify the former observation. In general, what we are hoping for is a classifier that has a large margin for all the observations. We are less optimistic about the generalization abilities of a classifier that has a small margin for more than a handful of observations.</p><p class="calibre8">One thing the reader may have noticed here is that in generating the set of predicted values for each model, we are using the same data on which the model was trained. If we look closely at <span class="strong"><em class="calibre9">step 3</em></span> of the procedure, we are classifying the same sampled data that we used in <span class="strong"><em class="calibre9">step 2</em></span> to train the model. Even though we are eventually relying on using an averaging process <a id="id673" class="calibre1"/>at the end in order to obtain the estimated <a id="id674" class="calibre1"/>accuracy of the bagged classifier for unseen data, we haven't actually used any unseen data at any step along the way.</p><p class="calibre8">Remember that in <span class="strong"><em class="calibre9">step 1</em></span> we constructed a sample of the training data with which to train our model. From the original dataset, we refer to the observations that were not chosen for a particular <a id="id675" class="calibre1"/>iteration of the procedure as the <span class="strong"><strong class="calibre2">out-of-bag</strong></span> (<span class="strong"><strong class="calibre2">OOB</strong></span>) observations. These observations are therefore not used in the training of the model at that iteration. Consequently, instead of relying on the observations used to train the <a id="id676" class="calibre1"/>models at every step, we can actually use the OOB observations to record the accuracy of a particular model.</p><p class="calibre8">In the end, we average over all the OOB accuracy rates to obtain an average accuracy. This average accuracy is far more likely to be a realistic and objective estimate of the performance of the bagged classifier on unseen data. For a particular observation, the assigned class is thus decided as the majority vote over all classifiers for which the observation was not picked in their corresponding training sample.</p><p class="calibre8">The samples <a id="id677" class="calibre1"/>generated from sampling the original dataset with replacement, known as <span class="strong"><strong class="calibre2">bootstrapped samples</strong></span>, are similar to drawing multiple samples from the same distribution. As we are trying to estimate the same target function using a number of different samples instead of just one, the averaging process reduces the variance of the result. To see this, consider trying to estimate the mean of a set of observations <a id="id678" class="calibre1"/>drawn from the same distribution and all mutually independent of each other. More formally, these are known as <span class="strong"><strong class="calibre2">independent and identically distributed</strong></span> (<span class="strong"><strong class="calibre2">iid</strong></span>) observations. The variance of the mean of these observations is <span class="strong"><img src="../images/00159.jpeg" alt="Margins and out-of-bag observations" class="calibre26"/></span>.</p><p class="calibre8">This shows that as the number of observations increases, the variance decreases. Bagging tries to achieve the same behavior for the function we are trying to model. We don't have truly independent training samples, and are instead forced to use bootstrapped samples, but this thought experiment should be enough to convince us that, in principle, bagging has the potential to reduce the variance of the model. At the same time, this averaging process is a form of smoothing over any localized bumps in the function that we are trying to estimate. Assuming that the target regression function or classification boundary that we are trying to estimate is actually smooth, then bagging may also reduce the bias of our model.</p></div></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods">
<div class="book" title="Bagging">
<div class="book" title="Predicting complex skill learning with bagging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec81" class="calibre1"/>Predicting complex skill learning with bagging</h2></div></div></div><p class="calibre8">Bagging <a id="id679" class="calibre1"/>and boosting are both very popular with the tree-based models that we studied in <a class="calibre1" title="Chapter 6. Support Vector Machines" href="part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7">Chapter 6</a>, <span class="strong"><em class="calibre9">Tree-Based Methods</em></span>. There are many notable implementations to apply these approaches to methodologies such as CART for building trees.</p><p class="calibre8">The <code class="email">ipred</code> package, for example, contains an implementation to build a bagged predictor for trees built with <code class="email">rpart()</code>. We can experiment with the <code class="email">bagging()</code> function that this package provides. To do this, we specify the number of bagged trees to make using the <code class="email">nbagg</code> parameter (default is <code class="email">25</code>) and indicate that we want to compute accuracy using the OOB samples by setting the <code class="email">coob</code> parameter to <code class="email">TRUE</code>.</p><p class="calibre8">We will do this for our complex skill learning dataset from the previous chapter, using the same training data frame:</p><div class="informalexample"><pre class="programlisting">&gt; baggedtree &lt;- bagging(LeagueIndex ~ ., data = skillcraft_train,    
                        nbagg = 100, coob = T)
&gt; baggedtree_predictions &lt;- predict(baggedtree, skillcraft_test)
&gt; (baggedtree_SSE &lt;- compute_SSE(baggedtree_predictions, 
                                 skillcraft_test$LeagueIndex))
[1] 646.3555</pre></div><p class="calibre8">As we can see, the SSE on the test set is less than the lowest SSE that we saw when tuning a single tree. Increasing the number of bagged iterations, however, does not seem to improve this performance substantially. We will revisit this dataset again later.</p></div></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods">
<div class="book" title="Bagging">
<div class="book" title="Predicting heart disease with bagging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec82" class="calibre1"/>Predicting heart disease with bagging</h2></div></div></div><p class="calibre8">The prototypical <a id="id680" class="calibre1"/>use case for bagging is the decision tree; however, it is important to remember that we can use this method with a variety of different models. In this section, we will show how we can build a bagged logistic regression classifier. We built a logistic regression classifier for the Statlog Heart dataset in <a class="calibre1" title="Chapter 3. Linear Regression" href="part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7">Chapter 3</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span>. Now, we will repeat that experiment but use bagging in order to see if we can improve our results. To begin with, we'll draw our samples with replacement and use these to train our models:</p><div class="informalexample"><pre class="programlisting">&gt; M &lt;- 11
&gt; seeds &lt;- 70000 : (70000 + M - 1)
&gt; n &lt;- nrow(heart_train)
&gt; sample_vectors &lt;- sapply(seeds, function(x) { set.seed(x); 
  return(sample(n, n, replace = T)) })</pre></div><p class="calibre8">In our code, the data frames <code class="email">heart_train</code> and <code class="email">heart_test</code> are referring to the same data frames that we prepared in <a class="calibre1" title="Chapter 3. Linear Regression" href="part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7">Chapter 3</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span>. We begin by deciding on the number of models that we will train and setting the appropriate value of <code class="email">M</code>. Here, we have used an initial value of <code class="email">11</code>.</p><p class="calibre8">Note that it is a good idea to use an odd number of models with bagging, so that during the majority voting process there can never be a tie with binary classification. For reproducibility, we set a vector of seeds that we will use. This is simply a counter from an arbitrarily <a id="id681" class="calibre1"/>chosen starting seed value of <code class="email">70000</code>. The <code class="email">sample_vectors</code> matrix in our code contains a matrix where the columns are the indexes of randomly selected rows from the training data with replacement. Note that the rows are numbered 1 through 230 in the training data, making the sampling process easy to code.</p><p class="calibre8">Next, we'll define a function that creates a single logistic regression model given a sampling vector of indices to use with our training data frame:</p><div class="informalexample"><pre class="programlisting"> train_1glm &lt;- function(sample_indices) { 
     data &lt;- heart_train[sample_indices,]; 
     model &lt;- glm(OUTPUT ~ ., data = data, family = binomial("logit")); 
     return(model)
 }

&gt; models &lt;- apply(sample_vectors, 2, train_1glm)</pre></div><p class="calibre8">In the last line of the preceding code, we iterate through the columns of the <code class="email">sample_vectors</code> matrix we produced earlier and supply them as an input to our logistic regression model training function, <code class="email">train_1glm()</code>. The resulting models are then stored in our final list variable, <code class="email">models</code>. This now contains 11 trained models.</p><p class="calibre8">As the first method of evaluating our models, we are going to use the data on which each individual model was trained. To that end, we'll construct the <code class="email">bags</code> variable that is a list of these data frames, this time with unique indexes, as we don't want to use any duplicate rows from the bootstrap sampling process in the evaluation. We'll also add a new column called <code class="email">ID</code> to these data frames that stores the original row names from the <code class="email">heart_train</code> data frame. We'll see why we do this shortly:</p><div class="informalexample"><pre class="programlisting"> get_1bag &lt;- function(sample_indices) {
     unique_sample &lt;- unique(sample_indices); 
     df &lt;- heart_train[unique_sample, ]; 
     df$ID &lt;- unique_sample; 
     return(df)
 }
 
&gt; bags &lt;- apply(sample_vectors, 2, get_1bag)</pre></div><p class="calibre8">We now have a list of models and a list of data frames that they were trained on, the latter without duplicate observations. From these two, we can create a list of predictions. For each training data frame, we will tack on a new column called <code class="email">PREDICTIONS {m}</code>, where <code class="email">{m}</code> will be the number of the model being used to make the predictions. Consequently, the first data frame in the bags list will have a predictions column called <code class="email">PREDICTIONS 1</code>. The second data frame will have a predictions column called <code class="email">PREDICTIONS 2</code>, the third will have one called <code class="email">PREDICTIONS 3</code>, and so on.</p><p class="calibre8">The following call produces a new set of data frames as just described, but only keeping the <code class="email">PREDICTIONS{m}</code> and <code class="email">ID</code> columns, and these data frames are stored as a list in the variable <code class="email">training_predictions</code>:</p><div class="informalexample"><pre class="programlisting"> glm_predictions &lt;- function(model, data, model_index) {
     colname &lt;- paste("PREDICTIONS", model_index);
     data[colname] &lt;- as.numeric( 
                      predict(model, data, type = "response") &gt; 0.5); 
     return(data[,c("ID", colname), drop = FALSE])
 }
 
&gt; training_predictions &lt;- 
     mapply(glm_predictions, models, bags, 1 : M, SIMPLIFY = F)</pre></div><p class="calibre8">Next, we <a id="id682" class="calibre1"/>want to merge all of these data frames onto a single data frame, where the rows are the rows of the original data frame (and thus, correspond to the observations in the dataset) and the columns are the predictions made by each model on the observations. Where a particular row (observation) was not selected by the sampling process to train a particular model, it will have an <code class="email">NA</code> value in the column corresponding to the predictions that that model makes.</p><p class="calibre8">Just to be clear, recall that each model is making predictions only on the observations that were used to train it and so the number of predictions that each model makes is smaller than the total number of observations available in our starting data.</p><p class="calibre8">As we have stored the original row numbers of the <code class="email">heart_train</code> data frame in the <code class="email">ID</code> column of every data frame created in the previous step, we can merge using this column. We use the <code class="email">Reduce()</code> function along with the <code class="email">merge()</code> function in order to merge all the data frames in our <code class="email">training_predictions</code> variable into one new data frame. Here is the code:</p><div class="informalexample"><pre class="programlisting">&gt; train_pred_df &lt;- Reduce(function(x, y) merge(x, y, by = "ID",  
                          all = T), training_predictions)</pre></div><p class="calibre8">Let's have a look at the first few lines and columns of this aggregated data frame:</p><div class="informalexample"><pre class="programlisting">&gt; head(training_prediction_df[, 1:5])
  ID PREDICTIONS 1 PREDICTIONS 2 PREDICTIONS 3 PREDICTIONS 4
1  1             1            NA             1            NA
2  2             0            NA            NA             0
3  3            NA             0             0            NA
4  4            NA             1             1             1
5  5             0             0             0            NA
6  6             0             1             0             0</pre></div><p class="calibre8">The first column is the <code class="email">ID</code> row that was used to merge the data frame. The numbers in this column are the row numbers of the observations from the starting training data frame. The <code class="email">PREDICTIONS 1</code> column contains the predictions that the first model makes. We can see that this model had rows <code class="email">1</code>, <code class="email">2</code>, <code class="email">5</code>, and <code class="email">6</code> as part of its training data. For the first row, the model predicts class <code class="email">1</code> and for the other three rows, it predicts class <code class="email">0</code>. Rows <code class="email">3</code> and <code class="email">4</code> were not part of its training data and so there are two <code class="email">NA</code> values. This reasoning can be used to understand the remaining columns, which correspond to the next three models trained.</p><p class="calibre8">With this data frame constructed, we can now produce our training data predictions for the whole bagged model using a majority vote across each row of the preceding data frame. Once we have these, we merely need to match the predictions with the labeled values of the corresponding rows of the original <code class="email">heart_train</code> data frame and compute our accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; train_pred_vote &lt;- apply(train_pred_df[,-1], 1, 
                function(x) as.numeric(mean(x, na.rm = TRUE) &gt; 0.5))
&gt; (training_accuracy &lt;- mean(train_pred_vote == 
                heart_train$OUTPUT[as.numeric(train_pred_df$ID)]))
[1] 0.9173913</pre></div><p class="calibre8">We now <a id="id683" class="calibre1"/>have our first accuracy measure for our bagged model--91.7 percent. This is analogous to measuring the accuracy on our training data. We will now repeat this process using the OOB observations for each model to compute the OOB accuracy.</p><p class="calibre8">There is one caveat here, however. In our data, the ECG column is a factor with three levels, one of which, level 1, is very rare. As a result of this, when we draw bootstrap samples from the original training data, we may encounter samples in which this factor level never appears. When that happens, the <code class="email">glm()</code> function will think this factor only takes two levels, and the resulting model will be unable to make predictions when it encounters an observation with a value for the ECG factor that it has never seen before.</p><p class="calibre8">To handle this situation, we need to replace the level 1 value of this factor with an <code class="email">NA</code> value for the OOB observations, if the model they correspond to did not have at least one observation with an ECG factor level of 1 in its training data. Essentially, for simplicity, we will just not attempt to make a prediction for these problematic observations when they arise. With this in mind, we will define a function to compute the OOB observations for a particular sample and then use this to find the OOB observations for all our samples:</p><div class="informalexample"><pre class="programlisting"> get_1oo_bag &lt;- function(sample_indices) {
     unique_sample &lt;- setdiff(1 : n, unique(sample_indices)); 
     df &lt;- heart_train[unique_sample,]; 
     df$ID &lt;- unique_sample; 
     if (length(unique(heart_train[sample_indices,]$ECG)) &lt; 3) 
         df[df$ECG == 1,"ECG"] = NA; 
     return(df)
 }
 
&gt; oo_bags &lt;- apply(sample_vectors, 2, get_1oo_bag)</pre></div><p class="calibre8">Next, we will use our <code class="email">glm_predictions()</code> function to compute predictions using our out-of-bag samples. The remainder of the process is identical to what we did earlier:</p><div class="informalexample"><pre class="programlisting">&gt; oob_predictions &lt;- mapply(glm_predictions, models, oo_bags, 
                            1 : M, SIMPLIFY = F)
&gt; oob_pred_df &lt;- Reduce(function(x, y) merge(x, y, by = "ID", 
                        all = T), oob_predictions)
&gt; oob_pred_vote &lt;- apply(oob_pred_df[,-1], 1, 
                   function(x) as.numeric(mean(x, na.rm = TRUE) &gt; 0.5))

&gt; (oob_accuracy &lt;- mean(oob_pred_vote == 
             heart_train$OUTPUT[as.numeric(oob_pred_df$ID)], 
             na.rm = TRUE))
[1] 0.8515284</pre></div><p class="calibre8">As expected, we see that our OOB accuracy, which is a better measure of performance on unseen data, is lower than the training data accuracy. In the last line of the previous code sample, we excluded <code class="email">NA</code> values when computing the out-of-bag accuracy. This is important because <a id="id684" class="calibre1"/>it is possible that a particular observation may appear in all the bootstrap samples and therefore will never be available for an OOB prediction.</p><p class="calibre8">Equally, our fix for the rare level of the ECG factor means that even if an observation is not selected by the sampling process, we may still not be able to make a prediction for it. The reader should verify that only one observation happens to produce an <code class="email">NA</code> value because of the combination of the two phenomena just described.</p><p class="calibre8">Finally, we'll repeat this process a third time using the <code class="email">heart_test</code> data frame to obtain the test set accuracy:</p><div class="informalexample"><pre class="programlisting"> get_1test_bag &lt;- function(sample_indices) {
     df &lt;- heart_test; 
     df$ID &lt;- row.names(df); 
     if (length(unique(heart_train[sample_indices,]$ECG)) &lt; 3) 
         df[df$ECG == 1,"ECG"] = NA; 
     return(df)
 }
 
&gt; test_bags &lt;- apply(sample_vectors, 2, get_1test_bag)
&gt; test_predictions &lt;- mapply(glm_predictions, models, test_bags, 
                             1 : M, SIMPLIFY = F)
&gt; test_pred_df &lt;- Reduce(function(x, y) merge(x, y, by = "ID",
                         all = T), test_predictions)
&gt; test_pred_vote &lt;- apply(test_pred_df[,-1], 1, 
              function(x) as.numeric(mean(x, na.rm = TRUE) &gt; 0.5))
&gt; (test_accuracy &lt;- mean(test_pred_vote == 
              heart_test[test_pred_df$ID,"OUTPUT"], na.rm = TRUE))
[1] 0.8</pre></div><p class="calibre8">The accuracy on the test set seems lower than what we found without a bagged model. This is not necessarily bad news for us, since the test set is very small. In fact, the difference between the performance of this bagged model and the original model trained in <a class="calibre1" title="Chapter 3. Linear Regression" href="part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7">Chapter 3</a>, <span class="strong"><em class="calibre9">Logistic Regression</em></span>, is 32/40 compared to 36/40, which is to say it is only worse by four observations in 40.</p><p class="calibre8">In a real-world situation, we generally want to have a much larger test set to estimate our unseen accuracy. In fact, because of this, we are more inclined to believe our OOB accuracy measurement, which is done over a larger number of observations and averaged over many models.</p><p class="calibre8">Bagging <a id="id685" class="calibre1"/>is actually very useful for us in this scenario as it gives us a model for which we can have a better estimate of the test accuracy, using the OOB observations because the test set is so small. As a final demonstration, we run the previous code a number of times with different values of <span class="strong"><em class="calibre9">M</em></span> and store the results in a data frame:</p><div class="informalexample"><pre class="programlisting">&gt; heart_bagger_df
     M Training Accuracy Out-of-bag Accuracy Test Accuracy
1   11         0.9173913           0.8515284         0.800
2   51         0.9130435           0.8521739         0.800
3  101         0.9173913           0.8478261         0.800
4  501         0.9086957           0.8521739         0.775
5 1001         0.9130435           0.8565217         0.775</pre></div><p class="calibre8">This table shows us that the test accuracy fluctuates around 80 percent. This isn't that surprising given the small size of our test set of only 40 observations. For the training accuracy, we see that we are fluctuating around 91 percent. The OOB accuracy, which is far more stable as an accuracy measure, shows us that the expected performance of the model is around 85 percent. As the number of models increases, we don't see much of an improvement over 11 models, though for most real-world datasets, we would usually see some improvement before tapering off.</p><p class="calibre8">Although our example focused exclusively on bagging for classification problems, the move to regression problems is relatively straightforward. Instead of using majority votes for a particular observation, we use the average value of the target function predicted by the individual models. Bagging is not always guaranteed to provide a performance improvement on a model. For starters, we should note that it makes sense to use bagging only when we have a nonlinear model. As the bagging process is performing an average (a linear operation) over the models generated, we will not see any improvements with linear regression, for example, because we aren't increasing the expressive power of our model. The <a id="id686" class="calibre1"/>next section talks about some other limitations of bagging.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note34" class="calibre1"/>Note</h3><p class="calibre8">For more information on bagging, consult the original paper of <span class="strong"><em class="calibre9">Leo Breiman</em></span> titled <span class="strong"><em class="calibre9">Bagging Predictors</em></span>, published in 1996 in the journal <span class="strong"><em class="calibre9">Machine Learning</em></span>.</p></div></div></div></div>

<div class="book" title="Chapter&#xA0;9.&#xA0;Ensemble Methods">
<div class="book" title="Bagging">
<div class="book" title="Limitations of bagging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_5"><a id="ch09lvl2sec83" class="calibre1"/>Limitations of bagging</h2></div></div></div><p class="calibre8">So far, we've only explored the upside of using bagging, but in some cases it may turn out not to <a id="id687" class="calibre1"/>be a good idea. Bagging involves taking the average across predictions made by several models, which are trained on bootstrapped samples of the training data. This averaging process smoothens the overall output, which may reduce bias when the target function is smooth. Unfortunately, if the target function is not smooth, we may actually introduce bias by using bagging.</p><p class="calibre8">Another way that bagging introduces bias is when one of the output classes is very rare. Under those circumstances, the majority voting system tends to be biased towards the more common class. Other problems may arise in relation to the sampling process itself. As we have already learned, when some categorical features include values that are rare, these may not appear at all in some of the bootstrap samples. When this happens, the models built for these samples will be unable to make a prediction when they encounter this new feature level in their test set.</p><p class="calibre8">High leverage points, which are highly influential in determining the model's output function compared to other points, can also be a problem. If a bootstrap sample is drawn that does not include one or more high leverage points, the resulting trained model will be quite different compared to when they are included. Therefore, bagging performance depends on how often these particular observations are sampled in order to win the majority vote. Due to this fact, our ensemble model will have a high variance in the presence of high leverage points. For a given dataset, we can often predict if we are in this situation by looking for outliers and highly skewed features.</p><p class="calibre8">We must also remember that the different models we build are not truly independent of each other in the strict sense because they still use the same set of input features. The averaging process would have been more effective if the models were independent. Also, bagging does not help when the type of model that we are using predicts a functional form that is very far from the true form of the target function. When this happens, training multiple models of this type merely reproduces the systematic errors across the different models. Put differently, bagging works better when we have low bias and high variance models as the averaging process is primarily designed to reduce the variance.</p><p class="calibre8">Finally, and this applies to ensemble models in general, we tend to lose the explanative power of our model. We saw an example of explanative power in linear regression where each <a id="id688" class="calibre1"/>model parameter (regression coefficient) corresponded to the amount of change in the output, for a unit increase in the corresponding feature. Decision trees are another example of a model with high explanatory power. Using bagging loses this benefit because of the majority voting process and so we cannot directly relate our inputs to the predicted output.</p></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Boosting" id="24L8G1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec65" class="calibre1"/>Boosting</h1></div></div></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Boosting</strong></span> offers <a id="id689" class="calibre1"/>an alternative take on the problem of how to combine models together to achieve greater performance. In particular, it is especially suited to <span class="strong"><strong class="calibre2">weak learners</strong></span>. Weak learners are models that produce an accuracy that is better than <a id="id690" class="calibre1"/>a model that randomly guesses, but not by much. One way to create a weak learner is to use a model whose complexity is configurable.</p><p class="calibre8">For example, we can train a multilayer perceptron network with a very small number of hidden layer neurons. Similarly, we can train a decision tree, but only allow the tree to comprise a single node, resulting in a single split in the input data. This special type of decision <a id="id691" class="calibre1"/>tree is known as a <span class="strong"><strong class="calibre2">stump</strong></span>.</p><p class="calibre8">When we looked at bagging, the key idea was to take a set of random bootstrapped samples of the training data and then train multiple versions of the same model using these different samples. In the classical boosting scenario, there is no random component, as all the models use all of the training data.</p><p class="calibre8">For classification, boosting works by building a model on the training data and then measuring the classification accuracy on that training data. The individual observations that were misclassified by the model are given a larger weight than those that were correctly classified, and then the model is retrained again using these new weights. This is then repeated multiple times, each time adjusting the weights of individual observations based on whether they were correctly classified or not in the last iteration.</p><p class="calibre8">To combat overfitting, the ensemble classifier is built as a weighted average of all the models trained in this sequence, with the weights usually being proportional to the classification accuracy of each individual model. As we are using the entire training data, there are no OOB observations, so the accuracy in each case is measured using the training data itself. Regression with boosting is usually done by adjusting the weights of observation based on some measure of the distance between the predicted value and the labeled value.</p></div>

<div class="book" title="Boosting" id="24L8G1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="AdaBoost"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec84" class="calibre1"/>AdaBoost</h2></div></div></div><p class="calibre8">Continuing <a id="id692" class="calibre1"/>our focus on classification problems, we now introduce <span class="strong"><strong class="calibre2">AdaBoost</strong></span>, which <a id="id693" class="calibre1"/>is short for <span class="strong"><strong class="calibre2">adaptive boosting</strong></span>. In particular, we will focus on <span class="strong"><strong class="calibre2">Discrete AdaBoost</strong></span>, as it makes predictions on binary classes. We <a id="id694" class="calibre1"/>will use <code class="email">-1</code> and <code class="email">1</code> as the class labels. <span class="strong"><strong class="calibre2">Real AdaBoost</strong></span> is an extension <a id="id695" class="calibre1"/>of AdaBoost, in which the outputs are the class probabilities. In our version of AdaBoost, all of the training data is used; however, there are other <a id="id696" class="calibre1"/>versions of AdaBoost in which the training data is also sampled. There are also multiclass extensions of AdaBoost as well as extensions that are suited to regression-type problems.</p><div class="book" title="AdaBoost for binary classification"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch09lvl3sec13" class="calibre1"/>AdaBoost for binary classification</h3></div></div></div><p class="calibre8">The <a id="id697" class="calibre1"/>following outlines the specifics – inputs, outputs, and methods used by AdaBoost:</p><p class="calibre8">
<span class="strong"><strong class="calibre2">Inputs</strong></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre9">data</em></span>: The input data frame containing the input features and a column with the binary output label</li><li class="listitem"><span class="strong"><em class="calibre9">M</em></span>: An integer, representing the number of models that we want to train</li></ul></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Output</strong></span>:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre9">models</em></span>: A series of <span class="strong"><em class="calibre9">Μ</em></span> trained models</li><li class="listitem"><span class="strong"><em class="calibre9">alphas</em></span>: A vector of <span class="strong"><em class="calibre9">M</em></span> model weights</li></ul></div><p class="calibre8">
<span class="strong"><strong class="calibre2">Method</strong></span>:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Initialize a vector of observation weights, <span class="strong"><em class="calibre9">w</em></span>, of length <span class="strong"><em class="calibre9">n</em></span> with entries <span class="strong"><em class="calibre9">w<sub class="calibre14">i</sub></em></span><span class="strong"><em class="calibre9"> = 1/n</em></span>. This vector will be updated in every iteration.</li><li class="listitem" value="2">Using the current value of the observation weights and all the data in the training set, train a classifier model <span class="strong"><em class="calibre9">G<sub class="calibre14">m</sub></em></span>.</li><li class="listitem" value="3">Compute the weighted error rate as the sum of all misclassified observations multiplied by their observation weights, divided by the sum of the weight vector. Following our usual convention of using <span class="strong"><em class="calibre9">x<sub class="calibre14">i</sub></em></span> as an observation and <span class="strong"><em class="calibre9">y<sub class="calibre14">i</sub></em></span> as its label, we can express this using the following equation:<div class="mediaobject"><img src="../images/00160.jpeg" alt="AdaBoost for binary classification" class="calibre10"/></div><p class="calibre16"> </p></li><li class="listitem" value="4">We then set the model weight for this model, <span class="strong"><em class="calibre9">a<sub class="calibre14">m</sub></em></span>, as the logarithm of the ratio between the accuracy and error rates. In a formula, this is:<div class="mediaobject"><img src="../images/00161.jpeg" alt="AdaBoost for binary classification" class="calibre10"/></div><p class="calibre16"> </p></li><li class="listitem" value="5">
We <a id="id698" class="calibre1"/>then update the observation weights vector, <span class="strong"><em class="calibre9">w</em></span>, for the next iteration. Incorrectly classified observations have their weight multiplied by <span class="strong"><img src="../images/00162.jpeg" alt="AdaBoost for binary classification" class="calibre26"/></span>, thereby increasing their weight for the next iteration. Correctly classified observations have their weight multiplied by <span class="strong"><img src="../images/00163.jpeg" alt="AdaBoost for binary classification" class="calibre26"/></span>, thereby reducing their weight for the next iteration.
</li><li class="listitem" value="6">Renormalize the weights vector so that the sum of the weights is 1.</li><li class="listitem" value="7">Repeat steps two through six <span class="strong"><em class="calibre9">M</em></span> times in order to produce <span class="strong"><em class="calibre9">M</em></span> models.</li><li class="listitem" value="8">Define our ensemble classifier as the sign of the weighted sum of the outputs of all the boosted models:<div class="mediaobject"><img src="../images/00164.jpeg" alt="AdaBoost for binary classification" class="calibre10"/></div><p class="calibre16"> </p></li></ol><div class="calibre13"/></div></div></div></div>
<div class="book" title="Predicting atmospheric gamma ray radiation"><div class="book" id="25JP22-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec66" class="calibre1"/>Predicting atmospheric gamma ray radiation</h1></div></div></div><p class="calibre8">In order <a id="id699" class="calibre1"/>to study boosting in action, in this section we'll introduce a new prediction problem from the field of atmospheric physics. More specifically, we will analyze the patterns made by radiation on a telescope camera in order to predict whether a particular pattern came from gamma rays leaking into the atmosphere, or from regular background radiation.</p><p class="calibre8">Gamma rays leave distinctive elliptical patterns and so we can create a set of features to describe these. The dataset we will use is the <span class="strong"><em class="calibre9">MAGIC Gamma Telescope dataset</em></span>, hosted by the <span class="strong"><em class="calibre9">UCI Machine Learning</em></span> repository at <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope">http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope</a>. Our data consists of 19,020 observations of the following attributes:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FLENGTH</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The major axis of the ellipse (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FWIDTH</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The minor axis of the ellipse (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FSIZE</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Logarithm to the base ten of the sum of the content of all pixels in the camera photo</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FCONC</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Ratio of the sum of the two highest pixels over <code class="literal">FSIZE</code>
</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FCONC1</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Ratio of the highest pixel over <code class="literal">FSIZE</code>
</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FASYM</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Distance from the highest pixel to the center, projected onto the major axis (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FM3LONG</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Third root of the third moment along the major axis (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FM3TRANS</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Third root of the third moment along the minor axis (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FALPHA</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Angle of the major axis with the vector to the origin (degrees)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FDIST</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Distance from the origin to the center of the ellipse (mm)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CLASS</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Gamma rays (<span><em class="calibre27">g</em></span>) or Background Hadron Radiation (<span><em class="calibre27">b</em></span>)</p>
</td></tr></tbody></table></div><p class="calibre8">First, we <a id="id700" class="calibre1"/>will load the data into a data frame called <code class="email">magic</code>, recoding the <code class="email">CLASS</code> output variable to use classes <code class="email">1</code> and <code class="email">-1</code> for gamma rays and background radiation, respectively:</p><div class="informalexample"><pre class="programlisting">&gt; magic &lt;- read.csv("magic04.data", header = FALSE)
&gt; names(magic) &lt;- c("FLENGTH", "FWIDTH", "FSIZE", "FCONC", "FCONC1",
  "FASYM", "FM3LONG", "FM3TRANS", "FALPHA", "FDIST", "CLASS")
&gt; magic$CLASS &lt;- as.factor(ifelse(magic$CLASS =='g', 1, -1))</pre></div><p class="calibre8">Next, we'll split our data frame into a training data and a test data frame using our typical 80-20 split:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; set.seed(33711209)
&gt; magic_sampling_vector &lt;- createDataPartition(magic$CLASS, 
                             p = 0.80, list = FALSE)
&gt; magic_train &lt;- magic[magic_sampling_vector, 1:10]
&gt; magic_train_output &lt;- magic[magic_sampling_vector, 11]
&gt; magic_test &lt;- magic[-magic_sampling_vector, 1:10]
&gt; magic_test_output &lt;- magic[-magic_sampling_vector, 11]</pre></div><p class="calibre8">The model that we are going to use with boosting is a simple multilayer perceptron with a single <a id="id701" class="calibre1"/>hidden layer. Harkening back to <a class="calibre1" title="Chapter 4. Generalized Linear Models" href="part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7">Chapter 4</a>, <span class="strong"><em class="calibre9">Neural Networks</em></span>, we know that the <code class="email">nnet</code> package is perfect for this task. With neural networks, we often get superior accuracy when we normalize the inputs and so before training any models, we will carry out this preprocessing step:</p><div class="informalexample"><pre class="programlisting">&gt; magic_pp &lt;- preProcess(magic_train, method = c("center", 
                                                 "scale"))
&gt; magic_train_pp &lt;- predict(magic_pp, magic_train)
&gt; magic_train_df_pp &lt;- cbind(magic_train_pp, 
                             CLASS = magic_train_output)
&gt; magic_test_pp &lt;- predict(magic_pp, magic_test)</pre></div><p class="calibre8">Boosting is designed to work best with weak learners, and for this reason, we are going to use a very small number of hidden neurons in our hidden layer. Concretely, we will begin with the simplest possible multilayer perceptron that uses a single hidden neuron. To understand the effect of using boosting, we will establish our baseline performance by training a single neural network and measuring its accuracy. We can do this as follows:</p><div class="informalexample"><pre class="programlisting">&gt; library(nnet)
&gt; n_model &lt;- nnet(CLASS ~ ., data = magic_train_df_pp, size = 1)
&gt; n_test_predictions &lt;- predict(n_model, magic_test_pp,
                                type = "class")
&gt; (n_test_accuracy &lt;- mean(n_test_predictions ==  
                           magic_test_output))
[1] 0.7948988</pre></div><p class="calibre8">This establishes that we have a baseline accuracy of around 79.5 percent. This isn't too bad, but we are going to use boosting to see if we can improve it. To that end, we are going to write our own function, <code class="email">AdaBoostNN()</code>, which will take as input a data frame, the name of the output variable, the number of single hidden layer neural network models we want to build, and finally the number of hidden units these neural networks will have. This function will then implement the AdaBoost algorithm that we previously described and finally return a list of models and their weights. Here is the function:</p><div class="informalexample"><pre class="programlisting">AdaBoostNN &lt;- function(training_data, output_column, M,  
                       hidden_units) {
  require("nnet")
  models &lt;- list()
  alphas &lt;- list()
  n &lt;- nrow(training_data)
  model_formula &lt;- as.formula(paste(output_column, '~ .', sep = ''))
  w &lt;- rep((1/n), n)
  for (m in 1:M) {
    model &lt;- nnet(model_formula, data = training_data, 
                size = hidden_units, weights = w)
    models[[m]] &lt;- model
    predictions &lt;- as.numeric(predict(model, 
                    training_data[, -which(names(training_data) ==
                    output_column)], type = "class"))
    errors &lt;- predictions != training_data[, output_column]
    error_rate &lt;- sum(w * as.numeric(errors)) / sum(w)
    alpha &lt;- 0.5 * log((1 - error_rate) / error_rate)
    alphas[[m]] &lt;- alpha
    temp_w &lt;- mapply(function(x, y) if (y) { x * exp(alpha) } 
                    else { x * exp(-alpha)}, w, errors)
    w &lt;- temp_w / sum(temp_w)
  }
  return(list(models = models, alphas = unlist(alphas)))
}</pre></div><p class="calibre8">Before proceeding, we will work our way through the function to understand what each line is doing. We first initialize empty lists of models and model weights (<code class="email">alphas</code>). We also <a id="id702" class="calibre1"/>compute the number of observations in our training data, storing this in the variable <code class="email">n</code>. The name of the output column provided is then used to create a formula that describes the neural network that we will build.</p><p class="calibre8">In our dataset, this formula will be <code class="email">CLASS ~ .</code>, which means that the neural network will compute <code class="email">CLASS</code> as a function of all the other columns as input features. We then initialize our weights vector, as we did in <span class="strong"><em class="calibre9">step 1</em></span> of AdaBoost, and define our loop that will run for <span class="strong"><em class="calibre9">M</em></span> iterations in order to build <span class="strong"><em class="calibre9">M</em></span> models.</p><p class="calibre8">In every iteration, the first step is to use the current setting of the weights vector to train a neural network using as many hidden units as specified in the input, <code class="email">hidden_units</code>. We then compute a vector of predictions that this model generates on the training data using the <code class="email">predict()</code> function. By comparing these predictions to the output column of the training data, we calculate the errors that the current model makes on the training data. This then allows us to compute the error rate. According to <span class="strong"><em class="calibre9">step 4</em></span> of the AdaBoost algorithm, this error rate is set as the weight of the current model. Finally, the observation weights to be used in the next iteration of the loop are updated according to whether each observation was correctly classified using <span class="strong"><em class="calibre9">step 5</em></span> of the AdaBoost algorithm. The weight vector is then normalized and we are ready to begin the next iteration. After completing <span class="strong"><em class="calibre9">M</em></span> iterations, we output a list of models and their corresponding model weights.</p><p class="calibre8">We now have a function that is able to train our ensemble classifier using AdaBoost, but we also need a function to make predictions. This function will take in the output list produced by our training function, <code class="email">AdaBoostNN()</code>, along with a test dataset. We've called this function <code class="email">AdaBoostNN.predict()</code> and it is shown here:</p><div class="informalexample"><pre class="programlisting">AdaBoostNN.predict &lt;- function(ada_model, test_data) {
  models &lt;- ada_model$models
  alphas &lt;- ada_model$alphas
  prediction_matrix &lt;- sapply(models, function (x) 
             as.numeric(predict(x, test_data, type = "class")))
  weighted_predictions &lt;- t(apply(prediction_matrix, 1, 
             function(x) mapply(function(y, z) y * z, x, alphas)))
  final_predictions &lt;- apply(weighted_predictions, 1, function(x) sign(sum(x)))
  return(final_predictions)
}</pre></div><p class="calibre8">In this function, we first extract the models and the model weights from the list produced by our <a id="id703" class="calibre1"/>previous function. We then create a matrix of predictions, where each column corresponds to the vector of predictions made by a particular model. Thus, we will have as many columns in this matrix as models that we used for boosting.</p><p class="calibre8">We then multiply the predictions produced by each model with their corresponding model weight. For example, every prediction from the first model is in the first column of the prediction matrix and will have its value multiplied by the first model weight <span class="strong"><em class="calibre9">α1</em></span>. Finally, in the last step, we reduce our matrix of weighted observations into a single vector of observations by summing the weighted predictions for each observation and taking the sign of the result. This vector of predictions is then returned by our function.</p><p class="calibre8">As an experiment, we will train 10 neural network models with a single hidden unit and see if boosting improves accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; ada_model &lt;- AdaBoostNN(magic_train_df_pp, 'CLASS', 10, 1)
&gt; predictions &lt;- AdaBoostNN.predict(ada_model, magic_test_pp, 
                                    'CLASS')
&gt; mean(predictions == magic_test_output)
 [1] 0.804365</pre></div><p class="calibre8">Boosting 10 models seems to give us a marginal improvement in accuracy, but perhaps training more models might make more of a difference. We are also interested in the relationship between the complexity of our weak learner, as measured by the number of hidden neurons, and the performance benefits we can expect from boosting on this dataset. </p><p class="calibre8">The following plot shows the results of experimenting with our functions using different numbers of models as well as hidden neurons:</p><div class="mediaobject"><img src="../images/00165.jpeg" alt="Predicting atmospheric gamma ray radiation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For the neural networks with one hidden unit, as the number of boosting models increases, we see an improvement in accuracy, but after 100 models, this tapers off and is actually slightly <a id="id704" class="calibre1"/>less for 200 models. The improvement over the baseline of a single model is substantial for these networks. When we increase the complexity of our learner by having a hidden layer with three hidden neurons, we get a much smaller improvement in performance. At 200 models, both ensembles perform at a similar level, indicating that at this point, our accuracy is being limited by the type of model we are training.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note35" class="calibre1"/>Note</h3><p class="calibre8">The original AdaBoost algorithm was presented by <span class="strong"><em class="calibre9">Freund</em></span> and <span class="strong"><em class="calibre9">Schapire</em></span> in the journal of <span class="strong"><em class="calibre9">Computer and System Sciences</em></span> in a 1997 paper titled <span class="strong"><em class="calibre9">A Decision-theoretic generalization of on-line learning and an application to boosting</em></span>. This is a good place to start learning more about AdaBoost.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Predicting complex skill learning with boosting"><div class="book" id="26I9K2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec67" class="calibre1"/>Predicting complex skill learning with boosting</h1></div></div></div><p class="calibre8">We will <a id="id705" class="calibre1"/>revisit our Skillcraft dataset in this section--this time in the context of another boosting technique known as <span class="strong"><strong class="calibre2">stochastic gradient boosting</strong></span>. The main characteristic of this method is that in every iteration <a id="id706" class="calibre1"/>of boosting, we compute a gradient in the direction of the errors that are made by the model trained in the current iteration.</p><p class="calibre8">This gradient is then used in order to guide the construction of the model that will be added in the next iteration. Stochastic gradient boosting is commonly used with decision trees, and a good implementation in R can be found in the <code class="email">gbm</code> package, which provides us with the <code class="email">gbm()</code> function. For regression problems, we need to specify the <code class="email">distribution</code> parameter to be <code class="email">gaussian</code>. In addition, we can specify the number of trees we want to build (which is equivalent to the number of iterations of boosting) via the <code class="email">n.trees</code> parameter, as well as a <code class="email">shrinkage</code> parameter that is used to control the algorithm's learning rate:</p><div class="informalexample"><pre class="programlisting">&gt; boostedtree &lt;- gbm(LeagueIndex ~ ., data = skillcraft_train, 
  distribution = "gaussian", n.trees = 10000, shrinkage = 0.1)</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note36" class="calibre1"/>Note</h3><p class="calibre8">To learn more about how stochastic gradient boosting works, a good source to consult is the paper titled <span class="strong"><em class="calibre9">Stochastic Gradient Boosting</em></span>. This was written by <span class="strong"><em class="calibre9">Jerome H. Friedman</em></span> and appears in the February 2002 issue of the journal <span class="strong"><em class="calibre9">Computational Statistics &amp; Data Analysis</em></span>.</p></div><p class="calibre8">In order to make predictions with this setup, we need to use the <code class="email">gbm.perf()</code> function, whose job it is to take the boosted model we built and pick out the optimal number of boosting iterations. We can then provide this to our <code class="email">predict()</code> function in order to make predictions on our test data. To measure the SSE on our test set, we will use the <code class="email">compute_SSE()</code> function that we wrote in <a class="calibre1" title="Chapter 6. Support Vector Machines" href="part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7">Chapter 6</a>, <span class="strong"><em class="calibre9">Tree-based Methods</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt; best.iter &lt;- gbm.perf(boostedtree, method = "OOB")
&gt; boostedtree_predictions &lt;- predict(boostedtree, 
                                     skillcraft_test, best.iter)
&gt; (boostedtree_SSE &lt;- compute_SSE(boostedtree_predictions, 
                                  skillcraft_test$LeagueIndex))
[1] 555.2997</pre></div><p class="calibre8">A bit of experimentation has revealed that we can't get substantially better results than this by allowing the algorithm to iterate over more trees. Despite this, we are already performing better using this method than both the single and bagged tree classifiers.</p></div>

<div class="book" title="Predicting complex skill learning with boosting">
<div class="book" title="Limitations of boosting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec85" class="calibre1"/>Limitations of boosting</h2></div></div></div><p class="calibre8">Boosting <a id="id707" class="calibre1"/>is a very powerful technique that continues to receive a lot of attention and research, but it is not without its limitations. Boosting relies on combining weak learners together. In particular, we can expect to get the most out of boosting when the models that are used are not already complex models themselves. We already saw an example of this with neural networks, by noting that the more complex architecture of three hidden neurons gives a better learner to begin with than the simpler architecture of a single hidden neuron.</p><p class="calibre8">Combining weak learners may be a way to reduce overfitting, but this is not always effective. By default, boosting uses all of its training data and progressively tries to correct mistakes that it makes without any penalizing or shrinkage criterion (although the individual models trained may themselves be regularized). Consequently, boosting can sometimes overfit.</p><p class="calibre8">Finally, a very important limitation is that many boosting algorithms have a symmetric loss function. Specifically, there is no distinction that is made in classification between a false positive classification error and a false negative classification error. Every type of error is treated <a id="id708" class="calibre1"/>the same when the observation weights are updated.</p><p class="calibre8">In practice, this might not be desirable, in that one of the two errors may be more costly. For example, on the website for our <span class="strong"><strong class="calibre2">Major Atmospheric Gamma Imaging Cherenkov</strong></span> (<span class="strong"><strong class="calibre2">MAGIC</strong></span>) telescope dataset, the authors state that a false positive of detecting gamma rays where there are none, is worse than a false negative of misclassifying gamma rays as background radiation. Cost-sensitive extensions of boosting algorithms have been proposed, however.</p><div class="book" title="Random forests"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch09lvl2sec0000000067" class="calibre1"/>Random forests</h3></div></div></div><p class="calibre8">The final <a id="id709" class="calibre1"/>ensemble model that we will discuss in this chapter is unique to tree-based models and is known as the <span class="strong"><strong class="calibre2">random forest</strong></span>. In a nutshell, the idea behind random forests stems from an observation on bagging trees. Let's suppose that the actual relationship between the features and the target variable can be adequately described with a tree structure. It is quite likely that during bagging with moderately sized bootstrapped samples, we will keep picking the same features to split on high up in the tree.</p><p class="calibre8">For example, in our Skillcraft dataset, we expect to see <code class="email">APM</code> as the feature that will be chosen at the top of most of the bagged trees. This is a form of tree correlation that essentially impedes our ability to derive the variance reduction benefits from bagging. Put differently, the different tree models that we build are not truly independent of each other because they will have many features and split points in common. Consequently, the averaging process at the end will be less successful in reducing the ensemble variance.</p><p class="calibre8">To counteract this effect, the random forest algorithm introduces an element of randomization in the tree construction process. Just as with bagging, random forests involve building a number of trees with bootstrapped samples and using the average of their predictions to form the ensemble prediction. When we construct individual trees, however, the random forest algorithm imposes a constraint.</p><p class="calibre8">At each node in the tree, we draw a random sample of size <span class="strong"><em class="calibre9">mtry</em></span> from the total number of input features. Whereas in regular tree construction, we consider all the features at each node to determine which one to split on, with random forests, we only consider features from the sample we created for that node. We can often use a relatively small number for <span class="strong"><em class="calibre9">mtry</em></span>.</p><p class="calibre8">The number of trees we build, in combination with the fact that each tree has several nodes, is often enough to ensure that the more important features are sampled a sufficient number of times. Various heuristics have been proposed for choosing appropriate values for this parameter, such as one third or the square root of the total number of features available.</p><p class="calibre8">This <a id="id710" class="calibre1"/>sampling step effectively forces the structure of the bagged trees to be different from each other and offers a number of different benefits. Feature sampling allows us to consider input features that are successful in splitting the data for only a small range of the target variable. These locally relevant features are rarely chosen without the sampling constraint because we usually prefer features that form good overall splits of the data at a given node in the tree. Nonetheless, we may want to include these features in our model if we don't want to overlook local variations in the output.</p><p class="calibre8">Similarly, sampling input features is useful when we have correlated input features. Regular tree construction tends to favor only one of the features from a correlated set while ignoring the rest despite the fact that the resulting splits from even highly correlated features are not exactly the same. When we sample input features we are less likely to have correlated features compete with each other and so we can choose a wider range of features to use with our model.</p><p class="calibre8">In general, the randomized nature of the sampling process is designed to combat overfitting because we can think of this process as applying regularization on the impact of each input feature. Overfitting can still be a problem if we happen to have too many input features unrelated to the target variable compared to those that are related, but this is a fairly rare scenario. Random forests in general scale quite favorably with the number of input features, precisely because of this sampling process that doesn't require us to consider all the features when splitting at each node. In particular, this model is a good choice when the number of features exceeds the number of observations. Finally, the sampling process mitigates the cost of constructing a large number of trees again because we consider a subset of input features when deciding on how to split at each node. The number of trees is another tuning parameter that we must decide on in a random forest model; it is very common to build anywhere between several hundred and a few thousand trees.</p><p class="calibre8">In R, we can use the <code class="email">randomForest</code> package in order to train random forest models. The <code class="email">randomForest()</code> function takes in a formula and a training data frame, as well as a number of other optional parameters. Of particular interest is the <code class="email">ntree</code> parameter, which controls the number of trees that will be built for the ensemble, and the <code class="email">mtry</code> parameter, which is the number of features sampled for use at each node for splitting. These parameters should be tuned by trying out different configurations, and we can use the <code class="email">tune()</code> function from the <code class="email">e1071</code> package to do just that:</p><div class="informalexample"><pre class="programlisting">&gt; library("randomForest")
&gt; library("e1071")
&gt; rf_ranges &lt;- list(ntree = c(500, 1000, 1500, 2000), mtry = 3:8)
&gt; rf_tune &lt;- tune(randomForest, LeagueIndex ~ ., data = 
                  skillcraft_train, ranges = rf_ranges)
&gt; rf_tune$best.parameters
   ntree mtry
14  1000    6
&gt; rf_best &lt;- rf_tune$best.model
&gt; rf_best_predictions &lt;- predict(rf_best, skillcraft_test)
&gt; (rf_best_SSE &lt;- compute_SSE(rf_best_predictions, 
                              skillcraft_test$LeagueIndex))
[1] 555.7611</pre></div><p class="calibre8">The results <a id="id711" class="calibre1"/>show that on this dataset, the best parameter combination is to train 1,000 trees and use a value of 6 for <code class="email">mtry</code>. This last value corresponds to one third of the number of input features, which is the typical heuristic for regression problems. The SSE value on our test set is almost identical to what we obtained using gradient boosting.</p></div></div></div>

<div class="book" title="Predicting complex skill learning with boosting">
<div class="book" title="The importance of variables in random forests"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec86" class="calibre1"/>The importance of variables in random forests</h2></div></div></div><p class="calibre8">We already <a id="id712" class="calibre1"/>discussed the fact that ensemble models do not, in general, have explanative power. For random forests, it turns out that we can still measure variable importance scores for the different input features by tallying and keeping track of the reductions in our error function across all the trees in the ensemble. In this way, we can obtain an analogous plot to the one we obtained for a single tree when we looked at this dataset in <a class="calibre1" title="Chapter 6. Support Vector Machines" href="part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7">Chapter 6</a>, <span class="strong"><em class="calibre9">Tree-based Methods</em></span>.</p><p class="calibre8">To compute variable importance, we use the <code class="email">importance()</code> function and plot the results:</p><div class="mediaobject"><img src="../images/00166.jpeg" alt="The importance of variables in random forests" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Looking at this plot, we can see that <code class="email">APM</code> and <code class="email">ActionLatency</code> are once again the most important features, but their order is reversed. We also see that <code class="email">TotalHours</code> is now third in importance, significantly higher than what we saw before in a single tree.</p><p class="calibre8">We have <a id="id713" class="calibre1"/>explored the Skillcraft dataset using a number of different methods, but each time we treated this as a regression problem and measured our accuracy using the SSE. Our target variable is the league index, which tells us the gaming league in which a player competes. As such, it is actually an ordered factor.</p><p class="calibre8">As we've already seen before, models whose output is an ordered factor can be tricky to train as well as assess. For example, perhaps a more appropriate method of assessing our model would be to first round our model's numerical output so that we obtain a prediction on an actual player league. Then, we could assess the model using a weighted classification error rate that more heavily penalizes a predicted league index that is very far from the actual league index. We leave this as an exercise for the reader.</p><p class="calibre8">One of the issues that we often face when we model the problem as a regression problem is that we <a id="id714" class="calibre1"/>have no way to force the output to predict across the full range of the original levels. In our particular dataset, for example, we might never predict the lowest or highest league. For some suggestions on alternative ways to model ordered factors with regression trees, there is an insightful paper published in 2000 by <span class="strong"><em class="calibre9">Kramer</em></span> and others, titled <span class="strong"><em class="calibre9">Prediction of Ordinal Classes Using Regression Trees</em></span>. This appears in the <span class="strong"><em class="calibre9">34th</em></span> issue of <span class="strong"><em class="calibre9">Fundamentals Informaticae</em></span> by <span class="strong"><em class="calibre9">IOS Press</em></span>.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note37" class="calibre1"/>Note</h3><p class="calibre8">For random forests, the original reference is a 2001 paper by <span class="strong"><em class="calibre9">Leo Breiman</em></span>, titled <span class="strong"><em class="calibre9">Random Forests</em></span>, published in the journal <span class="strong"><em class="calibre9">Machine Learning</em></span>. Besides this reference, a fantastic chapter with numerous examples appears in the book <span class="strong"><em class="calibre9">Statistical Learning from a Regression Perspective</em></span>, <span class="strong"><em class="calibre9">Richard A. Derk</em></span>, <span class="strong"><em class="calibre9">Springer</em></span>.</p></div></div></div>

<div class="book" title="Predicting complex skill learning with boosting">
<div class="book" title="XGBoost"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec87" class="calibre1"/>XGBoost</h2></div></div></div><p class="calibre8">Along the <a id="id715" class="calibre1"/>same vein as AdaBoost, and with yet another twist on gradient boosting, is <span class="strong"><strong class="calibre2">Extreme Gradient Boosting</strong></span> (<span class="strong"><strong class="calibre2">XGBoost</strong></span>). XGBoost <a id="id716" class="calibre1"/>is a library of functions designed and optimized specifically for boosting trees algorithms. It is a generally advanced tool kit that yields impressive results, but does takes some time to understand.</p><p class="calibre8">XGBoost is based upon the quite well known gradient boosting framework, but it is more efficient. Specifically, XGBoost leverages system optimization concepts such as out of core computations, parallelization, cache optimization, and distributed computing to create a faster and more flexible tool for learning tree ensembles.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note38" class="calibre1"/>Note</h3><p class="calibre8">Basically, XGBoost is built to perform the sequential process of <span class="strong"><em class="calibre9">boosting</em></span> by utilizing all cores of a machine as it recursively divides data into parts, retaining the first part to be used as the test data, then reintegrating the first part to the dataset and retaining the second part, do a training and repeat, and so on.</p></div><p class="calibre8">In addition, XGBoost has many parameters that can be customized and is extendable and therefore is much more flexible. Other advantages offered by XGBoost include regularization, customizable parameters, deeper tree pruning, and built-in cross validation.</p></div></div>
<div class="book" title="Summary" id="27GQ61-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec68" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">In this chapter, we deviated from our usual pattern of learning a new type of model and instead focused on techniques to build ensembles of models that we have seen before. We discovered that there are numerous ways to combine models in a meaningful way, each with its own advantages and limitations. Our first technique for building ensemble models was bagging. The central idea behind bagging is that we build multiple versions of the same model using bootstrap samples of the training data. We then average the predictions made by these models in order to construct our overall prediction. By building many different versions of the model we can smooth out errors made due to overfitting and end up with a model that has reduced variance.</p><p class="calibre8">A different approach to building model ensembles uses all of the training data and is known as boosting. Here, the defining characteristic is to train a sequence of models, but each time we weigh each observation with a different weight depending on whether we classified that observation correctly in the previous model. There are many variants of boosting and we presented two of the most well-known algorithms, AdaBoost and stochastic gradient boosting (as well as mentioning perhaps the newer, more efficient tree learner, XGBoost). The averaging process that operates over the predictions made by individual models to compute the final prediction often weighs each model by its performance.</p><p class="calibre8">Traditional texts that present bagging and boosting introduce them in the context of decision trees. There is good reason for this, as the decision tree is the prototypical model for which bagging and boosting have been applied. Boosting in particular works best on models that are weak learners and decision trees can easily be made into weak learners by significantly restricting their size and complexity during construction.</p><p class="calibre8">At the same time, however, this often leaves the reader with a view that ensemble methods only work for decision trees, or without any experience in how they can be applied to other methods. In this chapter, we emphasized how these techniques are general and how they can be used with a number of different types of models. Consequently, we applied these techniques to models that we have seen before, such as neural networks and logistic regression.</p><p class="calibre8">The final type of ensemble model that we studied was the random forest. This is a very popular and powerful algorithm based on bagging decision trees. The key breakthrough behind this model is the use of an input feature sampling procedure, which limits the choice of features that are available to split on during the construction of each tree. In doing this, the model reduces the correlation between trees, captures significant localized variations in the output, and improves the degree of variance reduction in the final result. Another key benefit of this model is that it scales well with a larger number of input features. For our real-world Skillcraft dataset, we discovered that random forests and stochastic gradient boosting produced the best performance.</p><p class="calibre8">In the next chapter, we will introduce another type of model with a distinct structure known as the probabilistic graphical model. These models use a graphical structure in order to explicitly represent the conditional independence between input features. Probabilistic graphical models find applications across a wide variety of predictive tasks from spam email identification to DNA sequence labeling.</p></div></body></html>