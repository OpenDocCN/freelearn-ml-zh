<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Machine Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's begin our journey by peering into the future and envision how we'll see ourselves interacting with computers. Unlike today's computers, where we are required to continuously type in our emails and passwords to access information, the computers of the future will <span>easily be able to recognize us by our face, voice, or activity.</span> <span>Unlike today's computers, which require step-by-step instructions to perform an action, the computer of the future </span>will anticipate our intent and provide a natural way for us to converse with it, similar to how we engage with other people, and then proceed to help us achieve our goal. Our computer will not only assist us but also be our friend, our doctor, and so on. It could deliver our groceries at the door and be our interface with an increasingly complex and information-rich physical world.</p>
<p>What is exciting about this vision is that it is no longer in the realm of science fiction but an emergent reality. One of the major drivers of this is the progress and adoption of <strong>machine learning</strong> (<strong>ML</strong>) techniques, a discipline that gives computers the perceptual power of humans, thus giving them the ability to see, hear, and make sense of the world—physical and digital.</p>
<p>But despite all the great progress over the last 3-4 years, most of the ideas and potential are locked away in research projects and papers rather than being in the hands of the user. So it's the aim of this book to help developers understand these concepts <span>better. </span>It will enable you to put them into practice so that we can arrive at this future—a future where computers augment us, rather than enslave us due to their inability to understand our world.</p>
<p>Because of the constraint of Core ML—it being only able to perform inference<span>—</span>this book differs vastly from other ML books, in the sense that the core focus is on the application of ML. Specifically we'll focus on computer vision applications rather than the details of ML. But in order to better enable you to take full advantage of ML, we will spend some time introducing the associated concepts with each example. </p>
<p>And before jumping into the hands-on examples, let's start from the beginning and build an appreciation for what ML is and how it can be applied. In this chapter we will:</p>
<ul>
<li><span>Start by i</span>ntroducing ML. We'll learn how it differs from classical programming and why you might choose it.</li>
<li>Look at some examples of how ML is being used today, along with the type of data and ML algorithm being used.</li>
<li>Finally, present the typical workflow for ML projects.</li>
</ul>
<p>Let's kick off by first discussing what ML is and why everyone is talking about it. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is machine learning?</h1>
                </header>
            
            <article>
                
<p>ML is a subfield of <strong>Artificial Intelligence</strong> (<strong>AI</strong>), a topic of computer science born in the 1950s with the goal of trying to get computers to think or provide a level of automated intelligence similar to that of us humans. </p>
<p><span>Early success in AI was achieved by using an extensive set of defined rules, known as <strong>symbolic AI</strong>, allowing expert decision making to be mimicked by computers. </span>This approach worked well for many domains but had a big shortfall in that in order to create an expert, you needed one. Not only this, but also their expertise needed to be digitized somehow, which normally required explicit programming. </p>
<p>ML provides an alternative; instead of having to handcraft rules, it learns from examples and experience. It also differs from classical programming in that it is probabilistic as opposed to being discrete. That is, it is able to handle fuzziness or uncertainty much better than its counterpart, which will likely fail when given an ambiguous input that wasn't explicitly identified and handled. </p>
<p>I am going to borrow an <span>example used by Google engineer Josh Godron in an introductory video to ML to</span> better highlight the differences and value of ML.</p>
<p><span>Suppose you were given the task of classifying apples and oranges. Let's first approach this using what we will call </span><span>classical programming:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/3089eaf2-9ce4-4d02-b02b-c428c5edfb14.png" style="width:40.17em;height:18.58em;"/></div>
<p>Our input is an array of pixels for each image, and for each input, we will need to explicitly define some rules that will be able to distinguish an apple from an orange. Using the preceding examples, you can solve this by simply counting the number of orange and green pixels. Those with a higher ratio of green pixels would be classified as an apple, while those with a higher ratio of orange pixels would be classified as an orange. This works well with these examples but breaks if our input becomes more complex:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/77a2450a-0737-42d0-8f01-32e032465cee.png" style="width:38.25em;height:22.42em;"/></div>
<p>The introduction of new images means our simple color-counting function can no longer sufficiently differentiates our apples from our oranges, or even classify apples. We are required to reimplement the function to handle the new nuances introduced. As a result, our function grows in complexity and becomes more tightly coupled to the inputs and less likely able to generalize to other inputs. Our function might resemble something like the following:</p>
<pre>func countColors(_ image:UIImage) -&gt; [(color:UIColor, count:Int)]{<br/>// <span>lots of code</span> <br/>}<br/><br/>func detectEdges(_ image:UIImage) -&gt; [(x1:Int, y1:<span>Int</span><span>, x2:</span><span>Int</span><span>, y2:</span><span>Int</span><span>)]<br/>{<br/></span>// lots of code<br/>}<br/><br/>func analyseTexture(_ image:UIImage) -&gt; [String]<br/>{<br/>// lots of code <br/>} <br/><br/>func fitBoundingBox(_ image:UIImage) -&gt; [(x:Int, y:Int, w:Int, h:Int)]<br/>{<br/>// lots of code <br/>}</pre>
<p><span>This function can be considered our model, which models the relationship of the inputs with respect to their labels (apple or orange), as illustrated in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/53712b8b-a731-4fc0-aaa5-093cb2abe92e.png" style="width:18.08em;height:3.50em;"/></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span> </span>The alternative, and the approach we're interested in, is getting this model created to automatically use examples; this, in essence, is what ML is all about. It provides us with an effective tool to model complex tasks that would otherwise be nearly impossible to define by rules. </p>
<p>The creation phase of the ML model is called <strong>training</strong> and is determined by the type of ML algorithm selected and data being fed. Once the model is trained, that is, once it has learned, we can use it to make inferences from the data, as illustrated in the following diagram: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/af1d0e57-9687-4f78-9474-aaac5bf93221.png" style="width:31.42em;height:4.33em;"/></div>
<p>The example we have presented here, classifying oranges and apples, is a specific type of ML algorithm called a <strong>classifier</strong>, or, more specifically, a multi-class classifier. The model was trained through <strong>supervision</strong>; that is, we fed in examples of input with their associated labels (or classes). It is useful to understand the types of ML algorithms that exist along with the types of training, which is the topic of the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief tour of ML algorithms</h1>
                </header>
            
            <article>
                
<p>In this section, we will look at some examples of how ML is used, and with each example, we'll speculate about the type of data, learning style, and ML algorithm used. I hope that by the end of this section, you will be inspired by what is possible with ML and gain some appreciation for the types of data, algorithms, and learning styles that exist. </p>
<div class="packt_infobox">In this section, we will be presenting some real-life examples in the context of introducing types of data, algorithms, and learning styles. It is <strong>not </strong>our intention to show accurate data representations or implementations for the example, but rather use the examples as a way of making the ideas more tangible. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Netflix – making recommendations </h1>
                </header>
            
            <article>
                
<p>No ML book is complete without mentioning recommendation engines—probably one of the most well known applications of ML. In part, this is thanks to the publicity gained when Netflix announced a $1 million competition for movie rating predictions, also known as <strong>recommendations</strong>. Add to this Amazon's commercial success in making use of it. </p>
<p>The goal of recommendation engines is to predict the likelihood of someone wanting a particular product or service. In the context of Netflix, this would mean recommending movies or TV shows to its users.</p>
<p>One intuitive way of making recommendations is to try and mimic the real world, where a person is likely to seek recommendations from like-minded people. What constitutes likeness is dependent on the domain. For example, you are most likely to have one <span>group</span> of friends that you would ask for restaurant recommendations and another group of friends for movie recommendations. What determines these groups is how similar their tastes are to your own taste for that particular domain. We can replicate this using the (user-based) <strong>Collaborative Filtering</strong> (<strong>CF</strong>) algorithm. This algorithm achieves this by finding the distance between each user and then using these distances as a similarity metric to infer predictions on movies for a particular user; that is, those that are more similar will contribute more to the prediction than those that have different preferences. Let's have a look at what form the data might take from Netflix: </p>
<table style="width: 100%" border="1">
<tbody>
<tr style="background-color: #d1d1d1">
<td style="width: 57px"><strong>User</strong></td>
<td style="width: 140px"><strong>Movie</strong></td>
<td style="width: 68px"><strong>Rating</strong></td>
</tr>
<tr>
<td style="width: 57px">0: Jo</td>
<td style="width: 140px">A: Monsters Inc</td>
<td style="width: 68px">5</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">B: The Bourne Identity</td>
<td style="width: 68px">2</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">C: The Martian</td>
<td style="width: 68px">2</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">D: Blade Runner</td>
<td style="width: 68px">1</td>
</tr>
<tr>
<td style="width: 57px">1: Sam</td>
<td style="width: 140px">C: The Martian</td>
<td style="width: 68px">4</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">D: Blade Runner</td>
<td style="width: 68px">4</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">E: The Matrix </td>
<td style="width: 68px">4</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">F: Inception</td>
<td style="width: 68px">5</td>
</tr>
<tr>
<td style="width: 57px">2: Chris</td>
<td style="width: 140px">B: The Bourne Identity</td>
<td style="width: 68px">4</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">C: The Martian</td>
<td style="width: 68px">5</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">D: Blade Runner</td>
<td style="width: 68px">5</td>
</tr>
<tr>
<td style="width: 57px"/>
<td style="width: 140px">F: Inception</td>
<td style="width: 68px">4</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For each example, we have a user, a movie, and an assigned rating. To find the similarity between each user, we can first calculate the Euclidean distance of the shared movies between each pair of users. The Euclidean distance gives us larger values for users who are most dissimilar; we invert this by dividing 1 by this distance to give us a result, where 1 represents perfect matches and 0 means the users are most dissimilar. The following is the formula for Euclidean distance and the function used to calculate similarities between two users:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b1e4ef9c-d020-4548-a992-271c303da8eb.png" style="width:21.17em;height:5.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Equation for Euclidian distance and similarity </div>
<pre>func calcSimilarity(userRatingsA: [String:Float], userRatingsB:[String:Float]) -&gt; Float{<br/>  var distance = userRatingsA.map( { (movieRating) -&gt; Float in <br/>    if userRatingsB[movieRating.key] == nil{<br/>      return 0 <br/>    }<br/>    let diff = movieRating.value - (userRatingsB[movieRating.key] ?? 0)<br/>    return diff * diff<br/>  }).reduce(0) { (prev, curr) -&gt; Float in <br/>    return prev + curr<br/>  }.squareRoot()<br/>  return 1 / (1 + distance)<br/>}</pre>
<p>To make this more concrete, let's walk through how we can find the most similar user for Sam, who has rated the following movies: <kbd>["The Martian" : 4, "Blade Runner" : 4, "The Matrix" : 4, "Inception" : 5]</kbd>. Let's now calculate the similarity between Sam and Jo and then <span>between </span>Sam and Chris. </p>
<p><strong>Sam and Jo </strong></p>
<p>Jo has rated the movies <kbd>["Monsters Inc." : 5, "The Bourne Identity" : 2, "The Martian" : 2, "Blade Runner" : 1]</kbd>; by calculating the <span>similarity</span> of intersection of the two sets of ratings for each user, we get a value of <em>0.22</em>.</p>
<p><strong>Sam and Chris </strong></p>
<p>Similar to the previous ones, but now, by calculating the <span>similarity</span> using the movie ratings from Chris (<kbd>["The Bourne Identity" : 4, "The Martian" : 5, "Blade Runner" : 5, "Inception" : 4]</kbd>), we get a value of <em>0.37</em>.</p>
<p>Through manual inspection, we can see that Chris is more similar to Sam than Jo is, and our similarity rating shows this by giving Chris a higher value than Jo.</p>
<p>To help illustrate why this works, let's project the ratings of each user onto a chart as shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed056a0d-b014-43ca-a054-711518fa3d6e.png" style="width:24.08em;height:24.08em;"/></div>
<p><span>The preceding graph shows the users plotted in a preference space; the closer two users are in this preference space, the more similar their preferences are. Here, we are just showing two axes, but, as seen in the preceding table, this extends to multiple dimensions. </span></p>
<p>We can now use these similarities as weights that contribute to predicting the rating a particular user would give to a particular movie. Then, using these predictions, we can recommend some movies that a user is likely to want to watch.</p>
<p>The preceding approach is a type of <strong>clustering</strong> algorithm that falls under <strong>unsupervised learning</strong>,<strong> </strong>a learning style where examples have no associated label and the job of the ML algorithm is to find patterns within the data. Other common unsupervised learning algorithms include the Apriori algorithm (basket analysis) and K-means.</p>
<p>Recommendations are applicable anytime when there is an abundance of <span>information</span> that can benefit from being filtered and ranked before being presented to the user. Having recommendations performed on the device offers many benefits, such as being able to incorporate the context of the user when filtering and ranking the results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shadow draw – real-time user guidance for freehand drawing</h1>
                </header>
            
            <article>
                
<p>To highlight the synergies between man and machine, AI is sometimes referred to as <strong>Augmented Intelligence</strong> (<strong>AI</strong>), putting the emphasis on the system to augment our abilities rather than replacing us altogether.</p>
<p>One area that is <span>becoming increasingly popular—and</span> of particular interest to myself<span>—</span>is assisted creation systems, an area that sits at the <span>intersection of the fields of <strong>human-computer interaction</strong> (<strong>HCI</strong>) and ML. These are systems created to assist in some creative tasks such as drawing, writing, video, and music. </span></p>
<p>The example we will discuss in this section is shadow draw, a research project undertaken at Microsoft in 2011 by Y.J. Lee, L. Zitnick, and M. Cohen. Shadow draw is a system that assists the user in drawing by matching and aligning a reference image from an existing dataset of objects and then lightly rendering shadows in the background to be used as guidelines for the user. For example, if the user is predicted to be drawing a bicycle, then the system would render guidelines under the user's pen to assist them in drawing the object, as illustrated in this diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d2c2bac9-45d4-4fc1-84dd-0b7bc23a5b9b.png" style="width:40.25em;height:21.42em;"/></div>
<p>As we did before, let's walk through how we might approach this, focusing specifically on classifying the sketch; that is, we'll predict what object the user is drawing. This will give us the opportunity to see new types of data, algorithms, and applications of ML.</p>
<p>The dataset used in this project consisted of 30,000 natural images collected from the internet via 40 category queries such as face, car, and bicycle, with each category stored in its own directory; the following diagram shows some examples of these images:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5631134e-855f-4119-ae0b-cc81c8c02b7a.png" style="width:35.75em;height:8.92em;"/></div>
<p>After obtaining the raw data, the next step, and typical of any ML project, is to perform <strong>data preprocessing</strong> and <strong>feature engineering</strong>. The following diagram shows the preprocessing steps, which consist of:</p>
<ul>
<li>Rescaling each image</li>
<li>Desaturating (turning black and white)</li>
<li>Edge detection</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aa7c256b-44f0-4bb4-94ae-d130d75ce389.png" style="width:36.58em;height:21.42em;"/></div>
<p>Our next step is to abstract our data into something more meaningful and useful for our ML algorithm to work with; this is known as <strong>feature engineering</strong>, and is a critical step in a typical ML workflow. </p>
<p>One approach, and the approach we will describe, is creating something known as a <strong>visual bag of words</strong>. This is essentially a histogram of features (visual words) used to describe each image, and collectively to describe each category. What constitutes a feature is dependent on the data and ML algorithm; for example, we can extract and count the colors of each image, where the colors become our features and collectively describe our image, as shown in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f6379527-35b9-4262-ab86-4752edf3f8c4.png" style="width:36.83em;height:14.67em;"/></div>
<p>But because we are dealing with sketches, we want something fairly coarse—something that can capture the general strokes directions that will encapsulate the general structure of the image. For example, if we were to describe a square and a circle, the square would consist of horizontal and vertical strokes, while the circle would consist mostly of diagonal strokes. To extract these features, we can use a computer vision algorithm called <strong>histogram of oriented gradients</strong> (<strong>HOG</strong>); after processing an image you are returned a histogram of gradient orientations in localized portions of the image. Exactly what we want! To help illustrate the concept, this process is summarized for a single image here:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ad5f2e2c-230f-4e80-8bce-f3c206c39dee.png" style="width:34.67em;height:15.08em;"/> </div>
<p>After processing all the images in our dataset, our next step is to find a histogram (or histograms) that can be used to identify each <span>category;</span> we can use an <strong>unsupervised learning</strong> <span>clustering </span>technique called <strong>K-means</strong>, where each category histogram is the centroid for that cluster. The following diagram describes this process; we first extract features for each image and then cluster these using K-means, where the distance is calculated using the histogram of gradients. Once our images have been clustered into their groups, we extract the center (mean) histogram of each of these groups to act as our category descriptor: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/23029d2b-f64b-4b1e-9baa-114ccdd35766.png" style="width:44.92em;height:25.92em;"/></div>
<p>Once we have obtained a histogram for each category (codebook), we can train a <strong>classifier</strong>  using each image's extracted features (visual words) and the associated category (label). One popular and effective classifier is <strong>support vector machines</strong> (<strong>SVM</strong>). What SVM tries to find <span>is </span>a hyperplane that best separates the categories; here, <em><span>best </span></em>refers to a plane that has the largest distance between each of the category members. The term <em>hyper</em> is used because it transforms the vectors into high-dimensional space such that the categories can be separated with a linear plane (plane because we are working within a space). The following diagram shows how this may look for two categories in a two-dimensional space:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/2fb64dc6-ac0a-4f1a-903d-b310f0e32389.png" style="width:26.00em;height:24.25em;"/></div>
<p>With our model now trained, we can perform real-time classification on the image as the user is drawing, thus allowing us to assist the user by providing them with guidelines for the object they are wanting to draw (or at least, mention the object we predicted them to be drawing). Perfectly suited for touch interfaces such as your iPhone or iPad! This assists <span>not just </span>in drawing applications, but anytime where an input is required by the user, such as image-based searching or note taking. </p>
<p>In this example, we showed how feature engineering and <strong>unsupervised learning</strong> are used to augment data, making it easier for our model to sufficiently perform <strong>classification</strong> using the <strong>supervised learning</strong> algorithm SVM. Prior to deep neural networks, feature engineering was a critical step in ML and sometimes a limiting factor for these reasons:</p>
<ul>
<li>It required special skills and sometimes domain expertise</li>
<li>It was at the mercy of a human being able to find and extract meaningful features</li>
<li>It required that the features extracted would generalize across the population, that is, be expressive enough to be applied to all examples</li>
</ul>
<p>In the next example, we introduce a type of <span>neural network called a <strong>convolutional neural network</strong> (<strong>CNN</strong> or <strong>ConvNet</strong>), which takes care of a lot of the feature engineering itself. </span></p>
<div class="packt_infobox">The paper describing the actual project and approach can be found here: <a href="http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html">http://vision.cs.utexas.edu/projects/shadowdraw/shadowdraw.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Shutterstock – image search based on composition</h1>
                </header>
            
            <article>
                
<p>Over the past 10 years, we have seen an explosive growth in visual content created and consumed on the Web, but before the success of CNNs, images were found by performing simple keyword searches on the tags <span>assigned manually</span>. All t<span>his </span>changed around 2012, when A. Krizhevsky, I. Sutskever, and G. E. Hinton published their paper <span><em>ImageNet Classification with Deep Convolutional Networks</em>. The paper described their architecture used to win the 2012 <strong>ImageNet Large-Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>). It's a competition like the Olympics of computer vision, where teams compete across a range of CV tasks such as classification, detection, and object localization. And that was the first year a CNN gained the top position with a test error rate of 15.4% (the next best entry achieved an test error rate of 26.2%). Ever since then, CNNs have become the de facto approach for computer vision tasks, including becoming the new approach for performing visual search. Most likely, it has been adopted by the likes of Google, Facebook, and Pinterest, making it easier than ever to find that right image. </span></p>
<p><span>Recently, (October 2017), Shutterstock announced one of the more novel uses of CNNs, where they introduced the ability for their users to search for not only multiple items in an image, but also the composition of those items. The following screenshot shows an example search for a kitten and a computer, with the kitten on the left of the computer:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7caaf8d3-6a1b-4e0e-847a-6f9607cf864f.jpg" style="width:53.50em;height:35.67em;"/></div>
<p>So what are CNNs? As previously <span>mentioned,</span> CNNs are a type of neural network that are well suited for visual content due to their ability to <span>retain spatial information</span>. They are somewhat similar to the previous example, where we explicitly define a filter to extract localized features from the image. A CNN performs a similar operation, but unlike our previous example, filters are not explicitly defined. They are learned through training, and they are not confined to a single layer but rather build with many layers. Each layer builds upon the previous one and each becomes increasingly more abstract (abstract here means a higher-order representation, that is, from pixels to shapes) in what it represents. </p>
<p>To help illustrate this, the following diagram visualizes how a network might build up its understanding of a cat. The first layer's filters extract simple features, such as edges and corners. The next layer builds on top of these with its own filters, resulting in higher-level concepts being extracted, such as shapes or parts of the cat. These high-level concepts are then combined for classification purposes:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9a864245-2210-4579-a19d-43a61b59e379.png" style="width:23.25em;height:25.00em;"/></div>
<div class="packt_infobox">This ability to get a deeper understanding of the data and reduce the dependency on manual feature engineering has made deep neural networks one of the most popular ML algorithms over the past few years.</div>
<p>To train the model, we feed the network examples using images as inputs and labels as the expected outputs. Given enough examples, the model will build an internal representation for each label, which can be sufficiently used for <strong>classification</strong>; this, of course, is a type of <strong>supervised learning</strong>. </p>
<p>Our last task is to find the location of the item or items; to achieve this, we can inspect the weights of the network to find out which pixels activated a particular class, and then create a bounding box around the inputs with the largest weights. </p>
<p>We have now identified the items and their locations within the image. With this information, we can preprocess our repository of images and cache it as metadata to make it accessible via search queries. We will revisit this idea later in the book when you will get a chance to implement a version of this to assist the user in finding images in their photo album. </p>
<p>In this section, we saw how ML can be used to improve user experience and briefly introduced the intuition behind CNNs, a neural network well suited for visual contexts, where retaining proximity of features and building higher levels of abstraction is important. In the next section, we will continue our exploration of ML applications by introducing another example that improves the user experience and a new type of neural network that is well suited for sequential data such as text. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">iOS keyboard prediction – next letter prediction</h1>
                </header>
            
            <article>
                
<p>Quoting usability expert Jared Spool, <em>Good design, when done well, should be invisible.</em> This holds true for ML as well. The application of ML need not be apparent to the user and sometimes (more often than not) more subtle uses of ML can prove just as impactful. </p>
<p>A good example of this is an iOS <span>feature</span> called <strong>dynamic target resizing</strong>; it is working e<span>very time you type on an iOS keyboard, where it actively tries to predict what word you're trying to type:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/f2b17f2a-3cb9-4c0c-a73a-c90a9c8f6fca.png" style="width:22.17em;height:17.92em;"/></div>
<p>Using this prediction, the iOS keyboard dynamically changes the touch area of a key (here illustrated by the red circles) that is the most likely character based on what has already been typed before it. </p>
<p>For example, in the preceding diagram, the user has entered <kbd>"Hell"</kbd>; now it would be reasonable to assume that the most likely next character the user wants to tap is <kbd>"o"</kbd>. This is intuitive given our knowledge of the English language, but how do we teach a machine to know this?</p>
<p>This is where <strong>recurrent neural networks</strong> (<strong>RNNs</strong>) come in; it's a type of neural network that persists state over time. You can think of this persisted state as a form of memory, making RNNs suitable for sequential data such as text (any data where the inputs and outputs are dependent on each other). This state is created by using a feedback loop from the output of the cell, as shown in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34a16c15-b94a-4a9d-88e3-af66e79228f3.png" style="width:14.58em;height:15.33em;"/></div>
<p>The preceding diagram shows a single RNN cell. If we unroll this over time, we would get something that looks like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6e01fc27-e451-4304-8bcc-d80337aaa38e.png" style="width:34.42em;height:18.17em;"/></div>
<p>Using <strong>hello</strong> as our example, the preceding diagram shows an unrolled RNN over five time steps; at each time step, the RNN predicts the next likely character. This prediction is determined by its internal representation of the language (from training) and subsequent inputs. This internal representation is built by training it on samples of text where the output is using the inputs but at the next time step (as illustrated earlier). Once trained, the inference follows a similar path, except that we feed to the network the predicted character from the output, to get the next output (to generate the sequence, that is, words).</p>
<p>Neural networks and most ML algorithms require their inputs to be numbers, so we need to convert our characters to numbers, and back again. When dealing with text (characters and words), there are generally two approaches: <strong>one-hot encoding</strong> and <strong>embeddings</strong>. Let's quickly cover each of these to get some intuition of how to handle text.</p>
<p>Text (characters and words) is considered categorical, meaning that we cannot use a single number to represent text because there is no inherit relationship between the text and the value; that is, assigning <strong>the</strong> 10 and <strong>cat</strong> 20 implies that <strong>cat</strong> has a greater value than <strong>the</strong>. Instead, we need to encode them into something where no bias is introduced. One solution to this is encoding them using one-hot encoding, which uses an array of the size of your vocabulary (number of characters in our case), with the index of the specific character set to 1 and the rest set to 0. The following diagram illustrates the encoding process for the corpus <strong>"hello"</strong>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-564 image-border" src="assets/4694039f-def1-441f-a43d-ecaaa8639385.png" style="width:39.25em;height:8.08em;"/></div>
<p>In the preceding diagram, we show some of the steps required when encoding characters; we start off by splitting the corpus into individual characters (called <strong>tokens</strong>, and the process is called <strong>tokenization</strong>). Then we create a set that acts as our vocabulary, and finally we encode this with each character being assigned a vector.</p>
<div class="packt_figref packt_infobox">Here, we'll only present some of the steps required for preparing text before passing it to our ML algorithm.</div>
<p>Once our inputs are encoded, we can feed them into our network. Outputs will also be represented in this format, with the most likely character being the index with the greatest value. For example, if <strong>'e'</strong> is predicted, then <span>the most likely </span>the output may resemble something like [0.95, 0.2, 0.2, 0.1].</p>
<p>But there are two problems with one-hot encoding. The first is that for a large vocabulary, we end up with a very <span>sparse </span>data structure. This is not only an inefficient use of memory, but also requires additional calculations for training and inference. The second problem, which is more obvious when operating on words, is that we lose any contextual meaning after they have been encoded. For example, if we were to encode the words <strong>dog</strong> and <strong>dogs</strong>, we would lose any relationship between these words after encoding. </p>
<p>An alternative, and something that addresses these two problems, is using an embedding. These are generally weights from a trained network that use a dense vector representation for each token, one that preserves some contextual meaning. This book focuses on computer vision tasks, so we won't be going into the details here. Just remember that we need to encode our text (characters) into something our ML algorithm will accept. </p>
<p>We train the model using <strong>weak supervision</strong>, similar to supervised learning, but inferring the label without it having been explicitly labelled. Once trained, we can predict the next character using <strong>multi-class classification</strong>, as described earlier. </p>
<p>Over the past couple of years, we have seen the evolution of assistive writing; one example is Google's Smart Reply, which provides an end-to-end method for automatically generating short email responses. Exciting times!</p>
<p>This concludes our brief tour of introducing types of ML problems along with the associated data types, algorithms, and learning style. We have only scratched the surface of each, but as you make your way through this book, you will be introduced to more data types, algorithms, and learning styles. </p>
<p>In the next section, we will take a step back and review the overall workflow for training and inference before wrapping up this chapter. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A typical ML workflow </h1>
                </header>
            
            <article>
                
<p>If we analyze each of the examples presented so far, we see that each follows a similar pattern. First is the definition of the problem or desired functionality. Once we have established what we want to do, we then identify the available data and/or what data is required. With the data in hand, our next step is to create our ML model and prepare the data for training.</p>
<p>After training, something we hadn't discussed here, is validating our ML model, that is, testing that it satisfactorily achieves what we require of it. An example is being able to make an accurate prediction. Once we have trained a model, we can make use of it by feeding in real data, that is, data outside our training set. In the following diagram, we see these steps summarized for training and inference: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/8925f3f4-0c7c-42fc-bbe0-7daf4518c481.png" style="width:38.83em;height:18.00em;"/></div>
<p>We will spend most of our time using trained models in this book, but understanding how we arrive at these models will prove helpful as you start creating your own intelligent apps. This will also help you identify opportunities to apply ML on existing data or inspire you to seek out new data <span>sources</span>. It's also worth noting that the preprocessing step on training data is equivalent to preprocessing on input data when performing inference—something we will spend a lot of time discussing and coding for throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced ML and its value by contrasting it against classical programming. We then spent some time exploring different applications of ML, and for each we speculated about the type of data, algorithms, and learning style used. This approach was taken to help demystify how ML works and to encourage you to start thinking about how you can leverage data to improve user experience and/or offer new functionality. We'll continue this approach throughout this book with (obviously) more emphasis on making use of ML by way of example applications related to computer vision.</p>
<p>In the next chapter, we will introduce Core ML, iOS's specifically designed <span>framework </span>for making ML accessible to developers with little or no experience with ML.</p>


            </article>

            
        </section>
    </body></html>