- en: '*Chapter 6*: Processing and Consuming Data on the Cloud'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：在云上处理和消费数据'
- en: The value proposition of edge computing is to process data closer to the source
    and deliver intelligent near real-time responsiveness for different kinds of applications
    across different use cases. Additionally, edge computing reduces the amount of
    data that is required to be transferred to the cloud, thus saving on network bandwidth
    costs. Often, high-performance edge applications require local compute, local
    storage, network, data analytics, and machine learning capabilities to process
    high-fidelity data in low latencies. Although AWS IoT Greengrass allows you to
    run sophisticated edge applications on devices and gateways, it will be resource-constrained
    compared to the horsepower from the cloud. Therefore, for different use cases,
    it's quite common to leverage the scale of cloud computing for high-volume complex
    data processing needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘计算的价值主张是在数据源附近处理数据，并为不同用例中的不同类型的应用提供智能近实时响应。此外，边缘计算减少了需要传输到云中的数据量，从而节省了网络带宽成本。通常，高性能边缘应用程序需要本地计算、本地存储、网络、数据分析以及机器学习能力，以在低延迟下处理高保真数据。尽管AWS
    IoT Greengrass允许你在设备和网关上运行复杂的边缘应用程序，但与云端的强大计算能力相比，它将受到资源限制。因此，对于不同的用例，利用云计算的规模来满足大量复杂数据处理需求是非常常见的。
- en: 'In the previous chapter, you learned about the different design patterns around
    data transformation strategies on the edge. This chapter will focus on explaining
    how you can build different data workflows on the cloud based on the data velocity,
    data variety, and data volume collected from the HBS hub running a Greengrass
    instance. Specifically, you will learn how to persist data in a transactional
    data store, develop API driven access, and build a serverless data warehouse to
    serve data to end users. Therefore, the chapter is divided into the following
    topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了关于边缘数据转换策略的不同设计模式。本章将重点解释如何根据从运行Greengrass实例的HBS中心收集到的数据速度、数据种类和数据量，在云上构建不同的数据工作流。具体来说，你将学习如何将数据持久化到事务性数据存储中，开发API驱动访问，并构建无服务器数据仓库以向最终用户提供数据。因此，本章分为以下主题：
- en: Defining big data for IoT workloads
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为物联网工作负载定义大数据
- en: Introduction to **Domain-Driven Design** (**DDD**) concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**领域驱动设计**（DDD）概念简介'
- en: Design data flow patterns on the cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云上设计数据流模式
- en: Remembering data flow anti-patterns for edge workloads
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住边缘工作负载的数据流反模式
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The technical requirements for this chapter are the same as those outlined in
    [*Chapter 2*](B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032)*, Foundations of
    Edge Workloads*. See the full requirements in that chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求与[*第2章*](B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032)*，边缘工作负载基础*中概述的要求相同。完整的要求数据见该章节。
- en: Defining big data for IoT workloads
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为物联网工作负载定义大数据
- en: The term *Big* in **Big data** is relative, as the influx of data has grown
    substantially in the last two decades from terabytes to exabytes due to the digital
    transformation of enterprises and connected ecosystems. The advent of big data
    technologies has allowed people (*think social media*) and enterprises (*think
    digital transformation*) to generate, store, and analyze huge amounts of data.
    To analyze datasets of this volume, sophisticated computing infrastructure is
    required that can scale elastically based on the amount of input data and required
    outcome. This characteristic of big data workloads, along with the availability
    of cloud computing, democratized the adoption of big data technologies by companies
    of all sizes. Even with the evolution of edge computing, big data processing on
    the cloud plays a key role in IoT workloads, as data is more valuable when it's
    adjacent and enriched with other data systems. In this chapter, we will learn
    how the big data ecosystem allows for advanced processing and analytical capabilities
    on the huge volume of raw measurements or events collected from the edge to enable
    the consumption of actionable information by different personas.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在**大数据**中的“大”是相对的，因为随着企业数字化和连接生态系统的转型，数据流入在过去二十年里从太字节增长到艾字节，数据量大幅增加。大数据技术的出现使得人们（如社交媒体）和企业（如数字化转型）能够生成、存储和分析大量数据。为了分析如此大量的数据集，需要复杂的计算基础设施，该基础设施可以根据输入数据量和所需结果弹性扩展。大数据工作负载的这一特性，加上云计算的可用性，使得所有规模的公司都能够民主化地采用大数据技术。即使在边缘计算的演变中，云上的大数据处理在物联网工作负载中也发挥着关键作用，因为当数据靠近并与其他数据系统丰富时，数据更有价值。在本章中，我们将学习大数据生态系统如何允许对从边缘收集的大量原始测量或事件进行高级处理和分析，从而使得不同角色能够消费可操作信息。
- en: 'The integration of IoT with big data ecosystems has opened up a diverse set
    of analytical capabilities that allows the generation of additional business insights.
    These include the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网与大数据生态系统的集成开辟了一系列分析能力，这允许生成额外的业务洞察。以下是一些例子：
- en: '**Descriptive analytics**: This type of analytics helps users answer the question
    of *what happened and why?* Examples of this include traditional queries and reporting
    dashboards.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**描述性分析**：这种分析类型帮助用户回答“发生了什么以及为什么？”的问题。例如，传统的查询和报告仪表板。'
- en: '**Predictive analytics**: This form of analytics helps users predict the probability
    of a given event in the future based on historical events or detected anomalies.
    Examples of this include early fraud detection in banking transactions and preventive
    maintenance for different systems.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测分析**：这种分析形式帮助用户根据历史事件或检测到的异常情况，预测未来某个事件的概率。例如，在银行交易中早期欺诈检测和不同系统的预防性维护。'
- en: '**Prescriptive analytics**: This kind of analytics helps users provide specific
    (clear) recommendations. They address the question of *what should I do if x happens?*
    Examples of this include an election campaign to reach out to targeted voters
    or statistical modeling in wealth management to maximize returns.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范性分析**：这种分析帮助用户提供具体的（清晰的）建议。它们解决的是“如果发生x，我应该做什么？”的问题。例如，选举活动接触目标选民或财富管理中的统计建模以最大化回报。'
- en: The outcome of these processes allows organizations to have increased visibility
    to new information, emerging trends, or hidden data correlation to improve efficiencies
    or generate new revenue streams. In this chapter, you will learn about the approaches
    of both descriptive and predictive analytics on data collected from the edge.
    In addition to this, you will learn how to implement design patterns such as streaming
    to a data lake or a transactional data store on the cloud, along with leveraging
    API driven access, which are considered anti-patterns for the edge. So, let's
    get started with the design methodologies of big data that are relevant for IoT
    workloads.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些流程的结果使得组织能够对新的信息、新兴趋势或隐藏的数据相关性有更高的可见性，以提高效率或产生新的收入来源。在本章中，你将了解对从边缘收集的数据进行描述性和预测性分析的方法。此外，你还将学习如何实现设计模式，如将流式传输到数据湖或云上的事务性数据存储，以及利用API驱动的访问，这些都被认为是边缘的反模式。因此，让我们开始学习与物联网工作负载相关的大数据设计方法。
- en: What is big data processing?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是大数据处理？
- en: 'Big data processing is generally categorized in terms of the three Vs: the
    volume of data (for example, a terabyte, petabyte, or more), the variety of data
    (that is, structured, semi-structured, or unstructured), and the velocity of data
    (that is, the speed with which it''s produced or consumed). However, as more organizations
    begin to adopt big data technologies, there have been additions to the list of
    Vs, such as the following:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据处理通常按照三个V来分类：数据量（例如，太字节、拍字节或更多），数据种类（即结构化、半结构化或非结构化），以及数据速度（即数据产生或消费的速度）。然而，随着越来越多的组织开始采用大数据技术，V列表中又增加了以下内容：
- en: '**Viscosity**: This emphasizes the ease of usability of data; for example,
    there could be noisy data collected from the edge that''s not easy to parse.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**粘度**：这强调数据的易用性；例如，可能存在从边缘收集的噪声数据，这些数据不易解析。'
- en: '**Volatility**: This refers to how often data changes occur and, therefore,
    how long the data is useful; for example, capturing specific events at home can
    be more useful than every other activity.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易变性**：这指的是数据变化的频率以及因此数据的有用性持续时间；例如，捕捉家庭中的特定事件可能比其他所有活动更有用。'
- en: '**Veracity**: This refers to how trustworthy the data is, for example, if images
    captured from outdoor cameras are of poor quality, they cannot be trusted to identify
    intrusion.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：这指的是数据的可信度，例如，如果从户外摄像头捕获的图像质量差，那么它们不能被信赖用于识别入侵。'
- en: 'For edge computing and the **Internet of Things** (**IoT**), all six Vs are
    relevant. The following diagram presents a visual summary of the range of data
    that has become available with the advent of IoT and big data technologies. This
    requires you to consider different ways in which to organize data at scale based
    on its respective characteristics:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于边缘计算和物联网（**IoT**），所有六个V都相关。以下图表展示了随着物联网和大数据技术的出现而变得可用的数据范围。这要求你考虑根据各自特征以不同方式组织大规模数据：
- en: '![Figure 6.1 – The evolution of big data'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 大数据的发展'
- en: '](img/B17595_06_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_01.jpg)'
- en: Figure 6.1 – The evolution of big data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 大数据的发展
- en: So, you have already learned about data modeling concepts in [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)*,
    Ingesting and Streaming Data from the Edge*, which is a standard way of organizing
    data into meaningful structures based on data types and relationships and extracting
    value out of it. However, collecting data, storing it in a stream or a persistent
    layer, and processing it quickly to take intelligent actions is only one side
    of the story. The next challenge is to work out how to keep a high quality of
    data throughout its life cycle so that it continues to generate business value
    for downstream applications over inconsistency or risk. For IoT workloads, this
    aspect is critical as the devices or gateways reside in a physical world, with
    intermittent connectivity, at times, being susceptible to different forms of interference.
    This is where the domain-driven design (DDD) approach can help.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你已经学习了关于数据建模概念的内容，在[*第五章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)“从边缘摄取和流式传输数据”，这是一种基于数据类型和关系组织数据到有意义结构的标准方式，并从中提取价值。然而，收集数据，将其存储在流或持久层中，并快速处理以采取智能行动只是故事的一方面。下一个挑战是如何在整个生命周期中保持数据的高质量，以确保它能够为下游应用持续产生业务价值，而不是不一致或风险。对于物联网工作负载，这一点至关重要，因为设备或网关位于物理世界，有时具有间歇性的连接性，并可能受到不同形式的干扰。这就是领域驱动设计（DDD）方法可以提供帮助的地方。
- en: What is domain-driven design?
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是领域驱动设计？
- en: To manage the quality of the data better, we need to learn how to organize data
    by content such as data domains or subject areas. One of the most common approaches
    in which to do that is through DDD, which was introduced by Eric Evans in 2003\.
    In his book, Eric states *The heart of software is its ability to solve domain-related
    problems for its user. All other features, vital though they may be, support this
    basic purpose*. Therefore, DDD is an approach to software development centering
    around the requirements, rules, and processes of a business domain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地管理数据质量，我们需要学习如何通过内容如数据域或主题领域来组织数据。其中最常见的方法之一是通过DDD，这是由埃里克·埃文斯（Eric Evans）在2003年提出的。在他的书中，埃里克表示“软件的核心是其解决用户领域相关问题的能力。所有其他功能，尽管它们可能很重要，但都支持这一基本目的”。因此，DDD是一种围绕业务领域的需求、规则和流程的软件开发方法。
- en: 'The DDD approach includes two core concepts: bounded context and ubiquitous
    language. Let''s dive deeper into each of them:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DDD方法包括两个核心概念：有界上下文和通用语言。让我们更深入地探讨每个概念：
- en: '**Bounded context**: Bounded contexts help you to define the logical boundaries
    of a solution. They can be implemented on the application or business layer, as
    per the requirements of the organization. However, the core concept is that a
    bounded context should have its own application, data, and process. This allows
    the respective teams to clearly define the components they own in a specific domain.
    These boundaries are important in managing data quality and minimizing data silos,
    as they grow with different Vs and get redistributed with different consumers
    within or outside an organization. For example, with a connected HBS solution,
    there can be different business capabilities required by the internal business
    functions of HBS and their end consumers. This could include the following:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有界上下文**：有界上下文有助于你定义解决方案的逻辑边界。它们可以根据组织的需要，在应用层或业务层实现。然而，核心概念是一个有界上下文应该拥有自己的应用、数据和流程。这允许相关团队在特定领域内清楚地定义他们拥有的组件。这些边界对于管理数据质量和最小化数据孤岛至关重要，因为随着不同的V值增长，它们会在组织内部或外部与不同的消费者一起重新分配。例如，使用连接的HBS解决方案，HBS的内部业务功能及其最终消费者可能需要不同的业务能力。这可以包括以下内容：'
- en: 'Internal capabilities (for the organizational entities):'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内部能力（针对组织实体）：
- en: '*Product engineering*: The utilization of different services or features'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产品工程*：利用不同的服务或功能'
- en: '*Fleet operation*: Monitoring fleet health'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*车队运营*：监控车队健康状况'
- en: '*Information security*: Monitoring the adherence to different regulatory requirements,
    such as GDPR'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息安全*：监控遵守不同的监管要求，例如GDPR'
- en: More such as CRM, ERP, and marketing
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多如CRM、ERP和营销
- en: 'External capabilities (for the end consumer):'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部能力（针对最终消费者）：
- en: '*Fleet telemetry*: The processing of data feeds such as a thermostat or HVAC
    readings from devices in near real time'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*车队遥测*：处理如恒温器或HVAC设备从设备中近实时读取的数据流'
- en: '*Fleet monitoring*: Capturing fleet health information or critical events such
    as the malfunctioning of sensors'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*车队监控*：捕获车队健康状况信息或关键事件，例如传感器的故障'
- en: '*Fleet analytics*: Enriching telemetry data with other metadata to perform
    analysis factoring in different environmental factors such as time, location,
    and altitude'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*车队分析*：通过其他元数据丰富遥测数据，以考虑不同环境因素（如时间、位置和海拔）进行分析'
- en: 'The following diagram is an illustration of a bounded context:'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下图表展示了有界上下文的示例：
- en: '![Figure 6.2 – A bounded context'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – 有界上下文'
- en: '](img/B17595_06_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_02.jpg)'
- en: Figure 6.2 – A bounded context
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 有界上下文
- en: All of these different business capabilities can be defined as a bounded context.
    So, now the business capabilities have been determined, we can define the technology
    requirements within this bounded context to deliver the required business outcome.
    The general rule of thumb is that the applications, data, or processes should
    be cohesive and not span for consumption by other contexts. In this chapter, we
    are going to primarily focus on building bounded contexts for external capabilities
    that are required by end consumers using different technologies.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些不同的业务能力都可以定义为有界上下文。因此，现在我们已经确定了业务能力，我们可以在有界上下文中定义技术要求，以实现所需的企业成果。一般规则是，应用、数据或流程应该是内聚的，不应跨越其他上下文的使用。在本章中，我们将主要关注构建外部能力所需的有界上下文，这些能力是由最终消费者使用不同技术实现的。
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: However, in the real world, there can be many additional factors to bear in
    mind when it comes to defining a bounded context, such as an organizational structure,
    product ownership, and more. We will not be diving deep into these factors as
    they are not relevant to the topic being discussed here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实世界中，在定义一个有界上下文时，可能需要考虑许多额外的因素，例如组织结构、产品所有权等。我们不会深入探讨这些因素，因为它们与这里讨论的主题无关。
- en: '**Ubiquitous language**: The second concept in DDD is ubiquitous language.
    Each bounded context is supposed to have its own ubiquitous language. Applications
    that belong together within a bounded context should all follow the same language.
    If the bounded context changes, the ubiquitous language is also expected to be
    different. This allows the bounded context to be developed and managed by one team
    and, therefore, aligns with the DevOps methodology as well. This operating model
    makes it easier for a single team, familiar with the ubiquitous language, to own
    and resolve different applications or data dependencies quicker.Later in this
    chapter, you will discover how the different bounded contexts (or workflows) are
    implemented using a diverse set of languages.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用语言**：DDD的第二个概念是通用语言。每个边界上下文都应该有自己的通用语言。属于同一边界上下文的应用程序都应该遵循相同的语言。如果边界上下文发生变化，通用语言也应该是不同的。这允许一个团队开发和管理工作负载的边界上下文，因此与DevOps方法相一致。这种运营模式使得一个熟悉通用语言的单一团队能够更快地拥有和解决不同的应用程序或数据依赖关系。在本章的后面部分，您将发现不同的边界上下文（或工作流程）是如何使用多种不同的语言实现的。'
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 备注
- en: The DDD model doesn't mandate how to determine the bounded context within application
    or data management. Therefore, it's recommended that you work backward from your
    use case and determine the appropriate cohesion.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DDD模型没有规定如何在应用程序或数据管理中确定边界上下文。因此，建议您从您的用例开始，逆向确定适当的内聚性。
- en: So, with this foundation, let's define some design principles of data management
    on the cloud – some of these will be used for the remainder of the chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，基于这个基础，让我们定义一些在云上管理数据的设计原则 – 其中一些将在本章的剩余部分使用。
- en: What are the principles to design data workflows using DDD?
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DDD设计数据工作流程的原则是什么？
- en: 'We will outline a set of guardrails (that is principles) to understand how
    to design data workloads using DDD:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将概述一系列指导原则（即原则），以了解如何使用领域驱动设计（DDD）设计数据工作负载：
- en: '*Principle 1: Manage data ownership through domains* – The quality of the data
    along with the ease of usability are the advantages of using domains. The team
    that knows the data best, owns and manages it. Therefore, the data ownership is
    distributed as opposed to being centralized.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 1：通过领域管理数据所有权* – 使用领域的优势在于数据的质量以及易用性。最了解数据的团队拥有并管理它。因此，数据所有权是分散的，而不是集中的。'
- en: '*Principle 2: Define domains using bounded contexts* – A domain implements
    a bounded context, which, in turn, is linked to a business capability.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 2：使用边界上下文定义领域* – 领域实现一个边界上下文，反过来，这个上下文又与一个业务能力相联系。'
- en: '*Principle 3: Link a bounded context to one or many application workloads*
    – A bounded context can include one or many applications. If there are multiple
    applications, all of them are expected to deliver value for the same business
    capability.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 3：将一个边界上下文链接到一个或多个应用程序工作负载* – 一个边界上下文可以包括一个或多个应用程序。如果有多个应用程序，所有这些应用程序都应提供相同业务能力的价值。'
- en: '\*Principle 4: Share the ubiquitous language within the bounded context* –
    Applications that are responsible for distributing data within their bounded context
    use the same ubiquitous language to ensure that different terminologies and data
    semantics do not conflict. Each bounded context has a one-to-one relationship
    with a conceptual data model.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 4：在边界上下文中共享通用语言* – 负责在其边界上下文中分发数据的应用程序使用相同的通用语言，以确保不同的术语和数据语义不会冲突。每个边界上下文与一个概念数据模型有一对一的关系。'
- en: '*Principle 5: Preserve the original sourced data* – Ingested raw data needs
    to be preserved as a source of truth in a centralized solution. This is often
    referred to as the golden dataset. This will allow different bounded contexts
    to repeat the processing of data in the case of failures.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 5：保留原始源数据* – 需要保留摄入的原始数据作为集中解决方案中的真相来源。这通常被称为黄金数据集。这将允许不同的边界上下文在发生故障的情况下重复处理数据。'
- en: '*Principle 6: Associate data with metadata* – With the growth of data in terms
    of variety and volume, it''s necessary for any dataset to be easily discoverable
    and classified. This eases the reusability of data by different downstream applications
    along with establishing a data lineage.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 6：将数据与元数据关联* – 随着数据在多样性和数量上的增长，任何数据集都应易于发现和分类。这有助于不同下游应用程序的数据重用，并建立数据血缘。'
- en: '*Principle 7: Use the right tool for the right job* – Based on the data workflow
    such as the speed layer or the batch layer, the persistence and compute tools
    will be different.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则7：使用合适的工具做合适的工作* – 根据数据工作流程，如速度层或批量层，持久性和计算工具将不同。'
- en: '*Principle 8: Tier data storage* – Choose the optimal storage layer for your
    data based on its access patterns. By distributing the datasets into different
    storage services, you can build a cost-optimized storage infrastructure.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则8：分层数据存储* – 根据数据的访问模式选择最佳存储层。通过将数据集分布到不同的存储服务中，你可以构建一个成本优化的存储基础设施。'
- en: '*Principle 9: Secure and govern the data pipeline* – Implement control to secure
    and govern all data at rest and in transit. A mechanism is required to only allow
    authorized entities to visualize, access, process, and modify data assets. This
    helps us to protect data confidentiality and data security.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则9：保护和管理数据管道* – 实施控制以保护和管理所有静态和传输中的数据。需要一个机制来仅允许授权实体可视化、访问、处理和修改数据资产。这有助于我们保护数据机密性和数据安全。'
- en: '*Principle 10: Design for Scale* – Last but not least, the cloud is all about
    the economies of scale. So, take advantage of the managed services to scale elastically
    and handle any volume of data reliably.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则10：设计以规模为基础* – 最后但同样重要的是，云的一切都是关于规模经济。因此，利用托管服务弹性扩展并可靠地处理任何数据量。'
- en: In the remainder of the chapter, we will touch upon most of these design principles
    (if not all) as we dive deeper into the different design patterns, data flows,
    and hands-on labs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将深入探讨这些设计原则（如果不是全部）作为我们深入研究不同的设计模式、数据流和动手实验室。
- en: Designing data patterns on the cloud
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云上设计数据模式
- en: As data flows from the edge to the cloud securely over different channels (such
    as through speed or batch layers), it is a common practice to store the data in
    different staging areas or a centralized location based on the data velocity or
    data variety.These data sources act as a single source of truth and help to ensure
    the quality of the data for their respective bounded contexts. Therefore, in this
    section, we will discuss different data storage options, data flow patterns, and
    anti-patterns on the cloud. Let's begin with data storage.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据通过不同的通道（如通过速度层或批量层）安全地从边缘流向云端时，根据数据速度或数据多样性，将数据存储在不同的临时区域或集中位置是一种常见做法。这些数据源作为单一的真实来源，有助于确保各自边界上下文中数据的品质。因此，在本节中，我们将讨论不同的数据存储选项、数据流模式以及云上的反模式。让我们从数据存储开始。
- en: Data storage
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储
- en: As we learned, in earlier chapters, since edge solutions are constrained in
    terms of computing resources, it's important to optimize the number of applications
    or the amount of data persisted locally based on the use case. On the other hand,
    the cloud doesn't have that constraint, as it comes with virtually unlimited resources
    with different compute and storage options. This makes it a perfect fit for big
    data applications to grow and contract based on the demand. In addition to this,
    it provides easy access to a global infrastructure to orchestrate data required
    by different downstream or end consumers in the region that is closer to them.
    Finally, data is more valuable when it's augmented with other data or metadata;
    thus, in recent times, patterns such as data lakes have become very popular. So,
    what is a data lake?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中学到的，由于边缘解决方案在计算资源方面受到限制，根据用例优化应用程序的数量或本地持久化的数据量是很重要的。另一方面，云没有这种限制，因为它几乎拥有无限的资源，以及不同的计算和存储选项。这使得它非常适合大数据应用根据需求增长和收缩。此外，它还提供了轻松访问全球基础设施的便利，以满足不同下游或终端消费者在更靠近他们的地区所需的数据。最后，当数据与其他数据或元数据相结合时，数据更有价值；因此，在最近几年，如数据湖等模式变得非常流行。那么，什么是数据湖呢？
- en: 'A data lake is a centralized, secure, and durable storage platform that allows
    you to ingest, store structured and unstructured data, and transform the raw data
    as required. You can think of data lake as a superset of the data pond concepts
    introduced in [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)*,*
    *Ingesting and Streaming Data from the Edge*. Since the IoT devices or gateways
    are relatively low in storage, only highly valuable data that''s relevant for
    the edge operations can be persisted locally in a data pond:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个集中式、安全且耐用的存储平台，允许您摄取、存储结构化和非结构化数据，并根据需要转换原始数据。您可以将数据湖视为在[*第5章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)*，*从边缘摄取和流式传输数据*中引入的数据池塘概念的超集。由于物联网设备或网关的存储相对较低，只有与边缘操作高度相关的、具有高度价值的数据才能在数据池塘中本地持久化：
- en: '![Figure 6.3 – The data lake architecture'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – 数据湖架构'
- en: '](img/B17595_06_03.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_03.jpg)'
- en: Figure 6.3 – The data lake architecture
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 数据湖架构
- en: 'Some of the foundational characteristics of the data lake architecture are
    explained here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖架构的一些基本特性在此进行解释：
- en: There is central storage for storing raw data with minimal or no transformation
    securely. This is a single source of truth of data. The choice of compute, storage layer,
    schema, ingestion frequency, and data quality is left to the data producer. Amazon
    S3 is commonly chosen as the central storage since it's a highly scalable, highly
    durable, and cost-effective service that allows the decoupling of the compute
    and storage layers. AWS offers different tiering options within Amazon S3 along
    with a full-fledged archival service referred to as Amazon Glacier.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储原始数据，并确保数据在最小或无转换的情况下安全存储的中心存储。这是数据的单一事实来源。计算层、存储层、模式、摄取频率和数据质量的选取由数据生产者决定。Amazon
    S3通常被选作中心存储，因为它是一个高度可扩展、高度耐用且成本效益高的服务，允许计算层和存储层的解耦。AWS在Amazon S3内提供不同的分层选项，以及一个称为Amazon
    Glacier的全面归档服务。
- en: There is a persistence layer for storing domain-specific data marts or transformed
    data in a columnar format (such as Parquet, ORC, or Avro) to achieve isolation
    by bounded contexts, faster performance, or lower cost. AWS offers different services
    such as AWS Glue for data transformation and data catalogs, Amazon Athena or Amazon
    Redshift for data warehouses and data marts, and Amazon EMR or Spark on EMR for
    managing big data processing.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储特定领域的数据集市或以列式格式（如Parquet、ORC或Avro）转换的数据的持久层，以通过边界上下文实现隔离、提高性能或降低成本。AWS提供不同的服务，如AWS
    Glue进行数据转换和数据目录，Amazon Athena或Amazon Redshift进行数据仓库和数据集市，以及Amazon EMR或EMR上的Spark进行大数据处理管理。
- en: There is a persistence layer for storing transactional data ingested from the
    edge securely. This layer is often referred to as the **Operational Data Store**
    (**ODS**). AWS offers different services that can be leveraged here based on the
    given data structures and access patterns, such as Amazon DynamoDB, Amazon RDS,
    and Amazon Timestream.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储从边缘安全摄取的事务数据的持久层。这一层通常被称为**操作数据存储**（**ODS**）。AWS提供不同的服务，可以根据给定的数据结构和访问模式利用这些服务，例如Amazon
    DynamoDB、Amazon RDS和Amazon Timestream。
- en: You must be wondering how data from a data lake is made available to a data
    warehouse or an ODS. That's where data integration patterns play a key role.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道数据湖中的数据是如何提供给数据仓库或ODS的。这正是数据集成模式发挥关键作用的地方。
- en: Data integration patterns
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集成模式
- en: '**Data Integration and Interoperability** (**DII**) happens through the batch,
    speed, and serving layers. A common methodology in the big data world that intertwines
    all these layers is **Extract, Transform, and Load** (**ETL**) or **Extract, Load,
    and Transform** (**ELT**). We have already explained these concepts, in detail,
    in [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090), *Ingesting and
    Streaming Data from the Edge*, and discussed how they have evolved with time into
    different data flow patterns such as event-driven, batch, lambda, and complex
    event processing. Therefore, we will not be repeating the concepts here. But in
    the next section, we will explain how they relate to data workflows in the cloud.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集成与互操作性**（**DII**）通过批量、速度和服务层实现。大数据世界中将这些所有层交织在一起的一种常见方法是**提取、转换和加载**（**ETL**）或**提取、加载和转换**（**ELT**）。我们已经在[*第5章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)，*从边缘摄取和流式传输数据*中详细解释了这些概念，并讨论了它们如何随着时间的推移演变成不同数据流模式，如事件驱动、批量、lambda和复杂事件处理。因此，我们在此不再重复这些概念。但在下一节中，我们将解释它们如何与云中的数据工作流相关。'
- en: Data flow patterns
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据流模式
- en: Earlier in this chapter, we discussed how bounded contexts can be used to segregate
    different external capabilities for end consumers, such as *fleet telemetry*,
    *fleet monitoring*, or *fleet analytics*. Now, it's time to learn how these concepts
    can be implemented using different data flow patterns.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们讨论了如何使用边界上下文来隔离针对最终消费者的不同外部能力，如*车队遥测*、*车队监控*或*车队分析*。现在，是时候学习如何使用不同的数据流模式来实现这些概念了。
- en: Batch (or aggregated processing)
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量（或聚合处理）
- en: Let's consider a scenario; you discover that you have been getting a higher
    electricity bill for the last six months, and you would like to compare the utilization
    of different equipment for that time period. Alternatively, you want visibility
    of more granular information, such as how many times did the washing machine run
    during the day in the last six months? And for how long? This led to how many
    X watts of consumption?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景；你发现过去六个月你的电费账单越来越高，你想要比较这段时间内不同设备的利用率。或者，你想要查看更细粒度的信息，比如在过去六个月中洗衣机一天运行了多少次？运行了多长时间？这导致了多少X瓦特的消耗？
- en: This is where batch processing helps. It had been the de facto standard of the
    industry before event-driven architecture gained popularity and is still heavily
    used for different use cases such as order management, billing, payroll, financial
    statements, and more. In this mode of processing, a large volume of data, such
    as thousands or hundreds of thousands of records (or more), is typically transmitted
    in a file format (such as `TXT` or `CSV`), cleaned, transformed, and loaded into
    a relational database or data warehouse. Thereafter, the data is used for data
    reconciliation or analytical purposes. A typical batch processing environment
    also includes a job scheduler that can trigger an analytical workflow based on
    schedules of feed availability or those that are required by the business.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是批量处理发挥作用的地方。在事件驱动架构变得流行之前，它一直是行业的实际标准，并且仍然被广泛用于不同的用例，如订单管理、账单、工资单、财务报表等。在这种处理模式下，大量数据，如数千或数百万条记录（或更多），通常以文件格式（如`TXT`或`CSV`）传输，清洗、转换并加载到关系数据库或数据仓库中。之后，数据用于数据核对或分析目的。典型的批量处理环境还包括一个作业调度器，可以根据数据馈送可用性或业务需求触发生成分析工作流。
- en: 'To design the *fleet analytics* bounded context, we have designed a batch workflow,
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计*车队分析*边界上下文，我们设计了以下批量工作流：
- en: '![Figure 6.4 – The batch architecture'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – 批量架构'
- en: '](img/B17595_06_04.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_04.jpg)'
- en: Figure 6.4 – The batch architecture
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 批量架构
- en: 'In this pattern, the following activities are taking place:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模式中，以下活动正在进行：
- en: Events streamed from the edge are routed through a streaming service (that is,
    Amazon Kinesis) to a data lake (that is, Amazon S3).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自边缘的事件通过流服务（即Amazon Kinesis）路由到数据湖（即Amazon S3）。
- en: Amazon Kinesis allows the preprocessing or enrichment of the data (if required)
    with additional metadata prior to persisting it to a data lake.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Kinesis允许在将数据持久化到数据湖之前，使用额外的元数据对数据进行预处理或增强（如果需要）。
- en: The data can be crawled or transformed through an ETL engine (that is, AWS Glue)
    and be easily queried using a serverless analytical service (that is, Amazon Athena).
    Amazon Athena uses a Presto engine under the hood and is compatible with ANSI
    SQL.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以通过ETL引擎（即AWS Glue）进行爬取或转换，并可以使用无服务器分析服务（即Amazon Athena）轻松查询。Amazon Athena在底层使用Presto引擎，并兼容ANSI
    SQL。
- en: Different services such as Amazon S3 and Amazon Athena offer integrations with
    Amazon QuickSight and different third-party **Business Intelligence** (**BI**)
    tools through JDBC and ODBC connectors.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的服务，如Amazon S3和Amazon Athena，通过JDBC和ODBC连接器提供与Amazon QuickSight和不同的第三方**商业智能**（**BI**）工具的集成。
- en: Amazon S3 is highly available and durable object storage that integrates with
    other big data services such as a fully managed Hadoop cluster (that is, Amazon
    EMR) or a data warehouse (that is, Amazon Redshift).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon S3是一个高度可用且耐用的对象存储，它与其他大数据服务集成，例如完全管理的Hadoop集群（即Amazon EMR）或数据仓库（即Amazon
    Redshift）。
- en: Fun fact
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: Amazon EMR and Amazon Redshift support big data processing through decoupling
    of the compute layer and the storage layer, which means there is no need to copy
    all the data to local storage from the data lake. Therefore, processing becomes
    more cost-efficient and operationally optimal.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Amazon EMR和Amazon Redshift通过解耦计算层和存储层来支持大数据处理，这意味着不需要从数据湖将所有数据复制到本地存储。因此，处理变得更加成本效益高，并且从操作上更加优化。
- en: 'The ubiquitous language used in this bounded context includes the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在此边界上下文中使用的通用语言包括以下内容：
- en: A REST API for stream processing on Amazon Kinesis, data processing on Amazon
    S3 buckets, and ETL processing on AWS Glue
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于Amazon Kinesis流处理的REST API、Amazon S3桶上的数据处理和AWS Glue上的ETL处理
- en: SQL for data analytics on Amazon Athena and Amazon Redshift
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon Athena和Amazon Redshift上使用SQL进行数据分析
- en: MapReduce or Spark for data processing on Amazon EMR
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于Amazon EMR上的数据处理MapReduce或Spark
- en: Rest APIs, JDBC, or ODBC connectors with Amazon QuickSight or third-party BI
    tools
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Amazon QuickSight或第三方BI工具的REST API、JDBC或ODBC连接器
- en: Batch processing is powerful since it doesn't have any windowing restrictions.
    There is a lot of flexibility in terms of how to correlate individual data points
    with the entire dataset, whether it's terabytes or exabytes in size for desired
    analytical outcomes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理非常强大，因为它没有任何窗口限制。在如何关联单个数据点与整个数据集方面有很多灵活性，无论是为了所需的分析结果而以千兆字节或太字节的大小。
- en: Event-driven processing
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 事件驱动处理
- en: 'Let''s consider the following scenario: you have rushed out of your home, and
    you get a notification after boarding your commute that you left the cooking stove
    on. Since you have a connected stove, you can immediately turn it off remotely
    from an app to avoid fire hazards. Bingo!'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下场景：您匆匆忙忙离开了家，在通勤途中收到通知，您忘记关上烹饪炉灶。由于您有一个连接的炉灶，您可以从应用程序中立即远程关闭它，以避免火灾风险。
    Bingo！
- en: 'This looks easy, but there is a certain level of intelligence required at the
    local hub (such as HBS hub) and a chain of events to facilitate this workflow.
    These might include the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很简单，但在本地枢纽（如HBS枢纽）需要一定程度的智能，并且需要一系列事件来促进这个工作流程。这些可能包括以下内容：
- en: Detect from motion sensors, occupancy sensors, or cameras that no one is at
    home.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从运动传感器、占用传感器或摄像头检测到家中无人。
- en: Capture multiple measurements from stove sensors over a period of time.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一段时间内从炉灶传感器捕获多个测量值。
- en: Correlate the events to identify this as a hazard scenario using local processes
    at the edge.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用边缘的本地过程关联事件，以识别这是一个危害场景。
- en: Stream an event to a message broker and persist it in an ODS.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将事件流式传输到消息代理，并将其持久化存储在ODS中。
- en: Trigger a microservice(s) to notify this event to the end user.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发微服务（s）以通知最终用户此事件。
- en: Remediate the issue based on user response.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据用户响应解决问题。
- en: So, as you can observe, a lot is happening in a matter of seconds between the
    edge, the cloud, and the end user to help mitigate the hazard. This is where patterns
    such as event-driven architectures became very popular in the last decade or so.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如您所观察到的，在边缘、云和最终用户之间，在短短几秒钟内发生了很多事情，以帮助减轻风险。这就是为什么在过去十年左右的时间里，事件驱动架构等模式变得非常受欢迎。
- en: Prior to EDA, polling and Webhooks were the common mechanisms in which to communicate
    events between different components. Polling is inefficient since there is always
    a lag in terms of how to fetch new updates from the data source and sync them
    with downstream services. Webhooks are not always the first choice, as they might
    require custom authorization and authentication configurations. In short, both
    of these methods require additional work to be integrated or have scaling issues.
    Therefore, you have the concept of events, which can be filtered, routed, and
    pushed to different other services or systems with less bandwidth and lower resource
    utilization since the data is transmitted as a stream of small events or datasets.
    Similar to the edge, streaming allows the data to be processed as it arrives without
    incurring any delay.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在EDA之前，轮询和Webhooks是用于在不同组件之间通信事件的常见机制。轮询效率低下，因为总是存在从数据源获取新更新并将其与下游服务同步的滞后。Webhooks并不总是首选，因为它们可能需要自定义授权和身份验证配置。简而言之，这两种方法都需要额外的工作来集成或存在扩展问题。因此，您有事件的概念，它可以被过滤、路由，并以较小的带宽和较低的资源利用率推送到不同的其他服务或系统，因为数据是以小事件或数据集的流的形式传输的。类似于边缘，流式处理允许数据在到达时进行处理，而不会产生任何延迟。
- en: 'Generally, event-driven architectures come in two topologies, the mediator
    topology and the broker topology. We have explained them here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，事件驱动架构有两种拓扑，即中介拓扑和代理拓扑。我们在这里进行了解释：
- en: '**The mediator topology**: There is a need for a central controller or coordinator
    for event processing. This is generally useful when there is a chain of steps
    for processing events.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中介拓扑**：在事件处理中需要一个中央控制器或协调器。这在存在一系列处理事件的步骤时通常很有用。'
- en: '**The broker topology**: There is no mediator, as the events are broadcast
    through a broker to different backend consumers.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理拓扑**：没有中介，因为事件通过代理广播到不同的后端消费者。'
- en: The broker topology is very common with edge workloads since it decouples the
    edge from the cloud and allows the overall solution to scale better. Therefore,
    for the fleet telemetry bounded context, we have designed an event-driven architecture
    using a broker topology, as shown in the following diagram.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它将边缘与云解耦并允许整体解决方案更好地扩展，因此边缘工作负载中非常常见的代理拓扑。因此，对于车队遥测边界上下文，我们设计了一个使用代理拓扑的事件驱动架构，如下所示。
- en: 'In the following data flow, the events streamed from a connected HBS hub (that
    is, the edge) are routed over MQTT to an IoT gateway (that is, AWS IoT Core),
    which allows the filtering of data (if required) through an in-built rules engine
    and persists the data to an ODS (that is, Amazon DynamoDB). Amazon DynamoDB is
    a highly performant nonrelational database service that can scale automatically
    based on the volume of data streamed from millions of edge devices. From the previous
    chapter, you should already be familiar with how to model data and optimize NoSQL
    databases for time series data. Once the data is persisted in Amazon DynamoDB,
    **Create, Read, Update, and Delete** (**CRUD**) operations can be performed on
    top of the data using serverless functions (that is, AWS Lambda). Finally, the
    data is made available through an API access layer (that is, the Amazon API gateway)
    in a synchronous or asynchronous manner:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下数据流中，从连接的 HBS 中心（即边缘）流出的事件通过 MQTT 路由到物联网网关（即 AWS IoT Core），这允许通过内置的规则引擎（如果需要）过滤数据，并将数据持久化到
    ODS（即 Amazon DynamoDB）。Amazon DynamoDB 是一个高性能的非关系型数据库服务，可以根据从数百万边缘设备流出的数据量自动扩展。从上一章，您应该已经熟悉了如何建模数据以及如何优化
    NoSQL 数据库以适应时间序列数据。一旦数据在 Amazon DynamoDB 中持久化，就可以使用无服务器函数（即 AWS Lambda）在数据上执行**创建、读取、更新和删除**（**CRUD**）操作。最后，数据通过
    API 访问层（即 Amazon API 网关）以同步或异步的方式提供。
- en: '![Figure 6.5 – Streaming architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 流式架构]'
- en: '](img/B17595_06_05.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B17595_06_05.jpg]'
- en: Figure 6.5 – Streaming architecture
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 流式架构]'
- en: 'The ubiquitous language used in this bounded context includes the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在此边界上下文中使用的通用语言包括以下内容：
- en: SQL for DynamoDB table access
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQL 访问 DynamoDB 表
- en: Python for developing a lambda function
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 开发 lambda 函数
- en: A REST API for API gateway and DynamoDB access
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于 API 网关和 DynamoDB 访问的 REST API
- en: Stream processing and EDA is powerful for many IoT use cases that require near-real-time
    attention such as alerting, anomaly detection, and more, as it analyzes the data
    as soon as it arrives. However, there is a trade-off with every architecture and
    EDA is no exception either. With a stream, since the processed results are made
    available immediately, the analysis of a particular data point cannot consider
    future values. Even for past values, it's restricted to a shorter time interval,
    which is generally specified through different windowing mechanisms (such as sliding,
    tumbling, and more). And that's where batch processing plays a key role.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理和事件驱动架构（EDA）对于许多需要近乎实时关注的物联网用例非常强大，例如警报、异常检测等，因为它在数据到达时立即分析数据。然而，每个架构都有权衡，EDA也不例外。由于流处理的结果可以立即提供，因此对特定数据点的分析不能考虑未来的值。即使是过去的数据，也受到较短的间隔限制，这通常通过不同的窗口机制（如滑动、滚动等）来指定。这正是批量处理发挥关键作用的地方。
- en: Complex event processing
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复杂事件处理
- en: Let's consider the following scenario where you plan to reduce food wastage
    at your home. Therefore, every time you check-in at a grocery store, you receive
    a notification with a list of perishable items in your refrigerator (or food shelf),
    as they have not even been opened or are underutilized and are nearing their expiry
    date.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下场景，您计划在家减少食物浪费。因此，每次您在杂货店签到时，您都会收到一个通知，列出了冰箱（或食品架）中即将过期的易腐物品，因为这些物品甚至还没有被打开或利用率低，即将到期。
- en: 'This might sound like an easy problem to solve, but there is a certain amount
    of intelligence required at the local hub (such as HBS hub) and a complex event
    processing workflow on the cloud to facilitate this. It might include the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来像是一个容易解决的问题，但需要在本地枢纽（例如HBS枢纽）有一定的智能处理，以及在云端的复杂事件处理工作流程来促进这一点。它可能包括以下内容：
- en: Based on location sharing and user behavior, the ability to recognize the pattern
    (or a special event) that the user plans to do grocery shopping.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于位置共享和用户行为，识别用户计划进行购物的事件模式（或特殊事件）。
- en: Detect from camera sensors installed in the refrigerator or on the food shelves
    that some of the perishable items are due to expire. Alternatively, use events
    from smell sensors to detect a pattern of rotten food items.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从冰箱或食品架中安装的摄像头传感器检测到一些易腐物品即将到期。或者，使用气味传感器来检测腐烂食品的模式。
- en: Correlate all these patterns (that is, the user, location, and food expiry date)
    through state machines and apply business rules to identify the list of items
    that requires attention.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过状态机关联所有这些模式（即用户、位置和食品到期日期），并应用业务规则以识别需要关注的物品清单。
- en: Trigger a microservice(s) to notify this information to the end user.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 触发微服务（s）将此信息通知最终用户。
- en: This problem might become further complicated for a restaurant business due
    to the volume of perishable items and the scale at which they operate. In such
    a scenario, having near real-time visibility to identify waste based on current
    practices can help the business optimize its supply chain and save a lot of costs.
    So, as you can imagine, the convergence of edge and IoT with big data processing
    capabilities such as CEP can help unblock challenging use cases.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于餐饮业来说，由于易腐物品的数量和它们运营的规模，这个问题可能会变得更加复杂。在这种情况下，基于当前实践对浪费的近实时可见性可以帮助企业优化其供应链并节省大量成本。因此，正如你可以想象的那样，边缘和物联网与CEP（复杂事件处理）等大数据处理能力的结合可以帮助解决具有挑战性的用例。
- en: Processing and querying events as they arrive in small chunks or in bulk is
    relatively easier compared to recognizing patterns by correlating events. That's
    where CEP is useful. It's considered as a subset of stream processing with the
    focus to identify special (or complex) events by correlating events from multiple
    sources or by listening to telemetry data for a longer period of time. One of
    the common patterns to implement CEP is by building state machines.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过关联事件来识别模式相比，以小块或批量处理到达的事件进行处理和查询相对容易。这就是CEP发挥作用的地方。它被认为是流处理的一个子集，重点是通过对多个来源的事件进行关联或通过监听更长时间段的遥测数据来识别特殊（或复杂）事件。实现CEP的常见模式之一是通过构建状态机。
- en: 'In the following flow, the events streamed from a connected HBS hub (that is,
    the edge) are routed over MQTT to an IoT gateway (that is, AWS IoT Core), which
    filters the complex events based on set criteria and pushes them to different
    state machines defined within the complex event processing engine (that is, AWS
    IoT events). AWS IoT events is a fully managed CEP service that allows you to
    monitor equipment or device fleets for failure or changes in operation and, thereafter,
    trigger actions based on defined events:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下流程中，从连接的HBS枢纽（即边缘）流出的事件通过MQTT路由到物联网网关（即AWS IoT Core），该网关根据设定的标准过滤复杂事件，并将它们推送到复杂事件处理引擎（即AWS
    IoT事件）内部定义的不同状态机。AWS IoT事件是一个完全管理的CEP服务，允许您监控设备或设备车队以检测故障或操作变化，然后根据定义的事件触发操作：
- en: '![Figure 6.6 – CEP architecture'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – CEP架构'
- en: '](img/B17595_06_06.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17595_06_06.jpg]'
- en: Figure 6.6 – CEP architecture
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – CEP架构
- en: 'The ubiquitous language used in the fleet monitoring bounded context includes
    the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在车队监控的边界上下文中使用的通用语言包括以下内容：
- en: State machines for complex event processing
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂事件处理的状态机
- en: A REST API for notifications or subscriptions through **Amazon Simple Notification
    Service** (**SNS**)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于通知或订阅的REST API通过**Amazon Simple Notification Service**（**SNS**）
- en: CEP can be useful for many IoT use cases that require attention based on events
    from multiple sensors, timelines, or other environmental factors.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: CEP对于许多需要根据来自多个传感器、时间线或其他环境因素的事件进行关注的物联网用例非常有用。
- en: There can be many other design patterns that you need to consider in order to
    design a real-life IoT workload. Those could be functional or non-functional requirements
    such as data archival for regulatory requirements, data replication for redundancy,
    and disaster recovery for achieving required RTO or RPO; however, that's beyond
    the scope of this book, as these are general principles and not necessarily related
    to edge computing or IoT workloads. There are many other books or resources available
    on those topics if they are of interest to you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计现实生活中的物联网工作负载时，可能需要考虑许多其他设计模式。这些可能是功能性的或非功能性的要求，例如为了合规要求进行数据归档、为了冗余进行数据复制以及为了实现所需的RTO或RPO进行灾难恢复；然而，这些内容超出了本书的范围，因为这些是通用原则，并不一定与边缘计算或物联网工作负载相关。如果这些主题对您感兴趣，有许多其他书籍或资源可供参考。
- en: Data flow anti-patterns for the cloud
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云端数据流反模式
- en: 'The anti-patterns for processing data on the cloud from edge devices can be
    better explained using three laws – the law of physics, the law of economics,
    and the law of the land:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用边缘设备在云端处理数据的反模式可以通过三个定律——物理学定律、经济学定律和土地法——来更好地解释：
- en: '**Law of physics**: For use cases where latency is critical, keeping data processing
    closer to the event source is usually the best approach, since we cannot beat
    the speed of light, and thus, the round-trip latency might not be affordable.
    Let''s consider a scenario where an autonomous vehicle needs to apply a hard brake
    after detecting a pedestrian; it cannot afford the round-trip latency from the
    cloud. This factor is also relevant for physically remote environments, such as
    mining, oil, and gas facilities where there is poor or intermittent network coverage.
    Even for our use case here, with connected HBS, if there is a power or network
    outage, the hub is still required to be intelligent enough to detect intrusion
    by analyzing local events.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理学定律**：对于需要低延迟的使用案例，通常将数据处理更靠近事件源是最佳方法，因为我们无法超越光速，因此往返延迟可能无法承受。让我们考虑一个场景，自动驾驶汽车在检测到行人后需要紧急制动；它无法承受从云端返回的往返延迟。这一因素对于物理上偏远的环境也很相关，例如采矿、石油和天然气设施，这些地方网络覆盖差或间歇性。即使在我们这里的使用案例中，如果出现电力或网络故障，中心节点仍然需要足够智能，能够通过分析本地事件来检测入侵。'
- en: '**Law of economics**: The cost of compute and storage has reduced exponentially
    in the last few decades compared to networking cost, which might still become
    prohibitive at scale. Although digital transformation has led to data proliferation
    across different industries, much of the data is of low quality. Therefore, local
    aggregation and the filtering of data on the edge will allow you to publish high-value
    data to the cloud only reducing networking bandwidth costs.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经济学定律**：与网络成本相比，过去几十年中计算和存储的成本呈指数级下降，但网络成本在规模上可能仍然过高。尽管数字化转型导致了不同行业数据量的激增，但其中大部分数据质量较低。因此，边缘的数据聚合和过滤将允许您仅将高价值数据发布到云端，从而降低网络带宽成本。'
- en: '**Law of the land**: Most industries need to comply with regulations or compliance
    requirements related to data sovereignty. Therefore, the local retention of data
    in a specific facility, region, or country might turn out to be a key factor in
    the processing of data. Even for our use case here with connected HBS, the workload
    might need to be in compliance with GDPR requirements.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**土地法**：大多数行业需要遵守与数据主权相关的法规或合规要求。因此，在特定设施、地区或国家本地保留数据可能是数据处理的关键因素。即使在我们这里与连接的HBS相关的使用案例中，工作负载可能需要符合GDPR要求。'
- en: 'AWS offers different edge services for supporting use cases that need to comply
    with the preceding laws and are not limited to IoT services only. For example,
    consider the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供不同的边缘服务来支持需要遵守上述法律的使用案例，而不仅限于物联网服务。例如，考虑以下情况：
- en: '**Infrastructure**: AWS Local Zones, AWS Outposts, and AWS Wavelength'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施**：AWS Local Zones、AWS Outposts和AWS Wavelength'
- en: '**Networking**: Amazon CloudFront, and POP locations'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：Amazon CloudFront和POP位置'
- en: '**Storage**: AWS Storage Gateway'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**：AWS Storage Gateway'
- en: '**Rugged and disconnected edge devices**: AWS Snowball Edge and AWS Snowcone'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**坚固且断开连接的边缘设备**：AWS Snowball Edge和AWS Snowcone'
- en: '**Robotics**: AWS Robomaker'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**：AWS Robomaker'
- en: '**Video analytics**: Amazon Kinesis Video Streams'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频分析**：Amazon Kinesis Video Streams'
- en: '**Machine learning**: Amazon Sagemaker Neo, Amazon Sagemaker Edge Manager,
    Amazon Monitron, and AWS Panorama'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习**：Amazon Sagemaker Neo、Amazon Sagemaker Edge Manager、Amazon Monitron和AWS
    Panorama'
- en: The preceding services are beyond the scope of the book, and the information
    is only provided for you to be well informed of the breadth and depth of AWS edge
    services.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 上述服务超出了本书的范围，信息仅提供给你，以便你能够全面了解AWS边缘服务的广度和深度。
- en: A hands-on approach with the lab
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过实验室的动手方法
- en: 'In this section, you will learn how to design a piece of architecture on the
    cloud leveraging the concepts that you have learned in this chapter. Specifically,
    you will continue to use the lambda architecture pattern introduced in [*Chapter
    5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090), *Ingesting and Streaming
    Data from the Edge*, to process the data on the cloud:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何利用本章学到的概念在云上设计一个架构。具体来说，你将继续使用在[*第5章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)，“从边缘摄取和流式传输数据”中引入的lambda架构模式，在云上处理数据：
- en: '![Figure 6.7 – Hands-on architecture'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – 动手架构'
- en: '](img/B17595_06_07.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_07.jpg)'
- en: Figure 6.7 – Hands-on architecture
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 动手架构
- en: 'In the previous chapter, you already completed steps 1 and 4\. This chapter
    will help you to complete steps 2, 3, 5, 6, and 7, which includes consuming the
    telemetry data and building an analytics pipeline for performing BI:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经完成了步骤1和4。本章将帮助你完成步骤2、3、5、6和7，包括消费遥测数据和构建用于执行BI的分析管道：
- en: '![Figure 6.8 – Hands-on lab components'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – 动手实验室组件'
- en: '](img/B17595_06_08.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17595_06_08.jpg)'
- en: Figure 6.8 – Hands-on lab components
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 动手实验室组件
- en: 'In this hands-on section, your objectives include the following:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节动手实践中，你的目标包括以下内容：
- en: Query the ODS.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询ODS。
- en: Build an API interface layer to enable data consumption.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个API接口层以启用数据消费。
- en: Build an ETL layer for processing telemetry data in the data lake.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据湖中为处理遥测数据构建ETL层。
- en: Visualize the data through a BI tool.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过BI工具可视化数据。
- en: Building cloud resources
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建云资源
- en: This lab builds on top of the cloud resources that you have already deployed
    in [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090), *Ingesting and
    Streaming Data from the Edge*. So, please ensure you have completed the hands-on
    section there prior to proceeding with the following steps here. In addition,
    please go ahead and deploy the CloudFormation template from the `chapter 6/cfn`
    folder to create the resources required in this lab, such as the AWS API gateway,
    lambda functions, and AWS Glue crawler.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验建立在你在[*第5章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)，“从边缘摄取和流式传输数据”中已部署的云资源之上。因此，请确保你在继续以下步骤之前已经完成了那里的动手实践部分。此外，请继续从`chapter
    6/cfn`文件夹部署CloudFormation模板，以创建本实验所需的资源，例如AWS API网关、lambda函数和AWS Glue爬虫。
- en: Note
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Please retrieve the parameters required for this CloudFormation template (such
    as an S3 bucket) from the *Output* section of the deployed CloudFormation stack
    of [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090), *Ingesting and
    Streaming Data from the Edge*.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请从[*第5章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)，“从边缘摄取和流式传输数据”中已部署的CloudFormation堆栈的*输出*部分检索此CloudFormation模板所需的参数（例如S3存储桶）。
- en: In addition to this, you can always find the specific resource names required
    for this lab (such as lambda functions) from the *Resources* or *Output* sections
    of the deployed CloudFormation stack. It is a good practice to copy those into
    a notepad locally so that you can refer to them quickly.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以从已部署的CloudFormation堆栈的*资源*或*输出*部分找到本实验所需的具体资源名称（例如lambda函数）。将它们复制到本地记事本中是一个好习惯，这样你可以快速参考它们。
- en: Once the CloudFormation has been deployed successfully, please continue with
    the following steps.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦CloudFormation成功部署，请继续以下步骤。
- en: Querying the ODS
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查询ODS
- en: 'Navigate to the AWS console and try to generate insights from the data persisted
    in the operational (or transactional) data store. As you learned in the previous
    chapter, all the near real-time data processed is persisted in a DynamoDB table
    (`packt_sensordata`):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到AWS控制台，并尝试从操作（或事务）数据存储中持久化的数据中生成洞察。正如你在上一章所学，所有近实时处理的数据都持久保存在DynamoDB表中（`packt_sensordata`）：
- en: To query the data, navigate to **DynamoDB Console**, select **Tables** (from
    the left-hand pane), click on the table, and then click on **View items**.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查询数据，请导航到**DynamoDB控制台**，从左侧面板中选择**表**，点击表，然后点击**查看项目**。
- en: Click on the `1` into the `device_id` partition key and click on **Run**. This
    should return a set of data points with all the attributes.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `device_id` 分区键中点击 `1`，然后点击**运行**。这应该会返回包含所有属性的数据点集。
- en: 'Expand the filters section, and add filters to the following attributes:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开过滤器部分，并添加以下属性的过滤器：
- en: '`temperature`.'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`温度`。'
- en: '`humidity`.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`湿度`。'
- en: '**Type** – **Number**.'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型** – **数字**。'
- en: '**Condition** – **Greater than or Equal**.'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件** – **大于或等于**。'
- en: '**Value** – **35**.'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值** – **35**。'
- en: Click on **Run**.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击**运行**。
- en: Here, the query interface allows you to filter the data quickly based on different
    criteria. If you are familiar with SQL, you can also try the PartiQL editor, which
    is on the DynamoDB console.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，查询接口允许您根据不同的标准快速过滤数据。如果您熟悉 SQL，您还可以尝试 DynamoDB 控制台上的 PartiQL 编辑器。
- en: Additionally, DynamoDB allows you to scan an entire table or index, but this
    is generally an expensive operation, particularly for a large dataset. To scan
    a table, click on the **Scan** tab (which is adjacent to **Query**) and then click
    on **Run**.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，DynamoDB 允许您扫描整个表或索引，但这通常是一个昂贵的操作，尤其是对于大数据集。要扫描表，请点击**扫描**标签（位于**查询**标签旁边），然后点击**运行**。
- en: For better performance and faster response times, we recommend that you use
    **Query** over **Scan**.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更好的性能和更快的响应时间，我们建议您使用**查询**而不是**扫描**。
- en: AWS Lambda
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Lambda
- en: 'In addition to having interactive query capabilities on data, you will often
    need to build a presentation layer and business logic for various other personas
    (such as consumers, fleet operators, and more) to access data. You can define
    the business logic layer using Lambda:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在数据上具有交互式查询功能外，您通常还需要为各种其他角色（如消费者、车队运营商等）构建表示层和业务逻辑来访问数据。您可以使用 Lambda 定义业务逻辑层：
- en: Navigate to the **AWS Lambda** console. Click on **Functions** (from the left-hand
    pane), and choose the function created using the CloudFormation template earlier.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 **AWS Lambda** 控制台。点击左侧面板中的**函数**，然后选择之前使用 CloudFormation 模板创建的函数。
- en: 'Do you remember that we created two facets (`getItems` and `putItems`) during
    the data modeling exercise in [*Chapter 5*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090),
    *Ingesting and Streaming Data from the Edge*, to access data? The following is
    the logic embedded in a lambda function to implement the equivalent functional
    construct. Please review the code to understand how the `get` and `put` functionalities
    work:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还记得我们在[*第 5 章*](B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090)，“从边缘摄取和流式传输数据”的数据建模练习中创建了两个方面（`getItems`
    和 `putItems`）来访问数据吗？以下是在 Lambda 函数中嵌入的逻辑，以实现等效的功能结构。请审查代码以了解 `get` 和 `put` 功能是如何工作的：
- en: '[PRE0]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Please note that here, we are using lambda functions. This is because serverless
    functions have become a common pattern to process event-driven data in near real
    time. Since it alleviates the need for you to manage or operate any servers throughout
    the life cycle of the application, your only responsibility is to write the code
    in a supported language and upload it to the Lambda console.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，我们使用 Lambda 函数。这是因为无服务器函数已成为处理近实时事件驱动数据的一种常见模式。由于它减轻了您在整个应用程序生命周期中管理或操作任何服务器的需求，您的唯一责任是使用支持的语言编写代码并将其上传到
    Lambda 控制台。
- en: Fun fact
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实
- en: AWS IoT Greengrass provides a Lambda runtime environment for the edge along
    with different languages such as Python, Java, Node.js, and C++. That means you
    don't need to manage two different code bases (such as embedded and cloud) or
    multiple development teams. This will cut down your development time, enable a
    uniform development stack from the edge to the cloud, and accelerate your time
    to market.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: AWS IoT Greengrass 为边缘设备提供 Lambda 运行时环境，以及 Python、Java、Node.js 和 C++ 等多种语言。这意味着您不需要管理两个不同的代码库（例如嵌入式和云）或多个开发团队。这将缩短您的开发时间，从边缘到云实现统一的开发生态，并加速您的上市时间。
- en: Amazon API gateway
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon API 网关
- en: 'Now the business logic has been developed using the lambda function, let''s
    create the HTTP interface (aka the presentation layer) using Amazon API gateway.
    This is a managed service for creating, managing, and deploying APIs at scale:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在业务逻辑已经使用 Lambda 函数开发，让我们使用 Amazon API 网关创建 HTTP 接口（即表示层）。这是一个用于创建、管理和大规模部署
    API 的托管服务：
- en: Navigate to the `MyPacktAPI`) created using the CloudFormation template.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到使用 CloudFormation 模板创建的 `MyPacktAPI`。
- en: Expand the `REST` methods.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开REST方法。
- en: 'You should observe the following operations:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该观察以下操作：
- en: '[PRE1]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Continue underneath the **Develop** drop-down menu. Click on **Authorization**
    and check the respective operations. We have not attached any authorizer in this
    lab, but it's recommended for real-world workloads. API gateway offers different
    forms of authorizers, including built-in IAM integrations, **JSON Web Tokens**
    (**JWT**), or custom logic using lambda functions.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**开发**下拉菜单下继续操作。点击**授权**并检查相应的操作。在这个实验室中，我们没有附加任何授权者，但在实际工作中推荐使用。API网关提供了不同形式的授权者，包括内置的IAM集成、**JSON
    Web Tokens**（**JWT**）或使用lambda函数的自定义逻辑。
- en: Next, click on `/items GET`). On the right-hand pane, you will see the associated
    lambda functions. For simplicity, we are using the same lambda function here for
    all operations, but you can choose other functions or targets such as Amazon SQS,
    Amazon Kinesis, a private resource in your VPC, or any other HTTP URI if required
    for your real-world use case.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击`/items GET`)。在右侧面板中，您将看到相关的lambda函数。为了简化，我们在这里对所有操作使用相同的lambda函数，但您可以选择其他函数或目标，例如Amazon
    SQS、Amazon Kinesis、您VPC中的私有资源，或者根据您的实际用例需要，任何其他HTTP URI。
- en: There are many additional options offered by API gateway that relate to CORS,
    reimport/export, and throttling, but they are not considered in the scope of this
    lab. Instead, we will focus on executing the HTTP APIs and retrieving the sensor
    data.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: API网关提供了许多与CORS、重新导入/导出和节流相关的附加选项，但它们不在本实验室的范围内。相反，我们将专注于执行HTTP API并检索传感器数据。
- en: 'Click on the `GET`) items from your device Terminal:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的设备终端中点击`GET`) `items`：
- en: '[PRE2]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You should see a long list of items on our Terminal that has been retrieved
    from the `dynamodb` table.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该在终端上看到一个长列表的项目，这些项目是从`dynamodb`表中检索出来的。
- en: Amazon API gateway allows you to create different types of APIs, and the one
    configured earlier falls into the HTTP API category that allows access to lambda
    functions and other HTTP endpoints. Additionally, we could have used the REST
    APIs here, but the HTTP option was chosen for its simplicity of use, as it can
    automatically stage and deploy the required APIs without any additional effort
    and can be more cost-effective. In summary, you have now completed implementing
    the bounded context of an ODS through querying or API interfaces.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊API网关允许您创建不同类型的API，之前配置的那个就属于HTTP API类别，它允许访问lambda函数和其他HTTP端点。此外，我们在这里也可以使用REST
    API，但选择了HTTP选项，因为它使用简单，可以自动部署所需的API而无需额外努力，并且可能更经济高效。总之，您现在已经通过查询或API接口完成了ODS边界上下文的实现。
- en: Building the analytics workflow
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分析工作流
- en: In this next section, you will build an analytics pipeline on the batch data
    persisted on Amazon S3 (that is, the data lake). To achieve this, you can use
    AWS Glue to crawl data and generate a data catalog. Thereafter, you will use Athena
    for interactive querying and use QuickSight for visualizing data through charts/dashboards.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将在Amazon S3上持久化的批量数据上构建一个分析管道（即数据湖）。为此，您可以使用AWS Glue爬取数据并生成数据目录。之后，您将使用Athena进行交互式查询，并使用QuickSight通过图表/仪表板可视化数据。
- en: AWS Glue
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Glue
- en: 'AWS Glue is a managed service that offers many ETL functionalities, such as
    data crawlers, data catalogs, batch jobs, integration with CI/CD pipelines, Jupyter
    notebook integration, and more. Primarily, you will use the data crawler and cataloging
    capabilities in this lab. We feel that might be sufficient for IoT professionals
    since data engineers will be mostly responsible for these activities in the real
    world. However, if you believe in learning and are curious, please feel free to
    play with the other features:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue是一项托管服务，提供了许多ETL功能，如数据爬虫、数据目录、批量作业、与CI/CD管道的集成、Jupyter笔记本集成等。主要来说，您将在本实验室中使用数据爬虫和目录功能。我们认为这应该足够满足物联网专业人士的需求，因为数据工程师在现实世界中将主要负责这些活动。然而，如果您相信学习并对此好奇，请随意探索其他功能：
- en: Navigate to the **AWS Glue** console, click on **Crawlers** (from the left-hand
    pane), and select the crawler created earlier using the CloudFormation template.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**AWS Glue**控制台，点击左侧面板中的**爬虫**，并使用CloudFormation模板选择之前创建的爬虫。
- en: 'Review some of the key attributes of the crawler definition, such as the following:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看爬虫定义的一些关键属性，如下所示：
- en: 'State: Is the crawler ready to run?'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态：爬虫是否已准备好运行？
- en: 'Schedule: Is the frequency of the crawler set correctly?'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度：爬虫的频率设置正确吗？
- en: 'Data store: S3.'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储：S3。
- en: 'Include path: Is the location of the dataset correct? This should point to
    the raw sensor data bucket.'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含路径：数据集的位置是否正确？这应该指向原始传感器数据存储桶。
- en: 'Configuration options: Is the table definition being updated in the catalog
    based on upstream changes?'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置选项：表定义是否根据上游更改在目录中更新？
- en: Additionally, Glue allows you to process different data formats through its
    classifier functionality. You can process the most common data formats such as
    Grok, XML, JSON, and CSV with its built-in classifiers along with specifying custom
    patterns if you have data in a proprietary format.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，Glue 允许您通过其分类器功能处理不同的数据格式。您可以使用内置的分类器处理最常见的数据格式，如 Grok、XML、JSON 和 CSV，如果您有专有格式的数据，还可以指定自定义模式。
- en: Here, the crawler should run on the specified schedule configured through CloudFormation;
    however, you can also run it on demand by clicking on **Run Crawler**. If you
    do the same, please wait for the crawler to complete the transition from the **starting**
    -> **running** -> **stopping** -> **ready** status.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，爬虫应该按照通过 CloudFormation 配置的指定计划运行；然而，您也可以通过点击 **运行爬虫** 来按需运行它。如果您这样做，请等待爬虫完成从
    **启动** -> **运行** -> **停止** -> **就绪** 状态的转换。
- en: Now the data crawling is complete, navigate to `*packt*` has been created. If
    you have a lot of tables already created, another quick option is to use the search
    button and filter on `packt_gluedb`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据抓取已完成，导航到 `*packt*` 已创建。如果您已经创建了大量的表，另一个快速选项是使用搜索按钮并筛选 `packt_gluedb`。
- en: Click on the table to verify the properties, such as the database, the location,
    the input/output formats, and the table schema. Confirm the schema is showing
    the attributes that you are interested in retaining. If not, you can click on
    **Edit** schema and make the necessary changes:![Figure 6.9 – The table schema
    in Glue
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击表以验证属性，例如数据库、位置、输入/输出格式和表架构。确认架构是否显示了您希望保留的属性。如果不是，您可以点击 **编辑** 架构并进行必要的更改：![图
    6.9 – Glue 中的表架构
- en: '](img/B17595_06_09.jpg)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17595_06_09.jpg)'
- en: Figure 6.9 – The table schema in Glue
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.9 – Glue 中的表架构
- en: Keep a note of the database and the table name, as you will need them in the
    next two sections.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录数据库和表名，因为您在下一两个部分中需要它们。
- en: In this lab, you used a crawler with a single data source only; however, you
    can add multiple data sources if required by your use case. Once the data catalog
    is updated and the data (or metadata) is available, you can consume it through
    different AWS services. You might need to often clean, filter, or transform your
    data as well. However, these responsibilities are not generally performed by the
    IoT practitioners and, primarily, fall with the data analysts or data scientists.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实验中，您只使用了一个数据源进行爬取；然而，如果您的用例需要，您可以添加多个数据源。一旦数据目录更新并且数据（或元数据）可用，您可以通过不同的 AWS
    服务来消费它们。您可能还需要经常清理、筛选或转换您的数据。然而，这些责任通常不是由物联网从业者执行的，主要是由数据分析师或数据科学家负责。
- en: Amazon Athena
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon Athena
- en: 'Amazon Athena acts as a serverless data warehouse where you run analytical
    queries on the data that''s curated by an ETL engine such as Glue. Athena uses
    a schema-on-read approach; thus, a schema is projected onto your data when you
    run a query. And since Athena enables the decoupling of the compute and storage
    layers, you can connect to different data lake services such as S3 to run these
    queries on. Athena uses Apache Hive for DDL operations such as defining tables
    and creating databases. For the different functions supported through queries,
    Presto is used under the hood. Both Hive and Presto are open source SQL engines:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Athena 作为无服务器数据仓库，您可以在由 Glue 等ETL 引擎整理的数据上运行分析查询。Athena 使用读取时模式；因此，当您运行查询时，架构会被投影到您的数据上。由于
    Athena 允许计算层和存储层的解耦，您可以将不同的数据湖服务（如 S3）连接起来以运行这些查询。Athena 使用 Apache Hive 进行DDL操作，如定义表和创建数据库。对于通过查询支持的不同功能，底层使用
    Presto。Hive 和 Presto 都是开源 SQL 引擎：
- en: Navigate to the **AWS Athena** console, and choose **Data Sources** from the
    left-hand pane.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 **AWS Athena** 控制台，并从左侧面板中选择 **数据源**。
- en: 'Keep the data source as `packt_gluedb`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持数据源为 `packt_gluedb`：
- en: This was created in the previous section by the Glue crawler automatically after
    scanning the S3 destination bucket, which is storing the batched sensor data.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是在上一节中由 Glue 爬虫自动创建的，在扫描存储批处理传感器数据的 S3 目标存储桶之后。
- en: This should populate the list of tables created under this database.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该会填充在此数据库下创建的表列表。
- en: Click on the three dots adjacent to the table resembling the name of `*mysensordatabucket*`
    and select **Preview table**. This should automatically build and execute the
    SQL query.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击与`*mysensordatabucket*`名称相似的表旁边的三个点，选择**预览表**。这将自动构建并执行SQL查询。
- en: This should bring up the data results with only 10 records. If you would like
    to view the entire dataset, please remove the 10-parameter limit from the end
    of the query. If you are familiar with SQL, please feel free to tweak the query
    and play with different attributes or join conditions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示只有10条记录的数据结果。如果您想查看整个数据集，请从查询末尾删除10个参数的限制。如果您熟悉SQL，请随意调整查询并尝试不同的属性或连接条件。
- en: Note
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Here, you processed JSON data streamed from an HBS hub device. But what if your
    organization wants to leverage a more lightweight data format? Athena offers native
    support for various data formats such as `CSV`, `AVRO`, `Parquet`, and `ORC` through
    the use of **serializer-deserializer** (**SerDe**) libraries. Even complex schemas
    are supported through regular expressions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您处理了从HBS中心设备流出的JSON数据。但如果您的组织希望利用更轻量级的数据格式呢？Athena通过使用**序列化-反序列化**（**SerDe**）库原生支持各种数据格式，如`CSV`、`AVRO`、`Parquet`和`ORC`。即使是复杂的模式也通过正则表达式得到支持。
- en: So far, you have crawled the data from the data lake, created the tables, and
    successfully queried the data. Now, in the final step, let's learn how to build
    dashboards and charts that can enable BI on this data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经从数据湖中爬取了数据，创建了表，并成功查询了数据。现在，在最后一步，让我们学习如何构建可以在此数据上启用BI的仪表板和图表。
- en: QuickSight
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QuickSight
- en: 'As an IoT practitioner, building business dashboards might not be part of your
    core responsibilities. However, some basic knowledge is always useful. If you
    think of traditional BI solutions, it might take data engineers weeks or months
    to build complex interactive ad hoc data exploration and visualization capabilities.
    Therefore, business users are constrained to pre-canned reports and preselected
    queries. Also, these traditional BI solutions require significant upfront investments
    and don''t perform as well at scale as the size of data sources grow. That''s
    where Amazon QuickSight helps. It''s a managed service that''s easy to use, highly
    scalable, and supports complex capabilities required for business:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 作为物联网从业者，构建业务仪表板可能不是您核心职责的一部分。然而，一些基本知识总是有用的。如果您考虑传统的BI解决方案，数据工程师可能需要几周或几个月的时间来构建复杂的交互式即席数据探索和可视化能力。因此，业务用户被限制在预定义的报告和预选查询中。此外，这些传统的BI解决方案需要大量的前期投资，并且当数据源规模增长时，其性能不如Amazon
    QuickSight。这就是Amazon QuickSight发挥作用的地方。它是一个易于使用、高度可扩展且支持业务所需复杂功能的管理服务：
- en: 'Navigate to the Amazon QuickSight console and complete the one-time setup,
    as explained here:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到Amazon QuickSight控制台并完成如这里所述的一次性设置：
- en: Enroll for the Standard Edition (if you have not used it before).
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您之前没有使用过，请注册标准版。
- en: Purchase SPICE capacity for the lab
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为实验室购买SPICE容量
- en: '*Note that this has a 60-day trial, so be sure to cancel the subscription after
    the workshop to prevent getting charged*.'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，这有一个60天的试用期，所以请确保在研讨会结束后取消订阅，以防止被收费。
- en: Click on your login user (in the upper-right corner), and select **Manage QuickSight**
    | **Security & Permissions** | **Add and Remove** | **Check Amazon Athena** |
    **Apply**.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击右上角的登录用户，然后选择**管理QuickSight** | **安全与权限** | **添加和删除** | **检查Amazon Athena**
    | **应用**。
- en: Click on the QuickSight logo (in the upper-left corner) to navigate to the home
    page.
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击左上角的QuickSight标志以导航到主页。
- en: Click on your login user (in the upper-right corner) and you will observe that
    your region preference is listed beneath your language preference.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击右上角的登录用户，您将观察到您的区域首选项列在您的语言首选项下方。
- en: Confirm or update the region so that it matches your working region.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认或更新区域，使其与您的作业区域相匹配。
- en: Click on **New Analysis**, then **New dataset**, and choose **Athena**.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**新建分析**，然后**新建数据集**，并选择**Athena**。
- en: Enter the data source name as `packt-data-visualization`, keep the workgroup
    as its default setting, and click on **Create Data Source**.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据源名称输入为`packt-data-visualization`，保持工作组为其默认设置，然后点击**创建数据源**。
- en: Keep the Catalog as **default**, choose **Database**, and then select the table
    created in *step 5* of the *AWS Glue* section.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持目录为**默认**，选择**数据库**，然后选择在*AWS Glue*部分的*步骤5*中创建的表。
- en: Click on **Select**, choose to directly query your data, and then click on **Visualize**
    again.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**选择**，选择直接查询你的数据，然后再次点击**可视化**。
- en: 'Now build the dashboard:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在构建仪表板：
- en: Choose a timestamp for the *X*-axis (select **MINUTE** from the **Value** drop-down
    menu).
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择X轴的日期时间戳（从**值**下拉菜单中选择**分钟**）。
- en: Choose the other readings such as **device_id**, **temperature**, and **humidity**
    for the *Y*-axis (select **Average** from the **Value** drop-down menu for each
    reading).
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择其他读取值，如**device_id**、**温度**和**湿度**作为Y轴（从每个读取的**值**下拉菜单中选择**平均值**）。
- en: Feel free to play with different fields or visual types to visualize other smart
    home-related information. As you might have observed, while creating the dataset,
    QuickSight natively supports different AWS and third-party data sources such as
    Salesforce, ServiceNow, Adobe Analytics, Twitter, Jira, and more. Additionally,
    it allows instant access to the data through mobile apps (such as iOS and Android)
    for business users or operations to quickly infer data insights for a specific
    workload along with integrations to machine learning augmentation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试不同的字段或视觉类型来可视化其他智能家居相关的信息。正如你可能观察到的，在创建数据集时，QuickSight原生支持不同的AWS和第三方数据源，如Salesforce、ServiceNow、Adobe
    Analytics、Twitter、Jira等。此外，它允许通过移动应用（如iOS和Android）为商业用户或操作快速推断特定工作负载的数据洞察，同时还集成了机器学习增强。
- en: Congratulations! You have completed the entire life cycle of data processing
    and data consumption on the cloud using different AWS applications and data services.
    Now, let's wrap up this chapter with a summary and knowledge-check questions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经使用不同的AWS应用程序和数据服务完成了云上数据处理的整个生命周期。现在，让我们通过摘要和知识检查问题来结束本章。
- en: Challenge zone (Optional)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战区域（可选）
- en: In the *Amazon API gateway* section, you built an interface to retrieve all
    the items from the `dynamodb` table. However, what if you need to extract a specific
    item (or set of items) for a particular device such as HVAC? That can be a less
    costly operation compared to scanning all data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Amazon API网关*部分，你构建了一个接口来检索`dynamodb`表中的所有项目。然而，如果你需要提取特定设备（如HVAC）的特定项目（或项目集），这可以比扫描所有数据更节省成本。
- en: '`GET /items {device_id}`. Check the lambda function to gain a better understanding
    of how it will map to the backend logic.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET /items {device_id}`. 检查lambda函数以更好地理解它如何映射到后端逻辑。'
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you were introduced to big data concepts relevant to IoT workloads.
    You learned how to design data flows using DDD approach along with different data
    storage and data integration patterns that are common with IoT workloads. You
    implemented a lambda architecture to process fleet telemetry data and an analytical
    pipeline. Finally, you validated the workflow by consuming data through the APIs
    and visualizing it through business dashboards. In the next chapter, you will
    learn how all of this data can be used to build, train, and deploy machine learning
    models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了与物联网工作负载相关的大数据概念。你学习了如何使用DDD方法设计数据流，以及与物联网工作负载常见的不同数据存储和数据集成模式。你实现了一个lambda架构来处理车队遥测数据和数据分析管道。最后，你通过通过API消费数据和通过业务仪表板可视化来验证了工作流程。在下一章中，你将学习如何使用所有这些数据来构建、训练和部署机器学习模型。
- en: Knowledge check
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识检查
- en: 'Before moving on to the next chapter, test your knowledge by answering these
    questions. The answers can be found at the end of the book:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续下一章之前，通过回答这些问题来测试你的知识。答案可以在书的末尾找到：
- en: Can you think of at least two benefits of domain-driven design from the standpoint
    of edge workloads?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到至少两个从边缘工作负载的角度来看领域驱动设计的优势吗？
- en: 'True or false: bounded context and ubiquitous language are the same.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对或错：边界上下文和通用语言是相同的。
- en: What do you think is necessary to have an operational datastore or a data lake/data
    warehouse?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你认为要有一个可操作的数据存储或数据湖/数据仓库需要什么？
- en: Can you recall the design pattern name that brings together streaming and batch
    workflows?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能回忆起将流式处理和批量工作流程结合在一起的设计模式名称吗？
- en: What strategy could you incorporate to transform raw data on the cloud?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以采用什么策略来将云上的原始数据转换？
- en: 'True or false: You cannot access data from a NoSQL data store through APIs.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对或错：你不能通过API访问NoSQL数据存储中的数据。
- en: When would you use a mediator versus broker topology for the event-driven workload?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会在什么情况下使用中介拓扑而不是代理拓扑来处理事件驱动的工作负载？
- en: Can you think of at least one benefit of using a serverless function for processing
    IoT data?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想到使用无服务器函数处理物联网数据至少一个好处吗？
- en: What **business intelligence** (**BI**) services can you use for data exposition
    to end consumers?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用哪些**商业智能**（**BI**）服务向最终消费者展示数据？
- en: 'True or false: JSON is the most optimized data format for big data processing
    on the cloud.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对或错：JSON是云上大数据处理最优化数据格式。
- en: How would you build an API interface on top of your operational data store (or
    data lake)?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会如何在你的运营数据存储（或数据湖）上构建API接口？
- en: References
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Take a look at the following resources for additional information on the concepts
    discussed in this chapter:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下资源，以获取本章讨论的概念的更多信息：
- en: '*Data Management – Body of Knowledge*: [https://www.dama.org/cpages/body-of-knowledge](https://www.dama.org/cpages/body-of-knowledge)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据管理 - 知识体系*：[https://www.dama.org/cpages/body-of-knowledge](https://www.dama.org/cpages/body-of-knowledge)'
- en: '*Domain-Driven Design* by Eric Evans: [https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software-ebook/dp/B00794TAUG](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software-ebook/dp/B00794TAUG)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*埃里克·埃文斯的领域驱动设计*：[https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software-ebook/dp/B00794TAUG](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software-ebook/dp/B00794TAUG)'
- en: '*Domain Language*: [https://www.domainlanguage.com/ddd](https://www.domainlanguage.com/ddd)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域语言*：[https://www.domainlanguage.com/ddd](https://www.domainlanguage.com/ddd)'
- en: '*Big Data on AWS*: [https://aws.amazon.com/big-data/use-cases/](https://aws.amazon.com/big-data/use-cases/)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS上的大数据*：[https://aws.amazon.com/big-data/use-cases/](https://aws.amazon.com/big-data/use-cases/)'
- en: '*AWS Serverless Data Analytics Pipeline*: [https://d1.awsstatic.com/whitepapers/aws-serverless-data-analytics-pipeline.pdf](https://d1.awsstatic.com/whitepapers/aws-serverless-data-analytics-pipeline.pdf)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS无服务器数据分析管道*：[https://d1.awsstatic.com/whitepapers/aws-serverless-data-analytics-pipeline.pdf](https://d1.awsstatic.com/whitepapers/aws-serverless-data-analytics-pipeline.pdf)'
- en: 'Modern serverless architecture on AWS: [https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/mobile-web-serverless-RA.pdf?did=wp_card&trk=wp_card](https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/mobile-web-serverless-RA.pdf?did=wp_card&trk=wp_card)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS上的现代无服务器架构：[https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/mobile-web-serverless-RA.pdf?did=wp_card&trk=wp_card](https://d1.awsstatic.com/architecture-diagrams/ArchitectureDiagrams/mobile-web-serverless-RA.pdf?did=wp_card&trk=wp_card)
- en: '*BI Tools*: [https://aws.amazon.com/blogs/big-data/tag/bi-tools/](https://aws.amazon.com/blogs/big-data/tag/bi-tools/)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*BI工具*：[https://aws.amazon.com/blogs/big-data/tag/bi-tools/](https://aws.amazon.com/blogs/big-data/tag/bi-tools/)'
