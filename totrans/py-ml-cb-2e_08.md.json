["```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom scipy.io import wavfile \n```", "```py\n# Read the input file \nsampling_freq, audio = wavfile.read('input_read.wav') \n```", "```py\n# Print the params \nprint('Shape:', audio.shape)\nprint('Datatype:', audio.dtype)\nprint('Duration:', round(audio.shape[0] / float(sampling_freq), 3), 'seconds')\n```", "```py\n# Normalize the values \naudio = audio / (2.**15) \n```", "```py\n# Extract first 30 values for plotting \naudio = audio[:30] \n```", "```py\n# Build the time axis \nx_values = np.arange(0, len(audio), 1) / float(sampling_freq) \n```", "```py\n# Convert to seconds \nx_values *= 1000\n```", "```py\n# Plotting the chopped audio signal \nplt.plot(x_values, audio, color='black') \nplt.xlabel('Time (ms)') \nplt.ylabel('Amplitude') \nplt.title('Audio signal') \nplt.show() \n```", "```py\nShape: (132300,)\nDatatype: int16\nDuration: 3.0 seconds\n```", "```py\nimport numpy as np \nfrom scipy.io import wavfile \nimport matplotlib.pyplot as plt \n```", "```py\n# Read the input file \nsampling_freq, audio = wavfile.read('input_freq.wav') \n```", "```py\n# Normalize the values \naudio = audio / (2.**15) \n```", "```py\n# Extract length \nlen_audio = len(audio)\n```", "```py\n# Apply Fourier transform \ntransformed_signal = np.fft.fft(audio) \nhalf_length = np.ceil((len_audio + 1) / 2.0) \ntransformed_signal = abs(transformed_signal[0:int(half_length)]) \ntransformed_signal /= float(len_audio) \ntransformed_signal **= 2\n```", "```py\n# Extract length of transformed signal \nlen_ts = len(transformed_signal) \n```", "```py\n# Take care of even/odd cases \nif len_audio % 2: \n    transformed_signal[1:len_ts] *= 2 \nelse: \n    transformed_signal[1:len_ts-1] *= 2 \n```", "```py\n# Extract power in dB \npower = 10 * np.log10(transformed_signal) \n```", "```py\n# Build the time axis \nx_values = np.arange(0, half_length, 1) * (sampling_freq / len_audio) / 1000.0 \n```", "```py\n# Plot the figure \nplt.figure() \nplt.plot(x_values, power, color='black') \nplt.xlabel('Freq (in kHz)') \nplt.ylabel('Power (in dB)') \nplt.show()\n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom scipy.io.wavfile import write \n```", "```py\n# File where the output will be saved \noutput_file = 'output_generated.wav' \n```", "```py\n# Specify audio parameters \nduration = 3  # seconds \nsampling_freq = 44100  # Hz \ntone_freq = 587 \nmin_val = -2 * np.pi \nmax_val = 2 * np.pi \n```", "```py\n# Generate audio \nt = np.linspace(min_val, max_val, duration * sampling_freq) \naudio = np.sin(2 * np.pi * tone_freq * t)\n```", "```py\n# Add some noise \nnoise = 0.4 * np.random.rand(duration * sampling_freq) \naudio += noise\n```", "```py\n# Scale it to 16-bit integer values \nscaling_factor = pow(2,15) - 1 \naudio_normalized = audio / np.max(np.abs(audio)) \naudio_scaled = np.int16(audio_normalized * scaling_factor)\n```", "```py\n# Write to output file \nwrite(output_file, sampling_freq, audio_scaled) \n```", "```py\n# Extract first 100 values for plotting \naudio = audio[:100] \n```", "```py\n# Build the time axis \nx_values = np.arange(0, len(audio), 1) / float(sampling_freq) \n```", "```py\n# Convert to seconds \nx_values *= 1000 \n```", "```py\n# Plotting the chopped audio signal \nplt.plot(x_values, audio, color='black') \nplt.xlabel('Time (ms)') \nplt.ylabel('Amplitude') \nplt.title('Audio signal') \nplt.show()\n```", "```py\nimport json \nimport numpy as np \nfrom scipy.io.wavfile import write \n```", "```py\n# Synthesize tone \ndef synthesizer(freq, duration, amp=1.0, sampling_freq=44100):\n```", "```py\n    # Build the time axis \n    t = np.linspace(0, duration, round(duration * sampling_freq)) \n```", "```py\n    # Construct the audio signal \n    audio = amp * np.sin(2 * np.pi * freq * t) \n\n    return audio.astype(np.int16)  \n```", "```py\nif __name__=='__main__': \n    tone_map_file = 'tone_freq_map.json' \n```", "```py\n    # Read the frequency map \n    with open(tone_map_file, 'r') as f: \n        tone_freq_map = json.loads(f.read()) \n```", "```py\n    # Set input parameters to generate 'G' tone \n    input_tone = 'G' \n    duration = 2     # seconds \n    amplitude = 10000 \n    sampling_freq = 44100    # Hz \n```", "```py\n    # Generate the tone \n    synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq) \n```", "```py\n    # Write to the output file \n    write('output_tone.wav', sampling_freq, synthesized_tone)\n```", "```py\n    # Tone-duration sequence \n    tone_seq = [('D', 0.3), ('G', 0.6), ('C', 0.5), ('A', 0.3), ('Asharp', 0.7)] \n```", "```py\n    # Construct the audio signal based on the chord sequence \n    output = np.array([]) \n    for item in tone_seq: \n        input_tone = item[0] \n        duration = item[1] \n        synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq) \n        output = np.append(output, synthesized_tone, axis=0) \n    output = output.astype(np.int16)\n```", "```py\n    # Write to the output file \n    write('output_tone_seq.wav', sampling_freq, output) \n```", "```py\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom scipy.io import wavfile  \nfrom python_speech_features import mfcc, logfbank \n```", "```py\n# Read input sound file \nsampling_freq, audio = wavfile.read(\"input_freq.wav\") \n```", "```py\n# Extract MFCC and Filter bank features \nmfcc_features = mfcc(audio, sampling_freq) \nfilterbank_features = logfbank(audio, sampling_freq) \n```", "```py\n# Print parameters \nprint('MFCC:\\nNumber of windows =', mfcc_features.shape[0])\nprint('Length of each feature =', mfcc_features.shape[1])\nprint('\\nFilter bank:\\nNumber of windows =', filterbank_features.shape[0])\nprint('Length of each feature =', filterbank_features.shape[1])\n```", "```py\n# Plot the features \nmfcc_features = mfcc_features.T \nplt.matshow(mfcc_features) \nplt.title('MFCC') \n```", "```py\nfilterbank_features = filterbank_features.T \nplt.matshow(filterbank_features) \nplt.title('Filter bank') \nplt.show() \n```", "```py\nMFCC:\nNumber of windows = 40\nLength of each feature = 13\n\nFilter bank:\nNumber of windows = 40\nLength of each feature = 26\n```", "```py\n# Class to handle all HMM related processing \nclass HMMTrainer(object): \n```", "```py\n    def __init__(self, model_name='GaussianHMM', n_components=4, cov_type='diag', n_iter=1000): \n```", "```py\n        self.model_name = model_name \n        self.n_components = n_components \n        self.cov_type = cov_type \n        self.n_iter = n_iter \n        self.models = [] \n```", "```py\n        if self.model_name == 'GaussianHMM': \n            self.model = hmm.GaussianHMM(n_components=self.n_components,  \n                    covariance_type=self.cov_type, n_iter=self.n_iter) \n        else: \n            raise TypeError('Invalid model type') \n```", "```py\n    # X is a 2D numpy array where each row is 13D \n    def train(self, X): \n        np.seterr(all='ignore') \n        self.models.append(self.model.fit(X))\n```", "```py\n    # Run the model on input data \n    def get_score(self, input_data): \n        return self.model.score(input_data) \n```", "```py\nimport os \nimport argparse  \n\nimport numpy as np \nfrom scipy.io import wavfile  \nfrom hmmlearn import hmm \nfrom python_speech_features import mfcc \n```", "```py\n# Function to parse input arguments \ndef build_arg_parser():\n    parser = argparse.ArgumentParser(description='Trains the HMM classifier')\n    parser.add_argument(\"--input-folder\", dest=\"input_folder\", required=True,\n            help=\"Input folder containing the audio files in subfolders\")\n    return parser\n```", "```py\nclass HMMTrainer(object):\n def __init__(self, model_name='GaussianHMM', n_components=4, cov_type='diag', n_iter=1000):\n self.model_name = model_name\n self.n_components = n_components\n self.cov_type = cov_type\n self.n_iter = n_iter\n self.models = []\n\nif self.model_name == 'GaussianHMM':\n self.model = hmm.GaussianHMM(n_components=self.n_components, \n covariance_type=self.cov_type, n_iter=self.n_iter)\n else:\n raise TypeError('Invalid model type')\n\n# X is a 2D numpy array where each row is 13D\n def train(self, X):\n np.seterr(all='ignore')\n self.models.append(self.model.fit(X))\n\n# Run the model on input data\n def get_score(self, input_data):\n return self.model.score(input_data)\n```", "```py\nif __name__=='__main__': \n    args = build_arg_parser().parse_args() \n    input_folder = args.input_folder \n```", "```py\n    hmm_models = [] \n```", "```py\n    # Parse the input directory \n    for dirname in os.listdir(input_folder):\n```", "```py\n        # Get the name of the subfolder  \n        subfolder = os.path.join(input_folder, dirname) \n\n        if not os.path.isdir(subfolder):  \n            continue \n```", "```py\n        # Extract the label \n        label = subfolder[subfolder.rfind('/') + 1:] \n```", "```py\n        # Initialize variables \n        X = np.array([]) \n        y_words = [] \n```", "```py\n        # Iterate through the audio files (leaving 1 file for testing in each class) \n        for filename in [x for x in os.listdir(subfolder) if x.endswith('.wav')][:-1]: \n```", "```py\n            # Read the input file \n            filepath = os.path.join(subfolder, filename) \n            sampling_freq, audio = wavfile.read(filepath) \n```", "```py\n            # Extract MFCC features \n            mfcc_features = mfcc(audio, sampling_freq) \n```", "```py\n            # Append to the variable X \n            if len(X) == 0: \n                X = mfcc_features \n            else: \n                X = np.append(X, mfcc_features, axis=0)\n```", "```py\n            # Append the label \n            y_words.append(label)    \n```", "```py\n        # Train and save HMM model \n        hmm_trainer = HMMTrainer() \n        hmm_trainer.train(X) \n        hmm_models.append((hmm_trainer, label)) \n        hmm_trainer = None \n```", "```py\n    # Test files \n    input_files = [ \n            'data/pineapple/pineapple15.wav', \n            'data/orange/orange15.wav', \n            'data/apple/apple15.wav', \n            'data/kiwi/kiwi15.wav' \n            ] \n```", "```py\n    # Classify input data \n    for input_file in input_files: \n```", "```py\n        # Read input file \n        sampling_freq, audio = wavfile.read(input_file) \n```", "```py\n        # Extract MFCC features \n        mfcc_features = mfcc(audio, sampling_freq) \n```", "```py\n        # Define variables \n        max_score = float('-inf')\n        output_label = None\n```", "```py\n        # Iterate through all HMM models and pick  \n        # the one with the highest score \n        for item in hmm_models: \n            hmm_model, label = item \n```", "```py\n            score = hmm_model.get_score(mfcc_features) \n            if score > max_score: \n                max_score = score \n                output_label = label \n```", "```py\n         # Print the output\n        print(\"True:\", input_file[input_file.find('/')+1:input_file.rfind('/')])\n        print(\"Predicted:\", output_label)\n```", "```py\n$ python speech_recognizer.py --input-folder data\n```", "```py\nTrue: pineapple\nPredicted: data\\pineapple\nTrue: orange\nPredicted: data\\orange\nTrue: apple\nPredicted: data\\apple\nTrue: kiwi\nPredicted: data\\kiwi\n```", "```py\n$ pip install pyttsx3\n```", "```py\n$ pip install pypiwin32\n```", "```py\nimport pyttsx3;\n```", "```py\nengine = pyttsx3.init();\n```", "```py\nrate = engine.getProperty('rate')\nengine.setProperty('rate', rate-50)\n```", "```py\nvoices = engine.getProperty('voices')\nengine.setProperty('voice', 'TTS_MS_EN-US_ZIRA_11.0')\n```", "```py\nengine.say(\"You are reading the Python Machine Learning Cookbook\");\nengine.say(\"I hope you like it.\");\n```", "```py\nengine.runAndWait();\n```"]