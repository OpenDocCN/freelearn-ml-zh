- en: Introducing Machine Learning Predictive Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍机器学习预测模型
- en: A large percentage of data mining opportunities involve machine learning, and
    these opportunities often come with greater financial rewards. This chapter will
    give you the basic knowledge that you need to bring the power of machine learning
    into your data mining work. In this chapter, we're going to talk about the characteristics
    of machine learning models and also see some examples of these models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分数据挖掘机会都涉及机器学习，这些机会通常伴随着更大的经济回报。本章将为你提供将机器学习的力量带入数据挖掘工作的基本知识。在本章中，我们将讨论机器学习模型的特点，并查看一些这些模型的示例。
- en: 'The following are the topics that we will be covering in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在本章中涉及的主题：
- en: Characteristics of machine learning predictive models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习预测模型的特点
- en: Types of machine learning predictive models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习预测模型的类型
- en: Working with neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与神经网络一起工作
- en: A sample neural network model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个示例神经网络模型
- en: Characteristics of machine learning predictive models
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习预测模型的特点
- en: Knowing the characteristics of machine learning predictive models will help
    you understand the advantages and limitations in comparison to any statistical
    or decision tree models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 了解机器学习预测模型的特点将有助于你理解它们与任何统计模型或决策树模型相比的优势和局限性。
- en: 'Let''s get some insights on a few characteristics of predictive models in machine
    learning:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下机器学习预测模型的一些特点：
- en: '**Optimized to learn complex patterns**: Machine learning models are designed
    to be optimized to learn complex patterns. In comparison to statistical models
    or decision tree models, predictive models greatly excel, when you have very complex
    patterns in data.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化以学习复杂模式**：机器学习模型被设计成优化以学习复杂模式。与统计模型或决策树模型相比，当数据中存在非常复杂的模式时，预测模型在这一点上表现得尤为出色。'
- en: '**Account for interactions and nonlinear relationships**: Machine learning
    predictive models can account for interactions in the data and nonlinear relationships
    to an even better degree than decision tree models.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑交互和非线性关系**：机器学习预测模型可以比决策树模型更好地考虑数据中的交互和非线性关系。'
- en: '**Few assumptions**: These models are powerful because they have very few assumptions.
    They can also be used with different types of data.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假设很少**：这些模型之所以强大，是因为它们有很少的假设。它们也可以用于不同类型的数据。'
- en: '**A black box model''s interpretation is not straightforward**: Predictive
    models are black box models, this is one of the drawbacks of predictive machine
    learning models, because this implies that the interpretation is not straightforward.
    This means that, if we end up building many different equations and combine them,
    it becomes very difficult to see exactly how each one of these variables ended
    up interacting and impacting an output variable. So, the predictive machine learning
    models are great when it comes to predictive accuracy, but they''re not that good
    for understanding the mechanics behind a prediction.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黑盒模型的解释并不简单**：预测模型是黑盒模型，这是预测机器学习模型的一个缺点，因为这意味着解释并不简单。这意味着，如果我们最终构建了许多不同的方程并将它们组合起来，就很难确切地看到每个变量是如何相互作用并影响输出变量的。因此，预测机器学习模型在预测准确性方面非常出色，但它们在理解预测背后的机制方面并不那么好。'
- en: If you want to predict something, these models do a pretty good job and have
    amazing accuracy. But if you want to know why something is being predicted, and
    if you are looking forward to making some changes in the implementation so that
    you don't get a particular prediction, then it would be difficult to decipher.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要预测某件事，这些模型做得相当不错，并且具有惊人的准确性。但如果你想知道为什么会有这样的预测，如果你希望对实施进行一些更改以避免特定的预测，那么要解析出来将会很困难。
- en: Types of machine learning predictive models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习预测模型的类型
- en: 'The following are some of the different types of machine learning predictive
    models:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些不同的机器学习预测模型类型：
- en: Neural networks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Support Vector Machines
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Random forest
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Naive Bayesian algorithms
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单贝叶斯算法
- en: Gradient boosting algorithms
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升算法
- en: K-nearest neighbors
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K最近邻
- en: Self-learning response model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自学习响应模型
- en: We won't be covering all of them, but we'll focus on a very interesting model
    – the neural network. In the following sections, we will get an in-depth view
    of what neural networks are.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '我们不会涵盖所有这些，但我们将关注一个非常有趣的模型——神经网络。在接下来的章节中，我们将深入了解神经网络是什么。 '
- en: Working with neural networks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与神经网络一起工作
- en: Neural networks were initially developed in an attempt to understand how the
    brain operates. They were originally used in the areas of neuroscience and linguistics.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最初是为了尝试理解大脑如何运作而开发的。最初它们被用于神经科学和语言学领域。
- en: In these fields, researchers noticed that something happened in the environment
    (input), the individual processed the information (in the brain), and then reacted
    in some way (output).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些领域，研究人员注意到环境中发生了某些事情（输入），个体处理了信息（在大脑中），然后以某种方式做出反应（输出）。
- en: So, the idea behind neural networks or neural nets is that they will serve as
    a brain, which is like a black box. We then have to try to figure out what is
    going on so that the findings can be applied.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络或神经网络的背后理念是它们将充当大脑，就像一个黑盒。然后我们必须尝试弄清楚发生了什么，以便可以将发现应用于实践。
- en: Advantages of neural networks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的优点
- en: 'The following are the advantages of using a neural network:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用神经网络的优点：
- en: '**Good for many types of problems**: They work well with most of the complex
    problems that you might come across.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于许多类型的问题**：它们与您可能遇到的许多复杂问题都兼容得很好。'
- en: '**They generalize very well**: Accurate generalization is a very important
    feature.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力很好**：准确的泛化是一个非常重要的特性。'
- en: '**They are very common**: Neural networks have become very common in today''s
    world, and they are readily accepted and implemented for real-world problems.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它们非常普遍**：神经网络在当今世界非常普遍，并且它们被轻松接受并用于现实世界的问题。'
- en: '**A lot is known about them**: Owing to the popularity that neural networks
    have gained, there is a lot of research being done and implemented successfully
    in different areas, so there is a lot of information available on neural networks.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关于它们了解很多**：由于神经网络获得了广泛的流行，因此在不同的领域进行了大量研究并成功实施，因此关于神经网络的信息很多。'
- en: '**Works well with non-clustered data**: When you have non-clustered data, neural
    networks can be used in several situations, such as where the data itself is very
    complex, where you have many interactions, or where you have nonlinear relationships;
    neural networks are certainly very powerful and very robust solutions for such
    situations.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与非聚类数据兼容性良好**：当您拥有非聚类数据时，神经网络可以在几种情况下使用，例如数据本身非常复杂，您有很多交互，或者您有非线性关系；在这种情况下，神经网络无疑是强大且非常稳健的解决方案。'
- en: Disadvantages of neural networks
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的缺点
- en: 'Good models come at the cost of a few disadvantages:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 优秀的模型是以一些缺点为代价的：
- en: '**They take time to train**: Neural networks do take a long time to train;
    they are generally slower than a linear regression model or a decision tree model,
    as these basically just do one pass on the data, while, with neural networks,
    you actually go through many, many iterations.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练需要时间**：神经网络确实需要很长时间来训练；它们通常比线性回归模型或决策树模型慢，因为这些基本上只是对数据进行一次遍历，而神经网络实际上要经过许多次迭代。'
- en: '**The best solution is not guaranteed**: You''re not guaranteed to find the
    best solution. This also means that, in addition to running a single neural network
    through many iterations, you''ll also need to run it multiple times using different
    starting points so that you can try to get closer to the best solution.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳方案不保证**：您不能保证找到最佳方案。这也意味着，除了运行单个神经网络多次迭代外，您还需要使用不同的起始点多次运行它，以便尝试接近最佳方案。'
- en: '**Black boxes**: As we discussed earlier, it is hard to decipher what gave
    a certain output and how.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黑盒**：正如我们之前讨论的，很难弄清楚是什么导致了特定的输出以及如何做到这一点。'
- en: Representing the errors
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示错误
- en: While building our neural network, our actual goal is to build the best possible
    solution, and not to get stuck with a sub-optimal one. We'll need to run a neural
    network multiple times.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们的神经网络时，我们的实际目标是构建最佳可能的解决方案，而不是陷入次优方案。我们需要多次运行神经网络。
- en: 'Consider this error graph as an example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个错误图为例：
- en: '![](img/d7429624-77cb-41e4-b447-0598c6c169c4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7429624-77cb-41e4-b447-0598c6c169c4.png)'
- en: This is a graph depicting the amount of errors in different solutions. The **Global
    Solution** is the best possible solution and is really optimal. A **Sub-Optimal
    Solution** is a solution that terminates, gets stuck, and no longer improves,
    but it isn't really the best solution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个表示不同解决方案中错误数量的图表。**全局解**是最佳可能的解决方案，并且确实是最佳的。**次优解**是一个终止、陷入困境且不再改进的解决方案，但它并不是真正的最佳解决方案。
- en: Types of neural network models
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型的类型
- en: There are different types of neural networks available for us; in this section,
    we will gain insights into these.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种类型的神经网络可供我们使用；在本节中，我们将深入了解这些。
- en: Multi-layer perceptron
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'The most common type is called the **multi-layer perceptron model**. This neural
    network model consists of neurons represented by circles, as shown in the following
    diagram. These neurons are organized into layers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的类型被称为**多层感知器模型**。这个神经网络模型由圆圈表示的神经元组成，如下面的图所示。这些神经元被组织成层：
- en: '![](img/5649117b-17a8-4dbe-ad61-8918f2eb2a4f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5649117b-17a8-4dbe-ad61-8918f2eb2a4f.png)'
- en: 'Every multi-layer perceptron model will have at least three layers:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个多层感知器模型至少有三个层：
- en: '**Input Layer**: This layer consists of all the predictors in our data.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这一层包含我们数据中的所有预测因子。'
- en: '**Output Layer**: This will consist of the outcome variable, which is also
    known as the **dependent variable** or **target variable**.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这将包括结果变量，也称为**因变量**或**目标变量**。'
- en: '**Hidden Layer**: This layer is where you maximize the power of a neural network.
    Non-linear relationships can also be created in this layer, and all the complex
    interactions are carried out here. You can have many such hidden layers.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：这是你最大化神经网络功能的地方。在这个层也可以创建非线性关系，并且所有复杂的交互都在这里进行。你可以有多个这样的隐藏层。'
- en: You will also notice in the preceding diagram that every neuron in a layer is
    connected to every neuron in the next layer. This forms connections, and every
    connecting line will have a weight associated with it. These weights will form
    different equations in the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你也会在先前的图中注意到，同一层的每个神经元都与下一层的每个神经元相连。这形成了连接，并且每条连接线都会有一个与之相关的权重。这些权重将在模型中形成不同的方程。
- en: Why are weights important?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么权重很重要？
- en: Weights are important for several reasons. First because all neurons in one
    layer are connected to every neuron in the next layer, this means that the layers
    are connected. It also means that a neural network model, unlike many other models,
    doesn't drop any predictors. So for example, you may start off with 20 predictors,
    and these 20 predictors will be kept. A second reason why weights are important
    is that they provide information on the impact or importance of each predictor
    to the prediction. As will be shown later, these weights start off randomly, however
    through multiple iterations, the weights are modified so as to provide meaningful
    information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 权重之所以重要，有几个原因。首先，因为同一层的所有神经元都与下一层的每个神经元相连，这意味着层之间是相连的。这也意味着与许多其他模型不同，神经网络模型不会丢弃任何预测因子。例如，你可能开始时有20个预测因子，这20个预测因子将被保留。权重重要的第二个原因是它们提供了关于每个预测因子对预测的影响或重要性的信息。正如稍后所示，这些权重最初是随机的，然而通过多次迭代，权重被修改以提供有意义的信息。
- en: An example representation of a multilayer perceptron model
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器模型的示例表示
- en: Here, we will look at an example of a multilayer perceptron model. We will try
    to predict a potential buyer of a particular item based on an individual's **age**,
    **income**, and **gender**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将查看一个多层感知器模型的示例。我们将尝试根据个人的**年龄**、**收入**和**性别**预测一个特定商品的潜在买家。
- en: 'Consider the following, for example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下内容：
- en: '![](img/a77c97ce-86c8-48e4-9066-a2a68f23152c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a77c97ce-86c8-48e4-9066-a2a68f23152c.png)'
- en: As you can see, our input predictors that form the **Input Layer** are **age**, **income**,
    and **gender**. The outcome variable that forms our **Output Layer** is **Buy**,
    which will determine whether someone bought a product or not. There is a hidden
    layer where the input predictors end up combining.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的输入预测因子，即**输入层**，是**年龄**、**收入**和**性别**。形成我们**输出层**的因变量是**购买**，这将决定某人是否购买了产品。有一个隐藏层，输入预测因子在这里结合。
- en: To better understand what goes on behind the scenes of a neural network model,
    lets take a look at a linear regression model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解神经网络模型背后的情况，让我们看看线性回归模型。
- en: The linear regression model
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归模型
- en: Let's understand the linear regression model with the help of an example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来理解线性回归模型。
- en: 'Consider the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下：
- en: '![](img/64f2c33d-bbce-4d95-9ddd-45909ead45e5.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/64f2c33d-bbce-4d95-9ddd-45909ead45e5.png)'
- en: In linear regression, every input predictor in the **Input Layer** is connected
    to the outcome field by a single connection weight, also known as the **coefficient**,
    and these coefficients are estimated by a single pass through the data. The number
    of coefficients will be equal to the number of predictors. This means that every
    predictor will have a coefficient associated with it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，**输入层**中的每个输入预测器通过单个连接权重与结果字段相连，也称为**系数**，这些系数通过数据的一次遍历来估计。系数的数量将与预测器的数量相等。这意味着每个预测器都将有一个与之相关的系数。
- en: Every input predictor is directly connected to the **Target** with a particular
    coefficient as its weight. So, we can easily see the impact of a one unit change
    in the input predictor on the outcome variable or the **Target**. These kind of
    connections make it easy to determine the effect of each predictor on the Target
    variable as well as on the equation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入预测器都直接与**目标**连接，并以其特定的系数作为权重。因此，我们可以很容易地看到输入预测器中一个单位的变化对结果变量或**目标**的影响。这种类型的连接使得确定每个预测器对目标变量以及方程的影响变得容易。
- en: A sample neural network model
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个示例神经网络模型
- en: 'Let''s use an example to understand neural networks in more detail:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来更详细地了解神经网络：
- en: '![](img/c4bba314-9ff5-4c34-bda5-d22ceb1a7350.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c4bba314-9ff5-4c34-bda5-d22ceb1a7350.png)'
- en: Notice that every neuron in the **Input Layer **is connected to every neuron
    in the **Hidden Layer**, for example, **Input 1 **is connected to the first, second,
    and even the third neuron in the **Hidden Layer**. This implies that there will
    be three different weights, and these weights will be a part of three different
    equations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，**输入层**中的每个神经元都与**隐藏层**中的每个神经元相连，例如，**输入1**与**隐藏层**中的第一个、第二个甚至第三个神经元相连。这意味着将有三个不同的权重，这些权重将是三个不同方程的一部分。
- en: 'This is what happens in this example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在这个例子中发生的事情：
- en: The **Hidden Layer** intervenes between the **Input Layer** and the **Output
    Layer**.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**位于**输入层**和**输出层**之间。'
- en: The Hidden Layer allows for more complex models with nonlinear relationships.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**允许有更复杂的模型，具有非线性关系。'
- en: There are many equations, so the influence of a single predictor on the outcome
    variable occurs through a variety of paths.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多方程，所以单个预测器对结果变量的影响是通过多种路径发生的。
- en: The interpretation of weights won't be straightforward.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重的解释不会是直接的。
- en: Weights correspond to the variable importance; they will initially be random,
    and then they will go through a bunch of different iterations and will be changed
    based on the feedback of the iterations. They will then have their real meaning
    of being associated with variable importance.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重对应于变量重要性；它们最初将是随机的，然后将通过一系列不同的迭代并基于迭代的反馈而改变。然后它们将具有与变量重要性相关的真实含义。
- en: So, let's go ahead and see how these weights are determined and how we can form
    a functional neural network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们继续看看这些权重是如何确定的，以及我们如何形成一个功能性的神经网络。
- en: Feed-forward backpropagation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播反向传播
- en: '**Feed-forward backpropagation** is a method through which we can predict things
    such as weights, and ultimately the outcome of a neural network.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传播反向传播**是一种我们可以预测诸如权重和神经网络最终结果的方法。'
- en: 'According to this method, the following iterations occur on predictions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种方法，预测上发生以下迭代：
- en: If a prediction is correct, the weight associated with it is strengthened. Imagine
    the neural network saying, *H**ey, you know what, we used the weight of 0.75 for
    the first part of this equation for the first predictor and we got the correct
    prediction; that's probably a good starting point*.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果预测是正确的，与之相关的权重将得到加强。想象一下神经网络说：“嘿，你知道什么，我们为第一个预测器的这个方程的第一部分使用了0.75的权重，并且得到了正确的预测；这可能是一个好的起点。”
- en: 'Suppose the prediction is incorrect; the error is fed back or back propagated
    into the model so that the weights or weight coefficients are modified, as shown
    here:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设预测是不正确的；错误被反馈或反向传播到模型中，以便修改权重或权重系数，如图所示：
- en: '![](img/90639680-c388-4e0a-abbd-2c2a9a9e2c1e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/90639680-c388-4e0a-abbd-2c2a9a9e2c1e.png)'
- en: 'This backpropagation won''t just take place in-between the **Hidden Layers**
    and the **Target** layer, but will also take place toward the **Input Layer**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种反向传播不仅会在**隐藏层**和**目标层**之间发生，还会向**输入层**发生：
- en: '![](img/8b3fba01-e62b-46f8-ac89-f1eb0ea1c29a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8b3fba01-e62b-46f8-ac89-f1eb0ea1c29a.png)'
- en: While these iterations are happening, we are actually making our neural network
    better and better with every error propagation. The connections now make a neural
    network capable of learning different patterns in the data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些迭代正在进行时，我们实际上正在通过每次错误传播使我们的神经网络越来越好。现在的连接使神经网络能够学习数据中的不同模式。
- en: So, unlike any linear regression or a decision tree model, a neural network
    tries to learn patterns in the data. If it's given enough time to learn those
    patterns, the neural network, combined with its experience, understands and predicts
    better, improving the rate of accuracy to a great extent.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，与任何线性回归或决策树模型不同，神经网络试图学习数据中的模式。如果给它足够的时间来学习这些模式，神经网络结合其经验，将更好地理解和预测，极大地提高准确率。
- en: Model training ethics
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练伦理
- en: When you are training the neural network model, never train the model with the
    whole dataset. We need to hold back some data for testing purposes. This will allow
    us to test whether the neural network is able to apply what its learned from the
    training dataset to a new data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在训练神经网络模型时，永远不要用整个数据集来训练模型。我们需要保留一些数据用于测试目的。这将使我们能够测试神经网络是否能够将训练数据集中学到的知识应用到新数据上。
- en: We want the neural network to generalize well to new data and capture the generalities
    of different types of data, not just little nuances that would then make it sample-specific.
    Instead, we want the results to be translated to the new data as well. After the
    model has been trained, the new data can be predicted using the model's experience.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望神经网络能够很好地泛化到新数据，并捕捉不同类型数据的共性，而不仅仅是那些会使它样本特定的细微差别。相反，我们希望结果能够被转换到新数据上。在模型训练完成后，可以使用模型的经验来预测新数据。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: I hope you are now clear on machine learning predictive models and have understood
    the basic concepts. In this chapter, we have seen the characteristics of machine
    learning predictive models and have learned about some of the different types.
    These concepts are stepping stones to further chapters. We have also looked at
    an example of a basic neural network model. In the next chapter, we will implement
    a live neural network on a dataset and you will also be introduced to support
    vector machines and their implementation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你现在对机器学习预测模型已经清楚，并且理解了基本概念。在本章中，我们了解了机器学习预测模型的特点，并了解了一些不同类型。这些概念是进一步章节的垫脚石。我们还查看了一个基本神经网络模型的示例。在下一章中，我们将在数据集上实现一个实时神经网络，你还将被介绍到支持向量机及其实现。
