- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter was all about starting the conversation around how we get
    our solutions out into the world using different deployment patterns, as well
    as some of the tools we can use to do this. This chapter will aim to build on
    that conversation by discussing the concepts and tools we can use to scale up
    our solutions to cope with large volumes of data or traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Running some simple **machine learning** (**ML**) models on a few thousand data
    points on your laptop is a good exercise, especially when you’re performing the
    discovery and proof-of-concept steps we outlined previously at the beginning of
    any ML development project. This approach, however, is not appropriate if we have
    to run millions upon millions of data points at a relatively high frequency, or
    if we have to train thousands of models of a similar scale at any one time. This
    requires a different approach, mindset, and toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following pages, we will cover some details of two of the most popular
    frameworks for distributing data computations in use today: **Apache Spark** and
    **Ray**. In particular, we will discuss some of the key points about how these
    frameworks tick under the hood so that, in development, we can make some good
    decisions about how to use them. We will then move onto a discussion of how to
    use these in your ML workflows with some concrete examples, these examples being
    specifically aimed to help you when it comes to processing large batches of data.
    Next, a brief introduction to creating serverless applications that allow you
    to scale out inference endpoints will be provided. Finally, we will cover an introduction
    to scaling containerized ML applications with Kubernetes, which complements the
    work we did in *Chapter 5*, *Deployment Patterns and Tools*, and will be built
    upon in detail with a full end-to-end example in *Chapter 8*, *Building an Example
    ML Microservice*.'
  prefs: []
  type: TYPE_NORMAL
- en: This will help you build on some of the practical examples we already looked
    at earlier in this book, when we used Spark to solve our ML problems, with some
    more concrete theoretical understanding and further detailed practical examples.
    After this chapter, you should feel confident in how to use some of the best frameworks
    and techniques available to scale your ML solutions to larger and larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover all of this in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinning up serverless infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerizing at scale with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling with Ray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing systems at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with the other chapters, you can set up your Python development environment
    to be able to run the examples in this chapter by using the supplied Conda environment
    `yml` file or the `requirements.txt` files from the book repository, under *Chapter06:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This chapter’s examples will also require some non-Python tools to be installed
    to follow the examples end to end; please see the respective documentation for
    each tool:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS CLI v2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Postman
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark (version 3.0.0 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Apache Spark**, or just Spark, came from the work of some brilliant researchers
    at the *University of California, Berkeley* in 2012 and since then, it has revolutionized
    how we tackle problems with large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark is a cluster computing framework, which means it works on the principle
    that several computers are linked together in a way that allows computational
    tasks to be shared. This allows us to coordinate these tasks effectively. Whenever
    we discuss running Spark jobs, we always talk about *the cluster* we are running
    on.
  prefs: []
  type: TYPE_NORMAL
- en: This is the collection of computers that perform the tasks, the worker nodes,
    and the computer that hosts the organizational workload, known as the head node.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is written in Scala, a language with a strong functional flavor and that
    compiles down to **Java Virtual Machines** (**JVMs**). Since this is a book about
    ML engineering in Python, we won’t discuss too much about the underlying Scala
    components of Spark, except where they help us use it in our work. Spark has several
    popular APIs that allow programmers to develop with it in a variety of languages,
    including Python. This gives rise to the PySpark syntax we have used in several
    examples throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: So, how is this all put together?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, first of all, one of the things that makes Apache Spark so incredibly
    popular is the large array of connectors, components, and APIs it has available.
    For example, four main components interface with *Spark Core*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark SQL**, **DataFrames**, **and** **Datasets**: This component allows
    you to create very scalable programs that deal with structured data. The ability
    to write SQL-compliant queries and create data tables that leverage the underlying
    Spark engine through one of the main **structured APIs** of Spark (Python, Java,
    Scala, or R) gives very easy access to the main bulk of Spark’s functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Structured Streaming**: This component allows engineers to work with
    streaming data that’s, for example, provided by a solution such as Apache Kafka.
    The design is incredibly simple and allows developers to simply work with streaming
    data as if it is a growing Spark structured table, with the same querying and
    manipulation functionality as for a standard one. This provides a low entry barrier
    for creating scalable streaming solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GraphX**: This is a library that allows you to implement graph parallel processing
    and apply standard algorithms to graph-based data (for example, algorithms such
    as PageRank or Triangle Counting). The **GraphFrames** project from Databricks
    makes this functionality even easier to use by allowing us to work with DataFrame-based
    APIs in Spark and still analyze graph data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark ML**: Last but not least, we have the component that’s most appropriate
    for us as ML engineers: Spark’s native library for ML. This library contains the
    implementation of many algorithms and feature engineering capabilities we have
    already seen in this book. Being able to use **DataFrame** APIs in the library
    makes this extremely easy to use, while still giving us a route to creating very
    powerful code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The potential speedups you can gain for your ML training by using Spark ML on
    a Spark cluster versus running another ML library on a single thread can be tremendous.
    There are other tricks we can apply to our favorite ML implementations and then
    use Spark to scale them out; we’ll look at this later.
  prefs: []
  type: TYPE_NORMAL
- en: Spark’s architecture is based on the driver/executor architecture. The driver
    is the program that acts as the main entry point for the Spark application and
    is where the **SparkContext** object is created. **SparkContext** sends tasks
    to the executors (which run on their own JVMs) and communicates with the cluster
    manager in a manner appropriate to the given manager and what mode the solution
    is running in. One of the driver’s main tasks is to convert the code we write
    into a logical set of steps in a **Directed Acyclic Graph** (**DAG**) (the same
    concept that we used with Apache Airflow in *Chapter 5*, *Deployment Patterns
    and Tools*), and then convert that DAG into a set of tasks that needs to be executed
    across the available compute resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the pages that follow, we will assume we are running Spark with the **Hadoop
    YARN** resource manager, which is one of the most popular options and is also
    used by the **AWS****Elastic MapReduce** (**EMR**) solution by default (more on
    this later). When running with YARN in *cluster mode*, the driver program runs
    in a container on the YARN cluster, which allows a client to submit jobs or requests
    through the driver and then exit (rather than requiring the client to remain connected
    to the cluster manager, which can happen when you’re running in so-called *client
    mode*, which we will not discuss here).
  prefs: []
  type: TYPE_NORMAL
- en: The cluster manager is responsible for launching the executors across the resources
    that are available on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Spark’s architecture allows us, as ML engineers, to build solutions with the
    same API and syntax, regardless of whether we are working locally on our laptop
    or a cluster with thousands of nodes. The connection between the driver, the resource
    manager, and the executors is what allows this magic to happen.
  prefs: []
  type: TYPE_NORMAL
- en: Spark tips and tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will cover some simple but effective tips for writing
    performant solutions with Spark. We will focus on key pieces of syntax for data
    manipulation and preparation, which are always the first steps in any ML pipeline.
    Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will cover the basics of writing good Spark SQL. The entry point for
    any Spark program is the `SparkSession` object, which we need to import an instance
    of in our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is often instantiated with the `spark` variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can then run Spark SQL commands against your available data using the `spark`
    object and the `sql` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are a variety of ways to make the data you need available inside your
    Spark programs, depending on where they exist. The following example has been
    taken from some of the code we went through in *Chapter 3*, *From Model to Model
    Factory*, and shows how to pull data into a DataFrame from a `csv` file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can create a temporary view of this data using the following syntax:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can query against this data using the methods mentioned previously
    to see the records or to create new DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When writing Spark SQL, some standard practices help your code to be efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: Try not to join big tables on the left with small tables on the right as this
    is inefficient. In general, try and make datasets used for joins as lean as possible,
    so, for example, do not join using unused columns or rows as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid query syntax that will scan full datasets if they are very large; for
    example, `select max(date_time_value)`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, try and define logic that can filter the data more aggressively
    before finding min or max values and in general allow the solution to scan over
    a smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other good practices when working with Spark are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Avoid data skew**: Do what you can to understand how your data will be split
    across executors. If your data is partitioned on a date column, this may be a
    good choice if volumes of data are comparable for each day but bad if some days
    have most of your data and others very little. Repartitioning using a more appropriate
    column (or on a Spark-generated ID from the `repartition` command) will be required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid data shuffling**: This is when data is redistributed across different
    partitions. For example, we may have a dataset that is partitioned at the day
    level and then we ask Spark to sum over one column of the dataset for all of time.
    This will cause all of the daily partitions to be accessed and the result to be
    written to a new partition. For this to occur, disk writes and a network transfer
    have to occur, which can often lead to performance bottlenecks for your Spark
    job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Avoid actions in large datasets**: For example, when you run the `collect()`
    command, you will bring all of your data back onto the driver node. This can be
    very bad if it is a large dataset but may be needed to convert the result of a
    calculation into something else. Note that the `toPandas()` command, which converts
    your Spark `DataFrame` into a pandas `DataFrame`, also collects all the data in
    the driver’s memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use UDFs when they make sense**: Another excellent tool to have in your arsenal,
    as an ML engineer using Apache Spark, is the **User-Defined Function** (**UDF**).
    UDFs allow you to wrap up more complex and bespoke logic and apply it at scale
    in a variety of ways. An important aspect of this is that if you write a standard
    PySpark (or Scala) UDF, then you can apply this *inside* Spark SQL syntax, which
    allows you to efficiently reuse your code and even simplify the application of
    your ML models. The downside is that these are sometimes not the most efficient
    pieces of code, but if it helps to make your solution simpler and more maintainable,
    it may be the right choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a concrete example, let’s build a UDF that looks at the banking data we worked
    with in *Chapter 3*, *From Model to Model Factory*, to create a new column called
    ‘`month_as_int`' that converts the current string representation of the month
    into an integer for processing later. We will not concern ourselves with train/test
    splits or what this might be used for; instead, we will just highlight how to
    apply some logic to a PySpark UDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must read the data. Note that the relative path given here is consistent
    with the `spark_example_udfs.py` script, which can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we show the current data with the `data.show()` command, we will see something
    like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1 – A sample of the data from the initial DataFrame in the banking
    dataset ](img/B19525_06_01.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.1: A sample of the data from the initial DataFrame in the banking
    dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can double-check the schema of this DataFrame using the `data.printSchema()`
    command. This confirms that `month` is stored as a string currently, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can define our UDF, which will use the Python `datetime` library to
    convert the string representation of the month into an integer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we want to apply our function inside Spark SQL, then we must register the
    function as a UDF. The arguments for the `register()` function are the registered
    name of the function, the name of the Python function we have just written, and
    the return type. The return type is `StringType()` by default, but we have made
    this explicit here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, now that we have registered the function, we can apply it to our data.
    First, we will create a temporary view of the bank dataset and then run a Spark
    SQL query against it that references our UDF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running the preceding syntax with the `show()` command shows that we have successfully
    calculated the new column. The last few columns of the resulting `DataFrame` are
    shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3 – The new column has been calculated successfully by applying
    our UDF ](img/B19525_06_02.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.2: The new column has been calculated succesfully by applying our
    UDF.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Alternatively, we can create our UDF with the following syntax and apply the
    result to a Spark `DataFrame`. As mentioned before, using a UDF can sometimes
    allow you to wrap up relatively complex syntax quite simply. The syntax here is
    quite simple but I’ll show you it anyway. This gives us the same result that’s
    shown in the preceding screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, PySpark also provides a nice decorator syntax for creating our UDF,
    meaning that if you are indeed building some more complex functionality, you can
    just place this inside the Python function that is being decorated. The following
    code block also gives the same results as the preceding screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows how we can apply some simple logic in a UDF, but for us to deploy
    a model at scale using this approach, we have to put the ML logic inside the function
    and apply it in the same manner. This can become a bit tricky if we want to work
    with some of the standard tools we are used to from the data science world, such
    as Pandas and **Scikit-learn**. Luckily, there is another option we can use that
    has a few benefits. We will discuss this now.
  prefs: []
  type: TYPE_NORMAL
- en: The UDFs currently being considered have a slight issue when we are working
    in Python in that translating data between the JVM and Python can take a while.
    One way to get around this is to use what is known as **pandas UDFs**, which use
    the Apache Arrow library under the hood to ensure that the data is read quickly
    for the execution of our UDFs. This gives us the flexibility of UDFs without any
    slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: pandas UDFs are also extremely powerful because they work with the syntax of
    – you guessed it – pandas **Series** and **DataFrame** objects. This means that
    a lot of data scientists who are used to working with pandas to build models locally
    can easily adapt their code to scale up using Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s walk through how to apply a simple classifier to the wine
    dataset that we used earlier in this book. Note that the model was not optimized
    for this data; we are just showing an example of applying a pre-trained classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a simple **Support Vector Machine** (**SVM**)-based classifier
    on the wine dataset. We are not performing correct training/test splits, feature
    engineering, or other best practices here as we just want to show you how to apply
    any `sklearn` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then bring the feature data into a Spark DataFrame to show you how to
    apply the pandas UDF in later stages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'pandas UDFs are very easy to define. We just write our logic in a function
    and then add the `@pandas_udf` decorator, where we also have to provide the output
    type for the function. In the simplest case, we can just wrap the (normally serial
    or only locally parallelized) process of performing a prediction with the trained
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can apply this to the Spark `DataFrame` containing the data by
    passing in the appropriate inputs we needed for our function. In this case, we
    will pass in the column names of the features, of which there are 13:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, if you look at the results of this, you will see the following for the
    first few rows of the `df_pred` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The result of applying a simple pandas UDF ](img/B19525_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: The result of applying a simple pandas UDF.'
  prefs: []
  type: TYPE_NORMAL
- en: And that completes our whirlwind tour of UDFs and pandas UDFs in Spark, which
    allow us to take serial Python logic, such as data transformations or our ML models,
    and apply them in a manifestly parallel way.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will focus on how to set ourselves up to perform Spark-based
    computations in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Spark on the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As should be clear from the preceding discussion, writing and deploying PySpark-based
    ML solutions can be done on your laptop, but for you to see the benefits when
    working at scale, you must have an appropriately sized computing cluster to hand.
    Provisioning this sort of infrastructure can be a long and painful process but
    as discussed already in this book, a plethora of options for infrastructure are
    available from the main public cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: For Spark, AWS has a particularly nice solution called **AWS Elastic MapReduce**
    (**EMR**), which is a managed big data platform that allows you to easily configure
    clusters of a few different flavors across the big data ecosystem. In this book,
    we will focus on Spark-based solutions, so we will focus on creating and using
    clusters that have Spark tooling to hand.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will go through a concrete example of spinning up a
    Spark cluster on EMR and then deploying a simple Spark ML-based application onto
    it.
  prefs: []
  type: TYPE_NORMAL
- en: So, with that, let’s explore Spark on the cloud with **AWS EMR**!
  prefs: []
  type: TYPE_NORMAL
- en: AWS EMR example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand how EMR works, we will continue in the practical vein that the
    rest of this book will follow and dive into an example. We will begin by learning
    how to create a brand-new cluster before discussing how to write and deploy our
    first PySpark ML solution to it. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: First, navigate to the **EMR** page on AWS and find the **Create Cluster** button.
    You will then be brought to a page that allows you to input configuration data
    for your cluster. The first section is where you specify the name of the cluster
    and the applications you want to install on it. I will call this cluster `mlewp2-cluster`,
    use the latest EMR release available at the time of writing, 6.11.0, and select
    the **Spark** application bundle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All other configurations can remain as default in this first section. This is
    shown in *Figure 6.4*:![A screenshot of a computer program  Description automatically
    generated](img/B19525_06_04.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.4: Creating our EMR cluster with some default configurations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next comes the configuration of the compute used in the cluster. You can just
    use the defaults here again but it is important to understand what is going on.
    First, there is the selection of whether to use “instance groups” or “instance
    fleets,” which refers to the strategy deployed for scaling up your compute given
    some constraints you provide. Instance groups are simpler and define the specific
    servers you want to run for each node type, more on this in a second, and you
    can choose between “on-demand” or “spot instances” for acquiring more servers
    if needed during the lifetime of the cluster. Instance fleets allow for a lot
    more complex acquisition strategies and for blends of different server instance
    types for each node type. For more information, read the AWS documentation to
    make sure you get a clear view of the different options, [https://docs.aws.amazon.com/emr/index.xhtml](https://docs.aws.amazon.com/emr/index.xhtml);
    we will proceed by using an instance group with default settings. Now, onto nodes.
    There are different nodes in an EMR cluster; primary, core and task. The primary
    node will run our YARN Resource Manager and will track job status and instance
    group health. The core nodes run some daemons and the Spark executors. Finally,
    the task nodes perform the actual distributed calculations. For now, let’s proceed
    with the defaults provided for the instance groups option, as shown for the **Primary**
    nodes in *Figure 6.5*.![A screenshot of a computer  Description automatically
    generated](img/B19525_06_05.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.5: Compute configuration for our EMR cluster. We have selected the
    simpler “instance groups” option for the configuration and have gone with the
    server type defaults.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we move onto defining the explicit cluster scaling behavior that we mentioned
    is used in the instance groups and instance fleet compute options in *step 2*.
    Again, select the defaults for now, but you can play around to make the cluster
    larger here in terms of numbers of nodes or you can define auto-scaling behaviour
    that will dynamically increase the cluster size upon larger workloads. *Figure
    6.6* shows what this should look like.![A screenshot of a computer screen  Description
    automatically generated](img/B19525_06_06.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.6: The cluster provisioning and scaling strategy selection. Here we
    have gone with the defaults of a specific, small cluster size, but you can increase
    these values to have a bigger cluster or use the auto-scaling option to provide
    min and max size limits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, there is a **Networking** section, which is easier if you have already
    created some **virtual private clouds** (**VPCs**) and subnets for the other examples
    in the book; see *Chapter 5*, *Deployment Patterns and Tools* and the AWS documentation
    for more information. Just remember that VPCs are all about keeping the infrastructure
    you are provisioning isolated from other services in your own AWS account and
    even from the wider internet, so it’s definitely good to become familiar with
    them and their application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For completeness, *Figure 6.7* shows the setup I used for this example.![A screenshot
    of a computer  Description automatically generated](img/B19525_06_07.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.7: The networking configuration requires the use of a VPC; it will
    automatically create a subnet for the cluster if there is not one selected.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We only have a couple more sections we need to input into to define our cluster.
    The next mandatory section is around cluster termination policies. I would always
    recommend having an automated teardown policy for infrastructure where possible
    as this helps to manage costs. There have been many stories across the industry
    of teams leaving un-used servers running and racking up huge bills! *Figure 6.8*
    shows that we are using such an automated cluster termination policy where the
    cluster will terminate after 1 hour of not being utilized.![A screenshot of a
    computer  Description automatically generated](img/B19525_06_08.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.8: Defining a cluster termination policy like this one is considered
    best practice and can help avoid unnecessary costs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final required section for completion is the definition of the appropriate
    **Identity and Access Management** (**IAM**) roles, which defines what accounts
    can access the resources we are creating. If you already have IAM roles that you
    are happy to reuse as your EMR service role, then you can do this; for this example,
    however, let’s create a new service role specifically for this cluster. *Figure
    6.9* shows that selecting the option to create a new role pre-populates the VPC,
    subnet, and security group with values matching what you have already selected
    through this process. You can add more to these. *Figure 6.10* shows that we can
    also select to create an “instance profile”, which is just the name given to a
    service role that applies to all server instances in an EC2 cluster at launch.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_09.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.9: Creating an AWS EMR service role.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_10.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6.10: Creating an instance profile for the EC2 servers being used in
    this EMR cluster. An instance profile is just the name for the service role that
    is assigned to all cluster EC2 instances at spin-up time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The sections discussed are all the mandatory ones required to create your cluster,
    but there are also some optional sections that I would like to briefly mention
    to guide your further exploration. There is the option to specify **steps**, which
    is where you can define shell scripts, JAR applications, or Spark applications
    to run in sequence. This means you can spin up your cluster with your applications
    ready to start processing data in the sequence you desire, rather than submitting
    jobs after the infrastructure deployment. There is a section on **Bootstrap actions**,
    which allows you to define custom installation or configuration steps that should
    run before any applications are installed or any data is processed on the EMR
    cluster. Cluster logs locations, tags, and some basic software considerations
    are also available for configuration. The final important point to mention is
    on the security configuration. *Figure 6.11* shows the options. Although we will
    deploy this cluster without specifying any EC2 key pair or security configuration,
    it is crucially important that you understand the security requirements and norms
    of your organization if you want to run this cluster in production. Please consult
    your security or networking teams to ensure this is all in line with expectations
    and requirements. For now, we can leave it blank and proceed to create the cluster.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_11.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.11: The security configuration for the cluster shown here is optional
    but should be considered carefully if you aim to run the cluster in production.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have selected all of the mandatory options, click the **Create cluster**
    button to launch. Upon successful creation, you should see a review page like
    that shown in *Figure 6.12*. And that’s it; you have now created your own Spark
    cluster in the cloud!![A screenshot of a computer  Description automatically generated](img/B19525_06_12.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.12: EMR cluster creation review page shown upon successful launch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After spinning up our EMR cluster, we want to be able to submit work to it.
    Here, we will adapt the example Spark ML pipeline we produced in *Chapter 3*,
    *From Model to Model Factory*, to analyze the banking dataset and submit this
    as a step to our newly created cluster. We will do this as a standalone single
    PySpark script that acts as the only step in our application, but it is easy to
    build on this to make far more complex applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will take the code from *Chapter 3*, *From Model to Model Factory*,
    and perform some nice refactoring based on our discussions around good practices.
    We can more effectively modularize the code so that it contains a function that
    provides all our modeling steps (not all of the steps have been reproduced here,
    for brevity). We have also included a final step that writes the results of the
    modeling to a `parquet` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Building on this, we will wrap all of the main boilerplate code into a `main`
    function that can be called at the `if __name__=="__main__":` entry point to the
    program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We put the preceding functions into a script called `spark_example_emr.py`,
    which we will submit to our EMR cluster later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to submit this script to the EMR cluster we have just created, we need
    to find out the cluster ID, which we can get from the AWS UI or by running the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we need to send the `spark_example_emr.py` script to S3 to be read in
    by the cluster. We can create an S3 bucket called `s3://mlewp-ch6-emr-examples`
    to store this and our other artifacts using either the CLI or the AWS console
    (see *Chapter 5*, *Deployment Patterns and Tools*). Once copied over, we are ready
    for the final step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we must submit the script using the following command, with `<CLUSTER_ID>`
    replaced with the ID of the cluster we just created. Note that if your cluster
    has been terminated due to the automated termination policy we set, you can’t
    restart it but you can clone it. After a few minutes, the step should have completed
    and the outputs should have been written to the `results.parquet` file in the
    same S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And that is it – that is how we can start developing PySpark ML pipelines on
    the cloud using **AWS EMR**!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see that this previous process has worked successfully by navigating
    to the appropriate S3 bucket and confirming that the `results.parquet` file was
    created succesfully; see *Figure 6.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Successful creation of the results.parquet file upon submission
    of the EMR script.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore another method of scaling up our solutions
    by using so-called serverless tools.
  prefs: []
  type: TYPE_NORMAL
- en: Spinning up serverless infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we do any ML or software engineering, we have to run the requisite
    tasks and computations on computers, often with appropriate networking, security,
    and other protocols and software already in place, which we have often referred
    to already as constituting our *infrastructure*. A big part of our infrastructure
    is the servers we use to run the actual computations. This might seem a bit strange,
    so let’s start by talking about *serverless* infrastructure (how can there be
    such a thing?). This section will explain this concept and show you how to use
    it to scale out your ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless** is a bit misleading as a term as it does not mean that no physical
    servers are running your programs. It does mean, however, that the programs you
    are running should not be thought of as being statically hosted on one machine,
    but as ephemeral instances on another layer on top of the underlying hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of serverless tools for your ML solution include (but are not
    limited to) the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No servers**: Don’t underestimate the savings in time and energy you can
    get by offloading infrastructure management to your cloud provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified scaling**: It’s usually very easy to define the scaling behavior
    of your serverless components by using clearly defined maximum instances, for
    example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low barrier to entry**: These components are usually extremely easy to set
    up and run, allowing you and your team members to focus on writing high-quality
    code, logic, and models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural integration points**: Serverless tools are often nice to use for
    handovers between other tools and components. Their ease of setup means you can
    be up and running with simple jobs that pass data or trigger other services in
    no time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified serving**: Some serverless tools are excellent for providing a
    serving layer to your ML models. The scaling and low barrier to entry mentioned
    previously mean you can quickly create a very scalable service that provides predictions
    upon request or upon being triggered by some other event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the best and most widely used examples of serverless functionality is
    **AWS Lambda**, which allows us to write programs in a variety of languages with
    a simple web browser interface or through our usual development tools, and then
    have them run completely independently of any infrastructure that’s been set up.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda is an amazing low-entry-barrier solution to getting some code up and
    running and scaling it up. However, it is very much aimed at creating simple APIs
    that can be hit over an HTTP request. Deploying your ML model with Lambda is particularly
    useful if you are aiming for an event- or request-driven system.
  prefs: []
  type: TYPE_NORMAL
- en: To see this in action, let’s build a basic system that takes incoming image
    data as an HTTP request with a JSON body and returns a similar message containing
    the classification of the data using a pre-built Scikit-Learn model. This walkthrough
    is based on the AWS example at [https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/](https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/).
  prefs: []
  type: TYPE_NORMAL
- en: For this, we can save a lot of time by leveraging templates already built and
    maintained as part of the AWS **Serverless Application Model** (**SAM**) framework
    ([https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/)).
  prefs: []
  type: TYPE_NORMAL
- en: To install the AWS SAM CLI on your relevant platform, follow the instructions
    at [https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s perform the following steps to set up a template serverless deployment
    for hosting and serving a ML model that classifies images of handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must run the `sam init` command and select the AWS `Quick Start Templates`
    option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will then be offered a choice of `AWS Quick Start` Application templates
    to use; select option 15, `Machine Learning`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next are the options for the Python runtime you want to use; in line with the
    rest of the book, we will use the Python 3.10 runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the time of writing, the SAM CLI will then auto-select some options based
    on these choices, first the package type and then the dependency manager. You
    will then be asked to confirm the ML starter template you want to use. For this
    example, select `XGBoost Machine Learning API`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The SAM CLI then helpfully asks about some options for configuring request
    tracing and monitoring; you can select yes or no depending on your own preferences.
    I have selected no for the purposes of this example. You can then give the solution
    a name; here I have gone with `mlewp-sam-ml-api`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, your command line will provide some helpful information about the
    installation and next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the preceding steps have created a template for an XGBoost-based
    system that classifies handwritten digits. For other applications and project
    use cases, you will need to adapt the source code of the template as you require.
    If you want to deploy this example, follow the next few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must build the application container provided with the template.
    First, navigate to the top directory of your project, you can see the directory
    structure should be something like below. I have used the `tree` command to provide
    a clean outline of the directory structure in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we are in the top directory, we can run the `build` command. This
    requires that Docker is running in the background on your machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon a successful build, you should receive a success message similar to the
    following in your terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can test the service locally to ensure that everything is working well
    with the mock data that’s supplied with the repository. This uses a JSON file
    that encodes a basic image and runs the inference step for the service. If this
    has worked, you will see an output that looks something like the following for
    your service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In a real project, you would edit the source code for the solution in the `app.py`
    and other files as required before deploying up to the cloud. We will do this
    using the SAM CLI , with the understanding that if you want to automate this process,
    you can use the CI/CD processes and tools we discussed in several places in this
    book, especially in *Chapter 4*, *Packaging Up*. To deploy, you can use the guided
    deployment wizard with the CLI by running the `deploy` command, which will return
    the below output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then have to configure the application for each of the provided elements.
    I have selected the defaults in most cases, but you can refer to the AWS documentation
    and make the choices most relevant to your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous step will generate a lot of data in the terminal; you can monitor
    this to see if there are any errors or issues. If the deployment was successful,
    then you should see some final metadata about the application that looks like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As a quick test to confirm the cloud-hosted solution is working, we can use
    a tool such as Postman to hit our shiny new ML API. Simply copy the `InferenceApi`
    URL from the output screen from *step 8* as the destination for the request, select
    **POST** for the request type, and then choose **binary** as the body type. Note
    that if you need to get the inference URL, again you can run the `sam list endpoints
    --output json` command in your terminal. Then, you can choose an image of a handwritten
    digit, or any other image for that matter, to send up to the API . You can do
    this in Postman either by selecting the **binary** body option and attaching an
    image file or you can copy in the encoded string of an image. In *Figure 6.14*,
    I have used the encoded string for the `body` key-value pair in the `events/event.json`
    file we used to test the function locally:![A screenshot of a computer  Description
    automatically generated](img/B19525_06_14.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.14: Calling our serverless ML endpoint with Postman. This uses an
    encoded example image as the body of the request that is provided with the SAM
    XGBoost ML API template.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also test this more programmatically with a `curl` command like the
    following – just replace the encoded binary string of the image with the appropriate
    values, or indeed edit the command to point to a data binary if you wish, and
    you are good to go:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step and *step 9*, the body of the response from the Lambda function
    is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: And that’s it – we have just built and deployed a simple serverless ML inference
    service on AWS!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we will touch upon the final way of scaling our solutions
    that we will discuss in this chapter, which is using Kubernetes (K8s) and Kubeflow
    to horizontally scale containerized applications.
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing at scale with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered how to use containers for building and deploying our
    ML solutions. The next step is understanding how to orchestrate and manage several
    containers to deploy and run applications at scale. This is where the open source
    tool **Kubernetes** (**K8s**)comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s is an extremely powerful tool that provides a variety of different functionalities
    that help us create and manage very scalable containerized applications, including
    (but not limited to) the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Load Balancing**: K8s will manage routing incoming traffic to your containers
    for you so that the load is split evenly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal Scaling**: K8s provides simple interfaces so that you can control
    the number of container instances you have at any one time, allowing you to scale
    massively if needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self Healing**: There is built-in management for replacing or rescheduling
    components that are not passing health checks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Rollbacks**: K8s stores the history of your system so that you
    can revert to a previous working version if something goes wrong.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these features help ensure that your deployed solutions are robust and
    able to perform as required under all circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: K8s is designed to ensure the preceding features are embedded from the ground
    up by using a microservice architecture, with a control plane interacting with
    nodes (servers), each of which host pods (one or more containers) that run the
    components of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key thing that K8s gives you is the ability to scale your application based
    on load by creating replicas of the base solution. This is extremely useful if
    you are building services with API endpoints that could feasibly face surges in
    demand at different times. To learn about some of the ways you can do this, see
    [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – The K8s architecture ](img/B19525_06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: The K8s architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But what about ML? In this case, we can look to a newer piece of the K8s ecosystem:
    **Kubeflow**, which we learned how to use in *Chapter 5*, *Deployment Patterns
    and Tools*.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow styles itself as the *ML toolkit for K8s* ([https://www.kubeflow.org/](https://www.kubeflow.org/)),
    so as ML engineers, it makes sense for us to be aware of this rapidly developing
    solution. This is a very exciting tool and an active area of development.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of horizontal scaling for K8s generally still applies here, but
    Kubeflow provides some standardized tools for converting the pipelines you build
    into standard K8s resources, which can then be managed and resourced in the ways
    described previously. This can help reduce *boilerplate* and lets us, as ML engineers,
    focus on building our modelling logic rather than setting up the infrastructure.
    We leveraged this when we built some example pipelines in *Chapter 5*.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore Kubernetes in far more detail in *Chapter 8*, *Building an Example
    ML Microservice*, where we use it to scale out our own wrapped ML model in a REST
    API. This will complement nicely the work we have done in this chapter on higher-level
    abstractions that can be used for scaling out, especially in the *Spinning up
    serverless infrastructure* section. We will only touch on K8s and Kubeflow very
    briefly here, to make sure you are aware of these tools for your exploration.
    For more details on K8s and Kubeflow, consult the documentation. I would also
    recommend another Packt title called *Kubernetes in Production Best Practices*
    by *Aly Saleh* and *Murat Karslioglu*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on and discuss another very powerful toolkit for scaling out
    compute-intensive Python workloads, which has now become extremely popular across
    the ML engineering community and been used by organizations such as Uber, Amazon
    and even used by OpenAI for training their large language **Generative Pre-trained
    Transformer** (**GPT**) models, which we discuss at length in *Chapter 7*, *Deep
    Learning, Generative AI, and LLMOps*. Let’s meet **Ray**.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Ray
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ray** is a Python native distributed computing framework that was specifically
    designed to help ML engineers meet the needs of massive data and massively scalable
    ML systems. Ray has an ethos of making scalable compute available to every ML
    developer, and in doing this in a way such that you can run anywhere by abstracting
    out all interactions with underlying infrastructure. One of the unique features
    of Ray that is particularly interesting is that it has a distributed scheduler,
    rather than a scheduler or DAG creation mechanism that runs in a central process,
    like in Spark. At its core, Ray has been developed with compute-intensive tasks
    such as ML model training in mind from the beginning, which is slightly different
    from Apache Spark, which has data intensity in mind. You can therefore think about
    this in a simplified manner: if you need to process lots of data a couple of times,
    Spark; if you need to process one piece of data lots of times, Ray. This is just
    a heuristic so should not be followed strictly, but hopefully it gives you a helpful
    rule of thumb. As an example, if you need to transform millions and millions of
    rows of data in a large batch process, then it makes sense to use Spark, but if
    you want to train an ML model on the same data, including hyperparameter tuning,
    then Ray may make a lot of sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two tools can be used together quite effectively, with Spark transforming
    the feature set before feeding this into a Ray workload for ML training. This
    is taken care of in particular by the **Ray AI Runtime** (**AIR**), which has
    a series of different libraries to help scale different pieces of an ML solution.
    These include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ray Data**: Focused on providing data pre-processing and transformation primitives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Train**: Facilitates large model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Tune**: Helps with scalable hyperparameter training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray RLib**: Supports methods for the development of reinforcement learning
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Batch Predictor**: For batch inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Serving**: For re al-time inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AIR framework provides a unified API through which to interact with all
    of these capabilities and nicely integrates with a huge amount of the standard
    ML ecosystem that you will be used to, and that we have leveraged in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: The Ray AI runtime, from a presentation by Jules Damji from Anyscale:
    https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: The Ray architecture including the Raylet scheduler. From a presentation
    by Jules Damji: https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ray Core API has a series of different objects that you leverage when using
    Ray in order to distribute your solution. The first is tasks, which are asynchronous
    items of work for the system to perform. To define a task, you can take a Python
    function like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And then add the `@remote` decorator and then use the `.remote()` syntax in
    order to submit this task to the cluster. This is not a blocking function so will
    just return an ID that Ray uses to refer to the task in later computation steps
    ([https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832](https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832))
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'In the same vein, the Ray API can extend the same concepts to classes as well;
    in this case, these are called `Actors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, Ray also has a distributed immutable object store. This is a smart
    way to have one shared data store across all the nodes of the cluster without
    shifting lots of data around and using up bandwidth. You can write to the object
    store with the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: An Actor in this context is a service or stateful worker, a concept used in
    other distributed frameworks like Akka, which runs on the JVM and has bindings
    to Java and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Ray for ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started you can install Ray with AI Runtime, as well as some the hyperparameter
    optimization package, the central dashboard and a Ray enhanced XGBoost implementation,
    by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: a reminder here that whenver you see `pip install` in this book, you can also
    use Poetry as outlined in *Chapter 4*, *Packaging Up*. So, in this case, you would
    have the following commands after running `poetry new` `project_name:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start by looking at Ray Train, which provides an API to a series of `Trainer`
    objects that helps facilitate distributed training. At the time of writing, Ray
    2.3.0 supports trainers across a variety of different frameworks including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning**: Horovod, Tensorflow and PyTorch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree based**: LightGBM and XGBoost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other**: Scikit-learn, HuggingFace, and Ray’s reinforcement learning library
    RLlib.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a company  Description automatically generated](img/B19525_06_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Ray Trainers as shown in the Ray docs at https://docs.ray.io/en/latest/train/train.xhtml.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first look at a tree-based learner example using XGBoost. Open up a
    script and begin adding to it; in the repo, this is called `getting_started_with_ray.py`.
    What follows is based on an introductory example given in the Ray documentation.
    First, we can use Ray to download one of the standard datasets; we could also
    have used `sklearn.datasets` or another source if we wanted to, like we have done
    elsewhere in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that here we use the `ray.data.read_csv()` method, which returns a `PyArrow`
    dataset. The Ray API has methods for reading from other data formats as well such
    as JSON or Parquet, as well as from databases like MongoDB or your own custom
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define a preprocessing step that will standardize the features
    we want to use; for more information on feature engineering, you can check out
    *Chapter 3*, *From Model to Model Factory*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then is the fun part where we define the `Trainer` object for the XGBoost model.
    This has several different parameters and inputs we will need to define shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: You’ll then see something like that shown in *Figure 6.19* as output if you
    run this code in a Jupyter notebook or Python script.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Outptut from parallel training of an XGBoost model using Ray.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `result` object contains tons of useful information; one of the attributes
    of it is called `metrics` and you can print this to reveal details about the end
    state of the run. Execute `print(result.metrics)` and you will see something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the instantiation of the `XGBoostTrainer`, we defined some important scaling
    information that was omitted in the previous example; here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `num_workers` parameter tells Ray how many actors to launch, with each actor
    by default getting one CPU. The `use_gpu` flag is set to false since we are not
    using GPU acceleration here. Finally, by setting the `_max_cpu_fraction_per_node`
    parameter to `0.9` we have left some spare capacity on each CPU, which can be
    used for other operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, there were also some XGBoost specific parameters we
    supplied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wanted to use GPU acceleration for the XGBoost training you would add
    `tree_method`: `gpu_hist` as a key-value pair in this `params` dictionary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A line graph with blue and orange lines  Description automatically generated](img/B19525_06_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: A few experiments show how changing the number of workers and
    CPUs available per worker results in different XGBoost training times on the author’s
    laptop (an 8 core Macbook Pro).'
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss briefly how you can scale compute with Ray when working
    in environments other than your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your compute for Ray
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The examples we’ve seen so far use a local Ray cluster that is automatically
    set up on the first call to the Ray API. This local cluster grabs all the available
    CPUs on your machine and makes them available to execute work. Obviously, this
    will only get you so far. The next stage is to work with clusters that can scale
    to far larger numbers of available workers in order to get more speedup. You have
    a few options if you want to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '**On the cloud**: Ray provides the ability to deploy on to Google Cloud Platform
    and AWS resources, with Azure deployments handled by a community maintained solution.
    For more information on deploying and running Ray on AWS, you can check out its
    online documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Kubernetes**: We have already met Kubeflow in *Chapter 5*, *Deployment
    Patterns and Tools*, which is used to build Kubernetes enabled ML pipelines. And
    we have also discussed Kubernetes in the Containerizing at Scale with Kubernetes
    section in this chapter.. As mentioned there, Kubernetes is a container orchestration
    toolkit designed to create massively scalable solutions based on containers. If
    you want to work with Ray on Kubernetes, you can use the **KubeRay** project,
    [https://ray-project.github.io/kuberay/](https://ray-project.github.io/kuberay/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The setup of Ray on either the cloud or Kubernetes mainly involves defining
    the cluster configuration and its scaling behaviour. Once you have done this,
    the beauty of Ray is that scaling your solution is as simple as editing the `ScalingConfig`
    object we used in the previous example, and you can keep all your other code the
    same. So, for example, if you have a 20-node CPU cluster, you could simply change
    the definition to the following and run it as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Scaling your serving layer with Ray
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the ways you can use Ray to distributed ML training jobs but
    now let’s have a look at how you can use Ray to help you scale your application
    layer. As mentioned before, Ray AIR provides some nice functionality for this
    that is badged under **Ray Serve**.
  prefs: []
  type: TYPE_NORMAL
- en: Ray Serve is a framework-agnostic library that helps you easily define ML endpoints
    based on your models. Like with the rest of the Ray API that we have interacted
    with, it has been built to provide easy interoperability and access to scaling
    without large development overheads.
  prefs: []
  type: TYPE_NORMAL
- en: Building on the examples from the previous few sections, let us assume we have
    trained a model, stored it in our appropriate registry, such as MLflow, and we
    have retrieved this model and have it in memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Ray Serve, we create **deployments** by using the `@ray.serve.deployments`
    decorator. These contain the logic we wish to use to process incoming API requests,
    including through any ML models we have built. As an example, let’s build a simple
    wrapper class that uses an XGBoost model like the one we worked with in the previous
    example to make a prediction based on some pre-processed feature data that comes
    in via the request object. First, the Ray documentation encourages the use of
    the Starlette requests library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we can define the simple class and use the `serve` decorator to define
    the service. I will assume that logic for pulling from MLflow or any other model
    storage location is wrapped into the utility function `get_model` in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: You can then deploy this across an existing Ray cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to Ray. We will now finish with a final discussion
    on *designing systems at scale* and then a summary of everything we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Designing systems at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build on the ideas presented in *Chapter 5*, *Deployment Patterns and Tools,*
    and in this chapter, we should now consider some of the ways in which the scaling
    capabilities we have discussed can be employed to maximum effect in your ML engineering
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea of scaling should be thought of in terms of providing an increase
    in the throughput of analyses or inferences or ultimate size of data that can
    be processed. There is no real difference in the kind of analyses or solution
    you can develop, at least in most cases. This means that applying scaling tools
    and techniques successfully is more dependent on selecting the correct processes
    that will benefit from them, even when we include any overheads that come from
    using these tools. That is what we will discuss now in this section, so that you
    have a few guiding principles to revisit when it comes to making your own scaling
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in several places throughout this book, the pipelines you develop
    for your ML projects will usually have to have stages that cover the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion/pre-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering (if different from above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization or distribution can help in many of these steps but usually
    in some different ways. For ingestion/pre-processing, if you are operating in
    a large scheduled batch setting, then the ability to scale to larger datasets
    in a distrubuted manner is going to be of huge benefit. In this case, the use
    of Apache Spark will make sense. For feature engineering, similarly they main
    bottleneck is in processing large amounts of data once as we perform the transformations,
    so again Spark is useful for this. The compute-intensive steps for training ML
    models that we discussed in detail in *Chapter 3*, *From Model to Model Factory*,
    are very amenable to frameworks that are optimized for this intensive computation,
    irrespective of the data size. This is where Ray comes in as discussed in the
    previous sections. Ray will mean that you can also neatly parallelize your hyperparameter
    tuning if you need to do that too. Note that you could run these steps in Spark
    as well but Ray’s low task overheads and its distributed state management mean
    that it is particularly amenable to splitting up these compute-intensive tasks.
    Spark on the other hand has centralized state and schedule management. Finally,
    when it comes to the inference and application layers, where we produce and surface
    the results of the ML model, we need to think about the requirements for the specific
    use case. As an example, if you want to serve your model as a REST API endpoint,
    we showed in the previous section how Ray’s distribution model and API can help
    facilitate this very easily, but this would not make sense to do in Spark. If,
    however, the model results are to be produced in large batches, then Spark or
    Ray may make sense. Also, as alluded to in the feature engineering and ingestion
    steps, if the end result should be transformed in large batches as well, perhaps
    into a specific data model such as a star schema, then performing that transformation
    in Spark may again make sense due to the data scale requirements of this task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this a bit more concrete by considering a potential example taken
    from industry. Many organizations with a retail element will analyze transactions
    and customer data in order to determine whether the customer is likely to churn.
    Let’s explore some of the decisions we can make to design and develop this solution
    with a particular focus on the questions of scaling up using the tools and techniques
    we have covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: First, we have the ingestion of the data. For this scenario, we will assume
    that the customer data, including interactions with different applications and
    systems, is processed at the end of the business day and numbers millions of records.
    This data contains numerical and categorical values and these need to be processed
    in order to feed into the downstream ML algorithm. If the data is partitioned
    by date, and maybe some other feature of the data, then this plays really naturally
    into the use of Spark, as you can read this into a Spark DataFrame and use the
    partitions to parallelize the data processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the feature engineering. If we are using a Spark DataFrame in
    the first step, then we can apply our transformation logic using the base PySpark
    syntax we have discussed earlier in this chapter. For example, if we want to apply
    some feature transformations available from Scikit-Learn or another ML library,
    we can wrap these in UDFs and apply at the scale we need to. The data can then
    be exported in our chosen data format using the PySpark API. For the customer
    churn model, this could mean a combination of encoding of categorical variables
    and scaling of numerical variables, in line with the techniques explored in *Chapter
    3*, *From Model to Model Factory*.
  prefs: []
  type: TYPE_NORMAL
- en: Switching into the training of the model, now are moving from the data-intensive
    to the compute-intensive tasks. This means it is natural to start using Ray for
    model training, as you can easily set up parallel tasks to train models with different
    hyperparameter settings and distribute the training steps as well. There are particular
    benefits to using Ray for training deep learning or tree-based models as these
    are algorithms that are amenable to parallelization. So, if we are performing
    classification using one of the available models in Spark ML, then this can be
    done in a few lines, but if we are using something else, we will likely need to
    start wrapping in UDFs. Ray is far more library-agnostic but again the benefits
    really come if we are using a neural network in PyTorch or TensorFlow or using
    XGBoost or LightGBM, as these more naturally parallelize.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, onto the model inference step. In a batch setting, it is less clear
    who the winner is in terms of suggested framework here. Using UDFs or the core
    PySpark APIs, you can easily create a quite scalable batch prediction stage using
    Apache Spark and your Spark cluster. This is essentially because prediction on
    a large batch is really just another large-scale data transformation, where Spark
    excels. If, however, you wish to serve your model as an endpoint that can scale
    across a cluster, this is where Ray has very easy-to-use capabilities as shown
    in the *Scaling your serving layer with Ray* section. Spark does not have a facility
    for creating endpoints in this way and the scheduling and task overheads required
    to get a Spark job up and running mean that it would not be worth running Spark
    on small packets of data coming in as requests like this.
  prefs: []
  type: TYPE_NORMAL
- en: For the customer churn example, this may mean that if we want to perform a churn
    classification on the whole customer base, Spark provides a nice way to process
    all of that data and leverage concepts like the underlying data partitions. You
    can still do this in Ray, but the lower-level API may mean it is slightly more
    work. Note that we can create this serving layer using many other mechanisms,
    as discussed in *Chapter 5*, *Deployment Patterns and Tools*, and the section
    on *Spinning up serverless infrastructure* in this chapter. *Chapter 8*, *Building
    an Example ML Microservice*, will also cover in detail how to use Kubernetes to
    scale out a deployment of an ML endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I have called the last stage the *application layer* to cover any “last
    mile” integrations between the output system and downstream systems in the solution.
    In this case, Spark does not really have a role to play since it can really be
    thought of as as a large-scale data transformation engine. Ray, on the other hand,
    has more of a philosophy of general Python acceleration, so if there are tasks
    that would benefit from parallelization in the backend of your applications, such
    as data retrieval, general calculations, simulation, or some other process, then
    the likelihood is you can still use Ray in some capacity, although there may be
    other tools available. So, in the customer churn example, Ray could be used for
    performing analysis at the level of individual customers and doing this in parallel
    before serving the results through a **Ray Serve** endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The point of going through this high-level example was to highlight the points
    along your ML engineering project where you can make choices about how to scale
    effectively. I like to say that there are often *no right answers, but very often
    wrong answers*. What I mean by this is that there are often several ways to build
    a good solution that are equally valid and may leverage different tools. The important
    thing is to avoid the biggest pitfalls and dead ends. Hopefully, the example gives
    some indication of how you can apply this thinking to scaling up your ML solutions.
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Although I have presented a lot of questions here in terms of Spark vs. Ray,
    with a nod to Kubernetes as a more *base infrastructure* scaling option, there
    is now the ability to combine Spark and Ray through the use of **RayDP**. This
    toolkit now allows you to run Spark jobs on Ray clusters, so it nicely allows
    you to still use Ray as your base scaling layer but then leveRage the Spark APIs
    and capabilities where it excels. RayDP was introduced in 2021 and is in active
    development, so this is definitely a capability to watch. For more information,
    see the project repository here: [https://github.com/oap-project/raydp](https://github.com/oap-project/raydp).'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our look at how we can start to apply some of the scaling techniques
    we have discussed to our ML use cases.
  prefs: []
  type: TYPE_NORMAL
- en: We will now finish the chapter with a brief summary of what we have covered
    in the last few pages.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to take the ML solutions we have built in
    the past few chapters and thought about how to scale them up to larger data volumes
    or higher numbers of requests for predictions. To do this, we mainly focused on
    **Apache Spark** as this is the most popular general-purpose engine for distributed
    computing. During our discussion of Apache Spark, we revisited some coding patterns
    and syntax we used previously in this book. By doing so, we developed a more thorough
    understanding of how and why to do certain things when developing in PySpark.
    We discussed the concept of **UDFs** in detail and how these can be used to create
    massively scalable ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we explored how to work with Spark on the cloud, specifically through
    the **EMR** service provided by AWS. Then, we looked at some of the other ways
    we can scale our solutions; that is, through serverless architectures and horizontal
    scaling with containers. In the former case, we walked through how to build a
    service for serving an ML model using **AWS Lambda**. This used standard templates
    provided by the AWS SAM framework. We provided a high-level view of how to use
    K8s and Kubeflow to scale out ML pipelines horizontally, as well as some of the
    other benefits of using these tools. A section covering the Ray parallel computing
    framework then followed, showing how you can use its relatively simple API to
    scale compute on heterogenous clusters to supercharge your ML workflows. Ray is
    now one of the most important scalable computing toolkits for Python and has been
    used to train some of the largest models on the planet, including the GPT-4 model
    from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we are going to build on the ideas of scale here by discussing
    the largest ML models you can build: deep learning models, including **large language
    models** (**LLM**s). Everything we will discuss in this next chapter could only
    have been developed, and can often only be effectively utilized, by considering
    the techniques we have covered here. The question of scaling up your ML solutions
    will also be revisited in *Chapter 8*, *Building an Example ML Microservice*,
    where we will focus on the use of Kubernetes to horizontally scale an ML microservice.
    This complements nicely the work we have done here on scaling large batch workloads
    by showing you how to scale more real-time workloads. Also, in *Chapter 9*, *Building
    an Extract, Transform, Machine Learning Use Case*, many of the scaling discussions
    we have had here are prerequisites; so, everything we have covered here puts you
    in a good place to get the most from the rest of the book. So, armed with all
    this new knowledge, let’s go and explore the world of the largest models known.'
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  prefs: []
  type: TYPE_IMG
