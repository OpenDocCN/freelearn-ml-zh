- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Scaling Up
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: The previous chapter was all about starting the conversation around how we get
    our solutions out into the world using different deployment patterns, as well
    as some of the tools we can use to do this. This chapter will aim to build on
    that conversation by discussing the concepts and tools we can use to scale up
    our solutions to cope with large volumes of data or traffic.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章全部关于开始讨论我们如何使用不同的部署模式将我们的解决方案推向世界，以及我们可以使用的某些工具。本章将在此基础上进行讨论，讨论我们可以使用的概念和工具，以扩展我们的解决方案以应对大量数据或流量。
- en: Running some simple **machine learning** (**ML**) models on a few thousand data
    points on your laptop is a good exercise, especially when you’re performing the
    discovery and proof-of-concept steps we outlined previously at the beginning of
    any ML development project. This approach, however, is not appropriate if we have
    to run millions upon millions of data points at a relatively high frequency, or
    if we have to train thousands of models of a similar scale at any one time. This
    requires a different approach, mindset, and toolkit.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的笔记本电脑上运行一些简单的**机器学习**（**ML**）模型，在几千个数据点上是一个很好的练习，尤其是在您执行我们在任何机器学习开发项目开始时概述的发现和概念验证步骤时。然而，如果我们必须以相对较高的频率运行数百万个数据点，或者如果我们必须同时训练数千个类似规模的模型，这种方法就不合适了。这需要不同的方法、心态和工具集。
- en: 'In the following pages, we will cover some details of two of the most popular
    frameworks for distributing data computations in use today: **Apache Spark** and
    **Ray**. In particular, we will discuss some of the key points about how these
    frameworks tick under the hood so that, in development, we can make some good
    decisions about how to use them. We will then move onto a discussion of how to
    use these in your ML workflows with some concrete examples, these examples being
    specifically aimed to help you when it comes to processing large batches of data.
    Next, a brief introduction to creating serverless applications that allow you
    to scale out inference endpoints will be provided. Finally, we will cover an introduction
    to scaling containerized ML applications with Kubernetes, which complements the
    work we did in *Chapter 5*, *Deployment Patterns and Tools*, and will be built
    upon in detail with a full end-to-end example in *Chapter 8*, *Building an Example
    ML Microservice*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下页面中，我们将介绍目前使用最广泛的两个用于分布式数据计算的框架的详细信息：**Apache Spark**和**Ray**。特别是，我们将讨论这些框架在底层的一些关键点，以便在开发过程中，我们可以就如何使用它们做出一些好的决策。然后，我们将讨论如何使用这些框架在您的机器学习工作流程中，并提供一些具体的示例，这些示例专门旨在帮助您在处理大量数据时。接下来，将提供一个关于创建允许您扩展推理端点的无服务器应用的简要介绍。最后，我们将介绍如何使用Kubernetes扩展容器化的机器学习应用，这补充了我们在*第5章*，*部署模式和工具*中完成的工作，并在*第8章*，*构建示例ML微服务*中详细展开。
- en: This will help you build on some of the practical examples we already looked
    at earlier in this book, when we used Spark to solve our ML problems, with some
    more concrete theoretical understanding and further detailed practical examples.
    After this chapter, you should feel confident in how to use some of the best frameworks
    and techniques available to scale your ML solutions to larger and larger datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助您在我们之前在这本书中已经查看的一些实际示例的基础上进行构建，当时我们使用Spark来解决我们的机器学习问题，并增加一些更具体的理论理解和更详细的实际示例。在本章之后，您应该对如何使用一些最好的框架和技术来扩展您的机器学习解决方案以适应更大的数据集感到自信。
- en: 'In this chapter, we will cover all of this in the following sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在以下部分中涵盖所有这些内容：
- en: Scaling with Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行扩展
- en: Spinning up serverless infrastructure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动无服务器基础设施
- en: Containerizing at scale with Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes进行大规模容器化
- en: Scaling with Ray
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ray进行扩展
- en: Designing systems at scale
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计大规模系统
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As with the other chapters, you can set up your Python development environment
    to be able to run the examples in this chapter by using the supplied Conda environment
    `yml` file or the `requirements.txt` files from the book repository, under *Chapter06:*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节一样，您可以通过使用提供的Conda环境`yml`文件或从书库中的`requirements.txt`文件来设置您的Python开发环境，以便能够运行本章中的示例，在*第06章*下：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This chapter’s examples will also require some non-Python tools to be installed
    to follow the examples end to end; please see the respective documentation for
    each tool:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例还需要安装一些非Python工具，以便从头到尾遵循示例；请参阅每个工具的相关文档：
- en: AWS CLI v2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI v2
- en: Docker
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Postman
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Postman
- en: Ray
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray
- en: Apache Spark (version 3.0.0 or higher)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark（版本3.0.0或更高）
- en: Scaling with Spark
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行扩展
- en: '**Apache Spark**, or just Spark, came from the work of some brilliant researchers
    at the *University of California, Berkeley* in 2012 and since then, it has revolutionized
    how we tackle problems with large datasets.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**，或简称Spark，起源于2012年加州大学伯克利分校一些杰出研究人员的工作，自那时起，它彻底改变了我们处理大数据集问题的方法。'
- en: Spark is a cluster computing framework, which means it works on the principle
    that several computers are linked together in a way that allows computational
    tasks to be shared. This allows us to coordinate these tasks effectively. Whenever
    we discuss running Spark jobs, we always talk about *the cluster* we are running
    on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个集群计算框架，这意味着它基于几个计算机以允许计算任务共享的方式相互连接的原则。这使我们能够有效地协调这些任务。每次我们讨论运行Spark作业时，我们总是谈论我们在其上运行的*集群*。
- en: This is the collection of computers that perform the tasks, the worker nodes,
    and the computer that hosts the organizational workload, known as the head node.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一组执行任务的计算机，即工作节点，以及托管组织工作负载的计算机，被称为头节点。
- en: Spark is written in Scala, a language with a strong functional flavor and that
    compiles down to **Java Virtual Machines** (**JVMs**). Since this is a book about
    ML engineering in Python, we won’t discuss too much about the underlying Scala
    components of Spark, except where they help us use it in our work. Spark has several
    popular APIs that allow programmers to develop with it in a variety of languages,
    including Python. This gives rise to the PySpark syntax we have used in several
    examples throughout this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala编写的，这是一种具有强烈函数式风格的编程语言，并编译成**Java虚拟机**（**JVMs**）。由于这是一本关于Python机器学习工程的书籍，我们不会过多讨论Spark底层的Scala组件，除非它们有助于我们在工作中使用它。Spark有几个流行的API，允许程序员用多种语言（包括Python）与之一起开发。这导致了我们在本书中使用的PySpark语法。
- en: So, how is this all put together?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这一切是如何组合在一起的？
- en: 'Well, first of all, one of the things that makes Apache Spark so incredibly
    popular is the large array of connectors, components, and APIs it has available.
    For example, four main components interface with *Spark Core*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使Apache Spark如此受欢迎的一个原因是它拥有大量可用的连接器、组件和API。例如，四个主要组件与*Spark Core*接口：
- en: '**Spark SQL**, **DataFrames**, **and** **Datasets**: This component allows
    you to create very scalable programs that deal with structured data. The ability
    to write SQL-compliant queries and create data tables that leverage the underlying
    Spark engine through one of the main **structured APIs** of Spark (Python, Java,
    Scala, or R) gives very easy access to the main bulk of Spark’s functionality.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**、**DataFrames**和**Datasets**：这个组件允许你创建非常可扩展的程序，用于处理结构化数据。通过Spark的主要**结构化API**（Python、Java、Scala或R）编写符合SQL规范的查询并创建利用底层Spark引擎的数据表，可以非常容易地访问Spark的主要功能集。'
- en: '**Spark Structured Streaming**: This component allows engineers to work with
    streaming data that’s, for example, provided by a solution such as Apache Kafka.
    The design is incredibly simple and allows developers to simply work with streaming
    data as if it is a growing Spark structured table, with the same querying and
    manipulation functionality as for a standard one. This provides a low entry barrier
    for creating scalable streaming solutions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Structured Streaming**：这个组件允许工程师处理由例如Apache Kafka提供的流数据。设计极其简单，允许开发者像处理一个不断增长的Spark结构化表一样简单地处理流数据，具有与标准表相同的查询和处理功能。这为创建可扩展的流解决方案提供了低门槛。'
- en: '**GraphX**: This is a library that allows you to implement graph parallel processing
    and apply standard algorithms to graph-based data (for example, algorithms such
    as PageRank or Triangle Counting). The **GraphFrames** project from Databricks
    makes this functionality even easier to use by allowing us to work with DataFrame-based
    APIs in Spark and still analyze graph data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：这是一个库，允许你实现图并行处理并将标准算法应用于基于图的数据（例如，如PageRank或三角形计数）。Databricks的**GraphFrames**项目通过允许我们在Spark中使用基于DataFrame的API来分析图数据，使得这一功能更加易于使用。'
- en: '**Spark ML**: Last but not least, we have the component that’s most appropriate
    for us as ML engineers: Spark’s native library for ML. This library contains the
    implementation of many algorithms and feature engineering capabilities we have
    already seen in this book. Being able to use **DataFrame** APIs in the library
    makes this extremely easy to use, while still giving us a route to creating very
    powerful code.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark ML**：最后但同样重要的是，我们有最适合我们作为机器学习工程师的组件：Spark 的原生机器学习库。这个库包含了我们在本书中已经看到过的许多算法和特征工程能力。能够在库中使用
    **DataFrame** API 使得它极其易于使用，同时仍然为我们提供了创建非常强大代码的途径。'
- en: The potential speedups you can gain for your ML training by using Spark ML on
    a Spark cluster versus running another ML library on a single thread can be tremendous.
    There are other tricks we can apply to our favorite ML implementations and then
    use Spark to scale them out; we’ll look at this later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 Spark 集群上使用 Spark ML 与在单个线程上运行另一个机器学习库相比，你可以为你的机器学习训练获得巨大的速度提升。我们还可以应用其他技巧到我们最喜欢的机器学习实现中，然后使用
    Spark 来扩展它们；我们稍后会探讨这一点。
- en: Spark’s architecture is based on the driver/executor architecture. The driver
    is the program that acts as the main entry point for the Spark application and
    is where the **SparkContext** object is created. **SparkContext** sends tasks
    to the executors (which run on their own JVMs) and communicates with the cluster
    manager in a manner appropriate to the given manager and what mode the solution
    is running in. One of the driver’s main tasks is to convert the code we write
    into a logical set of steps in a **Directed Acyclic Graph** (**DAG**) (the same
    concept that we used with Apache Airflow in *Chapter 5*, *Deployment Patterns
    and Tools*), and then convert that DAG into a set of tasks that needs to be executed
    across the available compute resources.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的架构基于驱动程序/执行器架构。驱动程序是作为 Spark 应用程序的主要入口点的程序，也是创建 **SparkContext** 对象的地方。**SparkContext**
    将任务发送到执行器（它们在自己的 JVM 上运行），并以适合给定管理器和解决方案运行模式的方式与集群管理器进行通信。驱动程序的主要任务之一是将我们编写的代码转换为
    **有向无环图**（**DAG**）中的逻辑步骤集合（与我们在第 5 章 *部署模式和工具* 中使用的 Apache Airflow 的概念相同），然后将该
    DAG 转换为需要在可用的计算资源上执行的任务集合。
- en: In the pages that follow, we will assume we are running Spark with the **Hadoop
    YARN** resource manager, which is one of the most popular options and is also
    used by the **AWS****Elastic MapReduce** (**EMR**) solution by default (more on
    this later). When running with YARN in *cluster mode*, the driver program runs
    in a container on the YARN cluster, which allows a client to submit jobs or requests
    through the driver and then exit (rather than requiring the client to remain connected
    to the cluster manager, which can happen when you’re running in so-called *client
    mode*, which we will not discuss here).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的页面中，我们将假设我们正在使用 **Hadoop YARN** 资源管理器运行 Spark，这是最受欢迎的选项之一，也是 **AWS Elastic
    MapReduce**（**EMR**）解决方案的默认选项（关于这一点稍后还会详细介绍）。在以 *集群模式* 运行 YARN 时，驱动程序程序在 YARN
    集群上的一个容器中运行，这使得客户端可以通过驱动程序提交作业或请求，然后退出（而不是要求客户端保持与集群管理器的连接，这在所谓的 *客户端模式* 下可能会发生，这里我们不会讨论）。
- en: The cluster manager is responsible for launching the executors across the resources
    that are available on the cluster.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器负责在集群上可用的资源上启动执行器。
- en: Spark’s architecture allows us, as ML engineers, to build solutions with the
    same API and syntax, regardless of whether we are working locally on our laptop
    or a cluster with thousands of nodes. The connection between the driver, the resource
    manager, and the executors is what allows this magic to happen.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的架构允许我们作为机器学习工程师，无论我们是在笔记本电脑上本地工作还是在拥有数千个节点的集群上工作，都可以使用相同的 API 和语法来构建解决方案。驱动程序、资源管理器和执行器之间的连接是实现这种魔法的关键。
- en: Spark tips and tricks
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 技巧和技巧
- en: 'In this subsection, we will cover some simple but effective tips for writing
    performant solutions with Spark. We will focus on key pieces of syntax for data
    manipulation and preparation, which are always the first steps in any ML pipeline.
    Let’s get started:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍一些简单但有效的技巧，以使用 Spark 编写高性能的解决方案。我们将重点关注数据操作和准备的关键语法，这些通常是任何机器学习管道中的第一步。让我们开始吧：
- en: First, we will cover the basics of writing good Spark SQL. The entry point for
    any Spark program is the `SparkSession` object, which we need to import an instance
    of in our application.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将介绍编写良好的 Spark SQL 的基础知识。任何 Spark 程序的入口点是 `SparkSession` 对象，我们需要在我们的应用程序中导入其实例。
- en: 'It is often instantiated with the `spark` variable:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它通常使用`spark`变量实例化：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then run Spark SQL commands against your available data using the `spark`
    object and the `sql` method:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`spark`对象和`sql`方法运行Spark SQL命令，针对你的可用数据：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are a variety of ways to make the data you need available inside your
    Spark programs, depending on where they exist. The following example has been
    taken from some of the code we went through in *Chapter 3*, *From Model to Model
    Factory*, and shows how to pull data into a DataFrame from a `csv` file:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据数据存在的地方，有各种方法可以在Spark程序内部提供所需的数据。以下示例取自我们在*第3章*，“从模型到模型工厂”中经过的一些代码，展示了如何从`csv`文件中将数据拉入DataFrame：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can create a temporary view of this data using the following syntax:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下语法创建此数据的临时视图：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we can query against this data using the methods mentioned previously
    to see the records or to create new DataFrames:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用之前提到的方法查询此数据，以查看记录或创建新的DataFrames：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When writing Spark SQL, some standard practices help your code to be efficient:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写Spark SQL时，一些标准做法有助于提高代码的效率：
- en: Try not to join big tables on the left with small tables on the right as this
    is inefficient. In general, try and make datasets used for joins as lean as possible,
    so, for example, do not join using unused columns or rows as much as possible.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量不要将左边的大的表格与右边的小的表格连接，因为这效率低下。通常，尽量使用于连接的数据集尽可能瘦，例如，尽可能少地使用未使用的列或行进行连接。
- en: Avoid query syntax that will scan full datasets if they are very large; for
    example, `select max(date_time_value)`.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免查询语法扫描非常大的数据集；例如，`select max(date_time_value)`。
- en: In this case, try and define logic that can filter the data more aggressively
    before finding min or max values and in general allow the solution to scan over
    a smaller dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，尝试定义逻辑，在找到最小或最大值之前更积极地过滤数据，并且通常允许解决方案扫描更小的数据集。
- en: 'Some other good practices when working with Spark are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark时，以下是一些其他的好做法：
- en: '**Avoid data skew**: Do what you can to understand how your data will be split
    across executors. If your data is partitioned on a date column, this may be a
    good choice if volumes of data are comparable for each day but bad if some days
    have most of your data and others very little. Repartitioning using a more appropriate
    column (or on a Spark-generated ID from the `repartition` command) will be required.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免数据倾斜**：尽可能了解你的数据将如何在执行器之间分割。如果你的数据是在日期列上分区的，如果每天的数据量相当，这可能是一个不错的选择，但如果某些天有大部分数据而其他天很少，这可能是一个坏选择。可能需要使用更合适的列（或使用`repartition`命令生成的Spark生成的ID）重新分区。'
- en: '**Avoid data shuffling**: This is when data is redistributed across different
    partitions. For example, we may have a dataset that is partitioned at the day
    level and then we ask Spark to sum over one column of the dataset for all of time.
    This will cause all of the daily partitions to be accessed and the result to be
    written to a new partition. For this to occur, disk writes and a network transfer
    have to occur, which can often lead to performance bottlenecks for your Spark
    job.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免数据洗牌**：这是指数据在不同分区之间重新分配。例如，我们可能有一个按日级别分区的数据集，然后我们要求Spark对所有时间的数据集的一个列求和。这将导致所有每日分区被访问，并将结果写入一个新的分区。为此，必须发生磁盘写入和网络传输，这通常会导致你的Spark作业的性能瓶颈。'
- en: '**Avoid actions in large datasets**: For example, when you run the `collect()`
    command, you will bring all of your data back onto the driver node. This can be
    very bad if it is a large dataset but may be needed to convert the result of a
    calculation into something else. Note that the `toPandas()` command, which converts
    your Spark `DataFrame` into a pandas `DataFrame`, also collects all the data in
    the driver’s memory.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免在大数据集中执行操作**：例如，当你运行`collect()`命令时，你将把所有数据都带回驱动节点。如果这是一个大数据集，这可能会非常糟糕，但可能需要将计算结果转换为其他东西。请注意，`toPandas()`命令，它将你的Spark
    `DataFrame`转换为pandas `DataFrame`，也会收集驱动器内存中的所有数据。'
- en: '**Use UDFs when they make sense**: Another excellent tool to have in your arsenal,
    as an ML engineer using Apache Spark, is the **User-Defined Function** (**UDF**).
    UDFs allow you to wrap up more complex and bespoke logic and apply it at scale
    in a variety of ways. An important aspect of this is that if you write a standard
    PySpark (or Scala) UDF, then you can apply this *inside* Spark SQL syntax, which
    allows you to efficiently reuse your code and even simplify the application of
    your ML models. The downside is that these are sometimes not the most efficient
    pieces of code, but if it helps to make your solution simpler and more maintainable,
    it may be the right choice.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当适用时使用UDF**：作为Apache Spark的ML工程师，你武器库中的另一个优秀工具是**用户定义函数**（**UDF**）。UDF允许你封装更复杂和定制的逻辑，并以各种方式大规模应用。这个方面的重要之处在于，如果你编写了一个标准的PySpark（或Scala）UDF，那么你可以在Spark
    SQL语法内部应用这个UDF，这允许你高效地重用你的代码，甚至简化ML模型的适用。缺点是这些代码有时可能不是最有效的，但如果它有助于使你的解决方案更简单、更易于维护，那么它可能是一个正确的选择。'
- en: As a concrete example, let’s build a UDF that looks at the banking data we worked
    with in *Chapter 3*, *From Model to Model Factory*, to create a new column called
    ‘`month_as_int`' that converts the current string representation of the month
    into an integer for processing later. We will not concern ourselves with train/test
    splits or what this might be used for; instead, we will just highlight how to
    apply some logic to a PySpark UDF.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体示例，让我们构建一个UDF，它将查看我们在第3章“从模型到模型工厂”中处理过的银行数据，创建一个名为‘`month_as_int`’的新列，该列将当前月份的字符串表示形式转换为整数以便后续处理。我们不会关注训练/测试分割或这可能被用于什么；相反，我们将突出如何将一些逻辑应用于PySpark
    UDF。
- en: 'Let’s get started:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, we must read the data. Note that the relative path given here is consistent
    with the `spark_example_udfs.py` script, which can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须读取数据。注意这里给出的相对路径与本书GitHub仓库中的`spark_example_udfs.py`脚本一致，该脚本位于[https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py)：
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we show the current data with the `data.show()` command, we will see something
    like this:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们使用`data.show()`命令显示当前数据，我们会看到类似以下内容：
- en: '![Figure 6.1 – A sample of the data from the initial DataFrame in the banking
    dataset ](img/B19525_06_01.png)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.1 – 银行数据集中初始DataFrame的数据样本](img/B19525_06_01.png)'
- en: 'Figure 6.1: A sample of the data from the initial DataFrame in the banking
    dataset.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.1：银行数据集中初始DataFrame的数据样本。
- en: 'Now, we can double-check the schema of this DataFrame using the `data.printSchema()`
    command. This confirms that `month` is stored as a string currently, as shown
    here:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`data.printSchema()`命令双重检查这个DataFrame的模式。这确认了`month`目前是以字符串形式存储的，如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can define our UDF, which will use the Python `datetime` library to
    convert the string representation of the month into an integer:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们的UDF，它将使用Python的`datetime`库将月份的字符串表示形式转换为整数：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we want to apply our function inside Spark SQL, then we must register the
    function as a UDF. The arguments for the `register()` function are the registered
    name of the function, the name of the Python function we have just written, and
    the return type. The return type is `StringType()` by default, but we have made
    this explicit here:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想在Spark SQL内部应用我们的函数，那么我们必须将函数注册为UDF。`register()`函数的参数是函数的注册名称、我们刚刚编写的Python函数的名称以及返回类型。默认情况下，返回类型是`StringType()`，但我们在这里明确指定了它：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, now that we have registered the function, we can apply it to our data.
    First, we will create a temporary view of the bank dataset and then run a Spark
    SQL query against it that references our UDF:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，既然我们已经注册了函数，我们就可以将其应用于我们的数据。首先，我们将创建银行数据集的一个临时视图，然后运行一个Spark SQL查询，该查询引用我们的用户定义函数（UDF）：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the preceding syntax with the `show()` command shows that we have successfully
    calculated the new column. The last few columns of the resulting `DataFrame` are
    shown here:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`show()`命令运行前面的语法显示我们已经成功计算了新列。结果`DataFrame`的最后几列如下所示：
- en: '![Figure 6.3 – The new column has been calculated successfully by applying
    our UDF ](img/B19525_06_02.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.3 – 通过应用我们的UDF成功计算了新列](img/B19525_06_02.png)'
- en: 'Figure 6.2: The new column has been calculated succesfully by applying our
    UDF.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.2：通过应用我们的UDF成功计算了新列。
- en: 'Alternatively, we can create our UDF with the following syntax and apply the
    result to a Spark `DataFrame`. As mentioned before, using a UDF can sometimes
    allow you to wrap up relatively complex syntax quite simply. The syntax here is
    quite simple but I’ll show you it anyway. This gives us the same result that’s
    shown in the preceding screenshot:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以使用以下语法创建我们的UDF，并将结果应用于Spark `DataFrame`。如前所述，使用UDF有时可以让你非常简单地封装相对复杂的语法。这里的语法相当简单，但我仍然会向你展示。这给我们带来了与前面截图相同的结果：
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, PySpark also provides a nice decorator syntax for creating our UDF,
    meaning that if you are indeed building some more complex functionality, you can
    just place this inside the Python function that is being decorated. The following
    code block also gives the same results as the preceding screenshot:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，PySpark还提供了一个很好的装饰器语法来创建我们的UDF，这意味着如果你确实在构建一些更复杂的功能，你只需将这个装饰器放在被装饰的Python函数中即可。下面的代码块也给出了与前面截图相同的结果：
- en: '[PRE12]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This shows how we can apply some simple logic in a UDF, but for us to deploy
    a model at scale using this approach, we have to put the ML logic inside the function
    and apply it in the same manner. This can become a bit tricky if we want to work
    with some of the standard tools we are used to from the data science world, such
    as Pandas and **Scikit-learn**. Luckily, there is another option we can use that
    has a few benefits. We will discuss this now.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了如何在UDF中应用一些简单的逻辑，但为了使用这种方法在规模上部署模型，我们必须在函数内部放置ML逻辑并以相同的方式应用它。如果我们想使用我们习惯于从数据科学世界使用的标准工具，如Pandas和**Scikit-learn**，这可能会变得有点棘手。幸运的是，我们还有另一个可以使用的选项，它有一些优点。我们现在就来讨论这个。
- en: The UDFs currently being considered have a slight issue when we are working
    in Python in that translating data between the JVM and Python can take a while.
    One way to get around this is to use what is known as **pandas UDFs**, which use
    the Apache Arrow library under the hood to ensure that the data is read quickly
    for the execution of our UDFs. This gives us the flexibility of UDFs without any
    slowdown.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Python中工作时，目前考虑的UDF存在一个小问题，那就是在JVM和Python之间转换数据可能需要一段时间。一种解决方法是使用所谓的**pandas
    UDFs**，它底层使用Apache Arrow库来确保我们的UDF执行时数据读取快速。这给我们带来了UDF的灵活性，而没有任何减速。
- en: pandas UDFs are also extremely powerful because they work with the syntax of
    – you guessed it – pandas **Series** and **DataFrame** objects. This means that
    a lot of data scientists who are used to working with pandas to build models locally
    can easily adapt their code to scale up using Spark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: pandas UDFs也非常强大，因为它们与pandas **Series** 和 **DataFrame** 对象的语法一起工作。这意味着许多习惯于使用pandas在本地构建模型的科学家可以轻松地将他们的代码扩展到使用Spark。
- en: 'As an example, let’s walk through how to apply a simple classifier to the wine
    dataset that we used earlier in this book. Note that the model was not optimized
    for this data; we are just showing an example of applying a pre-trained classifier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们回顾一下如何将一个简单的分类器应用于我们在这本书中之前使用过的wine数据集。请注意，该模型并未针对这些数据进行优化；我们只是展示了一个应用预训练分类器的示例：
- en: 'First, let’s create a simple **Support Vector Machine** (**SVM**)-based classifier
    on the wine dataset. We are not performing correct training/test splits, feature
    engineering, or other best practices here as we just want to show you how to apply
    any `sklearn` model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们在wine数据集上创建一个简单的**支持向量机**（**SVM**）分类器。我们在这里没有进行正确的训练/测试分割、特征工程或其他最佳实践，因为我们只是想向你展示如何应用任何`sklearn`模型：
- en: '[PRE13]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then bring the feature data into a Spark DataFrame to show you how to
    apply the pandas UDF in later stages:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将特征数据带入Spark DataFrame，以展示如何在后续阶段应用pandas UDF：
- en: '[PRE14]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'pandas UDFs are very easy to define. We just write our logic in a function
    and then add the `@pandas_udf` decorator, where we also have to provide the output
    type for the function. In the simplest case, we can just wrap the (normally serial
    or only locally parallelized) process of performing a prediction with the trained
    model:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pandas UDFs非常容易定义。我们只需在函数中编写我们的逻辑，然后添加`@pandas_udf`装饰器，在那里我们还需要为函数提供输出类型。在最简单的情况下，我们可以将使用训练模型进行预测的（通常是串行或仅本地并行化）过程封装起来：
- en: '[PRE15]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can apply this to the Spark `DataFrame` containing the data by
    passing in the appropriate inputs we needed for our function. In this case, we
    will pass in the column names of the features, of which there are 13:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过传递我们函数所需的适当输入来将此应用于包含数据的Spark `DataFrame`。在这种情况下，我们将传递特征列的名称，共有13个：
- en: '[PRE16]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, if you look at the results of this, you will see the following for the
    first few rows of the `df_pred` DataFrame:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您查看这个结果，您将看到`df_pred` DataFrame的前几行如下所示：
- en: '![Figure 6.4 – The result of applying a simple pandas UDF ](img/B19525_06_03.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 应用简单pandas UDF的结果](img/B19525_06_03.png)'
- en: 'Figure 6.3: The result of applying a simple pandas UDF.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：应用简单的pandas UDF的结果。
- en: And that completes our whirlwind tour of UDFs and pandas UDFs in Spark, which
    allow us to take serial Python logic, such as data transformations or our ML models,
    and apply them in a manifestly parallel way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了对Spark和pandas UDF在Spark中的快速浏览，这使我们能够以明显并行的方式应用诸如数据转换或我们的机器学习模型之类的串行Python逻辑。
- en: In the next section, we will focus on how to set ourselves up to perform Spark-based
    computations in the cloud.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将专注于如何在云端设置Spark-based计算。
- en: Spark on the cloud
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云端Spark
- en: As should be clear from the preceding discussion, writing and deploying PySpark-based
    ML solutions can be done on your laptop, but for you to see the benefits when
    working at scale, you must have an appropriately sized computing cluster to hand.
    Provisioning this sort of infrastructure can be a long and painful process but
    as discussed already in this book, a plethora of options for infrastructure are
    available from the main public cloud providers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，应该很清楚，编写和部署基于PySpark的机器学习解决方案可以在您的笔记本电脑上完成，但为了在工作规模上看到好处，您必须拥有适当规模的计算集群。提供此类基础设施可能是一个漫长而痛苦的过程，但正如本书中已经讨论的那样，主要公共云提供商提供了大量的基础设施选项。
- en: For Spark, AWS has a particularly nice solution called **AWS Elastic MapReduce**
    (**EMR**), which is a managed big data platform that allows you to easily configure
    clusters of a few different flavors across the big data ecosystem. In this book,
    we will focus on Spark-based solutions, so we will focus on creating and using
    clusters that have Spark tooling to hand.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark，AWS有一个特别好的解决方案，称为**AWS Elastic MapReduce**（**EMR**），这是一个托管的大数据平台，允许您轻松配置大数据生态系统中的几种不同类型的集群。在这本书中，我们将专注于基于Spark的解决方案，因此我们将专注于创建和使用带有Spark工具的集群。
- en: In the next section, we will go through a concrete example of spinning up a
    Spark cluster on EMR and then deploying a simple Spark ML-based application onto
    it.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过一个具体的例子来展示如何在EMR上启动一个Spark集群，然后将其部署一个简单的基于Spark ML的应用程序。
- en: So, with that, let’s explore Spark on the cloud with **AWS EMR**!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们在**AWS EMR**上探索Spark在云端的应用！
- en: AWS EMR example
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS EMR示例
- en: 'To understand how EMR works, we will continue in the practical vein that the
    rest of this book will follow and dive into an example. We will begin by learning
    how to create a brand-new cluster before discussing how to write and deploy our
    first PySpark ML solution to it. Let’s get started:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解EMR是如何工作的，我们将继续遵循本书的实践方法，并深入一个例子。我们将首先学习如何创建一个全新的集群，然后再讨论如何编写和部署我们的第一个PySpark
    ML解决方案到集群中。让我们开始吧：
- en: First, navigate to the **EMR** page on AWS and find the **Create Cluster** button.
    You will then be brought to a page that allows you to input configuration data
    for your cluster. The first section is where you specify the name of the cluster
    and the applications you want to install on it. I will call this cluster `mlewp2-cluster`,
    use the latest EMR release available at the time of writing, 6.11.0, and select
    the **Spark** application bundle.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到AWS上的**EMR**页面，找到**创建集群**按钮。然后，您将被带到允许您输入集群配置数据的页面。第一个部分是您指定集群名称和要安装在其上的应用程序的地方。我将把这个集群命名为`mlewp2-cluster`，使用写作时的最新EMR版本6.11.0，并选择**Spark**应用程序包。
- en: All other configurations can remain as default in this first section. This is
    shown in *Figure 6.4*:![A screenshot of a computer program  Description automatically
    generated](img/B19525_06_04.png)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个第一部分，所有其他配置都可以保持默认设置。这如图6.4所示：![计算机程序截图 描述自动生成](img/B19525_06_04.png)
- en: 'Figure 6.4: Creating our EMR cluster with some default configurations.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.4：使用一些默认配置创建我们的EMR集群。
- en: Next comes the configuration of the compute used in the cluster. You can just
    use the defaults here again but it is important to understand what is going on.
    First, there is the selection of whether to use “instance groups” or “instance
    fleets,” which refers to the strategy deployed for scaling up your compute given
    some constraints you provide. Instance groups are simpler and define the specific
    servers you want to run for each node type, more on this in a second, and you
    can choose between “on-demand” or “spot instances” for acquiring more servers
    if needed during the lifetime of the cluster. Instance fleets allow for a lot
    more complex acquisition strategies and for blends of different server instance
    types for each node type. For more information, read the AWS documentation to
    make sure you get a clear view of the different options, [https://docs.aws.amazon.com/emr/index.xhtml](https://docs.aws.amazon.com/emr/index.xhtml);
    we will proceed by using an instance group with default settings. Now, onto nodes.
    There are different nodes in an EMR cluster; primary, core and task. The primary
    node will run our YARN Resource Manager and will track job status and instance
    group health. The core nodes run some daemons and the Spark executors. Finally,
    the task nodes perform the actual distributed calculations. For now, let’s proceed
    with the defaults provided for the instance groups option, as shown for the **Primary**
    nodes in *Figure 6.5*.![A screenshot of a computer  Description automatically
    generated](img/B19525_06_05.png)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是集群中使用的计算配置。您在这里也可以再次使用默认设置，但了解正在发生的事情很重要。首先，是选择使用“实例组”还是“实例舰队”，这指的是根据您提供的某些约束条件部署的计算扩展策略。实例组更简单，定义了您为每种节点类型想要运行的特定服务器，关于这一点我们稍后再详细说明，并且您可以在集群生命周期内需要更多服务器时选择“按需”或“竞价实例”。实例舰队允许采用更多复杂的获取策略，并为每种节点类型混合不同的服务器实例类型。有关更多信息，请阅读
    AWS 文档，以确保您对不同的选项有清晰的了解，[https://docs.aws.amazon.com/emr/index.xhtml](https://docs.aws.amazon.com/emr/index.xhtml)；我们将通过使用具有默认设置的实例组来继续操作。现在，让我们转到节点。EMR
    集群中有不同的节点；主节点、核心节点和任务节点。主节点将运行我们的 YARN 资源管理器，并跟踪作业状态和实例组健康。核心节点运行一些守护程序和 Spark
    执行器。最后，任务节点执行实际的分布式计算。现在，让我们按照为实例组选项提供的默认设置进行操作，如图 6.5 中的**主节点**所示。![计算机屏幕截图  自动生成的描述](img/B19525_06_05.png)
- en: 'Figure 6.5: Compute configuration for our EMR cluster. We have selected the
    simpler “instance groups” option for the configuration and have gone with the
    server type defaults.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.5：我们的 EMR 集群的计算配置。我们选择了更简单的“实例组”选项进行配置，并采用了服务器类型的默认设置。
- en: Next, we move onto defining the explicit cluster scaling behavior that we mentioned
    is used in the instance groups and instance fleet compute options in *step 2*.
    Again, select the defaults for now, but you can play around to make the cluster
    larger here in terms of numbers of nodes or you can define auto-scaling behaviour
    that will dynamically increase the cluster size upon larger workloads. *Figure
    6.6* shows what this should look like.![A screenshot of a computer screen  Description
    automatically generated](img/B19525_06_06.png)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们在*步骤 2*中提到的用于实例组和实例舰队计算选项的显式集群缩放行为。再次提醒，现在请选择默认设置，但您可以在这里尝试调整集群的大小，无论是通过增加节点数量，还是定义在负载增加时动态增加集群大小的自动缩放行为。*图
    6.6*展示了它应该看起来是什么样子。![计算机屏幕截图  自动生成的描述](img/B19525_06_06.png)
- en: 'Figure 6.6: The cluster provisioning and scaling strategy selection. Here we
    have gone with the defaults of a specific, small cluster size, but you can increase
    these values to have a bigger cluster or use the auto-scaling option to provide
    min and max size limits.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.6：集群配置和缩放策略选择。在这里，我们选择了特定的小集群大小的默认设置，但您可以增加这些值以获得更大的集群，或者使用自动缩放选项来提供最小和最大大小限制。
- en: Now, there is a **Networking** section, which is easier if you have already
    created some **virtual private clouds** (**VPCs**) and subnets for the other examples
    in the book; see *Chapter 5*, *Deployment Patterns and Tools* and the AWS documentation
    for more information. Just remember that VPCs are all about keeping the infrastructure
    you are provisioning isolated from other services in your own AWS account and
    even from the wider internet, so it’s definitely good to become familiar with
    them and their application.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，有一个**网络**部分，如果你已经为书中的其他示例创建了一些**虚拟专用网络**（**VPC**）和子网，这将更容易；参见*第5章*，*部署模式和工具*以及AWS文档以获取更多信息。只需记住，VPCs主要是关于将你正在配置的基础设施与其他AWS账户中的服务以及更广泛的互联网隔离开来，因此熟悉它们及其应用绝对是件好事。
- en: For completeness, *Figure 6.7* shows the setup I used for this example.![A screenshot
    of a computer  Description automatically generated](img/B19525_06_07.png)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完整性，*图6.7*显示了我在这个示例中使用的设置。![计算机截图 自动生成描述](img/B19525_06_07.png)
- en: 'Figure 6.7: The networking configuration requires the use of a VPC; it will
    automatically create a subnet for the cluster if there is not one selected.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.7：网络配置需要使用VPC；如果没有选择，它将自动为集群创建一个子网。
- en: We only have a couple more sections we need to input into to define our cluster.
    The next mandatory section is around cluster termination policies. I would always
    recommend having an automated teardown policy for infrastructure where possible
    as this helps to manage costs. There have been many stories across the industry
    of teams leaving un-used servers running and racking up huge bills! *Figure 6.8*
    shows that we are using such an automated cluster termination policy where the
    cluster will terminate after 1 hour of not being utilized.![A screenshot of a
    computer  Description automatically generated](img/B19525_06_08.png)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要输入几个更多部分来定义我们的集群。下一个强制性的部分是关于集群终止策略。我总是建议在可能的情况下为基础设施设置自动拆解策略，因为这有助于管理成本。整个行业都有很多关于团队留下未使用的服务器运行并产生巨额账单的故事！*图6.8*显示，我们正在使用这样的自动集群终止策略，其中集群将在1小时未被使用后终止。![计算机截图
    自动生成描述](img/B19525_06_08.png)
- en: 'Figure 6.8: Defining a cluster termination policy like this one is considered
    best practice and can help avoid unnecessary costs.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.8：定义一个类似于这样的集群终止策略被认为是最佳实践，并且可以帮助避免不必要的成本。
- en: The final required section for completion is the definition of the appropriate
    **Identity and Access Management** (**IAM**) roles, which defines what accounts
    can access the resources we are creating. If you already have IAM roles that you
    are happy to reuse as your EMR service role, then you can do this; for this example,
    however, let’s create a new service role specifically for this cluster. *Figure
    6.9* shows that selecting the option to create a new role pre-populates the VPC,
    subnet, and security group with values matching what you have already selected
    through this process. You can add more to these. *Figure 6.10* shows that we can
    also select to create an “instance profile”, which is just the name given to a
    service role that applies to all server instances in an EC2 cluster at launch.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_09.png)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成所需的最后一个部分是定义适当的**身份和访问管理**（**IAM**）角色，它定义了哪些账户可以访问我们正在创建的资源。如果你已经有了一些你愿意作为EMR服务角色重用的IAM角色，那么你可以这样做；然而，对于这个示例，让我们为这个集群创建一个新的服务角色。*图6.9*显示，选择创建新角色的选项会预先填充VPC、子网和安全组，其值与通过此过程已选择的值相匹配。你可以添加更多内容。*图6.10*显示，我们还可以选择创建一个“实例配置文件”，这只是一个在启动时应用于EC2集群中所有服务器实例的服务角色的名称。![计算机截图
    自动生成描述](img/B19525_06_09.png)
- en: 'Figure 6.9: Creating an AWS EMR service role.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.9：创建AWS EMR服务角色。
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_10.png)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机截图 自动生成描述](img/B19525_06_10.png)'
- en: 'Figure 6.10: Creating an instance profile for the EC2 servers being used in
    this EMR cluster. An instance profile is just the name for the service role that
    is assigned to all cluster EC2 instances at spin-up time.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.10：为在此EMR集群中使用的EC2服务器创建实例配置文件。实例配置文件只是分配给所有集群EC2实例在启动时的服务角色的名称。
- en: The sections discussed are all the mandatory ones required to create your cluster,
    but there are also some optional sections that I would like to briefly mention
    to guide your further exploration. There is the option to specify **steps**, which
    is where you can define shell scripts, JAR applications, or Spark applications
    to run in sequence. This means you can spin up your cluster with your applications
    ready to start processing data in the sequence you desire, rather than submitting
    jobs after the infrastructure deployment. There is a section on **Bootstrap actions**,
    which allows you to define custom installation or configuration steps that should
    run before any applications are installed or any data is processed on the EMR
    cluster. Cluster logs locations, tags, and some basic software considerations
    are also available for configuration. The final important point to mention is
    on the security configuration. *Figure 6.11* shows the options. Although we will
    deploy this cluster without specifying any EC2 key pair or security configuration,
    it is crucially important that you understand the security requirements and norms
    of your organization if you want to run this cluster in production. Please consult
    your security or networking teams to ensure this is all in line with expectations
    and requirements. For now, we can leave it blank and proceed to create the cluster.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_11.png)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 讨论的部分都是创建您的集群所必需的章节，但也有一些可选章节，我想简要提及以指导您进一步探索。这里有指定**步骤**的选项，您可以在其中定义要按顺序运行的shell脚本、JAR应用程序或Spark应用程序。这意味着您可以在基础设施部署后提交作业之前，启动集群并准备好应用程序以按您希望的顺序处理数据。还有一个关于**引导操作**的章节，它允许您定义在安装任何应用程序或处理EMR集群上的任何数据之前应运行的定制安装或配置步骤。集群日志位置、标签和一些基本软件考虑因素也适用于配置。最后要提到的重要一点是安全配置。*图6.11*显示了选项。尽管我们将不指定任何EC2密钥对或安全配置来部署此集群，但如果您想在生产中运行此集群，了解您组织的网络安全要求和规范至关重要。请咨询您的安全或网络团队以确保一切符合预期和要求。目前，我们可以将其留空，然后继续创建集群。![计算机屏幕截图  自动生成描述](img/B19525_06_11.png)
- en: 'Figure 6.11: The security configuration for the cluster shown here is optional
    but should be considered carefully if you aim to run the cluster in production.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11：此处显示的集群安全配置是可选的，但如果您打算在生产中运行集群，应仔细考虑。
- en: Now that we have selected all of the mandatory options, click the **Create cluster**
    button to launch. Upon successful creation, you should see a review page like
    that shown in *Figure 6.12*. And that’s it; you have now created your own Spark
    cluster in the cloud!![A screenshot of a computer  Description automatically generated](img/B19525_06_12.png)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经选择了所有必需的选项，点击**创建集群**按钮以启动。创建成功后，您应该会看到一个类似于*图6.12*所示的审查页面。就这样；现在您已经在云中创建了自己的Spark集群了！！![计算机屏幕截图  自动生成描述](img/B19525_06_12.png)
- en: 'Figure 6.12: EMR cluster creation review page shown upon successful launch.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12：成功启动后显示的EMR集群创建审查页面。
- en: 'After spinning up our EMR cluster, we want to be able to submit work to it.
    Here, we will adapt the example Spark ML pipeline we produced in *Chapter 3*,
    *From Model to Model Factory*, to analyze the banking dataset and submit this
    as a step to our newly created cluster. We will do this as a standalone single
    PySpark script that acts as the only step in our application, but it is easy to
    build on this to make far more complex applications:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动我们的EMR集群后，我们希望能够向其提交工作。在这里，我们将调整我们在*第3章*，*从模型到模型工厂*中生产的示例Spark ML管道，以分析银行数据集，并将其作为步骤提交到我们新创建的集群。我们将这样做为一个独立的单个PySpark脚本，作为我们应用程序的唯一步骤，但很容易在此基础上构建更复杂的应用程序：
- en: 'First, we will take the code from *Chapter 3*, *From Model to Model Factory*,
    and perform some nice refactoring based on our discussions around good practices.
    We can more effectively modularize the code so that it contains a function that
    provides all our modeling steps (not all of the steps have been reproduced here,
    for brevity). We have also included a final step that writes the results of the
    modeling to a `parquet` file:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从*第3章*，*从模型到模型工厂*中提取代码，并根据我们围绕良好实践的讨论进行一些精心的重构。我们可以更有效地模块化代码，使其包含一个提供所有建模步骤的功能（为了简洁，并非所有步骤都在此处重现）。我们还包括了一个最终步骤，将建模结果写入`parquet`文件：
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Building on this, we will wrap all of the main boilerplate code into a `main`
    function that can be called at the `if __name__=="__main__":` entry point to the
    program:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此基础上，我们将所有主要样板代码封装到一个名为`main`的函数中，该函数可以在程序的`if __name__=="__main__":`入口点被调用：
- en: '[PRE18]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We put the preceding functions into a script called `spark_example_emr.py`,
    which we will submit to our EMR cluster later:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将前面的函数放入一个名为`spark_example_emr.py`的脚本中，稍后我们将将其提交到我们的EMR集群：
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, to submit this script to the EMR cluster we have just created, we need
    to find out the cluster ID, which we can get from the AWS UI or by running the
    following command:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了将此脚本提交到我们刚刚创建的EMR集群，我们需要找到集群ID，我们可以从AWS UI或通过运行以下命令来获取：
- en: '[PRE20]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Then, we need to send the `spark_example_emr.py` script to S3 to be read in
    by the cluster. We can create an S3 bucket called `s3://mlewp-ch6-emr-examples`
    to store this and our other artifacts using either the CLI or the AWS console
    (see *Chapter 5*, *Deployment Patterns and Tools*). Once copied over, we are ready
    for the final step.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要将`spark_example_emr.py`脚本发送到S3，以便集群读取。我们可以创建一个名为`s3://mlewp-ch6-emr-examples`的S3存储桶来存储这个和其他工件，无论是使用CLI还是AWS控制台（参见*第五章*，*部署模式和工具*）。一旦复制完成，我们就为最后一步做好了准备。
- en: 'Now, we must submit the script using the following command, with `<CLUSTER_ID>`
    replaced with the ID of the cluster we just created. Note that if your cluster
    has been terminated due to the automated termination policy we set, you can’t
    restart it but you can clone it. After a few minutes, the step should have completed
    and the outputs should have been written to the `results.parquet` file in the
    same S3 bucket:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须使用以下命令提交脚本，用我们刚刚创建的集群ID替换`<CLUSTER_ID>`。请注意，如果你的集群由于我们设置的自动终止策略而终止，你无法重新启动它，但你可以克隆它。几分钟后，步骤应该已经完成，输出应该已经写入同一S3存储桶中的`results.parquet`文件：
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: And that is it – that is how we can start developing PySpark ML pipelines on
    the cloud using **AWS EMR**!
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就这样——这就是我们如何在云上使用**AWS EMR**开始开发PySpark ML管道的方法！
- en: You will see that this previous process has worked successfully by navigating
    to the appropriate S3 bucket and confirming that the `results.parquet` file was
    created succesfully; see *Figure 6.13*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，通过导航到适当的S3存储桶并确认`results.parquet`文件已成功创建，这个先前的过程已经成功；参见*图6.13*。
- en: '![](img/B19525_06_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B19525_06_13.png)'
- en: 'Figure 6.13: Successful creation of the results.parquet file upon submission
    of the EMR script.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：提交EMR脚本后成功创建results.parquet文件。
- en: In the next section, we will explore another method of scaling up our solutions
    by using so-called serverless tools.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用所谓的无服务器工具来扩展我们解决方案的另一种方法。
- en: Spinning up serverless infrastructure
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动无服务器基础设施
- en: Whenever we do any ML or software engineering, we have to run the requisite
    tasks and computations on computers, often with appropriate networking, security,
    and other protocols and software already in place, which we have often referred
    to already as constituting our *infrastructure*. A big part of our infrastructure
    is the servers we use to run the actual computations. This might seem a bit strange,
    so let’s start by talking about *serverless* infrastructure (how can there be
    such a thing?). This section will explain this concept and show you how to use
    it to scale out your ML solutions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时我们进行机器学习或软件工程，都必须在计算机上运行必要的任务和计算，通常伴随着适当的网络、安全和其它协议及软件，这些我们通常称之为构成我们的*基础设施*。我们基础设施的一个大组成部分是我们用来运行实际计算的服务器。这可能会显得有些奇怪，所以让我们先从*无服务器*基础设施（这怎么可能存在呢？）开始谈。本节将解释这个概念，并展示如何使用它来扩展你的机器学习解决方案。
- en: '**Serverless** is a bit misleading as a term as it does not mean that no physical
    servers are running your programs. It does mean, however, that the programs you
    are running should not be thought of as being statically hosted on one machine,
    but as ephemeral instances on another layer on top of the underlying hardware.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**无服务器**作为一个术语有点误导，因为它并不意味着没有物理服务器在运行你的程序。然而，它确实意味着你正在运行的程序不应被视为静态托管在一台机器上，而应被视为在底层硬件之上的另一层上的短暂实例。'
- en: 'The benefits of serverless tools for your ML solution include (but are not
    limited to) the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器工具对你的机器学习解决方案的好处包括（但不限于）以下内容：
- en: '**No servers**: Don’t underestimate the savings in time and energy you can
    get by offloading infrastructure management to your cloud provider.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器**：不要低估通过将基础设施管理外包给云服务提供商所能节省的时间和精力。'
- en: '**Simplified scaling**: It’s usually very easy to define the scaling behavior
    of your serverless components by using clearly defined maximum instances, for
    example.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化扩展**：通常，通过使用明确定义的最大实例等，很容易定义您无服务器组件的扩展行为。'
- en: '**Low barrier to entry**: These components are usually extremely easy to set
    up and run, allowing you and your team members to focus on writing high-quality
    code, logic, and models.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低门槛**：这些组件通常设置和运行起来非常简单，让您和您的团队成员能够专注于编写高质量的代码、逻辑和模型。'
- en: '**Natural integration points**: Serverless tools are often nice to use for
    handovers between other tools and components. Their ease of setup means you can
    be up and running with simple jobs that pass data or trigger other services in
    no time.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然集成点**：无服务器工具通常非常适合在与其他工具和组件之间进行交接。它们的易于设置意味着您可以在极短的时间内启动简单的作业，这些作业可以传递数据或触发其他服务。'
- en: '**Simplified serving**: Some serverless tools are excellent for providing a
    serving layer to your ML models. The scaling and low barrier to entry mentioned
    previously mean you can quickly create a very scalable service that provides predictions
    upon request or upon being triggered by some other event.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化服务**：一些无服务器工具非常适合为您的机器学习模型提供服务层。之前提到的可扩展性和低门槛意味着您可以快速创建一个非常可扩展的服务，该服务可以根据请求或由其他事件触发提供预测。'
- en: One of the best and most widely used examples of serverless functionality is
    **AWS Lambda**, which allows us to write programs in a variety of languages with
    a simple web browser interface or through our usual development tools, and then
    have them run completely independently of any infrastructure that’s been set up.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器功能中最好和最广泛使用的例子之一是 **AWS Lambda**，它允许我们通过简单的网页界面或通过我们常用的开发工具用各种语言编写程序，然后让它们在完全独立于任何已设置的基础设施的情况下运行。
- en: Lambda is an amazing low-entry-barrier solution to getting some code up and
    running and scaling it up. However, it is very much aimed at creating simple APIs
    that can be hit over an HTTP request. Deploying your ML model with Lambda is particularly
    useful if you are aiming for an event- or request-driven system.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 是一个惊人的低门槛解决方案，可以快速将一些代码部署并扩展。然而，它主要针对创建可以通过 HTTP 请求触发的简单 API。如果您旨在构建事件或请求驱动的系统，使用
    Lambda 部署您的机器学习模型特别有用。
- en: To see this in action, let’s build a basic system that takes incoming image
    data as an HTTP request with a JSON body and returns a similar message containing
    the classification of the data using a pre-built Scikit-Learn model. This walkthrough
    is based on the AWS example at [https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/](https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到这个功能在实际中的运用，让我们构建一个基本的系统，该系统接受带有 JSON 体的 HTTP 请求作为输入图像数据，并使用预构建的 Scikit-Learn
    模型返回包含数据分类的类似消息。这个教程基于 AWS 的示例，请参阅[https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/](https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/)。
- en: For this, we can save a lot of time by leveraging templates already built and
    maintained as part of the AWS **Serverless Application Model** (**SAM**) framework
    ([https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个，我们可以通过利用作为 AWS **无服务器应用程序模型**（**SAM**）框架的一部分已经构建和维护的模板来节省大量时间（[https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/)）。
- en: To install the AWS SAM CLI on your relevant platform, follow the instructions
    at [https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的相关平台上安装 AWS SAM CLI，请遵循[https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml)中的说明。
- en: 'Now, let’s perform the following steps to set up a template serverless deployment
    for hosting and serving a ML model that classifies images of handwritten digits:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行以下步骤来设置一个模板无服务器部署，用于托管和提供用于对手写数字图像进行分类的机器学习模型：
- en: 'First, we must run the `sam init` command and select the AWS `Quick Start Templates`
    option:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须运行 `sam init` 命令并选择 AWS 的 `Quick Start Templates` 选项：
- en: '[PRE22]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will then be offered a choice of `AWS Quick Start` Application templates
    to use; select option 15, `Machine Learning`:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你将获得选择使用`AWS Quick Start`应用程序模板的机会；选择选项15，`Machine Learning`：
- en: '[PRE23]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next are the options for the Python runtime you want to use; in line with the
    rest of the book, we will use the Python 3.10 runtime:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是你要使用的Python运行时的选项；与本书的其他部分一致，我们将使用Python 3.10运行时：
- en: '[PRE24]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At the time of writing, the SAM CLI will then auto-select some options based
    on these choices, first the package type and then the dependency manager. You
    will then be asked to confirm the ML starter template you want to use. For this
    example, select `XGBoost Machine Learning API`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在撰写本文时，SAM CLI将根据这些选择自动选择一些选项，首先是包类型，然后是依赖管理器。然后，你将被要求确认你想要使用的ML起始模板。对于这个示例，选择`XGBoost
    Machine Learning API`：
- en: '[PRE25]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The SAM CLI then helpfully asks about some options for configuring request
    tracing and monitoring; you can select yes or no depending on your own preferences.
    I have selected no for the purposes of this example. You can then give the solution
    a name; here I have gone with `mlewp-sam-ml-api`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SAM CLI随后会友好地询问一些配置请求跟踪和监控的选项；你可以根据自己的喜好选择是或否。在这个示例中，我选择了否。
- en: '[PRE26]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, your command line will provide some helpful information about the
    installation and next steps:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你的命令行将提供一些有关安装和下一步操作的有用信息：
- en: '[PRE27]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that the preceding steps have created a template for an XGBoost-based
    system that classifies handwritten digits. For other applications and project
    use cases, you will need to adapt the source code of the template as you require.
    If you want to deploy this example, follow the next few steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的步骤创建了一个基于XGBoost的系统模板，用于对手写数字进行分类。对于其他应用程序和项目用例，你需要根据需要调整模板的源代码。如果你想部署这个示例，请按照以下步骤操作：
- en: 'First, we must build the application container provided with the template.
    First, navigate to the top directory of your project, you can see the directory
    structure should be something like below. I have used the `tree` command to provide
    a clean outline of the directory structure in the command line:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须构建模板中提供的应用程序容器。首先，导航到你的项目的顶级目录，你应该能看到目录结构应该是这样的。我使用了`tree`命令在命令行中提供一个干净的目录结构概览：
- en: '[PRE28]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we are in the top directory, we can run the `build` command. This
    requires that Docker is running in the background on your machine:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们处于顶级目录，我们可以运行`build`命令。这要求你的机器在后台运行Docker：
- en: '[PRE29]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Upon a successful build, you should receive a success message similar to the
    following in your terminal:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在成功构建后，你应该在你的终端收到类似以下的成功消息：
- en: '[PRE30]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can test the service locally to ensure that everything is working well
    with the mock data that’s supplied with the repository. This uses a JSON file
    that encodes a basic image and runs the inference step for the service. If this
    has worked, you will see an output that looks something like the following for
    your service:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以在本地测试该服务，以确保一切都能与存储库中提供的模拟数据良好地工作。这使用了一个编码基本图像的JSON文件，并运行服务的推理步骤。如果一切顺利，你将看到类似以下输出的服务：
- en: '[PRE31]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In a real project, you would edit the source code for the solution in the `app.py`
    and other files as required before deploying up to the cloud. We will do this
    using the SAM CLI , with the understanding that if you want to automate this process,
    you can use the CI/CD processes and tools we discussed in several places in this
    book, especially in *Chapter 4*, *Packaging Up*. To deploy, you can use the guided
    deployment wizard with the CLI by running the `deploy` command, which will return
    the below output:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际项目中，你需要在部署到云之前，编辑解决方案的`app.py`和其他所需文件。我们将使用SAM CLI来完成这项工作，理解如果你想要自动化这个过程，你可以使用本书中讨论的CI/CD流程和工具，特别是在*第4章*，*打包*部分。要部署，你可以通过运行`deploy`命令来使用CLI的引导部署向导，这将返回以下输出：
- en: '[PRE32]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then have to configure the application for each of the provided elements.
    I have selected the defaults in most cases, but you can refer to the AWS documentation
    and make the choices most relevant to your project:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须为提供的每个元素配置应用程序。在大多数情况下，我选择了默认设置，但你也可以参考AWS文档，并根据你的项目做出最相关的选择：
- en: '[PRE33]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The previous step will generate a lot of data in the terminal; you can monitor
    this to see if there are any errors or issues. If the deployment was successful,
    then you should see some final metadata about the application that looks like
    this:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一个步骤将在终端生成大量数据；你可以监控这些数据以查看是否有任何错误或问题。如果部署成功，那么你应该会看到一些关于应用程序的最终元数据，如下所示：
- en: '[PRE34]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As a quick test to confirm the cloud-hosted solution is working, we can use
    a tool such as Postman to hit our shiny new ML API. Simply copy the `InferenceApi`
    URL from the output screen from *step 8* as the destination for the request, select
    **POST** for the request type, and then choose **binary** as the body type. Note
    that if you need to get the inference URL, again you can run the `sam list endpoints
    --output json` command in your terminal. Then, you can choose an image of a handwritten
    digit, or any other image for that matter, to send up to the API . You can do
    this in Postman either by selecting the **binary** body option and attaching an
    image file or you can copy in the encoded string of an image. In *Figure 6.14*,
    I have used the encoded string for the `body` key-value pair in the `events/event.json`
    file we used to test the function locally:![A screenshot of a computer  Description
    automatically generated](img/B19525_06_14.png)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了快速测试确认云托管解决方案是否正常工作，我们可以使用Postman等工具来调用我们闪亮的新ML API。只需将*步骤8*输出屏幕中的`InferenceApi`
    URL复制为请求的目的地，选择**POST**作为请求类型，然后选择**二进制**作为主体类型。注意，如果你需要获取推理URL，你还可以在终端中运行`sam
    list endpoints --output json`命令。然后，你可以选择一个手写数字的图像，或者任何其他图像，发送到API。你可以在Postman中通过选择**二进制**主体选项并附加图像文件，或者复制图像的编码字符串。在*图6.14*中，我使用了`events/event.json`文件中`body`键值对的编码字符串，这是我们用来本地测试函数的：![计算机屏幕截图  自动生成的描述](img/B19525_06_14.png)
- en: 'Figure 6.14: Calling our serverless ML endpoint with Postman. This uses an
    encoded example image as the body of the request that is provided with the SAM
    XGBoost ML API template.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.14：使用Postman调用我们的无服务器ML端点。这使用了一个编码的示例图像作为请求的主体，该请求与SAM XGBoost ML API模板一起提供。
- en: 'You can also test this more programmatically with a `curl` command like the
    following – just replace the encoded binary string of the image with the appropriate
    values, or indeed edit the command to point to a data binary if you wish, and
    you are good to go:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也可以使用以下`curl`命令以更程序化的方式测试这个服务——只需将图像的编码二进制字符串替换为适当的值，或者实际上编辑命令以指向数据二进制文件，如果你愿意，就可以开始了：
- en: '[PRE35]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In this step and *step 9*, the body of the response from the Lambda function
    is as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个步骤和*步骤9*中，Lambda函数的响应主体如下：
- en: '[PRE36]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And that’s it – we have just built and deployed a simple serverless ML inference
    service on AWS!
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就这样——我们已经在AWS上构建和部署了一个简单的无服务器ML推理服务！
- en: In the next section, we will touch upon the final way of scaling our solutions
    that we will discuss in this chapter, which is using Kubernetes (K8s) and Kubeflow
    to horizontally scale containerized applications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将简要介绍本章中将要讨论的最终扩展解决方案，即使用Kubernetes（K8s）和Kubeflow来水平扩展容器化应用程序。
- en: Containerizing at scale with Kubernetes
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kubernetes进行大规模容器化
- en: We have already covered how to use containers for building and deploying our
    ML solutions. The next step is understanding how to orchestrate and manage several
    containers to deploy and run applications at scale. This is where the open source
    tool **Kubernetes** (**K8s**)comes in.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了如何使用容器来构建和部署我们的ML解决方案。下一步是了解如何编排和管理多个容器以大规模部署和运行应用程序。这就是开源工具**Kubernetes**（**K8s**）发挥作用的地方。
- en: 'K8s is an extremely powerful tool that provides a variety of different functionalities
    that help us create and manage very scalable containerized applications, including
    (but not limited to) the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: K8s是一个非常强大的工具，它提供了各种不同的功能，帮助我们创建和管理非常可扩展的容器化应用程序，包括但不限于以下内容：
- en: '**Load Balancing**: K8s will manage routing incoming traffic to your containers
    for you so that the load is split evenly.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：K8s将为你管理路由到你的容器的入站流量，以确保负载均匀分配。'
- en: '**Horizontal Scaling**: K8s provides simple interfaces so that you can control
    the number of container instances you have at any one time, allowing you to scale
    massively if needed.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水平扩展**：K8s提供了简单的接口，让你可以控制任何时刻拥有的容器实例数量，如果需要，可以大规模扩展。'
- en: '**Self Healing**: There is built-in management for replacing or rescheduling
    components that are not passing health checks.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自我修复**：有内置的管理来替换或重新安排未通过健康检查的组件。'
- en: '**Automated Rollbacks**: K8s stores the history of your system so that you
    can revert to a previous working version if something goes wrong.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动回滚**：K8s存储了你的系统历史，以便在出现问题时可以回滚到先前的有效版本。'
- en: All of these features help ensure that your deployed solutions are robust and
    able to perform as required under all circumstances.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能都有助于确保你的部署解决方案是健壮的，并且能够在所有情况下按要求执行。
- en: K8s is designed to ensure the preceding features are embedded from the ground
    up by using a microservice architecture, with a control plane interacting with
    nodes (servers), each of which host pods (one or more containers) that run the
    components of your application.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: K8s的设计是通过使用微服务架构，并使用控制平面与节点（服务器）交互，每个节点都托管运行应用程序组件的pods（一个或多个容器）来确保上述功能从底层开始嵌入。
- en: 'The key thing that K8s gives you is the ability to scale your application based
    on load by creating replicas of the base solution. This is extremely useful if
    you are building services with API endpoints that could feasibly face surges in
    demand at different times. To learn about some of the ways you can do this, see
    [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: K8s提供的关键功能是，通过创建基础解决方案的副本来根据负载扩展应用程序。如果你正在构建具有API端点的服务，这些端点在不同时间可能会面临需求激增，这将非常有用。了解你可以这样做的一些方法，请参阅[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment)：
- en: '![Figure 6.19 – The K8s architecture ](img/B19525_06_15.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – K8s架构](img/B19525_06_15.png)'
- en: 'Figure 6.15: The K8s architecture.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15：K8s架构。
- en: 'But what about ML? In this case, we can look to a newer piece of the K8s ecosystem:
    **Kubeflow**, which we learned how to use in *Chapter 5*, *Deployment Patterns
    and Tools*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 但关于机器学习（ML）呢？在这种情况下，我们可以看看K8s生态系统中的新成员：**Kubeflow**，我们在第5章“部署模式和工具”中学习了如何使用它。
- en: Kubeflow styles itself as the *ML toolkit for K8s* ([https://www.kubeflow.org/](https://www.kubeflow.org/)),
    so as ML engineers, it makes sense for us to be aware of this rapidly developing
    solution. This is a very exciting tool and an active area of development.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow将自己定位为**K8s的ML工具包**([https://www.kubeflow.org/](https://www.kubeflow.org/))，因此作为机器学习工程师，了解这个快速发展的解决方案是有意义的。这是一个非常激动人心的工具，也是一个活跃的开发领域。
- en: The concept of horizontal scaling for K8s generally still applies here, but
    Kubeflow provides some standardized tools for converting the pipelines you build
    into standard K8s resources, which can then be managed and resourced in the ways
    described previously. This can help reduce *boilerplate* and lets us, as ML engineers,
    focus on building our modelling logic rather than setting up the infrastructure.
    We leveraged this when we built some example pipelines in *Chapter 5*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于K8s的水平扩展概念通常仍然适用，但Kubeflow提供了一些标准化的工具，可以将你构建的流水线转换为标准的K8s资源，然后可以按照之前描述的方式管理和分配资源。这有助于减少模板代码，并让我们作为机器学习工程师专注于构建我们的建模逻辑，而不是设置基础设施。我们在第5章构建示例流水线时利用了这一点。
- en: We will explore Kubernetes in far more detail in *Chapter 8*, *Building an Example
    ML Microservice*, where we use it to scale out our own wrapped ML model in a REST
    API. This will complement nicely the work we have done in this chapter on higher-level
    abstractions that can be used for scaling out, especially in the *Spinning up
    serverless infrastructure* section. We will only touch on K8s and Kubeflow very
    briefly here, to make sure you are aware of these tools for your exploration.
    For more details on K8s and Kubeflow, consult the documentation. I would also
    recommend another Packt title called *Kubernetes in Production Best Practices*
    by *Aly Saleh* and *Murat Karslioglu*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8章“构建示例ML微服务”中更详细地探讨Kubernetes，我们将使用它来扩展我们自己的封装ML模型在REST API中。这将很好地补充本章中关于可以用于扩展的高级抽象的工作，特别是在“启动无服务器基础设施”部分。我们在这里只会简要提及K8s和Kubeflow，以确保你了解这些工具以供探索。有关K8s和Kubeflow的更多详细信息，请参阅文档。我还推荐另一本Packt出版的书籍，名为*Aly
    Saleh*和*Murat Karslioglu*的《Kubernetes in Production Best Practices》。
- en: Now, we will move on and discuss another very powerful toolkit for scaling out
    compute-intensive Python workloads, which has now become extremely popular across
    the ML engineering community and been used by organizations such as Uber, Amazon
    and even used by OpenAI for training their large language **Generative Pre-trained
    Transformer** (**GPT**) models, which we discuss at length in *Chapter 7*, *Deep
    Learning, Generative AI, and LLMOps*. Let’s meet **Ray**.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论另一个非常强大的用于扩展计算密集型Python工作负载的工具包，它现在在机器学习工程社区中变得极为流行，并被Uber、Amazon等组织以及OpenAI用于训练其大型语言**生成预训练变换器**（**GPT**）模型，我们将在第7章*深度学习、生成式AI和LLMOps*中详细讨论。让我们来认识**Ray**。
- en: Scaling with Ray
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ray进行扩展
- en: '**Ray** is a Python native distributed computing framework that was specifically
    designed to help ML engineers meet the needs of massive data and massively scalable
    ML systems. Ray has an ethos of making scalable compute available to every ML
    developer, and in doing this in a way such that you can run anywhere by abstracting
    out all interactions with underlying infrastructure. One of the unique features
    of Ray that is particularly interesting is that it has a distributed scheduler,
    rather than a scheduler or DAG creation mechanism that runs in a central process,
    like in Spark. At its core, Ray has been developed with compute-intensive tasks
    such as ML model training in mind from the beginning, which is slightly different
    from Apache Spark, which has data intensity in mind. You can therefore think about
    this in a simplified manner: if you need to process lots of data a couple of times,
    Spark; if you need to process one piece of data lots of times, Ray. This is just
    a heuristic so should not be followed strictly, but hopefully it gives you a helpful
    rule of thumb. As an example, if you need to transform millions and millions of
    rows of data in a large batch process, then it makes sense to use Spark, but if
    you want to train an ML model on the same data, including hyperparameter tuning,
    then Ray may make a lot of sense.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ray** 是一个专为帮助机器学习工程师满足大规模数据和大规模可扩展机器学习系统需求而设计的Python原生分布式计算框架。Ray有一个使可扩展计算对每个机器学习开发者都可用，并且以抽象出与底层基础设施的所有交互的方式来运行在任何地方的理念。Ray的独特特性之一是它有一个分布式调度器，而不是像Spark那样在中央进程中运行的调度器或DAG创建机制。从其核心来看，Ray从一开始就考虑了计算密集型任务，如机器学习模型训练，这与以数据密集型为目标的Apache
    Spark略有不同。因此，你可以这样简单地思考：如果你需要多次处理大量数据，那么选择Spark；如果你需要多次处理同一份数据，那么Ray可能更合适。这只是一个经验法则，不应严格遵循，但希望它能给你一个有用的指导原则。例如，如果你需要在大型批量处理中转换数百万行数据，那么使用Spark是有意义的，但如果你想在同一数据上训练机器学习模型，包括超参数调整，那么Ray可能更有意义。'
- en: 'The two tools can be used together quite effectively, with Spark transforming
    the feature set before feeding this into a Ray workload for ML training. This
    is taken care of in particular by the **Ray AI Runtime** (**AIR**), which has
    a series of different libraries to help scale different pieces of an ML solution.
    These include:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个工具可以非常有效地一起使用，Spark在将特征集转换后，将其输入到用于机器学习训练的Ray工作负载中。这特别由**Ray AI Runtime**（**AIR**）负责，它提供了一系列不同的库来帮助扩展机器学习解决方案的不同部分。这些包括：
- en: '**Ray Data**: Focused on providing data pre-processing and transformation primitives.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Data**: 专注于提供数据预处理和转换原语。'
- en: '**Ray Train**: Facilitates large model training.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Train**: 促进大型模型训练。'
- en: '**Ray Tune**: Helps with scalable hyperparameter training.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Tune**: 帮助进行可扩展的超参数训练。'
- en: '**Ray RLib**: Supports methods for the development of reinforcement learning
    models.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray RLib**: 支持强化学习模型开发的方法。'
- en: '**Ray Batch Predictor**: For batch inference.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Batch Predictor**: 用于批量推理。'
- en: '**Ray Serving**: For re al-time inference.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Serving**: 用于实时推理。'
- en: The AIR framework provides a unified API through which to interact with all
    of these capabilities and nicely integrates with a huge amount of the standard
    ML ecosystem that you will be used to, and that we have leveraged in this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: AIR框架提供了一个统一的API，通过它可以与所有这些功能进行交互，并且很好地集成了你将习惯使用的以及我们在本书中利用的大量标准机器学习生态系统。
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_16.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B19525_06_16.png)'
- en: 'Figure 6.16: The Ray AI runtime, from a presentation by Jules Damji from Anyscale:
    https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：来自Anyscale的Jules Damji的演示中的Ray AI运行时，来自：https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf。经许可复制。
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_17.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B19525_06_17.png)'
- en: 'Figure 6.17: The Ray architecture including the Raylet scheduler. From a presentation
    by Jules Damji: https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：包括Raylet调度器的Ray架构。来自Jules Damji的演示：https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf。经许可复制。
- en: 'The Ray Core API has a series of different objects that you leverage when using
    Ray in order to distribute your solution. The first is tasks, which are asynchronous
    items of work for the system to perform. To define a task, you can take a Python
    function like:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Ray核心API有一系列不同的对象，当你在使用Ray时可以利用这些对象来分发你的解决方案。首先是任务，这是系统要执行的工作的异步项。为了定义一个任务，你可以使用一个Python函数，例如：
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And then add the `@remote` decorator and then use the `.remote()` syntax in
    order to submit this task to the cluster. This is not a blocking function so will
    just return an ID that Ray uses to refer to the task in later computation steps
    ([https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832](https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832))
    :'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后添加`@remote`装饰器，然后使用`.remote()`语法来将此任务提交到集群。这不是一个阻塞函数，所以它将只返回一个ID，Ray将使用该ID在后续的计算步骤中引用任务（[https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832](https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832)）：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the same vein, the Ray API can extend the same concepts to classes as well;
    in this case, these are called `Actors`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Ray API可以将相同的概念扩展到类中；在这种情况下，这些被称为`Actors`：
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, Ray also has a distributed immutable object store. This is a smart
    way to have one shared data store across all the nodes of the cluster without
    shifting lots of data around and using up bandwidth. You can write to the object
    store with the following syntax:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Ray还有一个分布式不可变对象存储。这是一种智能的方法，可以在集群的所有节点之间共享一个数据存储，而不需要移动大量数据并消耗带宽。你可以使用以下语法向对象存储写入：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: IMPORTANT NOTE
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: An Actor in this context is a service or stateful worker, a concept used in
    other distributed frameworks like Akka, which runs on the JVM and has bindings
    to Java and Scala.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个语境中，一个Actor是一个服务或具有状态的工作者，这是一个在其他分布式框架（如Akka）中使用的概念，它运行在JVM上，并且有Java和Scala的绑定。
- en: Getting started with Ray for ML
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray用于机器学习的入门
- en: 'To get started you can install Ray with AI Runtime, as well as some the hyperparameter
    optimization package, the central dashboard and a Ray enhanced XGBoost implementation,
    by running:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，你可以通过运行以下命令安装带有AI运行时的Ray，以及一些超参数优化包、中央仪表板和Ray增强的XGBoost实现：
- en: '[PRE41]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: IMPORTANT NOTE
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: a reminder here that whenver you see `pip install` in this book, you can also
    use Poetry as outlined in *Chapter 4*, *Packaging Up*. So, in this case, you would
    have the following commands after running `poetry new` `project_name:`
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提醒一下，当你在这本书中看到`pip install`时，你还可以使用在第4章“打包”中概述的Poetry。因此，在这种情况下，在运行`poetry
    new` `project_name:`之后，你会得到以下命令：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s start by looking at Ray Train, which provides an API to a series of `Trainer`
    objects that helps facilitate distributed training. At the time of writing, Ray
    2.3.0 supports trainers across a variety of different frameworks including:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看Ray Train开始，它提供了一系列`Trainer`对象的API，有助于简化分布式训练。在撰写本文时，Ray 2.3.0支持跨各种不同框架的培训师，包括：
- en: '**Deep learning**: Horovod, Tensorflow and PyTorch.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习**：Horovod、Tensorflow和PyTorch。'
- en: '**Tree based**: LightGBM and XGBoost.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于树的**：LightGBM和XGBoost。'
- en: '**Other**: Scikit-learn, HuggingFace, and Ray’s reinforcement learning library
    RLlib.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他**：Scikit-learn、HuggingFace和Ray的强化学习库RLlib。'
- en: '![A diagram of a company  Description automatically generated](img/B19525_06_18.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![公司图表  自动生成的描述](img/B19525_06_18.png)'
- en: 'Figure 6.18: Ray Trainers as shown in the Ray docs at https://docs.ray.io/en/latest/train/train.xhtml.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18：如Ray文档中所示（https://docs.ray.io/en/latest/train/train.xhtml）的Ray训练器。
- en: 'We will first look at a tree-based learner example using XGBoost. Open up a
    script and begin adding to it; in the repo, this is called `getting_started_with_ray.py`.
    What follows is based on an introductory example given in the Ray documentation.
    First, we can use Ray to download one of the standard datasets; we could also
    have used `sklearn.datasets` or another source if we wanted to, like we have done
    elsewhere in the book:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将查看一个基于树的XGBoost学习器示例。打开一个脚本并开始向其中添加内容；在仓库中，这个脚本被称为`getting_started_with_ray.py`。以下内容基于Ray文档中给出的一个入门示例。首先，我们可以使用Ray下载标准数据集之一；如果我们想的话，我们也可以使用`sklearn.datasets`或其他来源，就像我们在本书的其他地方所做的那样：
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that here we use the `ray.data.read_csv()` method, which returns a `PyArrow`
    dataset. The Ray API has methods for reading from other data formats as well such
    as JSON or Parquet, as well as from databases like MongoDB or your own custom
    data sources.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里我们使用`ray.data.read_csv()`方法，该方法返回一个`PyArrow`数据集。Ray API有从其他数据格式读取的方法，例如JSON或Parquet，以及从数据库如MongoDB或您自己的自定义数据源读取。
- en: 'Next, we will define a preprocessing step that will standardize the features
    we want to use; for more information on feature engineering, you can check out
    *Chapter 3*, *From Model to Model Factory*:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个预处理步骤，该步骤将标准化我们想要使用的特征；有关特征工程的信息，您可以查看*第3章*，*从模型到模型工厂*：
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then is the fun part where we define the `Trainer` object for the XGBoost model.
    This has several different parameters and inputs we will need to define shortly:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是定义XGBoost模型的`Trainer`对象的有趣部分。这有几个不同的参数和输入，我们将在稍后定义：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You’ll then see something like that shown in *Figure 6.19* as output if you
    run this code in a Jupyter notebook or Python script.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Jupyter笔记本或Python脚本中运行此代码，您将看到类似于*图6.19*所示的输出。
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_19.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](img/B19525_06_19.png)'
- en: 'Figure 6.19: Outptut from parallel training of an XGBoost model using Ray.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19：使用Ray并行训练XGBoost模型的输出。
- en: 'The `result` object contains tons of useful information; one of the attributes
    of it is called `metrics` and you can print this to reveal details about the end
    state of the run. Execute `print(result.metrics)` and you will see something like
    the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`result`对象包含大量有用的信息；它的一个属性称为`metrics`，您可以打印出来以揭示运行结束状态的相关细节。执行`print(result.metrics)`，您将看到如下内容：'
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In the instantiation of the `XGBoostTrainer`, we defined some important scaling
    information that was omitted in the previous example; here it is:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在`XGBoostTrainer`的实例化中，我们定义了一些在先前的示例中省略的重要缩放信息；如下所示：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `num_workers` parameter tells Ray how many actors to launch, with each actor
    by default getting one CPU. The `use_gpu` flag is set to false since we are not
    using GPU acceleration here. Finally, by setting the `_max_cpu_fraction_per_node`
    parameter to `0.9` we have left some spare capacity on each CPU, which can be
    used for other operations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_workers`参数告诉Ray启动多少个actor，默认情况下每个actor分配一个CPU。由于我们在这里没有使用GPU加速，所以将`use_gpu`标志设置为false。最后，通过将`_max_cpu_fraction_per_node`参数设置为`0.9`，我们在每个CPU上留下了一些备用容量，这些容量可以用于其他操作。'
- en: 'In the previous example, there were also some XGBoost specific parameters we
    supplied:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们还提供了一些XGBoost特定的参数：
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If you wanted to use GPU acceleration for the XGBoost training you would add
    `tree_method`: `gpu_hist` as a key-value pair in this `params` dictionary.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您想为XGBoost训练使用GPU加速，您可以在`params`字典中添加一个键值对`tree_method`: `gpu_hist`。'
- en: '![A line graph with blue and orange lines  Description automatically generated](img/B19525_06_20.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![带有蓝色和橙色线条的折线图  自动生成的描述](img/B19525_06_20.png)'
- en: 'Figure 6.20: A few experiments show how changing the number of workers and
    CPUs available per worker results in different XGBoost training times on the author’s
    laptop (an 8 core Macbook Pro).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20：几个实验展示了在作者的笔记本电脑（一台8核心的Macbook Pro）上，改变每个worker可用的worker数量和CPU数量如何导致XGBoost训练时间不同。
- en: We will now discuss briefly how you can scale compute with Ray when working
    in environments other than your local machine.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将简要讨论如何在除本地机器以外的环境中使用Ray扩展计算。
- en: Scaling your compute for Ray
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为Ray扩展计算能力
- en: 'The examples we’ve seen so far use a local Ray cluster that is automatically
    set up on the first call to the Ray API. This local cluster grabs all the available
    CPUs on your machine and makes them available to execute work. Obviously, this
    will only get you so far. The next stage is to work with clusters that can scale
    to far larger numbers of available workers in order to get more speedup. You have
    a few options if you want to do this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止看到的示例使用的是本地 Ray 集群，该集群在第一次调用 Ray API 时自动设置。这个本地集群抓取您机器上所有可用的 CPU 并使其可用于执行工作。显然，这只能让您走这么远。下一个阶段是与可以扩展到更多可用工作者的集群一起工作，以获得更多的加速。如果您想这样做，您有几个选择：
- en: '**On the cloud**: Ray provides the ability to deploy on to Google Cloud Platform
    and AWS resources, with Azure deployments handled by a community maintained solution.
    For more information on deploying and running Ray on AWS, you can check out its
    online documentation.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在云上**：Ray 提供了部署到 Google Cloud Platform 和 AWS 资源的能力，Azure 部署由社区维护的解决方案处理。有关在
    AWS 上部署和运行 Ray 的更多信息，您可以查看其在线文档。'
- en: '**Using Kubernetes**: We have already met Kubeflow in *Chapter 5*, *Deployment
    Patterns and Tools*, which is used to build Kubernetes enabled ML pipelines. And
    we have also discussed Kubernetes in the Containerizing at Scale with Kubernetes
    section in this chapter.. As mentioned there, Kubernetes is a container orchestration
    toolkit designed to create massively scalable solutions based on containers. If
    you want to work with Ray on Kubernetes, you can use the **KubeRay** project,
    [https://ray-project.github.io/kuberay/](https://ray-project.github.io/kuberay/).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Kubernetes**：我们在 *第五章*，*部署模式和工具* 中已经遇到了 Kubeflow，它用于构建支持 Kubernetes 的
    ML 管道。在本章的“在规模上容器化 Kubernetes”部分中，我们也讨论了 Kubernetes。如前所述，Kubernetes 是一个容器编排工具包，旨在基于容器创建可大规模扩展的解决方案。如果您想在
    Kubernetes 上使用 Ray，可以使用 **KubeRay** 项目，[https://ray-project.github.io/kuberay/](https://ray-project.github.io/kuberay/)。'
- en: 'The setup of Ray on either the cloud or Kubernetes mainly involves defining
    the cluster configuration and its scaling behaviour. Once you have done this,
    the beauty of Ray is that scaling your solution is as simple as editing the `ScalingConfig`
    object we used in the previous example, and you can keep all your other code the
    same. So, for example, if you have a 20-node CPU cluster, you could simply change
    the definition to the following and run it as before:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在云或 Kubernetes 上设置 Ray 主要涉及定义集群配置及其扩展行为。一旦完成这些操作，Ray 的美妙之处在于扩展您的解决方案就像编辑我们在上一个示例中使用的
    `ScalingConfig` 对象一样简单，并且您可以保持所有其他代码不变。例如，如果您有一个 20 节点的 CPU 集群，您可以简单地将其定义更改为以下内容，并像以前一样运行：
- en: '[PRE49]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Scaling your serving layer with Ray
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ray 扩展您的服务层
- en: We have discussed the ways you can use Ray to distributed ML training jobs but
    now let’s have a look at how you can use Ray to help you scale your application
    layer. As mentioned before, Ray AIR provides some nice functionality for this
    that is badged under **Ray Serve**.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了您可以使用 Ray 来使用分布式 ML 训练作业的方法，但现在让我们看看您如何使用 Ray 来帮助您扩展应用程序层。如前所述，Ray AIR
    提供了一些在 **Ray Serve** 下的良好功能。
- en: Ray Serve is a framework-agnostic library that helps you easily define ML endpoints
    based on your models. Like with the rest of the Ray API that we have interacted
    with, it has been built to provide easy interoperability and access to scaling
    without large development overheads.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Serve 是一个框架无关的库，它帮助您轻松地根据您的模型定义 ML 端点。就像我们与之交互的 Ray API 的其余部分一样，它被构建为提供易于互操作性和访问扩展，而无需大量的开发开销。
- en: Building on the examples from the previous few sections, let us assume we have
    trained a model, stored it in our appropriate registry, such as MLflow, and we
    have retrieved this model and have it in memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前几节提供的示例，让我们假设我们已经训练了一个模型，并将其存储在我们的适当注册表中，例如 MLflow，并且我们已经检索了这个模型并将其保存在内存中。
- en: 'In Ray Serve, we create **deployments** by using the `@ray.serve.deployments`
    decorator. These contain the logic we wish to use to process incoming API requests,
    including through any ML models we have built. As an example, let’s build a simple
    wrapper class that uses an XGBoost model like the one we worked with in the previous
    example to make a prediction based on some pre-processed feature data that comes
    in via the request object. First, the Ray documentation encourages the use of
    the Starlette requests library:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ray Serve 中，我们通过使用 `@ray.serve.deployments` 装饰器来创建**部署**。这些部署包含我们希望用于处理传入
    API 请求的逻辑，包括通过我们构建的任何机器学习模型。例如，让我们构建一个简单的包装类，它使用与上一个示例中我们使用的类似的 XGBoost 模型，根据通过请求对象传入的一些预处理特征数据来进行预测。首先，Ray
    文档鼓励使用 Starlette 请求库：
- en: '[PRE50]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next we can define the simple class and use the `serve` decorator to define
    the service. I will assume that logic for pulling from MLflow or any other model
    storage location is wrapped into the utility function `get_model` in the following
    code block:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义一个简单的类，并使用 `serve` 装饰器来定义服务。我将假设从 MLflow 或任何其他模型存储位置提取的逻辑被封装在以下代码块中的实用函数
    `get_model` 中：
- en: '[PRE51]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can then deploy this across an existing Ray cluster.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以将此部署到现有的 Ray 集群中。
- en: This concludes our introduction to Ray. We will now finish with a final discussion
    on *designing systems at scale* and then a summary of everything we have learned.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 Ray 的介绍。我们现在将结束于对*设计大规模系统*的最终讨论，然后是对我们所学到的一切的总结。
- en: Designing systems at scale
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计大规模系统
- en: To build on the ideas presented in *Chapter 5*, *Deployment Patterns and Tools,*
    and in this chapter, we should now consider some of the ways in which the scaling
    capabilities we have discussed can be employed to maximum effect in your ML engineering
    projects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在*第 5 章*，“部署模式和工具”以及本章中提出的思想的基础上进行扩展，我们现在应该考虑一些方法，这些方法可以让我们在机器学习工程项目中最大限度地发挥我们讨论过的扩展能力。
- en: The whole idea of scaling should be thought of in terms of providing an increase
    in the throughput of analyses or inferences or ultimate size of data that can
    be processed. There is no real difference in the kind of analyses or solution
    you can develop, at least in most cases. This means that applying scaling tools
    and techniques successfully is more dependent on selecting the correct processes
    that will benefit from them, even when we include any overheads that come from
    using these tools. That is what we will discuss now in this section, so that you
    have a few guiding principles to revisit when it comes to making your own scaling
    decisions.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展的整体思想应该从提供分析或推理吞吐量的增加或可以处理的数据的最终大小增加的角度来考虑。在大多数情况下，您可以开发的分析或解决方案的类型没有真正的区别。这意味着成功应用扩展工具和技术更多地取决于选择将从中受益的正确流程，即使包括使用这些工具带来的任何开销。这就是我们现在在本节中要讨论的，以便您在做出自己的扩展决策时有一些指导原则。
- en: 'As discussed in several places throughout this book, the pipelines you develop
    for your ML projects will usually have to have stages that cover the following
    tasks:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如本书中多处所述，您为机器学习项目开发的管道通常需要包含以下任务的一些阶段：
- en: Ingestion/pre-processing
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取/预处理
- en: Feature engineering (if different from above)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程（如果与上述不同）
- en: Model training
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Model inference
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型推理
- en: Application layer
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用层
- en: Parallelization or distribution can help in many of these steps but usually
    in some different ways. For ingestion/pre-processing, if you are operating in
    a large scheduled batch setting, then the ability to scale to larger datasets
    in a distrubuted manner is going to be of huge benefit. In this case, the use
    of Apache Spark will make sense. For feature engineering, similarly they main
    bottleneck is in processing large amounts of data once as we perform the transformations,
    so again Spark is useful for this. The compute-intensive steps for training ML
    models that we discussed in detail in *Chapter 3*, *From Model to Model Factory*,
    are very amenable to frameworks that are optimized for this intensive computation,
    irrespective of the data size. This is where Ray comes in as discussed in the
    previous sections. Ray will mean that you can also neatly parallelize your hyperparameter
    tuning if you need to do that too. Note that you could run these steps in Spark
    as well but Ray’s low task overheads and its distributed state management mean
    that it is particularly amenable to splitting up these compute-intensive tasks.
    Spark on the other hand has centralized state and schedule management. Finally,
    when it comes to the inference and application layers, where we produce and surface
    the results of the ML model, we need to think about the requirements for the specific
    use case. As an example, if you want to serve your model as a REST API endpoint,
    we showed in the previous section how Ray’s distribution model and API can help
    facilitate this very easily, but this would not make sense to do in Spark. If,
    however, the model results are to be produced in large batches, then Spark or
    Ray may make sense. Also, as alluded to in the feature engineering and ingestion
    steps, if the end result should be transformed in large batches as well, perhaps
    into a specific data model such as a star schema, then performing that transformation
    in Spark may again make sense due to the data scale requirements of this task.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化或分布式处理可以在许多步骤中提供帮助，但通常以不同的方式。对于摄取/预处理，如果你在一个大型的预定批量设置中操作，那么以分布式方式扩展到更大的数据集将带来巨大的好处。在这种情况下，使用Apache
    Spark是有意义的。对于特征工程，同样，主要瓶颈在于我们执行转换时一次性处理大量数据，因此Spark对于这一点也是有用的。我们在*第3章*，“从模型到模型工厂”中详细讨论的训练机器学习模型的计算密集型步骤，非常适合用于这种密集计算的框架，无论数据大小如何。这就是Ray在前面章节中发挥作用的地方。Ray意味着你也可以整洁地并行化你的超参数调整，如果你也需要这样做的话。请注意，你可以在Spark中运行这些步骤，但Ray的低任务开销和其分布式状态管理意味着它特别适合分割这些计算密集型任务。另一方面，Spark具有集中的状态和调度管理。最后，当涉及到推理和应用层，即我们产生和展示机器学习模型结果的地方，我们需要考虑特定用例的需求。例如，如果你想将你的模型作为REST
    API端点提供服务，我们在上一节中展示了Ray的分布式模型和API如何帮助非常容易地实现这一点，但在Spark中这样做是没有意义的。然而，如果模型结果需要以大量批次的形式生成，那么Spark或Ray可能是合适的。此外，正如在特征工程和摄取步骤中提到的，如果最终结果也需要在大批量中进行转换，例如转换成特定的数据模型，如星型模式，那么由于这个任务的数据规模要求，在Spark中执行这种转换可能是合理的。
- en: Let’s make this a bit more concrete by considering a potential example taken
    from industry. Many organizations with a retail element will analyze transactions
    and customer data in order to determine whether the customer is likely to churn.
    Let’s explore some of the decisions we can make to design and develop this solution
    with a particular focus on the questions of scaling up using the tools and techniques
    we have covered in this chapter.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过考虑一个来自行业的潜在示例来使这个问题更加具体。许多具有零售元素的机构将分析交易和客户数据，以确定客户是否可能流失。让我们探讨一些我们可以做出的决策，以设计和开发这个解决方案，特别关注使用我们在本章中介绍的工具和技术进行扩展的问题。
- en: First, we have the ingestion of the data. For this scenario, we will assume
    that the customer data, including interactions with different applications and
    systems, is processed at the end of the business day and numbers millions of records.
    This data contains numerical and categorical values and these need to be processed
    in order to feed into the downstream ML algorithm. If the data is partitioned
    by date, and maybe some other feature of the data, then this plays really naturally
    into the use of Spark, as you can read this into a Spark DataFrame and use the
    partitions to parallelize the data processing steps.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有数据摄取。对于这种情况，我们将假设客户数据，包括与不同应用程序和系统的交互，在业务日结束时处理，数量达到数百万条记录。这些数据包含数值和分类值，并且需要经过处理才能输入到下游机器学习算法中。如果数据按日期分区，或者数据的一些其他特征，那么这非常自然地适用于
    Spark 的使用，因为你可以将其读入 Spark DataFrame，并使用分区来并行化数据处理步骤。
- en: Next, we have the feature engineering. If we are using a Spark DataFrame in
    the first step, then we can apply our transformation logic using the base PySpark
    syntax we have discussed earlier in this chapter. For example, if we want to apply
    some feature transformations available from Scikit-Learn or another ML library,
    we can wrap these in UDFs and apply at the scale we need to. The data can then
    be exported in our chosen data format using the PySpark API. For the customer
    churn model, this could mean a combination of encoding of categorical variables
    and scaling of numerical variables, in line with the techniques explored in *Chapter
    3*, *From Model to Model Factory*.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论特征工程。如果在第一步中使用 Spark DataFrame，那么我们可以使用本章前面讨论的基础 PySpark 语法来应用我们的转换逻辑。例如，如果我们想应用来自
    Scikit-Learn 或其他机器学习库的一些特征转换，我们可以将这些转换封装在 UDFs 中，并在所需的规模上应用。然后，我们可以使用 PySpark
    API 将数据导出为我们选择的数据格式。对于客户流失模型，这可能意味着对分类变量的编码和对数值变量的缩放，这与在 *第 3 章*，*从模型到模型工厂* 中探讨的技术一致。
- en: Switching into the training of the model, now are moving from the data-intensive
    to the compute-intensive tasks. This means it is natural to start using Ray for
    model training, as you can easily set up parallel tasks to train models with different
    hyperparameter settings and distribute the training steps as well. There are particular
    benefits to using Ray for training deep learning or tree-based models as these
    are algorithms that are amenable to parallelization. So, if we are performing
    classification using one of the available models in Spark ML, then this can be
    done in a few lines, but if we are using something else, we will likely need to
    start wrapping in UDFs. Ray is far more library-agnostic but again the benefits
    really come if we are using a neural network in PyTorch or TensorFlow or using
    XGBoost or LightGBM, as these more naturally parallelize.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 转向模型的训练，我们现在正从数据密集型任务转向计算密集型任务。这意味着自然地开始使用 Ray 进行模型训练，因为你可以轻松设置并行任务来训练具有不同超参数设置的模型，并分配训练步骤。使用
    Ray 进行深度学习或基于树的模型训练有特定的好处，因为这些算法易于并行化。所以，如果我们使用 Spark ML 中可用的模型之一进行分类，这可以在几行代码内完成，但如果我们使用其他东西，我们可能需要开始封装
    UDFs。Ray 对库的依赖性更少，但再次，真正的好处来自于我们使用 PyTorch 或 TensorFlow 中的神经网络，或者使用 XGBoost 或
    LightGBM，因为这些更自然地并行化。
- en: Finally, onto the model inference step. In a batch setting, it is less clear
    who the winner is in terms of suggested framework here. Using UDFs or the core
    PySpark APIs, you can easily create a quite scalable batch prediction stage using
    Apache Spark and your Spark cluster. This is essentially because prediction on
    a large batch is really just another large-scale data transformation, where Spark
    excels. If, however, you wish to serve your model as an endpoint that can scale
    across a cluster, this is where Ray has very easy-to-use capabilities as shown
    in the *Scaling your serving layer with Ray* section. Spark does not have a facility
    for creating endpoints in this way and the scheduling and task overheads required
    to get a Spark job up and running mean that it would not be worth running Spark
    on small packets of data coming in as requests like this.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看模型推理步骤。在批量设置中，关于这里建议的框架，谁是赢家并不那么明确。使用UDFs或PySpark核心API，你可以轻松地使用Apache
    Spark和你的Spark集群创建一个相当可扩展的批量预测阶段。这主要是因为在大批量上的预测实际上只是另一种大规模数据转换，而Spark在这方面表现卓越。然而，如果你希望将你的模型作为一个可以跨集群扩展的端点提供服务，那么正如*使用Ray扩展你的服务层*部分所示，Ray提供了非常易于使用的功能。Spark没有创建这种端点的功能，并且启动Spark作业所需的调度和任务开销意味着，对于像这种作为请求传入的小数据包，运行Spark可能并不值得。
- en: For the customer churn example, this may mean that if we want to perform a churn
    classification on the whole customer base, Spark provides a nice way to process
    all of that data and leverage concepts like the underlying data partitions. You
    can still do this in Ray, but the lower-level API may mean it is slightly more
    work. Note that we can create this serving layer using many other mechanisms,
    as discussed in *Chapter 5*, *Deployment Patterns and Tools*, and the section
    on *Spinning up serverless infrastructure* in this chapter. *Chapter 8*, *Building
    an Example ML Microservice*, will also cover in detail how to use Kubernetes to
    scale out a deployment of an ML endpoint.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于客户流失的例子，这可能意味着如果我们想在整个客户基础上进行流失分类，Spark提供了一个很好的方式来处理所有这些数据，并利用像底层数据分区这样的概念。你仍然可以在Ray中这样做，但较低级别的API可能意味着这需要更多的工作。请注意，我们可以使用许多其他机制来创建这个服务层，如*第5章*、*部署模式和工具*以及本章中关于*启动无服务器基础设施*的部分所述。*第8章*、*构建示例ML微服务*也将详细说明如何使用Kubernetes扩展ML端点的部署。
- en: Finally, I have called the last stage the *application layer* to cover any “last
    mile” integrations between the output system and downstream systems in the solution.
    In this case, Spark does not really have a role to play since it can really be
    thought of as as a large-scale data transformation engine. Ray, on the other hand,
    has more of a philosophy of general Python acceleration, so if there are tasks
    that would benefit from parallelization in the backend of your applications, such
    as data retrieval, general calculations, simulation, or some other process, then
    the likelihood is you can still use Ray in some capacity, although there may be
    other tools available. So, in the customer churn example, Ray could be used for
    performing analysis at the level of individual customers and doing this in parallel
    before serving the results through a **Ray Serve** endpoint.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将最后一个阶段称为*应用层*，以涵盖解决方案中输出系统与下游系统之间的任何“最后一公里”集成。在这种情况下，Spark实际上并没有扮演什么角色，因为它实际上可以被视为一个大规模数据转换引擎。另一方面，Ray则更侧重于通用的Python加速哲学，所以如果你的应用程序后端有任务可以从并行化中受益，比如数据检索、一般计算、模拟或其他一些过程，那么你仍然可以在某种程度上使用Ray，尽管可能还有其他可用的工具。因此，在客户流失的例子中，Ray可以用于在服务结果之前对单个客户进行分析，并在**Ray
    Serve**端点并行执行此操作。
- en: The point of going through this high-level example was to highlight the points
    along your ML engineering project where you can make choices about how to scale
    effectively. I like to say that there are often *no right answers, but very often
    wrong answers*. What I mean by this is that there are often several ways to build
    a good solution that are equally valid and may leverage different tools. The important
    thing is to avoid the biggest pitfalls and dead ends. Hopefully, the example gives
    some indication of how you can apply this thinking to scaling up your ML solutions.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个高级示例，我们的目的是突出你在机器学习工程项目中可以做出关于如何有效扩展的选择的点。我常说，通常没有“正确答案”，但往往有很多“错误答案”。我的意思是，通常有几种构建良好解决方案的方法都是同样有效的，并且可能利用不同的工具。重要的是要避免最大的陷阱和死胡同。希望这个例子能给你一些关于如何将这种思考应用到扩展你的机器学习解决方案的启示。
- en: IMPORTANT NOTE
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Although I have presented a lot of questions here in terms of Spark vs. Ray,
    with a nod to Kubernetes as a more *base infrastructure* scaling option, there
    is now the ability to combine Spark and Ray through the use of **RayDP**. This
    toolkit now allows you to run Spark jobs on Ray clusters, so it nicely allows
    you to still use Ray as your base scaling layer but then leveRage the Spark APIs
    and capabilities where it excels. RayDP was introduced in 2021 and is in active
    development, so this is definitely a capability to watch. For more information,
    see the project repository here: [https://github.com/oap-project/raydp](https://github.com/oap-project/raydp).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在这里提出了很多关于Spark与Ray的问题，并提到了Kubernetes作为更基础的扩展基础设施选项，但现在有了通过使用**RayDP**结合Spark和Ray的能力。这个工具包现在允许你在Ray集群上运行Spark作业，这样你就可以继续使用Ray作为你的基础扩展层，同时利用Spark的API和功能，这些功能是Spark擅长的。RayDP于2021年推出，目前正在积极开发中，因此这绝对是一个值得关注的功能。更多信息，请参阅项目仓库：[https://github.com/oap-project/raydp](https://github.com/oap-project/raydp)。
- en: This concludes our look at how we can start to apply some of the scaling techniques
    we have discussed to our ML use cases.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对如何开始将我们讨论的一些扩展技术应用到我们的机器学习用例中的探讨。
- en: We will now finish the chapter with a brief summary of what we have covered
    in the last few pages.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将本章的内容以简要总结结束，总结我们在过去几页中涵盖的内容。
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to take the ML solutions we have built in
    the past few chapters and thought about how to scale them up to larger data volumes
    or higher numbers of requests for predictions. To do this, we mainly focused on
    **Apache Spark** as this is the most popular general-purpose engine for distributed
    computing. During our discussion of Apache Spark, we revisited some coding patterns
    and syntax we used previously in this book. By doing so, we developed a more thorough
    understanding of how and why to do certain things when developing in PySpark.
    We discussed the concept of **UDFs** in detail and how these can be used to create
    massively scalable ML workflows.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何将我们在过去几章中构建的机器学习解决方案进行扩展，以适应更大的数据量或更高的预测请求数量。为此，我们主要关注了**Apache
    Spark**，因为它是分布式计算中最受欢迎的通用引擎。在讨论Apache Spark的过程中，我们回顾了在此书中之前使用的一些编码模式和语法。通过这样做，我们更深入地理解了在PySpark开发中如何以及为什么进行某些操作。我们详细讨论了**UDFs（用户定义函数）**的概念，以及如何使用这些函数创建可大规模扩展的机器学习工作流程。
- en: After this, we explored how to work with Spark on the cloud, specifically through
    the **EMR** service provided by AWS. Then, we looked at some of the other ways
    we can scale our solutions; that is, through serverless architectures and horizontal
    scaling with containers. In the former case, we walked through how to build a
    service for serving an ML model using **AWS Lambda**. This used standard templates
    provided by the AWS SAM framework. We provided a high-level view of how to use
    K8s and Kubeflow to scale out ML pipelines horizontally, as well as some of the
    other benefits of using these tools. A section covering the Ray parallel computing
    framework then followed, showing how you can use its relatively simple API to
    scale compute on heterogenous clusters to supercharge your ML workflows. Ray is
    now one of the most important scalable computing toolkits for Python and has been
    used to train some of the largest models on the planet, including the GPT-4 model
    from OpenAI.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们探讨了如何在云上使用Spark，特别是通过AWS提供的**EMR**服务。然后，我们查看了一些其他可以扩展我们解决方案的方法；即，通过无服务器架构和容器化的水平扩展。在前一种情况下，我们介绍了如何使用**AWS
    Lambda**构建一个用于服务机器学习模型的服务。这使用了AWS SAM框架提供的标准模板。我们提供了如何使用K8s和Kubeflow水平扩展机器学习管道的高级视图，以及使用这些工具的一些其他好处。随后，我们介绍了一个关于Ray并行计算框架的部分，展示了如何使用其相对简单的API在异构集群上扩展计算，以加速你的机器学习工作流程。Ray现在是Python最重要的可扩展计算工具包之一，并被用于训练地球上的一些最大的模型，包括OpenAI的GPT-4模型。
- en: 'In the next chapter, we are going to build on the ideas of scale here by discussing
    the largest ML models you can build: deep learning models, including **large language
    models** (**LLM**s). Everything we will discuss in this next chapter could only
    have been developed, and can often only be effectively utilized, by considering
    the techniques we have covered here. The question of scaling up your ML solutions
    will also be revisited in *Chapter 8*, *Building an Example ML Microservice*,
    where we will focus on the use of Kubernetes to horizontally scale an ML microservice.
    This complements nicely the work we have done here on scaling large batch workloads
    by showing you how to scale more real-time workloads. Also, in *Chapter 9*, *Building
    an Extract, Transform, Machine Learning Use Case*, many of the scaling discussions
    we have had here are prerequisites; so, everything we have covered here puts you
    in a good place to get the most from the rest of the book. So, armed with all
    this new knowledge, let’s go and explore the world of the largest models known.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过讨论你可以构建的最大机器学习模型来扩展这里的规模概念：深度学习模型，包括**大型语言模型**（**LLM**s）。我们将在下一章中讨论的所有内容，都只能通过考虑我们在这里介绍的技术来开发和有效利用。在*第8章*，*构建一个示例机器学习微服务*中，我们也将重新审视扩展你的机器学习解决方案的问题，我们将重点关注使用Kubernetes水平扩展机器学习微服务。这很好地补充了我们在这里通过展示如何扩展更多实时工作负载来扩展大型批量工作负载的工作。此外，在*第9章*，*构建一个提取、转换、机器学习用例*中，我们在这里讨论的许多扩展讨论都是先决条件；因此，我们在这里介绍的所有内容都将为你从本书的其余部分中获得最大收益奠定良好的基础。因此，带着所有这些新知识，让我们去探索已知最大模型的领域。
- en: Join our community on Discord
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/mle](https://packt.link/mle)'
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code102810325355484.png)'
