- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Scaling Up
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: The previous chapter was all about starting the conversation around how we get
    our solutions out into the world using different deployment patterns, as well
    as some of the tools we can use to do this. This chapter will aim to build on
    that conversation by discussing the concepts and tools we can use to scale up
    our solutions to cope with large volumes of data or traffic.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章全部关于开始讨论我们如何使用不同的部署模式将我们的解决方案推向世界，以及我们可以使用的某些工具。本章将在此基础上进行讨论，讨论我们可以使用的概念和工具，以扩展我们的解决方案以应对大量数据或流量。
- en: Running some simple **machine learning** (**ML**) models on a few thousand data
    points on your laptop is a good exercise, especially when you’re performing the
    discovery and proof-of-concept steps we outlined previously at the beginning of
    any ML development project. This approach, however, is not appropriate if we have
    to run millions upon millions of data points at a relatively high frequency, or
    if we have to train thousands of models of a similar scale at any one time. This
    requires a different approach, mindset, and toolkit.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的笔记本电脑上运行一些简单的**机器学习**（**ML**）模型，在几千个数据点上是一个很好的练习，尤其是在您执行我们在任何机器学习开发项目开始时概述的发现和概念验证步骤时。然而，如果我们必须以相对较高的频率运行数百万个数据点，或者如果我们必须同时训练数千个类似规模的模型，这种方法就不合适了。这需要不同的方法、心态和工具集。
- en: 'In the following pages, we will cover some details of two of the most popular
    frameworks for distributing data computations in use today: **Apache Spark** and
    **Ray**. In particular, we will discuss some of the key points about how these
    frameworks tick under the hood so that, in development, we can make some good
    decisions about how to use them. We will then move onto a discussion of how to
    use these in your ML workflows with some concrete examples, these examples being
    specifically aimed to help you when it comes to processing large batches of data.
    Next, a brief introduction to creating serverless applications that allow you
    to scale out inference endpoints will be provided. Finally, we will cover an introduction
    to scaling containerized ML applications with Kubernetes, which complements the
    work we did in *Chapter 5*, *Deployment Patterns and Tools*, and will be built
    upon in detail with a full end-to-end example in *Chapter 8*, *Building an Example
    ML Microservice*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下页面中，我们将介绍目前使用最广泛的两个用于分布式数据计算的框架的详细信息：**Apache Spark**和**Ray**。特别是，我们将讨论这些框架在底层的一些关键点，以便在开发过程中，我们可以就如何使用它们做出一些好的决策。然后，我们将讨论如何使用这些框架在您的机器学习工作流程中，并提供一些具体的示例，这些示例专门旨在帮助您在处理大量数据时。接下来，将提供一个关于创建允许您扩展推理端点的无服务器应用的简要介绍。最后，我们将介绍如何使用Kubernetes扩展容器化的机器学习应用，这补充了我们在*第5章*，*部署模式和工具*中完成的工作，并在*第8章*，*构建示例ML微服务*中详细展开。
- en: This will help you build on some of the practical examples we already looked
    at earlier in this book, when we used Spark to solve our ML problems, with some
    more concrete theoretical understanding and further detailed practical examples.
    After this chapter, you should feel confident in how to use some of the best frameworks
    and techniques available to scale your ML solutions to larger and larger datasets.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这将帮助您在我们之前在这本书中已经查看的一些实际示例的基础上进行构建，当时我们使用Spark来解决我们的机器学习问题，并增加一些更具体的理论理解和更详细的实际示例。在本章之后，您应该对如何使用一些最好的框架和技术来扩展您的机器学习解决方案以适应更大的数据集感到自信。
- en: 'In this chapter, we will cover all of this in the following sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在以下部分中涵盖所有这些内容：
- en: Scaling with Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行扩展
- en: Spinning up serverless infrastructure
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动无服务器基础设施
- en: Containerizing at scale with Kubernetes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kubernetes进行大规模容器化
- en: Scaling with Ray
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ray进行扩展
- en: Designing systems at scale
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计大规模系统
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As with the other chapters, you can set up your Python development environment
    to be able to run the examples in this chapter by using the supplied Conda environment
    `yml` file or the `requirements.txt` files from the book repository, under *Chapter06:*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节一样，您可以通过使用提供的Conda环境`yml`文件或从书库中的`requirements.txt`文件来设置您的Python开发环境，以便能够运行本章中的示例，在*第06章*下：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This chapter’s examples will also require some non-Python tools to be installed
    to follow the examples end to end; please see the respective documentation for
    each tool:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例还需要安装一些非Python工具，以便从头到尾遵循示例；请参阅每个工具的相关文档：
- en: AWS CLI v2
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI v2
- en: Docker
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Postman
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Postman
- en: Ray
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray
- en: Apache Spark (version 3.0.0 or higher)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark（版本3.0.0或更高）
- en: Scaling with Spark
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行扩展
- en: '**Apache Spark**, or just Spark, came from the work of some brilliant researchers
    at the *University of California, Berkeley* in 2012 and since then, it has revolutionized
    how we tackle problems with large datasets.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**，或简称Spark，起源于2012年加州大学伯克利分校一些杰出研究人员的工作，自那时起，它彻底改变了我们处理大数据集问题的方法。'
- en: Spark is a cluster computing framework, which means it works on the principle
    that several computers are linked together in a way that allows computational
    tasks to be shared. This allows us to coordinate these tasks effectively. Whenever
    we discuss running Spark jobs, we always talk about *the cluster* we are running
    on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是一个集群计算框架，这意味着它基于几个计算机以允许计算任务共享的方式相互连接的原则。这使我们能够有效地协调这些任务。每次我们讨论运行Spark作业时，我们总是谈论我们在其上运行的*集群*。
- en: This is the collection of computers that perform the tasks, the worker nodes,
    and the computer that hosts the organizational workload, known as the head node.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一组执行任务的计算机，即工作节点，以及托管组织工作负载的计算机，被称为头节点。
- en: Spark is written in Scala, a language with a strong functional flavor and that
    compiles down to **Java Virtual Machines** (**JVMs**). Since this is a book about
    ML engineering in Python, we won’t discuss too much about the underlying Scala
    components of Spark, except where they help us use it in our work. Spark has several
    popular APIs that allow programmers to develop with it in a variety of languages,
    including Python. This gives rise to the PySpark syntax we have used in several
    examples throughout this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是用Scala编写的，这是一种具有强烈函数式风格的编程语言，并编译成**Java虚拟机**（**JVMs**）。由于这是一本关于Python机器学习工程的书籍，我们不会过多讨论Spark底层的Scala组件，除非它们有助于我们在工作中使用它。Spark有几个流行的API，允许程序员用多种语言（包括Python）与之一起开发。这导致了我们在本书中使用的PySpark语法。
- en: So, how is this all put together?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这一切是如何组合在一起的？
- en: 'Well, first of all, one of the things that makes Apache Spark so incredibly
    popular is the large array of connectors, components, and APIs it has available.
    For example, four main components interface with *Spark Core*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使Apache Spark如此受欢迎的一个原因是它拥有大量可用的连接器、组件和API。例如，四个主要组件与*Spark Core*接口：
- en: '**Spark SQL**, **DataFrames**, **and** **Datasets**: This component allows
    you to create very scalable programs that deal with structured data. The ability
    to write SQL-compliant queries and create data tables that leverage the underlying
    Spark engine through one of the main **structured APIs** of Spark (Python, Java,
    Scala, or R) gives very easy access to the main bulk of Spark’s functionality.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**、**DataFrames**和**Datasets**：这个组件允许你创建非常可扩展的程序，用于处理结构化数据。通过Spark的主要**结构化API**（Python、Java、Scala或R）编写符合SQL规范的查询并创建利用底层Spark引擎的数据表，可以非常容易地访问Spark的主要功能集。'
- en: '**Spark Structured Streaming**: This component allows engineers to work with
    streaming data that’s, for example, provided by a solution such as Apache Kafka.
    The design is incredibly simple and allows developers to simply work with streaming
    data as if it is a growing Spark structured table, with the same querying and
    manipulation functionality as for a standard one. This provides a low entry barrier
    for creating scalable streaming solutions.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Structured Streaming**：这个组件允许工程师处理由例如Apache Kafka提供的流数据。设计极其简单，允许开发者像处理一个不断增长的Spark结构化表一样简单地处理流数据，具有与标准表相同的查询和处理功能。这为创建可扩展的流解决方案提供了低门槛。'
- en: '**GraphX**: This is a library that allows you to implement graph parallel processing
    and apply standard algorithms to graph-based data (for example, algorithms such
    as PageRank or Triangle Counting). The **GraphFrames** project from Databricks
    makes this functionality even easier to use by allowing us to work with DataFrame-based
    APIs in Spark and still analyze graph data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：这是一个库，允许你实现图并行处理并将标准算法应用于基于图的数据（例如，如PageRank或三角形计数）。Databricks的**GraphFrames**项目通过允许我们在Spark中使用基于DataFrame的API来分析图数据，使得这一功能更加易于使用。'
- en: '**Spark ML**: Last but not least, we have the component that’s most appropriate
    for us as ML engineers: Spark’s native library for ML. This library contains the
    implementation of many algorithms and feature engineering capabilities we have
    already seen in this book. Being able to use **DataFrame** APIs in the library
    makes this extremely easy to use, while still giving us a route to creating very
    powerful code.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark ML**：最后但同样重要的是，我们有最适合我们作为机器学习工程师的组件：Spark 的原生机器学习库。这个库包含了我们在本书中已经看到过的许多算法和特征工程能力。能够在库中使用
    **DataFrame** API 使得它极其易于使用，同时仍然为我们提供了创建非常强大代码的途径。'
- en: The potential speedups you can gain for your ML training by using Spark ML on
    a Spark cluster versus running another ML library on a single thread can be tremendous.
    There are other tricks we can apply to our favorite ML implementations and then
    use Spark to scale them out; we’ll look at this later.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 Spark 集群上使用 Spark ML 与在单个线程上运行另一个机器学习库相比，你可以为你的机器学习训练获得巨大的速度提升。我们还可以应用其他技巧到我们最喜欢的机器学习实现中，然后使用
    Spark 来扩展它们；我们稍后会探讨这一点。
- en: Spark’s architecture is based on the driver/executor architecture. The driver
    is the program that acts as the main entry point for the Spark application and
    is where the **SparkContext** object is created. **SparkContext** sends tasks
    to the executors (which run on their own JVMs) and communicates with the cluster
    manager in a manner appropriate to the given manager and what mode the solution
    is running in. One of the driver’s main tasks is to convert the code we write
    into a logical set of steps in a **Directed Acyclic Graph** (**DAG**) (the same
    concept that we used with Apache Airflow in *Chapter 5*, *Deployment Patterns
    and Tools*), and then convert that DAG into a set of tasks that needs to be executed
    across the available compute resources.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的架构基于驱动程序/执行器架构。驱动程序是作为 Spark 应用程序的主要入口点的程序，也是创建 **SparkContext** 对象的地方。**SparkContext**
    将任务发送到执行器（它们在自己的 JVM 上运行），并以适合给定管理器和解决方案运行模式的方式与集群管理器进行通信。驱动程序的主要任务之一是将我们编写的代码转换为
    **有向无环图**（**DAG**）中的逻辑步骤集合（与我们在第 5 章 *部署模式和工具* 中使用的 Apache Airflow 的概念相同），然后将该
    DAG 转换为需要在可用的计算资源上执行的任务集合。
- en: In the pages that follow, we will assume we are running Spark with the **Hadoop
    YARN** resource manager, which is one of the most popular options and is also
    used by the **AWS****Elastic MapReduce** (**EMR**) solution by default (more on
    this later). When running with YARN in *cluster mode*, the driver program runs
    in a container on the YARN cluster, which allows a client to submit jobs or requests
    through the driver and then exit (rather than requiring the client to remain connected
    to the cluster manager, which can happen when you’re running in so-called *client
    mode*, which we will not discuss here).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的页面中，我们将假设我们正在使用 **Hadoop YARN** 资源管理器运行 Spark，这是最受欢迎的选项之一，也是 **AWS Elastic
    MapReduce**（**EMR**）解决方案的默认选项（关于这一点稍后还会详细介绍）。在以 *集群模式* 运行 YARN 时，驱动程序程序在 YARN
    集群上的一个容器中运行，这使得客户端可以通过驱动程序提交作业或请求，然后退出（而不是要求客户端保持与集群管理器的连接，这在所谓的 *客户端模式* 下可能会发生，这里我们不会讨论）。
- en: The cluster manager is responsible for launching the executors across the resources
    that are available on the cluster.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理器负责在集群上可用的资源上启动执行器。
- en: Spark’s architecture allows us, as ML engineers, to build solutions with the
    same API and syntax, regardless of whether we are working locally on our laptop
    or a cluster with thousands of nodes. The connection between the driver, the resource
    manager, and the executors is what allows this magic to happen.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的架构允许我们作为机器学习工程师，无论我们是在笔记本电脑上本地工作还是在拥有数千个节点的集群上工作，都可以使用相同的 API 和语法来构建解决方案。驱动程序、资源管理器和执行器之间的连接是实现这种魔法的关键。
- en: Spark tips and tricks
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark 技巧和技巧
- en: 'In this subsection, we will cover some simple but effective tips for writing
    performant solutions with Spark. We will focus on key pieces of syntax for data
    manipulation and preparation, which are always the first steps in any ML pipeline.
    Let’s get started:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍一些简单但有效的技巧，以使用 Spark 编写高性能的解决方案。我们将重点关注数据操作和准备的关键语法，这些通常是任何机器学习管道中的第一步。让我们开始吧：
- en: First, we will cover the basics of writing good Spark SQL. The entry point for
    any Spark program is the `SparkSession` object, which we need to import an instance
    of in our application.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将介绍编写良好的 Spark SQL 的基础知识。任何 Spark 程序的入口点是 `SparkSession` 对象，我们需要在我们的应用程序中导入其实例。
- en: 'It is often instantiated with the `spark` variable:'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它通常使用`spark`变量实例化：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can then run Spark SQL commands against your available data using the `spark`
    object and the `sql` method:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`spark`对象和`sql`方法运行Spark SQL命令，针对你的可用数据：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are a variety of ways to make the data you need available inside your
    Spark programs, depending on where they exist. The following example has been
    taken from some of the code we went through in *Chapter 3*, *From Model to Model
    Factory*, and shows how to pull data into a DataFrame from a `csv` file:'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据数据存在的地方，有各种方法可以在Spark程序内部提供所需的数据。以下示例取自我们在*第3章*，“从模型到模型工厂”中经过的一些代码，展示了如何从`csv`文件中将数据拉入DataFrame：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can create a temporary view of this data using the following syntax:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下语法创建此数据的临时视图：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we can query against this data using the methods mentioned previously
    to see the records or to create new DataFrames:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用之前提到的方法查询此数据，以查看记录或创建新的DataFrames：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When writing Spark SQL, some standard practices help your code to be efficient:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写Spark SQL时，一些标准做法有助于提高代码的效率：
- en: Try not to join big tables on the left with small tables on the right as this
    is inefficient. In general, try and make datasets used for joins as lean as possible,
    so, for example, do not join using unused columns or rows as much as possible.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量不要将左边的大的表格与右边的小的表格连接，因为这效率低下。通常，尽量使用于连接的数据集尽可能瘦，例如，尽可能少地使用未使用的列或行进行连接。
- en: Avoid query syntax that will scan full datasets if they are very large; for
    example, `select max(date_time_value)`.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免查询语法扫描非常大的数据集；例如，`select max(date_time_value)`。
- en: In this case, try and define logic that can filter the data more aggressively
    before finding min or max values and in general allow the solution to scan over
    a smaller dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，尝试定义逻辑，在找到最小或最大值之前更积极地过滤数据，并且通常允许解决方案扫描更小的数据集。
- en: 'Some other good practices when working with Spark are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Spark时，以下是一些其他的好做法：
- en: '**Avoid data skew**: Do what you can to understand how your data will be split
    across executors. If your data is partitioned on a date column, this may be a
    good choice if volumes of data are comparable for each day but bad if some days
    have most of your data and others very little. Repartitioning using a more appropriate
    column (or on a Spark-generated ID from the `repartition` command) will be required.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免数据倾斜**：尽可能了解你的数据将如何在执行器之间分割。如果你的数据是在日期列上分区的，如果每天的数据量相当，这可能是一个不错的选择，但如果某些天有大部分数据而其他天很少，这可能是一个坏选择。可能需要使用更合适的列（或使用`repartition`命令生成的Spark生成的ID）重新分区。'
- en: '**Avoid data shuffling**: This is when data is redistributed across different
    partitions. For example, we may have a dataset that is partitioned at the day
    level and then we ask Spark to sum over one column of the dataset for all of time.
    This will cause all of the daily partitions to be accessed and the result to be
    written to a new partition. For this to occur, disk writes and a network transfer
    have to occur, which can often lead to performance bottlenecks for your Spark
    job.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免数据洗牌**：这是指数据在不同分区之间重新分配。例如，我们可能有一个按日级别分区的数据集，然后我们要求Spark对所有时间的数据集的一个列求和。这将导致所有每日分区被访问，并将结果写入一个新的分区。为此，必须发生磁盘写入和网络传输，这通常会导致你的Spark作业的性能瓶颈。'
- en: '**Avoid actions in large datasets**: For example, when you run the `collect()`
    command, you will bring all of your data back onto the driver node. This can be
    very bad if it is a large dataset but may be needed to convert the result of a
    calculation into something else. Note that the `toPandas()` command, which converts
    your Spark `DataFrame` into a pandas `DataFrame`, also collects all the data in
    the driver’s memory.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免在大数据集中执行操作**：例如，当你运行`collect()`命令时，你将把所有数据都带回驱动节点。如果这是一个大数据集，这可能会非常糟糕，但可能需要将计算结果转换为其他东西。请注意，`toPandas()`命令，它将你的Spark
    `DataFrame`转换为pandas `DataFrame`，也会收集驱动器内存中的所有数据。'
- en: '**Use UDFs when they make sense**: Another excellent tool to have in your arsenal,
    as an ML engineer using Apache Spark, is the **User-Defined Function** (**UDF**).
    UDFs allow you to wrap up more complex and bespoke logic and apply it at scale
    in a variety of ways. An important aspect of this is that if you write a standard
    PySpark (or Scala) UDF, then you can apply this *inside* Spark SQL syntax, which
    allows you to efficiently reuse your code and even simplify the application of
    your ML models. The downside is that these are sometimes not the most efficient
    pieces of code, but if it helps to make your solution simpler and more maintainable,
    it may be the right choice.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当适用时使用UDF**：作为Apache Spark的ML工程师，你武器库中的另一个优秀工具是**用户定义函数**（**UDF**）。UDF允许你封装更复杂和定制的逻辑，并以各种方式大规模应用。这个方面的重要之处在于，如果你编写了一个标准的PySpark（或Scala）UDF，那么你可以在Spark
    SQL语法内部应用这个UDF，这允许你高效地重用你的代码，甚至简化ML模型的适用。缺点是这些代码有时可能不是最有效的，但如果它有助于使你的解决方案更简单、更易于维护，那么它可能是一个正确的选择。'
- en: As a concrete example, let’s build a UDF that looks at the banking data we worked
    with in *Chapter 3*, *From Model to Model Factory*, to create a new column called
    ‘`month_as_int`' that converts the current string representation of the month
    into an integer for processing later. We will not concern ourselves with train/test
    splits or what this might be used for; instead, we will just highlight how to
    apply some logic to a PySpark UDF.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为具体示例，让我们构建一个UDF，它将查看我们在第3章“从模型到模型工厂”中处理过的银行数据，创建一个名为‘`month_as_int`’的新列，该列将当前月份的字符串表示形式转换为整数以便后续处理。我们不会关注训练/测试分割或这可能被用于什么；相反，我们将突出如何将一些逻辑应用于PySpark
    UDF。
- en: 'Let’s get started:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'First, we must read the data. Note that the relative path given here is consistent
    with the `spark_example_udfs.py` script, which can be found in this book’s GitHub
    repository at [https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须读取数据。注意这里给出的相对路径与本书GitHub仓库中的`spark_example_udfs.py`脚本一致，该脚本位于[https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py](https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition/blob/main/Chapter06/mlewp2-spark/spark_example_udfs.py)：
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we show the current data with the `data.show()` command, we will see something
    like this:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们使用`data.show()`命令显示当前数据，我们会看到类似以下内容：
- en: '![Figure 6.1 – A sample of the data from the initial DataFrame in the banking
    dataset ](img/B19525_06_01.png)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.1 – 银行数据集中初始DataFrame的数据样本](img/B19525_06_01.png)'
- en: 'Figure 6.1: A sample of the data from the initial DataFrame in the banking
    dataset.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.1：银行数据集中初始DataFrame的数据样本。
- en: 'Now, we can double-check the schema of this DataFrame using the `data.printSchema()`
    command. This confirms that `month` is stored as a string currently, as shown
    here:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`data.printSchema()`命令双重检查这个DataFrame的模式。这确认了`month`目前是以字符串形式存储的，如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can define our UDF, which will use the Python `datetime` library to
    convert the string representation of the month into an integer:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们的UDF，它将使用Python的`datetime`库将月份的字符串表示形式转换为整数：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we want to apply our function inside Spark SQL, then we must register the
    function as a UDF. The arguments for the `register()` function are the registered
    name of the function, the name of the Python function we have just written, and
    the return type. The return type is `StringType()` by default, but we have made
    this explicit here:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想在Spark SQL内部应用我们的函数，那么我们必须将函数注册为UDF。`register()`函数的参数是函数的注册名称、我们刚刚编写的Python函数的名称以及返回类型。默认情况下，返回类型是`StringType()`，但我们在这里明确指定了它：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, now that we have registered the function, we can apply it to our data.
    First, we will create a temporary view of the bank dataset and then run a Spark
    SQL query against it that references our UDF:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，既然我们已经注册了函数，我们就可以将其应用于我们的数据。首先，我们将创建银行数据集的一个临时视图，然后运行一个Spark SQL查询，该查询引用我们的用户定义函数（UDF）：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the preceding syntax with the `show()` command shows that we have successfully
    calculated the new column. The last few columns of the resulting `DataFrame` are
    shown here:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`show()`命令运行前面的语法显示我们已经成功计算了新列。结果`DataFrame`的最后几列如下所示：
- en: '![Figure 6.3 – The new column has been calculated successfully by applying
    our UDF ](img/B19525_06_02.png)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.3 – 通过应用我们的UDF成功计算了新列](img/B19525_06_02.png)'
- en: 'Figure 6.2: The new column has been calculated succesfully by applying our
    UDF.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.2：通过应用我们的UDF成功计算了新列。
- en: 'Alternatively, we can create our UDF with the following syntax and apply the
    result to a Spark `DataFrame`. As mentioned before, using a UDF can sometimes
    allow you to wrap up relatively complex syntax quite simply. The syntax here is
    quite simple but I’ll show you it anyway. This gives us the same result that’s
    shown in the preceding screenshot:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以使用以下语法创建我们的UDF，并将结果应用于Spark `DataFrame`。如前所述，使用UDF有时可以让你非常简单地封装相对复杂的语法。这里的语法相当简单，但我仍然会向你展示。这给我们带来了与前面截图相同的结果：
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, PySpark also provides a nice decorator syntax for creating our UDF,
    meaning that if you are indeed building some more complex functionality, you can
    just place this inside the Python function that is being decorated. The following
    code block also gives the same results as the preceding screenshot:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，PySpark还提供了一个很好的装饰器语法来创建我们的UDF，这意味着如果你确实在构建一些更复杂的功能，你只需将这个装饰器放在被装饰的Python函数中即可。下面的代码块也给出了与前面截图相同的结果：
- en: '[PRE12]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This shows how we can apply some simple logic in a UDF, but for us to deploy
    a model at scale using this approach, we have to put the ML logic inside the function
    and apply it in the same manner. This can become a bit tricky if we want to work
    with some of the standard tools we are used to from the data science world, such
    as Pandas and **Scikit-learn**. Luckily, there is another option we can use that
    has a few benefits. We will discuss this now.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了如何在UDF中应用一些简单的逻辑，但为了使用这种方法在规模上部署模型，我们必须在函数内部放置ML逻辑并以相同的方式应用它。如果我们想使用我们习惯于从数据科学世界使用的标准工具，如Pandas和**Scikit-learn**，这可能会变得有点棘手。幸运的是，我们还有另一个可以使用的选项，它有一些优点。我们现在就来讨论这个。
- en: The UDFs currently being considered have a slight issue when we are working
    in Python in that translating data between the JVM and Python can take a while.
    One way to get around this is to use what is known as **pandas UDFs**, which use
    the Apache Arrow library under the hood to ensure that the data is read quickly
    for the execution of our UDFs. This gives us the flexibility of UDFs without any
    slowdown.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Python中工作时，目前考虑的UDF存在一个小问题，那就是在JVM和Python之间转换数据可能需要一段时间。一种解决方法是使用所谓的**pandas
    UDFs**，它底层使用Apache Arrow库来确保我们的UDF执行时数据读取快速。这给我们带来了UDF的灵活性，而没有任何减速。
- en: pandas UDFs are also extremely powerful because they work with the syntax of
    – you guessed it – pandas **Series** and **DataFrame** objects. This means that
    a lot of data scientists who are used to working with pandas to build models locally
    can easily adapt their code to scale up using Spark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: pandas UDFs也非常强大，因为它们与pandas **Series** 和 **DataFrame** 对象的语法一起工作。这意味着许多习惯于使用pandas在本地构建模型的科学家可以轻松地将他们的代码扩展到使用Spark。
- en: 'As an example, let’s walk through how to apply a simple classifier to the wine
    dataset that we used earlier in this book. Note that the model was not optimized
    for this data; we are just showing an example of applying a pre-trained classifier:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们回顾一下如何将一个简单的分类器应用于我们在这本书中之前使用过的wine数据集。请注意，该模型并未针对这些数据进行优化；我们只是展示了一个应用预训练分类器的示例：
- en: 'First, let’s create a simple **Support Vector Machine** (**SVM**)-based classifier
    on the wine dataset. We are not performing correct training/test splits, feature
    engineering, or other best practices here as we just want to show you how to apply
    any `sklearn` model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们在wine数据集上创建一个简单的**支持向量机**（**SVM**）分类器。我们在这里没有进行正确的训练/测试分割、特征工程或其他最佳实践，因为我们只是想向你展示如何应用任何`sklearn`模型：
- en: '[PRE13]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then bring the feature data into a Spark DataFrame to show you how to
    apply the pandas UDF in later stages:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以将特征数据带入Spark DataFrame，以展示如何在后续阶段应用pandas UDF：
- en: '[PRE14]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'pandas UDFs are very easy to define. We just write our logic in a function
    and then add the `@pandas_udf` decorator, where we also have to provide the output
    type for the function. In the simplest case, we can just wrap the (normally serial
    or only locally parallelized) process of performing a prediction with the trained
    model:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pandas UDFs非常容易定义。我们只需在函数中编写我们的逻辑，然后添加`@pandas_udf`装饰器，在那里我们还需要为函数提供输出类型。在最简单的情况下，我们可以将使用训练模型进行预测的（通常是串行或仅本地并行化）过程封装起来：
- en: '[PRE15]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can apply this to the Spark `DataFrame` containing the data by
    passing in the appropriate inputs we needed for our function. In this case, we
    will pass in the column names of the features, of which there are 13:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过传递我们函数所需的适当输入来将此应用于包含数据的Spark `DataFrame`。在这种情况下，我们将传递特征列的名称，共有13个：
- en: '[PRE16]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, if you look at the results of this, you will see the following for the
    first few rows of the `df_pred` DataFrame:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您查看这个结果，您将看到`df_pred` DataFrame的前几行如下所示：
- en: '![Figure 6.4 – The result of applying a simple pandas UDF ](img/B19525_06_03.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 应用简单pandas UDF的结果](img/B19525_06_03.png)'
- en: 'Figure 6.3: The result of applying a simple pandas UDF.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：应用简单的pandas UDF的结果。
- en: And that completes our whirlwind tour of UDFs and pandas UDFs in Spark, which
    allow us to take serial Python logic, such as data transformations or our ML models,
    and apply them in a manifestly parallel way.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了对Spark和pandas UDF在Spark中的快速浏览，这使我们能够以明显并行的方式应用诸如数据转换或我们的机器学习模型之类的串行Python逻辑。
- en: In the next section, we will focus on how to set ourselves up to perform Spark-based
    computations in the cloud.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将专注于如何在云端设置Spark-based计算。
- en: Spark on the cloud
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云端Spark
- en: As should be clear from the preceding discussion, writing and deploying PySpark-based
    ML solutions can be done on your laptop, but for you to see the benefits when
    working at scale, you must have an appropriately sized computing cluster to hand.
    Provisioning this sort of infrastructure can be a long and painful process but
    as discussed already in this book, a plethora of options for infrastructure are
    available from the main public cloud providers.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，应该很清楚，编写和部署基于PySpark的机器学习解决方案可以在您的笔记本电脑上完成，但为了在工作规模上看到好处，您必须拥有适当规模的计算集群。提供此类基础设施可能是一个漫长而痛苦的过程，但正如本书中已经讨论的那样，主要公共云提供商提供了大量的基础设施选项。
- en: For Spark, AWS has a particularly nice solution called **AWS Elastic MapReduce**
    (**EMR**), which is a managed big data platform that allows you to easily configure
    clusters of a few different flavors across the big data ecosystem. In this book,
    we will focus on Spark-based solutions, so we will focus on creating and using
    clusters that have Spark tooling to hand.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Spark，AWS有一个特别好的解决方案，称为**AWS Elastic MapReduce**（**EMR**），这是一个托管的大数据平台，允许您轻松配置大数据生态系统中的几种不同类型的集群。在这本书中，我们将专注于基于Spark的解决方案，因此我们将专注于创建和使用带有Spark工具的集群。
- en: In the next section, we will go through a concrete example of spinning up a
    Spark cluster on EMR and then deploying a simple Spark ML-based application onto
    it.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过一个具体的例子来展示如何在EMR上启动一个Spark集群，然后将其部署一个简单的基于Spark ML的应用程序。
- en: So, with that, let’s explore Spark on the cloud with **AWS EMR**!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们在**AWS EMR**上探索Spark在云端的应用！
- en: AWS EMR example
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS EMR示例
- en: 'To understand how EMR works, we will continue in the practical vein that the
    rest of this book will follow and dive into an example. We will begin by learning
    how to create a brand-new cluster before discussing how to write and deploy our
    first PySpark ML solution to it. Let’s get started:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解EMR是如何工作的，我们将继续遵循本书的实践方法，并深入一个例子。我们将首先学习如何创建一个全新的集群，然后再讨论如何编写和部署我们的第一个PySpark
    ML解决方案到集群中。让我们开始吧：
- en: First, navigate to the **EMR** page on AWS and find the **Create Cluster** button.
    You will then be brought to a page that allows you to input configuration data
    for your cluster. The first section is where you specify the name of the cluster
    and the applications you want to install on it. I will call this cluster `mlewp2-cluster`,
    use the latest EMR release available at the time of writing, 6.11.0, and select
    the **Spark** application bundle.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到AWS上的**EMR**页面，找到**创建集群**按钮。然后，您将被带到允许您输入集群配置数据的页面。第一个部分是您指定集群名称和要安装在其上的应用程序的地方。我将把这个集群命名为`mlewp2-cluster`，使用写作时的最新EMR版本6.11.0，并选择**Spark**应用程序包。
- en: All other configurations can remain as default in this first section. This is
    shown in *Figure 6.4*:![A screenshot of a computer program  Description automatically
    generated](img/B19525_06_04.png)
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个第一部分，所有其他配置都可以保持默认设置。这如图6.4所示：![计算机程序截图 描述自动生成](img/B19525_06_04.png)
- en: 'Figure 6.4: Creating our EMR cluster with some default configurations.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.4：使用一些默认配置创建我们的EMR集群。
- en: Next comes the configuration of the compute used in the cluster. You can just
    use the defaults here again but it is important to understand what is going on.
    First, there is the selection of whether to use “instance groups” or “instance
    fleets,” which refers to the strategy deployed for scaling up your compute given
    some constraints you provide. Instance groups are simpler and define the specific
    servers you want to run for each node type, more on this in a second, and you
    can choose between “on-demand” or “spot instances” for acquiring more servers
    if needed during the lifetime of the cluster. Instance fleets allow for a lot
    more complex acquisition strategies and for blends of different server instance
    types for each node type. For more information, read the AWS documentation to
    make sure you get a clear view of the different options, [https://docs.aws.amazon.com/emr/index.xhtml](https://docs.aws.amazon.com/emr/index.xhtml);
    we will proceed by using an instance group with default settings. Now, onto nodes.
    There are different nodes in an EMR cluster; primary, core and task. The primary
    node will run our YARN Resource Manager and will track job status and instance
    group health. The core nodes run some daemons and the Spark executors. Finally,
    the task nodes perform the actual distributed calculations. For now, let’s proceed
    with the defaults provided for the instance groups option, as shown for the **Primary**
    nodes in *Figure 6.5*.![A screenshot of a computer  Description automatically
    generated](img/B19525_06_05.png)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是集群中使用的计算配置。您在这里也可以再次使用默认设置，但了解正在发生的事情很重要。首先，是选择使用“实例组”还是“实例舰队”，这指的是根据您提供的某些约束条件部署的计算扩展策略。实例组更简单，定义了您为每种节点类型想要运行的特定服务器，关于这一点我们稍后再详细说明，并且您可以在集群生命周期内需要更多服务器时选择“按需”或“竞价实例”。实例舰队允许采用更多复杂的获取策略，并为每种节点类型混合不同的服务器实例类型。有关更多信息，请阅读
    AWS 文档，以确保您对不同的选项有清晰的了解，[https://docs.aws.amazon.com/emr/index.xhtml](https://docs.aws.amazon.com/emr/index.xhtml)；我们将通过使用具有默认设置的实例组来继续操作。现在，让我们转到节点。EMR
    集群中有不同的节点；主节点、核心节点和任务节点。主节点将运行我们的 YARN 资源管理器，并跟踪作业状态和实例组健康。核心节点运行一些守护程序和 Spark
    执行器。最后，任务节点执行实际的分布式计算。现在，让我们按照为实例组选项提供的默认设置进行操作，如图 6.5 中的**主节点**所示。![计算机屏幕截图  自动生成的描述](img/B19525_06_05.png)
- en: 'Figure 6.5: Compute configuration for our EMR cluster. We have selected the
    simpler “instance groups” option for the configuration and have gone with the
    server type defaults.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.5：我们的 EMR 集群的计算配置。我们选择了更简单的“实例组”选项进行配置，并采用了服务器类型的默认设置。
- en: Next, we move onto defining the explicit cluster scaling behavior that we mentioned
    is used in the instance groups and instance fleet compute options in *step 2*.
    Again, select the defaults for now, but you can play around to make the cluster
    larger here in terms of numbers of nodes or you can define auto-scaling behaviour
    that will dynamically increase the cluster size upon larger workloads. *Figure
    6.6* shows what this should look like.![A screenshot of a computer screen  Description
    automatically generated](img/B19525_06_06.png)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们在*步骤 2*中提到的用于实例组和实例舰队计算选项的显式集群缩放行为。再次提醒，现在请选择默认设置，但您可以在这里尝试调整集群的大小，无论是通过增加节点数量，还是定义在负载增加时动态增加集群大小的自动缩放行为。*图
    6.6*展示了它应该看起来是什么样子。![计算机屏幕截图  自动生成的描述](img/B19525_06_06.png)
- en: 'Figure 6.6: The cluster provisioning and scaling strategy selection. Here we
    have gone with the defaults of a specific, small cluster size, but you can increase
    these values to have a bigger cluster or use the auto-scaling option to provide
    min and max size limits.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.6：集群配置和缩放策略选择。在这里，我们选择了特定的小集群大小的默认设置，但您可以增加这些值以获得更大的集群，或者使用自动缩放选项来提供最小和最大大小限制。
- en: Now, there is a **Networking** section, which is easier if you have already
    created some **virtual private clouds** (**VPCs**) and subnets for the other examples
    in the book; see *Chapter 5*, *Deployment Patterns and Tools* and the AWS documentation
    for more information. Just remember that VPCs are all about keeping the infrastructure
    you are provisioning isolated from other services in your own AWS account and
    even from the wider internet, so it’s definitely good to become familiar with
    them and their application.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，有一个**网络**部分，如果你已经为书中的其他示例创建了一些**虚拟专用网络**（**VPC**）和子网，这将更容易；参见*第5章*，*部署模式和工具*以及AWS文档以获取更多信息。只需记住，VPCs主要是关于将你正在配置的基础设施与其他AWS账户中的服务以及更广泛的互联网隔离开来，因此熟悉它们及其应用绝对是件好事。
- en: For completeness, *Figure 6.7* shows the setup I used for this example.![A screenshot
    of a computer  Description automatically generated](img/B19525_06_07.png)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了完整性，*图6.7*显示了我在这个示例中使用的设置。![计算机截图 自动生成描述](img/B19525_06_07.png)
- en: 'Figure 6.7: The networking configuration requires the use of a VPC; it will
    automatically create a subnet for the cluster if there is not one selected.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.7：网络配置需要使用VPC；如果没有选择，它将自动为集群创建一个子网。
- en: We only have a couple more sections we need to input into to define our cluster.
    The next mandatory section is around cluster termination policies. I would always
    recommend having an automated teardown policy for infrastructure where possible
    as this helps to manage costs. There have been many stories across the industry
    of teams leaving un-used servers running and racking up huge bills! *Figure 6.8*
    shows that we are using such an automated cluster termination policy where the
    cluster will terminate after 1 hour of not being utilized.![A screenshot of a
    computer  Description automatically generated](img/B19525_06_08.png)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要输入几个更多部分来定义我们的集群。下一个强制性的部分是关于集群终止策略。我总是建议在可能的情况下为基础设施设置自动拆解策略，因为这有助于管理成本。整个行业都有很多关于团队留下未使用的服务器运行并产生巨额账单的故事！*图6.8*显示，我们正在使用这样的自动集群终止策略，其中集群将在1小时未被使用后终止。![计算机截图
    自动生成描述](img/B19525_06_08.png)
- en: 'Figure 6.8: Defining a cluster termination policy like this one is considered
    best practice and can help avoid unnecessary costs.'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.8：定义一个类似于这样的集群终止策略被认为是最佳实践，并且可以帮助避免不必要的成本。
- en: The final required section for completion is the definition of the appropriate
    **Identity and Access Management** (**IAM**) roles, which defines what accounts
    can access the resources we are creating. If you already have IAM roles that you
    are happy to reuse as your EMR service role, then you can do this; for this example,
    however, let’s create a new service role specifically for this cluster. *Figure
    6.9* shows that selecting the option to create a new role pre-populates the VPC,
    subnet, and security group with values matching what you have already selected
    through this process. You can add more to these. *Figure 6.10* shows that we can
    also select to create an “instance profile”, which is just the name given to a
    service role that applies to all server instances in an EC2 cluster at launch.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_09.png)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成所需的最后一个部分是定义适当的**身份和访问管理**（**IAM**）角色，它定义了哪些账户可以访问我们正在创建的资源。如果你已经有了一些你愿意作为EMR服务角色重用的IAM角色，那么你可以这样做；然而，对于这个示例，让我们为这个集群创建一个新的服务角色。*图6.9*显示，选择创建新角色的选项会预先填充VPC、子网和安全组，其值与通过此过程已选择的值相匹配。你可以添加更多内容。*图6.10*显示，我们还可以选择创建一个“实例配置文件”，这只是一个在启动时应用于EC2集群中所有服务器实例的服务角色的名称。![计算机截图
    自动生成描述](img/B19525_06_09.png)
- en: 'Figure 6.9: Creating an AWS EMR service role.'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.9：创建AWS EMR服务角色。
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_10.png)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机截图 自动生成描述](img/B19525_06_10.png)'
- en: 'Figure 6.10: Creating an instance profile for the EC2 servers being used in
    this EMR cluster. An instance profile is just the name for the service role that
    is assigned to all cluster EC2 instances at spin-up time.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.10：为在此EMR集群中使用的EC2服务器创建实例配置文件。实例配置文件只是分配给所有集群EC2实例在启动时的服务角色的名称。
- en: The sections discussed are all the mandatory ones required to create your cluster,
    but there are also some optional sections that I would like to briefly mention
    to guide your further exploration. There is the option to specify **steps**, which
    is where you can define shell scripts, JAR applications, or Spark applications
    to run in sequence. This means you can spin up your cluster with your applications
    ready to start processing data in the sequence you desire, rather than submitting
    jobs after the infrastructure deployment. There is a section on **Bootstrap actions**,
    which allows you to define custom installation or configuration steps that should
    run before any applications are installed or any data is processed on the EMR
    cluster. Cluster logs locations, tags, and some basic software considerations
    are also available for configuration. The final important point to mention is
    on the security configuration. *Figure 6.11* shows the options. Although we will
    deploy this cluster without specifying any EC2 key pair or security configuration,
    it is crucially important that you understand the security requirements and norms
    of your organization if you want to run this cluster in production. Please consult
    your security or networking teams to ensure this is all in line with expectations
    and requirements. For now, we can leave it blank and proceed to create the cluster.![A
    screenshot of a computer  Description automatically generated](img/B19525_06_11.png)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 讨论的部分都是创建您的集群所必需的章节，但也有一些可选章节，我想简要提及以指导您进一步探索。这里有指定**步骤**的选项，您可以在其中定义要按顺序运行的shell脚本、JAR应用程序或Spark应用程序。这意味着您可以在基础设施部署后提交作业之前，启动集群并准备好应用程序以按您希望的顺序处理数据。还有一个关于**引导操作**的章节，它允许您定义在安装任何应用程序或处理EMR集群上的任何数据之前应运行的定制安装或配置步骤。集群日志位置、标签和一些基本软件考虑因素也适用于配置。最后要提到的重要一点是安全配置。*图6.11*显示了选项。尽管我们将不指定任何EC2密钥对或安全配置来部署此集群，但如果您想在生产中运行此集群，了解您组织的网络安全要求和规范至关重要。请咨询您的安全或网络团队以确保一切符合预期和要求。目前，我们可以将其留空，然后继续创建集群。![计算机屏幕截图  自动生成描述](img/B19525_06_11.png)
- en: 'Figure 6.11: The security configuration for the cluster shown here is optional
    but should be considered carefully if you aim to run the cluster in production.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11：此处显示的集群安全配置是可选的，但如果您打算在生产中运行集群，应仔细考虑。
- en: Now that we have selected all of the mandatory options, click the **Create cluster**
    button to launch. Upon successful creation, you should see a review page like
    that shown in *Figure 6.12*. And that’s it; you have now created your own Spark
    cluster in the cloud!![A screenshot of a computer  Description automatically generated](img/B19525_06_12.png)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经选择了所有必需的选项，点击**创建集群**按钮以启动。创建成功后，您应该会看到一个类似于*图6.12*所示的审查页面。就这样；现在您已经在云中创建了自己的Spark集群了！！![计算机屏幕截图  自动生成描述](img/B19525_06_12.png)
- en: 'Figure 6.12: EMR cluster creation review page shown upon successful launch.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12：成功启动后显示的EMR集群创建审查页面。
- en: 'After spinning up our EMR cluster, we want to be able to submit work to it.
    Here, we will adapt the example Spark ML pipeline we produced in *Chapter 3*,
    *From Model to Model Factory*, to analyze the banking dataset and submit this
    as a step to our newly created cluster. We will do this as a standalone single
    PySpark script that acts as the only step in our application, but it is easy to
    build on this to make far more complex applications:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动我们的EMR集群后，我们希望能够向其提交工作。在这里，我们将调整我们在*第3章*，*从模型到模型工厂*中生产的示例Spark ML管道，以分析银行数据集，并将其作为步骤提交到我们新创建的集群。我们将这样做为一个独立的单个PySpark脚本，作为我们应用程序的唯一步骤，但很容易在此基础上构建更复杂的应用程序：
- en: 'First, we will take the code from *Chapter 3*, *From Model to Model Factory*,
    and perform some nice refactoring based on our discussions around good practices.
    We can more effectively modularize the code so that it contains a function that
    provides all our modeling steps (not all of the steps have been reproduced here,
    for brevity). We have also included a final step that writes the results of the
    modeling to a `parquet` file:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将从*第3章*，*从模型到模型工厂*中提取代码，并根据我们围绕良好实践的讨论进行一些精心的重构。我们可以更有效地模块化代码，使其包含一个提供所有建模步骤的功能（为了简洁，并非所有步骤都在此处重现）。我们还包括了一个最终步骤，将建模结果写入`parquet`文件：
- en: '[PRE17]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Building on this, we will wrap all of the main boilerplate code into a `main`
    function that can be called at the `if __name__=="__main__":` entry point to the
    program:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We put the preceding functions into a script called `spark_example_emr.py`,
    which we will submit to our EMR cluster later:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, to submit this script to the EMR cluster we have just created, we need
    to find out the cluster ID, which we can get from the AWS UI or by running the
    following command:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Then, we need to send the `spark_example_emr.py` script to S3 to be read in
    by the cluster. We can create an S3 bucket called `s3://mlewp-ch6-emr-examples`
    to store this and our other artifacts using either the CLI or the AWS console
    (see *Chapter 5*, *Deployment Patterns and Tools*). Once copied over, we are ready
    for the final step.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we must submit the script using the following command, with `<CLUSTER_ID>`
    replaced with the ID of the cluster we just created. Note that if your cluster
    has been terminated due to the automated termination policy we set, you can’t
    restart it but you can clone it. After a few minutes, the step should have completed
    and the outputs should have been written to the `results.parquet` file in the
    same S3 bucket:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: And that is it – that is how we can start developing PySpark ML pipelines on
    the cloud using **AWS EMR**!
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will see that this previous process has worked successfully by navigating
    to the appropriate S3 bucket and confirming that the `results.parquet` file was
    created succesfully; see *Figure 6.13*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19525_06_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Successful creation of the results.parquet file upon submission
    of the EMR script.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore another method of scaling up our solutions
    by using so-called serverless tools.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Spinning up serverless infrastructure
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever we do any ML or software engineering, we have to run the requisite
    tasks and computations on computers, often with appropriate networking, security,
    and other protocols and software already in place, which we have often referred
    to already as constituting our *infrastructure*. A big part of our infrastructure
    is the servers we use to run the actual computations. This might seem a bit strange,
    so let’s start by talking about *serverless* infrastructure (how can there be
    such a thing?). This section will explain this concept and show you how to use
    it to scale out your ML solutions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless** is a bit misleading as a term as it does not mean that no physical
    servers are running your programs. It does mean, however, that the programs you
    are running should not be thought of as being statically hosted on one machine,
    but as ephemeral instances on another layer on top of the underlying hardware.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of serverless tools for your ML solution include (but are not
    limited to) the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**No servers**: Don’t underestimate the savings in time and energy you can
    get by offloading infrastructure management to your cloud provider.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified scaling**: It’s usually very easy to define the scaling behavior
    of your serverless components by using clearly defined maximum instances, for
    example.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low barrier to entry**: These components are usually extremely easy to set
    up and run, allowing you and your team members to focus on writing high-quality
    code, logic, and models.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural integration points**: Serverless tools are often nice to use for
    handovers between other tools and components. Their ease of setup means you can
    be up and running with simple jobs that pass data or trigger other services in
    no time.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplified serving**: Some serverless tools are excellent for providing a
    serving layer to your ML models. The scaling and low barrier to entry mentioned
    previously mean you can quickly create a very scalable service that provides predictions
    upon request or upon being triggered by some other event.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the best and most widely used examples of serverless functionality is
    **AWS Lambda**, which allows us to write programs in a variety of languages with
    a simple web browser interface or through our usual development tools, and then
    have them run completely independently of any infrastructure that’s been set up.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Lambda is an amazing low-entry-barrier solution to getting some code up and
    running and scaling it up. However, it is very much aimed at creating simple APIs
    that can be hit over an HTTP request. Deploying your ML model with Lambda is particularly
    useful if you are aiming for an event- or request-driven system.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: To see this in action, let’s build a basic system that takes incoming image
    data as an HTTP request with a JSON body and returns a similar message containing
    the classification of the data using a pre-built Scikit-Learn model. This walkthrough
    is based on the AWS example at [https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/](https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: For this, we can save a lot of time by leveraging templates already built and
    maintained as part of the AWS **Serverless Application Model** (**SAM**) framework
    ([https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-sam-launches-machine-learning-inference-templates-for-aws-lambda/)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: To install the AWS SAM CLI on your relevant platform, follow the instructions
    at [https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.xhtml).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s perform the following steps to set up a template serverless deployment
    for hosting and serving a ML model that classifies images of handwritten digits:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must run the `sam init` command and select the AWS `Quick Start Templates`
    option:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will then be offered a choice of `AWS Quick Start` Application templates
    to use; select option 15, `Machine Learning`:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next are the options for the Python runtime you want to use; in line with the
    rest of the book, we will use the Python 3.10 runtime:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'At the time of writing, the SAM CLI will then auto-select some options based
    on these choices, first the package type and then the dependency manager. You
    will then be asked to confirm the ML starter template you want to use. For this
    example, select `XGBoost Machine Learning API`:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The SAM CLI then helpfully asks about some options for configuring request
    tracing and monitoring; you can select yes or no depending on your own preferences.
    I have selected no for the purposes of this example. You can then give the solution
    a name; here I have gone with `mlewp-sam-ml-api`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, your command line will provide some helpful information about the
    installation and next steps:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that the preceding steps have created a template for an XGBoost-based
    system that classifies handwritten digits. For other applications and project
    use cases, you will need to adapt the source code of the template as you require.
    If you want to deploy this example, follow the next few steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must build the application container provided with the template.
    First, navigate to the top directory of your project, you can see the directory
    structure should be something like below. I have used the `tree` command to provide
    a clean outline of the directory structure in the command line:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we are in the top directory, we can run the `build` command. This
    requires that Docker is running in the background on your machine:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Upon a successful build, you should receive a success message similar to the
    following in your terminal:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, we can test the service locally to ensure that everything is working well
    with the mock data that’s supplied with the repository. This uses a JSON file
    that encodes a basic image and runs the inference step for the service. If this
    has worked, you will see an output that looks something like the following for
    your service:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In a real project, you would edit the source code for the solution in the `app.py`
    and other files as required before deploying up to the cloud. We will do this
    using the SAM CLI , with the understanding that if you want to automate this process,
    you can use the CI/CD processes and tools we discussed in several places in this
    book, especially in *Chapter 4*, *Packaging Up*. To deploy, you can use the guided
    deployment wizard with the CLI by running the `deploy` command, which will return
    the below output:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then have to configure the application for each of the provided elements.
    I have selected the defaults in most cases, but you can refer to the AWS documentation
    and make the choices most relevant to your project:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The previous step will generate a lot of data in the terminal; you can monitor
    this to see if there are any errors or issues. If the deployment was successful,
    then you should see some final metadata about the application that looks like
    this:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As a quick test to confirm the cloud-hosted solution is working, we can use
    a tool such as Postman to hit our shiny new ML API. Simply copy the `InferenceApi`
    URL from the output screen from *step 8* as the destination for the request, select
    **POST** for the request type, and then choose **binary** as the body type. Note
    that if you need to get the inference URL, again you can run the `sam list endpoints
    --output json` command in your terminal. Then, you can choose an image of a handwritten
    digit, or any other image for that matter, to send up to the API . You can do
    this in Postman either by selecting the **binary** body option and attaching an
    image file or you can copy in the encoded string of an image. In *Figure 6.14*,
    I have used the encoded string for the `body` key-value pair in the `events/event.json`
    file we used to test the function locally:![A screenshot of a computer  Description
    automatically generated](img/B19525_06_14.png)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 6.14: Calling our serverless ML endpoint with Postman. This uses an
    encoded example image as the body of the request that is provided with the SAM
    XGBoost ML API template.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can also test this more programmatically with a `curl` command like the
    following – just replace the encoded binary string of the image with the appropriate
    values, or indeed edit the command to point to a data binary if you wish, and
    you are good to go:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In this step and *step 9*, the body of the response from the Lambda function
    is as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: And that’s it – we have just built and deployed a simple serverless ML inference
    service on AWS!
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we will touch upon the final way of scaling our solutions
    that we will discuss in this chapter, which is using Kubernetes (K8s) and Kubeflow
    to horizontally scale containerized applications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Containerizing at scale with Kubernetes
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already covered how to use containers for building and deploying our
    ML solutions. The next step is understanding how to orchestrate and manage several
    containers to deploy and run applications at scale. This is where the open source
    tool **Kubernetes** (**K8s**)comes in.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s is an extremely powerful tool that provides a variety of different functionalities
    that help us create and manage very scalable containerized applications, including
    (but not limited to) the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Load Balancing**: K8s will manage routing incoming traffic to your containers
    for you so that the load is split evenly.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal Scaling**: K8s provides simple interfaces so that you can control
    the number of container instances you have at any one time, allowing you to scale
    massively if needed.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self Healing**: There is built-in management for replacing or rescheduling
    components that are not passing health checks.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Rollbacks**: K8s stores the history of your system so that you
    can revert to a previous working version if something goes wrong.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these features help ensure that your deployed solutions are robust and
    able to perform as required under all circumstances.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: K8s is designed to ensure the preceding features are embedded from the ground
    up by using a microservice architecture, with a control plane interacting with
    nodes (servers), each of which host pods (one or more containers) that run the
    components of your application.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The key thing that K8s gives you is the ability to scale your application based
    on load by creating replicas of the base solution. This is extremely useful if
    you are building services with API endpoints that could feasibly face surges in
    demand at different times. To learn about some of the ways you can do this, see
    [https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – The K8s architecture ](img/B19525_06_15.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: The K8s architecture.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'But what about ML? In this case, we can look to a newer piece of the K8s ecosystem:
    **Kubeflow**, which we learned how to use in *Chapter 5*, *Deployment Patterns
    and Tools*.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow styles itself as the *ML toolkit for K8s* ([https://www.kubeflow.org/](https://www.kubeflow.org/)),
    so as ML engineers, it makes sense for us to be aware of this rapidly developing
    solution. This is a very exciting tool and an active area of development.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The concept of horizontal scaling for K8s generally still applies here, but
    Kubeflow provides some standardized tools for converting the pipelines you build
    into standard K8s resources, which can then be managed and resourced in the ways
    described previously. This can help reduce *boilerplate* and lets us, as ML engineers,
    focus on building our modelling logic rather than setting up the infrastructure.
    We leveraged this when we built some example pipelines in *Chapter 5*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: We will explore Kubernetes in far more detail in *Chapter 8*, *Building an Example
    ML Microservice*, where we use it to scale out our own wrapped ML model in a REST
    API. This will complement nicely the work we have done in this chapter on higher-level
    abstractions that can be used for scaling out, especially in the *Spinning up
    serverless infrastructure* section. We will only touch on K8s and Kubeflow very
    briefly here, to make sure you are aware of these tools for your exploration.
    For more details on K8s and Kubeflow, consult the documentation. I would also
    recommend another Packt title called *Kubernetes in Production Best Practices*
    by *Aly Saleh* and *Murat Karslioglu*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on and discuss another very powerful toolkit for scaling out
    compute-intensive Python workloads, which has now become extremely popular across
    the ML engineering community and been used by organizations such as Uber, Amazon
    and even used by OpenAI for training their large language **Generative Pre-trained
    Transformer** (**GPT**) models, which we discuss at length in *Chapter 7*, *Deep
    Learning, Generative AI, and LLMOps*. Let’s meet **Ray**.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Ray
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ray** is a Python native distributed computing framework that was specifically
    designed to help ML engineers meet the needs of massive data and massively scalable
    ML systems. Ray has an ethos of making scalable compute available to every ML
    developer, and in doing this in a way such that you can run anywhere by abstracting
    out all interactions with underlying infrastructure. One of the unique features
    of Ray that is particularly interesting is that it has a distributed scheduler,
    rather than a scheduler or DAG creation mechanism that runs in a central process,
    like in Spark. At its core, Ray has been developed with compute-intensive tasks
    such as ML model training in mind from the beginning, which is slightly different
    from Apache Spark, which has data intensity in mind. You can therefore think about
    this in a simplified manner: if you need to process lots of data a couple of times,
    Spark; if you need to process one piece of data lots of times, Ray. This is just
    a heuristic so should not be followed strictly, but hopefully it gives you a helpful
    rule of thumb. As an example, if you need to transform millions and millions of
    rows of data in a large batch process, then it makes sense to use Spark, but if
    you want to train an ML model on the same data, including hyperparameter tuning,
    then Ray may make a lot of sense.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'The two tools can be used together quite effectively, with Spark transforming
    the feature set before feeding this into a Ray workload for ML training. This
    is taken care of in particular by the **Ray AI Runtime** (**AIR**), which has
    a series of different libraries to help scale different pieces of an ML solution.
    These include:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '**Ray Data**: Focused on providing data pre-processing and transformation primitives.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Train**: Facilitates large model training.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Tune**: Helps with scalable hyperparameter training.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray RLib**: Supports methods for the development of reinforcement learning
    models.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Batch Predictor**: For batch inference.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ray Serving**: For re al-time inference.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AIR framework provides a unified API through which to interact with all
    of these capabilities and nicely integrates with a huge amount of the standard
    ML ecosystem that you will be used to, and that we have leveraged in this book.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_16.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: The Ray AI runtime, from a presentation by Jules Damji from Anyscale:
    https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_17.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: The Ray architecture including the Raylet scheduler. From a presentation
    by Jules Damji: https://microsites.databricks.com/sites/default/files/2022-07/Scaling%20AI%20Workloads%20with%20the%20Ray%20Ecosystem.pdf.
    Reproduced with permission.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'The Ray Core API has a series of different objects that you leverage when using
    Ray in order to distribute your solution. The first is tasks, which are asynchronous
    items of work for the system to perform. To define a task, you can take a Python
    function like:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And then add the `@remote` decorator and then use the `.remote()` syntax in
    order to submit this task to the cluster. This is not a blocking function so will
    just return an ID that Ray uses to refer to the task in later computation steps
    ([https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832](https://www.youtube.com/live/XME90SGL6Vs?feature=share&t=832))
    :'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In the same vein, the Ray API can extend the same concepts to classes as well;
    in this case, these are called `Actors`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, Ray also has a distributed immutable object store. This is a smart
    way to have one shared data store across all the nodes of the cluster without
    shifting lots of data around and using up bandwidth. You can write to the object
    store with the following syntax:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: IMPORTANT NOTE
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: An Actor in this context is a service or stateful worker, a concept used in
    other distributed frameworks like Akka, which runs on the JVM and has bindings
    to Java and Scala.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Ray for ML
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started you can install Ray with AI Runtime, as well as some the hyperparameter
    optimization package, the central dashboard and a Ray enhanced XGBoost implementation,
    by running:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: IMPORTANT NOTE
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: a reminder here that whenver you see `pip install` in this book, you can also
    use Poetry as outlined in *Chapter 4*, *Packaging Up*. So, in this case, you would
    have the following commands after running `poetry new` `project_name:`
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s start by looking at Ray Train, which provides an API to a series of `Trainer`
    objects that helps facilitate distributed training. At the time of writing, Ray
    2.3.0 supports trainers across a variety of different frameworks including:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep learning**: Horovod, Tensorflow and PyTorch.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree based**: LightGBM and XGBoost.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other**: Scikit-learn, HuggingFace, and Ray’s reinforcement learning library
    RLlib.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a company  Description automatically generated](img/B19525_06_18.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.18: Ray Trainers as shown in the Ray docs at https://docs.ray.io/en/latest/train/train.xhtml.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first look at a tree-based learner example using XGBoost. Open up a
    script and begin adding to it; in the repo, this is called `getting_started_with_ray.py`.
    What follows is based on an introductory example given in the Ray documentation.
    First, we can use Ray to download one of the standard datasets; we could also
    have used `sklearn.datasets` or another source if we wanted to, like we have done
    elsewhere in the book:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that here we use the `ray.data.read_csv()` method, which returns a `PyArrow`
    dataset. The Ray API has methods for reading from other data formats as well such
    as JSON or Parquet, as well as from databases like MongoDB or your own custom
    data sources.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will define a preprocessing step that will standardize the features
    we want to use; for more information on feature engineering, you can check out
    *Chapter 3*, *From Model to Model Factory*:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then is the fun part where we define the `Trainer` object for the XGBoost model.
    This has several different parameters and inputs we will need to define shortly:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You’ll then see something like that shown in *Figure 6.19* as output if you
    run this code in a Jupyter notebook or Python script.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B19525_06_19.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: Outptut from parallel training of an XGBoost model using Ray.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'The `result` object contains tons of useful information; one of the attributes
    of it is called `metrics` and you can print this to reveal details about the end
    state of the run. Execute `print(result.metrics)` and you will see something like
    the following:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In the instantiation of the `XGBoostTrainer`, we defined some important scaling
    information that was omitted in the previous example; here it is:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The `num_workers` parameter tells Ray how many actors to launch, with each actor
    by default getting one CPU. The `use_gpu` flag is set to false since we are not
    using GPU acceleration here. Finally, by setting the `_max_cpu_fraction_per_node`
    parameter to `0.9` we have left some spare capacity on each CPU, which can be
    used for other operations.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, there were also some XGBoost specific parameters we
    supplied:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'If you wanted to use GPU acceleration for the XGBoost training you would add
    `tree_method`: `gpu_hist` as a key-value pair in this `params` dictionary.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![A line graph with blue and orange lines  Description automatically generated](img/B19525_06_20.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.20: A few experiments show how changing the number of workers and
    CPUs available per worker results in different XGBoost training times on the author’s
    laptop (an 8 core Macbook Pro).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss briefly how you can scale compute with Ray when working
    in environments other than your local machine.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your compute for Ray
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The examples we’ve seen so far use a local Ray cluster that is automatically
    set up on the first call to the Ray API. This local cluster grabs all the available
    CPUs on your machine and makes them available to execute work. Obviously, this
    will only get you so far. The next stage is to work with clusters that can scale
    to far larger numbers of available workers in order to get more speedup. You have
    a few options if you want to do this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '**On the cloud**: Ray provides the ability to deploy on to Google Cloud Platform
    and AWS resources, with Azure deployments handled by a community maintained solution.
    For more information on deploying and running Ray on AWS, you can check out its
    online documentation.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using Kubernetes**: We have already met Kubeflow in *Chapter 5*, *Deployment
    Patterns and Tools*, which is used to build Kubernetes enabled ML pipelines. And
    we have also discussed Kubernetes in the Containerizing at Scale with Kubernetes
    section in this chapter.. As mentioned there, Kubernetes is a container orchestration
    toolkit designed to create massively scalable solutions based on containers. If
    you want to work with Ray on Kubernetes, you can use the **KubeRay** project,
    [https://ray-project.github.io/kuberay/](https://ray-project.github.io/kuberay/).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The setup of Ray on either the cloud or Kubernetes mainly involves defining
    the cluster configuration and its scaling behaviour. Once you have done this,
    the beauty of Ray is that scaling your solution is as simple as editing the `ScalingConfig`
    object we used in the previous example, and you can keep all your other code the
    same. So, for example, if you have a 20-node CPU cluster, you could simply change
    the definition to the following and run it as before:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Scaling your serving layer with Ray
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have discussed the ways you can use Ray to distributed ML training jobs but
    now let’s have a look at how you can use Ray to help you scale your application
    layer. As mentioned before, Ray AIR provides some nice functionality for this
    that is badged under **Ray Serve**.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Ray Serve is a framework-agnostic library that helps you easily define ML endpoints
    based on your models. Like with the rest of the Ray API that we have interacted
    with, it has been built to provide easy interoperability and access to scaling
    without large development overheads.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Building on the examples from the previous few sections, let us assume we have
    trained a model, stored it in our appropriate registry, such as MLflow, and we
    have retrieved this model and have it in memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'In Ray Serve, we create **deployments** by using the `@ray.serve.deployments`
    decorator. These contain the logic we wish to use to process incoming API requests,
    including through any ML models we have built. As an example, let’s build a simple
    wrapper class that uses an XGBoost model like the one we worked with in the previous
    example to make a prediction based on some pre-processed feature data that comes
    in via the request object. First, the Ray documentation encourages the use of
    the Starlette requests library:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next we can define the simple class and use the `serve` decorator to define
    the service. I will assume that logic for pulling from MLflow or any other model
    storage location is wrapped into the utility function `get_model` in the following
    code block:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can then deploy this across an existing Ray cluster.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to Ray. We will now finish with a final discussion
    on *designing systems at scale* and then a summary of everything we have learned.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Designing systems at scale
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build on the ideas presented in *Chapter 5*, *Deployment Patterns and Tools,*
    and in this chapter, we should now consider some of the ways in which the scaling
    capabilities we have discussed can be employed to maximum effect in your ML engineering
    projects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea of scaling should be thought of in terms of providing an increase
    in the throughput of analyses or inferences or ultimate size of data that can
    be processed. There is no real difference in the kind of analyses or solution
    you can develop, at least in most cases. This means that applying scaling tools
    and techniques successfully is more dependent on selecting the correct processes
    that will benefit from them, even when we include any overheads that come from
    using these tools. That is what we will discuss now in this section, so that you
    have a few guiding principles to revisit when it comes to making your own scaling
    decisions.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in several places throughout this book, the pipelines you develop
    for your ML projects will usually have to have stages that cover the following
    tasks:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Ingestion/pre-processing
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering (if different from above)
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model inference
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application layer
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization or distribution can help in many of these steps but usually
    in some different ways. For ingestion/pre-processing, if you are operating in
    a large scheduled batch setting, then the ability to scale to larger datasets
    in a distrubuted manner is going to be of huge benefit. In this case, the use
    of Apache Spark will make sense. For feature engineering, similarly they main
    bottleneck is in processing large amounts of data once as we perform the transformations,
    so again Spark is useful for this. The compute-intensive steps for training ML
    models that we discussed in detail in *Chapter 3*, *From Model to Model Factory*,
    are very amenable to frameworks that are optimized for this intensive computation,
    irrespective of the data size. This is where Ray comes in as discussed in the
    previous sections. Ray will mean that you can also neatly parallelize your hyperparameter
    tuning if you need to do that too. Note that you could run these steps in Spark
    as well but Ray’s low task overheads and its distributed state management mean
    that it is particularly amenable to splitting up these compute-intensive tasks.
    Spark on the other hand has centralized state and schedule management. Finally,
    when it comes to the inference and application layers, where we produce and surface
    the results of the ML model, we need to think about the requirements for the specific
    use case. As an example, if you want to serve your model as a REST API endpoint,
    we showed in the previous section how Ray’s distribution model and API can help
    facilitate this very easily, but this would not make sense to do in Spark. If,
    however, the model results are to be produced in large batches, then Spark or
    Ray may make sense. Also, as alluded to in the feature engineering and ingestion
    steps, if the end result should be transformed in large batches as well, perhaps
    into a specific data model such as a star schema, then performing that transformation
    in Spark may again make sense due to the data scale requirements of this task.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Let’s make this a bit more concrete by considering a potential example taken
    from industry. Many organizations with a retail element will analyze transactions
    and customer data in order to determine whether the customer is likely to churn.
    Let’s explore some of the decisions we can make to design and develop this solution
    with a particular focus on the questions of scaling up using the tools and techniques
    we have covered in this chapter.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: First, we have the ingestion of the data. For this scenario, we will assume
    that the customer data, including interactions with different applications and
    systems, is processed at the end of the business day and numbers millions of records.
    This data contains numerical and categorical values and these need to be processed
    in order to feed into the downstream ML algorithm. If the data is partitioned
    by date, and maybe some other feature of the data, then this plays really naturally
    into the use of Spark, as you can read this into a Spark DataFrame and use the
    partitions to parallelize the data processing steps.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the feature engineering. If we are using a Spark DataFrame in
    the first step, then we can apply our transformation logic using the base PySpark
    syntax we have discussed earlier in this chapter. For example, if we want to apply
    some feature transformations available from Scikit-Learn or another ML library,
    we can wrap these in UDFs and apply at the scale we need to. The data can then
    be exported in our chosen data format using the PySpark API. For the customer
    churn model, this could mean a combination of encoding of categorical variables
    and scaling of numerical variables, in line with the techniques explored in *Chapter
    3*, *From Model to Model Factory*.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Switching into the training of the model, now are moving from the data-intensive
    to the compute-intensive tasks. This means it is natural to start using Ray for
    model training, as you can easily set up parallel tasks to train models with different
    hyperparameter settings and distribute the training steps as well. There are particular
    benefits to using Ray for training deep learning or tree-based models as these
    are algorithms that are amenable to parallelization. So, if we are performing
    classification using one of the available models in Spark ML, then this can be
    done in a few lines, but if we are using something else, we will likely need to
    start wrapping in UDFs. Ray is far more library-agnostic but again the benefits
    really come if we are using a neural network in PyTorch or TensorFlow or using
    XGBoost or LightGBM, as these more naturally parallelize.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Finally, onto the model inference step. In a batch setting, it is less clear
    who the winner is in terms of suggested framework here. Using UDFs or the core
    PySpark APIs, you can easily create a quite scalable batch prediction stage using
    Apache Spark and your Spark cluster. This is essentially because prediction on
    a large batch is really just another large-scale data transformation, where Spark
    excels. If, however, you wish to serve your model as an endpoint that can scale
    across a cluster, this is where Ray has very easy-to-use capabilities as shown
    in the *Scaling your serving layer with Ray* section. Spark does not have a facility
    for creating endpoints in this way and the scheduling and task overheads required
    to get a Spark job up and running mean that it would not be worth running Spark
    on small packets of data coming in as requests like this.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: For the customer churn example, this may mean that if we want to perform a churn
    classification on the whole customer base, Spark provides a nice way to process
    all of that data and leverage concepts like the underlying data partitions. You
    can still do this in Ray, but the lower-level API may mean it is slightly more
    work. Note that we can create this serving layer using many other mechanisms,
    as discussed in *Chapter 5*, *Deployment Patterns and Tools*, and the section
    on *Spinning up serverless infrastructure* in this chapter. *Chapter 8*, *Building
    an Example ML Microservice*, will also cover in detail how to use Kubernetes to
    scale out a deployment of an ML endpoint.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I have called the last stage the *application layer* to cover any “last
    mile” integrations between the output system and downstream systems in the solution.
    In this case, Spark does not really have a role to play since it can really be
    thought of as as a large-scale data transformation engine. Ray, on the other hand,
    has more of a philosophy of general Python acceleration, so if there are tasks
    that would benefit from parallelization in the backend of your applications, such
    as data retrieval, general calculations, simulation, or some other process, then
    the likelihood is you can still use Ray in some capacity, although there may be
    other tools available. So, in the customer churn example, Ray could be used for
    performing analysis at the level of individual customers and doing this in parallel
    before serving the results through a **Ray Serve** endpoint.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The point of going through this high-level example was to highlight the points
    along your ML engineering project where you can make choices about how to scale
    effectively. I like to say that there are often *no right answers, but very often
    wrong answers*. What I mean by this is that there are often several ways to build
    a good solution that are equally valid and may leverage different tools. The important
    thing is to avoid the biggest pitfalls and dead ends. Hopefully, the example gives
    some indication of how you can apply this thinking to scaling up your ML solutions.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: IMPORTANT NOTE
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'Although I have presented a lot of questions here in terms of Spark vs. Ray,
    with a nod to Kubernetes as a more *base infrastructure* scaling option, there
    is now the ability to combine Spark and Ray through the use of **RayDP**. This
    toolkit now allows you to run Spark jobs on Ray clusters, so it nicely allows
    you to still use Ray as your base scaling layer but then leveRage the Spark APIs
    and capabilities where it excels. RayDP was introduced in 2021 and is in active
    development, so this is definitely a capability to watch. For more information,
    see the project repository here: [https://github.com/oap-project/raydp](https://github.com/oap-project/raydp).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our look at how we can start to apply some of the scaling techniques
    we have discussed to our ML use cases.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: We will now finish the chapter with a brief summary of what we have covered
    in the last few pages.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to take the ML solutions we have built in
    the past few chapters and thought about how to scale them up to larger data volumes
    or higher numbers of requests for predictions. To do this, we mainly focused on
    **Apache Spark** as this is the most popular general-purpose engine for distributed
    computing. During our discussion of Apache Spark, we revisited some coding patterns
    and syntax we used previously in this book. By doing so, we developed a more thorough
    understanding of how and why to do certain things when developing in PySpark.
    We discussed the concept of **UDFs** in detail and how these can be used to create
    massively scalable ML workflows.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: After this, we explored how to work with Spark on the cloud, specifically through
    the **EMR** service provided by AWS. Then, we looked at some of the other ways
    we can scale our solutions; that is, through serverless architectures and horizontal
    scaling with containers. In the former case, we walked through how to build a
    service for serving an ML model using **AWS Lambda**. This used standard templates
    provided by the AWS SAM framework. We provided a high-level view of how to use
    K8s and Kubeflow to scale out ML pipelines horizontally, as well as some of the
    other benefits of using these tools. A section covering the Ray parallel computing
    framework then followed, showing how you can use its relatively simple API to
    scale compute on heterogenous clusters to supercharge your ML workflows. Ray is
    now one of the most important scalable computing toolkits for Python and has been
    used to train some of the largest models on the planet, including the GPT-4 model
    from OpenAI.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we are going to build on the ideas of scale here by discussing
    the largest ML models you can build: deep learning models, including **large language
    models** (**LLM**s). Everything we will discuss in this next chapter could only
    have been developed, and can often only be effectively utilized, by considering
    the techniques we have covered here. The question of scaling up your ML solutions
    will also be revisited in *Chapter 8*, *Building an Example ML Microservice*,
    where we will focus on the use of Kubernetes to horizontally scale an ML microservice.
    This complements nicely the work we have done here on scaling large batch workloads
    by showing you how to scale more real-time workloads. Also, in *Chapter 9*, *Building
    an Extract, Transform, Machine Learning Use Case*, many of the scaling discussions
    we have had here are prerequisites; so, everything we have covered here puts you
    in a good place to get the most from the rest of the book. So, armed with all
    this new knowledge, let’s go and explore the world of the largest models known.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussion with the author and other
    readers:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/mle](https://packt.link/mle)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code102810325355484.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
