- en: Chapter 5. Regression and Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。回归与分类
- en: In the previous chapter, we got familiar with supervised and unsupervised learning.
    Another standard taxonomy of the machine learning methods is based on the label
    is from continuous or discrete space. Even if the discrete labels are ordered,
    there is a significant difference, particularly how the goodness of fit metrics
    is evaluated.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们熟悉了监督学习和无监督学习。机器学习方法的另一种标准分类是基于标签来自连续或离散空间。即使离散标签是有序的，也存在显著差异，尤其是如何评估拟合优度指标。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the origin of the word regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解“回归”一词的起源
- en: Learning metrics for evaluating the goodness of fit in continuous and discrete
    space
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估连续和离散空间中拟合优度的学习指标
- en: Discussing how to write simple code in Scala for linear and logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论如何用Scala编写线性回归和逻辑回归的简单代码
- en: Learning about advanced concepts such as regularization, multiclass predictions,
    and heteroscedasticity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解高级概念，如正则化、多类预测和异方差性
- en: Discussing an example of MLlib application for regression tree analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论MLlib回归树分析的MLlib应用示例
- en: Learning about the different ways of evaluating classification models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解评估分类模型的不同方法
- en: What regression stands for?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归代表什么？
- en: 'While the word classification is intuitively clear, the word regression does
    not seem to imply a predictor of a continuous label. According to the Webster
    dictionary, regression is:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分类这个词直观上很清楚，但回归这个词似乎并不暗示一个连续标签的预测器。根据韦伯斯特词典，回归是：
- en: '*"a return to a former or less developed state."*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"回到以前或较不发达的状态。"*'
- en: 'It does also mention a special definition for statistics as *a measure of the
    relation between the mean value of one variable (for example, output) and corresponding
    values of other variables (for example, time and cost)*, which is actually correct
    these days. However, historically, the regression coefficient was meant to signify
    the hereditability of certain characteristics, such as weight and size, from one
    generation to another, with the hint of planned gene selection, including humans
    ([http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html)).
    More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished
    19th-century scientist in his own right, which was also widely criticized for
    the promotion of eugenics, had distributed packets of sweet pea seeds to seven
    friends. Each friend received seeds of uniform weight, but with substantial variation
    across the seven packets. Galton''s friends were supposed to harvest the next
    generation seeds and ship them back to him. Galton then proceeded to analyze the
    statistical properties of the seeds within each group, and one of the analysis
    was to plot the regression line, which always appeared to have the slope less
    than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance
    (5th ed.), New York: Macmillan and Company), as opposed to either *0*, in the
    case of no correlation and no inheritance; or *1*, in the case the total replication
    of the parent''s characteristics in the descendants. We will discuss why the coefficient
    of the regression line should always be less than *1* in the presence of noise
    in the data, even if the correlation is perfect. However, beyond the discussion
    and details, the origin of the term regression is partly due to planned breeding
    of plants and humans. Of course, Galton did not have access to PCA, Scala, or
    any other computing machinery at the time, which might shed more light on the
    differences between correlation and the slope of the regression line.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Continuous space and metrics
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As most of this chapter''s content will be dealing with trying to predict or
    optimize continuous variables, let''s first understand how to measure the difference
    in a continuous space. Unless a drastically new discovery is made pretty soon,
    the space we live in is a three-dimensional Euclidian space. Whether we like it
    or not, this is the world we are mostly comfortable with today. We can completely
    specify our location with three continuous numbers. The difference in locations
    is usually measured by distance, or a metric, which is a function of a two arguments
    that returns a single positive real number. Naturally, the distance, ![Continuous
    space and metrics](img/B04935_05_01F.jpg), between *X* and *Y* should always be
    equal or smaller than the sum of distances between *X* and *Z* and *Y* and *Z*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_02F.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: 'For any *X*, *Y*, and *Z*, which is also called triangle inequality. The two
    other properties of a metric is symmetry:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_03F.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: 'Non-negativity of distance:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_04F.jpg)![Continuous space and
    metrics](img/B04935_05_05F.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'Here, the metric is `0` if, and only if, *X=Y*. The ![Continuous space and
    metrics](img/B04935_05_06F.jpg) distance is the distance as we understand it in
    everyday life, the square root of the sum of the squared differences along each
    of the dimensions. A generalization of our physical distance is p-norm (*p = 2*
    for the ![Continuous space and metrics](img/B04935_05_06F.jpg) distance):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_07F.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'Here, the sum is the overall components of the *X* and *Y* vectors. If *p=1*,
    the 1-norm is the sum of absolute differences, or Manhattan distance, as if the
    only path from point *X* to point *Y* would be to move only along one of the components.
    This distance is also often referred to as ![Continuous space and metrics](img/B04935_05_08F.jpg)
    distance:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 05-1\. The ![Continuous space and metrics](img/B04935_05_08F.jpg) circle
    in two-dimensional space (the set of points exactly one unit from the origin (0,
    0))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a representation of a circle in a two-dimensional space:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Figure 05-2\. ![Continuous space and metrics](img/B04935_05_06F.jpg) circle
    in two-dimensional space (the set of points equidistant from the origin (0, 0)),
    which actually looks like a circle in our everyday understanding of distance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Another frequently used special case is ![Continuous space and metrics](img/B04935_05_09F.jpg),
    the limit when ![Continuous space and metrics](img/B04935_05_10F.jpg), which is
    the maximum deviation along any of the components, as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_11F.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: 'The equidistant circle for the ![Continuous space and metrics](img/B04935_05_09F.jpg)
    distance is shown in *Figure 05-3*:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous space and metrics](img/B04935_05_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 05-3\. ![Continuous space and metrics](img/B04935_05_09F.jpg) circle
    in two-dimensional space (the set of points equidistant from the origin (0, 0)).
    This is a square as the ![Continuous space and metrics](img/B04935_05_09F.jpg)
    metric is the maximum distance along any of the components.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: I'll consider the **Kullback-Leibler** (**KL**) distance later when I talk about
    classification, which measures the difference between two probability distributions,
    but it is an example of distance that is not symmetric and thus it is not a metric.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The metric properties make it easier to decompose the problem. Due to the triangle
    inequality, one can potentially reduce a difficult problem of optimizing a goal
    by substituting it by a set of problems by optimizing along a number of dimensional
    components of the problem separately.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines and Modeling"),
    *Data Pipelines and Modeling*, most complex machine learning problems can be reduced
    to optimization as our final goal is to optimize the whole process where the machine
    is involved as an intermediary or the complete solution. The metric can be explicit,
    such as error rate, or more indirect, such as **Monthly Active Users** (**MAU**),
    but the effectiveness of an algorithm is finally judged by how it improves some
    metrics and processes in our lives. Sometimes, the goals may consist of multiple
    subgoals, or other metrics such as maintainability and stability might eventually
    be considered, but essentially, we need to either maximize or minimize a continuous
    metric in one or other way.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rigor of the flow, let''s show how the linear regression can be formulated
    as an optimization problem. The classical linear regression needs to optimize
    the cumulative ![Linear regression](img/B04935_05_06F.jpg) error rate:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_16F.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Linear regression](img/B04935_05_17F.jpg) is the estimate given by
    a model, which, in the case of linear regression, is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_18F.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '(Other potential **loss functions** have been enumerated in [Chapter 3](ch03.xhtml
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*). As
    the ![Linear regression](img/B04935_05_06F.jpg) metric is a differentiable convex
    function of *a*, *b*, the extreme value can be found by equating the derivative
    of the cumulative error rate to `0`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_19F.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Computing the derivatives is straightforward in this case and leads to the
    following equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_21F.jpg)![Linear regression](img/B04935_05_22F.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'This can be solved to give:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_23F.jpg)![Linear regression](img/B04935_05_24F.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'Here, *avg()* denotes the average overall input records. Note that if *avg(x)=0*
    the preceding equation is reduced to the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_25F.jpg)![Linear regression](img/B04935_05_26F.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'So, we can quickly compute the linear regression coefficients using basic Scala
    operators (we can always make *avg(x)* to be zero by performing a ![Linear regression](img/B04935_05_27F.jpg)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Didn't I inform you previously that Scala is a very concise language? We just
    did linear regression with five lines of code, three of which were just data-generation
    statements.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Although there are libraries written in Scala for performing (multivariate)
    linear regression, such as Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)),
    which provides a more extensive functionality, it is nice to be able to use pure
    Scala functionality to get some simple statistical results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the problem of Mr. Galton, where he found that the regression
    line always has the slope of less than one, which implies that we should always
    regress to some predefined mean. I will generate the same points as earlier, but
    they will be distributed along the horizontal line with some predefined noise.
    Then, I will rotate the line by *45* degrees by doing a linear rotation transformation
    in the *xy*-space. Intuitively, it should be clear that if anything, *y* is strongly
    correlated with x and absent, the *y* noise should be nothing else but *x*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The slope is only `0.81`! Note that if one runs PCA on the `x1` and `y1` data,
    the first principal component is correctly along the diagonal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, I am giving a plot of (*x1, y1*) zipped here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Linear regression](img/B04935_05_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: Figure 05-4\. The regression curve slope of a seemingly perfectly correlated
    dataset is less than one. This has to do with the metric the regression problem
    optimizes (y-distance).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: I will leave it to the reader to find the reason why the slope is less than
    one, but it has to do with the specific question the regression problem is supposed
    to answer and the metric it optimizes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression optimizes the logit loss function with respect to *w*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04935_05_28F.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is binary (in this case plus or minus one). While there is no closed-form
    solution for the error minimization problem like there was in the previous case
    of linear regression, logistic function is differentiable and allows iterative
    algorithms that converge very fast.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression](img/B04935_05_29F.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Again, we can quickly concoct a Scala program that uses the gradient to converge
    to the value, where ![Logistic regression](img/B04935_05_30F.jpg) (we use the
    MLlib `LabeledPoint` data structure only for convenience of reading the data):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The logistic regression was reduced to only one line of Scala code! The last
    line was to normalize the weights—only the relative values are important to define
    the separating plane—to compare them to the one obtained with the MLlib in previous
    chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Stochastic Gradient Descent** (**SGD**) algorithm used in the actual
    implementation is essentially the same gradient descent, but optimized in the
    following ways:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The actual gradient is computed on a subsample of records, which may lead to
    faster conversion due to less rounding noise and avoid local minima.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The step—a fixed *0.1* in our case—is a monotonically decreasing function of
    the iteration as ![Logistic regression](img/B04935_05_31F.jpg), which might also
    lead to better conversion.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It incorporates regularization; instead of minimizing just the loss function,
    you minimize the sum of the loss function, plus some penalty metric, which is
    a function of model complexity. I will discuss this in the following section.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The regularization was originally developed to cope with ill-poised problems,
    where the problem was underconstrained—allowed multiple solutions given the data—or
    the data and the solution that contained too much noise (*A.N. Tikhonov*, *A.S.
    Leonov*, *A.G. Yagola. Nonlinear Ill-Posed Problems*, *Chapman and Hall*, *London*,
    *Weinhe*). Adding additional penalty function that skews a solution if it does
    not have a desired property, such as the smoothness in curve fitting or spectral
    analysis, usually solves the problem.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The choice of the penalty function is somewhat arbitrary, but it should reflect
    a desired skew in the solution. If the penalty function is differentiable, it
    can be incorporated into the gradient descent process; ridge regression is an
    example where the penalty is the ![Regularization](img/B04935_05_06F.jpg)metric
    for the weights or the sum of squares of the coefficients.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: MLlib currently implements ![Regularization](img/B04935_05_06F.jpg), ![Regularization](img/B04935_05_08F.jpg),
    and a mixture thereof called **Elastic Net**, as was shown in [Chapter 3](ch03.xhtml
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*. The
    ![Regularization](img/B04935_05_08F.jpg) regularization effectively penalizes
    for the number of non-zero entries in the regression weights, but has been known
    to have slower convergence. **Least Absolute Shrinkage and Selection Operator**
    (**LASSO**) uses the ![Regularization](img/B04935_05_08F.jpg) regularization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the uncertainty in underconstrained problems is to take
    the prior information that may be coming from domain experts into account. This
    can be done using Bayesian analysis and introducing additional factors into the
    posterior probability—the probabilistic rules are generally expressed as multiplication
    rather than sum. However, since the goal is often minimizing the log likelihood,
    the Bayesian correction can often be expressed as standard regularizer as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate regression
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is possible to minimize multiple metrics at the same time. While Spark only
    has a few multivariate analysis tools, other more traditional well-established
    packages come with **Multivariate Analysis of Variance** (**MANOVA**), a generalization
    of **Analysis of Variance** (**ANOVA**) method. I will cover ANOVA and MANOVA
    in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph Algorithms"), *Working
    with Graph Algorithms*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: For a practical analysis, we first need to understand if the target variables
    are correlated, for which we can use the PCA Spark implementation covered in [Chapter
    3](ch03.xhtml "Chapter 3. Working with Spark and MLlib"), *Working with Spark
    and MLlib*. If the dependent variables are strongly correlated, maximizing one
    leads to maximizing the other, and we can just maximize the first principal component
    (and potentially build a regression model on the second component to understand
    what drives the difference).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: If the targets are uncorrelated, building a separate model for each of them
    can pinpoint the important variables that drive either and whether these two sets
    are disjoint. In the latter case, we could build two separate models to predict
    each of the targets independently.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Heteroscedasticity
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the fundamental assumptions in regression approach is that the target
    variance is not correlated with either independent (attributes) or dependent (target)
    variables. An example where this assumption might break is counting data, which
    is generally described by **Poisson distribution**. For Poisson distribution,
    the variance is proportional to the expected value, and the higher values can
    contribute more to the final variance of the weights.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'While heteroscedasticity may or may not significantly skew the resulting weights,
    one practical way to compensate for heteroscedasticity is to perform a log transformation,
    which will compensate for it in the case of Poisson distribution:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/B04935_05_32F.jpg)![Heteroscedasticity](img/B04935_05_33F.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Some other (parametrized) transformations are the **Box-Cox transformation**:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/B04935_05_34F.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Heteroscedasticity](img/B04935_05_35F.jpg) is a parameter (the log
    transformation is a partial case, where ![Heteroscedasticity](img/B04935_05_36F.jpg))
    and Tuckey''s lambda transformation (for attributes between *0* and *1*):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![Heteroscedasticity](img/B04935_05_38F.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: These compensate for Poisson binomial distributed attributes or the estimates
    of the probability of success in a sequence of trails with potentially a mix of
    *n* Bernoulli distributions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Heteroscedasticity is one of the main reasons that logistic function minimization
    works better than linear regression with ![Heteroscedasticity](img/B04935_05_06F.jpg)
    minimization in a binary prediction problem. Let's consider discrete labels in
    more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Regression trees
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen classification trees in the previous chapter. One can build a
    recursive split-and-concur structure for a regression problem, where a split is
    chosen to minimize the remaining variance. Regression trees are less popular than
    decision trees or classical ANOVA analysis; however, let''s provide an example
    of a regression tree here as a part of MLlib:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The splits at each level are made to minimize the variance, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression trees](img/B04935_05_39F.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: which is equivalent to minimizing the ![Regression trees](img/B04935_05_06F.jpg)
    distances between the label values and their mean within each leaf summed over
    all the leaves of the node.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Classification metrics
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the label is discrete, the prediction problem is called classification. In
    general, the target can take only one of the values for each record (even though
    multivalued targets are possible, particularly for text classification problems
    to be considered in [Chapter 6](ch06.xhtml "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: If the discrete values are ordered and the ordering makes sense, such as *Bad*,
    *Worse*, *Good*, the discrete labels can be cast into integer or double, and the
    problem is reduced to regression (we believe if you are between *Bad* and *Worse*,
    you are definitely farther away from being *Good* than *Worse*).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'A generic metric to optimize is the misclassification rate is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/B04935_05_40F.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: However, if the algorithm can predict the distribution of possible values for
    the target, a more general metric such as the KL divergence or Manhattan can be
    used.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'KL divergence is a measure of information loss when probability distribution
    ![Classification metrics](img/B04935_05_44F.jpg) is used to approximate probability
    distribution ![Classification metrics](img/B04935_05_41F.jpg):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Classification metrics](img/B04935_05_42F.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: It is closely related to entropy gain split criteria used in the decision tree
    induction, as the latter is the sum of KL divergences of the node probability
    distribution to the leaf probability distribution over all leaf nodes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass problems
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the number of possible outcomes for target is larger than two, in general,
    we have to predict either the expected probability distribution of the target
    values or at least the list of ordered values—hopefully augmented by a rank variable,
    which can be used for additional analysis.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: While some algorithms, such as decision trees, can natively predict multivalued
    attributes. A common technique is to reduce the prediction of one of the *K* target
    values to *(K-1)* binary classification problems by choosing one of the values
    as the base and building *(K-1)* binary classifiers. It is usually a good idea
    to select the most populated level as the base.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the early days of machine learning, researchers were trying to imitate the
    functionality of the human brain. At the beginning of the 20th century, people
    thought that the human brain consisted entirely of cells that are called neurons—cells
    with long appendages called axons that were able to transmit signals by means
    of electric impulses. The AI researchers were trying to replicate the functionality
    of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted
    sum of its input values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Perceptron](img/B04935_05_43F.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: This is a very simplistic representation of the processes in the human brain—biologists
    have since then discovered other ways in which information is transferred besides
    electric impulses such as chemical ones. Moreover, they have found over 300 different
    types of cells that may be classified as neurons ([http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)).
    Also, the process of neuron firing is more complex than just linear transmission
    of voltages as it involves complex time patterns as well. Nevertheless, the concept
    turned out to be very productive, and multiple algorithms and techniques were
    developed for neural nets, or the sets of perceptions connected to each other
    in layers. Specifically, it can be shown that the neural network, with certain
    modification, where the step function is replaced by a logistic function in the
    firing equation, can approximate an arbitrary differentiable function with any
    desired precision.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'MLlib implements **Multilayer Perceptron Classifier** (**MLCP**) as an `org.apache.spark.ml.classification.MultilayerPerceptronClassifier`
    class:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generalization error and overfitting
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, how do we know that the model we have discussed is good? One obvious and
    ultimate criterion is its performance in practice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: One common problem that plagues the more complex models, such as decision trees
    and neural nets, is overfitting. The model can minimize the desired metric on
    the provided data, but does a very poor job on a slightly different dataset in
    practical deployments, Even a standard technique, when we split the dataset into
    training and test, the training for deriving the model and test for validating
    that the model works well on a hold-out data, may not capture all the changes
    that are in the deployments. For example, linear models such as ANOVA, logistic,
    and linear regression are usually relatively stable and less of a subject to overfitting.
    However, you might find that any particular technique either works or doesn't
    work for your specific domain.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Another case when generalization may fail is time-drift. The data may change
    over time significantly so that the model trained on the old data no longer generalizes
    on the new data in a deployment. In practice, it is always a good idea to have
    several models in production and constantly monitor their relative performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: I will consider standard ways to avoid overfitting such as hold out datasets
    and cross-validation in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph
    Algorithms"), *Working with Graph Algorithms* and model monitoring in [Chapter
    9](ch09.xhtml "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have all the necessary tools to look at more complex problems that are
    more commonly called the big data problems. Armed with standard statistical algorithms—I
    understand that I have not covered many details and I am completely ready to accept
    the criticism—there is an entirely new ground to explore where we do not have
    clearly defined records, the variables in the datasets may be sparse and nested,
    and we have to cover a lot of ground and do a lot of preparatory work just to
    get to the stage where we can apply the standard statistical models. This is where
    Scala shines best.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look more at working with unstructured data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
