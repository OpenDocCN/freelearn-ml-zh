- en: Chapter 5. Regression and Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。回归与分类
- en: In the previous chapter, we got familiar with supervised and unsupervised learning.
    Another standard taxonomy of the machine learning methods is based on the label
    is from continuous or discrete space. Even if the discrete labels are ordered,
    there is a significant difference, particularly how the goodness of fit metrics
    is evaluated.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们熟悉了监督学习和无监督学习。机器学习方法的另一种标准分类是基于标签来自连续或离散空间。即使离散标签是有序的，也存在显著差异，尤其是如何评估拟合优度指标。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the origin of the word regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解“回归”一词的起源
- en: Learning metrics for evaluating the goodness of fit in continuous and discrete
    space
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于评估连续和离散空间中拟合优度的学习指标
- en: Discussing how to write simple code in Scala for linear and logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论如何用Scala编写线性回归和逻辑回归的简单代码
- en: Learning about advanced concepts such as regularization, multiclass predictions,
    and heteroscedasticity
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解高级概念，如正则化、多类预测和异方差性
- en: Discussing an example of MLlib application for regression tree analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论MLlib回归树分析的MLlib应用示例
- en: Learning about the different ways of evaluating classification models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解评估分类模型的不同方法
- en: What regression stands for?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归代表什么？
- en: 'While the word classification is intuitively clear, the word regression does
    not seem to imply a predictor of a continuous label. According to the Webster
    dictionary, regression is:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分类这个词直观上很清楚，但回归这个词似乎并不暗示一个连续标签的预测器。根据韦伯斯特词典，回归是：
- en: '*"a return to a former or less developed state."*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"回到以前或较不发达的状态。"*'
- en: 'It does also mention a special definition for statistics as *a measure of the
    relation between the mean value of one variable (for example, output) and corresponding
    values of other variables (for example, time and cost)*, which is actually correct
    these days. However, historically, the regression coefficient was meant to signify
    the hereditability of certain characteristics, such as weight and size, from one
    generation to another, with the hint of planned gene selection, including humans
    ([http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html)).
    More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished
    19th-century scientist in his own right, which was also widely criticized for
    the promotion of eugenics, had distributed packets of sweet pea seeds to seven
    friends. Each friend received seeds of uniform weight, but with substantial variation
    across the seven packets. Galton''s friends were supposed to harvest the next
    generation seeds and ship them back to him. Galton then proceeded to analyze the
    statistical properties of the seeds within each group, and one of the analysis
    was to plot the regression line, which always appeared to have the slope less
    than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance
    (5th ed.), New York: Macmillan and Company), as opposed to either *0*, in the
    case of no correlation and no inheritance; or *1*, in the case the total replication
    of the parent''s characteristics in the descendants. We will discuss why the coefficient
    of the regression line should always be less than *1* in the presence of noise
    in the data, even if the correlation is perfect. However, beyond the discussion
    and details, the origin of the term regression is partly due to planned breeding
    of plants and humans. Of course, Galton did not have access to PCA, Scala, or
    any other computing machinery at the time, which might shed more light on the
    differences between correlation and the slope of the regression line.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 它也提到了对统计学的一种特殊定义，即*一个变量（例如，输出）的平均值与其他变量的对应值（例如，时间和成本）之间关系的一种度量*，这在当今实际上是正确的。然而，从历史上看，回归系数原本是用来表示某些特征（如体重和大小）从一代传到另一代的遗传性，暗示着有计划的基因选择，包括人类（[http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html)）。更具体地说，在1875年，查尔斯·达尔文的表亲、一位杰出的19世纪科学家高尔顿，也因为推广优生学而受到广泛批评，他向七个朋友分发了甜豌豆种子。每个朋友都收到了重量均匀的种子，但七个包装之间的差异很大。高尔顿的朋友们应该收获下一代种子并将它们寄回给他。高尔顿随后对每个组内种子的统计特性进行了分析，其中一项分析就是绘制回归线，这条线似乎总是具有小于1的斜率——具体引用的数字是0.33（高尔顿，F.
    (1894)，《自然遗传》（第5版），纽约：麦克米伦公司），与没有相关性且没有遗传的情况下的*0*相反；或者与在后代中完全复制父母特征的情况下的*1*相反。我们将在有噪声数据的情况下，即使相关性完美，回归线的系数为什么总是小于*1*进行讨论。然而，除了讨论和细节之外，回归这个术语的起源部分是由于植物和人类的计划育种。当然，高尔顿当时没有接触到主成分分析（PCA）、Scala或其他任何计算设备，这些设备可能会更多地阐明相关性和回归线斜率之间的差异。
- en: Continuous space and metrics
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续空间和度量
- en: 'As most of this chapter''s content will be dealing with trying to predict or
    optimize continuous variables, let''s first understand how to measure the difference
    in a continuous space. Unless a drastically new discovery is made pretty soon,
    the space we live in is a three-dimensional Euclidian space. Whether we like it
    or not, this is the world we are mostly comfortable with today. We can completely
    specify our location with three continuous numbers. The difference in locations
    is usually measured by distance, or a metric, which is a function of a two arguments
    that returns a single positive real number. Naturally, the distance, ![Continuous
    space and metrics](img/B04935_05_01F.jpg), between *X* and *Y* should always be
    equal or smaller than the sum of distances between *X* and *Z* and *Y* and *Z*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章的大部分内容将涉及尝试预测或优化连续变量，让我们首先了解如何测量连续空间中的差异。除非很快有重大发现，我们所处的空间是一个三维欧几里得空间。无论我们是否喜欢，这是我们今天主要感到舒适的世界。我们可以用三个连续的数字完全指定我们的位置。位置之间的差异通常用距离或度量来衡量，度量是一个关于两个参数的函数，它返回一个正实数。自然地，*X*和*Y*之间的距离，![连续空间和度量](img/B04935_05_01F.jpg)，应该总是等于或小于*X*和*Z*以及*Y*和*Z*之间的距离之和：
- en: '![Continuous space and metrics](img/B04935_05_02F.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_02F.jpg)'
- en: 'For any *X*, *Y*, and *Z*, which is also called triangle inequality. The two
    other properties of a metric is symmetry:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 *X*，*Y* 和 *Z*，这也被称为三角不等式。度量的另外两个性质是对称性：
- en: '![Continuous space and metrics](img/B04935_05_03F.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_03F.jpg)'
- en: 'Non-negativity of distance:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 距离的非负性：
- en: '![Continuous space and metrics](img/B04935_05_04F.jpg)![Continuous space and
    metrics](img/B04935_05_05F.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_04F.jpg)![连续空间和度量](img/B04935_05_05F.jpg)'
- en: 'Here, the metric is `0` if, and only if, *X=Y*. The ![Continuous space and
    metrics](img/B04935_05_06F.jpg) distance is the distance as we understand it in
    everyday life, the square root of the sum of the squared differences along each
    of the dimensions. A generalization of our physical distance is p-norm (*p = 2*
    for the ![Continuous space and metrics](img/B04935_05_06F.jpg) distance):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果且仅当 *X=Y* 时，度量是 `0`。![连续空间和度量](img/B04935_05_06F.jpg) 距离是我们对距离的日常理解，即每个维度平方差的和的平方根。我们物理距离的推广是
    p-范数 (*p = 2* 对于 ![连续空间和度量](img/B04935_05_06F.jpg) 距离)：
- en: '![Continuous space and metrics](img/B04935_05_07F.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_07F.jpg)'
- en: 'Here, the sum is the overall components of the *X* and *Y* vectors. If *p=1*,
    the 1-norm is the sum of absolute differences, or Manhattan distance, as if the
    only path from point *X* to point *Y* would be to move only along one of the components.
    This distance is also often referred to as ![Continuous space and metrics](img/B04935_05_08F.jpg)
    distance:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，总和是 *X* 和 *Y* 向量的整体分量。如果 *p=1*，1-范数是绝对差之和，或曼哈顿距离，就像从点 *X* 到点 *Y* 的唯一路径是只沿一个分量移动一样。这种距离也常被称为
    ![连续空间和度量](img/B04935_05_08F.jpg) 距离：
- en: '![Continuous space and metrics](img/B04935_05_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_01.jpg)'
- en: Figure 05-1\. The ![Continuous space and metrics](img/B04935_05_08F.jpg) circle
    in two-dimensional space (the set of points exactly one unit from the origin (0,
    0))
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-1\. 二维空间中的 ![连续空间和度量](img/B04935_05_08F.jpg) 圆（距离原点 (0, 0) 精确为一单位的点的集合）
- en: 'Here is a representation of a circle in a two-dimensional space:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，这是一个圆的表示：
- en: '![Continuous space and metrics](img/B04935_05_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_02.jpg)'
- en: Figure 05-2\. ![Continuous space and metrics](img/B04935_05_06F.jpg) circle
    in two-dimensional space (the set of points equidistant from the origin (0, 0)),
    which actually looks like a circle in our everyday understanding of distance.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-2\. ![连续空间和度量](img/B04935_05_06F.jpg) 圆在二维空间中（距离原点 (0, 0) 等距的点的集合），在我们对距离的日常理解中实际上看起来像一个圆。
- en: 'Another frequently used special case is ![Continuous space and metrics](img/B04935_05_09F.jpg),
    the limit when ![Continuous space and metrics](img/B04935_05_10F.jpg), which is
    the maximum deviation along any of the components, as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的特殊情况是 ![连续空间和度量](img/B04935_05_09F.jpg)，当 ![连续空间和度量](img/B04935_05_10F.jpg)
    时的极限，这是沿任何分量方向的最大偏差，如下所示：
- en: '![Continuous space and metrics](img/B04935_05_11F.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_11F.jpg)'
- en: 'The equidistant circle for the ![Continuous space and metrics](img/B04935_05_09F.jpg)
    distance is shown in *Figure 05-3*:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ![连续空间和度量](img/B04935_05_09F.jpg) 的距离，等距圆如图 05-3* 所示：
- en: '![Continuous space and metrics](img/B04935_05_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/B04935_05_03.jpg)'
- en: Figure 05-3\. ![Continuous space and metrics](img/B04935_05_09F.jpg) circle
    in two-dimensional space (the set of points equidistant from the origin (0, 0)).
    This is a square as the ![Continuous space and metrics](img/B04935_05_09F.jpg)
    metric is the maximum distance along any of the components.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-3\. ![连续空间和度量](img/B04935_05_09F.jpg) 圆在二维空间中（距离原点 (0, 0) 等距的点的集合）。这是一个正方形，因为
    ![连续空间和度量](img/B04935_05_09F.jpg) 度量是沿任何分量方向的最大距离。
- en: I'll consider the **Kullback-Leibler** (**KL**) distance later when I talk about
    classification, which measures the difference between two probability distributions,
    but it is an example of distance that is not symmetric and thus it is not a metric.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我谈到分类时，我会考虑 **Kullback-Leibler** （**KL**） 距离，它衡量两个概率分布之间的差异，但它是不对称的距离，因此不是度量。
- en: The metric properties make it easier to decompose the problem. Due to the triangle
    inequality, one can potentially reduce a difficult problem of optimizing a goal
    by substituting it by a set of problems by optimizing along a number of dimensional
    components of the problem separately.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 度量性质使得问题分解更容易。由于三角不等式，可以通过分别优化问题的多个维度分量来替换一个优化目标的困难问题。
- en: Linear regression
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: As explained in [Chapter 2](ch02.xhtml "Chapter 2. Data Pipelines and Modeling"),
    *Data Pipelines and Modeling*, most complex machine learning problems can be reduced
    to optimization as our final goal is to optimize the whole process where the machine
    is involved as an intermediary or the complete solution. The metric can be explicit,
    such as error rate, or more indirect, such as **Monthly Active Users** (**MAU**),
    but the effectiveness of an algorithm is finally judged by how it improves some
    metrics and processes in our lives. Sometimes, the goals may consist of multiple
    subgoals, or other metrics such as maintainability and stability might eventually
    be considered, but essentially, we need to either maximize or minimize a continuous
    metric in one or other way.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第2章](ch02.xhtml "第2章。数据管道和建模")*数据管道和建模*中所述，大多数复杂的机器学习问题都可以简化为优化，因为我们的最终目标是优化整个机器作为中介或完整解决方案的过程。指标可以是明确的，如误差率，或者更间接的，如**月活跃用户**（**MAU**），但算法的有效性最终是通过它如何改善我们生活中的某些指标和过程来评判的。有时，目标可能包括多个子目标，或者最终可能考虑维护性和稳定性等其他指标，但本质上，我们需要以某种方式最大化或最小化一个连续指标。
- en: 'For the rigor of the flow, let''s show how the linear regression can be formulated
    as an optimization problem. The classical linear regression needs to optimize
    the cumulative ![Linear regression](img/B04935_05_06F.jpg) error rate:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了严谨性，让我们展示如何将线性回归表述为一个优化问题。经典的线性回归需要优化累积![线性回归](img/B04935_05_06F.jpg)误差率：
- en: '![Linear regression](img/B04935_05_16F.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_16F.jpg)'
- en: 'Here, ![Linear regression](img/B04935_05_17F.jpg) is the estimate given by
    a model, which, in the case of linear regression, is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![线性回归](img/B04935_05_17F.jpg)是模型给出的估计值，在线性回归的情况下，如下所示：
- en: '![Linear regression](img/B04935_05_18F.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_18F.jpg)'
- en: '(Other potential **loss functions** have been enumerated in [Chapter 3](ch03.xhtml
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*). As
    the ![Linear regression](img/B04935_05_06F.jpg) metric is a differentiable convex
    function of *a*, *b*, the extreme value can be found by equating the derivative
    of the cumulative error rate to `0`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （其他潜在的**损失函数**已在[第3章](ch03.xhtml "第3章。使用Spark和MLlib")中列举，*使用Spark和MLlib*）。由于![线性回归](img/B04935_05_06F.jpg)指标是*a*、*b*的可微凸函数，可以通过将累积误差率的导数等于`0`来找到极值：
- en: '![Linear regression](img/B04935_05_19F.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_19F.jpg)'
- en: 'Computing the derivatives is straightforward in this case and leads to the
    following equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，计算导数是直接的，并导致以下方程：
- en: '![Linear regression](img/B04935_05_21F.jpg)![Linear regression](img/B04935_05_22F.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_21F.jpg)![线性回归](img/B04935_05_22F.jpg)'
- en: 'This can be solved to give:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以解出：
- en: '![Linear regression](img/B04935_05_23F.jpg)![Linear regression](img/B04935_05_24F.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_23F.jpg)![线性回归](img/B04935_05_24F.jpg)'
- en: 'Here, *avg()* denotes the average overall input records. Note that if *avg(x)=0*
    the preceding equation is reduced to the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*avg()*表示整体输入记录的平均值。请注意，如果*avg(x)=0*，则前面的方程简化为以下形式：
- en: '![Linear regression](img/B04935_05_25F.jpg)![Linear regression](img/B04935_05_26F.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_25F.jpg)![线性回归](img/B04935_05_26F.jpg)'
- en: 'So, we can quickly compute the linear regression coefficients using basic Scala
    operators (we can always make *avg(x)* to be zero by performing a ![Linear regression](img/B04935_05_27F.jpg)):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以快速使用基本的Scala运算符来计算线性回归系数（我们总是可以通过执行![线性回归](img/B04935_05_27F.jpg)来使*avg(x)*为零）：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Didn't I inform you previously that Scala is a very concise language? We just
    did linear regression with five lines of code, three of which were just data-generation
    statements.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前没有告诉你Scala是一种非常简洁的语言吗？我们只用五行代码就完成了线性回归，其中三行只是数据生成语句。
- en: Although there are libraries written in Scala for performing (multivariate)
    linear regression, such as Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)),
    which provides a more extensive functionality, it is nice to be able to use pure
    Scala functionality to get some simple statistical results.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有使用Scala编写的用于执行（多元）线性回归的库，例如Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze))，它提供了更广泛的功能，但能够使用纯Scala功能来获取一些简单的统计结果是非常好的。
- en: 'Let''s look at the problem of Mr. Galton, where he found that the regression
    line always has the slope of less than one, which implies that we should always
    regress to some predefined mean. I will generate the same points as earlier, but
    they will be distributed along the horizontal line with some predefined noise.
    Then, I will rotate the line by *45* degrees by doing a linear rotation transformation
    in the *xy*-space. Intuitively, it should be clear that if anything, *y* is strongly
    correlated with x and absent, the *y* noise should be nothing else but *x*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看加尔顿先生的问题，他发现回归线总是小于一的斜率，这意味着我们应该始终回归到某个预定义的均值。我将生成与之前相同的点，但它们将分布在水平线上，并带有一些预定义的噪声。然后，我将通过在
    *xy*-空间中进行线性旋转变换将线旋转 *45* 度。直观上，如果 *y* 与 *x* 强烈相关且不存在，那么 *y* 的噪声应该只是 *x*：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The slope is only `0.81`! Note that if one runs PCA on the `x1` and `y1` data,
    the first principal component is correctly along the diagonal.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率仅为 `0.81`！请注意，如果对 `x1` 和 `y1` 数据运行 PCA，第一个主成分将正确地沿着对角线。
- en: 'For completeness, I am giving a plot of (*x1, y1*) zipped here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我给出了 (*x1, y1*) 的一个图表：
- en: '![Linear regression](img/B04935_05_04.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/B04935_05_04.jpg)'
- en: Figure 05-4\. The regression curve slope of a seemingly perfectly correlated
    dataset is less than one. This has to do with the metric the regression problem
    optimizes (y-distance).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-4\. 一个看似完美相关的数据集的回归曲线斜率小于一。这与回归问题优化的度量（y 距离）有关。
- en: I will leave it to the reader to find the reason why the slope is less than
    one, but it has to do with the specific question the regression problem is supposed
    to answer and the metric it optimizes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给读者去寻找斜率小于一的原因，但这与回归问题应该回答的具体问题和它优化的度量有关。
- en: Logistic regression
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression optimizes the logit loss function with respect to *w*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归优化的是关于 *w* 的对数损失函数：
- en: '![Logistic regression](img/B04935_05_28F.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04935_05_28F.jpg)'
- en: Here, *y* is binary (in this case plus or minus one). While there is no closed-form
    solution for the error minimization problem like there was in the previous case
    of linear regression, logistic function is differentiable and allows iterative
    algorithms that converge very fast.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 是二元的（在这种情况下是正负一）。虽然与线性回归的先前情况不同，没有闭合形式的解来最小化误差问题，但逻辑函数是可微的，允许快速收敛的迭代算法。
- en: 'The gradient is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度如下：
- en: '![Logistic regression](img/B04935_05_29F.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04935_05_29F.jpg)'
- en: 'Again, we can quickly concoct a Scala program that uses the gradient to converge
    to the value, where ![Logistic regression](img/B04935_05_30F.jpg) (we use the
    MLlib `LabeledPoint` data structure only for convenience of reading the data):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以快速编写一个 Scala 程序，使用梯度收敛到值，如图 ![逻辑回归](img/B04935_05_30F.jpg)（我们只使用 MLlib
    的 `LabeledPoint` 数据结构来方便读取数据）：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The logistic regression was reduced to only one line of Scala code! The last
    line was to normalize the weights—only the relative values are important to define
    the separating plane—to compare them to the one obtained with the MLlib in previous
    chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归被简化为只有一行 Scala 代码！最后一行是为了归一化权重——只有相对值对于定义分离平面很重要——以便与之前章节中用 MLlib 获得的值进行比较。
- en: 'The **Stochastic Gradient Descent** (**SGD**) algorithm used in the actual
    implementation is essentially the same gradient descent, but optimized in the
    following ways:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实际实现中使用的 **随机梯度下降**（**SGD**）算法本质上与梯度下降相同，但以下方面进行了优化：
- en: The actual gradient is computed on a subsample of records, which may lead to
    faster conversion due to less rounding noise and avoid local minima.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际梯度是在记录的子样本上计算的，这可能导致由于更少的舍入噪声和避免局部最小值而加快转化。
- en: The step—a fixed *0.1* in our case—is a monotonically decreasing function of
    the iteration as ![Logistic regression](img/B04935_05_31F.jpg), which might also
    lead to better conversion.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长——在我们的例子中是固定的 *0.1*——是迭代的单调递减函数，如图 ![逻辑回归](img/B04935_05_31F.jpg)，这也可能导致更好的转化。
- en: It incorporates regularization; instead of minimizing just the loss function,
    you minimize the sum of the loss function, plus some penalty metric, which is
    a function of model complexity. I will discuss this in the following section.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含了正则化；你不仅最小化损失函数，还最小化损失函数的总和加上一些惩罚度量，这是一个关于模型复杂度的函数。我将在下一节讨论这个问题。
- en: Regularization
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: The regularization was originally developed to cope with ill-poised problems,
    where the problem was underconstrained—allowed multiple solutions given the data—or
    the data and the solution that contained too much noise (*A.N. Tikhonov*, *A.S.
    Leonov*, *A.G. Yagola. Nonlinear Ill-Posed Problems*, *Chapman and Hall*, *London*,
    *Weinhe*). Adding additional penalty function that skews a solution if it does
    not have a desired property, such as the smoothness in curve fitting or spectral
    analysis, usually solves the problem.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化最初是为了应对病态问题而开发的，在这些问题中，问题被约束不足——给定数据允许有多个解，或者数据和包含过多噪声的解（*A.N. Tikhonov*,
    *A.S. Leonov*, *A.G. Yagola. 非线性病态问题*, *Chapman and Hall*, *London*, *Weinhe*）。添加额外的惩罚函数，如果解不具有期望的性质，如曲线拟合或频谱分析中的平滑性，通常可以解决问题。
- en: The choice of the penalty function is somewhat arbitrary, but it should reflect
    a desired skew in the solution. If the penalty function is differentiable, it
    can be incorporated into the gradient descent process; ridge regression is an
    example where the penalty is the ![Regularization](img/B04935_05_06F.jpg)metric
    for the weights or the sum of squares of the coefficients.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚函数的选择在一定程度上是任意的，但它应该反映对解的期望偏斜。如果惩罚函数是可微分的，它可以被纳入梯度下降过程；岭回归就是一个例子，其中惩罚是权重或系数平方和的度量。
- en: MLlib currently implements ![Regularization](img/B04935_05_06F.jpg), ![Regularization](img/B04935_05_08F.jpg),
    and a mixture thereof called **Elastic Net**, as was shown in [Chapter 3](ch03.xhtml
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*. The
    ![Regularization](img/B04935_05_08F.jpg) regularization effectively penalizes
    for the number of non-zero entries in the regression weights, but has been known
    to have slower convergence. **Least Absolute Shrinkage and Selection Operator**
    (**LASSO**) uses the ![Regularization](img/B04935_05_08F.jpg) regularization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib 目前实现了 ![正则化](img/B04935_05_06F.jpg), ![正则化](img/B04935_05_08F.jpg)，以及称为
    **弹性网络** 的混合形式，如 [第 3 章](ch03.xhtml "第 3 章。使用 Spark 和 MLlib") 中所示，*使用 Spark 和
    MLlib*。![正则化](img/B04935_05_08F.jpg) 正则化有效地惩罚了回归权重中非零项的数量，但已知其收敛速度较慢。**最小绝对收缩和选择算子**（**LASSO**）使用了
    ![正则化](img/B04935_05_08F.jpg) 正则化。
- en: Another way to reduce the uncertainty in underconstrained problems is to take
    the prior information that may be coming from domain experts into account. This
    can be done using Bayesian analysis and introducing additional factors into the
    posterior probability—the probabilistic rules are generally expressed as multiplication
    rather than sum. However, since the goal is often minimizing the log likelihood,
    the Bayesian correction can often be expressed as standard regularizer as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少约束不足问题不确定性的方法是考虑可能来自领域专家的先验信息。这可以通过贝叶斯分析实现，并在后验概率中引入额外的因素——概率规则通常表示为乘法而不是加法。然而，由于目标通常是最大化对数似然，贝叶斯校正通常也可以表示为标准正则化器。
- en: Multivariate regression
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元回归
- en: It is possible to minimize multiple metrics at the same time. While Spark only
    has a few multivariate analysis tools, other more traditional well-established
    packages come with **Multivariate Analysis of Variance** (**MANOVA**), a generalization
    of **Analysis of Variance** (**ANOVA**) method. I will cover ANOVA and MANOVA
    in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph Algorithms"), *Working
    with Graph Algorithms*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同时最小化多个指标是可能的。虽然 Spark 只有少数多元分析工具，但其他更传统且已建立的包包含 **多元方差分析**（**MANOVA**），它是 **方差分析**（**ANOVA**）方法的推广。我将在
    [第 7 章](ch07.xhtml "第 7 章。使用图算法") 中介绍 ANOVA 和 MANOVA，*使用图算法*。
- en: For a practical analysis, we first need to understand if the target variables
    are correlated, for which we can use the PCA Spark implementation covered in [Chapter
    3](ch03.xhtml "Chapter 3. Working with Spark and MLlib"), *Working with Spark
    and MLlib*. If the dependent variables are strongly correlated, maximizing one
    leads to maximizing the other, and we can just maximize the first principal component
    (and potentially build a regression model on the second component to understand
    what drives the difference).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际分析，我们首先需要了解目标变量是否相关，这可以通过 [第 3 章](ch03.xhtml "第 3 章。使用 Spark 和 MLlib") 中涵盖的
    PCA Spark 实现来完成，*使用 Spark 和 MLlib*。如果因变量高度相关，最大化一个会导致最大化另一个，我们只需最大化第一个主成分（并且可能基于第二个成分构建回归模型来了解导致差异的因素）。
- en: If the targets are uncorrelated, building a separate model for each of them
    can pinpoint the important variables that drive either and whether these two sets
    are disjoint. In the latter case, we could build two separate models to predict
    each of the targets independently.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标变量不相关，为每个目标构建一个单独的模型可以确定驱动它们的变量，以及这两个集合是否互斥。在后一种情况下，我们可以构建两个单独的模型来独立预测每个目标。
- en: Heteroscedasticity
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异方差性
- en: One of the fundamental assumptions in regression approach is that the target
    variance is not correlated with either independent (attributes) or dependent (target)
    variables. An example where this assumption might break is counting data, which
    is generally described by **Poisson distribution**. For Poisson distribution,
    the variance is proportional to the expected value, and the higher values can
    contribute more to the final variance of the weights.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法中的一个基本假设是目标方差与独立（属性）或依赖（目标）变量不相关。一个可能打破这个假设的例子是计数数据，它通常由**泊松分布**描述。对于泊松分布，方差与期望值成正比，高值可以更多地贡献到权重的最终方差。
- en: 'While heteroscedasticity may or may not significantly skew the resulting weights,
    one practical way to compensate for heteroscedasticity is to perform a log transformation,
    which will compensate for it in the case of Poisson distribution:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然异方差性可能会或可能不会显著地扭曲结果权重，但一种补偿异方差性的实用方法是进行对数变换，这在泊松分布的情况下会进行补偿：
- en: '![Heteroscedasticity](img/B04935_05_32F.jpg)![Heteroscedasticity](img/B04935_05_33F.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/B04935_05_32F.jpg)![异方差性](img/B04935_05_33F.jpg)'
- en: 'Some other (parametrized) transformations are the **Box-Cox transformation**:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他（参数化）变换是**Box-Cox变换**：
- en: '![Heteroscedasticity](img/B04935_05_34F.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/B04935_05_34F.jpg)'
- en: 'Here, ![Heteroscedasticity](img/B04935_05_35F.jpg) is a parameter (the log
    transformation is a partial case, where ![Heteroscedasticity](img/B04935_05_36F.jpg))
    and Tuckey''s lambda transformation (for attributes between *0* and *1*):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![异方差性](img/B04935_05_35F.jpg)是一个参数（对数变换是部分情况，其中![异方差性](img/B04935_05_36F.jpg)）和Tuckey的lambda变换（对于介于
    *0* 和 *1* 之间的属性）：
- en: '![Heteroscedasticity](img/B04935_05_38F.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/B04935_05_38F.jpg)'
- en: These compensate for Poisson binomial distributed attributes or the estimates
    of the probability of success in a sequence of trails with potentially a mix of
    *n* Bernoulli distributions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些补偿了泊松二项分布的属性或一系列试验中成功概率的估计，这些试验可能包含混合的 *n* 个伯努利分布。
- en: Heteroscedasticity is one of the main reasons that logistic function minimization
    works better than linear regression with ![Heteroscedasticity](img/B04935_05_06F.jpg)
    minimization in a binary prediction problem. Let's consider discrete labels in
    more details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差性是逻辑函数最小化在二元预测问题中比线性回归使用![异方差性](img/B04935_05_06F.jpg)最小化效果更好的主要原因之一。让我们更详细地考虑离散标签。
- en: Regression trees
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树
- en: 'We have seen classification trees in the previous chapter. One can build a
    recursive split-and-concur structure for a regression problem, where a split is
    chosen to minimize the remaining variance. Regression trees are less popular than
    decision trees or classical ANOVA analysis; however, let''s provide an example
    of a regression tree here as a part of MLlib:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中看到了分类树。可以为回归问题构建一个递归的分割和合并结构，其中分割是为了最小化剩余的方差。回归树不如决策树或经典方差分析分析流行；然而，让我们在这里提供一个回归树的例子，作为MLlib的一部分：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The splits at each level are made to minimize the variance, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个级别的分割是为了最小化方差，如下所示：
- en: '![Regression trees](img/B04935_05_39F.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![回归树](img/B04935_05_39F.jpg)'
- en: which is equivalent to minimizing the ![Regression trees](img/B04935_05_06F.jpg)
    distances between the label values and their mean within each leaf summed over
    all the leaves of the node.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于最小化每个叶子中标签值与其均值之间的![回归树](img/B04935_05_06F.jpg)距离，并将这些距离加总到节点的所有叶子中。
- en: Classification metrics
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类度量
- en: If the label is discrete, the prediction problem is called classification. In
    general, the target can take only one of the values for each record (even though
    multivalued targets are possible, particularly for text classification problems
    to be considered in [Chapter 6](ch06.xhtml "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签是离散的，预测问题被称为分类。一般来说，每个记录的目标只能取一个值（尽管可能存在多值目标，尤其是在[第6章](ch06.xhtml "第6章。处理非结构化数据")中考虑的文本分类问题），*处理非结构化数据*。
- en: If the discrete values are ordered and the ordering makes sense, such as *Bad*,
    *Worse*, *Good*, the discrete labels can be cast into integer or double, and the
    problem is reduced to regression (we believe if you are between *Bad* and *Worse*,
    you are definitely farther away from being *Good* than *Worse*).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果离散值是有序的，并且排序有意义，例如 *Bad*，*Worse*，*Good*，则可以将离散标签转换为整数或双精度浮点数，问题就简化为回归（我们相信如果你在
    *Bad* 和 *Worse* 之间，你肯定比 *Worse* 更远离 *Good*）。
- en: 'A generic metric to optimize is the misclassification rate is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化的通用度量是误分类率，如下所示：
- en: '![Classification metrics](img/B04935_05_40F.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![分类度量](img/B04935_05_40F.jpg)'
- en: However, if the algorithm can predict the distribution of possible values for
    the target, a more general metric such as the KL divergence or Manhattan can be
    used.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果算法可以预测目标可能值的分布，则可以使用更通用的度量，如KL散度或曼哈顿距离。
- en: 'KL divergence is a measure of information loss when probability distribution
    ![Classification metrics](img/B04935_05_44F.jpg) is used to approximate probability
    distribution ![Classification metrics](img/B04935_05_41F.jpg):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度是使用概率分布 ![分类度量](img/B04935_05_44F.jpg) 近似概率分布 ![分类度量](img/B04935_05_41F.jpg)
    时信息损失的一个度量：
- en: '![Classification metrics](img/B04935_05_42F.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![分类度量](img/B04935_05_42F.jpg)'
- en: It is closely related to entropy gain split criteria used in the decision tree
    induction, as the latter is the sum of KL divergences of the node probability
    distribution to the leaf probability distribution over all leaf nodes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它与决策树归纳中使用的熵增益分割标准密切相关，因为后者是所有叶节点节点概率分布到叶概率分布的KL散度的总和。
- en: Multiclass problems
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类问题
- en: If the number of possible outcomes for target is larger than two, in general,
    we have to predict either the expected probability distribution of the target
    values or at least the list of ordered values—hopefully augmented by a rank variable,
    which can be used for additional analysis.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标可能的结果数量大于两个，通常我们不得不预测目标值的期望概率分布，或者至少是按顺序排列的值列表——最好是通过一个排名变量来增强，该变量可以用于额外的分析。
- en: While some algorithms, such as decision trees, can natively predict multivalued
    attributes. A common technique is to reduce the prediction of one of the *K* target
    values to *(K-1)* binary classification problems by choosing one of the values
    as the base and building *(K-1)* binary classifiers. It is usually a good idea
    to select the most populated level as the base.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些算法，如决策树，可以原生地预测多值属性。一种常见的技术是通过选择一个值作为基准，将一个*K*个目标值的预测减少到*(K-1)*个二元分类问题，构建*(K-1)*个二元分类器。通常选择最密集的级别作为基准是一个好主意。
- en: Perceptron
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'In the early days of machine learning, researchers were trying to imitate the
    functionality of the human brain. At the beginning of the 20th century, people
    thought that the human brain consisted entirely of cells that are called neurons—cells
    with long appendages called axons that were able to transmit signals by means
    of electric impulses. The AI researchers were trying to replicate the functionality
    of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted
    sum of its input values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的早期，研究人员试图模仿人脑的功能。20世纪初，人们认为人脑完全由称为神经元的细胞组成——具有长突起的细胞称为轴突，能够通过电脉冲传递信号。AI研究人员试图通过感知器来复制神经元的功能，感知器是一个基于其输入值的线性加权和的激活函数：
- en: '![Perceptron](img/B04935_05_43F.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/B04935_05_43F.jpg)'
- en: This is a very simplistic representation of the processes in the human brain—biologists
    have since then discovered other ways in which information is transferred besides
    electric impulses such as chemical ones. Moreover, they have found over 300 different
    types of cells that may be classified as neurons ([http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)).
    Also, the process of neuron firing is more complex than just linear transmission
    of voltages as it involves complex time patterns as well. Nevertheless, the concept
    turned out to be very productive, and multiple algorithms and techniques were
    developed for neural nets, or the sets of perceptions connected to each other
    in layers. Specifically, it can be shown that the neural network, with certain
    modification, where the step function is replaced by a logistic function in the
    firing equation, can approximate an arbitrary differentiable function with any
    desired precision.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对人类大脑中过程的一种非常简化的表示——自那时起，生物学家已经发现了除了电脉冲之外的其他信息传递方式，例如化学脉冲。此外，他们已经发现了300多种可能被归类为神经元的细胞类型（[http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)）。此外，神经元放电的过程比仅仅电压的线性传输要复杂得多，因为它还涉及到复杂的时间模式。尽管如此，这个概念最终证明是非常有成效的，为神经网络或层间相互连接的感知集开发了许多算法和技术。具体来说，可以证明，通过某些修改，在放电方程中将步函数替换为逻辑函数，神经网络可以以任何所需的精度逼近任意可微函数。
- en: 'MLlib implements **Multilayer Perceptron Classifier** (**MLCP**) as an `org.apache.spark.ml.classification.MultilayerPerceptronClassifier`
    class:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib实现了**多层感知器分类器**（**MLCP**）作为一个`org.apache.spark.ml.classification.MultilayerPerceptronClassifier`类：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generalization error and overfitting
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化误差和过拟合
- en: So, how do we know that the model we have discussed is good? One obvious and
    ultimate criterion is its performance in practice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何知道我们讨论的模型是好的呢？一个明显且最终的标准是其实际表现。
- en: One common problem that plagues the more complex models, such as decision trees
    and neural nets, is overfitting. The model can minimize the desired metric on
    the provided data, but does a very poor job on a slightly different dataset in
    practical deployments, Even a standard technique, when we split the dataset into
    training and test, the training for deriving the model and test for validating
    that the model works well on a hold-out data, may not capture all the changes
    that are in the deployments. For example, linear models such as ANOVA, logistic,
    and linear regression are usually relatively stable and less of a subject to overfitting.
    However, you might find that any particular technique either works or doesn't
    work for your specific domain.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的难题困扰着更复杂的模型，例如决策树和神经网络，那就是过拟合问题。模型可以在提供的数据上最小化期望的指标，但在实际部署中，对稍微不同的数据集的处理却非常糟糕。即使是一个标准的技巧，当我们把数据集分成训练集和测试集，用于推导模型的训练和验证模型在保留数据集上表现良好的测试，也可能无法捕捉到部署中的所有变化。例如，线性模型如方差分析、逻辑回归和线性回归通常相对稳定，不太容易过拟合。然而，你可能会发现，任何特定的技术对于你的特定领域要么有效要么无效。
- en: Another case when generalization may fail is time-drift. The data may change
    over time significantly so that the model trained on the old data no longer generalizes
    on the new data in a deployment. In practice, it is always a good idea to have
    several models in production and constantly monitor their relative performance.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能导致泛化失败的情况是时间漂移。数据可能会随着时间的推移而显著变化，以至于在旧数据上训练的模型在部署中的新数据上不再泛化。在实践中，始终拥有几个生产模型并持续监控它们的相对性能总是一个好主意。
- en: I will consider standard ways to avoid overfitting such as hold out datasets
    and cross-validation in [Chapter 7](ch07.xhtml "Chapter 7. Working with Graph
    Algorithms"), *Working with Graph Algorithms* and model monitoring in [Chapter
    9](ch09.xhtml "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在[第7章](ch07.xhtml "第7章。使用图算法")*使用图算法*和[第9章](ch09.xhtml "第9章。Scala中的NLP")*Scala中的NLP*中考虑避免过拟合的标准方法，如保留数据集和交叉验证，以及模型监控。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We now have all the necessary tools to look at more complex problems that are
    more commonly called the big data problems. Armed with standard statistical algorithms—I
    understand that I have not covered many details and I am completely ready to accept
    the criticism—there is an entirely new ground to explore where we do not have
    clearly defined records, the variables in the datasets may be sparse and nested,
    and we have to cover a lot of ground and do a lot of preparatory work just to
    get to the stage where we can apply the standard statistical models. This is where
    Scala shines best.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了所有必要的工具来研究更复杂的问题，这些问题通常被称为大数据问题。凭借标准的统计算法——我明白我没有涵盖很多细节，我完全准备好接受批评——我们有了全新的领域去探索，在这个领域中我们没有明确定义的记录，数据集中的变量可能稀疏且嵌套，我们必须覆盖大量领域并做大量准备工作，仅仅为了达到可以应用标准统计模型的地步。这正是Scala最擅长的领域。
- en: In the next chapter, we will look more at working with unstructured data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨如何处理非结构化数据。
