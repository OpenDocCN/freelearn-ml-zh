<html><head></head><body>
		<div id="_idContainer096">
			<h1 id="_idParaDest-117" class="chapter-number"><a id="_idTextAnchor116"/>6</h1>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor117"/>Distributed Training of Machine Learning Models</h1>
			<p>When it comes to <strong class="bold">Machine Learning </strong>(<strong class="bold">ML</strong>) model training, the primary goal for a data scientist or ML practitioner is to train the optimal model based on the relevant data to address the business use case. While this goal is of primary importance, the panacea is to perform this task as quickly and effectively as possible. So, <em class="italic">how do we speed up model training?</em> Moreover, sometimes, the data or the model might be too big to fit into a single GPU memory. <em class="italic">So how do we prevent out-of-memory (</em><span class="No-Break"><em class="italic">OOM) errors?</em></span></p>
			<p>The simplest answer to this question is to basically throw more compute resources, in other words, more CPUs and GPUs, at<a id="_idIndexMarker523"/> the problem. This is essentially using larger compute hardware and is commonly referred to as a <strong class="bold">scale-up</strong> strategy. However, there is only a finite number of CPUs and GPUs that can be squeezed into a server. So, sometimes a <strong class="bold">scale-out</strong> strategy is<a id="_idIndexMarker524"/> required, whereby we add more servers into the mix, essentially distributing the workload across multiple physical <span class="No-Break">compute resources.</span></p>
			<p>Nonetheless, spreading the model training workload across more CPUs or GPUs, and even across more compute servers, will definitely speed up the overall training process. Making use of either a scale-up, scale-out, or a combination of the two strategies also adds further complexity to the overall orchestration and configuration of the model training activity. Therefore, this chapter will help navigate these challenges to help overcome the additional complexities imposed by the <strong class="bold">distributed training</strong> process by covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Building ML systems <span class="No-Break">using AWS</span></li>
				<li>Introducing the fundamentals of <span class="No-Break">distributed training</span></li>
				<li>Executing a distributed training workload <span class="No-Break">on AWS</span></li>
			</ul>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>You should have the following prerequisites before getting started with <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A web browser (for the best experience, it is recommended that you use a Chrome or <span class="No-Break">Firefox browser)</span></li>
				<li>Access to the AWS account that you used in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>An AWS account (if you are unfamiliar with how to get started with an AWS account, you can go to this <span class="No-Break">link </span><a href="https://aws.amazon.com/getting-started/"><span class="No-Break">https://aws.amazon.com/getting-started/</span></a><span class="No-Break">)</span></li>
				<li>Access to the SageMaker Studio development environment that we created in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span></li>
				<li>Example Jupyter notebooks for this chapter are provided in the companion GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06"><span class="No-Break">https://github.com/PacktPublishing/Applied-Machine-Learning-and-High-Performance-Computing-on-AWS/tree/main/Chapter06</span></a><span class="No-Break">)</span></li>
			</ul>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Building ML systems using AWS</h1>
			<p>Before we can explore the<a id="_idIndexMarker525"/> fundamentals of how to implement the distributed training strategies highlighted at the outset, we first need to level set and understand <a id="_idIndexMarker526"/>just how the ML model training exercise can be performed on the AWS platform. Once we understand how AWS handles model training, we can further expand on this concept to address the concept of <span class="No-Break">distributed training.</span></p>
			<p>To assist ML practitioners in building ML systems, AWS provides the SageMaker (<a href="https://aws.amazon.com/sagemaker/">https://aws.amazon.com/sagemaker/</a>) service. While SageMaker is a single AWS service, it comprises multiple modules that <a id="_idIndexMarker527"/>map specifically to an ML task. For example, SageMaker provides the Training job component that is purpose-built to take care of the heavy lifting and scaling of the model training task. ML practitioners can use SageMaker Training jobs to essentially provision ephemeral compute environments or clusters to handle the model training task. Essentially, all the ML practitioner needs to do is specify a few configuration parameters, and SageMaker Training jobs takes care of the rest. For example, we need to supply the following four <span class="No-Break">basic parameters:</span></p>
			<ul>
				<li>The URL for the S3 bucket, which contains the model training, testing, and optionally, the <span class="No-Break">validation data</span></li>
				<li>The type and quantity of ML compute instances required to perform the model <span class="No-Break">training task</span></li>
				<li>The location of the S3 bucket to store the <span class="No-Break">trained model</span></li>
				<li>The location, either locally or on S3, where the model training code <span class="No-Break">is stored</span></li>
			</ul>
			<p>The following code <a id="_idIndexMarker528"/>snippet shows just how easy it can be to formalize these<a id="_idIndexMarker529"/> four basic requirements into a SageMaker Training <span class="No-Break">job request:</span></p>
			<pre class="source-code">
...
from sagemaker.pytorch import PyTorch
estimator = PyTorch(entry_point='train.py',
                    source_dir='src',
                    role=role,
                    instance_count=1,
                    instance_type='ml.p3.2xlarge',
                    framework_version='1.8.0',
                    py_version='py3',
                    sagemaker_session=sagemaker_session,
                    hyperparameters={'epochs':10,
                                     'batch_size':32,
                                     'lr':3e-5,
                                     'gamma': 0.7},
                   )
...</pre>
			<p>Using this code snippet, we basically tell SageMaker that we want to use the built-in PyTorch estimator by declaring the <strong class="source-inline">estimator</strong> variable to use the PyTorch framework. We then supply the necessary requirements, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">entry_point</strong>: This is the location of the <span class="No-Break">training script.</span></li>
				<li><strong class="source-inline">instance_count</strong>: This is the number of compute servers to be provisioned in <span class="No-Break">the cluster.</span></li>
				<li><strong class="source-inline">instance_type</strong>: This is the<a id="_idIndexMarker530"/> type of compute resources required in <a id="_idIndexMarker531"/>the cluster. In this example, we are specifying the <span class="No-Break"><strong class="source-inline">ml.p3.16xlarge</strong></span><span class="No-Break"> instances.</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on the SageMaker PyTorch estimator, as well as how to leverage the SageMaker SDK to instantiate the <a id="_idIndexMarker532"/>estimator, see the AWS documentation on how to use PyTorch on SageMaker (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html">https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html</a>) and <a id="_idIndexMarker533"/>the SageMaker SDK <span class="No-Break">documentation (</span><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/sagemaker.pytorch.html#pytorch-estimator</span><span class="No-Break">).</span></p>
			<p>Once we have declared the estimator, we specify the location of the training and validation datasets on S3, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
...
from sagemaker.inputs import TrainingInput
train = TrainingInput(s3_train_data,
                      content_type='image/png',
                      input_mode='File')
val = TrainingInput(s3_val_data,
                    content_type='image/png',
                    input_mode='File')
...</pre>
			<p>We then call the <strong class="source-inline">fit()</strong> method of the PyTorch estimator to tell SageMaker to execute the Training job on the datasets, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
...
estimator.fit({'train':train, 'val': val})
...</pre>
			<p>Behind the scenes, SageMaker creates an ephemeral compute cluster, executes the training task on these resources, and then produces the resultant optimized model, which is then stored <a id="_idIndexMarker534"/>on Amazon S3. After this task has been <a id="_idIndexMarker535"/>performed, SageMaker tears down the ephemeral cluster with users only paying for the resources consumed during the <span class="No-Break">training time.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more detailed information as to how SageMaker Training jobs work behind the scenes, see the AWS <span class="No-Break">documentation (</span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html</span></a><span class="No-Break">).</span></p>
			<p>So now that we have a basic idea of how a model training exercise can be performed using Amazon SageMaker, <em class="italic">how can we improve on model training time and essentially speed up the process by leveraging more </em><span class="No-Break"><em class="italic">compute resources?</em></span></p>
			<p>To answer this question, we can very easily implement a scale-up strategy with the SageMaker Training job. All we have to do is change the <strong class="source-inline">instance_type</strong> parameter for the <strong class="source-inline">estimator</strong> variable from <strong class="source-inline">ml.p3.2xlarge</strong> to <strong class="source-inline">ml.p3.16xlarge</strong>. By doing this, we are increasing the size of, or scaling up the compute resource from, an instance with 8 vCPUs, 61 GB of RAM, and a single GPU to an instance with 64 vCPUs, 488 GB of RAM, and 8 GPUs. </p>
			<p>The resultant code now looks <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
...
from sagemaker.pytorch import PyTorch
estimator = PyTorch(entry_point='train.py',
                    source_dir='src',
                    role=role,
                    instance_count=1,
                    <strong class="bold">instance_type='ml.p3.16xlarge'</strong>,
                    framework_version='1.8.0',
                    py_version='py3',
                    sagemaker_session=sagemaker_session,
                    hyperparameters={'epochs':10,
                                     'batch_size':32,
                                     'lr':3e-5,
                                     'gamma': 0.7},
                   )
...</pre>
			<p>So, as you can see, implementing a scale-up strategy is very straightforward when using SageMaker. However, <em class="italic">what if we need to go beyond the maximum capacity of an accelerated </em><span class="No-Break"><em class="italic">computing instance?</em></span></p>
			<p>Well, then, we would <a id="_idIndexMarker536"/>need to implement a scale-out strategy and distribute<a id="_idIndexMarker537"/> the training process across multiple compute nodes. In the next section, we will explore how to apply a scale-out strategy using distributed training for SageMaker <span class="No-Break">Training jobs.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor120"/>Introducing the fundamentals of distributed training</h1>
			<p>In the previous section, we highlighted how to<a id="_idIndexMarker538"/> apply a scale-up strategy to SageMaker Training jobs by simply specifying a large compute resource or large instance type. Implementing a scale-out strategy for the training process is just as straightforward. For example, we can increase the <strong class="source-inline">instance_count</strong> parameter for the Training job from <strong class="source-inline">1</strong> to <strong class="source-inline">2</strong> and thereby instruct SageMaker to instantiate an ephemeral cluster consisting of 2 compute resources as opposed to 1 node. Thus, the following code snippet highlights what the <strong class="source-inline">estimator</strong> variable configuration will <span class="No-Break">look </span><span class="No-Break"><a id="_idIndexMarker539"/></span><span class="No-Break">like:</span></p>
			<pre class="source-code">
...
from sagemaker.pytorch import PyTorch
estimator = PyTorch(entry_point='train.py',
                    source_dir='src',
                    role=role,
                    <strong class="bold">instance_count=2</strong>,
                    instance_type='ml.p3.2xlarge',
                    framework_version='1.8.0',
                    py_version='py3',
                    sagemaker_session=sagemaker_session,
                    hyperparameters={'epochs':10,
                                     'batch_size':32,
                                     'lr':3e-5,
                                     'gamma': 0.7},
                   )
...</pre>
			<p>Unfortunately, simply changing the number of compute instances doesn’t completely solve the problem. As already stated at the outset of this chapter, applying a scale-out strategy to distribute the SageMaker Training job adds further complexity to the overall orchestration and configuration of the model training activity. For example, when distributing the training activity, we need to also take into consideration the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><em class="italic">How do the various compute resources get access to and share </em><span class="No-Break"><em class="italic">the data?</em></span></li>
				<li><em class="italic">How do the compute resources communicate and coordinate their training tasks with </em><span class="No-Break"><em class="italic">each other?</em></span></li>
			</ul>
			<p>So, while simply specifying the number of compute resources for the Training job will create an appropriately sized training cluster, we also need to inform SageMaker about our <strong class="bold">model placement strategy</strong>. A model <a id="_idIndexMarker540"/>placement strategy directs SageMaker on exactly how the model is allocated or assigned to each of the compute resources within each node of the cluster. In turn, SageMaker uses the placement strategy to coordinate how each node interacts with the <a id="_idIndexMarker541"/>associated training data and, subsequently, how each node coordinates and communicates its portion of the model <span class="No-Break">training task.</span></p>
			<p><em class="italic">So how do we determine an effective model placement strategy </em><span class="No-Break"><em class="italic">for SageMaker?</em></span></p>
			<p>The best way to answer this question is to understand what placement strategies are available and dissect how each of these strategies works. There are numerous placement strategies that are specific to each of the different training frameworks, as well as many open source<a id="_idIndexMarker542"/> frameworks. Nonetheless, all these different mechanisms can be grouped into two specific categories of <a id="_idIndexMarker543"/>placement strategies, namely <strong class="bold">data parallel</strong> and <span class="No-Break"><strong class="bold">model parallel</strong></span><span class="No-Break">.</span></p>
			<p>Let’s explore SageMaker’s data parallel <span class="No-Break">strategy first.</span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Reviewing the SageMaker distributed data parallel strategy</h2>
			<p>As the name implies, a data <a id="_idIndexMarker544"/>parallel strategy focuses on the placement of model’s training data. So, in order to fully understand just how this placement strategy is applied to the data, we should start by understanding just how a training activity interacts with the training data to optimize an <span class="No-Break">ML model.</span></p>
			<p>When we train an ML model, we basically create a training loop that applies the specific ML algorithm to the data. Typically, as is the case with deep learning algorithms, we break the data into smaller groups of records or batches of data. These batches are referred to as <strong class="bold">mini batches</strong>. We then <a id="_idIndexMarker545"/>pass each of these mini batches forward through the neural network layers, and then backward to optimize or train the model parameters. After completing one mini batch, we then apply the same procedure to the next mini batch, and so on, until we’ve run through the entirety of the data. A full execution of this process on the <a id="_idIndexMarker546"/>entire dataset is referred to as an <strong class="bold">epoch</strong>. Depending on the type of algorithm and, of course, the use case, we may have to run the algorithm to train the model for multiple epochs. It’s this task that invariably takes the most time, and it’s this task that we essentially want to improve on to reduce the overall time it takes to train <span class="No-Break">the model.</span></p>
			<p>So, when using a data parallel placement strategy, we are basically converting the training task from a sequential process to a parallel process. Instead of running the algorithm through a mini batch, then the next mini batch, then the next mini batch sequentially, we are now giving<a id="_idIndexMarker547"/> each individual mini batch to a separate compute resource, with each compute resource, in turn, running the model training process on its individual mini batch. Therefore, with each compute resource running its own mini batch at the same time, we are effectively distributing the epoch across multiple compute resources in parallel, therefore, improving the overall model training time. Consequently, using the data parallel technique does, however, introduce an additional complication, namely parallel optimization of all the weighted parameters for <span class="No-Break">the model.</span></p>
			<p>To further elaborate on this problem, we’ll use the example depicted in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>, detailing the individual node parameters for <span class="No-Break">the model:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18493_06_001.jpg" alt="Figure 6.1 – Individual node parameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Individual node parameters</p>
			<p>As you can see from the example in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>, we have four individual compute resources or nodes. Using the data parallel placement strategy, we have effectively placed a copy of the model algorithm onto each of these nodes and distributed the mini batch data across these resources. Now, each node computes the gradient reduction operation, in this case, the sum of weighted parameters of the model, on its individual mini batch of the data, in essence, producing four unique gradient calculation results. Since our goal is not to<a id="_idIndexMarker548"/> produce four separate representations of an optimized model but rather a single optimized model, <em class="italic">how do we combine the results across all </em><span class="No-Break"><em class="italic">four nodes?</em></span></p>
			<p>To solve this problem, SageMaker provides an additional optimization operation to the distributed training process<a id="_idIndexMarker549"/> and uses the <strong class="bold">AllReduce</strong> algorithm to share and communicate the results across the cluster. By including this additional step in the process, we can see the outcome in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B18493_06_002.jpg" alt="Figure 6.2 – Shared node parameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Shared node parameters</p>
			<p>From <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.2</em>, we can see that the AllReduce step takes the results from each node’s gradient reduction operation and shares the results with every other node, ensuring that each node’s representation of the model includes the optimizations from all the other nodes. This, therefore, guarantees that a single, consistent model is produced as the final output from the distributed <span class="No-Break">training process.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">While the initial concept of using the AllReduce step for distributed deep learning was initially introduced in a blog post by Baidu Research, the original post has since been removed. So, for more background <a id="_idIndexMarker550"/>information on the intricacies of how it works, you can review the open source implementation called <span class="No-Break"><strong class="bold">Horovod</strong></span><span class="No-Break"> (</span><a href="https://eng.uber.com/horovod/"><span class="No-Break">https://eng.uber.com/horovod/</span></a><span class="No-Break">).</span></p>
			<p>Up until this point in the chapter, we have used the broad term <em class="italic">compute resources</em> to denote CPUs, GPUs, and physical compute instances. However, when implementing a successful data parallel placement<a id="_idIndexMarker551"/> strategy, it’s important to fully understand just how SageMaker uses these compute resources to execute the distributed <span class="No-Break">training workload.</span></p>
			<p>Succinctly, when we instruct SageMaker to implement a data parallel placement strategy for the Training job, we are instructing SageMaker to distribute or shard the mini batches across all of the compute resources. SageMaker, in turn, shards the training data into all the GPUs and, from time to time, the CPUs on all of the instance types specified in the <strong class="source-inline">estimator</strong> object. So, in order to make this concept easier to understand, <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> illustrates an example of just how SageMaker handles <span class="No-Break">the task:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B18493_06_003.jpg" alt="Figure 6.3 – Data parallel training task on SageMaker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Data parallel training task on SageMaker</p>
			<p>As you can see from the example shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em> when calling the <strong class="source-inline">fit()</strong> method for the SageMaker <strong class="source-inline">estimator</strong> object, specifying two GPU instances (each with eight GPUs), SageMaker creates a copy of the model training <a id="_idIndexMarker552"/>routine, or <strong class="bold">training script</strong>, on both of the instances in the ephemeral cluster. Accordingly, each training script is further copied onto each GPU on <a id="_idIndexMarker553"/>each of the instances. Once every GPU has a copy of the training script, each GPU, in turn, executes the training script on its individually sharded mini batch of the <span class="No-Break">training data.</span></p>
			<p>The GPU worker then trains the model copy to produce a set of optimal parameters, which then shares with the other GPU workers, both within the same instance, as well as the other GPUs in the second<a id="_idIndexMarker554"/> instance, using the <strong class="bold">AllReduce optimization algorithm</strong>. This process is repeated in parallel until all of the specified epochs are completed, after which the ephemeral SageMaker cluster is dismantled, and the <strong class="source-inline">fit()</strong> operation is reported as being successful. The result is a single optimized model, stored on S3, and a reduction of the overall model training time, by a factor of 16, which is the total number of GPUs allocated to <span class="No-Break">the task.</span></p>
			<p>So, as you can see, we have effectively reduced the overall training time by implementing a data parallel placement strategy. While this strategy is effective for large training datasets and is a good start at reducing the time it takes to train a model, this strategy, however, doesn’t always work when we have large models <span class="No-Break">to train.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Since the model parallel placement strategy is essentially distributing the model’s computational graph or model pipeline<a id="_idIndexMarker555"/> across multiple nodes, this placement strategy is often referred to as a <strong class="bold">pipeline </strong><span class="No-Break"><strong class="bold">parallel</strong></span><span class="No-Break"> strategy.</span></p>
			<p>To address the challenge of reducing the overall training time when we have large ML models with millions or even billions of trainable parameters, we can review how to implement a model parallel <span class="No-Break">placement strategy.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Reviewing the SageMaker model data parallel strategy</h2>
			<p>The data parallel strategy<a id="_idIndexMarker556"/> was largely conceived as a method of reducing the overall model training time, where at the time of its induction, training on large quantities of data imposed the biggest challenge. However, with the invention of large-scale <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models, such as <strong class="bold">Generative Pre-trained Transformers</strong> (<strong class="bold">GPT</strong>) from OpenAI (<a href="https://openai.com/blog/gpt-3-apps/">https://openai.com/blog/gpt-3-apps/</a>), training a large ML model will billions of parameters now imposes the biggest challenge. Basically, these models are far too large to fit into the GPU’s <span class="No-Break">onboard memory.</span></p>
			<p>Now that we have a rudimentary idea of just how SageMaker implements a data parallel placement strategy, it’s relatively easy to translate the concept to a model parallel placement strategy. The key difference is that while a data parallel strategy breaks the large quantity of training data into smaller shards, the model parallel placement strategy performs a similar trick to a large ML model, allowing these smaller pieces of the model to fit into GPU <a id="_idIndexMarker557"/>memory. This also means that we don’t have to degrade the model’s capabilities by having to prune or <span class="No-Break">compress it.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em> highlights just how similar the model parallel execution is to a data parallel execution when SageMaker executes a Training job, using a model parallel <span class="No-Break">placement strategy:</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B18493_06_004.jpg" alt="Figure 6.4 – Model parallel training task on SageMaker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Model parallel training task on SageMaker</p>
			<p>You can see from <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.4</em> that when calling the <strong class="source-inline">fit()</strong> method for the SageMaker <strong class="source-inline">estimator</strong> object, just like the data parallel example in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em>, SageMaker allocates a copy of the model training script to each GPU on each of the two instances. However, instead of distributing the same copy of the model to each GPU worker, as was the case with the data parallel placement strategy, SageMaker splits the model into smaller pieces, or model partitions, assigning each model partition to a <span class="No-Break">GPU worker.</span></p>
			<p>To coordinate how the training of each model partition, SageMaker implements a <strong class="bold">pipeline scheduler</strong>. In the same way that<a id="_idIndexMarker558"/> the AllReduce optimization algorithm coordinates parameter optimizations across different GPU workers, the pipeline scheduler ensures that as each batch of data is fed into the model and computation for each partition of the model is correctly coordinated and scheduled between all the GPU workers. This ensures that the matrix calculations for each layer, during both the forward and backward passes over<a id="_idIndexMarker559"/> these network partitions, happen in accordance with the overall structure of the model architecture. For example, the scheduler would ensure that the mathematical calculations for layer two are executed before layer three on the forward pass and that layer three’s gradient calculation occurs before layer two. Once all the desired epochs have been executed, essentially both the forward and backward passes through the model architecture over the entirety of the training data, SageMaker dismantles the ephemeral cluster and stores the optimized model <span class="No-Break">on S3.</span></p>
			<p>In summation, the data parallel placement strategy was originally conceived to reduce the overall training time of a model by sharing the data and parallelizing the execution across multiple compute resources. The primary motivation behind a model parallel placement strategy is to address large models that don’t fit into the compute resource’s memory. This then begs the question as to whether it’s possible to combine both the data parallel and model parallel placement strategies to reduce the overall training time for both large models, as well as large datasets in a <span class="No-Break">distributed fashion.</span></p>
			<p>Let’s review this hybrid <span class="No-Break">methodology next.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor123"/>Reviewing a hybrid data parallel and model parallel strategy</h2>
			<p>The fact that both the data <a id="_idIndexMarker560"/>parallel and model parallel placement strategies were created to address specific challenges, it is fundamentally impossible to combine both strategies into a unified, hybrid strategy. Essentially, both strategies only solve their specific issues by either sharding the data or sharding <span class="No-Break">the model.</span></p>
			<p>Fortunately, because the entirety of the Training job is orchestrated and managed by SageMaker, the ability to combine both strategies into a hybrid strategy is now possible. For example, if we review <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em>, we can visualize just how SageMaker allows us to execute a Training job that implements<a id="_idIndexMarker561"/> both the data parallel and model <a id="_idIndexMarker562"/>parallel placement <span class="No-Break">strategies independently:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18493_06_005.jpg" alt="Figure 6.5 – Independent data parallel and model parallel strategies on SageMaker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Independent data parallel and model parallel strategies on SageMaker</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em> illustrates taking the same number of compute instances, and essentially implementing a two-node data parallel placement strategy, along with a four-way model parallel placement strategy. This translates to creating two copies of the training script and assigning each copy to one of the two compute instances. We then execute the distributed training task using the data parallel placement strategy. While the training task is being executed, we further partition the specific copy of the model architecture across each of the GPUs in the individual compute instance, using the model parallel <span class="No-Break">placement strategy.</span></p>
			<p>So, while each of these placement strategies is unique in its approach, by using SageMaker, we can reap the benefits of both approaches to reduce the overall training time on large datasets, as well as large ML models. In the next section, we will review examples of how to practically implement each of the placement strategies on SageMaker, including an example of this <span class="No-Break">hybrid approach.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>Executing a distributed training workload on AWS</h1>
			<p>Now that we’ve been introduced to<a id="_idIndexMarker563"/> some of the fundamentals of distributed training and what happens behind the scenes when we leverage SageMaker to launch a distributed Training job, let’s explore how we can execute such a workload on AWS. Since we’ve reviewed two placement techniques, namely data parallel and model parallel, we will start by reviewing how to execute distributed data parallel training. After which, we will then review how to execute distributed model parallel training, but also include the hybrid methodology and include an independent data parallel placement strategy alongside the model <span class="No-Break">parallel example.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In this example, we<a id="_idIndexMarker564"/> leverage a <strong class="bold">Vision Transformer</strong> (<strong class="bold">ViT</strong>) model to address an image classification use case. Since the objective of this section is to showcase how to practically implement both the data parallel and model parallel placement strategies, we will not be diving into the particulars of the model itself but rather using it within the context of transfer learning. To learn more about the ViT model, please review the <em class="italic">Transformers for Image Recognition at Scale</em> <span class="No-Break">paper (</span><a href="https://arxiv.org/pdf/2010.11929.pdf"><span class="No-Break">https://arxiv.org/pdf/2010.11929.pdf</span></a><span class="No-Break">).</span></p>
			<p>Let’s get started with the data <span class="No-Break">parallel workload.</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor125"/>Executing distributed data parallel training on Amazon SageMaker</h2>
			<p>There are two <a id="_idIndexMarker565"/>crucial elements to executing<a id="_idIndexMarker566"/> a distributed Training job using the data parallel placement strategy <span class="No-Break">on SageMaker:</span></p>
			<ol>
				<li>Configuring the <span class="No-Break">backend cluster</span></li>
				<li>Configuring the model <span class="No-Break">training script</span></li>
			</ol>
			<p>In the next section, we will start by walking through an example of how to configure the backend ephemeral <span class="No-Break">SageMaker cluster.</span></p>
			<h3>Configuring the backend cluster</h3>
			<p>To get started with setting up the SageMaker cluster, we will be leveraging the same SageMaker Studio environment, along with the sample code from the companion GitHub repository, that <a id="_idIndexMarker567"/>we introduced in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you haven’t provisioned the SageMaker Studio environment, please refer back to the <em class="italic">Setting up EMR and SageMaker Studio</em> section in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span><span class="No-Break">.</span></p>
			<p>The following steps will walk you through setting up <span class="No-Break">the example:</span></p>
			<ol>
				<li value="1">Log into the AWS account that was used for <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Data Analysis,</em> examples, and open the SageMaker management <span class="No-Break">console (</span><a href="https://console.aws.amazon.com/sagemaker/home"><span class="No-Break">https://console.aws.amazon.com/sagemaker/home</span></a><span class="No-Break">).</span></li>
				<li>With the SageMaker management console open, use the left-hand navigation panel to click on the <strong class="bold">SageMaker Domain</strong> link. Under the <strong class="bold">Users</strong> section, you will see <strong class="bold">Name</strong> of the user, and the <strong class="bold">Launch app</strong> <span class="No-Break">drop-down box.</span></li>
				<li>Click the <strong class="bold">Launch app</strong> drop-down and select the <strong class="bold">Studio</strong> option to launch the <span class="No-Break">Studio IDE.</span></li>
				<li>Once the Studio environment is open, double-click on the <strong class="bold">Applied-Machine-Learning-and-High-Performance-Computing-on-AWS</strong> folder that we cloned in <a href="B18493_05.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <span class="No-Break"><em class="italic">Data Analysis</em></span><span class="No-Break">.</span></li>
				<li>Now, double-click on the <strong class="source-inline">Chapter06</strong> folder to get access to the example <span class="No-Break">Jupyter notebooks.</span></li>
				<li>Double-click on the <strong class="source-inline">1_distributed_data_parallel_training.ipynb</strong> file to launch <span class="No-Break">the notebook.</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">The notebook will initialize an <strong class="bold">ml.m5.xlarge</strong> compute instance with <strong class="bold">4 vCPUs</strong>, and <strong class="bold">16 GB</strong> of RAM to run a pre-configured <strong class="bold">PyTorch 1.8</strong> kernel. This instance type exceeds the free resource type allowed by the AWS Free Tier (<a href="https://aws.amazon.com/free">https://aws.amazon.com/free</a>) and will therefore incur AWS <span class="No-Break">usage costs.</span></p>
			<ol>
				<li value="7">Once the example notebook has been launched and the kernel has been started, click on the <strong class="bold">Kernel</strong> menu option, and select the <strong class="bold">Restart Kernel and Run All Cells…</strong> option to execute the notebook <span class="No-Break">code cells.</span></li>
			</ol>
			<p>While the notebook is running, let’s review the code to understand exactly what’s happening. In the first two<a id="_idIndexMarker568"/> code cells, we download both the training and validation horses or humans datasets. These datasets have been provided by Laurence Moroney (<a href="https://laurencemoroney.com/datasets.html">https://laurencemoroney.com/datasets.html</a>) and contain 500 rendered images of various species of horse, as well as 527 rendered images of humans. We will be using this dataset to generate higher resolution versions, thereby creating much larger image file sizes to simulate having a large training and validation dataset. By increasing the size of the data, we are therefore creating a scenario where training a model on these large image files would, in effect, introduce a delay in the overall time it takes to train an image classification model. Consequently, we are setting up the requirement to leverage a data parallel placement strategy that will, in effect, reduce the overall time taken to train our image <span class="No-Break">classification model.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">These datasets are licensed under the Creative Commons 2.0 Attribution 2.0 <span class="No-Break">Unported License.</span></p>
			<p>In the third code cell, as shown in the following code snippet, we programmatically extract both the downloaded <strong class="source-inline">train.zip</strong> and <strong class="source-inline">validation.zip</strong> files and save them locally to a <span class="No-Break"><strong class="source-inline">data</strong></span><span class="No-Break"> folder:</span></p>
			<pre class="source-code">
...
import zipfile
with zipfile.ZipFile("train.zip","r") as train_zip_ref:
    train_zip_ref.extractall("data/train")
with zipfile.ZipFile("validation.zip","r") as val_zip_ref:
    val_zip_ref.extractall("data/validation")
...</pre>
			<p>Now that data has been downloaded and extracted, we should have two folders within the <strong class="source-inline">data</strong> directory called <strong class="source-inline">train</strong> and <strong class="source-inline">validation</strong>. Each of these folders contains images of both horses and humans. However, as already mentioned, these images are pretty small in <a id="_idIndexMarker569"/>size. For example, if we examine the <strong class="source-inline">horse01-0.png</strong> file in the <strong class="source-inline">./data/train/horses</strong> folder, you will note that the file is only 151.7 KB in size. Since we only have 500 of these tiny files representing horses, we need to somehow come up with a way to make these files bigger. Therefore, we will use an ML model called <strong class="bold">Enhanced Deep Residual Networks for Single Image Super-Resolution</strong> (<strong class="bold">EDSR</strong>) to increase<a id="_idIndexMarker570"/> the resolution of these files and, in effect, increase the size of the files to simulate a real-world use case where images are in MB, as opposed <span class="No-Break">to KB.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">While it’s not within the scope of this chapter to detail the EDSR model, we are simply using it to enhance the resolution of the images, thereby making the file size bigger. You can learn more about the pre-trained model from Hugging Face by referencing their model <span class="No-Break">repository (</span><a href="https://huggingface.co/eugenesiow/edsr-base"><span class="No-Break">https://huggingface.co/eugenesiow/edsr-base</span></a><span class="No-Break">).</span></p>
			<p>So, in the next set of code cells, as shown in the following code snippet, we run the pre-trained EDSR model on our image dataset to increase the image resolution and, as a byproduct, increase the image <span class="No-Break">file size:</span></p>
			<pre class="source-code">
...
from super_image import EdsrModel, ImageLoader
from PIL import Image
import requests
import os
from os import listdir
folder_dir = "data/validation/"
model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=4)
for folder in os.listdir(folder_dir):
    folder_path = f'{folder_dir}{folder}'
    for image_file in os.listdir(folder_path):
        path = f'{folder_path}/{image_file}'
        image = Image.open(path)
        inputs = ImageLoader.load_image(image)
        preds = model(inputs)
        ImageLoader.save_image(preds, path)
file_size = os.path.getsize(path)
print("File Size is :", file_size/1000000, "MB")
...</pre>
			<p>As you can see from the example output from the code cell, we have increased the file size of each image from approximately 178 KB to just under 2 MB. So, with the datasets ready for training, we<a id="_idIndexMarker571"/> can upload them to S3 so that the ephemeral SageMaker cluster can access them. The following code snippet shows how we initialize the SageMaker permissions to S3 and use the <strong class="source-inline">upload()</strong> method from the SageMaker Python SDK’s <strong class="source-inline">S3Upload</strong> class to store the data <span class="No-Break">on S3:</span></p>
			<pre class="source-code">
...
import sagemaker
from sagemaker import get_execution_role
from sagemaker.estimator import Estimator
from sagemaker.s3 import S3Uploader
import boto3
sagemaker_session = sagemaker.Session()
bucket = sagemaker_session.default_bucket()
prefix = 'horse-or-human'
role = get_execution_role()
client = boto3.client('sts')
account = client.get_caller_identity()['Account']
print(f'AWS account:{account}')
session = boto3.session.Session()
region = session.region_name
print(f'AWS region:{region}')
s3_train_data = S3Uploader.upload('data/train',f's3://{bucket}/{prefix}/data/train')
s3_val_data = S3Uploader.upload('data/validation',f's3://{bucket}/{prefix}/data/validation')
print('s3 train data path: ', s3_train_data)
print('s3 validation data path: ', s3_val_data)
...</pre>
			<p>Now, we are ready to define the SageMaker estimator. You will recall from the code snippet shown in the <em class="italic">Building ML systems using AWS</em> section that all we had to do was define a <strong class="source-inline">PyTorch</strong> estimator<a id="_idIndexMarker572"/> and provide the basic configuration parameters, such as <strong class="source-inline">instance_coun</strong>t and <strong class="source-inline">instance_type</strong>, and SageMaker took care of the rest of the heavy lifting to orchestrate the Training job. However, in order to configure a data parallel placement strategy, we need to provide an additional configuration parameter, called <strong class="source-inline">distribution</strong>, to the estimator. As you can see from the following code<a id="_idIndexMarker573"/> snippet, we declared the same instance of the estimator, but now we’ve added the <strong class="source-inline">distribution</strong> parameter to inform SageMaker that we wish to enable a <strong class="source-inline">dataparallel</strong> <span class="No-Break">placement strategy:</span></p>
			<pre class="source-code">
...
estimator = PyTorch(entry_point='train.py',
                    source_dir='src',
                    role=role,
                    instance_count=1,
                    instance_type='ml.p3.16xlarge',
                    framework_version='1.8.0',
                    py_version='py3',
                    sagemaker_session=sagemaker_session,
                    hyperparameters={'epochs':10,
                                     'batch_size':32,
                                     'lr':3e-5,
                                     'gamma': 0.7},
                    <strong class="bold">distribution={"smdistributed": {"dataparallel": {"enabled": True}}}</strong>,
                    debugger_hook_config=False,
                    metric_definitions=metric_definitions,
                   )
...</pre>
			<p>Now, all that’s left to do is initiate the Training job by calling the <strong class="source-inline">fit()</strong> method of our <strong class="source-inline">estimator</strong> object. The following code snippet shows how to initialize the distributed Training job using the training <a id="_idIndexMarker574"/>and validation data that we’ve already uploaded <span class="No-Break">to S3:</span></p>
			<pre class="source-code">
...
from sagemaker.inputs import TrainingInput
train = TrainingInput(s3_train_data,
                      content_type='image/png',
                      input_mode='File')
val = TrainingInput(s3_val_data,
                    content_type='image/png',
                    input_mode='File')
estimator.fit({'train':train, 'val': val})
...</pre>
			<p>Once the Training job has been initialized, SageMaker will redirect the logs so that we can see what’s happening inside the PyTorch training container, and we can match the log output to what we learned about how the distributed data parallel placement <span class="No-Break">strategy functions.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you receive a <strong class="bold">ResourceLimitExceeded</strong> error when calling the <strong class="source-inline">estimator.fit()</strong> method, you can follow the resolution steps from the <em class="italic">How do I resolve the ResourceLimitExceeded error in Amazon SageMaker?</em> knowledge <span class="No-Break">article (</span><a href="https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/"><span class="No-Break">https://aws.amazon.com/premiumsupport/knowledge-center/resourcelimitexceeded-sagemaker/</span></a><span class="No-Break">).</span></p>
			<p>You will recall from the <em class="italic">Reviewing the SageMaker distributed data parallel strategy</em> section that the training script, as well as the model algorithm, are copied to each GPU within the compute instance. Since each GPU is essentially executing its own copy of the training script to optimize its own unique set of model parameters and then share them with the GPU works using AllReduce, we also need to ensure that the training script itself is configured in such a way that it gets executed as a part of a larger distributed <span class="No-Break">training process.</span></p>
			<p>Basically, what this means is that when we specify the <strong class="source-inline">distribution</strong> parameter for the SageMaker <strong class="source-inline">estimator</strong> object, we are instructing SageMaker to configure the appropriate backend resources for a distributed Training job. But we also need to configure the training script to properly use this distributed backend cluster. So, to extend the training script’s ability to<a id="_idIndexMarker575"/> correctly leverage the backend cluster’s distributed capabilities, AWS provides the <strong class="bold">Distributed Data Parallel Library</strong>, called <strong class="source-inline">smdistributed</strong>, for the<a id="_idIndexMarker576"/> specified deep learning framework, which is PyTorch in <span class="No-Break">this case.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">AWS also provides the <strong class="source-inline">smdistributed</strong> library for the TensorFlow 2.x deep learning framework. For more information on how to leverage a distributed data parallel placement strategy for a TensorFlow training script, you can review the TensorFlow <span class="No-Break">Guide (</span><a href="https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/latest/smd_data_parallel_tensorflow.html</span></a><span class="No-Break">).</span></p>
			<p>In the next section, we will review how to configure the model training script using the <strong class="source-inline">smdistributed</strong> <span class="No-Break">Python library.</span></p>
			<h3>Configuring the model training script</h3>
			<p>There are five basic specific<a id="_idIndexMarker577"/> steps for incorporating the <strong class="source-inline">smdistributed</strong> library into a PyTorch training script. To review these steps, we can open the <strong class="source-inline">./src/train.py</strong> file within the Studio IDE and walk through the important code <span class="No-Break">as follows:</span></p>
			<ol>
				<li value="1">The first step is to import the <strong class="source-inline">smdistributed</strong> libraries for the PyTorch framework. As you can see from the following code snippet, by importing and then initializing these modules, we are essentially wrapping PyTorch’s ability to execute parallel training methods into the data parallel <span class="No-Break">placement strategy:</span><pre class="source-code">
...</pre><pre class="source-code">
from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP</pre><pre class="source-code">
import smdistributed.dataparallel.torch.distributed as dist</pre><pre class="source-code">
dist.init_process_group()</pre><pre class="source-code">
...</pre></li>
				<li>The next step is to integrate the data into PyTorch’s data loading mechanism so that PyTorch can iterate through the chunks of data assigned to the GPU worker. In the following code snippet, we specify <strong class="source-inline">num_replicas</strong> as the number of GPU <a id="_idIndexMarker578"/>workers participating in this distributed training exercise. We also supply the GPU worker’s local rank or its membership ranking within the current exercise, by specifying the <span class="No-Break"><strong class="source-inline">rank</strong></span><span class="No-Break"> parameter:</span><pre class="source-code">
...</pre><pre class="source-code">
train_sampler = torch.utils.data.distributed.DistributedSampler(</pre><pre class="source-code">
        train_dataset, num_replicas=world_size, rank=rank</pre><pre class="source-code">
)</pre><pre class="source-code">
...</pre></li>
				<li>From the previous code snippet, we used the <strong class="source-inline">world_size</strong> and <strong class="source-inline">rank</strong> variables. The <strong class="source-inline">world_size</strong> variable was used to denote the total number of GPU workers over which the data parallel task is being distributed. So, as you can see from the next code snippet, to get the total amount of GPUs, we call the <strong class="source-inline">get_world_size()</strong> method from the <strong class="source-inline">smdistributed.dataparallel.torch.distributed</strong> module. Similarly, we also use the <strong class="source-inline">get_rank()</strong> method from this library to get the current GPU’s <span class="No-Break">membership ranking:</span><pre class="source-code">
...</pre><pre class="source-code">
world_size = dist.get_world_size()</pre><pre class="source-code">
rank = dist.get_rank()</pre><pre class="source-code">
...</pre></li>
				<li>Lastly, we configured the mini batch size for the PyTorch <strong class="source-inline">DataLoader()</strong> method to sample, declared as the <strong class="source-inline">batch_size</strong> variable. This is the global batch size of the <a id="_idIndexMarker579"/>training job, divided by the number of GPU workers, represented by the <strong class="source-inline">world_size</strong> variable described in <span class="No-Break"><em class="italic">step 3</em></span><span class="No-Break">:</span><pre class="source-code">
...</pre><pre class="source-code">
args.batch_size //= world_size // 8</pre><pre class="source-code">
args.batch_size = max(args.batch_size, 1)</pre><pre class="source-code">
...</pre></li>
			</ol>
			<p>So, with these minimal code additions applied to the model training routine, we have effectively implemented an example of a data parallel placement strategy. Next, let’s look at how to use the same example but apply a model parallel <span class="No-Break">placement strategy.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor126"/>Executing distributed model parallel training on Amazon SageMaker</h2>
			<p>Since we are using the same image<a id="_idIndexMarker580"/> classification model, shown in the previous example, to illustrate an example of model parallel training, you can follow the same steps to open and execute the notebook. However, instead of opening the <strong class="source-inline">1_distributed_data_parallel_training.ipynb</strong> file, in this example, we are going to open the <strong class="source-inline">2_distributed_model_parallel_training.ipynb</strong> file and run all of the <span class="No-Break">code cells.</span></p>
			<p>So, just as with the data parallel placement strategy, there are two crucial components to successfully implementing the model parallel placement strategy on SageMaker, namely configuring the backend cluster and configuring the model training script. Let’s start by exploring all the changes that need to be made to the <span class="No-Break"><strong class="source-inline">estimator</strong></span><span class="No-Break"> configuration.</span></p>
			<h3>Configuring the backend cluster</h3>
			<p>When reviewing the estimator<a id="_idIndexMarker581"/> configuration, note that the options provided to the <strong class="source-inline">distribution</strong> parameter have changed. As you can see from the following code snippet, we now specify a <strong class="source-inline">modelparallel</strong> option instead of enabling the <strong class="source-inline">dataparallel</strong> setting <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">smdistributed</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
...
                    distribution={
                        "smdistributed": {"modelparallel": smp_options},
                        "mpi": mpi_options
                    },
...</pre>
			<p>Additionally, as shown in the following code snippet, we declare a variable called <strong class="source-inline">smp_options</strong>, whereby we specify a dictionary of the configuration options specific to the <span class="No-Break"><strong class="source-inline">modelparallel</strong></span><span class="No-Break"> strategy:</span></p>
			<pre class="source-code">
...
smp_options = {
    "enabled":True,
    "parameters": {
        "partitions": 1,
        <strong class="bold">"placement_strategy": "spread",</strong>
        <strong class="bold">"pipeline": "interleaved",</strong>
        "optimize": "speed",
        <strong class="bold">"ddp": True</strong>,
    }
}
mpi_options = {
    "enabled" : True,
    <strong class="bold">"processes_per_host" : 8</strong>,
}
...</pre>
			<p>As you can see from the previous code snippet, where the most important configuration options have been highlighted, we set the <strong class="source-inline">placement_strategy</strong> parameter as <strong class="source-inline">spread</strong>. In effect, we <a id="_idIndexMarker582"/>are configuring SageMaker to evenly spread the model partitions across all GPU devices within the compute instance. Since we are using a single <strong class="bold">ml.p3.16xlarge</strong> instance with eight GPUs, and not multiple compute instances, we are spreading the model partitions evenly within <span class="No-Break">the instance.</span></p>
			<p>Additionally, we are setting the pipeline scheduling mechanism, the <strong class="source-inline">pipeline</strong> parameter, to <strong class="source-inline">interleaved</strong>. This setting improves the overall performance of the backend cluster by prioritizing the backward execution model exaction tasks to free up <span class="No-Break">GPU memory.</span></p>
			<p>Lastly, to enable both a model parallel, as well as a hybrid implementation of the data parallel placement strategies, we set the distributed data parallel, or <strong class="source-inline">ddp</strong> parameter, to <strong class="source-inline">True</strong>. As we saw in the section entitled, <em class="italic">Reviewing a hybrid data parallel and model parallel strategy</em>, both the data parallel and model parallel strategies can be used at the same time to further reduce the overall time it takes to train <span class="No-Break">the model.</span></p>
			<p>So, since we are using both strategies concurrently<a id="_idIndexMarker583"/> for this example, we must also supply a <strong class="bold">Message Passing Interface</strong> (<strong class="bold">MPI</strong>), declared as the <strong class="source-inline">mpi</strong> parameter, to instruct SageMaker as to how each GPU worker communicates what it’s doing with the other GPU workers. For example, in the previous code snippet, after enabling the <strong class="source-inline">mpi_options</strong> setting, we have also set <strong class="source-inline">processes_per_host</strong> to <strong class="source-inline">8</strong>. This setting, in effect, configures the ephemeral SageMaker cluster architecture to match <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em>, where we set the GPU workers on the single <strong class="bold">ml.p3.16xlarge</strong> compute instance to use a four-way model parallel strategy to essentially partition the model across four GPU workers. Additionally, we also configure a two-way data parallel strategy to partition the training data into two shards and execute the model partitions in parallel across the shards. Therefore, two-way x four-way equates to eight processes per <span class="No-Break">single host.</span></p>
			<p>As you can see, adding<a id="_idIndexMarker584"/> these minimal configuration changes implements a data parallel and model parallel capable SageMaker cluster. Yet, just as with the previous example, there are also changes that need to be made to the training script. Let’s review <span class="No-Break">these next.</span></p>
			<h3>Configuring the model training script</h3>
			<p>Since implementing a<a id="_idIndexMarker585"/> model parallel placement strategy is more intricate than a data parallel strategy, there are a few extra requirements that need to be added to the training script. Let’s now open the <strong class="source-inline">./src/train_smp.py</strong> file to review the most important requirements. As you might immediately notice, there are 11 specific script changes required to execute a model parallel placement strategy for a <span class="No-Break">PyTorch model:</span></p>
			<ol>
				<li value="1">Once again, and as you can see from the following code snippet, the first step is to import the <strong class="source-inline">modelparallel</strong> modules from the <strong class="source-inline">smdistributed</strong> library and initialize these modules as a wrapper <span class="No-Break">for PyTorch:</span><pre class="source-code">
...</pre><pre class="source-code">
import smdistributed.modelparallel</pre><pre class="source-code">
import smdistributed.modelparallel.torch as smp</pre><pre class="source-code">
smp.init()</pre><pre class="source-code">
...</pre></li>
				<li>Once the module has been initialized, we then extend our image classification model, defined using the <strong class="source-inline">model</strong> variable, and wrap it into the <strong class="source-inline">DistributedModel()</strong> class, as shown in the following code block. This signals that our model is now <span class="No-Break">being distributed:</span><pre class="source-code">
...</pre><pre class="source-code">
model = smp.DistributedModel(model)</pre><pre class="source-code">
...</pre></li>
				<li>Since the model is now being distributed, we also need to distribute the optimizer. So, as you can see from the following code snippet, we optimize the model parameters using<a id="_idIndexMarker586"/> PyTorch’s implementation of the <strong class="source-inline">Adam()</strong> algorithm and subsequently distribute the optimization task across GPUs by wrapping it into the <span class="No-Break"><strong class="source-inline">DistributedOptimizer()</strong></span><span class="No-Break"> class:</span><pre class="source-code">
...</pre><pre class="source-code">
optimizer = smp.DistributedOptimizer(</pre><pre class="source-code">
    optim.Adam(model.parameters(), lr=args.lr))</pre><pre class="source-code">
...</pre></li>
				<li>Alongside distributing the model itself, as well as the optimizer, we also need to define exactly how the forward and backward pass through the model’s computational graph, or the model pipeline, are executed. Accordingly, we extend the computation results from both the forward and backward passes of the model by wrapping them within a <strong class="source-inline">step()</strong> decorator function. The following code snippet shows the <strong class="source-inline">step()</strong> decorator that extends <strong class="source-inline">train_step()</strong> for the forward pass, and <strong class="source-inline">test_step()</strong> for the <span class="No-Break">backward pass:</span><pre class="source-code">
...</pre><pre class="source-code">
@smp.step</pre><pre class="source-code">
def train_step(model, data, label):</pre><pre class="source-code">
    output = model(data)</pre><pre class="source-code">
    loss = F.nll_loss(F.log_softmax(output), label,</pre><pre class="source-code">
                      reduction="mean")</pre><pre class="source-code">
    # replace loss.backward() with model.backward in the train_step function.</pre><pre class="source-code">
    model.backward(loss)</pre><pre class="source-code">
    return output, loss</pre><pre class="source-code">
@smp.step</pre><pre class="source-code">
def test_step(model, data, label):</pre><pre class="source-code">
    val_output = model(data)</pre><pre class="source-code">
    val_loss = F.nll_loss(F.log_softmax(val_output),</pre><pre class="source-code">
                          label, reduction="mean")</pre><pre class="source-code">
    return val_loss</pre><pre class="source-code">
...</pre></li>
				<li>Lastly, once the model has been trained using the model parallel strategy, and as you can see from <a id="_idIndexMarker587"/>the following code snippet, we only save the final model on the highest-ranking GPU worker of <span class="No-Break">the cluster:</span><pre class="source-code">
...</pre><pre class="source-code">
    if smp.rank() == 0:</pre><pre class="source-code">
        model_save = model.module if hasattr(model, "module") else model</pre><pre class="source-code">
        save_model(model_save, args.model_dir)</pre><pre class="source-code">
...</pre></li>
			</ol>
			<p>While these are only a few of the most important parameters for configuring the training script using the <strong class="source-inline">smdistributed.modelprallel</strong> module, you can see that with a minimal amount of code, we can fully provision our training script to use an automatically <a id="_idIndexMarker588"/>configured SageMaker ephemeral cluster for both a data parallel and model parallel placement strategy, thus reducing the overall training time using this <span class="No-Break">hybrid implementation.</span></p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor127"/>Summary</h1>
			<p>In this chapter, we drew your attention to two potential challenges that ML practitioners may face when training ML models: firstly, the challenge of reducing the overall model training time, especially when there is a large amount of training data; and secondly, the challenge of reducing the overall model training time when there are large models with millions and billions of <span class="No-Break">trainable parameters.</span></p>
			<p>We reviewed three specific strategies that can be used to address these challenges, namely the data parallel placement strategy, which distributes a large amount of training data across multiple worker resources to execute the model training process in parallel. Additionally, we also reviewed the model parallel placement strategy, which distributes a very large ML model across multiple GPU resources to offset trying to squeeze these large models into the available memory resources. Lastly, we also explored how both these strategies can be combined, using a hybrid methodology, to further reap the benefits that <span class="No-Break">both offer.</span></p>
			<p>Furthermore, we also reviewed how Amazon SageMaker can be used to solve these challenges, specifically focusing on how SageMaker takes care of the heavy lifting of building a distributed training compute and storage infrastructure specifically configured to handle any of these three placement strategies. SageMaker not only provisions the ephemeral compute resources but also provides Python libraries that can be integrated into the model training script to fully make use of <span class="No-Break">the cluster.</span></p>
			<p>Now that we’ve seen how to carry out ML model training using distributed training, in the next chapter, we will review how to deploy the trained ML models <span class="No-Break">at scale.</span></p>
		</div>
		<div>
			<div id="_idContainer097" class="IMG---Figure">
			</div>
		</div>
	</body></html>