<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer184">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">12</span></h1>
<h1 class="chapterTitle" id="_idParaDest-327"><span class="koboSpan" id="kobo.2.1">AI Risk Management</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">As organizations increasingly rely on AI for critical decision-making and incorporate it into different areas of their businesses, effective AI risk management should be a top priority. </span><span class="koboSpan" id="kobo.3.2">Ensuring the safe and compliant deployment of ML systems is essential to establish trustworthiness in AI solutions. </span><span class="koboSpan" id="kobo.3.3">However, many organizations and individuals have very limited understanding of the risks associated with AI systems, often resulting in outcomes that may negatively impact organizations financially or legally. </span><span class="koboSpan" id="kobo.3.4">In this chapter, we will explore key AI risk scenarios, highlight the differences between AI risk management and traditional software risk management, and emphasize the importance of having a robust AI risk management practice. </span><span class="koboSpan" id="kobo.3.5">We will present a risk management framework that organizations can consider for managing AI risks. </span><span class="koboSpan" id="kobo.3.6">Finally, we will discuss how to manage risks at different stages of the ML lifecycle and design ML platforms that support risk management and AI governance.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">Specifically, we will cover the following key topics:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.5.1">Understanding AI risk scenarios</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.6.1">The regulatory landscape around AI risk management</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.7.1">Understanding AI risk management</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.8.1">Applying risk management across the AI lifecycle</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.9.1">Designing ML platforms with governance and risk management considerations</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-328"><span class="koboSpan" id="kobo.10.1">Understanding AI risk scenarios</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.11.1">Many of the</span><a id="_idIndexMarker1246"/><span class="koboSpan" id="kobo.12.1"> organizations I have worked with have very limited knowledge about the risks presented in their AI systems. </span><span class="koboSpan" id="kobo.12.2">They often treat AI risks the same way they deal with risks associated with traditional software. </span><span class="koboSpan" id="kobo.12.3">In reality, AI systems present a new set of risks that we do not normally see in traditional software. </span><span class="koboSpan" id="kobo.12.4">With traditional software, the risk is mainly about software vulnerability, a legacy technology stack, malware, misconfiguration, and unauthorized access to data. </span><span class="koboSpan" id="kobo.12.5">AI systems are exposed to many of the same software risks; additionally, AI systems can present new kinds of risks such as bias and misinformation. </span><span class="koboSpan" id="kobo.12.6">These risks can have significant negative consequences for organizations </span><a id="_idIndexMarker1247"/><span class="koboSpan" id="kobo.13.1">and individuals that rely on AI systems for business operations and decision-making. </span><span class="koboSpan" id="kobo.13.2">AI risks can manifest in many different ways, such as displaying biased behavior or producing unexpected prediction results. </span><span class="koboSpan" id="kobo.13.3">Many of the AI risk scenarios are also silent risks that are difficult to detect.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.14.1">The following are some scenarios where AI risks may arise:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.15.1">Bias and discrimination</span></strong><span class="koboSpan" id="kobo.16.1">: One of the most well-known risks associated with AI is the potential for displaying bias and discrimination in AI systems. </span><span class="koboSpan" id="kobo.16.2">This can occur when ML algorithms are trained on biased data or when the algorithms themselves are susceptible to biased behaviors. </span><span class="koboSpan" id="kobo.16.3">In these cases, the algorithms may learn to discriminate against certain groups, leading to unfair or discriminatory outcomes. </span><span class="koboSpan" id="kobo.16.4">For example, a bank can have an ML model that’s trained using biased datasets, such as including gender and ethnic groups as inputs, resulting in discrimination against certain gender or ethnic groups and potential violation of laws and regulations, such as the Equal Credit Opportunity Act. </span><span class="koboSpan" id="kobo.16.5">Nowadays, many organizations use AI to screen resumes, and it has been found that some of these AI systems have displayed preferences and biases toward certain types of candidates. </span><span class="koboSpan" id="kobo.16.6">In 2018, researchers from MIT revealed that facial recognition systems from several major technology companies exhibited significant racial bias, misidentifying darker-skinned individuals at higher rates compared to lighter-skinned individuals. </span><span class="koboSpan" id="kobo.16.7">This incident raised concerns about the potential for AI systems to perpetuate and amplify societal biases, leading to discriminatory outcomes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.17.1">Misinformation and misinterpretation</span></strong><span class="koboSpan" id="kobo.18.1">: Another risk associated with AI is the potential for generating misinformation and misinterpretation of facts. </span><span class="koboSpan" id="kobo.18.2">This can occur when ML algorithms are used to process large amounts of data, but the data contains errors or inconsistencies that are not easily detected. </span><span class="koboSpan" id="kobo.18.3">As a result, the algorithms may generate inaccurate or misleading results, leading to potential wrong decision-making. </span><span class="koboSpan" id="kobo.18.4">With the fast rise of generative AI technologies, such as ChatGPT and Stable Diffusion models, it is also becoming more difficult for humans to distinguish reality from hallucination. 
    </span><p class="normal"><span class="koboSpan" id="kobo.19.1">For example, the rapid advancement of deepfake technologies, which use AI to generate highly realistic synthetic audio, video, and images, has raised concerns about their potential misuse for spreading misinformation, impersonation, and manipulation. </span><span class="koboSpan" id="kobo.19.2">Incidents of deepfake videos being used to impersonate public figures and spread false narratives have already been reported.</span></p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.20.1">Lack of interpretability</span></strong><span class="koboSpan" id="kobo.21.1">: Many ML algorithms, such as neural networks, can be complex and difficult to understand, even for trained experts. </span><span class="koboSpan" id="kobo.21.2">This lack of transparency can make it difficult to identify the causes of problems when they arise, making it harder to develop effective mitigation to address the problem. </span><span class="koboSpan" id="kobo.21.3">For example, when ChatGPT provides incorrect responses to user prompts, it is often impossible to understand why it made the mistake. </span><span class="koboSpan" id="kobo.21.4">For regulated industries, this presents a significant challenge when organizations want to adopt more advanced black-box algorithms such as neural networks, as these organizations often need to provide deterministic responses to specific inputs and questions, and how the decisions are made.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.22.1">Unintended consequences</span></strong><span class="koboSpan" id="kobo.23.1">: ML algorithms can sometimes produce unintended consequences </span><a id="_idIndexMarker1248"/><span class="koboSpan" id="kobo.24.1">or side effects that were not foreseen during the development process. </span><span class="koboSpan" id="kobo.24.2">The reason for this is that AI models are often optimized for a specific objective, such as increasing company profit, while ignoring other factors such as gender and race. </span><span class="koboSpan" id="kobo.24.3">For example, an AI-based target marketing system might target a subset of customers with incentives and benefits, while discriminating against minority or low-income customers, in its pursuit of profit maximization.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.25.1">Adversarial attacks</span></strong><span class="koboSpan" id="kobo.26.1">: ML algorithms can be vulnerable to adversarial attacks, which involve deliberately manipulating the input data to produce unexpected or undesirable outcomes, or planting backdoor access to ML models. </span><span class="koboSpan" id="kobo.26.2">For example, an attacker could use an adversarial attack to trick an AI-based fraud detection system into classifying fake financial transactions as legitimate transactions. </span><span class="koboSpan" id="kobo.26.3">ML models can also be compromised to reveal training data by using adversarial techniques such as membership inference attacks. </span><span class="koboSpan" id="kobo.26.4">There have been real-world examples of adversarial attacks. </span><span class="koboSpan" id="kobo.26.5">In 2017, researchers from the University of Michigan demonstrated a method to generate small, innocuous-looking patches that could be placed on physical objects, such as stop signs or pedestrians, to cause state-of-the-art object detection models to misclassify or fail to detect those objects. </span><span class="koboSpan" id="kobo.26.6">In another example, a chatbot implemented by a car dealership faced disruptions when mischievous users exploited a loophole, which, at times, prompted the bot to unintentionally propose extraordinary deals like acquiring brand-new cars for minimal costs.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.27.1">Privacy violation and sensitive data exposure</span></strong><span class="koboSpan" id="kobo.28.1">: Nowadays, many of the state-of-the-art models are trained using enormous amounts of data from many different sources, and, sometimes, personal or sensitive information is used in the development of these models. </span><span class="koboSpan" id="kobo.28.2">This inherently increases the risk of potential privacy violations and unintended disclosure of sensitive data. </span><span class="koboSpan" id="kobo.28.3">For example, to train a medical imaging model for cancer detection, you often need real patient data such as CT scans and other </span><strong class="keyWord"><span class="koboSpan" id="kobo.29.1">protected health information</span></strong><span class="koboSpan" id="kobo.30.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.31.1">PHI</span></strong><span class="koboSpan" id="kobo.32.1">) or </span><strong class="keyWord"><span class="koboSpan" id="kobo.33.1">personal identifiable information</span></strong><span class="koboSpan" id="kobo.34.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">PII</span></strong><span class="koboSpan" id="kobo.36.1">). </span><span class="koboSpan" id="kobo.36.2">If not handled correctly, this information can be exposed to people who are not authorized to access it. </span><span class="koboSpan" id="kobo.36.3">Also, as part of the model training, some sensitive data can be memorized by the trained model, and the model can potentially disclose this information when making predictions. </span><span class="koboSpan" id="kobo.36.4">In 2020, an investigative report by the New York Times revealed that people’s images were used in AI model training without their consent and knowledge.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.37.1">Third-party risks</span></strong><span class="koboSpan" id="kobo.38.1">: While third-party risks also exist with traditional software from third-party vendors, AI systems elevate these risks in areas that we have not seen before. </span><span class="koboSpan" id="kobo.38.2">With the advent of ML techniques such as transfer learning and fine-tuning from pre-trained models, more and more organizations are building custom models based on existing pre-trained models. </span><span class="koboSpan" id="kobo.38.3">However, given the black-box nature of these pre-trained models, it increases the uncertainty of</span><a id="_idIndexMarker1249"/><span class="koboSpan" id="kobo.39.1"> the models’ behavior and unknowns around the scientific validity of the models. </span><span class="koboSpan" id="kobo.39.2">Since the pre-trained models were outside of the security and process controls of the consuming organizations, the consuming organization may inherit risks that may already exist in the pre-trained models such as bias or backdoor attack vulnerabilities. </span><span class="koboSpan" id="kobo.39.3">For example, models with vulnerabilities such as arbitrary code execution and file writes have been detected in model hubs such as Hugging Face.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.40.1">Model testing risks</span></strong><span class="koboSpan" id="kobo.41.1">: Compared to traditional software, the testing standards and tools for AI-based software and models are underdeveloped. </span><span class="koboSpan" id="kobo.41.2">Traditional software testing mainly focuses on functional components such as user interface flow or business logic that’s well defined, and non-functional areas such as scalability and latency. </span><span class="koboSpan" id="kobo.41.3">With AI/ML testing, in addition to many of the traditional software testing requirements, there are new testing concepts such as error analysis of different failure modes, model sensitivity, model robustness, and adversarial testing, which are more difficult to perform than traditional software testing. </span><span class="koboSpan" id="kobo.41.4">The available testing tools in this domain are also very limited; often, data scientists and testers need to manually prepare different testing scenarios and testing data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.42.1">As you can see, the capabilities of AI systems extend far beyond traditional software, introducing many new potential risks and challenges. </span><span class="koboSpan" id="kobo.42.2">As these advanced technologies gain wide adoption and integration into critical domains, concerns over their responsible development and deployment have come to the forefront. </span><span class="koboSpan" id="kobo.42.3">Recognizing the unique complexities and far-reaching implications of AI, various regulatory bodies have taken proactive steps to establish guidelines and regulations aimed at mitigating these risks and ensuring the ethical and trustworthy use of AI systems.</span></p>
<h1 class="heading-1" id="_idParaDest-329"><span class="koboSpan" id="kobo.43.1">The regulatory landscape around AI risk management</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.44.1">With the </span><a id="_idIndexMarker1250"/><span class="koboSpan" id="kobo.45.1">fast advancement of AI technologies and adoption in critical business decision-making, and the negative impacts that AI systems can potentially have on individuals, organizations, and societies, many countries and jurisdictions have established policies, guidance, and regulations to help manage the risks of AI adoption. </span><span class="koboSpan" id="kobo.45.2">It is also expected that more and more legislation will be proposed and passed by different countries and jurisdictions at a fast rate.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.46.1">In the </span><strong class="keyWord"><span class="koboSpan" id="kobo.47.1">United States</span></strong><span class="koboSpan" id="kobo.48.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.49.1">US</span></strong><span class="koboSpan" id="kobo.50.1">), the Federal Reserve and the </span><strong class="keyWord"><span class="koboSpan" id="kobo.51.1">Office of the Comptroller of the Currency</span></strong><span class="koboSpan" id="kobo.52.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.53.1">OCC</span></strong><span class="koboSpan" id="kobo.54.1">) published the Supervisory Guidance on Model Risk Management (OCC 2011-2012/SR 11-7) as early as 2011. </span><span class="koboSpan" id="kobo.54.2">SR 11-7 has become the key regulatory guidance for model risk management in the US. </span><span class="koboSpan" id="kobo.54.3">This guidance establishes the main principles for model risk management covering governance, policies and controls, model </span><a id="_idIndexMarker1251"/><span class="koboSpan" id="kobo.55.1">development, implementation and use, and model validation processes. </span><span class="koboSpan" id="kobo.55.2">In the governance and policy area, it provides guidance on model inventory management, risk rating, roles, and responsibilities. </span><span class="koboSpan" id="kobo.55.3">In the model development and implementation area, it covers topics such as the design process, data assessment, model testing, and documentation. </span><span class="koboSpan" id="kobo.55.4">In the validation area, it provides guidance on validation procedures, monitoring, and finding resolutions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.56.1">In Europe, the </span><strong class="keyWord"><span class="koboSpan" id="kobo.57.1">European Central Bank</span></strong><span class="koboSpan" id="kobo.58.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.59.1">ECB</span></strong><span class="koboSpan" id="kobo.60.1">) Banking Supervision launched the </span><strong class="keyWord"><span class="koboSpan" id="kobo.61.1">Targeted Review of Internal Models</span></strong><span class="koboSpan" id="kobo.62.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.63.1">TRIM</span></strong><span class="koboSpan" id="kobo.64.1">) guideline in 2016 to provide guidance on the </span><strong class="keyWord"><span class="koboSpan" id="kobo.65.1">model risk management</span></strong><span class="koboSpan" id="kobo.66.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">MRM</span></strong><span class="koboSpan" id="kobo.68.1">) framework. </span><span class="koboSpan" id="kobo.68.2">Specifically, the guideline states that an MRM framework needs to have a model inventory to allow a holistic view of the models and their applications, a guideline for identifying and mitigating known model deficiencies, definitions of roles and responsibilities, and definitions of policies, measurement procedures, and reporting.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.69.1">More recently, in 2021, the </span><strong class="keyWord"><span class="koboSpan" id="kobo.70.1">European Union</span></strong><span class="koboSpan" id="kobo.71.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.72.1">EU</span></strong><span class="koboSpan" id="kobo.73.1">) introduced the EU AI Act to promote the benefits of AI, while also ensuring the safe and responsible use of AI in the EU. </span><span class="koboSpan" id="kobo.73.2">The Act takes a risk-based approach to regulating AI, with different requirements based on the level of risks associated with AI systems. </span><span class="koboSpan" id="kobo.73.3">For example, AI systems supporting critical infrastructure would be rated with the highest risk designation and require the strictest oversight and regulations. </span><span class="koboSpan" id="kobo.73.4">The regulation also proposes provisions for AI transparency and accountability in the AI decision-making process, such as requirements for explainability and the ability to challenge the decisions made by AI systems. </span><span class="koboSpan" id="kobo.73.5">It also has new rules for AI use in biometric identification and surveillance.</span></p>
<h1 class="heading-1" id="_idParaDest-330"><span class="koboSpan" id="kobo.74.1">Understanding AI risk management</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.75.1">To address the various</span><a id="_idIndexMarker1252"/><span class="koboSpan" id="kobo.76.1"> risks associated with AI and to comply with different compliance regulations, many organizations, especially in the regulated industry, have developed and implemented AI risk management programs. </span><span class="koboSpan" id="kobo.76.2">In short, AI risk management is the process of identifying, assessing, and mitigating the risk associated with the use of AI in automated decision-making. </span><span class="koboSpan" id="kobo.76.3">The ultimate goal of AI risk management is to establish trust in the AI/ML systems and ensure compliance with applicable rules and regulations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.77.1">Trusting an AI system requires rigorous assessment and consideration of the AI system across many different dimensions and criteria. </span><span class="koboSpan" id="kobo.77.2">Functionally, a trusted AI system needs to provide valid predictions/responses reliably for its intended use. </span><span class="koboSpan" id="kobo.77.3">This means that generated predictions/responses are consistently valid and can be trusted for reliable decision-making. </span><span class="koboSpan" id="kobo.77.4">Ethically, a trusted AI system needs to be safe to use, explainable, privacy-protected, and fair with bias properly managed and mitigated. </span><span class="koboSpan" id="kobo.77.5">From a cybersecurity perspective, a trusted AI system also needs to be secure and resilient against adversarial attacks. </span><span class="koboSpan" id="kobo.77.6">Lastly, a</span><a id="_idIndexMarker1253"/><span class="koboSpan" id="kobo.78.1"> trustworthy AI system needs to provide transparency such as what and how data, algorithms, and models are used in the system. </span><span class="koboSpan" id="kobo.78.2">It is worth noting that it is often a balancing act and </span><a id="_idIndexMarker1254"/><span class="koboSpan" id="kobo.79.1">trade-off across these different dimensions when building and operating a trustworthy system, based on the needs and objectives of an organization. </span><span class="koboSpan" id="kobo.79.2">For example, to protect privacy, an organization might need to make sacrifices in model accuracy or prediction speed.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.80.1">Now we understand what it takes to have trust in AI systems, let’s explore and dive deep into the AI risk management framework and its various components. </span><span class="koboSpan" id="kobo.80.2">The following figure illustrates the key components of AI risk management, which mainly consists of applying MRM, enterprise risk management, and third-party risk management across the AI lifecycle, governed by a set of AI risk governance principles. </span><span class="koboSpan" id="kobo.80.3">In this chapter, our exclusive focus will be on MRM. </span><span class="koboSpan" id="kobo.80.4">Enterprise risk and third-party risk management extend beyond the realm of AI and are universal considerations.</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.81.1"><img alt="Diagram  Description automatically generated" src="../Images/B20836_12_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.82.1">Figure 12.1: AI risk management components</span></p>
<p class="normal"><span class="koboSpan" id="kobo.83.1">Next, we will delve into the details of governance oversight principles and the AI risk framework. </span><span class="koboSpan" id="kobo.83.2">We will focus mainly on </span><a id="_idIndexMarker1255"/><span class="koboSpan" id="kobo.84.1">the understanding of AI risk governance and MRM, with the understanding that traditional enterprise security and third-party risk management are also part of the overall AI risk management considerations.</span></p>
<h2 class="heading-2" id="_idParaDest-331"><span class="koboSpan" id="kobo.85.1">Governance oversight principles</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.86.1">Implementing</span><a id="_idIndexMarker1256"/><span class="koboSpan" id="kobo.87.1"> AI risk management starts with establishing key governance principles. </span><span class="koboSpan" id="kobo.87.2">The principles clarify the ultimate goals for what needs to be accomplished with risk management programs. </span><span class="koboSpan" id="kobo.87.3">Depending on the business and the regulatory environment an organization is in, an organization can make a decision on whether it should be included in its risk management framework. </span><span class="koboSpan" id="kobo.87.4">The following are some of the key areas for consideration:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.88.1">Transparency</span></strong><span class="koboSpan" id="kobo.89.1">: AI systems should be designed in a way that allows stakeholders to understand how decisions are made and why. </span><span class="koboSpan" id="kobo.89.2">This may include the ability to explain the ML model predictions, as well as transparency on what and how the data and algorithms are used and implemented.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.90.1">Accountability</span></strong><span class="koboSpan" id="kobo.91.1">: Organizations should be accountable for the decisions made by AI systems, including any negative consequences that may arise from their use. </span><span class="koboSpan" id="kobo.91.2">This accountability will ensure the owning organizations are incentivized to institute relevant policies and processes to govern the ML lifecycle.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.92.1">Data governance</span></strong><span class="koboSpan" id="kobo.93.1">: Organizations should ensure that the data used to train AI systems is accurate, representative, ethical, and unbiased. </span><span class="koboSpan" id="kobo.93.2">Without proper data governance, it would be highly challenging to trust AI systems that are built with ungoverned data.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.94.1">Human oversight</span></strong><span class="koboSpan" id="kobo.95.1">: While AI systems are meant to make decisions automatically in most cases without human intervention, organizations should have the ability to implement human oversight where required, meaning that humans should be involved in decision-making where it makes sense and should have the ability to override decisions when necessary.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.96.1">Privacy and security</span></strong><span class="koboSpan" id="kobo.97.1">: Appropriate policies and processes should be established to ensure AI systems are designed to protect the privacy and security of assets according to laws and regulations. </span><span class="koboSpan" id="kobo.97.2">Privacy and security breaches can have significant financial and non-financial implications for an organization.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.98.1">Fairness</span></strong><span class="koboSpan" id="kobo.99.1">: AI systems should not discriminate against certain individuals or groups based on attributes such as race and gender.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.100.1">Validity and reliability</span></strong><span class="koboSpan" id="kobo.101.1">: AI systems should be designed to produce reliable and valid results. </span><span class="koboSpan" id="kobo.101.2">Appropriate model validation and testing frameworks and processes need to be implemented to ensure highly reliable and predictable behaviors are displayed by AI systems in production. </span><span class="koboSpan" id="kobo.101.3">Mechanisms should be established to monitor system behaviors with processes for mitigation and rollback when abnormal behaviors are observed.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.102.1">With </span><a id="_idIndexMarker1257"/><span class="koboSpan" id="kobo.103.1">governance oversight, organizations should also consider requirements for regulatory compliance, policies and guidelines around roles and responsibility, and standards and processes around AI systems and model inventory and risk classification, as well as how to deal with lifecycle and change management.</span></p>
<h2 class="heading-2" id="_idParaDest-332"><span class="koboSpan" id="kobo.104.1">AI risk management framework</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.105.1">With the AI governance oversight</span><a id="_idIndexMarker1258"/><span class="koboSpan" id="kobo.106.1"> principles defined, organizations can move forward to establish a formalized AI risk management framework and detailed mechanisms for risk identification, risk assessment, and risk mitigation across the end-to-end ML lifecycle. </span><span class="koboSpan" id="kobo.106.2">One of the common frameworks adopted by many organizations is the Three Lines of Defense MRM model that is commonly used in the financial services industry. </span><span class="koboSpan" id="kobo.106.3">This framework focuses on establishing policies, roles, responsibilities, and processes designed to identify, assess, mitigate, and audit potential model risks associated with business problem identification, data management, and model development, deployment, and uses.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.107.1">The first line of defense is owned by business operations. </span><span class="koboSpan" id="kobo.107.2">This line of defense focuses on the development and use of ML models. </span><span class="koboSpan" id="kobo.107.3">Business operations are responsible for creating and retaining all data and model assumptions, model behavior, and model performance metrics in structured documents, based on model classification and risk exposure. </span><span class="koboSpan" id="kobo.107.4">Models are tested and registered, the associated artifacts are persisted, and results can be reproduced. </span><span class="koboSpan" id="kobo.107.5">Once models are deployed, system issues, model outputs, model bias, and data and model drifts are monitored and addressed according to the established procedures and guidelines.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.108.1">The second line of defense is owned by the risk management function, and it focuses on model validation. </span><span class="koboSpan" id="kobo.108.2">The risk management function is responsible for independently reviewing and validating the documents generated by the first line. </span><span class="koboSpan" id="kobo.108.3">This line of defense introduces standards on controls and documentation, making sure that documents are self-contained, results are reproducible, and the limitations of models are well-understood by stakeholders.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.109.1">The internal audit owns the third line of defense. </span><span class="koboSpan" id="kobo.109.2">The third line of defense focuses more on control and processes and less on model artifacts and theories. </span><span class="koboSpan" id="kobo.109.3">Specifically, this line of defense is responsible for auditing the first and second lines of defense to ensure all established processes and guidelines are effectively </span><a id="_idIndexMarker1259"/><span class="koboSpan" id="kobo.110.1">followed and implemented. </span><span class="koboSpan" id="kobo.110.2">This line of defense provides independent validation of internal controls and reviews the documentation, timeliness, frequency, and completeness of the MRM activities.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.111.1">The MRM alone mainly addresses the risks associated with model development and the development lifecycle. </span><span class="koboSpan" id="kobo.111.2">However, a comprehensive AI risk management framework also needs to cover other risks such as system scalability and reliability, unauthorized access of systems, denial of access, and third-party failure risks. </span><span class="koboSpan" id="kobo.111.3">MRM should also be combined with enterprise technology risk, cybersecurity management, and third-party risks to ensure comprehensive coverage of AI risks.</span></p>
<h1 class="heading-1" id="_idParaDest-333"><span class="koboSpan" id="kobo.112.1">Applying risk management across the AI lifecycle</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.113.1">AI risks can</span><a id="_idIndexMarker1260"/><span class="koboSpan" id="kobo.114.1"> exist in any stage of the AI lifecycle, spanning from business problem identification to the uses of AI systems. </span><span class="koboSpan" id="kobo.114.2">In the following sections, we will explore the various risks that can arise at each stage of the AI lifecycle (as illustrated in </span><em class="italic"><span class="koboSpan" id="kobo.115.1">Figure 12.1</span></em><span class="koboSpan" id="kobo.116.1">) and suggest effective strategies and considerations to mitigate them.</span></p>
<h2 class="heading-2" id="_idParaDest-334"><span class="koboSpan" id="kobo.117.1">Business problem identification and definition</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.118.1">In this</span><a id="_idIndexMarker1261"/><span class="koboSpan" id="kobo.119.1"> initial stage of the AI lifecycle, organizations develop a comprehensive understanding of the business problems that AI can address. </span><span class="koboSpan" id="kobo.119.2">They also outline the overall solution approach and data prerequisites. </span><span class="koboSpan" id="kobo.119.3">It is critical </span><a id="_idIndexMarker1262"/><span class="koboSpan" id="kobo.120.1">during this phase to verify that the AI solution aligns with governance principles, standards, and requirements while achieving specific business objectives.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.121.1">One significant risk is the regulatory compliance risk, which arises when there is a lack of consideration for potential regulatory requirements. </span><span class="koboSpan" id="kobo.121.2">Organizations must understand applicable regulatory requirements related to AI projects, such as the EU AI Act, and take appropriate measures to address them in the problem identification phase. </span><span class="koboSpan" id="kobo.121.3">Failure to do so can result in non-compliance, impacting the entire project. </span><span class="koboSpan" id="kobo.121.4">Another critical consideration is the ethical risk. </span><span class="koboSpan" id="kobo.121.5">Ethics can play a vital role in an organization’s values and brand reputation. </span><span class="koboSpan" id="kobo.121.6">If not integrated into business problem identification, the final system may cause misalignment with core values and the brand, leading to reputational damage.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.122.1">Unexpected consequences can arise if the system is not designed for the intended use. </span><span class="koboSpan" id="kobo.122.2">Misuse can result in unforeseen negative consequences, emphasizing the need for careful consideration of system design. </span><span class="koboSpan" id="kobo.122.3">Risk rating and classification help determine potential impacts and guide different levels of risk management. </span><span class="koboSpan" id="kobo.122.4">Without this, AI systems and their data may be mishandled, leading to unexpected consequences and potential adverse outcomes. </span><span class="koboSpan" id="kobo.122.5">Additionally, security and privacy requirement risks emphasize the </span><a id="_idIndexMarker1263"/><span class="koboSpan" id="kobo.123.1">need for implementing security and privacy measures. </span><span class="koboSpan" id="kobo.123.2">Without these requirements, organizations </span><a id="_idIndexMarker1264"/><span class="koboSpan" id="kobo.124.1">are at risk of privacy violations and adversarial manipulation of AI systems, compromising data integrity and security.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.125.1">For each of the potential risks identified at this stage, it is important to conduct an assessment to determine the severity and likelihood of the risk happening and the resulting impact. </span><span class="koboSpan" id="kobo.125.2">Determine whether any mitigation measures should be considered to reduce the risk, based on the risk tolerance level of each individual organization. </span><span class="koboSpan" id="kobo.125.3">Only move forward with the project if the key risks are understood, mitigated, or accepted.</span></p>
<h2 class="heading-2" id="_idParaDest-335"><span class="koboSpan" id="kobo.126.1">Data acquisition and management</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.127.1">During this phase </span><a id="_idIndexMarker1265"/><span class="koboSpan" id="kobo.128.1">of the project lifecycle, it is crucial for organizations to identify appropriate data sources, establish a data acquisition strategy, and assess their technology capabilities for data processing and management. </span><span class="koboSpan" id="kobo.128.2">AI systems</span><a id="_idIndexMarker1266"/><span class="koboSpan" id="kobo.129.1"> present a distinct set of data acquisition and processing risks, in addition to their exposure to many common data-related risks. </span><span class="koboSpan" id="kobo.129.2">These risks span from selecting the appropriate dataset to managing data end to end. </span><span class="koboSpan" id="kobo.129.3">These risks encompass every aspect, from the careful selection of datasets to the comprehensive management of data throughout its lifecycle.</span></p>
<h3 class="heading-3" id="_idParaDest-336"><span class="koboSpan" id="kobo.130.1">Risk considerations</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.131.1">One key risk</span><a id="_idIndexMarker1267"/><span class="koboSpan" id="kobo.132.1"> is the data selection risk, where an incorrect or inadequately sampled dataset may introduce significant data relevancy or bias issues. </span><span class="koboSpan" id="kobo.132.2">This can lead to the development of a biased model or a model that fails to effectively address the problem at hand. </span><span class="koboSpan" id="kobo.132.3">For instance, in a credit scoring project, if certain demographic groups are underrepresented in the dataset during data collection, the resulting model may exhibit bias toward the overrepresented groups.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.133.1">Data quality and missing data pose significant challenges for data scientists, affecting the development of high-quality ML models. </span><span class="koboSpan" id="kobo.133.2">Ensuring data accuracy and addressing missing data issues is crucial for the successful development of robust models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.134.1">Data labeling risks emerge as a concern due to the predominantly manual nature of data labeling processes. </span><span class="koboSpan" id="kobo.134.2">This not only becomes a bottleneck in model development but can also lead to poor model accuracy if mislabeling mistakes occur.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.135.1">Regulatory and compliance data checks are vital for projects falling under specific regulatory compliance requirements. </span><span class="koboSpan" id="kobo.135.2">Enforcing regulatory and compliance data checks, such as data sovereignty rules, is essential to avoid potential fines and lawsuits resulting from violations, safeguarding the organization’s financial and reputational standing.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.136.1">Data privacy becomes</span><a id="_idIndexMarker1268"/><span class="koboSpan" id="kobo.137.1"> pertinent as AI capabilities improve with increased data, including personal information. </span><span class="koboSpan" id="kobo.137.2">The ethical use of personal data for analysis and model training requires careful consideration to prevent privacy infringements.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.138.1">The adversarial attack risk introduces a new dimension of threats, wherein malicious actors can manipulate training data to cause the resulting model to behave incorrectly in targeted scenarios. </span><span class="koboSpan" id="kobo.138.2">For instance, manipulation of training data labels can lead to models learning incorrectly and producing inaccurate results. </span><span class="koboSpan" id="kobo.138.3">These data-related risks demand meticulous attention and mitigation strategies to ensure the successful and ethical deployment of AI systems.</span></p>
<h3 class="heading-3" id="_idParaDest-337"><span class="koboSpan" id="kobo.139.1">Risk mitigations </span></h3>
<p class="normal"><span class="koboSpan" id="kobo.140.1">To effectively </span><a id="_idIndexMarker1269"/><span class="koboSpan" id="kobo.141.1">mitigate data-related risks, comprehensive strategies and mechanisms must be in place to address issues related to data quality, bias, human errors, and regulatory compliance requirements. </span><span class="koboSpan" id="kobo.141.2">These mechanisms encompass various initiatives, including the implementation of robust data validation methods, the establishment of consistent definitions and standards for data selection and sampling, regular reviews of data quality and completeness, and the provision of guidance on mitigation approaches.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.142.1">To address data selection and sampling risks, it is crucial to establish standards and consistent definitions for different data types and sources, ensuring uniformity in data selection across various sources. </span><span class="koboSpan" id="kobo.142.2">These standards should encompass considerations for relevancy, data gaps, bias, and representation, providing guidance on mitigation approaches such as extending data sources, employing synthetic data, and utilizing appropriate sampling techniques.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.143.1">For mitigating data quality risks, stringent measures should be taken to verify that the minimum data quality standards are met to support high-quality data processing and model training. </span><span class="koboSpan" id="kobo.143.2">This includes the establishment of rules and sample data reviews for accuracy, completeness, consistency, and the valid representation of the target population.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.144.1">To address data labeling risks, controls should be implemented in the data labeling process to ensure consistency and mitigate subjective bias. </span><span class="koboSpan" id="kobo.144.2">Additionally, sample testing the validity of dataset labels can further enhance the quality and accuracy of the labeled data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.145.1">In terms of regulatory</span><a id="_idIndexMarker1270"/><span class="koboSpan" id="kobo.146.1"> compliance and privacy checks, a robust set of checks should be integrated into the MRM process. </span><span class="koboSpan" id="kobo.146.2">This involves establishing enhanced controls around data access, ownership, collection, storage, transmission, and assessment to satisfy regulatory requirements. </span><span class="koboSpan" id="kobo.146.3">Additionally, comprehensive regulatory compliance checks for data privacy protection should be embedded in the MRM process, linking these controls to the enterprise access and authentication platform for centralized governance. </span><span class="koboSpan" id="kobo.146.4">The enforcement of data encryption and data masking should be applied where necessary to bolster privacy</span></p>
<p class="normal"><span class="koboSpan" id="kobo.147.1">Ultimately, the mitigation strategy and mechanisms for data-related risks should be tailored to the specific needs of the organization and continuously reviewed and updated to keep up with the evolving landscape of AI/ML technologies and their associated risks.</span></p>
<h2 class="heading-2" id="_idParaDest-338"><span class="koboSpan" id="kobo.148.1">Experimentation and model development</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.149.1">During this phase</span><a id="_idIndexMarker1271"/><span class="koboSpan" id="kobo.150.1"> of the </span><a id="_idIndexMarker1272"/><span class="koboSpan" id="kobo.151.1">project lifecycle, data scientists utilize various algorithms and datasets to experiment and develop models to address business problems. </span><span class="koboSpan" id="kobo.151.2">The risks associated with this phase of the project lifecycle are mostly specific to AI/ML. </span><span class="koboSpan" id="kobo.151.3">They encompass a wide range of topics, such as algorithm selection and associated assumptions, limitations, model validation and robustness, model transparency and explainability, model fairness, compliance, and intended model use.</span></p>
<h3 class="heading-3" id="_idParaDest-339"><span class="koboSpan" id="kobo.152.1">Risk considerations</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.153.1">The risk </span><a id="_idIndexMarker1273"/><span class="koboSpan" id="kobo.154.1">associated with model assumptions and limitations arises from the potential inaccuracies, incompleteness, or inconsistencies in assumptions and limitations, leading the model to inadequately fit the situation. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.155.1">For instance, the linear regression algorithm assumes a linear relationship between predictor and response variables, and if such a relationship doesn’t exist, the predictions may be incorrect or biased. </span><span class="koboSpan" id="kobo.155.2">Certain algorithms may also have limitations on data sample size, impacting their performance with small datasets.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.156.1">Model selection introduces risks related to overfitting when models are chosen based solely on training performance and a lack of interpretability when there’s a requirement for explainability. </span><span class="koboSpan" id="kobo.156.2">Inadequate sensitivity and scenario analysis pose risks to model robustness, as a failure to understand the sensitivity of a credit risk model may lead to incorrect predictions. </span><span class="koboSpan" id="kobo.156.3">Similarly, financial forecast models that consider only a limited range of economic scenarios may fail to function correctly during unexpected events.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.157.1">Model transparency risk arises from a lack of transparency hindering the explainability and verification of model decisions, potentially leaving organizations legally vulnerable if they cannot justify AI-driven decisions. </span><span class="koboSpan" id="kobo.157.2">Model fairness risk acknowledges that both data and the model itself can introduce bias, impacting the fairness of the AI system. </span><span class="koboSpan" id="kobo.157.3">For instance, Naïve Bayes algorithms may introduce bias if they assume independence among features, leading to incorrect predictions when features are correlated.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.158.1">Model </span><a id="_idIndexMarker1274"/><span class="koboSpan" id="kobo.159.1">evaluation risk stems from inadequate independent validation or the use of incorrect validation methods or metrics, posing the threat of unexpected behavior when models are not thoroughly tested. </span><span class="koboSpan" id="kobo.159.2">Model use and impact risk encompass the potential negative consequences arising from real-world deployment, as models trained on historical data may perform poorly if the future differs significantly from the past.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.160.1">Missing lineage risk emphasizes the importance of understanding the lineage from the data source to model artifacts, including all transformations and experimentations during the modeling process, to comprehend model behavior and identify the root cause of issues.</span></p>
<h3 class="heading-3" id="_idParaDest-340"><span class="koboSpan" id="kobo.161.1">Risk mitigations</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.162.1">To mitigate</span><a id="_idIndexMarker1275"/><span class="koboSpan" id="kobo.163.1"> these risks, organizations must establish comprehensive MRM standards that cover model evaluation, validation, selection, and fairness. </span><span class="koboSpan" id="kobo.163.2">Additionally, the organization should enhance its capabilities and best practices for recognizing assumptions and limitations, addressing known gaps, and ensuring model transparency and lineage through the following approaches:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.164.1">For model assumption and limitation risk, it is crucial to clearly define and validate the underlying assumptions of algorithms. </span><span class="koboSpan" id="kobo.164.2">Test difficult-to-validate assumptions with various techniques, ensuring completeness, and calculate model uncertainty to determine confidence levels in outputs.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.165.1">To mitigate risks associated with model selection, data scientists should develop a robust set of candidate models, undergo diverse team reviews, and involve technical, business, and target audience representatives to ensure the selected model effectively addresses problems. </span><span class="koboSpan" id="kobo.165.2">The modeling approach should be assessed on whether it is fit for purpose, explainable, reproducible, and robust, with documented decisions and supporting evidence.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.166.1">To address deficiencies in sensitivity and scenario testing, standards for model sensitivity and scenario testing should be established within the MRM framework. </span><span class="koboSpan" id="kobo.166.2">These procedures, integral to the development process, offer insights into boundary conditions impacting model robustness, minimize errors, and enhance comprehension of input-output interplay.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.167.1">To tackle model transparency risks, establish standards promoting communication and feedback within the development team, ensuring transparency throughout the model development process. </span><span class="koboSpan" id="kobo.167.2">Thorough documentation, including model validation techniques, serves as corroborating evidence for transparency.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.168.1">For model fairness issues, define and incorporate model fairness standards. </span><span class="koboSpan" id="kobo.168.2">Embed fairness checks in the model lifecycle, involve stakeholders in issue mitigation, enhance governance techniques, and recognize the ongoing process of fixing discrimination in algorithmic systems.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.169.1">Incorporate</span><a id="_idIndexMarker1276"/><span class="koboSpan" id="kobo.170.1"> model performance evaluation standards within MRM for validation, involving business and technical stakeholders in issue discovery and mitigation. </span><span class="koboSpan" id="kobo.170.2">Employ standardized validation tools and techniques for consistent procedures, and conduct an end-to-end evaluation against agreed-upon standards, monitoring metrics during retraining.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.171.1">Evaluate model use and impact risks within MRM by verifying the understanding of decisions during design/deployment/validation. </span><span class="koboSpan" id="kobo.171.2">Conduct an impact assessment to assess risks against preset thresholds, and verify model outcomes for precision, consistency, relevance, and alignment with trustworthy AI criteria.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.172.1">Establish mechanisms and technology capabilities to track model lineage from data source to deployment. </span><span class="koboSpan" id="kobo.172.2">Implement comprehensive metadata management, version control for relevant artifacts, a model registry, and auditing/logging mechanisms to understand changes made, by whom, and for what purpose.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.173.1">Overall, a comprehensive MRM framework that includes technical standards, ongoing monitoring and updates, and a culture of ethical decision-making is crucial for organizations to effectively manage the risks associated with model development so they can be used in a trustworthy manner.</span></p>
<h2 class="heading-2" id="_idParaDest-341"><span class="koboSpan" id="kobo.174.1">AI system deployment and operations</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.175.1">During </span><a id="_idIndexMarker1277"/><span class="koboSpan" id="kobo.176.1">this stage </span><a id="_idIndexMarker1278"/><span class="koboSpan" id="kobo.177.1">of the lifecycle, organizations design and build technology environments for AI system/model deployment in production to handle real-world business workflows in the broader application ecosystem and establish operational processes and standards to monitor the environments and remediate production issues ranging from basic system failure to model performance degradation.</span></p>
<h3 class="heading-3" id="_idParaDest-342"><span class="koboSpan" id="kobo.178.1">Risk considerations</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.179.1">Human supervision</span><a id="_idIndexMarker1279"/><span class="koboSpan" id="kobo.180.1"> risk involves the crucial practice of subjecting AI models to human review before deployment into production to ensure ongoing suitability for their intended purpose, preventing the deployment of inadequately prepared systems that may lead to production problems or unforeseen outcomes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.181.1">Technology integration risk arises as AI systems are typically integrated into the broader technology ecosystem, supporting multiple business functions. </span><span class="koboSpan" id="kobo.181.2">Challenges may emerge when integrating with upstream and downstream systems for data transfer, model integration, or API integration, potentially causing compatibility issues, such as the wrong model version impacting different systems.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.182.1">Technology scalability risk is associated with unexpected spikes in data volumes, business users, and customers after AI system/model deployment. </span><span class="koboSpan" id="kobo.182.2">Failure to handle scalability scenarios can negatively impact business and user experiences.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.183.1">Model performance and behavior change risk stems from AI systems being developed using historical data. </span><span class="koboSpan" id="kobo.183.2">Unexpected changes in real-world environments, like data drift and outlier conditions, can cause the model to behave differently from the original assumption.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.184.1">Fallback procedure risk underscores the importance of well-established fallback procedures when production issues are detected. </span><span class="koboSpan" id="kobo.184.2">Inadequate fallback protocols can jeopardize system operation continuity.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.185.1">Adversarial attacks present new threats to AI systems. </span><span class="koboSpan" id="kobo.185.2">Adversarial attacks such as feeding bad data to AI systems can lead to incorrect predictions and faulty downstream decisions.</span></p>
<h3 class="heading-3" id="_idParaDest-343"><span class="koboSpan" id="kobo.186.1">Risk mitigations</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.187.1">To effectively</span><a id="_idIndexMarker1280"/><span class="koboSpan" id="kobo.188.1"> manage risks during deployment and operations, robust mitigation mechanisms and technological capabilities are essential. </span><span class="koboSpan" id="kobo.188.2">Rigorous testing and operational checks, integration standards, model performance monitoring, established issue resolution processes, and adversarial monitoring and remediation throughout the AI lifecycle are key focus areas. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.189.1">Let’s explore specific recommendations in depth.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.190.1">A crucial element for effective AI risk management is a model management system with a model registry. </span><span class="koboSpan" id="kobo.190.2">Providing details about models, performance metrics, uses, and related metadata, this system should incorporate MRM standards and embed operationalization checks. </span><span class="koboSpan" id="kobo.190.3">Stress testing and scaling simulation are vital to understanding behavior under heavy loads. </span><span class="koboSpan" id="kobo.190.4">Organizations should establish processes and tools for model owners to monitor, manage, govern, and analyze results.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.191.1">To mitigate AI integration risks, organizations need to incorporate integration standards and requirements in risk management. </span><span class="koboSpan" id="kobo.191.2">Ensuring interoperability among platforms and conducting robust integration testing is crucial. </span><span class="koboSpan" id="kobo.191.3">Verification of proper configuration and integration into the production environment is essential to prevent errors during migration or upgrading.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.192.1">Establishing</span><a id="_idIndexMarker1281"/><span class="koboSpan" id="kobo.193.1"> model deployment review and approval standards is crucial, encompassing a detailed review of design, algorithms, testing results, and performance metrics. </span><span class="koboSpan" id="kobo.193.2">It should outline steps for mitigating potential deployment risks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.194.1">For operational continuity regarding performance and behavior changes, MRM should include model monitoring, performance issue tracking, and resolution standards. </span><span class="koboSpan" id="kobo.194.2">Continuous monitoring of statistical, technical, and business metrics, along with real-time circuit breakers, helps ensure the model operates as intended. </span><span class="koboSpan" id="kobo.194.3">Pre-specifying benchmark or legacy models for fallback options is useful in case performance boundaries are breached.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.195.1">Adversarial attack monitoring standards in MRM are crucial to prevent malicious input from causing model malfunctions. </span><span class="koboSpan" id="kobo.195.2">Effective testing, auditing techniques, and certification programs are needed to address AI model vulnerabilities. </span><span class="koboSpan" id="kobo.195.3">Leveraging research on adversarial attacks and model data leakage for vulnerability testing and ensuring robustness and resilience to various attacks is essential. </span><span class="koboSpan" id="kobo.195.4">Proactive cyber threat hunting should be instituted to detect and isolate advanced threats in networks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.196.1">What we have covered so far does not include all the risks we might encounter throughout the AI lifecycle and new emergent risks are coming up regularly. </span><span class="koboSpan" id="kobo.196.2">It is also not possible and potentially counterproductive to mitigate all the risks in practice. </span><span class="koboSpan" id="kobo.196.3">Organizations should determine the tolerance levels for the different risks, which are going to be highly contextual and application- and use case-specific. </span><span class="koboSpan" id="kobo.196.4">Other factors such as policies established by system owners and regulators, organizational priority, and resource considerations can also influence risk tolerance. </span><span class="koboSpan" id="kobo.196.5">It is also worth noting that risk tolerance is likely to change over time as the influencing factors evolve.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.197.1">It is also important to prioritize the risk identified in the AI lifecycle. </span><span class="koboSpan" id="kobo.197.2">Organizations should recognize that not all risks are the same, and scarce resources should be allocated appropriately to address the different risks. </span><span class="koboSpan" id="kobo.197.3">The assessed risk levels and potential impact of an AI system should be used to prioritize the resource allocation to mitigate these risks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.198.1">Finally, AI risks are not isolated risks and should be considered and incorporated into the broader enterprise risk management strategies and processes. </span><span class="koboSpan" id="kobo.198.2">The roles and responsibilities of different players in managing risks will span different functional domains such as engineering, data science, cybersecurity, audit, and compliance.</span></p>
<h1 class="heading-1" id="_idParaDest-344"><span class="koboSpan" id="kobo.199.1">Designing ML platforms with governance and risk management considerations</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.200.1">ML technology </span><a id="_idIndexMarker1282"/><span class="koboSpan" id="kobo.201.1">systems play a crucial role in the AI risk management process and activities. </span><span class="koboSpan" id="kobo.201.2">To begin with, these systems must be developed and constructed to comply with both internal and external policies and guidelines. </span><span class="koboSpan" id="kobo.201.3">Additionally, technology can aid in streamlining and automating ML governance procedures. </span><span class="koboSpan" id="kobo.201.4">The following figure illustrates the different ML governance touchpoints in an enterprise ML platform. </span><span class="koboSpan" id="kobo.201.5">It is important to know that ML technology alone can only help address a subset of AI risks; other enterprise security technology needs to be incorporated to form a more comprehensive governance and defense mechanism.</span></p>
<p class="packt_figref"><span class="koboSpan" id="kobo.202.1"><img alt="" role="presentation" src="../Images/B20836_12_02.png"/></span></p>
<p class="packt_figref"><span class="koboSpan" id="kobo.203.1">Figure 12.2: ML platform and ML governance</span></p>
<div class="note">
<p class="normal"><span class="koboSpan" id="kobo.204.1">In the preceding figure, the ML governance touchpoints have been integrated into the MLOps architecture depicted in </span><em class="italic"><span class="koboSpan" id="kobo.205.1">Figure 9.4</span></em><span class="koboSpan" id="kobo.206.1"> of </span><em class="chapterRef"><span class="koboSpan" id="kobo.207.1">Chapter 9</span></em><span class="koboSpan" id="kobo.208.1">, </span><em class="italic"><span class="koboSpan" id="kobo.209.1">Designing an Enterprise ML Architecture with AWS ML Services</span></em><span class="koboSpan" id="kobo.210.1">.</span></p>
</div>
<p class="normal"><span class="koboSpan" id="kobo.211.1">When an ML platform is built with AI risk management and governance in mind, it can gather and furnish the information to support the MRM programs while optimizing the risk management workflows. </span><span class="koboSpan" id="kobo.211.2">Online data stores, workflow applications, document-sharing systems, and model inventory databases are</span><a id="_idIndexMarker1283"/><span class="koboSpan" id="kobo.212.1"> among the technology solutions employed for AI governance. </span><span class="koboSpan" id="kobo.212.2">In the following sections, let’s delve deeper into some of the core ML governance components and see where an ML platform or technology can fit in.</span></p>
<h2 class="heading-2" id="_idParaDest-345"><span class="koboSpan" id="kobo.213.1">Data and model documentation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.214.1">One of the </span><a id="_idIndexMarker1284"/><span class="koboSpan" id="kobo.215.1">essential elements of AI governance is documentation. </span><span class="koboSpan" id="kobo.215.2">All models used for decision-making should be properly documented. </span><span class="koboSpan" id="kobo.215.3">The scope of the documentation may include the following:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.216.1">Data overview, data quality report on the valuation, and assessment of the input data</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.217.1">Model development document including methodology and assumption, model usage instructions, performance and validation results, and other qualitative and quantitative analysis</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.218.1">Model validation strategy and report by the second and third lines of defense </span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.219.1">Model performance monitoring results and data drift reports</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.220.1">Model implementation and user acceptance testing reports</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.221.1">The role of the ML platform in ML governance documentation is usually to provide data points that feed into the formal risk management documentation or generate some ready-to-use reports. </span><span class="koboSpan" id="kobo.221.2">Specifically, an ML platform should be able to track, store, and report the following data points:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.222.1">Data quality metrics such as data description, statistics, bias, and errors</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.223.1">Model metrics and validation results in development and testing</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.224.1">Model bias and explainability reports</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.225.1">Model performance monitoring results in production</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.226.1">Model description and intended use</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.227.1">Risk rating and classification details</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.228.1">Different ML platforms have varying capabilities supporting the AI governance documentation requirements. </span><span class="koboSpan" id="kobo.228.2">Here, we will discuss the various capabilities of SageMaker in supporting these requirements. </span><span class="koboSpan" id="kobo.228.3">SageMaker can produce data and documents to be incorporated into model risk documentation. </span><span class="koboSpan" id="kobo.228.4">This includes tracking and producing information that is relevant for AI governance documentation, such as:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.229.1">Model metrics</span></strong><span class="koboSpan" id="kobo.230.1">: The SageMaker training service tracks model metrics such as training errors and validation errors.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.231.1">Data and model bias reports</span></strong><span class="koboSpan" id="kobo.232.1">: SageMaker Clarify is the bias detection component in SageMaker. </span><span class="koboSpan" id="kobo.232.2">If you enable SageMaker Clarify, you can get data and model bias reports for the training data and trained model. </span><span class="koboSpan" id="kobo.232.3">The data and model bias reports have details such as imbalances in training data and prediction behavior across different age groups and genders.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.233.1">Model explainability reports</span></strong><span class="koboSpan" id="kobo.234.1">: SageMaker Clarify also provides a model explainability feature. </span><span class="koboSpan" id="kobo.234.2">It uses SHAP to explain the contribution of each input to the final decision. </span><span class="koboSpan" id="kobo.234.3">You can get more details about SHAP at </span><a href="https://shap.readthedocs.io/en/latest/index.html"><span class="url"><span class="koboSpan" id="kobo.235.1">https://shap.readthedocs.io/en/latest/index.html</span></span></a><span class="koboSpan" id="kobo.236.1">.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.237.1">Model Card</span></strong><span class="koboSpan" id="kobo.238.1">: SageMaker Model Card can be used to document critical information about ML models, such as the intended use of the model, risk rating, and detailed descriptions of model training and performance.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.239.1">Both </span><a id="_idIndexMarker1285"/><span class="koboSpan" id="kobo.240.1">open-source and managed model registry platforms are available for managing model registries. </span><span class="koboSpan" id="kobo.240.2">For example, the MLflow Model Registry is an open-source option, while Amazon SageMaker offers a managed model registry service. </span><span class="koboSpan" id="kobo.240.3">The SageMaker Model Registry has several key capabilities that can aid in ML governance activities and processes:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.241.1">Model inventory</span></strong><span class="koboSpan" id="kobo.242.1">: All versions of the different models belong to a respective model group in the SageMaker Model Registry. </span><span class="koboSpan" id="kobo.242.2">You can view all the model groups and different versions of a model here. </span><span class="koboSpan" id="kobo.242.3">Metadata such as model metrics, training job details, hyperparameters used for training, and training data sources are important data points for the model reviews and model audit processes. </span><span class="koboSpan" id="kobo.242.4">Depending on specific business requirements, you can set up a central model registry for a single enterprise view, or distributed model registries if that will meet the governance and audit requirements.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.243.1">Model approval and lifecycle tracking</span></strong><span class="koboSpan" id="kobo.244.1">: You can track the approval of models and model stages directly inside the SageMaker Model Registry. </span><span class="koboSpan" id="kobo.244.2">This detail helps the business operations and audit to ensure the proper processes are followed.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.245.1">Monitoring models after deployment is crucial to identify any potential failures and take prompt remedial action to mitigate risks. </span><span class="koboSpan" id="kobo.245.2">To ensure smooth functioning, models must be monitored for system availability and errors, as well as data and model drift and prediction failure. </span><span class="koboSpan" id="kobo.245.3">Amazon SageMaker offers a model monitoring feature that can detect both data drift and model drift. </span><span class="koboSpan" id="kobo.245.4">SageMaker Model Monitor provides the</span><a id="_idIndexMarker1286"/><span class="koboSpan" id="kobo.246.1"> following capabilities:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.247.1">Data drift</span></strong><span class="koboSpan" id="kobo.248.1">: With SageMaker Model Monitor, you can monitor data quality issues and data distribution skews (aka data drift) in production. </span><span class="koboSpan" id="kobo.248.2">To use this feature, you create a baseline using a baseline dataset, such as a model training dataset, to get data statistics and data types and suggest constraints for monitoring. </span><span class="koboSpan" id="kobo.248.3">SageMaker Model Monitor can capture live inference traffic, calculate data statistics, examine data types, verify them against constraints, and trigger alerts. </span><span class="koboSpan" id="kobo.248.4">For example, if a feature’s mean and standard deviation change significantly from the baseline, an alert can be triggered.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.249.1">Model performance drift</span></strong><span class="koboSpan" id="kobo.250.1">: You can use SageMaker Model Monitor to detect model performance changes in production. </span><span class="koboSpan" id="kobo.250.2">To use this feature, you create a model performance baseline job using a baseline dataset that contains both the inputs and labels. </span><span class="koboSpan" id="kobo.250.3">The baseline job will suggest</span><a id="_idIndexMarker1287"/><span class="koboSpan" id="kobo.251.1"> constraints, which are the metrics thresholds that Model Monitor will monitor against the metrics to be calculated with ground truth data collected in production. </span><span class="koboSpan" id="kobo.251.2">Metrics can be optionally sent to CloudWatch for visualization.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.252.1">Feature attribution drift</span></strong><span class="koboSpan" id="kobo.253.1">: When enabled with SageMaker Clarify, SageMaker Model Monitor can report feature attribution drift. </span><span class="koboSpan" id="kobo.253.2">Feature attributions are indicators of feature importance to the prediction output. </span><span class="koboSpan" id="kobo.253.3">Similar to data and model drift, you create a SHAP baseline job using baseline data to generate constraint suggestions. </span><span class="koboSpan" id="kobo.253.4">The separate monitoring job is then scheduled to monitor predictions in production against the baseline.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-346"><span class="koboSpan" id="kobo.254.1">Lineage and reproducibility</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.255.1">MRM requires</span><a id="_idIndexMarker1288"/><span class="koboSpan" id="kobo.256.1"> establishing lineage across data and models to ensure reproducibility. </span><span class="koboSpan" id="kobo.256.2">Lineage is the systematic tracking and documentation of the origin, transformations, and dependencies of data, as well as the development and evolution of ML models, providing transparency and accountability throughout the entire process. </span><span class="koboSpan" id="kobo.256.3">Lineage information includes training data sources, algorithm selection, hyperparameter configurations, and the model training script. </span><span class="koboSpan" id="kobo.256.4">SageMaker offers several features that aid in establishing lineage:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.257.1">SageMaker training jobs keep lineage data such as the training data source, training job container (contains the algorithm and training script), hyperparameter configuration, and model artifact location. </span><span class="koboSpan" id="kobo.257.2">Historical training job data is immutable in the SageMaker environment for record retention purposes.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.258.1">SageMaker Experiments and ML Lineage Tracking can contain additional component details such as data processing for more complete lineage tracking.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.259.1">SageMaker hosting has information on the location of the original model artifact and the inference container to trace the lineage from the model to the endpoint.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.260.1">These lineage data points, such as training data source and training configurations, are available by calling the SageMaker API; an external application can call the SageMaker API directly to extract this data for review purposes. </span><span class="koboSpan" id="kobo.260.2">Alternatively, a data extraction job can be developed to extract these data points and load them into a purpose-built risk management store for analysis.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.261.1">The significance of ML privacy is growing rapidly in the implementation of ML systems. </span><span class="koboSpan" id="kobo.261.2">To comply with data privacy regulations or internal data privacy controls, ML systems must have fundamental infrastructure security features, such as data encryption, network isolation, compute isolation, and private connectivity. </span><span class="koboSpan" id="kobo.261.3">By utilizing a</span><a id="_idIndexMarker1289"/><span class="koboSpan" id="kobo.262.1"> SageMaker-based ML platform, you can enable the following essential security controls:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.263.1">Private networking</span></strong><span class="koboSpan" id="kobo.264.1">: As SageMaker is a fully managed service, it runs in an AWS-owned account. </span><span class="koboSpan" id="kobo.264.2">By default, resources in your own AWS account communicate with SageMaker APIs via the public internet. </span><span class="koboSpan" id="kobo.264.3">To enable private connectivity to SageMaker components from your own AWS environment, you can attach them to a subnet in your own </span><strong class="keyWord"><span class="koboSpan" id="kobo.265.1">virtual private cloud</span></strong><span class="koboSpan" id="kobo.266.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.267.1">VPC</span></strong><span class="koboSpan" id="kobo.268.1">).</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.269.1">Storage encryption</span></strong><span class="koboSpan" id="kobo.270.1">: Data-at-rest encryption can be enabled by providing an encryption key when you create a SageMaker notebook, a training job, a processing job, or a hosting endpoint.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.271.1">Disabling internet access</span></strong><span class="koboSpan" id="kobo.272.1">: By default, the SageMaker notebook, training job, and hosting service have access to the internet. </span><span class="koboSpan" id="kobo.272.2">Internet access can be disabled via configuration.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-347"><span class="koboSpan" id="kobo.273.1">Observability and auditing</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.274.1">Auditing</span><a id="_idIndexMarker1290"/><span class="koboSpan" id="kobo.275.1"> primarily concentrates on process verification and artifact collection to support audit activities. </span><span class="koboSpan" id="kobo.275.2">The platform on which the process takes place usually functions as an information source for collecting artifacts. </span><span class="koboSpan" id="kobo.275.3">For instance, suppose there is an MRM policy that necessitates approval before deploying a model into production. </span><span class="koboSpan" id="kobo.275.4">In that case, the audit will require access to the system of records to ensure that the required data is collected and retained.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.276.1">SageMaker and other related services can be a data source in support of the overall audit process. </span><span class="koboSpan" id="kobo.276.2">Specifically, it provides the following information that can be relevant for auditing purposes:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.277.1">Activity and access audit trail</span></strong><span class="koboSpan" id="kobo.278.1">: SageMaker sends all audit trail data to CloudWatch Logs, which can be retained and analyzed for audit purposes.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.279.1">Model approval tracking</span></strong><span class="koboSpan" id="kobo.280.1">: Model deployment approvals are tracked in the SageMaker Model Registry. </span><span class="koboSpan" id="kobo.280.2">This can be provided to the auditor as evidence that required approval processes are followed.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.281.1">Lineage tracking</span></strong><span class="koboSpan" id="kobo.282.1">: SageMaker Experiments and ML Lineage Tracking components can track and retain model lineages such as data processing, model training, and model deployment. </span><span class="koboSpan" id="kobo.282.2">Lineage Tracking information helps the auditor verify that the model can be reproduced using its original data and configuration dependencies.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.283.1">Configuration changes</span></strong><span class="koboSpan" id="kobo.284.1">: System configuration data is captured in AWS CloudTrail as change events. </span><span class="koboSpan" id="kobo.284.2">For example, when a SageMaker endpoint is deleted, there will be an entry in CloudTrail indicating this change.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-348"><span class="koboSpan" id="kobo.285.1">Scalability and performance</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.286.1">To mitigate the </span><a id="_idIndexMarker1291"/><span class="koboSpan" id="kobo.287.1">potential scalability risk, AI systems should be designed to handle dynamic and unexpected loads. </span><span class="koboSpan" id="kobo.287.2">For an ML platform, this usually means that the training infrastructure is designed and implemented to support a single large training job as well as many training jobs running in parallel. </span><span class="koboSpan" id="kobo.287.3">Similarly, the model hosting infrastructure should be capable of handling a large number of models running in parallel as well as running a large number of model instances across many nodes.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.288.1">If your platform of choice is SageMaker, then the following capabilities can help mitigate training and hosting scaling challenges:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.289.1">Training infrastructure scaling</span></strong><span class="koboSpan" id="kobo.290.1">: SageMaker has support for large-scale distributed training, with the ability to utilize hundreds of nodes and thousands of CPUs/GPUs. </span><span class="koboSpan" id="kobo.290.2">Additionally, SageMaker provides a purpose-built library for running both data-parallel and model-parallel training jobs. </span><span class="koboSpan" id="kobo.290.3">For storage scaling, high-performance storage solutions like </span><strong class="keyWord"><span class="koboSpan" id="kobo.291.1">Elastic File System</span></strong><span class="koboSpan" id="kobo.292.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.293.1">EFS</span></strong><span class="koboSpan" id="kobo.294.1">) and FSx can be mounted onto SageMaker training nodes to accommodate training jobs that require a large-scale dataset exceeding 1 TB. </span><span class="koboSpan" id="kobo.294.2">AWS accounts can run multiple training jobs in parallel, and the soft limit can be increased upon request.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.295.1">Hosting infrastructure scaling:</span></strong><span class="koboSpan" id="kobo.296.1"> SageMaker offers several options to scale model hosting needs. </span><span class="koboSpan" id="kobo.296.2">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.297.1">Multi-Model Endpoint</span></strong><span class="koboSpan" id="kobo.298.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.299.1">MME</span></strong><span class="koboSpan" id="kobo.300.1">) capability allows you to host multiple models behind a single</span><a id="_idIndexMarker1292"/><span class="koboSpan" id="kobo.301.1"> endpoint while reducing costs. </span><span class="koboSpan" id="kobo.301.2">SageMaker’s automatic scaling feature enables you to define a scaling policy based on metrics such as the number of invocations per host, which can increase the number of instances running the same model automatically when the traffic increases. </span><span class="koboSpan" id="kobo.301.3">Additionally, the serverless inference option allows you to run a single model concurrently up to the maximum number supported by SageMaker.</span></li>
</ul>
<h2 class="heading-2" id="_idParaDest-349"><span class="koboSpan" id="kobo.302.1">Data quality</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.303.1">Data quality checks</span><a id="_idIndexMarker1293"/><span class="koboSpan" id="kobo.304.1"> should take place in multiple phases of the lifecycle, including data acquisition and processing, exploratory data analysis and data wrangling, feature engineering, and model inference. </span><span class="koboSpan" id="kobo.304.2">The checks should cover many aspects of data quality, such as missing data, data accuracy, inconsistency across different sources, incorrect format, incompleteness, imbalanced data, and duplication. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.305.1">From an AWS technology perspective, there are several purpose-built tools and features that can help with data quality management:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.306.1">AWS Glue DataBrew offers a range of data quality features that can help ensure the accuracy and reliability of data used for analysis or model training. </span><span class="koboSpan" id="kobo.306.2">DataBrew is mainly used by data engineers who are responsible for sourcing and cleaning the data during the data acquisition and processing phase for downstream users such as data scientists. </span><span class="koboSpan" id="kobo.306.3">Some of these features include:</span><ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.307.1">Data profiling</span></strong><span class="koboSpan" id="kobo.308.1">: DataBrew can automatically profile datasets to identify data quality issues, such as missing or inconsistent values, outliers, or duplicates.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.309.1">Data cleaning</span></strong><span class="koboSpan" id="kobo.310.1">: DataBrew provides a range of data cleaning transformations that can be applied to address common data quality issues, such as filling in missing values, removing duplicates, or standardizing data formats.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.311.1">Data validation</span></strong><span class="koboSpan" id="kobo.312.1">: DataBrew can perform data validation checks to ensure that data values fall within expected ranges or conform to predefined standards or formats.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.313.1">Data lineage</span></strong><span class="koboSpan" id="kobo.314.1">: DataBrew tracks the lineage of data transformations to help ensure that data is being processed correctly and that any changes can be traced back to their source.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.315.1">Data versioning</span></strong><span class="koboSpan" id="kobo.316.1">: DataBrew supports the versioning of datasets, making it easy to track changes and roll back to previous versions if necessary.</span></li>
</ul>
</li>
<li class="bulletList"><span class="koboSpan" id="kobo.317.1">SageMaker Data Wrangler offers some similar data quality capabilities, targeting mainly data scientists who are doing data exploratory analysis and feature engineering in the SageMaker environment. </span><span class="koboSpan" id="kobo.317.2">The Data Quality and Insights report in Data Wrangler can automatically verify data quality (such as missing values, duplicate rows, and data types) and help detect anomalies (such as outliers, class imbalance, and data leakage) in your data.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.318.1">In conclusion, ML technology</span><a id="_idIndexMarker1294"/><span class="koboSpan" id="kobo.319.1"> systems serve as pivotal assets in the AI risk management landscape. </span><span class="koboSpan" id="kobo.319.2">The foundational step involves aligning these systems with internal and external policies, ensuring robust compliance. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.320.1">Furthermore, leveraging tools like SageMaker and MLOps systems emerges as a strategic approach, offering substantial support in documentation, lineage tracking, data management, and quality assurance. </span><span class="koboSpan" id="kobo.320.2">By enhancing observability and enabling thorough auditing, these technologies enable organizations to navigate the complexities of AI risk management with efficiency and precision.</span></p>
<h1 class="heading-1" id="_idParaDest-350"><span class="koboSpan" id="kobo.321.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.322.1">This chapter delved into several areas related to AI risk management and the technology platforms that support it. </span><span class="koboSpan" id="kobo.322.2">By now, you should have a solid understanding of the key AI-related risk scenarios, why AI risk management is critical, and how to detect and address potential risks throughout the AI lifecycle. </span><span class="koboSpan" id="kobo.322.3">Additionally, you should be aware of the significance of ML platforms in supporting AI risk management. </span><span class="koboSpan" id="kobo.322.4">It is worth noting that AI risk is a vast and complex domain with many unresolved risk challenges and new emergent risks arising rapidly. </span><span class="koboSpan" id="kobo.322.5">Moreover, the fast advancement in AI technology and adoption is also creating new risk exposure that risk management professionals must constantly address.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.323.1">In the next chapter, we will dive deeper into several specific AI risk topics and mitigation techniques, including bias, model explainability, model robustness, and adversarial attacks.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.324.1">Leave a review!</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.325.1">Enjoying this book? </span><span class="koboSpan" id="kobo.325.2">Help readers like you by leaving an Amazon review. </span><span class="koboSpan" id="kobo.325.3">Scan the QR code below to get a free eBook of your choice.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.326.1"><img alt="" role="presentation" src="../Images/Review_Copy.png"/></span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.327.1">*Limited Offer</span></em></p>
</div>
</body></html>