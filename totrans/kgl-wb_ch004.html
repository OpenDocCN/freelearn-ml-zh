<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>03 Cassava Leaf Disease competition</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>
<section id="cassava-leaf-disease-competition" class="level1 pkt" data-number="4">
<h1 data-number="4">03 Cassava Leaf Disease competition</h1>
<section id="join-our-book-community-on-discord-2" class="level2" data-number="4.1">
<h2 data-number="4.1">Join our book community on Discord</h2>
<p><a href="https://packt.link/EarlyAccessCommunity">https://packt.link/EarlyAccessCommunity</a></p>
<figure>
<img src="../media/file10.png" style="width:10em" />
</figure>
<p>In this chapter we will leave the domain of tabular data and focus on image processing. In order to demonstrate the steps necessary to do well in classification competitions, we will use the data from the Cassava Leaf Disease contest:</p>
<p><a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification">https://www.kaggle.com/competitions/cassava-leaf-disease-classification</a></p>
<p>The first thing to do upon starting a Kaggle competition is to read the descriptions properly:</p>
<blockquote>
<em>“As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions. At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields. With the help of data science, it may be possible to identify common diseases so they can be treated.”</em>
</blockquote>
<p>So this competition relates to an actually important real-life problem - your mileage might vary, but in general it is useful to know that.</p>
<blockquote>
<em>“Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants. This suffers from being labor-intensive, low-supply and costly. As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth.”</em>
</blockquote>
<p>This paragraph - especially the last sentence - sets the expectations: since the data is coming from diverse sources, we are likely to have some challenges related to quality of the images and (possibly) distribution shift.</p>
<blockquote>
<em>“Your task is to classify each cassava image into four disease categories or a fifth category indicating a healthy leaf. With your help, farmers may be able to quickly identify diseased plants, potentially saving their crops before they inflict irreparable damage.”</em>
</blockquote>
<p>This bit is rather important: it specifies that this is a classification competition, and the number of classes is small (5).</p>
<p>With the introductory footwork out of the way, let us have a look at the data.</p>
</section>
<section id="understanding-the-data-and-metrics" class="level2" data-number="4.2">
<h2 data-number="4.2">Understanding the data and metrics</h2>
<p>Upon entering the “Data” tab for this competition, we see the summary of the provided data:</p>
<figure>
<img src="../media/file11.png" alt="Figure 3.1: description of the Cassava competition dataset" /><figcaption aria-hidden="true">Figure 3.1: description of the Cassava competition dataset</figcaption>
</figure>
<p>What can we make of that?</p>
<ul>
<li>The data is in a fairly straightforward format, where the organisers even provided the mapping between disease names and numerical codes</li>
<li>We have the data in tfrecord format, which is good news for anyone interested in using a TPU</li>
<li>The provided test set is only a small subset and to be substituted with the full dataset at evaluation time. <strong>This suggests that loading a previously trained model at evaluation time and using it for inference is a preferred strategy.</strong></li>
</ul>
<p>The evaluation metric was chosen to be categorization accuracy: <a href="https://developers.google.com/machine-learning/crash-course/classification/accuracy">https://developers.google.com/machine-learning/crash-course/classification/accuracy</a>. This metric takes discrete values as inputs, which means potential ensembling strategies become somewhat more involved. Loss function is implemented during training to optimise a learning function and as long as we want to use methods based on gradient descent, this one needs to be continuous; evaluation metric, on the other hand, is used after training to measure overall performance and as such, can be discrete.</p>
<p><strong>Exercise</strong>: without building a model, write a code to conduct a basic EDA</p>
<ul>
<li>Compare the cardinality of classes in our classification problem</li>
</ul>
<p>Normally, this would also be the moment to check for distribution shift: if the images are very different in train and test set, it is something that most certainly needs to be taken into account. However, since we do not have access to the complete dataset in this case, the step is omitted - please check Chapter 1 for a discussion of adversarial validation, which is a popular technique for detecting concept drift between datasets.</p>
</section>
<section id="building-a-baseline-model" class="level2" data-number="4.3">
<h2 data-number="4.3">Building a baseline model</h2>
<p>We start our approach by building a baseline solution. The notebook running an end-to-end solution is available at:</p>
<p>While hopefully useful as a starting point for other competitions you might want to try, it is more educational to follow the flow described in this section, i.e. copying the code cell by cell, so that you can understand it better (and of course improve on it - it is called a baseline solution for a reason).</p>
<figure>
<img src="../media/file12.png" alt="Figure 3.2: the imports needed for our baseline solution" /><figcaption aria-hidden="true">Figure 3.2: the imports needed for our baseline solution</figcaption>
</figure>
<p>We begin by importing the necessary packages - while personal differences in style are a natural thing, it is our opinion that gathering the imports in one place makes the code easier to maintain as the competition progresses and you move towards more elaborate solutions. In addition, we create a configuration class: a placeholder for all the parameters defining our learning process:</p>
<figure>
<img src="../media/file13.png" alt="Figure 3.3: configuration class for our baseline solution" /><figcaption aria-hidden="true">Figure 3.3: configuration class for our baseline solution</figcaption>
</figure>
<p>The components include:</p>
<ul>
<li>The data folder is mostly useful if you train models outside of Kaggle sometimes (e.g. in Google Colab or on your local machine)</li>
<li>BATCH_SIZE is a parameter that sometimes needs adjusting if you want to optimise your training process (or make it possible at all, for large images in constrained memory environment)</li>
<li>Modifying EPOCHS is useful for debugging: start with small number of epochs to verify that your solution is running smoothly end-to-end and increase as you are moving towards a proper solution</li>
<li>TARGET_SIZE defines the size to which we want to rescale our images</li>
<li>NCLASSES corresponds to the number of possible classes in our classification problem</li>
</ul>
<p>A good practice for coding a solution is to encapsulate the important bits in functions - and creating our trainable model certainly qualifies as important:</p>
<figure>
<img src="../media/file14.png" alt="Figure 3.4: function to create our model" /><figcaption aria-hidden="true">Figure 3.4: function to create our model</figcaption>
</figure>
<p>Few remarks around this step:</p>
<ul>
<li>While more expressive options are available, it is practical to begin with a fast model that can quickly iterated upon; EfficientNet <a href="https://paperswithcode.com/method/efficientnet">https://paperswithcode.com/method/efficientnet</a> architecture fits the bill quite well</li>
<li>We add a pooling layer for regularisation purposes</li>
<li>Add a classification head - a Dense layer with CFG.NCLASSES indicating the number of possible results for the classifier</li>
<li>Finally, we compile the model with loss and metric corresponding to the requirements for this competition.</li>
</ul>
<p><strong>Exercise</strong>: Examine the possible choices for loss and metric - a useful guide is <a href="https://keras.io/api/losses/">https://keras.io/api/losses/</a> What would the other reasonable options be?</p>
<p>Next step is the data:</p>
<figure>
<img src="../media/file15.png" />
</figure>
<figure>
<img src="../media/file16.png" alt="Figure 3.5: setting up the data generator" /><figcaption aria-hidden="true">Figure 3.5: setting up the data generator</figcaption>
</figure>
<p>Next we setup the model - straightforward, thanks to the function we defined above:</p>
<figure>
<img src="../media/file17.png" alt="Figure 3.6: instantiating the model" /><figcaption aria-hidden="true">Figure 3.6: instantiating the model</figcaption>
</figure>
<p>Before we proceed with training the model, we should dedicate some attention to callbacks:</p>
<figure>
<img src="../media/file18.png" alt="Figure 3.7: Model callbacks" /><figcaption aria-hidden="true">Figure 3.7: Model callbacks</figcaption>
</figure>
<p>Some points worth mentioning:</p>
<ul>
<li>ModelCheckpoint is used to ensure we keep the weights for the best model only, where the optimality is decided on the basis of the metric to monitor (validation loss in this instance)</li>
<li>EarlyStopping helps us control the risk of overfitting by ensuring that the training is stopped if the validation loss does not improve (decrease) for a given number of epochs</li>
<li>ReduceLROnPlateau is a schema for learning rate</li>
</ul>
<p><strong>Exercise</strong>: what parameters would it make sense to modify in the above setup, and which ones can be left at their default values?</p>
<p>With this setup, we can fit the model:</p>
<figure>
<img src="../media/file19.png" alt="Figure 3.8: Fitting the model" /><figcaption aria-hidden="true">Figure 3.8: Fitting the model</figcaption>
</figure>
<p>Once the training is complete, we can use the model to build the prediction of image class for each image in the test set. Recall that in this competition the public (visible) test set consisted of a single image and the size of the full one was unknown - hence the need for a slightly convoluted manner of constructing the submission dataframe</p>
<figure>
<img src="../media/file20.png" alt="Figure 3.9: Generating a submission" /><figcaption aria-hidden="true">Figure 3.9: Generating a submission</figcaption>
</figure>
<p>In this section we have demonstrated how to start to compete in a competition focused on image classification - you can use this approach to move quickly from basic EDA to a functional submission. However, a rudimentary approach like this is unlikely to produce very competitive results. For this reason, in the next section we discuss more specialised techniques that were utilised in top scoring solutions.</p>
</section>
<section id="learnings-from-top-solutions" class="level2" data-number="4.4">
<h2 data-number="4.4">Learnings from top solutions</h2>
<p>In this section we gather aspects from the top solutions that could allow us to rise above the level of the baseline solution. Keep in mind that the leaderboard (both public and private) in this competition were quite tight; this was a combination of a few factors:</p>
<ul>
<li>The noisy data - it was easy to get to .89 accuracy by correctly identifying large part of the train data, and then each new correct one allowed for a tiny move upward</li>
<li>The metric - accuracy can be tricky to ensemble</li>
<li>Limited size of the data</li>
</ul>
<section id="pretraining" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1">Pretraining</h3>
<p>First and most obvious remedy to the issue of limited data size was pretraining: using more data. The Cassava competition was held a year before as well:</p>
<p><a href="https://www.kaggle.com/competitions/cassava-disease/overview">https://www.kaggle.com/competitions/cassava-disease/overview</a></p>
<p>With minimal adjustments, the data from the 2019 edition could be leveraged in the context of the current one. Several competitors addressed the topic:</p>
<ul>
<li>Combined 2019 + 2020 dataset in TFRecords format was released in the forum: <a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/199131</a></li>
<li>Winning solution from the 2019 edition served as a useful starting point: <a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/216985</a></li>
<li>Generating predictions on 2019 data and using the pseudo-labels to augment the dataset was reported to yield some (minor) improvements <a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/203594</a></li>
</ul>
</section>
<section id="tta" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2">TTA</h3>
<p>The idea behind Test Time Augmentation (TTA) is to apply different transformations to the test image: rotations, flipping and translations. This creates a few different versions of the test image and we generate a prediction for each of them. The resulting class probabilities are then averaged to get a more confident answer. An excellent demonstration of this technique is given in a notebook by Andrew Khael: <a href="https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it">https://www.kaggle.com/code/andrewkh/test-time-augmentation-tta-worth-it</a></p>
<p>TTA was used extensively by the top solutions in the Cassava competition, an excellent example being the top3 private LB result: <a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150</a></p>
</section>
<section id="transformers" class="level3" data-number="4.4.3">
<h3 data-number="4.4.3">Transformers</h3>
<p>While more known architectures like ResNext and EfficientNet were used a lot in the course of the competition, it was the addition of more novel ones that provided the extra edge to many competitors yearning for progress in a tightly packed leaderboard. Transformers emerged in 2017 as a revolutionary architecture for NLP (if somehow you missed the paper that started it all, here it is: <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>) and were such a spectacular success that inevitably many people started wondering if they could be applied to other modalities as well - vision being an obvious candidate. Aptly named Vision Transformer made one of its first appearances in a Kaggle competition in the Cassava contest. An excellent tutorial for ViT has been made public:</p>
<p><a href="https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline">https://www.kaggle.com/code/abhinand05/vision-transformer-vit-tutorial-baseline</a></p>
<p>Vision Transformer was an important component of the winning solution:</p>
<p><a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221150</a></p>
</section>
<section id="ensembling" class="level3" data-number="4.4.4">
<h3 data-number="4.4.4">Ensembling</h3>
<p>The core idea of ensembling is very popular on Kaggle (see Chapter 9 of the Kaggle Book for a more elaborate description) and Cassava competition was no exception. As it turned out, combining diverse architectures was very beneficial (by averaging the class probabilities): EfficientNet, ResNext and ViT are sufficiently different from each other that their predictions complement each other. Another important angle was stacking, i.e. using models in two stages: the 3rd place solution combined predictions from multiple models and then applied LightGBM as a blender</p>
<p><a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/220751</a></p>
<p>The winning solution involved a different approach (with fewer models in the final blend), but relying on the same core logic:</p>
<p><a href="https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957">https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957</a></p>
</section>
</section>
<section id="summary-2" class="level2" data-number="4.5">
<h2 data-number="4.5">Summary</h2>
<p>In this chapter, we have described how to get started with a baseline solution for an image classification competition and discussed a diverse set of possible extensions for moving to a competitive (medal) zone.</p>
</section>
</section>
</body>
</html>
