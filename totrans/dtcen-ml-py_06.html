<html><head></head><body>
<div id="_idContainer071">
<h1 class="chapter-number" id="_idParaDest-83"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-84"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.2.1">Techniques for Programmatic Labeling in Machine Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In machine learning, the accurate labeling of data is crucial for training effective models. </span><span class="koboSpan" id="kobo.3.2">Data labeling involves assigning meaningful categories or classes to data instances, and while traditionally a human-driven process, there are various programmatic approaches to dataset labeling. </span><span class="koboSpan" id="kobo.3.3">This chapter delves into the following methods of programmatic data labeling in </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">machine learning:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.5.1">Pattern matching</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.6.1">Database</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.8.1">DB</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">) lookup</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.10.1">Boolean flags</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.11.1">Weak supervision</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.12.1">Semi-weak supervision</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.13.1">Slicing functions</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.14.1">Active learning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">Transfer learning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.16.1">Semi-supervised learning</span></span></li>
</ul>
<h1 id="_idParaDest-85"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.17.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.18.1">To execute the code examples provided in this chapter on programmatic labeling techniques, ensure that you have the following technical prerequisites installed in your </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Python environment:</span></span></p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.20.1">Python version</span></h2>
<p><span class="koboSpan" id="kobo.21.1">The examples in </span><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.22.1">this chapter require Python version 3.7 or higher. </span><span class="koboSpan" id="kobo.22.2">You can check your Python version by running </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.24.1">
import sys
print(sys.version)</span></pre> <p><span class="koboSpan" id="kobo.25.1">We recommend using the</span><a id="_idIndexMarker319"/><span class="koboSpan" id="kobo.26.1"> Jupyter Notebook </span><strong class="bold"><span class="koboSpan" id="kobo.27.1">integrated development environment</span></strong><span class="koboSpan" id="kobo.28.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.29.1">IDE</span></strong><span class="koboSpan" id="kobo.30.1">) for an interactive and organized coding experience. </span><span class="koboSpan" id="kobo.30.2">If you don’t have it installed, you can install it using </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">this line:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.32.1">
pip install jupyter</span></pre> <p><span class="koboSpan" id="kobo.33.1">Launch Jupyter Notebook with the </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.35.1">
jupyter notebook</span></pre> <h2 id="_idParaDest-87"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.36.1">Library requirements</span></h2>
<p><span class="koboSpan" id="kobo.37.1">Ensure that the following Python packages are installed in your environment. </span><span class="koboSpan" id="kobo.37.2">You can install them using the </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">following commands:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.39.1">
pip install snorkel
pip install scikit-learn
pip install Pillow
pip install tensorflow
pip install pandas
pip install numpy</span></pre> <p><span class="koboSpan" id="kobo.40.1">Additionally, for the TensorFlow and Keras components, you may need GPU support for optimal performance. </span><span class="koboSpan" id="kobo.40.2">Refer to the TensorFlow documentation for GPU installation instructions if you have a </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">compatible GPU.</span></span></p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.42.1">Pattern matching</span></h1>
<p><span class="koboSpan" id="kobo.43.1">In machine learning, one</span><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.44.1"> of the most important tasks is to label or classify data based on some criteria or patterns. </span><span class="koboSpan" id="kobo.44.2">However, labeling data manually can be time consuming and costly, especially when dealing with a large amount of data. </span><span class="koboSpan" id="kobo.44.3">By leveraging predefined patterns, this labeling approach enables the automatic assignment of meaningful categories or classes to </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">data instances.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.46.1">Pattern matching</span></strong><span class="koboSpan" id="kobo.47.1"> involves the identification of specific patterns or sequences within data that can be used as indicators for assigning labels. </span><span class="koboSpan" id="kobo.47.2">These patterns can be defined using regular expressions, rule-based systems, or other pattern recognition algorithms. </span><span class="koboSpan" id="kobo.47.3">The objective is to capture relevant information and characteristics from the data that can be matched against predefined patterns to infer </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">labels accurately.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">Pattern matching can be applied to various domains and scenarios in machine learning. </span><span class="koboSpan" id="kobo.49.2">Some common applications</span><a id="_idIndexMarker321"/><span class="koboSpan" id="kobo.50.1"> include </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.52.1">Text classification</span></strong><span class="koboSpan" id="kobo.53.1">: In</span><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.54.1"> natural language processing, pattern matching can be utilized to label text data based on specific keywords, phrases, or syntactic patterns. </span><span class="koboSpan" id="kobo.54.2">This enables tasks such as sentiment analysis, spam detection, and </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">topic categorization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.56.1">Image recognition</span></strong><span class="koboSpan" id="kobo.57.1">: Pattern</span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.58.1"> matching can aid in labeling images by identifying distinctive visual patterns or features that correspond to specific classes. </span><span class="koboSpan" id="kobo.58.2">This technique can be valuable in tasks such as object recognition, facial detection, and </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">image segmentation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Time series analysis</span></strong><span class="koboSpan" id="kobo.61.1">: When</span><a id="_idIndexMarker324"/><span class="koboSpan" id="kobo.62.1"> dealing with time-dependent data, pattern matching can be employed to label sequences of events or patterns that occur over time. </span><span class="koboSpan" id="kobo.62.2">This is particularly useful in financial analysis, anomaly detection, and predicting stock </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">market trends.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.64.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.65.1">: Pattern matching can play a crucial role in identifying fraudulent activit</span><a id="_idIndexMarker325"/><span class="koboSpan" id="kobo.66.1">ies by matching suspicious patterns or anomalies against known fraud patterns. </span><span class="koboSpan" id="kobo.66.2">This technique can help in credit card fraud detection, network intrusion detection, </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">and cybersecurity.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.68.1">Pattern matching offers several </span><a id="_idIndexMarker326"/><span class="koboSpan" id="kobo.69.1">advantages as a labeling technique in </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">machine learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Automation and efficiency</span></strong><span class="koboSpan" id="kobo.72.1">: By automating the labeling process, pattern matching reduces the reliance on manual labeling, saving time and effort. </span><span class="koboSpan" id="kobo.72.2">It allows for large-scale dataset labeling with </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">increased efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">Flexibility and adaptability</span></strong><span class="koboSpan" id="kobo.75.1">: Patterns can be easily modified or extended to accommodate new data or evolving requirements. </span><span class="koboSpan" id="kobo.75.2">This provides flexibility in adapting to changing labeling criteria and </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">ensures scalability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.77.1">Interpretability</span></strong><span class="koboSpan" id="kobo.78.1">: Pattern matching provides a transparent and interpretable approach to labeling, as the rules and patterns can be examined and understood. </span><span class="koboSpan" id="kobo.78.2">This aids in the transparency and explainability of the </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">labeling process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Complementing other techniques</span></strong><span class="koboSpan" id="kobo.81.1">: Pattern matching can be used in conjunction with other labeling techniques, such as weak supervision or transfer learning, to enhance the overall labeling accuracy and robustness of machine </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">learning models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.83.1">While pattern matching is a valuable labeling technique, it also presents certain challenges</span><a id="_idIndexMarker327"/> <span class="No-Break"><span class="koboSpan" id="kobo.84.1">and </span></span><span class="No-Break"><a id="_idIndexMarker328"/></span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">considerations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Noise and ambiguity</span></strong><span class="koboSpan" id="kobo.87.1">: Data instances that do not perfectly match predefined patterns may introduce noise or ambiguity in the labeling process. </span><span class="koboSpan" id="kobo.87.2">Handling such cases requires careful design and consideration of </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">pattern definitions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.89.1">Scalability</span></strong><span class="koboSpan" id="kobo.90.1">: As datasets grow larger, the scalability of pattern matching becomes crucial. </span><span class="koboSpan" id="kobo.90.2">Efficient algorithms and techniques must be employed to handle the increasing </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">computational demands.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.92.1">Overfitting</span></strong><span class="koboSpan" id="kobo.93.1">: Overfitting can occur if patterns are too specific and fail to generalize well to unseen data instances. </span><span class="koboSpan" id="kobo.93.2">Regularization techniques and cross-validation can be used to mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">this risk.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.95.1">In this </span><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.96.1">section of the chapter, we will explore how to create pattern-matching labeling functions using Python and apply them to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.97.1">credit-g</span></strong><span class="koboSpan" id="kobo.98.1"> dataset. </span><span class="koboSpan" id="kobo.98.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">credit-g</span></strong><span class="koboSpan" id="kobo.100.1"> dataset, also known as the German Credit dataset, is a collection of data points used for risk analysis in the field of finance and machine learning. </span><span class="koboSpan" id="kobo.100.2">It’s used to classify people as good or bad credit risks based on a set </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">of attributes.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">The dataset consists of 20 variables, including both numerical and categorical data. </span><span class="koboSpan" id="kobo.102.2">These variables provide information about each individual, such as their checking account status, credit history, purpose of the loan, credit amount, savings account/bonds, employment, installment rate in percentage of disposable income, personal status and gender, and </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">other attributes.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">Each entry in the dataset represents an individual who has applied for a loan. </span><span class="koboSpan" id="kobo.104.2">The target variable indicates whether the individual is classified as a ‘good’ or ‘bad’ credit risk. </span><span class="koboSpan" id="kobo.104.3">This makes the dataset particularly useful for supervised machine learning tasks, especially binary </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">classification problems.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">credit-g</span></strong><span class="koboSpan" id="kobo.108.1"> dataset is widely used in academia and industry for developing and testing machine learning models for credit risk assessment. </span><span class="koboSpan" id="kobo.108.2">It is available on several platforms, such as DataHub, Kaggle, OpenML, and UCI Machine </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">Learning Repository.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.110.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.111.1">Please note that the specifics of the variables might differ slightly depending on the source of </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">the dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.113.1">We can start by loading the </span><strong class="source-inline"><span class="koboSpan" id="kobo.114.1">credit-g</span></strong><span class="koboSpan" id="kobo.115.1"> dataset into Python. </span><span class="koboSpan" id="kobo.115.2">The dataset contains information about loan applicants, including their demographic information, financial information, and loan approval status. </span><span class="koboSpan" id="kobo.115.3">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.116.1">pandas</span></strong><span class="koboSpan" id="kobo.117.1"> library to load the dataset and explore </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">its structure:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.119.1">
from sklearn.datasets import fetch_openml
import pandas as pd
# Fetch the credit-g dataset
credit_g = fetch_openml(name='credit-g')
# Convert to DataFrame
df = pd.DataFrame(credit_g.data, columns=credit_g.feature_names)
target = pd.Series(credit_g.target)
# If you want to add the target variable into your DataFrame
df['target'] = target
# Show top rows of the credit-g dataset
df.head().T</span></pre> <p><span class="koboSpan" id="kobo.120.1">Here</span><a id="_idIndexMarker330"/><span class="koboSpan" id="kobo.121.1"> are the first five rows of </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<span class="koboSpan" id="kobo.123.1"><img alt="Figure 6.1 – The features (first column) and first five rows of the credit-g dataset" src="image/B19297_06_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.124.1">Figure 6.1 – The features (first column) and first five rows of the credit-g dataset</span></p>
<p><span class="koboSpan" id="kobo.125.1">Now that </span><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.126.1">we have loaded the dataset, we can create pattern-matching labeling functions. </span><span class="koboSpan" id="kobo.126.2">In this example, we will create two labeling functions that assign labels to loan applicants based on their income and credit history. </span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">income_labeling_function</span></strong><span class="koboSpan" id="kobo.128.1"> assigns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">1</span></strong><span class="koboSpan" id="kobo.130.1"> to loan applicants with an income greater than 5,000 and a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">0</span></strong><span class="koboSpan" id="kobo.132.1"> to all others. </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">credit_history_labeling_function</span></strong><span class="koboSpan" id="kobo.134.1"> assigns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">1</span></strong><span class="koboSpan" id="kobo.136.1"> to loan applicants with a credit history of 1, and a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">0</span></strong><span class="koboSpan" id="kobo.138.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">all others.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">Given the features in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.141.1">credit-g</span></strong><span class="koboSpan" id="kobo.142.1"> dataset, we can create two labeling functions based on </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">credit_amount</span></strong><span class="koboSpan" id="kobo.144.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">age</span></strong><span class="koboSpan" id="kobo.146.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.147.1">credit_amount_labeling_function</span></strong><span class="koboSpan" id="kobo.148.1"> assigns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.149.1">1</span></strong><span class="koboSpan" id="kobo.150.1"> to loan applicants with a credit amount greater than 5,000 and a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.151.1">0</span></strong><span class="koboSpan" id="kobo.152.1"> to all others. </span><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">age_labeling_function</span></strong><span class="koboSpan" id="kobo.154.1"> assigns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">1</span></strong><span class="koboSpan" id="kobo.156.1"> to loan applicants older than 30 and a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.157.1">0</span></strong><span class="koboSpan" id="kobo.158.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">all others:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.160.1">
def credit_amount_labeling_function(df):
    if df["credit_amount"] &gt; 5000:
        return 1
    else:
        return 0
def age_labeling_function(df):
    if df["age"] &gt; 30:
        return 1
    else:
        return 0</span></pre> <p><span class="koboSpan" id="kobo.161.1">After </span><a id="_idIndexMarker332"/><span class="koboSpan" id="kobo.162.1">creating the labeling functions, we can apply them to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">credit-g</span></strong><span class="koboSpan" id="kobo.164.1"> dataset. </span><span class="koboSpan" id="kobo.164.2">We can use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1">apply</span></strong><span class="koboSpan" id="kobo.166.1"> function in pandas to apply the labeling functions to each row of the dataset. </span><span class="koboSpan" id="kobo.166.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">apply</span></strong><span class="koboSpan" id="kobo.168.1"> function applies the labeling functions to each row of the dataset and assigns the labels to new columns in </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.170.1">
df["credit_amount_label"] = df.apply(credit_amount_labeling_function, axis=1)
df["age_label"] = df.apply(age_labeling_function, axis=1)
df.head().T</span></pre> <p><span class="koboSpan" id="kobo.171.1">Here is the output DataFrame using these functions. </span><span class="koboSpan" id="kobo.171.2">The DataFrame now has two additional columns with newly </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">created labels:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.173.1"><img alt="Figure 6.2 – The updated credit-g dataset with two new features, credit_amount_label and age_label" src="image/B19297_06_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.174.1">Figure 6.2 – The updated credit-g dataset with two new features, credit_amount_label and age_label</span></p>
<p><span class="koboSpan" id="kobo.175.1">Having </span><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.176.1">explored pattern-matching functions, we now shift our focus to the simplicity and effectiveness of database lookup techniques. </span><span class="koboSpan" id="kobo.176.2">In this next section, we’ll harness structured databases to enhance labeling accuracy, making our approach even </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">more robust.</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.178.1">Database lookup</span></h1>
<p><span class="koboSpan" id="kobo.179.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.180.1">database lookup</span></strong><span class="koboSpan" id="kobo.181.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.182.1">DB lookup</span></strong><span class="koboSpan" id="kobo.183.1">) labeling technique provides a powerful means of assigning labels to</span><a id="_idIndexMarker334"/><span class="koboSpan" id="kobo.184.1"> data instances by leveraging information stored in databases. </span><span class="koboSpan" id="kobo.184.2">By querying relevant databases and retrieving labeled information, this approach enables automated and accurate labeling. </span><span class="koboSpan" id="kobo.184.3">This technique involves searching and retrieving labels from databases based on specific attributes or key-value pairs associated with data instances. </span><span class="koboSpan" id="kobo.184.4">It relies on the premise that databases contain valuable labeled information that can be utilized for data labeling purposes. </span><span class="koboSpan" id="kobo.184.5">By performing queries against databases, relevant labels are fetched and assigned to the corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">data instances.</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">The DB lookup technique</span><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.187.1"> finds application in various domains and scenarios within machine learning. </span><span class="koboSpan" id="kobo.187.2">Some common applications include </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.189.1">Entity recognition</span></strong><span class="koboSpan" id="kobo.190.1">: In natural language processing tasks, such as named entity recognition or entity classification, DB lookup can be used to retrieve labels for entities based on their attributes stored in databases. </span><span class="koboSpan" id="kobo.190.2">This aids in the accurate identification and categorization of entities in </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">text data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.192.1">Product categorization</span></strong><span class="koboSpan" id="kobo.193.1">: E-commerce platforms often maintain databases containing product information, including categories and attributes. </span><span class="koboSpan" id="kobo.193.2">DB lookup can be employed to fetch product labels based on their features, allowing for automated categorization and organization </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">of products.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Geospatial analysis</span></strong><span class="koboSpan" id="kobo.196.1">: Databases containing geographical information, such as maps or geotagged data, can be queried using DB lookup to assign labels based on spatial attributes. </span><span class="koboSpan" id="kobo.196.2">This technique facilitates tasks such as location-based recommendations, geospatial clustering, and </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">boundary identification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.198.1">Medical diagnosis</span></strong><span class="koboSpan" id="kobo.199.1">: Medical databases store extensive information about diseases, symptoms, and patient records. </span><span class="koboSpan" id="kobo.199.2">DB lookup can be utilized to retrieve relevant labels for patient symptoms, aiding in automated medical diagnosis and decision </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">support systems.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">Now, let’s talk about Boolean flag labeling. </span><span class="koboSpan" id="kobo.201.2">It’s a simple yet powerful method that helps us improve and automate labeling by using clear and </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">logical conditions.</span></span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.203.1">Boolean flags</span></h1>
<p><span class="koboSpan" id="kobo.204.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.205.1">Boolean flags</span></strong><span class="koboSpan" id="kobo.206.1"> labeling</span><a id="_idIndexMarker336"/><span class="koboSpan" id="kobo.207.1"> technique involves the use of binary indicators to assign labels to data instances. </span><span class="koboSpan" id="kobo.207.2">These indicators, often represented as Boolean variables (</span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">true</span></strong><span class="koboSpan" id="kobo.209.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">false</span></strong><span class="koboSpan" id="kobo.211.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">1</span></strong><span class="koboSpan" id="kobo.213.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">0</span></strong><span class="koboSpan" id="kobo.215.1">), are associated with specific characteristics or properties that help identify the desired label. </span><span class="koboSpan" id="kobo.215.2">By examining the presence or absence of these flags, data instances can be </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">automatically labeled.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">The Boolean flags labeling technique finds applications across various domains in machine learning. </span><span class="koboSpan" id="kobo.217.2">Some </span><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.218.1">common applications include </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.220.1">Data filtering</span></strong><span class="koboSpan" id="kobo.221.1">: Boolean flags can be used to filter and label data instances based on specific criteria. </span><span class="koboSpan" id="kobo.221.2">For example, in sentiment analysis, a positive sentiment flag can be assigned to text instances that contain positive language or keywords, while a negative sentiment flag can be assigned to instances with </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">negative language.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Event detection</span></strong><span class="koboSpan" id="kobo.224.1">: Boolean flags can aid in labeling instances to detect specific events or conditions. </span><span class="koboSpan" id="kobo.224.2">For instance, in cybersecurity, a flag can be set to indicate instances with suspicious network activity, enabling the identification of potential </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">security threats.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.226.1">Anomaly detection</span></strong><span class="koboSpan" id="kobo.227.1">: Boolean flags can be used to label instances as normal or anomalous. </span><span class="koboSpan" id="kobo.227.2">By defining flags that capture typical patterns or behaviors, instances that deviate from these patterns can be flagged as anomalies, facilitating anomaly </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">detection tasks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.229.1">Quality control</span></strong><span class="koboSpan" id="kobo.230.1">: Boolean flags can assist in labeling instances for quality control purposes. </span><span class="koboSpan" id="kobo.230.2">For example, in manufacturing, flags can be set to label instances as defective or non-defective based on predefined </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">quality criteria.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.232.1">The Boolean flags labeling technique</span><a id="_idIndexMarker338"/><span class="koboSpan" id="kobo.233.1"> offers several advantages in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">learning applications:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.235.1">Simplicity and efficiency</span></strong><span class="koboSpan" id="kobo.236.1">: Boolean flags provide a straightforward and efficient labeling mechanism. </span><span class="koboSpan" id="kobo.236.2">The labeling process involves checking the presence or absence of flags, which can be implemented using simple conditional statements or </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">logical operations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.238.1">Flexibility and customization</span></strong><span class="koboSpan" id="kobo.239.1">: Boolean flags allow for customization and adaptability to different labeling scenarios. </span><span class="koboSpan" id="kobo.239.2">Flags can be defined based on specific criteria or requirements, providing flexibility in assigning labels according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">desired characteristics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">Interpretability</span></strong><span class="koboSpan" id="kobo.242.1">: The Boolean flags labeling technique offers interpretability, as the presence or absence of flags directly corresponds to the assigned labels. </span><span class="koboSpan" id="kobo.242.2">This transparency allows for better understanding and validation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">labeling process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.244.1">Scalability</span></strong><span class="koboSpan" id="kobo.245.1">: Boolean flags </span><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.246.1">can be easily scaled to handle large datasets. </span><span class="koboSpan" id="kobo.246.2">Since the labeling decision is based on binary indicators, the computational overhead remains low, making it suitable for processing massive amounts </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">of data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.248.1">While the Boolean flags labeling technique provides simplicity and efficiency, certain challenges and </span><a id="_idIndexMarker340"/><span class="koboSpan" id="kobo.249.1">considerations </span><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.250.1">should be taken </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">into account:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Feature engineering</span></strong><span class="koboSpan" id="kobo.253.1">: Designing effective Boolean flags requires careful feature engineering. </span><span class="koboSpan" id="kobo.253.2">The flags should be informative and relevant to the desired labels, necessitating a deep understanding of the problem domain and </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">data characteristics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.255.1">Data imbalance</span></strong><span class="koboSpan" id="kobo.256.1">: In scenarios where the data is imbalanced, meaning one label dominates over others, the Boolean flags technique may face challenges. </span><span class="koboSpan" id="kobo.256.2">Proper handling techniques, such as oversampling or under-sampling, may be required to address the </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">imbalance issue.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Generalization</span></strong><span class="koboSpan" id="kobo.259.1">: Boolean flags may not capture the full complexity of the underlying data distribution, potentially leading to overfitting or limited generalization. </span><span class="koboSpan" id="kobo.259.2">It is important to consider complementary techniques, such as feature extraction or more advanced machine learning algorithms, to enhance the performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">generalization capabilities.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.261.1">Flag interpretation</span></strong><span class="koboSpan" id="kobo.262.1">: While Boolean flags provide interpretability, it is crucial to carefully interpret the flags’ meanings in relation to the assigned labels. </span><span class="koboSpan" id="kobo.262.2">In some cases, the flags may capture correlations rather than causal relationships, requiring further investigation for a more </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">accurate interpretation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.264.1">You may have already noticed some similarities between Boolean flags and one-hot encoding (covered in </span><a href="B19297_05.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.265.1">Chapter 5</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.266.1">, Techniques for Data Cleaning</span></em><span class="koboSpan" id="kobo.267.1">). </span><span class="koboSpan" id="kobo.267.2">Therefore, it’s important to understand when these techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">are appropriate.</span></span></p>
<p><span class="koboSpan" id="kobo.269.1">When choosing </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.270.1">between Boolean flags and one-hot encoding, the specific use case is a crucial factor. </span><span class="koboSpan" id="kobo.270.2">If you’re working with a categorical variable that can be naturally divided into two categories or states (such as yes/no, true/false), using a Boolean flag might be the best option. </span><span class="koboSpan" id="kobo.270.3">It’s simpler, more memory-efficient, and can make the model easier </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">to interpret.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">For example, if you’re predicting whether an email is spam or not, a Boolean flag such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">contains_link</span></strong><span class="koboSpan" id="kobo.274.1"> (</span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">1</span></strong><span class="koboSpan" id="kobo.276.1"> if the email contains a link, </span><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">0</span></strong><span class="koboSpan" id="kobo.278.1"> otherwise) could be a very effective feature. </span><span class="koboSpan" id="kobo.278.2">This simplicity can lead to more interpretable models, as each feature directly corresponds to a condition </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">or state.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">On the other hand, one-hot encoding is more suitable for categorical variables with multiple categories where no natural binary division exists. </span><span class="koboSpan" id="kobo.280.2">For instance, if you’re working with a feature such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">color</span></strong><span class="koboSpan" id="kobo.282.1"> with values such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">red</span></strong><span class="koboSpan" id="kobo.284.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">blue</span></strong><span class="koboSpan" id="kobo.286.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">green</span></strong><span class="koboSpan" id="kobo.288.1">, etc., one-hot encoding would be a better choice. </span><span class="koboSpan" id="kobo.288.2">That’s because the numbers assigned to each category shouldn’t imply a mathematical relationship between the categories unless one exists. </span><span class="koboSpan" id="kobo.288.3">For example, encoding red as </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">1</span></strong><span class="koboSpan" id="kobo.290.1"> and blue as </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">2</span></strong><span class="koboSpan" id="kobo.292.1"> doesn’t mean blue is </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">twice red.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">To avoid implying such unintended relationships, creating a separate feature for each possible color is preferred. </span><span class="koboSpan" id="kobo.294.2">This approach captures more information about the color feature and doesn’t impose an arbitrary order or importance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">different colors.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">Furthermore, the type of machine learning model being used also influences the choice. </span><span class="koboSpan" id="kobo.296.2">Some models, such as decision trees and random forests, can handle categorical variables quite well, so one-hot encoding (which increases the dimensionality of the dataset) might not be necessary. </span><span class="koboSpan" id="kobo.296.3">However, others, such as linear regression, logistic regression, and support vector machines, require numerical input, necessitating some form of encoding for </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">categorical variables.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">Lastly, it’s worth noting that these aren’t the only methods for handling categorical data. </span><span class="koboSpan" id="kobo.298.2">There are other techniques, such as ordinal encoding, target encoding, and bin counting, each with its own strengths and weaknesses. </span><span class="koboSpan" id="kobo.298.3">The key is to understand the nature of your data and the requirements of your specific use case to choose the most </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">appropriate method.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">Let’s explore how to utilize Boolean flags in Python with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">credit-g</span></strong><span class="koboSpan" id="kobo.302.1"> dataset. </span><span class="koboSpan" id="kobo.302.2">Imagine we want to create a function that applies Boolean flags to label data points according to basic rules or heuristics. </span><span class="koboSpan" id="kobo.302.3">For instance, we can write a function that evaluates whether a credit applicant’s credit amount is above a specific threshold, subsequently assigning a Boolean flag to the data point based on </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">this assessment.</span></span></p>
<p><span class="koboSpan" id="kobo.304.1">The </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.305.1">following functions will check if the credit amount is below or above the median </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">credit amount:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.307.1">
def lf_credit_amount_above_median(df):
    credit_amount_median = df['credit_amount'].median()
    return df['credit_amount'] &gt;= credit_amount_median</span></pre> <p><span class="koboSpan" id="kobo.308.1">Now that we have defined our function, we can apply it to our </span><strong class="source-inline"><span class="koboSpan" id="kobo.309.1">df</span></strong><span class="koboSpan" id="kobo.310.1"> DataFrame to label the data points with </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">Boolean flags:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.312.1">
df['LF_CreditAmountAboveMedian'] = lf_credit_amount_above_median(df)
df.head().T</span></pre> <p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.313.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.314.1">.3</span></em><span class="koboSpan" id="kobo.315.1"> is the output DataFrame after we have applied these functions. </span><span class="koboSpan" id="kobo.315.2">Notice that we have now created a new column giving us additional information on the applicant’s credit amount, which can be used as a feature in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">learning models.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.317.1"><img alt="Figure 6.3 – The credit-g dataset with the new Boolean flag LF_CreditAmountAboveMedian added" src="image/B19297_06_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.318.1">Figure 6.3 – The credit-g dataset with the new Boolean flag LF_CreditAmountAboveMedian added</span></p>
<p><span class="koboSpan" id="kobo.319.1">In the next </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.320.1">section, let’s explore weak supervision—a sophisticated labeling technique that adeptly integrates information from various sources, navigating the intricacies of real-world data to enhance precision </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">and adaptability.</span></span></p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.322.1">Weak supervision</span></h1>
<p><span class="koboSpan" id="kobo.323.1">Weak supervision</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.324.1"> is a labeling technique in machine learning that leverages imperfect or noisy sources of supervision to assign labels to data instances. </span><span class="koboSpan" id="kobo.324.2">Unlike traditional labeling methods that rely on manually annotated data, weak supervision allows for a more scalable and automated approach to labeling. </span><span class="koboSpan" id="kobo.324.3">It refers to the use of heuristics, rules, or probabilistic methods to generate approximate labels for </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">data instances.</span></span></p>
<p><span class="koboSpan" id="kobo.326.1">Rather than </span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.327.1">relying on a single authoritative source of supervision, weak supervision harnesses multiple sources that may introduce noise or inconsistency. </span><span class="koboSpan" id="kobo.327.2">The objective is to generate labels that are “weakly” indicative of the true underlying labels, enabling model training in scenarios where obtaining fully labeled data is challenging </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">or expensive.</span></span></p>
<p><span class="koboSpan" id="kobo.329.1">For instance, consider a task where we want to build a machine learning model to identify whether an email is spam or not. </span><span class="koboSpan" id="kobo.329.2">Ideally, we would have a large dataset of emails that are accurately labeled as “spam” or “not spam.” </span><span class="koboSpan" id="kobo.329.3">However, obtaining such a dataset could be challenging, time-consuming, </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">and expensive.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">With weak supervision, we can use alternative, less perfect ways to label our data. </span><span class="koboSpan" id="kobo.331.2">For instance, we could create some rules or heuristics based on common patterns in spam emails. </span><span class="koboSpan" id="kobo.331.3">Here are a few examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">such rules:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.333.1">If the email contains words such as “lottery”, “win”, or “prize”, it might </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">be spam</span></span></li>
<li><span class="koboSpan" id="kobo.335.1">If the email is from an unknown sender and contains many links, it might </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">be spam</span></span></li>
<li><span class="koboSpan" id="kobo.337.1">If the email contains phrases such as “urgent action required”, it might </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">be spam</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.339.1">Using these rules, we can automatically label our email dataset. </span><span class="koboSpan" id="kobo.339.2">These labels won’t be perfect— there will be false positives (non-spam emails incorrectly labeled as spam) and false negatives (spam emails incorrectly labeled as non-spam). </span><span class="koboSpan" id="kobo.339.3">But they give us a starting point for training our machine </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">learning model.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">The model can then learn from these “weak” labels and, with a sufficiently large and diverse dataset, should still be able to generalize well to new, unseen emails. </span><span class="koboSpan" id="kobo.341.2">This makes weak supervision a scalable and efficient approach to labeling, particularly useful when perfect labels are hard to </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">come by.</span></span></p>
<p><span class="koboSpan" id="kobo.343.1">Weak supervision can be derived from various sources, including </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.345.1">Rule-based systems</span></strong><span class="koboSpan" id="kobo.346.1">: Domain experts or heuristics-based approaches can define rules or guidelines for labeling data based on specific patterns, features, or conditions. </span><span class="koboSpan" id="kobo.346.2">These rules may be derived from knowledge bases, existing models, or </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">expert opinions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.348.1">Crowdsourcing</span></strong><span class="koboSpan" id="kobo.349.1">: Leveraging the power of human annotators through crowdsourcing platforms, weak supervision can be obtained by aggregating the annotations from multiple individuals. </span><span class="koboSpan" id="kobo.349.2">This approach introduces noise but can be cost-effective </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">and scalable.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.351.1">Distant supervision</span></strong><span class="koboSpan" id="kobo.352.1">: Distant supervision involves using existing labeled data that may not perfectly align with the target task but can serve as a proxy. </span><span class="koboSpan" id="kobo.352.2">An example is using existing data with auxiliary labels to train a model for a related but </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">different task.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.354.1">Data augmentation</span></strong><span class="koboSpan" id="kobo.355.1">: Weak supervision can be obtained through data augmentation techniques such as data synthesis, transformation, or perturbation. </span><span class="koboSpan" id="kobo.355.2">By generating new labeled instances based on existing labeled data, weak supervision can </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">be expanded.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.357.1">Weak supervision offers </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.358.1">several advantages in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">learning applications:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.360.1">Scalability</span></strong><span class="koboSpan" id="kobo.361.1">: Weak supervision allows for large-scale data labeling by leveraging automated or semi-automated techniques. </span><span class="koboSpan" id="kobo.361.2">It reduces the manual effort required for manual annotation, enabling the utilization of </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">larger datasets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.363.1">Cost-effectiveness</span></strong><span class="koboSpan" id="kobo.364.1">: By leveraging weakly supervised sources, the cost of obtaining labeled data can be significantly reduced compared to fully supervised approaches. </span><span class="koboSpan" id="kobo.364.2">This is particularly beneficial in scenarios where manual labeling is expensive </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">or impractical.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.366.1">Flexibility and adaptability</span></strong><span class="koboSpan" id="kobo.367.1">: Weak supervision techniques can be easily adapted and modified to incorporate new sources of supervision or update existing rules. </span><span class="koboSpan" id="kobo.367.2">This flexibility allows for iterative improvement and refinement of the </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">labeling process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.369.1">Handling noisy labels</span></strong><span class="koboSpan" id="kobo.370.1">: Weak supervision techniques can handle noisy or inconsistent labels by aggregating multiple weak signals. </span><span class="koboSpan" id="kobo.370.2">This robustness to noise reduces the impact of individual labeling errors on the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">training process.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.372.1">There are, however, certain</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.373.1"> challenges and considerations</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.374.1"> to be </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">aware of:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.376.1">Noise and label quality</span></strong><span class="koboSpan" id="kobo.377.1">: Weakly supervised labels may contain noise or errors due to the imperfect nature of the supervision sources. </span><span class="koboSpan" id="kobo.377.2">Careful evaluation and validation are necessary to ensure label quality and minimize the propagation of noisy labels during </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">model training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.379.1">Trade-off between precision and recall</span></strong><span class="koboSpan" id="kobo.380.1">: Weak supervision techniques often prioritize scalability and coverage over precision. </span><span class="koboSpan" id="kobo.380.2">Balancing the trade-off between recall (coverage) and precision (accuracy) is essential in obtaining reliable weakly </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">labeled data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.382.1">Labeling confidence and model training</span></strong><span class="koboSpan" id="kobo.383.1">: Handling the uncertainty associated with weakly supervised labels is crucial. </span><span class="koboSpan" id="kobo.383.2">Techniques such as label calibration, data augmentation, or active learning can be employed to mitigate the impact of label uncertainty during </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">model training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.385.1">Generalization and model performance</span></strong><span class="koboSpan" id="kobo.386.1">: Weakly supervised models may struggle with generalizing to unseen or challenging instances due to the inherent noise in the labels. </span><span class="koboSpan" id="kobo.386.2">Strategies such as regularization, ensemble methods, or transfer learning can be employed to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">model performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.388.1">In this section, we will explore how to use labeling functions in Python to train a machine learning model on the </span><em class="italic"><span class="koboSpan" id="kobo.389.1">Loan Prediction</span></em><span class="koboSpan" id="kobo.390.1"> dataset we introduced in </span><a href="B19297_05.xhtml#_idTextAnchor070"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.391.1">Chapter 5</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.392.1">, Techniques for Data Cleaning</span></em><span class="koboSpan" id="kobo.393.1">. </span><span class="koboSpan" id="kobo.393.2">First, we need to prepare the data by importing the necessary libraries and loading the dataset, and we need to preprocess the data by handling missing values and encoding </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">categorical variables:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.395.1">
import pandas as pd
import numpy as np
df = pd.read_csv('train_loan_prediction.csv')
df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)
df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)
df['Self_Employed'].fillna('No',inplace=True)
df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)
df['Married'].fillna(df['Married'].mode()[0], inplace=True)
df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)
df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)
df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)</span></pre> <p><span class="koboSpan" id="kobo.396.1">We will use </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.397.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.398.1">LabelEncoder</span></strong><span class="koboSpan" id="kobo.399.1"> function from scikit-learn’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">preprocessing</span></strong><span class="koboSpan" id="kobo.401.1"> class to encode </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">categorical columns:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.403.1">
from sklearn.preprocessing import LabelEncoder
cat_features = ['Gender', 'Married','Dependents', 'Education', 'Self_Employed', 'Property_Area']
for feature in cat_features:
    encoder = LabelEncoder()
    df[feature] = encoder.fit_transform(df[feature])</span></pre> <p><span class="koboSpan" id="kobo.404.1">Now, we can define our labeling functions. </span><span class="koboSpan" id="kobo.404.2">In this example, we will define three labeling functions based on some simple heuristics. </span><span class="koboSpan" id="kobo.404.3">These labeling functions take in a row of the dataset as input and return a label. </span><span class="koboSpan" id="kobo.404.4">The label is </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">1</span></strong><span class="koboSpan" id="kobo.406.1"> if the row is likely to be in the positive class, </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">0</span></strong><span class="koboSpan" id="kobo.408.1"> if it is likely to be in the negative class, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">-1</span></strong><span class="koboSpan" id="kobo.410.1"> if it is uncertain. </span><span class="koboSpan" id="kobo.410.2">Functions such as these are commonly used in weak supervision approaches where you have a large amount of unlabeled data and you want to generate noisy labels </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">for them:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.412.1">
from snorkel.labeling import labeling_function
@labeling_function()
def lf1(df):
    if df['Education'] == 0:
        return 0
    elif df['Self_Employed'] == 0:
        return 1
    else:
        return -1
@labeling_function()
def lf2(df):
    if df['Credit_History'] == 1:
        if df['LoanAmount'] &lt;= 120:
            return 1
        else:
            return 0
    else:
        return -1
@labeling_function()
def lf3(df):
    if df['Married'] == 1:
        if df['Dependents'] == 0:
            return 1
        elif df['Dependents'] == 1:
            return 0
        else:
            return -1
    else:
        return -1</span></pre> <p><span class="koboSpan" id="kobo.413.1">We can apply the labeling functions to the dataset using the Snorkel library. </span><span class="koboSpan" id="kobo.413.2">Here, we create a list of the three labeling functions and use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.414.1">PandasLFApplier</span></strong><span class="koboSpan" id="kobo.415.1"> to apply them to the dataset. </span><span class="koboSpan" id="kobo.415.2">The output is a </span><strong class="source-inline"><span class="koboSpan" id="kobo.416.1">L_train</span></strong><span class="koboSpan" id="kobo.417.1"> matrix where each row corresponds to a data point and each column corresponds to a </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">labeling function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.419.1">
LFs = [lf1, lf2, lf3]
from snorkel.labeling import PandasLFApplier
applier = PandasLFApplier(lfs=LFs)
L_train = applier.apply(df)</span></pre> <p><span class="koboSpan" id="kobo.420.1">You’ll see the </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.422.1"><img alt="Figure 6.4 – Progress bar showing the training progress" src="image/B19297_06_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.423.1">Figure 6.4 – Progress bar showing the training progress</span></p>
<p><span class="koboSpan" id="kobo.424.1">To improve</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.425.1"> the output of the labeling functions, we need to combine them to obtain a more accurate label for each data point. </span><span class="koboSpan" id="kobo.425.2">We can do this using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">LabelModel</span></strong><span class="koboSpan" id="kobo.427.1"> class in the Snorkel library. </span><span class="koboSpan" id="kobo.427.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.428.1">LabelModel</span></strong><span class="koboSpan" id="kobo.429.1"> class is a probabilistic model used for combining the outputs of multiple labeling functions to generate more accurate and reliable labeling for each data point. </span><span class="koboSpan" id="kobo.429.2">It plays a crucial role in addressing the noise and inaccuracies that may arise from individual labeling functions. </span><span class="koboSpan" id="kobo.429.3">We create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.430.1">LabelModel</span></strong><span class="koboSpan" id="kobo.431.1"> object as </span><strong class="source-inline"><span class="koboSpan" id="kobo.432.1">label_model</span></strong><span class="koboSpan" id="kobo.433.1"> and fit it to the output of the labeling functions. </span><span class="koboSpan" id="kobo.433.2">The cardinality parameter specifies the number of classes, which is </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">2</span></strong><span class="koboSpan" id="kobo.435.1"> in this case. </span><span class="koboSpan" id="kobo.435.2">We also specify the number of epochs to train for and a random seed </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">for reproducibility:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.437.1">
from snorkel.labeling.model import LabelModel
from snorkel.labeling import PandasLFApplier, LFAnalysis
from sklearn.metrics import accuracy_score
label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)</span></pre> <p><span class="koboSpan" id="kobo.438.1">After executing</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.439.1"> the preceding code snippet utilizing Snorkel’s labeling model, a progress bar will display the incremental application of </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">the labeling:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.441.1"><img alt="Figure 6.5 – The Snorkel progress bar showing the incremental progress of the labeling process" src="image/B19297_06_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.442.1">Figure 6.5 – The Snorkel progress bar showing the incremental progress of the labeling process</span></p>
<p><span class="koboSpan" id="kobo.443.1">We can now use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.444.1">LabelModel</span></strong><span class="koboSpan" id="kobo.445.1"> class to generate labels for the training data and evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">its performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.447.1">
probs_train = label_model.predict_proba(L_train)
mapping = {'Y': 1, 'N': 0}
Y_train = df['Loan_Status'].map(mapping).values
score_train = label_model.score(L_train, Y_train)
df['label'] = probs_train.argmax(axis=1)
df['label'] = df['label'].map({1: 'Y', 0: 'N'})
print(f"Accuracy: {accuracy_score(df['Loan_Status'].values, df['label'].values)}")</span></pre> <p><span class="koboSpan" id="kobo.448.1">Now, let’s learn how to use semi-weak supervision for labeling. </span><span class="koboSpan" id="kobo.448.2">It’s a smart technique that combines weak supervision with a bit of manual labeling to make our labels </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">more accurate.</span></span></p>
<h1 id="_idParaDest-92"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.450.1">Semi-weak supervision</span></h1>
<p><span class="koboSpan" id="kobo.451.1">Semi-weak supervision is </span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.452.1">a technique used in machine learning to improve the accuracy of a model by combining a small set of labeled data with a larger set of weakly labeled data. </span><span class="koboSpan" id="kobo.452.2">In this approach, the labeled data is used to guide the learning process, while the weakly labeled data provides additional information to improve the accuracy of </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.454.1">Semi-weak supervision is particularly useful when labeled data is limited or expensive to obtain and can be applied to a wide range of machine learning tasks, such as text classification, image recognition, and </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">object detection.</span></span></p>
<p><span class="koboSpan" id="kobo.456.1">In the loan prediction dataset, we have a set of data points representing loan applications, each with a set of features such as income, credit history, and loan amount, and a label indicating whether the loan was approved or not. </span><span class="koboSpan" id="kobo.456.2">However, this labeled data may be incomplete or inaccurate, which can lead to poor </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.458.1">To address this issue, we can use semi-weak supervision to generate additional labels for the loan prediction dataset. </span><span class="koboSpan" id="kobo.458.2">One approach is to use weak supervision techniques to generate labels automatically based on heuristics or rules. </span><span class="koboSpan" id="kobo.458.3">For example, we can use regular expressions to identify patterns in the loan application text data that are indicative of a high-risk loan. </span><span class="koboSpan" id="kobo.458.4">We can also use external data sources, such as credit reports or social media data, to generate additional weakly </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">labeled data.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Once we have a set of weakly labeled data, we can use it to train a model along with the small set of labeled data. </span><span class="koboSpan" id="kobo.460.2">The labeled data is used to guide the learning process, while the weakly labeled data provides additional information to improve the accuracy of the model. </span><span class="koboSpan" id="kobo.460.3">By using semi-weak supervision, we can effectively use all available data to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.462.1">Here is an </span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.463.1">example of how to implement semi-weak supervision for the loan prediction dataset using Snorkel and Python. </span><span class="koboSpan" id="kobo.463.2">We first import the necessary libraries and functions from those libraries. </span><span class="koboSpan" id="kobo.463.3">We then load the dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">using pandas:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.465.1">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from snorkel.labeling import labeling_function
from snorkel.labeling import PandasLFApplier
from snorkel.labeling import LFAnalysis
from snorkel.labeling.model import LabelModel
df = pd.read_csv('train_loan_prediction.csv')</span></pre> <p><span class="koboSpan" id="kobo.466.1">Let’s define a</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.467.1"> function to preprocess the dataset. </span><span class="koboSpan" id="kobo.467.2">We will use similar preprocessing methods to the ones discussed earlier in </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">this chapter:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
def preprocess_data(df):
    df['Gender'] = df['Gender'].fillna('Unknown')
    df['Married'] = df['Married'].fillna('Unknown')
    df['Dependents'] = df['Dependents'].fillna('0')
    df['Self_Employed'] = df['Self_Employed'].fillna('Unknown')
    df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].mean())
    df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())
    df['Credit_History'] = df['Credit_History'].fillna(-1)
    df['LoanAmount_bin'] = pd.cut(df['LoanAmount'], bins=[0, 100, 200, 700], labels=['Low', 'Average', 'High'])
    df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']
    df['TotalIncome_bin'] = pd.cut(df['TotalIncome'], bins=[0, 2500, 4000, 6000, 81000], labels=['Low', 'Average', 'High', 'Very high'])
    df = df.drop(['Loan_ID', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'TotalIncome'], axis=1)
    return df</span></pre> <p><span class="koboSpan" id="kobo.470.1">Now we create three labeling functions for </span><strong class="source-inline"><span class="koboSpan" id="kobo.471.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.472.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.474.1">. </span><span class="koboSpan" id="kobo.474.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.475.1">lf1(x)</span></strong><span class="koboSpan" id="kobo.476.1"> function takes a data instance </span><strong class="source-inline"><span class="koboSpan" id="kobo.477.1">x</span></strong><span class="koboSpan" id="kobo.478.1"> as input and performs a labeling operation based on the value of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.480.1"> feature. </span><span class="koboSpan" id="kobo.480.2">If the </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.482.1"> is less than 5,000, the function returns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.483.1">0</span></strong><span class="koboSpan" id="kobo.484.1">. </span><span class="koboSpan" id="kobo.484.2">Otherwise, if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.486.1"> is greater than or equal to 5,000, the function returns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">1</span></strong><span class="koboSpan" id="kobo.488.1">. </span><span class="koboSpan" id="kobo.488.2">Essentially, this function assigns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">0</span></strong><span class="koboSpan" id="kobo.490.1"> to instances with low applicant income and a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">1</span></strong><span class="koboSpan" id="kobo.492.1"> to instances with higher </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">applicant income.</span></span></p>
<p><span class="koboSpan" id="kobo.494.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.495.1">lf2(x)</span></strong><span class="koboSpan" id="kobo.496.1"> function</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.497.1"> also takes a data instance </span><strong class="source-inline"><span class="koboSpan" id="kobo.498.1">x</span></strong><span class="koboSpan" id="kobo.499.1"> as input and assigns a label based on the value of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.501.1"> feature. </span><span class="koboSpan" id="kobo.501.2">If the </span><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.503.1"> is greater than 200, the function returns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">0</span></strong><span class="koboSpan" id="kobo.505.1">. </span><span class="koboSpan" id="kobo.505.2">Conversely, if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.507.1"> is less than or equal to 200, the function returns a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">1</span></strong><span class="koboSpan" id="kobo.509.1">. </span><span class="koboSpan" id="kobo.509.2">This function categorizes instances with large loan amounts as label </span><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">0</span></strong><span class="koboSpan" id="kobo.511.1"> and instances with smaller loan amounts as </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">label </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.513.1">1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.515.1">Utilizing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.516.1">lf3(x)</span></strong><span class="koboSpan" id="kobo.517.1"> function, we compute the ratio between the loan amount and the applicant’s income. </span><span class="koboSpan" id="kobo.517.2">This ratio serves as a crucial metric in determining the feasibility of the loan. </span><span class="koboSpan" id="kobo.517.3">Based on this calculated ratio, we categorize the data points into different labels. </span><span class="koboSpan" id="kobo.517.4">If the loan-to-income ratio falls below or equals 0.3, we assign a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">1</span></strong><span class="koboSpan" id="kobo.519.1">, indicating approval of the loan request. </span><span class="koboSpan" id="kobo.519.2">In cases where the ratio exceeds 0.3 but remains less than or equal to 0.5, we designate the data point with a label of </span><strong class="source-inline"><span class="koboSpan" id="kobo.520.1">0</span></strong><span class="koboSpan" id="kobo.521.1">, signifying uncertainty regarding loan approval. </span><span class="koboSpan" id="kobo.521.2">Conversely, if the ratio surpasses 0.5, we assign the label </span><strong class="source-inline"><span class="koboSpan" id="kobo.522.1">-1</span></strong><span class="koboSpan" id="kobo.523.1">, indicating denial of the loan application. </span><span class="koboSpan" id="kobo.523.2">This approach enables us to incorporate the affordability aspect into our labeling process, enhancing the granularity of our weak supervision approach for loan </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">approval prediction:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.525.1">
@labeling_function()
def lf1(x):
    return 0 if x['ApplicantIncome'] &lt; 5000 else 1
@labeling_function()
def lf2(x):
    return 0 if x['LoanAmount'] &gt; 200 else 1
@labeling_function()
def lf3(x):
    # Calculate the ratio of loan amount to applicant's income
    loan_to_income_ratio = x['LoanAmount'] / x['ApplicantIncome']
    # Return label based on the loan-to-income ratio
    if loan_to_income_ratio &lt;= 0.3:
        return 1  # Approve loan
    elif loan_to_income_ratio &gt; 0.3 and loan_to_income_ratio &lt;= 0.5:
        return 0  # Label as uncertain
    else:
        return -1  # Deny loan</span></pre> <p><span class="koboSpan" id="kobo.526.1">We then apply the preprocessing</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.527.1"> techniques to the input data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">df</span></strong><span class="koboSpan" id="kobo.529.1">). </span><span class="koboSpan" id="kobo.529.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">preprocess_data</span></strong><span class="koboSpan" id="kobo.531.1"> function is used to perform the necessary preprocessing steps. </span><span class="koboSpan" id="kobo.531.2">The resulting preprocessed data is stored in the variable </span><strong class="source-inline"><span class="koboSpan" id="kobo.532.1">X</span></strong><span class="koboSpan" id="kobo.533.1">. </span><span class="koboSpan" id="kobo.533.2">Additionally, the target variable, </span><strong class="source-inline"><span class="koboSpan" id="kobo.534.1">Loan_Status</span></strong><span class="koboSpan" id="kobo.535.1">, is transformed from categorical values (</span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">N</span></strong><span class="koboSpan" id="kobo.537.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">Y</span></strong><span class="koboSpan" id="kobo.539.1">) to numerical values (</span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">0</span></strong><span class="koboSpan" id="kobo.541.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.542.1">1</span></strong><span class="koboSpan" id="kobo.543.1">) and stored in the variable </span><strong class="source-inline"><span class="koboSpan" id="kobo.544.1">y</span></strong><span class="koboSpan" id="kobo.545.1">. </span><span class="koboSpan" id="kobo.545.2">This step ensures that the data is ready for training </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">and evaluation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.547.1">
X = preprocess_data(df.drop('Loan_Status', axis=1))
y = df['Loan_Status'].replace({'N': 0, 'Y': 1})</span></pre> <p><span class="koboSpan" id="kobo.548.1">The next step involves splitting the preprocessed data into training and testing sets. </span><span class="koboSpan" id="kobo.548.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">train_test_split</span></strong><span class="koboSpan" id="kobo.550.1"> function from the scikit-learn library is used for this purpose. </span><span class="koboSpan" id="kobo.550.2">The data is divided into </span><strong class="source-inline"><span class="koboSpan" id="kobo.551.1">X_train</span></strong><span class="koboSpan" id="kobo.552.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.553.1">X_test</span></strong><span class="koboSpan" id="kobo.554.1"> for the features and </span><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">y_train</span></strong><span class="koboSpan" id="kobo.556.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">y_test</span></strong><span class="koboSpan" id="kobo.558.1"> for the corresponding labels. </span><span class="koboSpan" id="kobo.558.2">This separation allows for training the model on the training set and evaluating its performance on the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">test set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.560.1">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.561.1">Now we apply the two labeling functions, </span><strong class="source-inline"><span class="koboSpan" id="kobo.562.1">lf1</span></strong><span class="koboSpan" id="kobo.563.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.564.1">lf2</span></strong><span class="koboSpan" id="kobo.565.1">, to the training set (</span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">X_train</span></strong><span class="koboSpan" id="kobo.567.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">PandasLFApplier</span></strong><span class="koboSpan" id="kobo.569.1"> class. </span><span class="koboSpan" id="kobo.569.2">The resulting weakly labeled data is stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">L_train_weak</span></strong><span class="koboSpan" id="kobo.571.1">. </span><span class="koboSpan" id="kobo.571.2">The LFs analyze the features of each instance and assign labels based on predefined rules </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">or conditions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.573.1">
lfs = [lf1, lf2, lf3]
applier = PandasLFApplier(lfs)
L_t</span><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.574.1">rain_weak = applier.apply(X_train)</span></pre> <p><span class="koboSpan" id="kobo.575.1">The </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.576.1">label model is instantiated using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.577.1">LabelModel</span></strong><span class="koboSpan" id="kobo.578.1"> class. </span><span class="koboSpan" id="kobo.578.2">It is configured with a cardinality of </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">2</span></strong><span class="koboSpan" id="kobo.580.1"> (indicating binary classification) and set to run in verbose mode for progress updates. </span><span class="koboSpan" id="kobo.580.2">The label model is then trained on the training data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">L_train_weak</span></strong><span class="koboSpan" id="kobo.582.1">) using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">fit</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.584.1"> method:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.585.1">
label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train_weak)</span></pre> <p><span class="koboSpan" id="kobo.586.1">Once the label model is trained, it is evaluated on the test set (</span><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">X_test</span></strong><span class="koboSpan" id="kobo.588.1">) to assess its performance. </span><span class="koboSpan" id="kobo.588.2">The applier object is used again to apply the labeling functions to the test set, resulting in </span><strong class="source-inline"><span class="koboSpan" id="kobo.589.1">L_test</span></strong><span class="koboSpan" id="kobo.590.1">, which contains the weakly labeled instances. </span><span class="koboSpan" id="kobo.590.2">The score method of the label model is then used to calculate the accuracy of the label predictions compared to the ground truth </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">labels (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">y_test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.594.1">
L_test = applier.apply(X_test)
accuracy = label_model.score(L_test, y_test)["accuracy"]
print(f'Test accuracy: {accuracy:.3f}')</span></pre> <p><span class="koboSpan" id="kobo.595.1">In the upcoming section, we explore slicing functions for labeling—an advanced technique that allows us to finely segment our data. </span><span class="koboSpan" id="kobo.595.2">These functions provide a tailored approach, enabling us to apply specific labeling strategies to distinct subsets of </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">our dataset.</span></span></p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.597.1">Slicing functions</span></h1>
<p><span class="koboSpan" id="kobo.598.1">Slicing functions</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.599.1"> are functions that operate on data instances and produce binary labels based on specific conditions. </span><span class="koboSpan" id="kobo.599.2">Unlike traditional labeling functions that provide labels for the entire dataset, slicing functions are designed to focus on specific subsets of the data. </span><span class="koboSpan" id="kobo.599.3">These subsets, or slices, can be defined based on various features, patterns, or characteristics of the data. </span><span class="koboSpan" id="kobo.599.4">Slicing functions offer a fine-grained approach to labeling, enabling more targeted and precise labeling of </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">data instances.</span></span></p>
<p><span class="koboSpan" id="kobo.601.1">Slicing functions play a crucial role in weak supervision approaches, where multiple labeling sources are leveraged to assign approximate labels. </span><span class="koboSpan" id="kobo.601.2">Slicing functions complement other labeling techniques, such as rule-based systems or crowdsourcing, by capturing specific patterns or subsets of the data that may be challenging to label accurately using other methods. </span><span class="koboSpan" id="kobo.601.3">By applying slicing functions to the data, practitioners can exploit domain knowledge or specific data characteristics to improve the </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">labeling process.</span></span></p>
<p><span class="koboSpan" id="kobo.603.1">To fully understand the concept of slicing functions, let’s use an example of a dataset containing reviews for a range of products from an e-commerce website. </span><span class="koboSpan" id="kobo.603.2">Our goal is to label these reviews as either positive </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">or negative.</span></span></p>
<p><span class="koboSpan" id="kobo.605.1">For simplicity, let’s consider two </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">slicing functions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.607.1">Slicing Function 1</span></strong><span class="koboSpan" id="kobo.608.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.609.1">SF1</span></strong><span class="koboSpan" id="kobo.610.1">): This </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.611.1">function targets reviews that contain the word “refund”. </span><span class="koboSpan" id="kobo.611.2">It labels a review as negative if it includes the word “refund” and leaves it unlabeled otherwise. </span><span class="koboSpan" id="kobo.611.3">The intuition behind this slicing function is that customers asking for a refund are likely dissatisfied with their purchase, hence the </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">negative sentiment.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.613.1">Slicing Function 2</span></strong><span class="koboSpan" id="kobo.614.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.615.1">SF2</span></strong><span class="koboSpan" id="kobo.616.1">): This function</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.617.1"> targets reviews from customers who purchased electronics. </span><span class="koboSpan" id="kobo.617.2">It labels a review as positive if it includes words such as “great”, “excellent”, or “love” and labels it as negative if it includes words such as “broken”, “defective”, or “useless”. </span><span class="koboSpan" id="kobo.617.3">It leaves the review unlabeled if it doesn’t meet any of </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">these conditions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.619.1">You will notice that these slicing functions operate on specific subsets of the data and enable us to incorporate domain knowledge into the labeling process. </span><span class="koboSpan" id="kobo.619.2">Therefore, designing effective slicing functions requires a combination of domain knowledge, feature engineering, and experimentation. </span><span class="koboSpan" id="kobo.619.3">Here are some key considerations for designing and</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.620.1"> implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">slicing functions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.622.1">Identify relevant slices</span></strong><span class="koboSpan" id="kobo.623.1">: Determine the specific subsets or slices of the data that are relevant to the labeling task. </span><span class="koboSpan" id="kobo.623.2">This involves understanding the problem domain, analyzing the data, and identifying distinct patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">or characteristics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Define slicing conditions</span></strong><span class="koboSpan" id="kobo.626.1">: Specify the conditions or rules that capture the desired subsets of the data. </span><span class="koboSpan" id="kobo.626.2">These conditions can be based on feature thresholds, pattern matching, statistical properties, or any other </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">relevant criteria.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.628.1">Evaluate and iterate</span></strong><span class="koboSpan" id="kobo.629.1">: Assess the performance of the slicing functions by comparing the assigned labels to ground truth labels or existing labeling sources. </span><span class="koboSpan" id="kobo.629.2">Iterate on the design of the slicing functions, refining the conditions and rules to improve the quality of the </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">assigned labels.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.631.1">Slicing functions offer </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.632.1">several benefits in the </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">labeling process:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.634.1">Fine-grained labeling</span></strong><span class="koboSpan" id="kobo.635.1">: Slicing functions allow for targeted labeling of specific subsets of the data, providing more detailed and granular labels that capture distinct patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">or characteristics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.637.1">Domain knowledge incorporation</span></strong><span class="koboSpan" id="kobo.638.1">: Slicing functions enable the incorporation of domain expertise and specific domain knowledge into the labeling process. </span><span class="koboSpan" id="kobo.638.2">This allows for more informed and context-aware </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">labeling decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.640.1">Complementarity with other techniques</span></strong><span class="koboSpan" id="kobo.641.1">: Slicing functions complement other labeling techniques by capturing slices of the data that may be challenging to label using traditional methods. </span><span class="koboSpan" id="kobo.641.2">They provide an additional source of weak supervision that enhances the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">labeling process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.643.1">Scalability and efficiency</span></strong><span class="koboSpan" id="kobo.644.1">: Slicing functions can be automated and applied programmatically, allowing for scalable and efficient labeling of large datasets. </span><span class="koboSpan" id="kobo.644.2">This reduces the dependency </span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.645.1">on manual annotation and enables the labeling of data at a </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">larger scale.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.647.1">Let’s understand how </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.648.1">we can implement slicing functions in Python using the loan prediction dataset. </span><span class="koboSpan" id="kobo.648.2">We first import the required libraries and load the dataset into a pandas DataFrame. </span><span class="koboSpan" id="kobo.648.3">We will use the same preprocessing step discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">previous sections:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.650.1">
from snorkel.labeling import labeling_function
from snorkel.labeling import PandasLFApplier
from snorkel.labeling.model import LabelModel
from snorkel.labeling import LFAnalysis
df = pd.read_csv('train_loan_prediction.csv')
X = preprocess_data(df.drop('Loan_Status', axis=1))
y = df['Loan_Status'].replace({'N': 0, 'Y': 1})
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.651.1">To create slicing functions, we utilize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">@labeling_function</span></strong><span class="koboSpan" id="kobo.653.1"> decorator provided by Snorkel. </span><span class="koboSpan" id="kobo.653.2">These functions encapsulate the labeling logic based on specific conditions or rules. </span><span class="koboSpan" id="kobo.653.3">For example, we can define slicing functions based on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.654.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.655.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">LoanAmount</span></strong><span class="koboSpan" id="kobo.657.1">, or </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.658.1">Self_Employed</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.659.1"> features:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.660.1">
@labeling_function()
def slice_high_income(x):
    return 1 if x['ApplicantIncome'] &gt; 8000 else 0
@labeling_function()
def slice_low_income_high_loan(x):
    return 1 if x['ApplicantIncome'] &lt; 4000 and x['LoanAmount'] &gt; 150 else 0
@labeling_function()
def slice_self_employed(x):
    return 1 if x['Self_Employed'] == 'Yes' else 0</span></pre> <p><span class="koboSpan" id="kobo.661.1">To apply the</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.662.1"> slicing functions to the training data, we use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">PandasLFApplier</span></strong><span class="koboSpan" id="kobo.664.1"> class provided by Snorkel. </span><span class="koboSpan" id="kobo.664.2">This class takes the slicing functions as input and applies them to the training dataset, generating weak labels. </span><span class="koboSpan" id="kobo.664.3">The resulting weak labels will be used to train the label </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">model later:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.666.1">
lfs = [slice_high_income, slice_low_income_high_loan, slice_self_employed]
applier = PandasLFApplier(lfs)
L_train = applier.apply(df=X_train)</span></pre> <p><span class="koboSpan" id="kobo.667.1">Once we have the weak labels from the slicing functions, we can train a label model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">LabelModel</span></strong><span class="koboSpan" id="kobo.669.1"> class from Snorkel. </span><span class="koboSpan" id="kobo.669.2">The label model learns the correlation between the weak labels and the true labels and estimates the posterior probabilities for each data instance. </span><span class="koboSpan" id="kobo.669.3">In this step, we create a </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">LabelModel</span></strong><span class="koboSpan" id="kobo.671.1"> object, specify the cardinality of the labels (e.g., binary classification), and fit it to the weakly labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.673.1">
label_model = LabelModel(cardinality=2, verbose=True)
label_model.fit(L_train, n_epochs=500, seed=42)</span></pre> <p><span class="koboSpan" id="kobo.674.1">After training the label model, we want to evaluate its performance on the test data. </span><span class="koboSpan" id="kobo.674.2">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.675.1">PandasLFApplier</span></strong><span class="koboSpan" id="kobo.676.1"> to apply the slicing functions to the test dataset, obtaining the weak labels. </span><span class="koboSpan" id="kobo.676.2">Then, we calculate the accuracy of the label model’s predictions compared to the true labels of the </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">test set:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.678.1">
L_test = applier.apply(df=X_test)
accuracy = label_model.score(L=L_test, Y=y_test)
print(f'Test accuracy: {accuracy["accuracy"]:.3f}')</span></pre> <p><span class="koboSpan" id="kobo.679.1">Snorkel provides the </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">LFAnalysis</span></strong><span class="koboSpan" id="kobo.681.1"> module, which allows us to analyze the performance and characteristics of the labeling functions. </span><span class="koboSpan" id="kobo.681.2">We can compute various metrics such as</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.682.1"> coverage, conflicts, and accuracy for each labeling function to gain insights into their effectiveness and </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">potential issues:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.684.1">
LFAnalysis(L=L_train, lfs=lfs).lf_summary()</span></pre> <p><span class="koboSpan" id="kobo.685.1">This will generate the following </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">summary table:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<span class="koboSpan" id="kobo.687.1"><img alt="Figure 6.6 – Summary table showing statistics for each labeling function (LF)" src="image/B19297_06_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.688.1">Figure 6.6 – Summary table showing statistics for each labeling function (LF)</span></p>
<p><span class="koboSpan" id="kobo.689.1">In the next section, we’ll explore active learning for labeling—a clever strategy that involves picking the right data to label, making our model smarter with </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">each iteration.</span></span></p>
<h1 id="_idParaDest-94"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.691.1">Active learning</span></h1>
<p><span class="koboSpan" id="kobo.692.1">In this section, we</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.693.1"> will explore the concept of active learning and its application in data labeling. </span><span class="koboSpan" id="kobo.693.2">Active learning is a powerful technique that allows us to label data more efficiently by actively selecting the most informative samples for annotation. </span><span class="koboSpan" id="kobo.693.3">By strategically choosing which samples to label, we can achieve higher accuracy with a smaller dataset, all else being equal. </span><span class="koboSpan" id="kobo.693.4">On the following pages, we will discuss various active learning strategies and implement them using Python </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">code examples.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">Active learning is a semi-supervised learning approach that involves iteratively selecting a subset of data points for manual annotation based on their informativeness. </span><span class="koboSpan" id="kobo.695.2">The key idea is to actively query the labels of the most uncertain or informative instances to improve the learning process. </span><span class="koboSpan" id="kobo.695.3">This iterative process of selecting and labeling samples can significantly reduce the amount of labeled data required to achieve the desired level </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">of performance.</span></span></p>
<p><span class="koboSpan" id="kobo.697.1">Let’s start with a simple</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.698.1"> example of active learning to help you get the basic idea before going into detail on specific active </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">learning strategies.</span></span></p>
<p><span class="koboSpan" id="kobo.700.1">Suppose we are building a machine learning model to classify emails into spam and not spam. </span><span class="koboSpan" id="kobo.700.2">We have a large dataset of unlabeled emails, but manually labeling all of them would be </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">very time-consuming.</span></span></p>
<p><span class="koboSpan" id="kobo.702.1">Here is where </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.703.1">active learning </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">comes in:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.705.1">Initial training</span></strong><span class="koboSpan" id="kobo.706.1">: We start by randomly selecting a small subset of emails and manually labeling them as spam or not spam. </span><span class="koboSpan" id="kobo.706.2">We then train our model on this small </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">labeled dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.708.1">Uncertainty sampling</span></strong><span class="koboSpan" id="kobo.709.1">: After training, we use the model to make predictions on the rest of the unlabeled emails. </span><span class="koboSpan" id="kobo.709.2">However, instead of labeling all the emails, we choose the ones where the model is most uncertain about its predictions. </span><span class="koboSpan" id="kobo.709.3">For example, if our model outputs a probability close to 0.5 (i.e., it’s unsure whether the email is spam or not), these emails are considered ‘informative’ </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">or ‘uncertain’.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.711.1">Label query</span></strong><span class="koboSpan" id="kobo.712.1">: We then manually label these uncertain emails, adding them to our </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">training set.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.714.1">Iterative learning</span></strong><span class="koboSpan" id="kobo.715.1">: </span><em class="italic"><span class="koboSpan" id="kobo.716.1">Step 2</span></em><span class="koboSpan" id="kobo.717.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.718.1">step 3</span></em><span class="koboSpan" id="kobo.719.1"> are repeated in several iterations— retraining the model with the newly labeled data, using it to predict labels for the remaining unlabeled data, and choosing the most uncertain instances to </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">label next.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.721.1">This way, active learning allows us to strategically select the most informative examples to label, thereby potentially improving the model’s performance with fewer </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">labeled instances.</span></span></p>
<p><span class="koboSpan" id="kobo.723.1">There are several active learning strategies that can be employed based on different criteria for selecting informative samples. </span><span class="koboSpan" id="kobo.723.2">Let’s discuss a few commonly used strategies and their </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">Python implementations.</span></span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.725.1">Uncertainty sampling</span></h2>
<p><span class="koboSpan" id="kobo.726.1">Uncertainty sampling</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.727.1"> is based on the assumption that instances on which a model is uncertain are more informative and beneficial to label. </span><span class="koboSpan" id="kobo.727.2">The idea is to select instances that are close to the decision boundary or have conflicting predictions. </span><span class="koboSpan" id="kobo.727.3">By actively acquiring labels for these challenging instances, the model can refine its understanding of the data and improve </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">its performance.</span></span></p>
<p><span class="koboSpan" id="kobo.729.1">There are several </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.730.1">common approaches to uncertainty sampling</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.731.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">active learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.733.1">Least confidence</span></strong><span class="koboSpan" id="kobo.734.1">: This method selects instances for which the model has the lowest confidence in its predictions. </span><span class="koboSpan" id="kobo.734.2">It focuses on instances where the predicted class probability is closest to 0.5, indicating uncertainty. </span><span class="koboSpan" id="kobo.734.3">For instance, in our email example, if the model predicts a 0.52 probability of a particular email being spam and a 0.48 probability of it not being spam, this indicates that the model is uncertain about its prediction. </span><span class="koboSpan" id="kobo.734.4">This email would be a prime candidate for labeling under the</span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.735.1"> least </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">confidence method.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.737.1">Margin sampling</span></strong><span class="koboSpan" id="kobo.738.1">: Margin sampling</span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.739.1"> aims to find instances where the model’s top two predicted class probabilities are close. </span><span class="koboSpan" id="kobo.739.2">It selects instances with the smallest difference between the highest and second-highest probabilities, as these are likely to be near the decision boundary. </span><span class="koboSpan" id="kobo.739.3">Let’s say we have a model that classifies images of animals. </span><span class="koboSpan" id="kobo.739.4">If it predicts an image with probabilities of 0.4 for cat, 0.38 for dog, and 0.22 for bird, the small difference (0.02) between the top two probabilities suggests uncertainty. </span><span class="koboSpan" id="kobo.739.5">This image would be chosen for labeling in </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">margin sampling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.741.1">Entropy</span></strong><span class="koboSpan" id="kobo.742.1">: Entropy-based sampling </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.743.1">considers the entropy of the predicted class probabilities. </span><span class="koboSpan" id="kobo.743.2">It selects instances with high entropy, indicating a high level of uncertainty in the model’s predictions. </span><span class="koboSpan" id="kobo.743.3">Using the same animal classification model, if an image gets equal 0.33 probabilities for each class (cat, dog, bird), it shows high uncertainty. </span><span class="koboSpan" id="kobo.743.4">This image would be selected for labeling by the </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">entropy method.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.745.1">Let’s implement </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.746.1">uncertainty sampling in Python. </span><span class="koboSpan" id="kobo.746.2">For this example, we go back to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.747.1">credit-g</span></strong><span class="koboSpan" id="kobo.748.1"> dataset introduced at the beginning of this chapter. </span><span class="koboSpan" id="kobo.748.2">Let’s have a look at the features in this dataset to refresh </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">your memory:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.750.1"><img alt="Figure 6.7 – The features of the credit-g dataset" src="image/B19297_06_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.751.1">Figure 6.7 – The features of the credit-g dataset</span></p>
<p><span class="koboSpan" id="kobo.752.1">Under the </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.753.1">assumption that the dataset has already been loaded into a </span><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">df</span></strong><span class="koboSpan" id="kobo.755.1"> DataFrame, we start by preprocessing the dataset by standardizing</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.756.1"> the numerical features and one-hot encoding </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">categorical features:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.758.1">
import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
# Define preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['duration', 'credit_amount', 'installment_commitment',
                                   'residence_since', 'age', 'existing_credits', 'num_dependents']),
        ('cat', OneHotEncoder(), ['checking_status', 'credit_history', 'purpose',
                                  'savings_status', 'employment', 'personal_status', 'other_parties',
                                  'property_magnitude', 'other_payment_plans', 'housing', 'job',
                                  'own_telephone', 'foreign_worker'])])
# Define a mapping from current labels to desired labels
mapping = {'good': 1, 'bad': 0}
# Apply the mapping to the target variable
df['target'] = df['target'].map(mapping)
# Fit and transform the features
features = preprocessor.fit_transform(df.drop('target', axis=1))
# Convert the features to a dataframe
features_df = pd.DataFrame(features)
# Add the target back to the dataframe
df_preprocessed = pd.concat([features_df, df['target'].reset_index(drop=True)], axis=1)</span></pre> <p><span class="koboSpan" id="kobo.759.1">With our new </span><strong class="source-inline"><span class="koboSpan" id="kobo.760.1">df_preprocessed</span></strong><span class="koboSpan" id="kobo.761.1"> DataFrame in hand, we can perform uncertainty sampling. </span><span class="koboSpan" id="kobo.761.2">We start by importing the necessary libraries and modules, including pandas, NumPy and </span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.762.1">scikit-learn, for data manipulation and machine </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">learning operations:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.764.1">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score</span></pre> <p><span class="koboSpan" id="kobo.765.1">We split </span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.766.1">the </span><strong class="source-inline"><span class="koboSpan" id="kobo.767.1">df_preprocessed</span></strong><span class="koboSpan" id="kobo.768.1"> dataset into a small labeled dataset and the remaining unlabeled data. </span><span class="koboSpan" id="kobo.768.2">In this example, we randomly select a small portion of 10% as labeled data and leave the rest as </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">unlabeled data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.770.1">
labeled_data, unlabeled_data = train_test_split(df_preprocessed, test_size=0.9, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.771.1">We define the uncertainty sampling functions—</span><strong class="source-inline"><span class="koboSpan" id="kobo.772.1">least_confidence</span></strong><span class="koboSpan" id="kobo.773.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.774.1">margin_sampling</span></strong><span class="koboSpan" id="kobo.775.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.776.1">entropy_sampling</span></strong><span class="koboSpan" id="kobo.777.1">—as </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">discussed earlier.</span></span></p>
<p><span class="koboSpan" id="kobo.779.1">Here is an explanation of each of </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">these functions:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">least_confidence</span></strong><span class="koboSpan" id="kobo.782.1">: This function takes in a 2D array of probabilities with each row representing an instance and each column representing a class. </span><span class="koboSpan" id="kobo.782.2">For each instance, it calculates the confidence as 1 – </span><strong class="source-inline"><span class="koboSpan" id="kobo.783.1">max_probability</span></strong><span class="koboSpan" id="kobo.784.1">, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.785.1">max_probability</span></strong><span class="koboSpan" id="kobo.786.1"> is the largest predicted probability across all classes. </span><span class="koboSpan" id="kobo.786.2">It then sorts the instances by confidence in ascending order. </span><span class="koboSpan" id="kobo.786.3">The idea is that instances with lower confidence (i.e., higher uncertainty) are more informative and should be </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">labeled first:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.788.1">
def least_confidence(probabilities):
    confidence = 1 - np.max(probabilities, axis=1)
    return np.argsort(confidence)</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.789.1">margin_sampling</span></strong><span class="koboSpan" id="kobo.790.1">: This function also takes in a 2D array of probabilities. </span><span class="koboSpan" id="kobo.790.2">For each instance, it calculates the margin as the difference between the highest and second-highest</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.791.1"> predicted probabilities. </span><span class="koboSpan" id="kobo.791.2">It then sorts the instances by margin in </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.792.1">ascending order. </span><span class="koboSpan" id="kobo.792.2">The idea is that instances with smaller margins (i.e., closer top-two class probabilities) are more informative and should be </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">labeled first:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.794.1">
def margin_sampling(probabilities):
    sorted_probs = np.sort(probabilities, axis=1)
    margin = sorted_probs[:, -1] - sorted_probs[:, -2]
    return np.argsort(margin)</span></pre></li> <li><strong class="source-inline"><span class="koboSpan" id="kobo.795.1">entropy_sampling</span></strong><span class="koboSpan" id="kobo.796.1">: This function calculates the entropy of the predicted probabilities for each instance. </span><span class="koboSpan" id="kobo.796.2">Entropy is a measure of uncertainty or disorder, with higher values indicating greater uncertainty. </span><span class="koboSpan" id="kobo.796.3">It then sorts the instances by entropy in ascending order. </span><span class="koboSpan" id="kobo.796.4">The idea is that instances with higher entropy (i.e., more uncertainty in the class probabilities) are more informative and should be </span><span class="No-Break"><span class="koboSpan" id="kobo.797.1">labeled first:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.798.1">
def entropy_sampling(probabilities):
    entropy = -np.sum(probabilities * np.log2(probabilities), axis=1)
    return np.argsort(entropy)</span></pre></li> </ul>
<p><span class="koboSpan" id="kobo.799.1">We enter the active learning loop, where we iteratively train a model, select instances for labeling using uncertainty sampling, obtain labels for those instances, and update the </span><span class="No-Break"><span class="koboSpan" id="kobo.800.1">labeled dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.801.1">Firstly, a list named </span><strong class="source-inline"><span class="koboSpan" id="kobo.802.1">accuracies</span></strong><span class="koboSpan" id="kobo.803.1"> is used to keep track of the accuracy of the model on the labeled data at </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">each iteration.</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">The active learning loop is then implemented over a specified number of iterations. </span><span class="koboSpan" id="kobo.805.2">In each iteration, a logistic regression model is trained on the labeled data, and its accuracy is calculated </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.806.1">and stored. </span><span class="koboSpan" id="kobo.806.2">The model then makes predictions on the unlabeled data, and the instances about which it is least confident (as determined by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.807.1">least_confidence</span></strong><span class="koboSpan" id="kobo.808.1"> function) are added to the labeled dataset. </span><span class="koboSpan" id="kobo.808.2">These instances are removed from the </span><a id="_idIndexMarker385"/><span class="No-Break"><span class="koboSpan" id="kobo.809.1">unlabeled dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.810.1">
# Initialize a list to store the accuracy at each iteration
accuracies = []
# Implement the active learning loop.
</span><span class="koboSpan" id="kobo.810.2">num_iterations = 5
batch_size = 20
for _ in range(num_iterations):
    model = LogisticRegression()
    model.fit(X_labeled, y_labeled)
    # Calculate and store the accuracy on the labeled data at this iteration
    accuracies.append(accuracy_score(y_labeled, model.predict(X_labeled)))
    probabilities = model.predict_proba(X_unlabeled)
    indices = least_confidence(probabilities)[:batch_size]
    X_newly_labeled = X_unlabeled[indices]
    y_newly_labeled = y_unlabeled[indices]
    X_labeled = np.concatenate([X_labeled, X_newly_labeled])
    y_labeled = np.concatenate([y_labeled, y_newly_labeled])
    X_unlabeled = np.delete(X_unlabeled, indices, axis=0)
    y_unlabeled = np.delete(y_unlabeled, indices)</span></pre> <p><span class="koboSpan" id="kobo.811.1">The final output</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.812.1"> is an updated labeled dataset containing additional instances labeled during each iteration of active learning. </span><span class="koboSpan" id="kobo.812.2">The process aims to</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.813.1"> improve model performance by iteratively selecting and labeling the most informative instances from the unlabeled data. </span><span class="koboSpan" id="kobo.813.2">In the preceding code, the success and effectiveness of active learning depend on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">least_confidence</span></strong><span class="koboSpan" id="kobo.815.1"> custom function and the characteristics of the dataset. </span><span class="koboSpan" id="kobo.815.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">least_confidence</span></strong><span class="koboSpan" id="kobo.817.1"> function is assumed to return indices corresponding to the least </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">confident predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">Note that this code uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">least_confidence</span></strong><span class="koboSpan" id="kobo.821.1"> function to perform active learning. </span><span class="koboSpan" id="kobo.821.2">To perform the same process using </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">margin_sampling</span></strong><span class="koboSpan" id="kobo.823.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">entropy_sampling</span></strong><span class="koboSpan" id="kobo.825.1"> instead of </span><strong class="source-inline"><span class="koboSpan" id="kobo.826.1">least_confidence</span></strong><span class="koboSpan" id="kobo.827.1">, you could replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">least_confidence(probabilities)[:batch_size]</span></strong><span class="koboSpan" id="kobo.829.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">margin_sampling(probabilities)[:batch_size]</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.831.1">or </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.832.1">entropy_sampling(probabilities)[:batch_size]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.834.1">Let’s compare the performance of each of the three active learning functions on the same sample from </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">credit-g</span></strong><span class="koboSpan" id="kobo.836.1">. </span><span class="koboSpan" id="kobo.836.2">We use matplotlib to produce visual representations of the accuracy for each of the three active learning functions. </span><span class="koboSpan" id="kobo.836.3">To replicate the output, apply the following code to the outputs of </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">each function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.838.1">
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
ax = plt.figure().gca()
ax.xaxis.set_major_locator(MaxNLocator(integer=True))
plt.plot(range(1, num_iterations + 1), accuracies)
plt.xlabel('Iteration')
plt.ylabel('Accuracy')
plt.title('Model accuracy over iterations (least_confidence)')
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.839.1">The </span><em class="italic"><span class="koboSpan" id="kobo.840.1">least confidence</span></em><span class="koboSpan" id="kobo.841.1"> method</span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.842.1"> achieved a model accuracy of 0.878 after five iterations, with the best </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.843.1">performance observed after the </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">first iteration:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.845.1"><img alt="Figure 6.8 – Accuracy of the least_confidence active learning function over five iterations when predicting the target variable on the credit-g dataset" src="image/B19297_06_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.846.1">Figure 6.8 – Accuracy of the least_confidence active learning function over five iterations when predicting the target variable on the credit-g dataset</span></p>
<p><span class="koboSpan" id="kobo.847.1">Margin sampling achieved a slightly higher accuracy of 0.9 after two and </span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">three iterations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.849.1"><img alt="Figure 6.9 – Accuracy of the margin_sampling active learning function over five iterations when predicting the target variable on the credit-g dataset" src="image/B19297_06_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.850.1">Figure 6.9 – Accuracy of the margin_sampling active learning function over five iterations when predicting the target variable on the credit-g dataset</span></p>
<p><span class="koboSpan" id="kobo.851.1">Lastly, entropy sampling </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.852.1">and least confidence</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.853.1"> achieved identical results. </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">How come?</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.855.1"><img alt="Figure 6.10 – Accuracy of the entropy_sampling active learning function over five iterations when predicting the target variable on the credit-g dataset" src="image/B19297_06_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.856.1">Figure 6.10 – Accuracy of the entropy_sampling active learning function over five iterations when predicting the target variable on the credit-g dataset</span></p>
<p><span class="koboSpan" id="kobo.857.1">These</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.858.1"> two methods </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.859.1">may yield the same results in certain scenarios, especially when working with binary classification problems. </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">Here’s why.</span></span></p>
<p><span class="koboSpan" id="kobo.861.1">The least confidence method considers the class with the highest predicted probability. </span><span class="koboSpan" id="kobo.861.2">If the model is very confident about a particular class (i.e., the probability is close to 1), then it’s less likely that this instance will be selected </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">for labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.863.1">Entropy sampling considers the entropy or “disorder” of the predicted probabilities. </span><span class="koboSpan" id="kobo.863.2">For binary classification problems, entropy is maximized when the probabilities are both equal (i.e., the model is completely unsure which class to predict). </span><span class="koboSpan" id="kobo.863.3">This could coincide with </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">low-confidence predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.865.1">As a result, both methods will often select the same instances for labeling in the context of binary classification. </span><span class="koboSpan" id="kobo.865.2">However, this might not always be the case, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">multi-class problems.</span></span></p>
<p><span class="koboSpan" id="kobo.867.1">On the other hand, margin sampling </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.868.1">focuses on the difference between the highest and second-highest predicted probabilities. </span><span class="koboSpan" id="kobo.868.2">Even slight differences in these probabilities can lead to different instances being selected compared to the </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">other methods.</span></span></p>
<p><span class="koboSpan" id="kobo.870.1">In the next section, we’ll explore </span><strong class="bold"><span class="koboSpan" id="kobo.871.1">Query by Committee</span></strong><span class="koboSpan" id="kobo.872.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.873.1">QBC</span></strong><span class="koboSpan" id="kobo.874.1">) for labeling—a method that brings together a group of models to help decide which data points are most important </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">for labeling.</span></span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.876.1">Query by Committee (QBC)</span></h2>
<p><span class="koboSpan" id="kobo.877.1">QBC is based</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.878.1"> on</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.879.1"> the idea that instances for which the committee of models disagrees or exhibits high uncertainty are the most informative and should be prioritized for labeling. </span><span class="koboSpan" id="kobo.879.2">Instead of relying on a single model’s prediction, QBC takes advantage of the diversity and collective decision-making of the committee to make informed </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">labeling decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.881.1">The QBC process typically involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.883.1">Committee creation</span></strong><span class="koboSpan" id="kobo.884.1">: Create an initial committee of multiple models trained on the available labeled data. </span><span class="koboSpan" id="kobo.884.2">Models in a committee can be diverse in terms of their architectures, initializations, or </span><span class="No-Break"><span class="koboSpan" id="kobo.885.1">training methodologies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.886.1">Instance selection</span></strong><span class="koboSpan" id="kobo.887.1">: Apply the committee of models to the unlabeled instances and obtain predictions. </span><span class="koboSpan" id="kobo.887.2">Choose the instances that elicit the most disagreement or uncertainty among the committee members </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">for labeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.889.1">Committee update</span></strong><span class="koboSpan" id="kobo.890.1">: Label the selected instances and add them to the labeled dataset. </span><span class="koboSpan" id="kobo.890.2">Re-train or update the committee of models using the expanded </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">labeled dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.892.1">Repeat</span></strong><span class="koboSpan" id="kobo.893.1">: Iterate the process by returning to </span><em class="italic"><span class="koboSpan" id="kobo.894.1">step 2</span></em><span class="koboSpan" id="kobo.895.1"> until a desired performance level is achieved or labeling resources </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">are exhausted.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.897.1">QBC offers </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.898.1">several advantages in active learning </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">for labeling:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.900.1">Model diversity</span></strong><span class="koboSpan" id="kobo.901.1">: QBC utilizes a committee of models, allowing for diverse perspectives and capturing different aspects of the data distribution. </span><span class="koboSpan" id="kobo.901.2">This diversity helps identify instances that are challenging or ambiguous, leading to improved </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">labeling decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.903.1">Model confidence estimation</span></strong><span class="koboSpan" id="kobo.904.1">: By observing the disagreement or uncertainty among the committee members, QBC provides an estimate of the models’ confidence in their predictions. </span><span class="koboSpan" id="kobo.904.2">Instances that lead to disagreement or uncertainty can be considered more informative and valuable </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">for labeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.906.1">Labeling efficiency</span></strong><span class="koboSpan" id="kobo.907.1">: QBC aims to prioritize instances that have the greatest impact on the committee’s decision. </span><span class="koboSpan" id="kobo.907.2">This approach can save labeling efforts by focusing on instances that provide the most relevant information to improve the </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">model’s performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.909.1">Let’s implement</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.910.1"> this approach using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">credit-g</span></strong><span class="koboSpan" id="kobo.912.1"> dataset in Python. </span><span class="koboSpan" id="kobo.912.2">First, we define functions for creating the committee, obtaining committee predictions, and measuring disagreement or uncertainty among the </span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">committee members.</span></span></p>
<p><span class="koboSpan" id="kobo.914.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.915.1">create_committee(num_models)</span></strong><span class="koboSpan" id="kobo.916.1"> function creates a committee of logistic regression models. </span><span class="koboSpan" id="kobo.916.2">The number of models in the committee is specified </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">by </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.918.1">num_models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.920.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.921.1">get_committee_predictions(committee, data)</span></strong><span class="koboSpan" id="kobo.922.1"> function gets predictions from each model in the committee for the provided data. </span><span class="koboSpan" id="kobo.922.2">It returns an array of </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">prediction probabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.924.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.925.1">measure_disagreement(predictions)</span></strong><span class="koboSpan" id="kobo.926.1"> function measures the disagreement among the committee’s predictions. </span><span class="koboSpan" id="kobo.926.2">It calculates the variance of the predictions and returns the </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">mean disagreement:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.928.1">
def create_committee(num_models):
    committee = []
    for _ in range(num_models):
        model = LogisticRegression()
        # Customize and train each model as needed
        committee.append(model)
    return committee
def get_committee_predictions(committee, data):
    predictions = []
    for model in committee:
        preds = model.predict_proba(data)
        predictions.append(preds)
    return np.array(predictions)
def measure_disagreement(predictions):
    disagreement = np.var(predictions, axis=0)
    return np.mean(disagreement, axis=1)</span></pre> <p><span class="koboSpan" id="kobo.929.1">We enter the</span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.930.1"> active learning loop, where we iteratively train the committee, measure disagreement or uncertainty, select instances for labeling, obtain labels for those instances, and update the </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">labeled dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.932.1">
labeled_dataset = labeled_data.copy()
num_iterations = 5
batch_size = 20
committee = create_committee(num_models=3)
for _ in range(num_iterations):
    for model in committee:
        X_train = labeled_dataset.drop('target', axis=1)
        y_train = labeled_dataset['target']
        model.fit(X_train, y_train)
    X_unlabeled = unlabeled_data.drop('target', axis=1)
    committee_predictions = get_committee_predictions(committee, X_unlabeled)
    disagreement = measure_disagreement(committee_predictions)
    indices = np.argsort(disagreement)[-batch_size:]
    labeled_instances = unlabeled_data.iloc[indices]
    labels = labeled_instances['Loan_Status']
    labeled_dataset = pd.concat([labeled_dataset, labeled_instances])
    unlabeled_data = unlabeled_data.drop(labeled_instances.index)</span></pre> <p><span class="koboSpan" id="kobo.933.1">Here’s an explanation of the code. </span><strong class="source-inline"><span class="koboSpan" id="kobo.934.1">labeled_dataset = labeled_data.copy()</span></strong><span class="koboSpan" id="kobo.935.1"> creates a copy of the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">labeled dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.937.1">The </span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.938.1">loop over </span><strong class="source-inline"><span class="koboSpan" id="kobo.939.1">num_iterations</span></strong><span class="koboSpan" id="kobo.940.1"> represents the</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.941.1"> number of rounds of semi-supervised learning. </span><span class="koboSpan" id="kobo.941.2">In each round, the following </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">steps occur:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.943.1">Each model in the committee is trained on the current </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">labeled dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.945.1">The committee makes predictions on the </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">unlabeled data.</span></span></li>
<li><span class="koboSpan" id="kobo.947.1">The disagreement among the committee’s predictions </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">is calculated.</span></span></li>
<li><span class="koboSpan" id="kobo.949.1">The indices of the instances with the highest disagreement are identified. </span><span class="koboSpan" id="kobo.949.2">The size of this batch is specified </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">by </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.951.1">batch_size</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.953.1">These instances are added to the </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">labeled dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.955.1">Finally, these instances are removed from the </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">unlabeled data.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.957.1">The idea behind this approach is that the instances the models disagree about the most are the ones where the models are most uncertain. </span><span class="koboSpan" id="kobo.957.2">By adding these instances to the labeled dataset, the </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.958.1">models can learn more from them in the next round of training. </span><span class="koboSpan" id="kobo.958.2">This process continues for a specified number </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">of iterations.</span></span></p>
<p><span class="koboSpan" id="kobo.960.1">Now let’s discuss diversity sampling in labeling—a smart technique that focuses on selecting a varied set of data points to ensure a well-rounded and representative </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">labeled dataset.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.962.1">Diversity sampling</span></h2>
<p><span class="koboSpan" id="kobo.963.1">Diversity sampling</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.964.1"> is based on the principle that selecting instances that cover diverse patterns or regions in the dataset can provide a more comprehensive understanding of the underlying data distribution. </span><span class="koboSpan" id="kobo.964.2">By actively seeking diverse instances for labeling, diversity sampling aims to improve model generalization </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">and robustness.</span></span></p>
<p><span class="koboSpan" id="kobo.966.1">The diversity sampling </span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.967.1">process typically involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.969.1">Initial model training</span></strong><span class="koboSpan" id="kobo.970.1">: Train an initial machine learning model using a small, labeled dataset. </span><span class="koboSpan" id="kobo.970.2">This model will be used to guide the selection of diverse instances </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">for labeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.972.1">Instance selection</span></strong><span class="koboSpan" id="kobo.973.1">: Apply the trained model to the unlabeled instances and obtain predictions. </span><span class="koboSpan" id="kobo.973.2">Calculate a diversity metric to measure the dissimilarity or coverage of each instance with respect to the already labeled instances. </span><span class="koboSpan" id="kobo.973.3">Select instances with the highest diversity metric </span><span class="No-Break"><span class="koboSpan" id="kobo.974.1">for labeling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.975.1">Labeling and model update</span></strong><span class="koboSpan" id="kobo.976.1">: Label the selected instances and add them to the labeled dataset. </span><span class="koboSpan" id="kobo.976.2">Retrain or update the machine learning model using the expanded </span><span class="No-Break"><span class="koboSpan" id="kobo.977.1">labeled dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.978.1">Repeat</span></strong><span class="koboSpan" id="kobo.979.1">: Iterate the process by returning to </span><em class="italic"><span class="koboSpan" id="kobo.980.1">step 2</span></em><span class="koboSpan" id="kobo.981.1"> until a desired performance level is achieved or labeling resources </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">are exhausted.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.983.1">Diversity sampling offers </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.984.1">several advantages in active learning </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">for labeling:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.986.1">Comprehensive data coverage</span></strong><span class="koboSpan" id="kobo.987.1">: By selecting diverse instances, diversity sampling ensures that the labeled dataset covers a wide range of patterns or regions in the data. </span><span class="koboSpan" id="kobo.987.2">This approach helps the model generalize better to unseen instances and improves its ability to handle </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">different scenarios.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.989.1">Exploration of data distribution</span></strong><span class="koboSpan" id="kobo.990.1">: Diversity sampling encourages the exploration of the underlying data distribution by actively seeking instances from different parts of the feature space. </span><span class="koboSpan" id="kobo.990.2">This exploration can reveal important insights about the data and improve the model’s understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">complex relationships.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.992.1">Mitigation of bias and overfitting</span></strong><span class="koboSpan" id="kobo.993.1">: Diversity sampling can help mitigate biases and overfitting that may arise from selecting only easy or similar instances for labeling. </span><span class="koboSpan" id="kobo.993.2">By diversifying the labeled dataset, diversity sampling reduces the risk of model overconfidence and enhances </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">its robustness.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.995.1">Let’s explore</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.996.1"> this approach on our preprocessed </span><strong class="source-inline"><span class="koboSpan" id="kobo.997.1">credit-g</span></strong><span class="koboSpan" id="kobo.998.1"> dataset, using pairwise distances from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.999.1">sklearn</span></strong><span class="koboSpan" id="kobo.1000.1"> library in Python. </span><span class="koboSpan" id="kobo.1000.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">pairwise_distances</span></strong><span class="koboSpan" id="kobo.1002.1"> function from </span><strong class="source-inline"><span class="koboSpan" id="kobo.1003.1">sklearn</span></strong><span class="koboSpan" id="kobo.1004.1"> calculates the distance between each pair of instances in a dataset. </span><span class="koboSpan" id="kobo.1004.2">In the context of diversity sampling, this function is </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.1005.1">used to find instances that are most different from </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">each other.</span></span></p>
<p><span class="koboSpan" id="kobo.1007.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">the process:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1009.1">Compute the pairwise distances between all pairs of instances in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">unlabeled dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.1011.1">Identify the instances that have the greatest distances between them. </span><span class="koboSpan" id="kobo.1011.2">These are the most diverse instances according to the distance </span><span class="No-Break"><span class="koboSpan" id="kobo.1012.1">metric used.</span></span></li>
<li><span class="koboSpan" id="kobo.1013.1">Select these diverse instances for labeling and add them to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">labeled dataset.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1015.1">The idea is that by actively seeking out diverse instances (those that are farthest apart in terms of the chosen distance metric), you can cover a wider range of patterns in the underlying data distribution. </span><span class="koboSpan" id="kobo.1015.2">This helps to improve the model’s ability to generalize to new data and enhances </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">its robustness.</span></span></p>
<p><span class="koboSpan" id="kobo.1017.1">First, we import the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1018.1">pairwise_distances</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1"> function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1020.1">
from sklearn.metrics.pairwise import pairwise_distances</span></pre> <p><span class="koboSpan" id="kobo.1021.1">We define</span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.1022.1"> functions for calculating diversity and selecting instances with the highest diversity for labeling. </span><span class="koboSpan" id="kobo.1022.2">We will use pairwise Euclidean distance as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">diversity metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1024.1">
def calculate_diversity(data):
    distance_matrix = pairwise_distances(data, metric='euclidean')
    diversity = np.sum(distance_matrix, axis=1)
    return diversity
def select_diverse_instances(data, num_instances):
    diversity = calculate_diversity(data)
    indices = np.argsort(diversity)[-num_instances:]
    return data.iloc[indices]</span></pre> <p><span class="koboSpan" id="kobo.1025.1">We enter the</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.1026.1"> active learning loop, where we iteratively calculate diversity, select diverse instances for labeling, obtain labels for those instances, and update the </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">labeled dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1028.1">
labeled_dataset = labeled_data.copy()
num_iterations = 5
batch_size = 20
for _ in range(num_iterations):
    X_unlabeled = unlabeled_data.drop('target', axis=1)
    diversity = calculate_diversity(X_unlabeled)
    labeled_instances = select_diverse_instances(unlabeled_data, batch_size)
    labels = labeled_instances['target']
    labeled_dataset = pd.concat([labeled_dataset, labeled_instances])
    unlabeled_data = unlabeled_data.drop(labeled_instances.index)</span></pre> <p><span class="koboSpan" id="kobo.1029.1">In the next section, we’ll </span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.1030.1">explore transfer learning in labeling—an advanced method that leverages knowledge gained from one task to improve performance on a different but </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">related task.</span></span></p>
<h1 id="_idParaDest-98"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.1032.1">Transfer learning</span></h1>
<p><span class="koboSpan" id="kobo.1033.1">Transfer learning</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.1034.1"> involves using knowledge gained from a source task or domain to aid learning. </span><span class="koboSpan" id="kobo.1034.2">Instead of starting from scratch, transfer learning leverages pre-existing information, such as labeled data or pre-trained models, to bootstrap the learning process and improve the performance of the target task. </span><span class="koboSpan" id="kobo.1034.3">Transfer learning</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.1035.1"> offers several advantages in the labeling process of </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">machine learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1037.1">Reduced labeling effort</span></strong><span class="koboSpan" id="kobo.1038.1">: By leveraging pre-existing labeled data, transfer learning reduces the need for the manual labeling of a large amount of data for the target task. </span><span class="koboSpan" id="kobo.1038.2">It enables the reuse of knowledge from related tasks, domains, or datasets, saving time and effort in acquiring </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">new labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1040.1">Improved model performance</span></strong><span class="koboSpan" id="kobo.1041.1">: Transfer learning allows the target model to benefit from the knowledge learned by a source model. </span><span class="koboSpan" id="kobo.1041.2">The source model might have been trained on a large, labeled dataset or a different but related task, providing valuable insights and patterns that can enhance the target </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">model’s performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1043.1">Adaptability to limited labeled data</span></strong><span class="koboSpan" id="kobo.1044.1">: Transfer learning is particularly useful when the target task has limited labeled data. </span><span class="koboSpan" id="kobo.1044.2">By leveraging labeled data from a source task or domain, transfer learning can help generalize the target model better and mitigate the risk of overfitting on a small, </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">labeled dataset.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1046.1">Transfer learning can be applied </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.1047.1">in various ways for labeling in </span><span class="No-Break"><span class="koboSpan" id="kobo.1048.1">machine learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1049.1">Feature extraction</span></strong><span class="koboSpan" id="kobo.1050.1">: Utilize pre-trained models as feature extractors. </span><span class="koboSpan" id="kobo.1050.2">Extract high-level features from pre-trained models and feed them as inputs to a new model that is trained on the target </span><span class="No-Break"><span class="koboSpan" id="kobo.1051.1">labeled dataset.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1052.1">Fine-tuning pre-trained models</span></strong><span class="koboSpan" id="kobo.1053.1">: Use pre-trained models that have been trained on large, labeled datasets, such as models from popular deep learning architectures such as VGG, ResNet, or BERT. </span><span class="koboSpan" id="kobo.1053.2">Fine-tune these pre-trained models on a smaller labeled dataset specific to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">target task.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1055.1">Let’s discuss these in </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">more detail.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.1057.1">Feature extraction</span></h2>
<p><span class="koboSpan" id="kobo.1058.1">Feature extraction</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.1059.1"> involves using the representations learned by a pre-trained model as input features for a new model. </span><span class="koboSpan" id="kobo.1059.2">This approach is particularly useful when the pre-trained model has been trained on a large, general-purpose dataset such as ImageNet. </span><span class="koboSpan" id="kobo.1059.3">Here’s an example of using transfer learning for image labeling using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">VGG16 model.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.1061.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1062.1">The data used in this example is available </span><span class="No-Break"><span class="koboSpan" id="kobo.1063.1">from </span></span><a href="https://github.com/odegeasslbc/FastGAN-pytorch"><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">https://github.com/odegeasslbc/FastGAN-pytorch</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1065.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1066.1">We first import the </span><span class="No-Break"><span class="koboSpan" id="kobo.1067.1">necessary libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1068.1">
import PIL
import PIL.Image
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten</span></pre> <p><span class="koboSpan" id="kobo.1069.1">We will use an </span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.1070.1">image of a Golden Retriever for labeling. </span><span class="koboSpan" id="kobo.1070.2">We can view this image using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1071.1">PIL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1"> library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1073.1">
PIL.Image.open('path_to_image_2.jpg')</span></pre> <p><span class="koboSpan" id="kobo.1074.1">This will display the </span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">following image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.1076.1"><img alt="Figure 6.11 – The Golden Retriever: man’s best friend and our sample image" src="image/B19297_06_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1077.1">Figure 6.11 – The Golden Retriever: man’s best friend and our sample image</span></p>
<p><span class="koboSpan" id="kobo.1078.1">Let’s load the pre-trained VGG16 model from the TensorFlow library and predict the label for this </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">sample image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1080.1">
model = VGG16(weights='imagenet')
img_path = 'path_to_image_2.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)
features = model.predict(x)
decoded_predictions = decode_predictions(features, top=5)[0]
for _, label, confidence in decoded_predictions:
    print(label, confidence)</span></pre> <p><span class="koboSpan" id="kobo.1081.1">This will </span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.1082.1">generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.1084.1"><img alt="Figure 6.12 – The VGG16 model’s prediction with confidence levels; the model has labeled the image as golden_retriever with a high level of confidence" src="image/B19297_06_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1085.1">Figure 6.12 – The VGG16 model’s prediction with confidence levels; the model has labeled the image as golden_retriever with a high level of confidence</span></p>
<p><span class="koboSpan" id="kobo.1086.1">The model has correctly predicted the image as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1087.1">golden_retriever</span></strong><span class="koboSpan" id="kobo.1088.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1089.1">0.92119014</span></strong><span class="koboSpan" id="kobo.1090.1"> confidence. </span><span class="koboSpan" id="kobo.1090.2">We now understand how a pre-trained model can be used on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">new dataset.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.1092.1">Fine-tuning pre-trained models</span></h2>
<p><span class="koboSpan" id="kobo.1093.1">Fine-tuning</span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.1094.1"> in transfer learning refers to the process of adapting or updating the pre-trained model’s parameters to better fit a specific task or dataset of interest. </span><span class="koboSpan" id="kobo.1094.2">When using transfer learning, the pre-trained model is initially trained on a large-scale dataset, typically from a different but related task or domain. </span><span class="koboSpan" id="kobo.1094.3">Fine-tuning allows us to take advantage of the knowledge learned by the pre-trained model and customize it for a specific task </span><span class="No-Break"><span class="koboSpan" id="kobo.1095.1">or dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.1096.1">The fine-tuning process typically involves the </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1098.1">Pre-trained model initialization</span></strong><span class="koboSpan" id="kobo.1099.1">: The pre-trained model, which has already learned useful representations from a source task or dataset, is loaded. </span><span class="koboSpan" id="kobo.1099.2">The model’s parameters are frozen initially, meaning they are not updated during the </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">initial training.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1101.1">Modification of the model</span></strong><span class="koboSpan" id="kobo.1102.1">: Depending on the specific task or dataset, the last few layers or specific parts of the pre-trained model may be modified or replaced. </span><span class="koboSpan" id="kobo.1102.2">The architecture of the model can be adjusted to match the desired output or accommodate the characteristics of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1103.1">target task.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1104.1">Unfreezing and training</span></strong><span class="koboSpan" id="kobo.1105.1">: After modifying the model, the previously frozen parameters are unfrozen, allowing them to be updated during training. </span><span class="koboSpan" id="kobo.1105.2">The model is then trained on the target task-specific dataset, often referred to as the fine-tuning dataset. </span><span class="koboSpan" id="kobo.1105.3">The weights of the model are updated using backpropagation and gradient-based optimization algorithms to minimize the task-specific </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">loss function.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1107.1">Training with a lower learning rate</span></strong><span class="koboSpan" id="kobo.1108.1">: During fine-tuning, a smaller learning rate is typically used compared to the initial training of the pre-trained model. </span><span class="koboSpan" id="kobo.1108.2">This smaller learning rate helps to ensure that the previously learned representations are preserved to some extent while allowing the model to adapt to the target task </span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">or dataset.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1110.1">The process </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.1111.1">of fine-tuning strikes a balance between utilizing the knowledge captured by the pre-trained model and tailoring it to the specifics of the target task. </span><span class="koboSpan" id="kobo.1111.2">By fine-tuning, the model can learn task-specific patterns and optimize its performance for the new task or dataset. </span><span class="koboSpan" id="kobo.1111.3">The amount of fine-tuning required may vary depending on the similarity between the source and target tasks or domains. </span><span class="koboSpan" id="kobo.1111.4">In some cases, only a few training iterations may be sufficient, while in others, more extensive training may </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">be necessary.</span></span></p>
<p><span class="koboSpan" id="kobo.1113.1">Fine-tuning is a crucial step in transfer learning, as it enables the transfer of knowledge from a source task or dataset to a target task, resulting in improved performance and faster convergence on the target task. </span><span class="koboSpan" id="kobo.1113.2">Here’s an example of using transfer learning for image labeling using the VGG16 model. </span><span class="koboSpan" id="kobo.1113.3">We first import the </span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">necessary libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1115.1">
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten</span></pre> <p><span class="koboSpan" id="kobo.1116.1">We have </span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.1117.1">two classes of images – that of dogs and cats – and therefore we set the number of classes variable to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1118.1">2</span></strong><span class="koboSpan" id="kobo.1119.1">. </span><span class="koboSpan" id="kobo.1119.2">We will also load a pre-trained VGG16 model without the top layers. </span><span class="koboSpan" id="kobo.1119.3">The image sizes we have here are 256 x 256 </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">x 3:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1121.1">
num_classes = 2
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))</span></pre> <p><span class="koboSpan" id="kobo.1122.1">We now freeze the pre-trained layers and create a new model for fine-tuning. </span><span class="koboSpan" id="kobo.1122.2">We then compile the model using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1123.1">'adam'</span></strong><span class="koboSpan" id="kobo.1124.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">the optimizer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1126.1">
for layer in base_model.layers:
    layer.trainable = False
model = Sequential()
model.add(base_model)
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</span></pre> <p><span class="koboSpan" id="kobo.1127.1">To prepare for training, we are configuring data generators for both training and validation datasets. </span><span class="koboSpan" id="kobo.1127.2">This crucial step involves rescaling pixel values to a range between 0 and 1 using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1128.1">ImageDataGenerator</span></strong><span class="koboSpan" id="kobo.1129.1">. </span><span class="koboSpan" id="kobo.1129.2">By doing so, we ensure consistent and efficient processing of image data, enhancing the model’s ability to learn patterns and features </span><span class="No-Break"><span class="koboSpan" id="kobo.1130.1">during training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1131.1">
train_data_dir = /path_to_training_data'
validation_data_dir = '/path_to_validation_data'
batch_size = 32
train_datagen = ImageDataGenerator(rescale=1.0/255.0)
validation_datagen = ImageDataGenerator(rescale=1.0/255.0)
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(256, 256),
    batch_size=batch_size,
    class_mode='categorical')
validation_generator = validation_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(256, 256),
    batch_s</span><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.1132.1">ize=batch_size,
    class_mode='categorical')</span></pre> <p><span class="koboSpan" id="kobo.1133.1">We can now</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.1134.1"> fine-tune the model and </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">save it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1136.1">
epochs = 10
model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size)
model.save('fine_tuned_model.h5')</span></pre> <p><span class="koboSpan" id="kobo.1137.1">With this </span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.1138.1">saved model, we can deploy it for various applications, such as making predictions on new data, integrating it into larger systems, or further fine-tuning similar tasks. </span><span class="koboSpan" id="kobo.1138.2">The saved model file encapsulates the learned patterns and features from the training process, providing a valuable resource for future use </span><span class="No-Break"><span class="koboSpan" id="kobo.1139.1">and analysis.</span></span></p>
<p><span class="koboSpan" id="kobo.1140.1">In the next section, we’ll delve into the concept of semi-supervised learning in labeling—a sophisticated yet approachable technique that combines the strengths of both labeled and </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">unlabeled data.</span></span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.1142.1">Semi-supervised learning</span></h1>
<p><span class="koboSpan" id="kobo.1143.1">Traditional supervised learning </span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.1144.1">relies on a fully labeled dataset, which can be time-consuming and costly to obtain. </span><span class="koboSpan" id="kobo.1144.2">Semi-supervised learning, on the other hand, allows us to leverage both labeled and unlabeled data to train models and make predictions. </span><span class="koboSpan" id="kobo.1144.3">This approach offers a more efficient way to label data and improve </span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.1146.1">Semi-supervised learning is particularly useful when labeled data is scarce or expensive to obtain. </span><span class="koboSpan" id="kobo.1146.2">It allows us to make use of the vast amounts of readily available unlabeled data, which is often abundant in real-world scenarios. </span><span class="koboSpan" id="kobo.1146.3">By leveraging unlabeled data, semi-supervised </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.1147.1">learning offers </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">several benefits:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1149.1">Cost-effectiveness</span></strong><span class="koboSpan" id="kobo.1150.1">: Semi-supervised learning reduces the reliance on expensive manual labeling efforts. </span><span class="koboSpan" id="kobo.1150.2">By using unlabeled data, which can be collected at a lower cost, we can significantly reduce the expenses associated with acquiring </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">labeled data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1152.1">Utilization of large unlabeled datasets</span></strong><span class="koboSpan" id="kobo.1153.1">: Unlabeled data is often abundant and easily accessible. </span><span class="koboSpan" id="kobo.1153.2">Semi-supervised learning enables us to tap into this vast resource, allowing us to train models on much larger datasets compared to fully supervised learning. </span><span class="koboSpan" id="kobo.1153.3">This can lead to better model generalization </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">and performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1155.1">Improved model performance</span></strong><span class="koboSpan" id="kobo.1156.1">: By incorporating unlabeled data during training, semi-supervised learning can improve model performance. </span><span class="koboSpan" id="kobo.1156.2">The unlabeled data provides additional information and helps the model capture the underlying </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.1157.1">data distribution more accurately. </span><span class="koboSpan" id="kobo.1157.2">This can lead to better generalization and increased accuracy on </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">unseen data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1159.1">There are different approaches within semi-supervised learning that leverage the unlabeled data in different ways. </span><span class="koboSpan" id="kobo.1159.2">Some </span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.1160.1">common methods include </span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1162.1">Self-training</span></strong><span class="koboSpan" id="kobo.1163.1">: Self-training involves training a model initially on the limited labeled data. </span><span class="koboSpan" id="kobo.1163.2">Then, the model is used to make predictions on the unlabeled data, and the confident predictions are considered as pseudo-labels for the unlabeled instances. </span><span class="koboSpan" id="kobo.1163.3">These pseudo-labeled instances are then combined with the labeled data to retrain the </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">model iteratively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1165.1">Co-training</span></strong><span class="koboSpan" id="kobo.1166.1">: Co-training involves training multiple models on different subsets or views of the data. </span><span class="koboSpan" id="kobo.1166.2">Each model learns from the labeled data and then predicts labels for the unlabeled data. </span><span class="koboSpan" id="kobo.1166.3">The agreement or disagreement between the models’ predictions on the unlabeled data is used to select the most confident instances, which are then labeled and added to the training set for </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">further iterations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1168.1">Generative models</span></strong><span class="koboSpan" id="kobo.1169.1">: Generative models, such as </span><strong class="bold"><span class="koboSpan" id="kobo.1170.1">variational autoencoders</span></strong><span class="koboSpan" id="kobo.1171.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1172.1">VAEs</span></strong><span class="koboSpan" id="kobo.1173.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.1174.1">generative adversarial networks</span></strong><span class="koboSpan" id="kobo.1175.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1176.1">GANs</span></strong><span class="koboSpan" id="kobo.1177.1">), can be used in semi-supervised learning. </span><span class="koboSpan" id="kobo.1177.2">These </span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.1178.1">models learn the underlying</span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.1179.1"> data distribution and generate plausible instances. </span><span class="koboSpan" id="kobo.1179.2">By incorporating the generated instances into the training process, the model can capture more diverse representations and improve its </span><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">generalization performance.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1181.1">Let’s see a simple implementation in Python of this labeling approach. </span><span class="koboSpan" id="kobo.1181.2">First, we import the </span><span class="No-Break"><span class="koboSpan" id="kobo.1182.1">necessary libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1183.1">
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.semi_supervised import LabelPropagation</span></pre> <p><span class="koboSpan" id="kobo.1184.1">We utilize </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.1185.1">the preprocessed </span><strong class="source-inline"><span class="koboSpan" id="kobo.1186.1">credit-g</span></strong><span class="koboSpan" id="kobo.1187.1"> dataset from previous examples and split it into labeled and unlabeled subsets. </span><span class="koboSpan" id="kobo.1187.2">This example assumes that you are using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1188.1">df_preprocessed</span></strong><span class="koboSpan" id="kobo.1189.1"> DataFrame we created in the </span><em class="italic"><span class="koboSpan" id="kobo.1190.1">Uncertainty </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1191.1">sampling</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1192.1"> section:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1193.1">
X = df_preprocessed.drop('target', axis=1)
y = df_preprocessed['target']
# Split the dataset into labeled and unlabeled
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
labeled_percentage = 0.1  # Percentage of labeled data
X_train_labeled, X_train_unlabeled, y_train_labeled, _ = train_test_split(
    X_train, y_train, test_size=1 - labeled_percentage, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.1194.1">Then we train a supervised machine learning model using the labeled data. </span><span class="koboSpan" id="kobo.1194.2">In this example, we will use logistic regression as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1195.1">supervised model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1196.1">
supervised_model = LogisticRegression()
supervised_model.fit(X_train_labeled, y_train_labeled)</span></pre> <p><span class="koboSpan" id="kobo.1197.1">We then apply the trained supervised model to predict labels for the unlabeled data. </span><span class="koboSpan" id="kobo.1197.2">The predicted labels are considered as pseudo-labels for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">unlabeled instances:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1199.1">
# Predict labels for the unlabeled data
pseudo_labels = supervised_model.predict(X_train_unlabeled)</span></pre> <p><span class="koboSpan" id="kobo.1200.1">Now </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.1201.1">concatenate the labeled data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1202.1">X_labeled</span></strong><span class="koboSpan" id="kobo.1203.1">) with the pseudo-labeled data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1204.1">X_unlabeled</span></strong><span class="koboSpan" id="kobo.1205.1">) to create the combined feature dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1206.1">X_combined</span></strong><span class="koboSpan" id="kobo.1207.1">). </span><span class="koboSpan" id="kobo.1207.2">Concatenate the corresponding labels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1208.1">y_labeled</span></strong><span class="koboSpan" id="kobo.1209.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1210.1">pseudo_labels</span></strong><span class="koboSpan" id="kobo.1211.1">) to create the combined label </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">dataset (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1213.1">y_combined</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1214.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1215.1">
# Concatenate the labeled data with the pseudo-labeled data
X_combined = np.concatenate((X_labeled, X_unlabeled))
y_combined = np.concatenate((y_labeled, pseudo_labels))</span></pre> <p><span class="koboSpan" id="kobo.1216.1">Next, train a semi-supervised machine learning model using the combined feature dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1217.1">X_combined</span></strong><span class="koboSpan" id="kobo.1218.1">) and label dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1219.1">y_combined</span></strong><span class="koboSpan" id="kobo.1220.1">). </span><span class="koboSpan" id="kobo.1220.2">In this example, we will use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1221.1">LabelPropagation</span></strong><span class="koboSpan" id="kobo.1222.1"> as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1223.1">semi-supervised model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1224.1">
semi_supervised_model = LabelPropagation()
semi_supervised_model.fit(X_combined, y_combined)</span></pre> <p><span class="koboSpan" id="kobo.1225.1">Use the trained semi-supervised model to make predictions on the test set and calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.1226.1">the accuracy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1227.1">
y_pred = semi_supervised_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Test accuracy: {accuracy:.3f}')</span></pre> <p><span class="koboSpan" id="kobo.1228.1">The print statement outputs the resulting accuracy score, which, in this case, </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">is 0.635.</span></span></p>
<p><span class="koboSpan" id="kobo.1230.1">After training our </span><strong class="source-inline"><span class="koboSpan" id="kobo.1231.1">semi_supervised_model</span></strong><span class="koboSpan" id="kobo.1232.1"> using </span><strong class="source-inline"><span class="koboSpan" id="kobo.1233.1">LabelPropagation</span></strong><span class="koboSpan" id="kobo.1234.1">, the resulting model has effectively learned from both labeled and unlabeled data. </span><span class="koboSpan" id="kobo.1234.2">The predictions on the test set (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1235.1">y_pred</span></strong><span class="koboSpan" id="kobo.1236.1">) showcase the model’s ability to generalize and infer labels for previously unseen instances. </span><span class="koboSpan" id="kobo.1236.2">This output serves as a valuable demonstration of how semi-supervised learning techniques, leveraging both labeled and unlabeled data, can contribute to robust and accurate predictions in </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">real-world scenarios.</span></span></p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.1238.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1239.1">In this chapter, we explored various programmatic labeling techniques in machine learning. </span><span class="koboSpan" id="kobo.1239.2">Labeling data is essential for training effective models, and manual labeling can be time-consuming and expensive. </span><span class="koboSpan" id="kobo.1239.3">Programmatic labeling offers automated ways to assign meaningful categories or classes to instances of data. </span><span class="koboSpan" id="kobo.1239.4">We discussed a range of techniques, including pattern matching, DB lookup, Boolean flags, weak supervision, semi-weak supervision, slicing functions, active learning, transfer learning, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1240.1">semi-supervised learning.</span></span></p>
<p><span class="koboSpan" id="kobo.1241.1">Each technique offers unique benefits and considerations based on the nature of the data and the specific labeling requirements. </span><span class="koboSpan" id="kobo.1241.2">By leveraging these techniques, practitioners can streamline the labeling process, reduce manual effort, and train effective models using large amounts of labeled or weakly labeled data. </span><span class="koboSpan" id="kobo.1241.3">Understanding and utilizing programmatic labeling techniques are crucial for building robust and scalable machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1242.1">learning systems.</span></span></p>
<p><span class="koboSpan" id="kobo.1243.1">In the next chapter, we’ll explore the role of synthetic data in data-centric </span><span class="No-Break"><span class="koboSpan" id="kobo.1244.1">machine learning.</span></span></p>
</div>
</body></html>