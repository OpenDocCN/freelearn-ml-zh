- en: '*Chapter 8*: Hyperparameter Tuning via Hyperopt'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hyperopt** is an optimization package in Python that provides several implementations
    of hyperparameter tuning methods, including **Random Search**, **Simulated Annealing**
    (**SA**), **Tree-Structured Parzen Estimators** (**TPE**), and **Adaptive TPE**
    (**ATPE**). It also supports various types of hyperparameters with ranging types
    of sampling distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll introduce the `Hyperopt` package, starting with its capabilities
    and limitations, how to utilize it to perform hyperparameter tuning, and all the
    other important things you need to know about `Hyperopt`. We’ll learn not only
    how to utilize `Hyperopt` to perform hyperparameter tuning with its default configurations
    but also discuss the available configurations, along with their usage. Moreover,
    we’ll discuss how the implementation of the hyperparameter tuning methods is related
    to the theory that we learned about in the previous chapters, since there some
    minor differences or adjustments may have been made in the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to understand all the important
    things you need to know about `Hyperopt` and be able to implement various hyperparameter
    tuning methods available in this package. You’ll also be able to understand each
    of the important parameters of their classes and how they are related to the theory
    that we learned about in the previous chapters. Finally, equipped with the knowledge
    from previous chapters, you will be able to understand what’s happening if there
    are errors or unexpected results, as well as how to set up the method configuration
    so that it matches your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hyperopt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Tree-Structured Parzen Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Adaptive Tree-Structured Parzen Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing simulated annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn how to implement various hyperparameter tuning
    methods with Hyperopt. To ensure that you can reproduce the code examples in this
    chapter, you will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 (version 3.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `pandas` package (version 1.3.4 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `NumPy` package (version 1.21.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Matplotlib` package (version 3.5.0 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `scikit-learn` package (version 1.0.1 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Hyperopt` package (version 0.2.7 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LightGBM` package (version 3.3.2 or above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hyperopt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the implemented optimization methods in the `Hyperopt` package assume
    we are working with a *minimization problem*. If your objective function is categorized
    as a maximization problem, for example, when you are using accuracy as the objective
    function score, you must *add a negative sign to your objective function*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilizing the `Hyperopt` package to perform hyperparameter tuning is very simple.
    The following steps show how to perform any hyperparameter tuning methods provided
    in the `Hyperopt` package. More detailed steps, including the code implementation,
    will be given through various examples in the upcoming sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the objective function to be minimized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (*Optional*) Initiate the `Trials()` object and pass it to the `fmin()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform hyperparameter tuning by calling the `fmin()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on full training data using the best set of hyperparameters
    that have been found from the output of the `fmin()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the final trained model on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The simplest case of the objective function is when we only return the floating
    type of objective function score. However, we can also add other additional information
    to the output of the objective function, for example, the evaluation time or any
    other statistics we want to get for further analysis. When we add additional information
    to the output of the objective function score, `Hyperopt` expects the output of
    the objective function to be in the form of a Python dictionary that has at least
    two mandatory key-value pairs – that is, `status` and `loss`. The former key stores
    the status value of the run, while the latter key stores the objective function
    that we want to minimize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest type of hyperparameter space in Hyperopt is in the form of a Python
    dictionary, where the keys refer to the name of the hyperparameters and the values
    contain the distribution of the hyperparameters to be sampled from. The following
    example shows how we can define a very simple hyperparameter space in `Hyperopt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the values of the `hyperparameter_space` dictionary are the
    distributions that accompany each of the hyperparameters we have in the space.
    `Hyperopt` provides a lot of sampling distributions that we can utilize, such
    as `hp.choice`, `hp.randint`, `hp.uniform`, `hp.loguniform`, `hp.normal`, and
    `hp.lognormal`. The `hp.choice` distribution will randomly choose one option from
    the several given options. The `hp.randint` distribution will randomly choose
    an integer within the range of `[0, high)`, where `high` is the input given by
    us. In the previous example, we passed `195` as the `high` value and added a value
    of `5`. This means `Hyperopt` will randomly choose an integer within the range
    of `[5,200)`.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the distributions are dedicated to real/floating hyperparameter
    values. Note that Hyperopt also provides distributions dedicated to integer hyperparameter
    values that mimic the distribution of those four distributions – that is, `hp.quniform`,
    `hp.qloguniform`, `hp.qnormal`, and `hp.qlognormal`. For more information regarding
    the sampling distributions provided by Hyperopt, please refer to its official
    wiki page ([https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions)).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that Hyperopt enables us to define a **conditional hyperparameter
    space** (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Bayesian Optimization*)
    that suits our needs. The following code example shows how we can define such
    a search space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the only difference between a conditional hyperparameter space
    and a non-conditional one is that we add `hp.choice` before defining the hyperparameters
    for each condition. In this example, when `class_weight` is `None`, we will only
    search for the best `n_estimators` hyperparameters within the range `[5,50)`.
    On the other hand, when `class_weight` is `“balanced”`, the range becomes `[5,200)`.
  prefs: []
  type: TYPE_NORMAL
- en: Once the hyperparameter space is defined, we can start the hyperparameter tuning
    process via the `fmin()` function. The output of this function is the best set
    of hyperparameters that has been found from the tuning process. There are several
    important parameters available in this function that you need to know about. The
    `fn` parameter refers to the objective function we are trying to minimize, the
    `space` parameter refers to the hyperparameter space that will be used in our
    experiment, the `algo` parameter refers to the hyperparameter tuning algorithm
    that we want to utilize, the `rstate` parameter refers to the random seed for
    the tuning process, the `max_evals` parameter refers to the stopping criterion
    of the tuning process based on the number of trials, and the `timeout` parameter
    refers to the stopping criterion based on the time limit in seconds. Another important
    parameter is the `trials` parameter, which expects to receive the `Hyperopt` `Trials()`
    object.
  prefs: []
  type: TYPE_NORMAL
- en: The `Trials()` object in `Hyperopt` logs all the relevant information during
    the tuning process. This object is also responsible for storing all of the additional
    information we put in the dictionary output of the objective function. We can
    utilize this object for debugging purposes or to pass it directly to the built-in
    plotting module in `Hyperopt`.
  prefs: []
  type: TYPE_NORMAL
- en: Several built-in plotting modules are implemented in the `Hyperopt` package,
    such as `main_plot_history`, `main_plot_histogram`, and `main_plot_vars modules`.
    The first plotting module can help us understand the relationship between the
    loss values and the execution time. The second plotting module shows the histogram
    of all of the losses in all trials. The third plotting module is useful for understanding
    more about the heatmap of each hyperparameter in the space relative to the loss
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Last but not least, it is worth noting that Hyperopt also supports parallel
    search processes by utilizing `Trials()` to `MongoTrials()`. We can change from
    `Trials()` to `SparkTrials()` if we want to utilize Spark instead of MongoDB.
    Please refer to the official documentation of Hyperopt for more information about
    parallel computations ([https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB](https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB
    ) and [http://hyperopt.github.io/hyperopt/scaleout/spark/](http://hyperopt.github.io/hyperopt/scaleout/spark/)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you were introduced to the overall capability of the `Hyperopt`
    package, along with the general steps to perform hyperparameter tuning with this
    package. In the next few sections, we will learn how to implement each of the
    hyperparameter tuning methods available in `Hyperopt` through examples.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Random Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To implement Random Search (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031))
    in Hyperopt, we can simply follow the steps explained in the previous section
    and pass the `rand.suggest` object to the `algo` parameter in the `fmin()` function.
    Let’s learn how we can utilize the `Hyperopt` package to perform Random Search.
    We will use the same data and `sklearn` pipeline definition as in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*, but with a slightly different definition of
    the hyperparameter space. Let’s follow the steps that were introduced in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the objective function to be minimized. Here, we are utilizing the defined
    pipeline, `pipe`, to calculate the *5-fold cross-validation* score by utilizing
    the `cross_val_score` function from `sklearn`. We will use the *F1 score* as the
    evaluation metric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the defined `objective` function only receives one input, which is
    the predefined hyperparameter space, `space`, and outputs a dictionary that contains
    two mandatory key-value pairs – that is, `status` and `loss`. It is also worth
    noting that the reason why we multiply the average cross-validation score output
    with `–1` is that `Hyperopt` always assumes that we are working with a minimization
    problem, while we are not in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the hyperparameter space. Since we are using the `sklearn` pipeline
    as our estimator, we still need to follow the naming convention of the hyperparameters
    within the defined space (see [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)).
    Note that the naming convention just needs to be applied to the hyperparameter
    names in the keys of the search space dictionary, not to the names within the
    sampling distribution objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initiate the `Trials()` object. In this example, we will utilize this object
    for plotting purposes after the tuning process has been done:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform hyperparameter tuning by calling the `fmin()` function. Here, we are
    performing a Random Search by passing the defined objective function and hyperparameter
    space. We have set the `algo` parameter with the `rand.suggest` object and set
    the number of trials to `100` as the stopping criterion. We also set the random
    state to ensure reproducibility. Last but not least, we passed the defined `Trials()`
    object to the `trials` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the preceding code, we get around `-0.621` of the objective function
    score, which refers to `0.621` of the average 5-fold cross-validation F--score.
    We also get a dictionary consisting of the best set of hyperparameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As can be seen, `Hyperopt` will only return the index of the hyperparameter
    values when we use `hp.choice` as the sampling distribution (see the `class_weight`
    and `criterion` hyperparameters). Here, by referring to the predefined hyperparameter
    space, `0` for `class_weight` refers to *balanced* and `1` for `criterion` refers
    to *entropy*. Thus, the best set of hyperparameters is `{‘model__class_weight’:
    ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0004701700193524210,
    ‘model__n_estimators’: 186}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model on the full training data using the best set of hyperparameters
    that have been found in the output of the `fmin()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the final trained model on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Based on the preceding code, we get around `0.624` for the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last but not least, we can also utilize the built-in plotting modules implemented
    in `Hyperopt`. The following code shows how to do this. Note that we need to pass
    the `trials` object from the tuning process to the plotting modules since all
    of the tuning process logs are in there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must plot the relationship between the loss values and the execution
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Relationship between the loss values and the execution time'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_08_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Relationship between the loss values and the execution time
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must plot the histogram of all of the objective function scores from
    all the trials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Histogram of all of the objective function scores from all trials'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_08_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Histogram of all of the objective function scores from all trials
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must plot the heatmap of each hyperparameter in the space relative
    to the loss values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Heatmap of each hyperparameter in the space relative to the
    loss values (the darker, the better)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_08_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – Heatmap of each hyperparameter in the space relative to the loss
    values (the darker, the better)
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to perform Random Search in `Hyperopt` by looking
    at an example similar example to the one shown in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*. We also saw what kind of figures we can get
    from utilizing the built-in plotting modules in Hyperopt.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that we are not bounded to using only the `sklearn` implementation
    of models to perform hyperparameter tuning with `Hyperopt`. We can also use implementations
    from other packages, such as `PyTorch`, `Tensorflow`, and so on. One thing that
    needs to be kept in mind is to be careful with the *data leakage issue* (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*)
    when performing cross-validation. We must fit all of the data preprocessing methods
    on the training data and apply the fitted preprocessors to the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to utilize `Hyperopt` to perform hyperparameter
    tuning with one of the available Bayesian Optimization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Tree-structured Parzen Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Hyperopt` package. To perform hyperparameter tuning with this method, we can
    follow a similar procedure as in the previous section by only changing the `algo`
    parameter to `tpe.suggest` in *Step 4*. The following code shows how to perform
    hyperparameter tuning with TPE in `Hyperopt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.620` for the objective function score, which refers
    to `0.620` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.621` in terms of the F1-score when we test the final Random Forest
    model that’s been trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to perform hyperparameter tuning using the TPE
    method with `Hyperopt`. In the next section, we will learn how to implement a
    variant of TPE called Adaptive TPE with the `Hyperopt` package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Adaptive TPE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Adaptive TPE** (**ATPE**) is a variant of the TPE hyperparameter tuning method
    that is developed based on several improvements compared to TPE, such as automatically
    tuning several hyperparameters of the TPE method based on the data that we have.
    For more information about this method, please refer to the original white papers.
    These can be found in the GitHub repository of the author ([https://github.com/electricbrainio/hypermax](https://github.com/electricbrainio/hypermax)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While you can experiment with this method directly using the original GitHub
    repository of ATPE, `Hyperopt` has also included this method as part of the package.
    You can simply follow a similar procedure as in the *Implementing Random Search*
    section by only changing the `algo` parameter to `atpe.suggest` in *Step 4*. The
    following code shows how to perform hyperparameter tuning with ATPE in `Hyperopt`.
    Please note that ATPE utilizes the `LightGBM` package installed before we can
    start to perform hyperparameter tuning with ATPE in `Hyperopt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.621` for the objective function score, which refers
    to `0.621` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.622` in terms of the F1 score when we test the final Random Forest
    model that was trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to perform hyperparameter tuning using the ATPE
    method with `Hyperopt`. In the next section, we will learn how to implement a
    hyperparameter tuning method that is part of the Heuristic Search group with the
    `Hyperopt` package.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing simulated annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Hyperopt` package. Similar to TPE and ATPE, to perform hyperparameter tuning
    with this method, we can simply follow the procedure shown in the *Implementing
    Random Search* section; we only need to change the `algo` parameter to `anneal.suggest`
    in *Step 4*. The following code shows how to perform hyperparameter tuning with
    SA in `Hyperopt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.620` for the objective function score, which refers
    to `0.620` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.625` in terms of the F1-score when we test the final Random Forest
    model that was trained on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'While `Hyperopt` has built-in plotting modules, we can also create a customized
    plotting function by utilizing the `Trials()` object. The following code shows
    how to visualize the distribution of each hyperparameter over the number of trials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the value of each hyperparameter in each of the trials:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the values into a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the relationship between each hyperparameter’s distribution and the number
    of trials:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Based on the preceding code, we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Relationship between each hyperparameter’s distribution and
    the number of trials'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_08_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – Relationship between each hyperparameter’s distribution and the
    number of trials
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to implement SA in `Hyperopt` by using the same
    example as in the *Implementing Random Search* section. We also learned how to
    create a custom plotting function to visualize the relationship between each hyperparameter’s
    distribution and the number of trials.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned all the important things about the `Hyperopt` package,
    including its capabilities and limitations, and how to utilize it to perform hyperparameter
    tuning. We saw that `Hyperopt` supports various types of sampling distribution
    methods but can only work with a minimization problem. We also learned how to
    implement various hyperparameter tuning methods with the help of this package,
    which has helped us understand each of the important parameters of the classes
    and how are they related to the theory that we learned about in the previous chapters.
    At this point, you should be able to utilize `Hyperopt` to implement your chosen
    hyperparameter tuning method and, ultimately, boost the performance of your ML
    model. Equipped with the knowledge from [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031),
    to [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054), you should be able to
    understand what’s happening if there are errors or unexpected results, as well
    as understand how to set up the method configuration so that it matches your specific
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the `Optuna` package and how to utilize
    it to perform various hyperparameter tuning methods. The goal of the next chapter
    is similar to this chapter – that is, being able to utilize the package for hyperparameter
    tuning purposes and understanding each of the parameters of the implemented classes.
  prefs: []
  type: TYPE_NORMAL
