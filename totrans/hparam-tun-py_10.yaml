- en: '*Chapter 8*: Hyperparameter Tuning via Hyperopt'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 8 章*：通过 Hyperopt 进行超参数调整'
- en: '**Hyperopt** is an optimization package in Python that provides several implementations
    of hyperparameter tuning methods, including **Random Search**, **Simulated Annealing**
    (**SA**), **Tree-Structured Parzen Estimators** (**TPE**), and **Adaptive TPE**
    (**ATPE**). It also supports various types of hyperparameters with ranging types
    of sampling distributions.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hyperopt** 是一个 Python 优化包，提供了多种超参数调整方法的实现，包括 **随机搜索**、**模拟退火**（**SA**）、**树结构帕累托估计器**（**TPE**）和
    **自适应 TPE**（**ATPE**）。它还支持各种类型的超参数，以及不同类型的采样分布。'
- en: In this chapter, we’ll introduce the `Hyperopt` package, starting with its capabilities
    and limitations, how to utilize it to perform hyperparameter tuning, and all the
    other important things you need to know about `Hyperopt`. We’ll learn not only
    how to utilize `Hyperopt` to perform hyperparameter tuning with its default configurations
    but also discuss the available configurations, along with their usage. Moreover,
    we’ll discuss how the implementation of the hyperparameter tuning methods is related
    to the theory that we learned about in the previous chapters, since there some
    minor differences or adjustments may have been made in the implementation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍 `Hyperopt` 包，从其功能和限制开始，学习如何利用它进行超参数调整，以及你需要了解的关于 `Hyperopt` 的所有其他重要事项。我们将学习如何利用
    `Hyperopt` 的默认配置进行超参数调整，并讨论可用的配置及其用法。此外，我们还将讨论超参数调整方法的实现与我们在前几章中学到的理论之间的关系，因为实现中可能有一些细微的差异或调整。
- en: By the end of this chapter, you will be able to understand all the important
    things you need to know about `Hyperopt` and be able to implement various hyperparameter
    tuning methods available in this package. You’ll also be able to understand each
    of the important parameters of their classes and how they are related to the theory
    that we learned about in the previous chapters. Finally, equipped with the knowledge
    from previous chapters, you will be able to understand what’s happening if there
    are errors or unexpected results, as well as how to set up the method configuration
    so that it matches your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够了解关于 `Hyperopt` 的所有重要事项，并能够实现该包中提供的各种超参数调整方法。你还将能够理解它们类的重要参数以及它们与我们之前章节中学到的理论之间的关系。最后，凭借前几章的知识，你将能够理解如果出现错误或意外结果时会发生什么，以及如何设置方法配置以匹配你的特定问题。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing Hyperopt
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Hyperopt
- en: Implementing Random Search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现随机搜索
- en: Implementing Tree-Structured Parzen Estimators
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现树结构帕累托估计器
- en: Implementing Adaptive Tree-Structured Parzen Estimators
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自适应树结构帕累托估计器
- en: Implementing simulated annealing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will learn how to implement various hyperparameter tuning
    methods with Hyperopt. To ensure that you can reproduce the code examples in this
    chapter, you will require the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用 Hyperopt 实现各种超参数调整方法。为了确保你能重现本章中的代码示例，你需要以下条件：
- en: Python 3 (version 3.7 or above)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本 3.7 或更高版本）
- en: The `pandas` package (version 1.3.4 or above)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 包（版本 1.3.4 或更高版本）'
- en: The `NumPy` package (version 1.21.2 or above)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumPy` 包（版本 1.21.2 或更高版本）'
- en: The `Matplotlib` package (version 3.5.0 or above)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Matplotlib` 包（版本 3.5.0 或更高版本）'
- en: The `scikit-learn` package (version 1.0.1 or above)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn` 包（版本 1.0.1 或更高版本）'
- en: The `Hyperopt` package (version 0.2.7 or above)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Hyperopt` 包（版本 0.2.7 或更高版本）'
- en: The `LightGBM` package (version 3.3.2 or above)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LightGBM` 包（版本 3.3.2 或更高版本）'
- en: All the code examples for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在 GitHub 上找到，链接为 [https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python](https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python)。
- en: Introducing Hyperopt
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Hyperopt
- en: All of the implemented optimization methods in the `Hyperopt` package assume
    we are working with a *minimization problem*. If your objective function is categorized
    as a maximization problem, for example, when you are using accuracy as the objective
    function score, you must *add a negative sign to your objective function*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperopt`包中实现的全部优化方法都假设我们正在处理一个*最小化问题*。如果你的目标函数被分类为最大化问题，例如，当你使用准确率作为目标函数得分时，你必须*对你的目标函数添加一个负号*。'
- en: 'Utilizing the `Hyperopt` package to perform hyperparameter tuning is very simple.
    The following steps show how to perform any hyperparameter tuning methods provided
    in the `Hyperopt` package. More detailed steps, including the code implementation,
    will be given through various examples in the upcoming sections:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 利用`Hyperopt`包进行超参数调整非常简单。以下步骤展示了如何执行`Hyperopt`包中提供的任何超参数调整方法。更详细的步骤，包括代码实现，将在接下来的章节中通过各种示例给出：
- en: Define the objective function to be minimized.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义要最小化的目标函数。
- en: Define the hyperparameter space.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间。
- en: (*Optional*) Initiate the `Trials()` object and pass it to the `fmin()` function.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (*可选*) 初始化`Trials()`对象并将其传递给`fmin()`函数。
- en: Perform hyperparameter tuning by calling the `fmin()` function.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fmin()`函数进行超参数调整。
- en: Train the model on full training data using the best set of hyperparameters
    that have been found from the output of the `fmin()` function.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用从`fmin()`函数输出中找到的最佳超参数集在全部训练数据上训练模型。
- en: Test the final trained model on the test data.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练好的模型。
- en: The simplest case of the objective function is when we only return the floating
    type of objective function score. However, we can also add other additional information
    to the output of the objective function, for example, the evaluation time or any
    other statistics we want to get for further analysis. When we add additional information
    to the output of the objective function score, `Hyperopt` expects the output of
    the objective function to be in the form of a Python dictionary that has at least
    two mandatory key-value pairs – that is, `status` and `loss`. The former key stores
    the status value of the run, while the latter key stores the objective function
    that we want to minimize.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数的最简单情况是我们只返回目标函数得分的浮点类型。然而，我们也可以将其他附加信息添加到目标函数的输出中，例如评估时间或我们想要用于进一步分析的任何其他统计数据。当我们向目标函数得分的输出添加附加信息时，`Hyperopt`期望目标函数的输出形式为Python字典，该字典至少包含两个强制性的键值对——即`status`和`loss`。前者键存储运行的状态值，而后者键存储我们想要最小化的目标函数。
- en: 'The simplest type of hyperparameter space in Hyperopt is in the form of a Python
    dictionary, where the keys refer to the name of the hyperparameters and the values
    contain the distribution of the hyperparameters to be sampled from. The following
    example shows how we can define a very simple hyperparameter space in `Hyperopt`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Hyperopt中最简单的超参数空间形式是Python字典的形式，其中键指的是超参数的名称，值包含从其中采样的超参数分布。以下示例展示了我们如何在`Hyperopt`中定义一个非常简单的超参数空间：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the values of the `hyperparameter_space` dictionary are the
    distributions that accompany each of the hyperparameters we have in the space.
    `Hyperopt` provides a lot of sampling distributions that we can utilize, such
    as `hp.choice`, `hp.randint`, `hp.uniform`, `hp.loguniform`, `hp.normal`, and
    `hp.lognormal`. The `hp.choice` distribution will randomly choose one option from
    the several given options. The `hp.randint` distribution will randomly choose
    an integer within the range of `[0, high)`, where `high` is the input given by
    us. In the previous example, we passed `195` as the `high` value and added a value
    of `5`. This means `Hyperopt` will randomly choose an integer within the range
    of `[5,200)`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`hyperparameter_space`字典的值是伴随空间中每个超参数的分布。`Hyperopt`提供了许多采样分布，我们可以利用，例如`hp.choice`、`hp.randint`、`hp.uniform`、`hp.loguniform`、`hp.normal`和`hp.lognormal`。`hp.choice`分布将随机从几个给定选项中选择一个。`hp.randint`分布将在`[0,
    high)`范围内随机选择一个整数，其中`high`是我们输入的值。在先前的示例中，我们传递了`195`作为`high`值并添加了`5`的值。这意味着`Hyperopt`将在`[5,200)`范围内随机选择一个整数。
- en: The rest of the distributions are dedicated to real/floating hyperparameter
    values. Note that Hyperopt also provides distributions dedicated to integer hyperparameter
    values that mimic the distribution of those four distributions – that is, `hp.quniform`,
    `hp.qloguniform`, `hp.qnormal`, and `hp.qlognormal`. For more information regarding
    the sampling distributions provided by Hyperopt, please refer to its official
    wiki page ([https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的分布都是针对实数/浮点超参数值的。请注意，Hyperopt还提供了针对整数超参数值的分布，这些分布模仿了上述四个分布的分布情况——即`hp.quniform`、`hp.qloguniform`、`hp.qnormal`和`hp.qlognormal`。有关Hyperopt提供的采样分布的更多信息，请参阅其官方维基页面([https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions](https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions))。
- en: 'It is worth noting that Hyperopt enables us to define a **conditional hyperparameter
    space** (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Bayesian Optimization*)
    that suits our needs. The following code example shows how we can define such
    a search space:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Hyperopt使我们能够定义一个**条件超参数空间**（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，贝叶斯优化*），以满足我们的需求。以下代码示例展示了我们如何定义这样的搜索空间：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the only difference between a conditional hyperparameter space
    and a non-conditional one is that we add `hp.choice` before defining the hyperparameters
    for each condition. In this example, when `class_weight` is `None`, we will only
    search for the best `n_estimators` hyperparameters within the range `[5,50)`.
    On the other hand, when `class_weight` is `“balanced”`, the range becomes `[5,200)`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，条件超参数空间和非条件超参数空间之间的唯一区别是在定义每个条件的超参数之前添加了`hp.choice`。在这个例子中，当`class_weight`为`None`时，我们只会在范围`[5,50)`内搜索最佳的`n_estimators`超参数。另一方面，当`class_weight`为`“balanced”`时，范围变为`[5,200)`。
- en: Once the hyperparameter space is defined, we can start the hyperparameter tuning
    process via the `fmin()` function. The output of this function is the best set
    of hyperparameters that has been found from the tuning process. There are several
    important parameters available in this function that you need to know about. The
    `fn` parameter refers to the objective function we are trying to minimize, the
    `space` parameter refers to the hyperparameter space that will be used in our
    experiment, the `algo` parameter refers to the hyperparameter tuning algorithm
    that we want to utilize, the `rstate` parameter refers to the random seed for
    the tuning process, the `max_evals` parameter refers to the stopping criterion
    of the tuning process based on the number of trials, and the `timeout` parameter
    refers to the stopping criterion based on the time limit in seconds. Another important
    parameter is the `trials` parameter, which expects to receive the `Hyperopt` `Trials()`
    object.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了超参数空间，我们就可以通过`fmin()`函数开始超参数调整过程。该函数的输出是从调整过程中找到的最佳超参数集。此函数中提供了几个重要的参数，你需要了解它们。`fn`参数指的是我们试图最小化的目标函数，`space`参数指的是我们实验中将要使用的超参数空间，`algo`参数指的是我们想要利用的超参数调整算法，`rstate`参数指的是调整过程的随机种子，`max_evals`参数指的是基于试验次数的调整过程停止标准，而`timeout`参数指的是基于秒数时间限制的停止标准。另一个重要的参数是`trials`参数，它期望接收`Hyperopt`的`Trials()`对象。
- en: The `Trials()` object in `Hyperopt` logs all the relevant information during
    the tuning process. This object is also responsible for storing all of the additional
    information we put in the dictionary output of the objective function. We can
    utilize this object for debugging purposes or to pass it directly to the built-in
    plotting module in `Hyperopt`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperopt`中的`Trials()`对象在调整过程中记录所有相关信息。此对象还负责存储我们放入目标函数字典输出中的所有附加信息。我们可以利用此对象进行调试或直接将其传递给`Hyperopt`内置的绘图模块。'
- en: Several built-in plotting modules are implemented in the `Hyperopt` package,
    such as `main_plot_history`, `main_plot_histogram`, and `main_plot_vars modules`.
    The first plotting module can help us understand the relationship between the
    loss values and the execution time. The second plotting module shows the histogram
    of all of the losses in all trials. The third plotting module is useful for understanding
    more about the heatmap of each hyperparameter in the space relative to the loss
    values.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperopt`包中实现了几个内置的绘图模块，例如`main_plot_history`、`main_plot_histogram`和`main_plot_vars`模块。第一个绘图模块可以帮助我们理解损失值与执行时间之间的关系。第二个绘图模块显示了所有试验中所有损失的直方图。第三个绘图模块对于理解每个超参数相对于损失值的热图非常有用。'
- en: Last but not least, it is worth noting that Hyperopt also supports parallel
    search processes by utilizing `Trials()` to `MongoTrials()`. We can change from
    `Trials()` to `SparkTrials()` if we want to utilize Spark instead of MongoDB.
    Please refer to the official documentation of Hyperopt for more information about
    parallel computations ([https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB](https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB
    ) and [http://hyperopt.github.io/hyperopt/scaleout/spark/](http://hyperopt.github.io/hyperopt/scaleout/spark/)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，值得注意的是，Hyperopt还通过利用`Trials()`到`MongoTrials()`支持并行搜索过程。如果我们想使用Spark而不是MongoDB，我们可以从`Trials()`切换到`SparkTrials()`。请参阅Hyperopt的官方文档以获取有关并行计算更多信息（[https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB](https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB)
    和 [http://hyperopt.github.io/hyperopt/scaleout/spark/](http://hyperopt.github.io/hyperopt/scaleout/spark/)）。
- en: In this section, you were introduced to the overall capability of the `Hyperopt`
    package, along with the general steps to perform hyperparameter tuning with this
    package. In the next few sections, we will learn how to implement each of the
    hyperparameter tuning methods available in `Hyperopt` through examples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已了解了`Hyperopt`包的整体功能，以及使用此包进行超参数调优的一般步骤。在接下来的几节中，我们将通过示例学习如何实现`Hyperopt`中可用的每种超参数调优方法。
- en: Implementing Random Search
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现随机搜索
- en: 'To implement Random Search (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031))
    in Hyperopt, we can simply follow the steps explained in the previous section
    and pass the `rand.suggest` object to the `algo` parameter in the `fmin()` function.
    Let’s learn how we can utilize the `Hyperopt` package to perform Random Search.
    We will use the same data and `sklearn` pipeline definition as in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*, but with a slightly different definition of
    the hyperparameter space. Let’s follow the steps that were introduced in the previous
    section:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Hyperopt中实现随机搜索（见[*第3章*](B18753_03_ePub.xhtml#_idTextAnchor031)），我们可以简单地遵循上一节中解释的步骤，并将`rand.suggest`对象传递给`fmin()`函数中的`algo`参数。让我们学习如何利用`Hyperopt`包来执行随机搜索。我们将使用与[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)*，通过Scikit进行超参数调优*相同的相同数据和`sklearn`管道定义，但使用稍有不同的超参数空间定义。让我们遵循上一节中介绍的步骤：
- en: 'Define the objective function to be minimized. Here, we are utilizing the defined
    pipeline, `pipe`, to calculate the *5-fold cross-validation* score by utilizing
    the `cross_val_score` function from `sklearn`. We will use the *F1 score* as the
    evaluation metric:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义要最小化的目标函数。在这里，我们利用定义的管道`pipe`，通过使用`sklearn`中的`cross_val_score`函数来计算*5折交叉验证*分数。我们将使用*F1分数*作为评估指标：
- en: '[PRE16]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the defined `objective` function only receives one input, which is
    the predefined hyperparameter space, `space`, and outputs a dictionary that contains
    two mandatory key-value pairs – that is, `status` and `loss`. It is also worth
    noting that the reason why we multiply the average cross-validation score output
    with `–1` is that `Hyperopt` always assumes that we are working with a minimization
    problem, while we are not in this example.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，定义的`objective`函数只接收一个输入，即预定义的超参数空间`space`，并输出一个包含两个强制性的键值对——即`status`和`loss`。还值得注意的是，我们之所以将平均交叉验证分数输出乘以`-1`，是因为`Hyperopt`始终假设我们正在处理一个最小化问题，而在这个例子中我们并非如此。
- en: 'Define the hyperparameter space. Since we are using the `sklearn` pipeline
    as our estimator, we still need to follow the naming convention of the hyperparameters
    within the defined space (see [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)).
    Note that the naming convention just needs to be applied to the hyperparameter
    names in the keys of the search space dictionary, not to the names within the
    sampling distribution objects:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间。由于我们使用`sklearn`管道作为我们的估计器，我们仍然需要遵循定义空间内超参数的命名约定（参见[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)）。请注意，命名约定只需应用于搜索空间字典键中的超参数名称，而不是采样分布对象内的名称：
- en: '[PRE17]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Initiate the `Trials()` object. In this example, we will utilize this object
    for plotting purposes after the tuning process has been done:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`Trials()`对象。在这个例子中，我们将在调整过程完成后利用此对象进行绘图：
- en: '[PRE18]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Perform hyperparameter tuning by calling the `fmin()` function. Here, we are
    performing a Random Search by passing the defined objective function and hyperparameter
    space. We have set the `algo` parameter with the `rand.suggest` object and set
    the number of trials to `100` as the stopping criterion. We also set the random
    state to ensure reproducibility. Last but not least, we passed the defined `Trials()`
    object to the `trials` parameter:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fmin()`函数进行超参数调整。在这里，我们通过传递定义的目标函数和超参数空间进行随机搜索。我们将`algo`参数设置为`rand.suggest`对象，并将试验次数设置为`100`作为停止标准。我们还设置了随机状态以确保可重复性。最后但同样重要的是，我们将定义的`Trials()`对象传递给`trials`参数：
- en: '[PRE19]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Based on the preceding code, we get around `-0.621` of the objective function
    score, which refers to `0.621` of the average 5-fold cross-validation F--score.
    We also get a dictionary consisting of the best set of hyperparameters, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，我们得到目标函数分数大约为`-0.621`，这指的是平均5折交叉验证F--分数的`0.621`。我们还得到一个包含最佳超参数集的字典，如下所示：
- en: '[PRE20]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As can be seen, `Hyperopt` will only return the index of the hyperparameter
    values when we use `hp.choice` as the sampling distribution (see the `class_weight`
    and `criterion` hyperparameters). Here, by referring to the predefined hyperparameter
    space, `0` for `class_weight` refers to *balanced* and `1` for `criterion` refers
    to *entropy*. Thus, the best set of hyperparameters is `{‘model__class_weight’:
    ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0004701700193524210,
    ‘model__n_estimators’: 186}`.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '如所示，当我们使用`hp.choice`作为采样分布时，`Hyperopt`将仅返回超参数值的索引。在这里，通过参考预定义的超参数空间，`class_weight`的`0`表示*平衡*，而`criterion`的`1`表示*熵*。因此，最佳超参数集是`{‘model__class_weight’:
    ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0004701700193524210,
    ‘model__n_estimators’: 186}`。'
- en: 'Train the model on the full training data using the best set of hyperparameters
    that have been found in the output of the `fmin()` function:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fmin()`函数输出中找到的最佳超参数集在全部训练数据上训练模型：
- en: '[PRE21]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Test the final trained model on the test data:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上测试最终训练的模型：
- en: '[PRE22]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Based on the preceding code, we get around `0.624` for the F1-score when testing
    our final trained Random Forest model with the best set of hyperparameters on
    the test set.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的代码，当我们在测试集上使用最佳超参数集测试最终训练的随机森林模型时，我们得到大约`0.624`的F1分数。
- en: 'Last but not least, we can also utilize the built-in plotting modules implemented
    in `Hyperopt`. The following code shows how to do this. Note that we need to pass
    the `trials` object from the tuning process to the plotting modules since all
    of the tuning process logs are in there:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们还可以利用`Hyperopt`中实现的内置绘图模块。以下代码展示了如何进行这一操作。请注意，我们需要将调整过程中的`trials`对象传递给绘图模块，因为所有调整过程日志都存储在其中：
- en: '[PRE23]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we must plot the relationship between the loss values and the execution
    time:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须绘制损失值与执行时间的关系：
- en: '[PRE24]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will get the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '![Figure 8.1 – Relationship between the loss values and the execution time'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.1 – 损失值与执行时间的关系'
- en: '](img/B18753_08_001.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18753_08_001.jpg](img/B18753_08_001.jpg)'
- en: Figure 8.1 – Relationship between the loss values and the execution time
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 损失值与执行时间的关系
- en: 'Now, we must plot the histogram of all of the objective function scores from
    all the trials:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须绘制所有试验的目标函数分数的直方图：
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will get the following output.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出。
- en: '![Figure 8.2 – Histogram of all of the objective function scores from all trials'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.2 – 所有试验的目标函数分数直方图'
- en: '](img/B18753_08_002.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 B18753_08_002.jpg](img/B18753_08_002.jpg)'
- en: Figure 8.2 – Histogram of all of the objective function scores from all trials
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 所有试验中目标函数分数的直方图
- en: 'Now, we must plot the heatmap of each hyperparameter in the space relative
    to the loss values:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须绘制空间中每个超参数相对于损失值的热图：
- en: '[PRE26]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We will get the following output.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出。
- en: '![Figure 8.3 – Heatmap of each hyperparameter in the space relative to the
    loss values (the darker, the better)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.3 – 空间中每个超参数相对于损失值的热图（越暗，越好）'
- en: '](img/B18753_08_003.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_08_003.jpg]'
- en: Figure 8.3 – Heatmap of each hyperparameter in the space relative to the loss
    values (the darker, the better)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 空间中每个超参数相对于损失值的热图（越暗，越好）
- en: In this section, we learned how to perform Random Search in `Hyperopt` by looking
    at an example similar example to the one shown in [*Chapter 7*](B18753_07_ePub.xhtml#_idTextAnchor062)*,
    Hyperparameter Tuning via Scikit*. We also saw what kind of figures we can get
    from utilizing the built-in plotting modules in Hyperopt.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过查看与[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)中展示的类似示例相同的示例，学习了如何在`Hyperopt`中执行随机搜索。我们还看到了通过利用Hyperopt内置的绘图模块，我们可以得到什么样的图形。
- en: It is worth noting that we are not bounded to using only the `sklearn` implementation
    of models to perform hyperparameter tuning with `Hyperopt`. We can also use implementations
    from other packages, such as `PyTorch`, `Tensorflow`, and so on. One thing that
    needs to be kept in mind is to be careful with the *data leakage issue* (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*)
    when performing cross-validation. We must fit all of the data preprocessing methods
    on the training data and apply the fitted preprocessors to the validation data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们不仅限于使用`sklearn`模型的实现来使用`Hyperopt`进行超参数调整。我们还可以使用来自其他包的实现，例如`PyTorch`、`Tensorflow`等。需要记住的一点是在进行交叉验证时要注意*数据泄露问题*（参见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)，“评估机器学习模型”）。我们必须将所有数据预处理方法拟合到训练数据上，并将拟合的预处理程序应用于验证数据。
- en: In the next section, we will learn how to utilize `Hyperopt` to perform hyperparameter
    tuning with one of the available Bayesian Optimization methods.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何利用`Hyperopt`通过可用的贝叶斯优化方法之一进行超参数调整。
- en: Implementing Tree-structured Parzen Estimators
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现树结构帕累托估计器
- en: '`Hyperopt` package. To perform hyperparameter tuning with this method, we can
    follow a similar procedure as in the previous section by only changing the `algo`
    parameter to `tpe.suggest` in *Step 4*. The following code shows how to perform
    hyperparameter tuning with TPE in `Hyperopt`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperopt`包。要使用此方法进行超参数调整，我们可以遵循与上一节类似的程序，只需将*步骤4*中的`algo`参数更改为`tpe.suggest`。以下代码显示了如何在`Hyperopt`中使用TPE进行超参数调整：'
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.620` for the objective function score, which refers
    to `0.620` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的数据、超参数空间和`fmin()`函数的参数，我们得到了大约`-0.620`的目标函数分数，这相当于平均5折交叉验证F1分数的`0.620`。我们还得到了一个包含最佳超参数集的字典，如下所示：
- en: '[PRE36]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.621` in terms of the F1-score when we test the final Random Forest
    model that’s been trained on the test data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用最佳超参数集在全部数据上训练了模型，我们在测试数据上测试训练好的最终随机森林模型时，F1分数大约为`0.621`。
- en: In this section, we learned how to perform hyperparameter tuning using the TPE
    method with `Hyperopt`. In the next section, we will learn how to implement a
    variant of TPE called Adaptive TPE with the `Hyperopt` package.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用`Hyperopt`中的TPE方法进行超参数调整。在下一节中，我们将学习如何使用`Hyperopt`包实现TPE的一个变体，称为自适应TPE。
- en: Implementing Adaptive TPE
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自适应TPE
- en: '**Adaptive TPE** (**ATPE**) is a variant of the TPE hyperparameter tuning method
    that is developed based on several improvements compared to TPE, such as automatically
    tuning several hyperparameters of the TPE method based on the data that we have.
    For more information about this method, please refer to the original white papers.
    These can be found in the GitHub repository of the author ([https://github.com/electricbrainio/hypermax](https://github.com/electricbrainio/hypermax)).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应TPE**（**ATPE**）是TPE超参数调优方法的变体，它基于与TPE相比的几个改进而开发，例如根据我们拥有的数据自动调整TPE方法的几个超参数。有关此方法的更多信息，请参阅原始白皮书。这些可以在作者的GitHub仓库中找到（[https://github.com/electricbrainio/hypermax](https://github.com/electricbrainio/hypermax)）。'
- en: 'While you can experiment with this method directly using the original GitHub
    repository of ATPE, `Hyperopt` has also included this method as part of the package.
    You can simply follow a similar procedure as in the *Implementing Random Search*
    section by only changing the `algo` parameter to `atpe.suggest` in *Step 4*. The
    following code shows how to perform hyperparameter tuning with ATPE in `Hyperopt`.
    Please note that ATPE utilizes the `LightGBM` package installed before we can
    start to perform hyperparameter tuning with ATPE in `Hyperopt`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以直接使用ATPE的原始GitHub仓库来实验这种方法，但`Hyperopt`也已将其作为包的一部分包含在内。您只需遵循*实现随机搜索*部分中的类似程序，只需在*步骤4*中将`algo`参数更改为`atpe.suggest`即可。以下代码展示了如何在`Hyperopt`中使用ATPE进行超参数调优。请注意，在`Hyperopt`中使用ATPE进行超参数调优之前，我们需要安装`LightGBM`包：
- en: '[PRE37]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.621` for the objective function score, which refers
    to `0.621` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的`fmin()`函数数据、超参数空间和参数，我们得到目标函数得分为约`-0.621`，这相当于平均5折交叉验证F1分数的`0.621`。我们还得到一个包含最佳超参数集的字典，如下所示：
- en: '[PRE46]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.622` in terms of the F1 score when we test the final Random Forest
    model that was trained on the test data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用最佳超参数集在全部数据上训练了模型，我们在测试数据上测试最终训练的随机森林模型时，F1分数大约为`0.622`。
- en: In this section, we learned how to perform hyperparameter tuning using the ATPE
    method with `Hyperopt`. In the next section, we will learn how to implement a
    hyperparameter tuning method that is part of the Heuristic Search group with the
    `Hyperopt` package.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用`Hyperopt`中的ATPE方法进行超参数调优。在下一节中，我们将学习如何使用`Hyperopt`包实现属于启发式搜索组的超参数调优方法。
- en: Implementing simulated annealing
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现模拟退火
- en: '`Hyperopt` package. Similar to TPE and ATPE, to perform hyperparameter tuning
    with this method, we can simply follow the procedure shown in the *Implementing
    Random Search* section; we only need to change the `algo` parameter to `anneal.suggest`
    in *Step 4*. The following code shows how to perform hyperparameter tuning with
    SA in `Hyperopt`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hyperopt`包。类似于TPE和ATPE，要使用此方法进行超参数调优，我们只需遵循*实现随机搜索*部分中显示的程序；我们只需要在*步骤4*中将`algo`参数更改为`anneal.suggest`。以下代码展示了如何在`Hyperopt`中使用SA进行超参数调优：'
- en: '[PRE47]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Using the same data, hyperparameter space, and parameters for the `fmin()`
    function, we get around `-0.620` for the objective function score, which refers
    to `0.620` of the average 5-fold cross-validation F1-score. We also get a dictionary
    consisting of the best set of hyperparameters, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的`fmin()`函数数据、超参数空间和参数，我们得到目标函数得分为约`-0.620`，这相当于平均5折交叉验证F1分数的`0.620`。我们还得到一个包含最佳超参数集的字典，如下所示：
- en: '[PRE56]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Once the model has been trained on the full data using the best set of hyperparameters,
    we get around `0.625` in terms of the F1-score when we test the final Random Forest
    model that was trained on the test data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用最佳超参数集在全部数据上训练了模型，我们在测试数据上测试最终训练的随机森林模型时，F1分数大约为`0.625`。
- en: 'While `Hyperopt` has built-in plotting modules, we can also create a customized
    plotting function by utilizing the `Trials()` object. The following code shows
    how to visualize the distribution of each hyperparameter over the number of trials:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`Hyperopt`具有内置的绘图模块，但我们也可以通过利用`Trials()`对象来创建自定义的绘图函数。以下代码展示了如何可视化每个超参数在试验次数中的分布：
- en: 'Get the value of each hyperparameter in each of the trials:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个试验中每个超参数的值：
- en: '[PRE57]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Convert the values into a pandas DataFrame:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将值转换为pandas DataFrame：
- en: '[PRE58]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Plot the relationship between each hyperparameter’s distribution and the number
    of trials:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个超参数分布与试验次数之间的关系图：
- en: '[PRE59]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Based on the preceding code, we will get the following output:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面的代码，我们将得到以下输出：
- en: '![Figure 8.4 – Relationship between each hyperparameter’s distribution and
    the number of trials'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – 每个超参数分布与试验次数之间的关系'
- en: '](img/B18753_08_004.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_08_004.jpg)'
- en: Figure 8.4 – Relationship between each hyperparameter’s distribution and the
    number of trials
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 每个超参数分布与试验次数之间的关系
- en: In this section, we learned how to implement SA in `Hyperopt` by using the same
    example as in the *Implementing Random Search* section. We also learned how to
    create a custom plotting function to visualize the relationship between each hyperparameter’s
    distribution and the number of trials.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过使用与“实现随机搜索”部分相同的示例来在`Hyperopt`中实现模拟退火（SA）。我们还学习了如何创建一个自定义绘图函数来可视化每个超参数分布与试验次数之间的关系。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned all the important things about the `Hyperopt` package,
    including its capabilities and limitations, and how to utilize it to perform hyperparameter
    tuning. We saw that `Hyperopt` supports various types of sampling distribution
    methods but can only work with a minimization problem. We also learned how to
    implement various hyperparameter tuning methods with the help of this package,
    which has helped us understand each of the important parameters of the classes
    and how are they related to the theory that we learned about in the previous chapters.
    At this point, you should be able to utilize `Hyperopt` to implement your chosen
    hyperparameter tuning method and, ultimately, boost the performance of your ML
    model. Equipped with the knowledge from [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031),
    to [*Chapter 6*](B18753_06_ePub.xhtml#_idTextAnchor054), you should be able to
    understand what’s happening if there are errors or unexpected results, as well
    as understand how to set up the method configuration so that it matches your specific
    problem.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关于`Hyperopt`包的所有重要内容，包括其功能和限制，以及如何利用它来进行超参数调整。我们了解到`Hyperopt`支持各种类型的采样分布方法，但只能与最小化问题一起工作。我们还学习了如何借助这个包实现各种超参数调整方法，这有助于我们理解每个类的重要参数以及它们与我们在前几章中学到的理论之间的关系。此时，你应该能够利用`Hyperopt`来实现你选择的超参数调整方法，并最终提高你的机器学习（ML）模型的性能。凭借从[第3章](B18753_03_ePub.xhtml#_idTextAnchor031)到[第6章](B18753_06_ePub.xhtml#_idTextAnchor054)的知识，你应该能够理解如果出现错误或意外结果时会发生什么，以及如何设置方法配置以匹配你的具体问题。
- en: In the next chapter, we will learn about the `Optuna` package and how to utilize
    it to perform various hyperparameter tuning methods. The goal of the next chapter
    is similar to this chapter – that is, being able to utilize the package for hyperparameter
    tuning purposes and understanding each of the parameters of the implemented classes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于`Optuna`包以及如何利用它来执行各种超参数调整方法。下一章的目标与本章节类似——即能够利用该包进行超参数调整，并理解实现类中的每个参数。
