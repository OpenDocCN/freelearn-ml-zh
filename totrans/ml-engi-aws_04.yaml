- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Serverless Data Management on AWS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 上的无服务器数据管理
- en: Businesses generally utilize systems that collect and store user information,
    along with transaction data, inside databases. One good example of this would
    be an e-commerce startup that has a web application where customers can create
    an account and use their credit card to make online purchases. The user profiles,
    transaction data, and purchase history stored in several production databases
    can be used to build a **product recommendation engine**, which can help suggest
    products that customers would probably want to purchase as well. However, before
    this stored data is analyzed and used to train **machine learning** (**ML**) models,
    it must be merged and joined into a **centralized data store** so that it can
    be transformed and processed using a variety of tools and services. Several options
    are frequently used for these types of use cases, but we will focus on two of
    these in this chapter – **data warehouses** and **data lakes**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常使用收集和存储用户信息以及交易数据的系统。一个很好的例子是一个电子商务初创公司，它有一个网站应用程序，客户可以创建账户并使用信用卡进行在线购买。存储在几个生产数据库中的用户资料、交易数据和购买历史可以用来构建一个
    **产品推荐引擎**，这可以帮助建议客户可能想要购买的产品。然而，在分析并使用这些存储数据来训练 **机器学习**（**ML**）模型之前，必须将其合并并连接到一个
    **集中式数据存储** 中，以便可以使用各种工具和服务进行转换和处理。对于这些类型的用例，经常使用几种选项，但我们将在本章中关注其中的两个——**数据仓库**
    和 **数据湖**。
- en: Data warehouses and data lakes play a crucial role when it comes to **data storage**
    and **data management**. When generating reports, companies without a data warehouse
    or a data lake may end up performing queries in the production database of a running
    application directly. This approach is not recommended since it could cause service
    degradation or even unplanned downtime for the application connected to the database.
    This will inevitably affect the sales numbers since the customers would not be
    able to use the e-commerce application to purchase products online. Data warehouses
    and data lakes help us handle and analyze large amounts of data that could come
    from multiple smaller databases connected to running applications. If you have
    experience setting up a data warehouse or a data lake, then you probably know
    that it takes skill, experience, and patience to manage the overall cost, stability,
    and performance of these types of environments. It is a good thing that *serverless*
    services have started to become available to help us with these types of requirements.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库和数据湖在 **数据存储** 和 **数据管理** 方面发挥着至关重要的作用。当生成报告时，没有数据仓库或数据湖的公司可能会直接在运行中的应用程序的数据库中执行查询。这种方法并不可取，因为它可能会降低应用程序的运行性能，甚至导致数据库连接的应用程序出现计划外的停机。这不可避免地会影响销售额，因为客户将无法使用电子商务应用程序在线购买产品。数据仓库和数据湖帮助我们处理和分析来自多个连接到运行应用程序的较小数据库的大量数据。如果您有设置数据仓库或数据湖的经验，那么您可能知道，管理这些类型环境的整体成本、稳定性和性能需要技能、经验和耐心。幸运的是，*无服务器*
    服务已经开始提供，帮助我们满足这些类型的需求。
- en: In this chapter, we will focus on data management and use a variety of *serverless*
    services to manage and query our data. We will start by preparing a few prerequisites,
    including a new IAM user, a VPC, and an S3 bucket where the sample dataset will
    be stored. Once the prerequisites are ready, we will set up and configure a serverless
    data warehouse using **Redshift Serverless**. After that, we will use **AWS Lake
    Formation**, **AWS Glue**, and **Amazon Athena** to prepare a serverless data
    lake.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注数据管理，并使用各种 *无服务器* 服务来管理和查询我们的数据。我们将首先准备一些先决条件，包括一个新的 IAM 用户、一个 VPC
    以及一个用于存储样本数据集的 S3 桶。一旦先决条件准备就绪，我们将设置并配置一个使用 **Redshift Serverless** 的无服务器数据仓库。之后，我们将使用
    **AWS Lake Formation**、**AWS Glue** 和 **Amazon Athena** 准备一个无服务器数据湖。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Getting started with serverless data management
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用无服务器数据管理
- en: Preparing the essential prerequisites
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备基本先决条件
- en: Running analytics at scale with Amazon Redshift Serverless
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon Redshift Serverless 进行大规模数据分析
- en: Setting up Lake Formation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Lake Formation
- en: Using Amazon Athena to query data in Amazon S3
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Amazon Athena 查询 Amazon S3 中的数据
- en: At this point, you are probably wondering what these services are and how these
    services are used. Before proceeding, let’s first have a quick discussion on how
    serverless data management works!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道这些服务是什么以及如何使用这些服务。在继续之前，让我们首先简要讨论一下无服务器数据管理是如何工作的！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before we start, we must have the following ready:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们必须准备好以下内容：
- en: A web browser (preferably Chrome or Firefox)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个网络浏览器（最好是Chrome或Firefox）
- en: Access to the AWS account that was used in the first few chapters of this book
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问本书前几章中使用的AWS账户
- en: 'The Jupyter notebooks, source code, and other files for each chapter are available
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每章的Jupyter笔记本、源代码和其他文件都可在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS)。
- en: Getting started with serverless data management
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用无服务器数据管理
- en: 'Years ago, developers, data scientists, and ML engineers had to spend hours
    or even days setting up the infrastructure needed for data management and data
    engineering. If a large dataset stored in S3 needed to be analyzed, a team of
    data scientists and ML engineers performed the following sequence of steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，开发人员、数据科学家和机器学习工程师不得不花费数小时甚至数天来设置数据管理和数据工程所需的基础设施。如果需要分析存储在S3中的大量数据集，一组数据科学家和机器学习工程师会执行以下一系列步骤：
- en: Launch and configure a cluster of EC2 instances.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动并配置一个EC2实例集群。
- en: Copy the data from S3 to the volumes attached to the EC2 instances.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据从S3复制到附加到EC2实例的卷中。
- en: Perform queries on the data using one or more of the applications installed
    in the EC2 instances.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用安装在EC2实例中的应用程序之一或多个对数据进行查询。
- en: One of the known challenges with this approach is that the provisioned resources
    may end up being underutilized. If the schedule of the data query operations is
    unpredictable, it would be tricky to manage the uptime, cost, and compute specifications
    of the setup as well. In addition to these, system administrators and DevOps engineers
    need to spend time managing the security, stability, performance, and configuration
    of the applications installed in the cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个已知挑战是，配置的资源可能会被低效使用。如果数据查询操作的调度不可预测，那么管理设置的正常运行时间、成本和计算规格将会变得很棘手。除此之外，系统管理员和DevOps工程师还需要花费时间来管理集群中安装的应用程序的安全性、稳定性、性能和配置。
- en: 'Nowadays, it is much more practical to utilize **serverless** and managed services
    with these types of scenarios and use cases. As shown in the following diagram,
    we will have more time to focus on what we need to do since we no longer need
    to worry about server and infrastructure management when using serverless services:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，利用**无服务器**和托管服务来处理这些类型的场景和用例要实际得多。如图所示，由于我们不再需要担心服务器和基础设施管理，因此我们将有更多时间专注于我们需要做的事情：
- en: '![Figure 4.1 – Serverless versus not serverless ](img/B18638_04_001.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 无服务器与有服务器](img/B18638_04_001.jpg)'
- en: Figure 4.1 – Serverless versus not serverless
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 无服务器与有服务器
- en: 'What do we mean by *actual work*? Here’s a quick list of what data analysts,
    data scientists, and data engineers need to work on outside of server management:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的“实际工作”是什么意思？以下是一个快速列表，列出了数据分析师、数据科学家和数据工程师在服务器管理之外需要处理的工作：
- en: Generate charts and reports.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成图表和报告。
- en: Analyze trends and patterns.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析趋势和模式。
- en: Detect and resolve data integrity issues.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测并解决数据完整性问题。
- en: Integrate data stores with business intelligence tools.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据存储与商业智能工具集成。
- en: Make recommendations to management.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向管理层提供建议。
- en: Use the data to train an ML model.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据来训练机器学习模型。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using serverless services, we also only pay for what we use. This means
    that we will not pay for the idle time the compute resources are not running.
    If we were to have both staging and production environments set up, we can be
    assured that the staging environment would only cost a fraction of the production
    environment setup since the resources in the production environment would have
    a higher expected utilization rate.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用无服务器服务时，我们只为我们使用的部分付费。这意味着我们不会为计算资源未运行时的空闲时间付费。如果我们同时设置了预生产和生产环境，我们可以确信预生产环境只会是生产环境设置成本的一小部分，因为生产环境中的资源预期利用率会更高。
- en: 'We can utilize different AWS services when dealing with the following *serverless*
    data management and data processing requirements:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理以下*无服务器*数据管理和数据处理需求时，我们可以利用不同的 AWS 服务：
- en: '**Serverless Data Warehouse**: Amazon Redshift Serverless'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器数据仓库**: Amazon Redshift 无服务器'
- en: '**Serverless Data Lake**: AWS Lake Formation, AWS Glue, and Amazon Athena'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器数据湖**: AWS Lake Formation、AWS Glue 和 Amazon Athena'
- en: '**Serverless Stream Processing**: Amazon Kinesis, AWS Lambda, and DynamoDB'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器流处理**: Amazon Kinesis、AWS Lambda 和 DynamoDB'
- en: '**Serverless Distributed Data Processing**: Amazon EMR Serverless'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器分布式数据处理**: Amazon EMR 无服务器'
- en: Note that this is just the tip of the iceberg and that there are more serverless
    services we can use for our needs. In this chapter, we will focus on setting up
    and querying a **serverless data warehouse** and a **serverless data lake**. Before
    we proceed with these, first, let’s prepare the prerequisites.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这仅仅是冰山一角，我们还可以使用更多无服务器服务来满足我们的需求。在本章中，我们将重点关注设置和查询**无服务器数据仓库**和**无服务器数据湖**。在我们继续进行这些操作之前，首先，让我们准备必要的先决条件。
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At this point, you might be wondering when to use a data lake and when to use
    a data warehouse. A data warehouse is best used when the data being queried and
    processed is relational and defined in advance. The data quality of what is stored
    in a data warehouse is expected to be high as well. That said, a data warehouse
    is used as the “source of truth” of data and is generally used for use cases involving
    batch reporting and business intelligence. On the other hand, a data lake is best
    used when the data being queried and processed involves both relational and non-relational
    data from different data sources. Data stored in data lakes may include both raw
    and clean data. In addition to this, data is stored in a data lake without you
    having to worry about the data structure and schema during data capture. Finally,
    data lakes can be used for use cases involving ML, predictive analytics, and **exploratory
    data analysis** (**EDA**). Since data lakes and data warehouses serve different
    purposes, some organizations utilize both options for their data management needs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您可能想知道何时使用数据湖，何时使用数据仓库。当查询和处理的数据是关系型且预先定义时，数据仓库是最佳选择。存储在数据仓库中的数据质量预期也较高。话虽如此，数据仓库用作数据的“事实来源”，通常用于涉及批量报告和商业智能的场景。另一方面，当查询和处理的数据涉及来自不同数据源的关系型和非关系型数据时，数据湖是最佳选择。存储在数据湖中的数据可能包括原始数据和清洗数据。此外，数据在数据湖中存储时，您无需在数据捕获期间担心数据结构和模式。最后，数据湖可用于涉及机器学习、预测分析和**探索性数据分析**（**EDA**）的场景。由于数据湖和数据仓库服务于不同的目的，一些组织利用这两种选项来满足他们的数据管理需求。
- en: Preparing the essential prerequisites
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备必要的先决条件
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with setting up our data warehouse and data lake in this chapter:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在设置本章中的数据仓库和数据湖之前，确保以下先决条件已准备就绪：
- en: A text editor (for example, VS Code) on your local machine
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的本地机器上有一个文本编辑器（例如，VS Code）
- en: An IAM user with the permissions to create and manage the resources we will
    use in this chapter
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有创建和管理本章中我们将使用的资源的权限的 IAM 用户
- en: A VPC where we will launch the Redshift Serverless endpoint
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个我们将启动 Redshift 无服务器端点的 VPC
- en: A new S3 bucket where our data will be uploaded using AWS CloudShell
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新的 S3 存储桶，我们将使用 AWS CloudShell 将数据上传到该存储桶
- en: In this chapter, we will create and manage our resources in the `us-west-2`)
    region. Make sure that you have set the correct region before proceeding with
    the next steps.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将创建和管理位于 `us-west-2` 区域的资源。在继续下一步之前，请确保您已设置正确的区域。
- en: Opening a text editor on your local machine
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的本地机器上打开文本编辑器
- en: 'Make sure you have an open text editor (for example, **VS Code**) on your local
    machine. We will copy some string values into the text editor for later use in
    this chapter. Here are the values we will have to copy later in this chapter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您在本地机器上有一个打开的文本编辑器（例如，**VS Code**）。我们将在此章中复制一些字符串值以供后续使用。以下是本章中我们将要复制的值：
- en: IAM sign-in link, username, and password (*Preparing the essential prerequisites*
    > *Creating an IAM user*)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IAM 登录链接、用户名和密码（*准备必要的先决条件* > *创建 IAM 用户*）
- en: VPC ID (*Preparing the essential prerequisites* > *Creating a new VPC*)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VPC ID（*准备必要的先决条件* > *创建新的 VPC*）
- en: Name of the created IAM role currently set as the default role (*Running analytics
    at scale with Amazon Redshift Serverless* > *Setting up a Redshift Serverless
    endpoint*)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建的 IAM 角色名称目前设置为默认角色 (*使用 Amazon Redshift Serverless 进行大规模分析* > *设置 Redshift
    Serverless 端点*)
- en: AWS Account ID (*Running analytics at scale with Amazon Redshift Serverless*
    > *Unloading data to S3*)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 账户 ID (*使用 Amazon Redshift Serverless 进行大规模分析* > *将数据卸载到 S3*)
- en: If you do not have VS Code installed, you can use **TextEdit**, **Notepad**,
    **Notepad++**, or **GEdit**, depending on what you have installed on your local
    machine.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有安装 VS Code，您可以使用 **TextEdit**、**记事本**、**Notepad++** 或 **GEdit**，具体取决于您在本地计算机上安装了哪些。
- en: Creating an IAM user
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 IAM 用户
- en: It is important to note that we may encounter issues when running queries in
    **Redshift Serverless** if we were to use the root account directly. That said,
    we will be creating an IAM user in this section. This IAM user will be configured
    to have the appropriate set of permissions needed to perform all the hands-on
    solutions in this chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：如果我们直接使用根账户运行查询，我们可能会在 **Redshift Serverless** 中遇到问题。话虽如此，我们将在本节中创建一个 IAM
    用户。此 IAM 用户将配置为具有执行本章中所有动手实践所需的适当权限集。
- en: Note
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: Make sure that you use the root account when creating a new IAM user.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在创建新的 IAM 用户时使用根账户。
- en: 'Follow these steps to create an IAM user from the IAM console:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤从 IAM 控制台创建 IAM 用户：
- en: 'Navigate to the IAM console using the search bar, as shown in the following
    screenshot:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下截图所示的方式，使用搜索栏导航到 IAM 控制台：
- en: '![Figure 4.2 – Navigating to the IAM console ](img/B18638_04_002.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 导航到 IAM 控制台](img/B18638_04_002.jpg)'
- en: Figure 4.2 – Navigating to the IAM console
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 导航到 IAM 控制台
- en: After typing `iam` in the search bar, we must select the **IAM** service from
    the list of search results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索栏中键入 `iam` 后，我们必须从搜索结果列表中选择 **IAM** 服务。
- en: Locate **Access management** in the sidebar and then click **Users** to navigate
    to the **Users** list page.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在侧边栏中找到 **访问管理**，然后单击 **用户** 以导航到 **用户** 列表页面。
- en: At the top right-hand corner of the screen, locate and click the **Add users**
    button.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕右上角，找到并单击 **添加用户** 按钮。
- en: 'On the **Set user details** page, add a new user using a configuration similar
    to what we have in the following screenshot:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **设置用户详情** 页面上，使用以下截图所示的类似配置添加新用户：
- en: '![Figure 4.3 – Creating a new IAM user ](img/B18638_04_003.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 创建新的 IAM 用户](img/B18638_04_003.jpg)'
- en: Figure 4.3 – Creating a new IAM user
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 创建新的 IAM 用户
- en: Here, we set `mle-ch4-user` as the value for the **User name** field. Under
    **Select AWS access type**, we ensure that the checkbox for **Password – AWS Management
    Console access** is checked under **Select AWS credential type**. For **Console
    password**, we choose **Autogenerated password**. For **Require password reset**,
    we uncheck **User must create a new password at next sign-in**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 `mle-ch4-user` 设置为 **用户名** 字段的值。在 **选择 AWS 访问类型** 下，确保在 **选择 AWS 凭据类型**
    下勾选了 **密码 – AWS 管理控制台访问** 的复选框。对于 **控制台密码**，我们选择 **自动生成密码**。对于 **要求密码重置**，我们取消勾选
    **用户必须在下次登录时创建新密码**。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: A more secure configuration would involve requiring a password reset when the
    IAM user account is used to sign in for the first time. However, we will skip
    this step in this chapter to reduce the overall number of steps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更安全的配置将涉及在 IAM 用户账户首次用于登录时要求重置密码。然而，在本章中我们将跳过此步骤以减少总步骤数。
- en: 'Click the **Next: Permissions** button.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **下一步：权限** 按钮。
- en: On the **Set permissions** page, select **Attach existing policies directly**.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **设置权限** 页面上，选择 **直接附加现有策略**。
- en: 'Use the search filter to locate and check the checkboxes for the following
    managed policies:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用搜索过滤器查找并勾选以下托管策略的复选框：
- en: '**AmazonS3FullAccess**'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AmazonS3FullAccess**'
- en: '**AmazonRedshiftFullAccess**'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AmazonRedshiftFullAccess**'
- en: '**AmazonVPCFullAccess**'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AmazonVPCFullAccess**'
- en: '**AWSCloudShellFullAccess**'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWSCloudShellFullAccess**'
- en: '**AWSGlueConsoleFullAccess**'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWSGlueConsoleFullAccess**'
- en: '**AmazonAthenaFullAccess**'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AmazonAthenaFullAccess**'
- en: '**IAMFullAccess**'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IAMFullAccess**'
- en: 'An example of this can be seen in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是一个示例：
- en: '![Figure 4.4 – Attaching existing policies directly ](img/B18638_04_004.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 直接附加现有策略](img/B18638_04_004.jpg)'
- en: Figure 4.4 – Attaching existing policies directly
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 直接附加现有策略
- en: These are policies that have been prepared and managed by AWS to make it convenient
    and easy for AWS account users to manage IAM permissions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是由AWS准备和管理的策略，以便AWS账户用户方便地管理IAM权限。
- en: Important Note
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that the permission configuration we have in this chapter can be improved
    further. When managing IAM permissions in a production-level account, make sure
    that you practice the **principle of least privilege**. This means that IAM identities
    should only have the minimum set of permissions to perform their tasks, which
    involves granting granular access to specific actions from specific resources
    when using a service. For more information, feel free to check out [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章中我们讨论的权限配置还可以进一步优化。在生产级别的账户中管理IAM权限时，请确保你遵循**最小权限原则**。这意味着IAM身份应仅拥有执行其任务所需的最小权限集，这涉及到在使用服务时，从特定资源授予特定操作的细粒度访问权限。有关更多信息，请随时查阅[*第9章*](B18638_09.xhtml#_idTextAnchor187)，*安全、治理和合规策略*。
- en: 'Once you have selected the managed policies, click the **Next: Tags** button.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择好管理策略后，点击**下一步：标签**按钮。
- en: 'On the **Add tags (optional)** page, click **Next: Review**.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**添加标签（可选）**页面，点击**下一步：审查**。
- en: On the **Review** page, click the **Create user** button.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**审查**页面，点击**创建用户**按钮。
- en: You should see a success notification, along with the sign-in link and credentials
    of the new user. Copy the sign-in link (for example, `https://<account>.signin.aws.amazon.com/console`),
    username, and password to a text editor in your local machine (for example, Visual
    Studio Code). Click the Close button afterward.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该会看到一个成功通知，以及新用户的登录链接和凭证。将登录链接（例如，`https://<account>.signin.aws.amazon.com/console`）、用户名和密码复制到本地机器上的文本编辑器（例如，Visual
    Studio Code）。之后点击关闭按钮。
- en: Important Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Do not share the sign-in link, username, and password with anyone. The IAM user
    with these credentials can easily take over the entire account, given the permissions
    we have configured for the IAM user during creation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将登录链接、用户名和密码与任何人分享。具有这些凭证的IAM用户可以轻松接管整个账户，考虑到我们在创建IAM用户时为其配置的权限。
- en: On the `mle-ch4-user`) in the success notification to navigate to the user details
    page.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在成功通知中的`mle-ch4-user`)中点击以导航到用户详情页面。
- en: '**Add inline policy**, as shown in the following screenshot:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**添加内联策略**，如图所示：'
- en: '![Figure 4.5 – Adding an inline policy ](img/B18638_04_005.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 添加内联策略](img/B18638_04_005.jpg)'
- en: Figure 4.5 – Adding an inline policy
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 添加内联策略
- en: In addition to the directly attached managed policies, we will be attaching
    an inline policy. We will customize the permissions that are configured with the
    inline policy in the next step.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了直接附加的管理策略外，我们还将附加一个内联策略。我们将在下一步中自定义内联策略中配置的权限。
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: For more information on managed policies and inline policies, check out [https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有关管理策略和内联策略的更多信息，请参阅[https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml)。
- en: 'On the **Create policy** page, navigate to the **JSON** tab, as shown here:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**创建策略**页面，导航到**JSON**选项卡，如图所示：
- en: '![Figure 4.6 – Creating a policy using the JSON editor ](img/B18638_04_006.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 使用JSON编辑器创建策略](img/B18638_04_006.jpg)'
- en: Figure 4.6 – Creating a policy using the JSON editor
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 使用JSON编辑器创建策略
- en: 'In the policy editor highlighted in the preceding screenshot, specify the following
    JSON configuration:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个屏幕截图中突出显示的策略编辑器中，指定以下JSON配置：
- en: '[PRE0]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This policy gives our IAM user permission to create and manage **Redshift Serverless**,
    **Lake Formation**, and **SQL Workbench** (a SQL query tool) resources. Without
    this additional inline policy, we would have issues using Redshift Serverless
    later in this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此策略授予我们的IAM用户创建和管理**Redshift Serverless**、**Lake Formation**和**SQL Workbench**（一个SQL查询工具）资源的权限。如果没有这个额外的内联策略，我们在本章后面使用Redshift
    Serverless时可能会遇到问题。
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: 'You can find a copy of this inline policy in the official GitHub repository:
    [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方GitHub仓库中找到此内联策略的副本：[https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy)。
- en: Next, click **Review policy**.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**审查策略**。
- en: On the `custom-inline-policy` as the value for the **Name** field. Then, click
    **Create policy**.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Name**字段中输入`custom-inline-policy`作为值。然后，点击**创建策略**。
- en: 'At this point, the `mle-ch4-user` IAM user should have eight policies attached:
    seven AWS-managed policies and one inline policy. This IAM user should have more
    than enough permissions to perform all actions and operations until the end of
    this chapter.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，`mle-ch4-user` IAM用户应该附加了八项策略：七项AWS管理的策略和一项内联策略。此IAM用户应有足够的权限执行所有操作和操作，直到本章结束。
- en: 'Next, we will sign in using the credentials we have copied to the text editor
    in our local machine and test if we can sign in successfully:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用我们复制到本地机器文本编辑器中的凭据进行登录，并测试我们是否可以成功登录：
- en: 'Sign out of the AWS Management Console session by doing the following:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下方式从AWS管理控制台会话中注销：
- en: Clicking your name in the top right-hand corner of the screen
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击屏幕右上角的您的名字
- en: Clicking the **Sign out** button
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**注销**按钮
- en: 'Navigate to the sign-in link (with a format similar to `https://<account>.signin.aws.amazon.com/console`).
    Make sure that you replace `<account>` with the account ID or alias of your AWS
    account:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到登录链接（格式类似于`https://<account>.signin.aws.amazon.com/console`）。确保将`<account>`替换为您的AWS账号ID或别名：
- en: '![Figure 4.7 – Sign in as IAM user ](img/B18638_04_007.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 以IAM用户身份登录](img/B18638_04_007.jpg)'
- en: Figure 4.7 – Sign in as IAM user
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 以IAM用户身份登录
- en: This should redirect you to the **Sign in as IAM user** page, similar to what
    we have in the preceding screenshot. Input the **Account ID**, **IAM user name**,
    and **Password** values, and then click the **Sign in** button.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会将您重定向到**以IAM用户身份登录**页面，类似于前面的截图。输入**账号ID**、**IAM用户名**和**密码**值，然后点击**登录**按钮。
- en: '*Wasn’t that easy?* Now that we have successfully created our IAM user, we
    can create a new VPC. This VPC will be used later when we create our Redshift
    Serverless endpoint.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*这不是很容易吗？* 现在我们已成功创建了IAM用户，我们可以创建一个新的VPC。此VPC将在我们创建Redshift Serverless端点时使用。'
- en: Creating a new VPC
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建新的VPC
- en: '**Amazon Virtual Private Cloud** (**VPC**) enables us to create and configure
    isolated virtual networks for our resources. In this section, we will create a
    new VPC from scratch even if we already have an existing one in the current region.
    This allows our Redshift Serverless instance to be launched in its own isolated
    network, which allows the network to be configured and secured separately from
    other existing VPCs.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**Amazon虚拟私有云**（**VPC**）使我们能够为我们的资源创建和配置隔离的虚拟网络。在本节中，即使我们已经在当前区域中有一个现有的VPC，我们也将从头开始创建一个新的VPC。这允许我们的Redshift
    Serverless实例在其自己的隔离网络中启动，这使得网络可以与其他现有的VPC分开进行配置和安全性设置。'
- en: There are different ways to create and configure a VPC. One of the faster ways
    is to use the **VPC Wizard**, which lets us set up a new VPC in just a few minutes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和配置VPC有不同的方法。其中一种更快的方法是使用**VPC向导**，它允许我们在几分钟内设置一个新的VPC。
- en: Important Note
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding, make sure that you are logged in as the `mle-ch4-user` IAM
    user.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您已以`mle-ch4-user` IAM用户身份登录。
- en: 'Follow these steps to create a new VPC using the **VPC Wizard**:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用**VPC向导**创建新的VPC：
- en: Navigate to the region of choice using the region drop-down in the menu bar.
    In this chapter, we’ll assume that we will create and manage our resources in
    the `us-west-2` region.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用菜单栏中的区域下拉菜单选择所需区域。在本章中，我们将假设我们将在`us-west-2`区域创建和管理我们的资源。
- en: 'Navigate to the VPC console by doing the following:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下方式导航到VPC控制台：
- en: Typing `VPC` in the search bar of the AWS Management Console
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS管理控制台的搜索栏中键入`VPC`
- en: Selecting the **VPC** service under the list of search results
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索结果列表下选择**VPC**服务
- en: 'Next, click the **Launch VPC Wizard/Create VPC** button. This will redirect
    you to the **Create VPC** wizard, as shown in the following screenshot:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击**启动VPC向导/创建VPC**按钮。这将带您转到**创建VPC**向导，如下截图所示：
- en: '![Figure 4.8 – The Create VPC wizard ](img/B18638_04_008.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图4.8 – 创建VPC向导](img/B18638_04_008.jpg)'
- en: Figure 4.8 – The Create VPC wizard
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 创建 VPC 向导
- en: Here, we can see that we can create and configure the relevant VPC resources
    with just a few clicks using the VPC Wizard.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们可以通过几个点击来创建和配置相关的 VPC 资源。
- en: Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may want to customize and secure this VPC setup further, but this is outside
    the scope of this chapter. For more information, check out [https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想要进一步自定义和确保此 VPC 设置的安全，但这超出了本章的范围。有关更多信息，请参阅 [https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml)。
- en: 'In the VPC Wizard, leave everything as-is except for the following:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 VPC 向导中，除了以下内容外，保持所有设置不变：
- en: '`mle-ch4-vpc`'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mle-ch4-vpc`'
- en: '**Number of Availability Zones (AZs)**: **3**'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用区 (AZ) 数量**：**3**'
- en: '**Number of public subnets**: **3**'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公共子网数量**：**3**'
- en: '**Number of private subnets**: **0**'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**私有子网数量**：**0**'
- en: '**NAT gateways ($)**: **None**'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT 网关 ($)**：**无**'
- en: Once you are done configuring the VPC, click the **Create VPC** button located
    at the bottom of the page.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您完成 VPC 的配置，请点击页面底部的 **创建 VPC** 按钮。
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: VPC creation may take about 1 to 2 minutes to complete.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 创建可能需要大约 1 到 2 分钟才能完成。
- en: Click **View VPC**.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **查看 VPC**。
- en: Copy the VPC ID (for example, `vpc-abcdefghijklmnop`) into an editor on your
    local machine (for example, Visual Studio Code).
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 VPC ID（例如，`vpc-abcdefghijklmnop`）复制到您本地机器上的编辑器中（例如，Visual Studio Code）。
- en: Now that the required VPC resources have been created, we can proceed with the
    last set of prerequisites.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所需的 VPC 资源已创建，我们可以继续进行最后一组先决条件。
- en: Uploading the dataset to S3
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集上传到 S3
- en: In [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering
    on AWS*, we used an **AWS Cloud9** environment to upload a sample dataset to **Amazon
    S3**. In this chapter, we will use **AWS CloudShell** instead to upload and download
    data to and from S3\. If this is your first time hearing about AWS CloudShell,
    it is a browser-based shell where we can run different commands to manage our
    resources. With CloudShell, we can run commands using the AWS CLI without having
    to worry about infrastructure management.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 1 章*](B18638_01.xhtml#_idTextAnchor017)，*AWS 上的机器学习工程简介*，我们使用 **AWS Cloud9**
    环境将样本数据集上传到 **Amazon S3**。在本章中，我们将使用 **AWS CloudShell** 来上传和下载数据到 S3。如果您第一次听说
    AWS CloudShell，它是一个基于浏览器的 shell，我们可以运行不同的命令来管理我们的资源。使用 CloudShell，我们可以运行 AWS CLI
    命令而无需担心基础设施管理。
- en: Important Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding, make sure that you are using the same region where the VPC
    resources were created. This chapter assumes that we are using the `us-west-2`)
    region. At the same time, please make sure that you are logged in as the `mle-ch4-user`
    IAM user.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您正在使用创建 VPC 资源相同的区域。本章假设我们正在使用 `us-west-2` 区域。同时，请确保您已以 `mle-ch4-user`
    IAM 用户登录。
- en: 'Follow these steps to use CloudShell and the AWS CLI to upload our sample dataset
    to S3:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用 CloudShell 和 AWS CLI 将我们的示例数据集上传到 S3：
- en: 'Navigate to **CloudShell** by clicking the button highlighted in the following
    screenshot:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击以下截图突出显示的按钮导航到 **CloudShell**：
- en: '![Figure 4.9 – Launching CloudShell ](img/B18638_04_009.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 启动 CloudShell](img/B18638_04_009.jpg)'
- en: Figure 4.9 – Launching CloudShell
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 启动 CloudShell
- en: You can find this button in the top right-hand corner of the AWS Management
    Console. You may also use the search bar to navigate to the CloudShell console.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 AWS 管理控制台右上角找到此按钮。您还可以使用搜索栏导航到 CloudShell 控制台。
- en: If you see the **Welcome to AWS CloudShell** popup window, click the **Close**
    button.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您看到 **欢迎使用 AWS CloudShell** 弹出窗口，请点击 **关闭** 按钮。
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It may take a minute or two for the `[cloudshell-user@ip-XX-XX-XX-XX ~]$`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`[cloudshell-user@ip-XX-XX-XX-XX ~]$` 可能需要一分钟左右。'
- en: 'Run the following single-line `wget` command in the Terminal console (after
    the **$** sign) to download a CSV file containing 100,000 booking records:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端控制台（在 **$** 符号之后）运行以下单行 `wget` 命令以下载包含 100,000 个预订记录的 CSV 文件：
- en: '[PRE1]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, inspect the downloaded file using the `head` command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `head` 命令检查下载的文件：
- en: '[PRE2]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This should yield the first few lines of the CSV file, similar to what we have
    in the following screenshot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会生成 CSV 文件的前几行，类似于以下截图所示：
- en: '![Figure 4.10 – Results after using the head command ](img/B18638_04_010.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 使用 head 命令后的结果](img/B18638_04_010.jpg)'
- en: Figure 4.10 – Results after using the head command
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 使用head命令后的结果
- en: As we can see, the `head` command displayed the first 10 lines of the `synthetic.bookings.100000.csv`
    file. Here, we have the header with all the column names of the CSV file in the
    first line.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`head`命令显示了`synthetic.bookings.100000.csv`文件的第一个10行。在这里，我们有了第一行中的CSV文件所有列名的标题。
- en: Note that this dataset is similar to the hotel bookings dataset we used in [*Chapter
    1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering on AWS*.
    The only major difference is that the CSV file we will use in this chapter contains
    100,000 records since we want to test how fast we can query data from our data
    warehouse and data lake.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个数据集与我们用于[*第1章*](B18638_01.xhtml#_idTextAnchor017)“AWS机器学习工程简介”中的酒店预订数据集相似。唯一的重大区别是，我们将在本章中使用的CSV文件包含100,000条记录，因为我们想测试从我们的数据仓库和数据湖查询数据有多快。
- en: 'Create a new S3 bucket using the `aws s3 mb` command. Make sure to replace
    `<INSERT BUCKET NAME>` with a globally unique bucket name – an S3 bucket name
    that has never been used before by all other AWS users:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`aws s3 mb`命令创建一个新的S3存储桶。确保用全局唯一的存储桶名称替换`<INSERT BUCKET NAME>` – 一个所有其他AWS用户以前从未使用过的S3存储桶名称：
- en: '[PRE3]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For more information on S3 bucket naming rules, check out [https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关于S3存储桶命名规则的更多信息，请参阅[https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml)。
- en: Important Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Make sure to remember the name of the bucket that was created in this step.
    We will use this S3 bucket in the different solutions and examples in this chapter.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 确保记住在此步骤中创建的存储桶名称。我们将在本章的不同解决方案和示例中使用此S3存储桶。
- en: Copy the name of the S3 bucket you created into the text editor on your local
    machine.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您创建的S3存储桶名称复制到您本地机器上的文本编辑器中。
- en: 'Upload the `synthetic.bookings.100000.csv` file using the `aws s3 cp` command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`aws s3 cp`命令上传`synthetic.bookings.100000.csv`文件：
- en: '[PRE5]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that all the prerequisites are ready, we can use **Redshift Serverless**
    to load and query our data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有先决条件都已准备就绪，我们可以使用**Redshift Serverless**来加载数据并进行查询。
- en: Running analytics at scale with Amazon Redshift Serverless
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Amazon Redshift Serverless进行大规模数据分析
- en: Data warehouses play a crucial role in data management, data analysis, and data
    engineering. Data engineers and ML engineers spend time building data warehouses
    to work on projects involving **batch reporting** and **business intelligence**.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库在数据管理、数据分析和数据工程中发挥着至关重要的作用。数据工程师和机器学习工程师花费时间构建数据仓库，以处理涉及**批量报告**和**商业智能**的项目。
- en: '![Figure 4.11 – Data warehouse ](img/B18638_04_011.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 数据仓库](img/B18638_04_011.jpg)'
- en: Figure 4.11 – Data warehouse
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 数据仓库
- en: As shown in the preceding diagram, a data warehouse contains combined data from
    different relational data sources such as PostgreSQL and MySQL databases. It generally
    serves as the single source of truth when querying data for reporting and business
    intelligence requirements. In ML experiments, a data warehouse can serve as the
    source of clean data where we can extract the dataset used to build and train
    ML models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，数据仓库包含来自不同关系型数据源（如PostgreSQL和MySQL数据库）的合并数据。它通常在查询数据以进行报告和商业智能需求时作为单一事实来源。在机器学习实验中，数据仓库可以作为清洁数据的来源，我们可以从中提取用于构建和训练机器学习模型的训练集。
- en: Note
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When generating reports, businesses and start-ups may end up performing queries
    directly on the production databases used by running web applications. It is important
    to note that these queries may cause unplanned downtime for the web applications
    connected to the databases (since the databases might become “busy” processing
    the additional queries). To avoid these types of scenarios, it is recommended
    to join and load the data from the application databases to a central data warehouse,
    where queries can be run safely. This means that we can generate automated reports
    and perform read queries on a copy of the data without worrying about any unexpected
    downtime.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成报告时，企业和初创公司可能会直接在运行Web应用的数据库上执行查询。需要注意的是，这些查询可能会对连接到数据库的Web应用造成计划外的停机时间（因为数据库可能会因为处理额外的查询而变得“繁忙”）。为了避免这些类型的场景，建议将应用程序数据库中的数据合并并加载到中央数据仓库中，在那里可以安全地运行查询。这意味着我们可以生成自动报告，并在数据的副本上执行读取查询，而不用担心任何意外的停机时间。
- en: If you need to set up a data warehouse on AWS, Amazon Redshift is one of the
    primary options available. With the announcement of **Amazon Redshift Serverless**,
    data engineers and ML engineers no longer need to worry about infrastructure management.
    Compared to its non-serverless counterparts and alternatives, there is no charge
    when the data warehouse is idle and not being used.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在 AWS 上设置数据仓库，Amazon Redshift 是可用的主要选项之一。随着 **Amazon Redshift Serverless**
    的宣布，数据工程师和机器学习工程师不再需要担心基础设施管理。与它的非无服务器版本和替代品相比，当数据仓库空闲且未被使用时，无需收费。
- en: Setting up a Redshift Serverless endpoint
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Redshift Serverless 端点
- en: Getting started and setting up Redshift Serverless is easy. All we need to do
    is navigate to the Redshift console and create a new Redshift Serverless endpoint.
    When creating a new Redshift Serverless endpoint, all we need to worry about is
    the VPC and the IAM user, which we prepared in the *Preparing the essential prerequisites*
    section of this chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用并设置 Redshift Serverless 非常简单。我们所需做的只是导航到 Redshift 控制台并创建一个新的 Redshift Serverless
    端点。在创建新的 Redshift Serverless 端点时，我们只需关注 VPC 和 IAM 用户，这些我们在本章的 *准备基本先决条件* 部分中已准备就绪。
- en: Important Note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您正在使用创建 S3 存储桶和 VPC 资源相同的区域。本章假设我们正在使用 `us-west-2` 区域。同时，请确保您已以 `mle-ch4-user`
    IAM 用户登录。
- en: 'Follow these steps to set up our Redshift Serverless endpoint:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤设置我们的 Redshift Serverless 端点：
- en: Navigate to the `redshift` in the search bar of the AWS Management Console,
    and then selecting the **Redshift** service from the list of results.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 管理控制台的搜索栏中导航到 `redshift`，然后从结果列表中选择 **Redshift** 服务。
- en: Next, click the **Try Amazon Redshift Serverless** button.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击 **尝试 Amazon Redshift Serverless** 按钮。
- en: On the `Customize settings`
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `自定义设置`
- en: '`dev`'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dev`'
- en: '**Admin user credentials** > **Customize admin user credentials**: *[UNCHECKED]*'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**管理员用户凭证** > **自定义管理员用户凭证**: *[未勾选]*'
- en: Under **Permissions**, open the **Manage IAM roles** dropdown, and then select
    **Create IAM role** from the list of options.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **权限** 下，打开 **管理 IAM 角色** 下拉菜单，然后从选项列表中选择 **创建 IAM 角色**。
- en: In the **Create the default IAM role** popup window, select **Any S3 bucket**.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **创建默认 IAM 角色** 弹出窗口中，选择 **任何 S3 存储桶**。
- en: Important Note
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that this configuration needs to be secured further once we need to configure
    our setup for production use. Ideally, Redshift is configured to access only a
    limited set of S3 buckets.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦我们需要为生产使用配置设置，此配置需要进一步加固。理想情况下，Redshift 应配置为仅访问有限数量的 S3 存储桶。
- en: Click the **Create IAM role as default** button.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建 IAM 角色作为默认角色** 按钮。
- en: Note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You should see a notification message similar to **The IAM role AmazonRedshift-CommandsAccessRole-XXXXXXXXXXXXXXX
    was successfully created and set as the default.** after clicking the **Create
    IAM role as default** button. Make sure that you copy the name of the created
    IAM role currently set as the default role into the text editor on your local
    machine.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在点击 **创建 IAM 角色作为默认角色** 按钮后，您应该会看到一个类似 **The IAM role AmazonRedshift-CommandsAccessRole-XXXXXXXXXXXXXXX
    was successfully created and set as the default.** 的通知消息。请确保将当前设置为默认角色的创建 IAM
    角色名称复制到您本地机器上的文本编辑器中。
- en: 'Next, use the following configuration settings for **Network and security**:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下配置设置进行 **网络和安全**：
- en: '**Virtual private cloud (VPC)**: Use the VPC you created in this chapter by
    selecting the appropriate VPC ID.'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟专用云 (VPC)**: 通过选择适当的 VPC ID 使用本章中创建的 VPC。'
- en: '**VPC security groups**: Use the default VPC security group.'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VPC 安全组**: 使用默认的 VPC 安全组。'
- en: '**Subnet**: Check all the subnets in the list of options available in the dropdown
    menu.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子网**: 在下拉菜单中检查所有可用的子网选项。'
- en: Click the **Save configuration** button.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **保存配置** 按钮。
- en: Note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step may take 3 to 5 minutes to complete. You should see a popup window
    while you wait for the setup to complete. In the meantime, you may grab a cup
    of coffee or tea!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤可能需要 3 到 5 分钟才能完成。在您等待设置完成的同时，您应该会看到一个弹出窗口。在此期间，您可以喝杯咖啡或茶！
- en: Once the setup is complete, click the **Continue** button to close the popup
    window.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置完成，点击 **继续** 按钮以关闭弹出窗口。
- en: '*Wasn’t that easy?* At this point, you might be worried about the costs associated
    with the **Redshift Serverless** setup we have right now. The cool thing here
    is that there is no charge for the compute capacity when our serverless data warehouse
    is idle. Note that we’ll still be charged for storage, depending on the data stored.
    Once you have completed the hands-on solutions in this chapter, make sure to delete
    this setup and perform the relevant AWS resource cleanup steps to avoid any unexpected
    charges.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*这不是很简单吗？* 到目前为止，您可能担心我们现在设置的**Redshift无服务器**相关的费用。这里酷的地方是，当我们的无服务器数据仓库空闲时，没有计算能力的费用。请注意，根据存储的数据，我们仍将收取存储费用。一旦您完成本章的动手实践解决方案，请确保删除此设置并执行相关的AWS资源清理步骤，以避免任何意外费用。'
- en: Note
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on Redshift Serverless billing, feel free to check out
    [https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml](https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解有关Redshift无服务器计费的信息，请随时查看[https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml](https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml)。
- en: Opening Redshift query editor v2
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打开Redshift查询编辑器v2
- en: There are different ways to access the Redshift Serverless endpoint we have
    configured and prepared. One of the more convenient ways is to use the **Redshift
    query editor**, which we can access using our web browser.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以访问我们已配置和准备好的Redshift无服务器端点。其中一种更方便的方式是使用**Redshift查询编辑器**，我们可以通过我们的网络浏览器访问它。
- en: Important Note
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请确保您正在使用创建S3存储桶和VPC资源相同的区域。本章假设我们正在使用`us-west-2`区域。同时，请确保您已登录为`mle-ch4-user`
    IAM用户。
- en: 'Let’s open the Redshift query editor and see what we can do with it:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Redshift查询编辑器，看看我们能用它做什么：
- en: 'In the **Serverless dashboard** area, click **Query data**, as highlighted
    in the following screenshot:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**无服务器仪表板**区域，点击**查询数据**，如下截图所示：
- en: '![Figure 4.12 – Locating the Query data button in the Serverless dashboard
    ](img/B18638_04_012.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – 在无服务器仪表板中定位查询数据按钮](img/B18638_04_012.jpg)'
- en: Figure 4.12 – Locating the Query data button in the Serverless dashboard
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – 在无服务器仪表板中定位查询数据按钮
- en: 'Here, we can see that the **Query data** button is located near the top right
    of the **Serverless dashboard** page. Clicking the **Query data** button will
    open the **Redshift query editor v2** service (in a new tab), as shown in the
    following screenshot:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到**查询数据**按钮位于**无服务器仪表板**页面右上角。点击**查询数据**按钮将打开**Redshift查询编辑器v2**服务（在新标签页中），如下截图所示：
- en: '![Figure 4.13 – Redshift query editor v2 ](img/B18638_04_013.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13 – Redshift查询编辑器v2](img/B18638_04_013.jpg)'
- en: Figure 4.13 – Redshift query editor v2
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – Redshift查询编辑器v2
- en: Using the Redshift query editor is straightforward. We can manage our resources
    using the options available in the left-hand sidebar and we can run SQL queries
    on the editor on the right-hand side.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Redshift查询编辑器非常简单。我们可以通过左侧边栏中的选项来管理我们的资源，我们可以在右侧的编辑器上运行SQL查询。
- en: Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you are having issues opening Redshift query editor v2 after clicking the
    **Query data** button, make sure that your browser is not blocking new windows
    or popups from being opened.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在点击**查询数据**按钮后遇到无法打开Redshift查询编辑器v2的问题，请确保您的浏览器没有阻止新窗口或弹出窗口的打开。
- en: 'Click the arrow sign of the **Serverless** connection resource, as highlighted
    in the following screenshot:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击以下截图中所突出显示的**无服务器**连接资源的箭头符号：
- en: '![Figure 4.14 – Connecting to the Serverless resource ](img/B18638_04_014.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图4.14 – 连接到无服务器资源](img/B18638_04_014.jpg)'
- en: Figure 4.14 – Connecting to the Serverless resource
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 连接到无服务器资源
- en: You should see a **Connecting to Serverless** notification while the Redshift
    query editor is connecting to the Redshift Serverless endpoint.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当Redshift查询编辑器连接到Redshift无服务器端点时，您应该会看到一个**连接到无服务器**的通知。
- en: Once the connection has been made, we can proceed with creating a table.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立连接，我们就可以继续创建表格。
- en: Creating a table
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建表格
- en: 'There are different ways to create a table in Amazon Redshift. Follow these
    steps to create a table using a CSV file as a reference for the table schema:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `synthetic.bookings.10.csv` file from the official GitHub repository
    to your local machine. You can access the CSV file, which contains 10 sample rows,
    here: [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv).'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Redshift query editor v2**, click the **+ Create** dropdown and then select
    **Table** from the list of options.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Create table** popup window, set the **Schema** dropdown value to **public**
    and the **Table** field value to **bookings**.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Schemas are used to manage and group database objects and tables together. A
    newly created database will have the `PUBLIC` schema by default. In this chapter,
    we will not create a new schema and simply use the default `PUBLIC` schema.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the `synthetic.bookings.10.csv` file from your local machine:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Load from CSV ](img/B18638_04_015.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Load from CSV
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the **Load from CSV** option used the records stored in
    the selected CSV file to infer the column names, data types, and encodings that
    will be used to configure and create the new table.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Click **Create table**. You should see a notification stating **bookings table
    is created successfully**.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the CSV file used as a reference to create the table should be, ideally,
    a subset of the larger complete dataset. In our case, we used a CSV file that
    contains 10 records from the original CSV file with 100,000 records.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from S3
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that our table is ready, we can load the data stored in S3 into our table.
    Follow these steps to load the data from S3 using **Redshift query editor v2**:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Load data** button (beside the **+ Create** dropdown). A popup
    window similar to the following should appear:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Loading data from S3 ](img/B18638_04_016.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Loading data from S3
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the different configuration options available for loading data
    from S3\.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Open the **S3 file location** dropdown under **S3 URI** and select **us-west-2**
    from the list of options available.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: This setup assumes that we are using the `us-west-2`) region when performing
    the hands-on solutions in this chapter. Feel free to change this if the S3 file
    is located in another region.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click the `synthetic.bookings.100000.csv` file inside the **input** folder
    of the S3 bucket we created in this chapter:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Choose archive in S3 ](img/B18638_04_017.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Choose archive in S3
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: After selecting the `synthetic.bookings.100000.csv` file, click the **Choose**
    button.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Choose an IAM role** dropdown and select the IAM role with the same
    name as what you have copied into the text editor on your local machine in the
    *Setting up a Redshift Serverless endpoint* section.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: If you were unable to copy the IAM role to a text editor in your local machine,
    you may open a new tab and navigate to the `default` **Namespace configuration**
    page (in the AWS Management Console). You should find the IAM role (tagged as
    **Default** under **Role type**) in the **Security and encryption** tab.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Under **Advanced settings**, click **Data conversion parameters**. Ensure that
    the checkbox for **Ignore header rows** is *checked*. Click **Done**.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Select a schema** and then choose **public** from the list of dropdown
    options. Next, click **Select a table** and then select **bookings** from the
    list of dropdown options:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Loading data from the S3 bucket ](img/B18638_04_018.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Loading data from the S3 bucket
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a set of configuration parameters similar to
    what’s shown in the preceding screenshot.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Load data** button. This will close the **Load data** window and
    automatically run the load data operation.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: You may run the `SELECT * FROM sys_load_error_detail;` SQL statement in the
    query editor to troubleshoot any issues or errors you may have encountered.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: The final step may take about 1 to 2 minutes to complete. If you did not encounter
    any issues after running the load data operation, you can proceed with querying
    the database!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Querying the database
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have successfully loaded the CSV file from the S3 bucket to our
    Redshift Serverless table, let’s focus on performing queries using SQL statements:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **+** button (located at the left of the first tab) and then select
    **Notebook**, as shown here:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Creating a new SQL Notebook ](img/B18638_04_019.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Creating a new SQL Notebook
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '**SQL Notebooks** help organize and document the results of multiple SQL queries
    run using the Redshift query editor.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following SQL statement:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Make sure to click the **Run** button, as highlighted in the following screenshot:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Running the SQL query ](img/B18638_04_020.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Running the SQL query
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'This should return a set of results, similar to what we have in the following
    screenshot:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – The Add SQL button ](img/B18638_04_021.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – The Add SQL button
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Here, we should only get a maximum of 100 records after running the query since
    the **Limit 100** checkbox is toggled *to on*.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Add SQL** button afterward to create a new SQL cell below the current
    set of results.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, run the following SQL statement in the new SQL cell:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We should get `66987` as the result after running the query.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: You may run the `SELECT * FROM sys_load_error_detail;` SQL statement to troubleshoot
    and debug any issues.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try reviewing the bookings that have been canceled by guests with at
    least one previous cancellation. That said, let’s run the following SQL statement
    in a new SQL cell:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s also review the bookings that have been canceled by guests where the
    number of days on the waiting list exceeds 50:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that we can also check for **data integrity issues** using queries similar
    to the following:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With this query, we should be able to list the records where the `booking_changes`
    column value does not match the `has_booking_changes` column value.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can find other records with data integrity concerns using the
    following query:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With this query, we should be able to list the records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: These types of data integrity issues should be fixed before using the data to
    train an ML model.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a materialized view containing a precomputed result set,
    which can help speed up repeated queries:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we can query the precomputed data in the materialized view using the
    following query:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This should give us the list of records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value, along with
    the records where the `booking_changes` column value does not match the `has_booking_changes`
    column value.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: For more information about this topic, feel free to check out [https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to run other SQL queries to explore the data stored in the *bookings*
    table.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: Unloading data to S3
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we will copy and unload the data stored in the *bookings* table to
    Amazon S3\. Here, we will configure and use the `UNLOAD` command to perform this
    operation in parallel, split the data, and store it in S3 across several files.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been unloaded to Amazon S3, we can perform other operations
    on this data using services, tools, and libraries that can load the data directly
    from S3\. In our case, we will use the unloaded data files in the next section,
    *Setting up Lake Formation*, and use **AWS Glue** along with **Amazon Athena**
    to process the data files.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to unload the data stored in our Redshift Serverless table
    into an S3 bucket:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the menu (`mle-ch4-user@<ACCOUNT ALIAS>`) at the top right-hand side of
    the screen. Copy the account ID by clicking the boxes highlighted in the following
    screenshot. Save the copied account ID value to the text editor on your local
    machine:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Copying the account ID ](img/B18638_04_022.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Copying the account ID
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Note that the account ID should have no dashes when copied to the text editor
    on your local machine.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Redshift query editor v2**, click the **Add SQL** button and then run
    the following SQL statement in the new SQL cell:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Make sure you replace the following values:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '`<INSERT BUCKET NAME>` with the name of the bucket we created in the *Uploading
    the dataset to S3* section'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<ACCOUNT ID>` with the account ID of the AWS account'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<ROLE NAME>` with the IAM role name you copied into the text editor on your
    local machine in the *Setting up a Redshift Serverless endpoint* section'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since `PARALLEL ON` is specified when running the `UNLOAD` command, this `UNLOAD`
    operation will split the data stored in the *bookings* table and store these in
    multiple files in parallel.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to **AWS CloudShell** by clicking the button highlighted in the following
    screenshot:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Launching CloudShell ](img/B18638_04_023.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Launching CloudShell
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: We can find this button in the top right-hand corner of the AWS Management Console.
    You may also use the search bar to navigate to the CloudShell console.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to list the files inside the `unloaded` folder of
    our S3 bucket. Make sure to replace `<INSERT BUCKET NAME>` with the name of the
    bucket we created in the *Uploading the dataset to S3* section:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Move all the files in the current working directory to the `/tmp` directory
    using the `tmp` command:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Use the `aws s3 cp` command to download a copy of the files stored inside the
    `unloaded` folder inside the S3 bucket:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Use the `ls` command to check the filenames of the files that have been downloaded:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This should yield a list of filenames, similar to what we have in the following
    screenshot:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Results after using the ls command ](img/B18638_04_024.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Results after using the ls command
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the `UNLOAD` operation that was performed in the *Unloading
    data to S3* section divided and stored a copy of the *bookings* table in several
    files.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `head` command to inspect the first few lines of each of the downloaded
    files:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This should yield an output similar to the following:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Results after using the head command ](img/B18638_04_025.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Results after using the head command
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that each of the output files has a header with the corresponding
    names of each column.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished unloading data from the Redshift *bookings* table
    into our S3 bucket, we will proceed with setting up our data lake using AWS Lake
    Formation!
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot more we can use in Amazon Redshift and Amazon Redshift Serverless.
    This includes performance tuning techniques (to significantly speed up slow queries),
    **Redshift ML** (which we can use to train and use ML models for inference using
    SQL statements), and **Redshift Spectrum** (which we can use to query data directly
    from files stored in S3 buckets). These topics are outside the scope of this book,
    so feel free to check out [https://docs.aws.amazon.com/redshift/index.xhtml](https://docs.aws.amazon.com/redshift/index.xhtml)
    for more information.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Lake Formation
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it’s time to take a closer look at setting up our serverless data lake
    on AWS! Before we begin, let’s define what a data lake is and what type of data
    is stored in it. A **data lake** is a centralized data store that contains a variety
    of structured, semi-structured, and unstructured data from different data sources.
    As shown in the following diagram, data can be stored in a data lake without us
    having to worry about the structure and format. We can use a variety of file types
    such as JSON, CSV, and Apache Parquet when storing data in a data lake. In addition
    to these, data lakes may include both raw and processed (clean) data:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Getting started with data lakes ](img/B18638_04_026.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Getting started with data lakes
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: ML engineers and data scientists can use data lakes as the source of the data
    used for building and training ML models. Since the data stored in data lakes
    may be a mixture of both raw and clean data, additional data processing, data
    cleaning, and data transformation steps are needed before it can be used in ML
    requirements.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: If you are planning to set up and manage data lakes in AWS, **AWS Lake Formation**
    is the way to go! AWS Lake Formation is a service that helps set up and secure
    a data lake using a variety of services on AWS such as **Amazon S3**, **AWS Glue**,
    and **Amazon Athena**. Since we are utilizing *serverless* services with AWS Lake
    Formation, we won’t have to worry about managing any servers while we are setting
    up our data lake.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Creating a database
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to how databases work in Redshift and other services such as **Relational
    Database Service** (**RDS**), **AWS Lake Formation** databases can contain one
    or more tables. However, before we create a table, we will need to create a new
    database.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create a new database in AWS Lake Formation:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Lake Formation console by typing `lake formation` in the
    search box of the AWS Management Console and then selecting **AWS Lake Formation**
    from the list of results.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Welcome to Lake Formation** popup window, make sure that the **Add
    myself** checkbox is *checked*. Click the **Get started** button.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the sidebar, locate and click **Databases** under **Data catalog**.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create database** button located in the top right-hand corner of
    the **Databases** page.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under `mle-ch4-db` as the value for the **Name** field. Leave everything else
    as-is and then click the **Create database** button:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Creating a Lake Formation database ](img/B18638_04_027.jpg)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Creating a Lake Formation database
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: You should see a success notification stating that your database has been created
    successfully. You may ignore the **Unknown error** message notification shown
    in the preceding screenshot.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: The **Unknown error** message is most likely due to the limited permissions
    allowed with the current IAM user being used to perform the actions.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our Lake Formation database, let’s proceed with creating
    a table using an AWS Glue Crawler.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table using an AWS Glue Crawler
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS Glue** is a serverless **extract-transform-load** (**ETL**) service that
    provides different relevant components and capabilities for data integration.
    In this chapter, we will use one of the components of **AWS Glue** – the **AWS
    Glue Crawler**:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – How an AWS Glue Crawler works ](img/B18638_04_028.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – How an AWS Glue Crawler works
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, an **AWS Glue Crawler** processes the files
    stored in the target data stores and then infers a schema based on the structure
    and content of the files processed. This schema is used to create a table or a
    set of tables in **AWS Glue Data Catalog**. These tables can then be used by services
    such as **Amazon Athena** when querying data directly in S3.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'With these in mind, let’s proceed with creating an AWS Glue Crawler:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Tables** list page by clicking **Tables** from the sidebar.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Create table using a crawler** button (located in the top-left
    corner of the page). This will open **AWS Glue Console** in a new tab.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Crawlers** (**legacy**), as highlighted in the following screenshot:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Navigating to the Crawlers page ](img/B18638_04_029.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Navigating to the Crawlers page
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can navigate to the **Crawlers** page using the sidebar on
    the left-hand side of the screen.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Create a new crawler by clicking the **Add crawler** button.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `mle-ch4-crawler` as the **Crawler name** field’s value. Then, click
    **Next**.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Specify crawler source type** page, choose **Data
    stores** for **Crawler source type**. Under **Repeat crawls of S3 data stores**,
    select **Crawl all folders**. Then, click **Next**.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Add crawler** > **Add a data store** page, click the folder icon to
    set the S3 path location for the **Include path** field. This should open the
    **Choose S3 path** popup window, as shown here:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Choose S3 path ](img/B18638_04_030.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Choose S3 path
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Locate and toggle the checkbox for the `unloaded` folder inside the S3 bucket
    we created in the *Preparing the essential prerequisites* section of this chapter.
    Click the **Select** button afterward.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: If you skipped the *Getting started with Redshift Serverless* section of this
    chapter, you may create an empty `unloaded` folder in the S3 bucket and then upload
    the `synthetic.bookings.100000.csv` file to the `unloaded` folder. You may manually
    do this using the AWS Management Console or by using `input` folder of the S3
    bucket with the AWS CLI.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Set the `100`.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that the configuration you set on the **Add a data store** page is
    similar to what we have in the following screenshot before proceeding:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Adding a data store ](img/B18638_04_031.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – Adding a data store
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Once you have reviewed the data store configuration, click **Next**.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Add another data store** page, select the **No**
    option. Click the **Next** button afterward.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `ch4-iam` as the input field value under `AWSGlueServiceRole-ch4-iam`.
    After that, click **Next**.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Create a schedule for this crawler** page, choose
    **Run on-demand** from the list of dropdown options under **Frequency**. Click
    the **Next** button afterward.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Configure the crawler’s output** page, choose the
    database we have created (for example, **mle-ch4-db**) from the list of dropdown
    options under **Database**. Click the **Next** button afterward.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Finish** to create the AWS Glue crawler using the specified configuration
    parameters.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s run the Crawler by navigating to the **Crawlers** page (the new interface
    / NOT the **Legacy** pages), selecting the crawler, and then clicking the **Run**
    button:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Running the crawler ](img/B18638_04_032.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Navigating to the Crawlers page
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: This step may take about 1 to 3 minutes to complete.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the **Lake Formation** console (using the search box).
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the `unloaded` table that was generated by our AWS Glue crawler:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Refreshing the Tables list page ](img/B18638_04_033.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – Refreshing the Tables list page
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the refresh button, you should see the `unloaded` table in the
    list of tables.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **unloaded** link to navigate to the **Table details** page:'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.34 – Table details and Schema of the unloaded table ](img/B18638_04_034.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – Table details and Schema of the unloaded table
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, we should see the **Table details** and
    **Schema** information.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Actions** drop-down menu and select **View data** from the list of
    options. This should open the **Preview data** pop-up window, informing us that
    we will be taken to the Athena console. Click the **OK** button to proceed.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* Note that we are just scratching the surface of what we
    can do with **AWS Glue**. For more information, check out [https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml).'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Athena to query data in Amazon S3
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Athena** is a serverless query service that allows us to use SQL statements
    to query data from files stored in S3\. With Amazon Athena, we don’t have to worry
    about infrastructure management and it scales automatically to handle our queries:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.35 – How Amazon Athena works ](img/B18638_04_035.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
- en: Figure 4.35 – How Amazon Athena works
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: If you were to set this up yourself, you may need to set up an EC2 instance
    cluster with an application such as **Presto**. In addition to this, you will
    need to manage the overall cost, security, performance, and stability of this
    EC2 cluster setup yourself.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the query result location
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the **Before you run your first query, you need to set up a query result
    location in Amazon S3** notification appears on the **Editor** page, this means
    that you must make a quick configuration change on the Amazon Athena **Settings**
    page so that Athena can store the query results in a specified S3 bucket location
    every time there’s a query. These query results are then displayed in the UI in
    the Athena console.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to set up the query result location where our Amazon Athena
    queries will be stored:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the **Before you run your first query, you need to set up a query
    result location in Amazon S3** notification, click **View settings** to navigate
    to the **Settings** page. Otherwise, you may click the **Settings** tab, as shown
    in the following screenshot:'
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: f
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.36 – Navigating to the Settings tab ](img/B18638_04_036.jpg)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
- en: Figure 4.36 – Navigating to the Settings tab
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Manage** located in the right-hand corner of the **Query result and
    encryption settings** pane:'
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.37 – Managing the query result and encryption settings ](img/B18638_04_037.jpg)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
- en: Figure 4.37 – Managing the query result and encryption settings
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: Under **Query result location and encryption** in **Manage settings**, click
    **Browse S3** and locate the S3 bucket you created in this chapter. Toggle on
    the radio button and click the **Choose** button.
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Save** button.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have finished configuring the query result location for Amazon Athena,
    we can start running our queries.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL queries using Athena
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With everything ready, we can start using SQL statements to query the data stored
    in S3\. In this section, we’ll inspect our data and run a few queries to check
    for existing data integrity issues as well.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to query the data stored in the S3 bucket:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the **Editor** page by clicking the **Editor** tab.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Editor** tab, run the following query:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Make sure that you click the **Run** button, as highlighted in the following
    screenshot:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.38 – Running the SQL query ](img/B18638_04_038.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
- en: Figure 4.38 – Running the SQL query
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: 'This should return a set of results similar to what we have in the following
    screenshot. Note that Amazon Athena may return a different set of results every
    time the same query is run. You may add an `ORDER BY` clause in the query to ensure
    that there is consistency in the results that are returned when using the same
    query:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.39 – Athena query results ](img/B18638_04_039.jpg)'
  id: totrans-481
  prefs: []
  type: TYPE_IMG
- en: Figure 4.39 – Athena query results
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our query was processed in less than half a second. If
    we were to run the same query without the `LIMIT` clause, the run time may be
    more than a second.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance tuning** is outside the scope of this book, but feel free to
    check out [https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)
    for more information on this topic.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following query to count the number of bookings that were not canceled:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This should give us a result of `66987`, which should be the same result we
    got when we performed a similar Redshift Serverless query (in the *Running analytics
    at scale with Amazon Redshift Serverless* section).
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s list the bookings that were canceled by guests with at least one
    previous cancellation:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s also review the bookings that were canceled by guests where the number
    of days on the waiting list exceeds 50:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Note that we can also check for **data integrity issues** using queries similar
    to the following:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Using this query, we should be able to list the records where the `booking_changes`
    column value does not match the `has_booking_changes` column value.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 'On a similar note, we can find other records with data integrity concerns using
    the following query:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: With this query, we should be able to list the records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a view that can be referenced by future queries:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Note that views do *NOT* contain any data – the query defined in the view runs
    every time the view is referenced by another query.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, let’s run a sample query that references the view we prepared in
    the previous step:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This should give us the list of records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value, along with
    the records where the `booking_changes` column value does not match the `has_booking_changes`
    column value.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are wondering if we can programmatically query our data in S3 using
    **boto3** (AWS SDK for Python), then the answer is *yes*. We can even generate
    predictions with a deployed ML model directly in SQL statements using Amazon Athena
    and Amazon SageMaker. For more information on this topic, check out [*Chapter
    4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management on AWS*, of
    the book *Machine Learning with Amazon SageMaker Cookbook*. You can also find
    a quick example of how to use Python and boto3 to run Athena and Athena ML queries
    here: [https://bit.ly/36AiPpR](https://bit.ly/36AiPpR).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* Setting up a **serverless data lake** on AWS is easy, so
    long as we use the right set of tools and services. Before continuing to the next
    chapter, make sure that you review and delete all the resources that you created
    in this chapter.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-539
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were able to take a closer look at several AWS services
    that help enable serverless data management in organizations. When using **serverless**
    services, we no longer need to worry about infrastructure management, which helps
    us focus on what we need to do.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: We were able to utilize **Amazon Redshift Serverless** to prepare a serverless
    data warehouse. We were also able to use **AWS Lake Formation**, **AWS Glue**,
    and **Amazon Athena** to create and query data from a serverless data lake. With
    these *serverless* services, we were able to load and query data in just a few
    minutes.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-542
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, feel
    free to check out the following resources:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '*Security best practices for your VPC* ([https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml))'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing Amazon Redshift Serverless* ([https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/](https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/))'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security in AWS Lake Formation* ([https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml](https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml))'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
