- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless Data Management on AWS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Businesses generally utilize systems that collect and store user information,
    along with transaction data, inside databases. One good example of this would
    be an e-commerce startup that has a web application where customers can create
    an account and use their credit card to make online purchases. The user profiles,
    transaction data, and purchase history stored in several production databases
    can be used to build a **product recommendation engine**, which can help suggest
    products that customers would probably want to purchase as well. However, before
    this stored data is analyzed and used to train **machine learning** (**ML**) models,
    it must be merged and joined into a **centralized data store** so that it can
    be transformed and processed using a variety of tools and services. Several options
    are frequently used for these types of use cases, but we will focus on two of
    these in this chapter – **data warehouses** and **data lakes**.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses and data lakes play a crucial role when it comes to **data storage**
    and **data management**. When generating reports, companies without a data warehouse
    or a data lake may end up performing queries in the production database of a running
    application directly. This approach is not recommended since it could cause service
    degradation or even unplanned downtime for the application connected to the database.
    This will inevitably affect the sales numbers since the customers would not be
    able to use the e-commerce application to purchase products online. Data warehouses
    and data lakes help us handle and analyze large amounts of data that could come
    from multiple smaller databases connected to running applications. If you have
    experience setting up a data warehouse or a data lake, then you probably know
    that it takes skill, experience, and patience to manage the overall cost, stability,
    and performance of these types of environments. It is a good thing that *serverless*
    services have started to become available to help us with these types of requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on data management and use a variety of *serverless*
    services to manage and query our data. We will start by preparing a few prerequisites,
    including a new IAM user, a VPC, and an S3 bucket where the sample dataset will
    be stored. Once the prerequisites are ready, we will set up and configure a serverless
    data warehouse using **Redshift Serverless**. After that, we will use **AWS Lake
    Formation**, **AWS Glue**, and **Amazon Athena** to prepare a serverless data
    lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with serverless data management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running analytics at scale with Amazon Redshift Serverless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Lake Formation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Amazon Athena to query data in Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, you are probably wondering what these services are and how these
    services are used. Before proceeding, let’s first have a quick discussion on how
    serverless data management works!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we must have the following ready:'
  prefs: []
  type: TYPE_NORMAL
- en: A web browser (preferably Chrome or Firefox)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the AWS account that was used in the first few chapters of this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks, source code, and other files for each chapter are available
    in this book’s GitHub repository: [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with serverless data management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Years ago, developers, data scientists, and ML engineers had to spend hours
    or even days setting up the infrastructure needed for data management and data
    engineering. If a large dataset stored in S3 needed to be analyzed, a team of
    data scientists and ML engineers performed the following sequence of steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch and configure a cluster of EC2 instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the data from S3 to the volumes attached to the EC2 instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform queries on the data using one or more of the applications installed
    in the EC2 instances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the known challenges with this approach is that the provisioned resources
    may end up being underutilized. If the schedule of the data query operations is
    unpredictable, it would be tricky to manage the uptime, cost, and compute specifications
    of the setup as well. In addition to these, system administrators and DevOps engineers
    need to spend time managing the security, stability, performance, and configuration
    of the applications installed in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, it is much more practical to utilize **serverless** and managed services
    with these types of scenarios and use cases. As shown in the following diagram,
    we will have more time to focus on what we need to do since we no longer need
    to worry about server and infrastructure management when using serverless services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Serverless versus not serverless ](img/B18638_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Serverless versus not serverless
  prefs: []
  type: TYPE_NORMAL
- en: 'What do we mean by *actual work*? Here’s a quick list of what data analysts,
    data scientists, and data engineers need to work on outside of server management:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate charts and reports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze trends and patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect and resolve data integrity issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate data stores with business intelligence tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make recommendations to management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the data to train an ML model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When using serverless services, we also only pay for what we use. This means
    that we will not pay for the idle time the compute resources are not running.
    If we were to have both staging and production environments set up, we can be
    assured that the staging environment would only cost a fraction of the production
    environment setup since the resources in the production environment would have
    a higher expected utilization rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can utilize different AWS services when dealing with the following *serverless*
    data management and data processing requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless Data Warehouse**: Amazon Redshift Serverless'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless Data Lake**: AWS Lake Formation, AWS Glue, and Amazon Athena'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless Stream Processing**: Amazon Kinesis, AWS Lambda, and DynamoDB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless Distributed Data Processing**: Amazon EMR Serverless'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that this is just the tip of the iceberg and that there are more serverless
    services we can use for our needs. In this chapter, we will focus on setting up
    and querying a **serverless data warehouse** and a **serverless data lake**. Before
    we proceed with these, first, let’s prepare the prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you might be wondering when to use a data lake and when to use
    a data warehouse. A data warehouse is best used when the data being queried and
    processed is relational and defined in advance. The data quality of what is stored
    in a data warehouse is expected to be high as well. That said, a data warehouse
    is used as the “source of truth” of data and is generally used for use cases involving
    batch reporting and business intelligence. On the other hand, a data lake is best
    used when the data being queried and processed involves both relational and non-relational
    data from different data sources. Data stored in data lakes may include both raw
    and clean data. In addition to this, data is stored in a data lake without you
    having to worry about the data structure and schema during data capture. Finally,
    data lakes can be used for use cases involving ML, predictive analytics, and **exploratory
    data analysis** (**EDA**). Since data lakes and data warehouses serve different
    purposes, some organizations utilize both options for their data management needs.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the essential prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will ensure that the following prerequisites are ready
    before proceeding with setting up our data warehouse and data lake in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A text editor (for example, VS Code) on your local machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An IAM user with the permissions to create and manage the resources we will
    use in this chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A VPC where we will launch the Redshift Serverless endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new S3 bucket where our data will be uploaded using AWS CloudShell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will create and manage our resources in the `us-west-2`)
    region. Make sure that you have set the correct region before proceeding with
    the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Opening a text editor on your local machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure you have an open text editor (for example, **VS Code**) on your local
    machine. We will copy some string values into the text editor for later use in
    this chapter. Here are the values we will have to copy later in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: IAM sign-in link, username, and password (*Preparing the essential prerequisites*
    > *Creating an IAM user*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VPC ID (*Preparing the essential prerequisites* > *Creating a new VPC*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Name of the created IAM role currently set as the default role (*Running analytics
    at scale with Amazon Redshift Serverless* > *Setting up a Redshift Serverless
    endpoint*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Account ID (*Running analytics at scale with Amazon Redshift Serverless*
    > *Unloading data to S3*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you do not have VS Code installed, you can use **TextEdit**, **Notepad**,
    **Notepad++**, or **GEdit**, depending on what you have installed on your local
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an IAM user
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to note that we may encounter issues when running queries in
    **Redshift Serverless** if we were to use the root account directly. That said,
    we will be creating an IAM user in this section. This IAM user will be configured
    to have the appropriate set of permissions needed to perform all the hands-on
    solutions in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that you use the root account when creating a new IAM user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create an IAM user from the IAM console:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the IAM console using the search bar, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Navigating to the IAM console ](img/B18638_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Navigating to the IAM console
  prefs: []
  type: TYPE_NORMAL
- en: After typing `iam` in the search bar, we must select the **IAM** service from
    the list of search results.
  prefs: []
  type: TYPE_NORMAL
- en: Locate **Access management** in the sidebar and then click **Users** to navigate
    to the **Users** list page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the top right-hand corner of the screen, locate and click the **Add users**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Set user details** page, add a new user using a configuration similar
    to what we have in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Creating a new IAM user ](img/B18638_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Creating a new IAM user
  prefs: []
  type: TYPE_NORMAL
- en: Here, we set `mle-ch4-user` as the value for the **User name** field. Under
    **Select AWS access type**, we ensure that the checkbox for **Password – AWS Management
    Console access** is checked under **Select AWS credential type**. For **Console
    password**, we choose **Autogenerated password**. For **Require password reset**,
    we uncheck **User must create a new password at next sign-in**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A more secure configuration would involve requiring a password reset when the
    IAM user account is used to sign in for the first time. However, we will skip
    this step in this chapter to reduce the overall number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Next: Permissions** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Set permissions** page, select **Attach existing policies directly**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the search filter to locate and check the checkboxes for the following
    managed policies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AmazonS3FullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AmazonRedshiftFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AmazonVPCFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWSCloudShellFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWSGlueConsoleFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AmazonAthenaFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IAMFullAccess**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of this can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Attaching existing policies directly ](img/B18638_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Attaching existing policies directly
  prefs: []
  type: TYPE_NORMAL
- en: These are policies that have been prepared and managed by AWS to make it convenient
    and easy for AWS account users to manage IAM permissions.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the permission configuration we have in this chapter can be improved
    further. When managing IAM permissions in a production-level account, make sure
    that you practice the **principle of least privilege**. This means that IAM identities
    should only have the minimum set of permissions to perform their tasks, which
    involves granting granular access to specific actions from specific resources
    when using a service. For more information, feel free to check out [*Chapter 9*](B18638_09.xhtml#_idTextAnchor187),
    *Security, Governance, and Compliance Strategies*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have selected the managed policies, click the **Next: Tags** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Add tags (optional)** page, click **Next: Review**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Review** page, click the **Create user** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see a success notification, along with the sign-in link and credentials
    of the new user. Copy the sign-in link (for example, `https://<account>.signin.aws.amazon.com/console`),
    username, and password to a text editor in your local machine (for example, Visual
    Studio Code). Click the Close button afterward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not share the sign-in link, username, and password with anyone. The IAM user
    with these credentials can easily take over the entire account, given the permissions
    we have configured for the IAM user during creation.
  prefs: []
  type: TYPE_NORMAL
- en: On the `mle-ch4-user`) in the success notification to navigate to the user details
    page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add inline policy**, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Adding an inline policy ](img/B18638_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Adding an inline policy
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the directly attached managed policies, we will be attaching
    an inline policy. We will customize the permissions that are configured with the
    inline policy in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on managed policies and inline policies, check out [https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Create policy** page, navigate to the **JSON** tab, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Creating a policy using the JSON editor ](img/B18638_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Creating a policy using the JSON editor
  prefs: []
  type: TYPE_NORMAL
- en: 'In the policy editor highlighted in the preceding screenshot, specify the following
    JSON configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This policy gives our IAM user permission to create and manage **Redshift Serverless**,
    **Lake Formation**, and **SQL Workbench** (a SQL query tool) resources. Without
    this additional inline policy, we would have issues using Redshift Serverless
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a copy of this inline policy in the official GitHub repository:
    [https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy](https://github.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/blob/main/chapter04/inline-policy).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, click **Review policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `custom-inline-policy` as the value for the **Name** field. Then, click
    **Create policy**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this point, the `mle-ch4-user` IAM user should have eight policies attached:
    seven AWS-managed policies and one inline policy. This IAM user should have more
    than enough permissions to perform all actions and operations until the end of
    this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will sign in using the credentials we have copied to the text editor
    in our local machine and test if we can sign in successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sign out of the AWS Management Console session by doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicking your name in the top right-hand corner of the screen
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Clicking the **Sign out** button
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the sign-in link (with a format similar to `https://<account>.signin.aws.amazon.com/console`).
    Make sure that you replace `<account>` with the account ID or alias of your AWS
    account:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Sign in as IAM user ](img/B18638_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Sign in as IAM user
  prefs: []
  type: TYPE_NORMAL
- en: This should redirect you to the **Sign in as IAM user** page, similar to what
    we have in the preceding screenshot. Input the **Account ID**, **IAM user name**,
    and **Password** values, and then click the **Sign in** button.
  prefs: []
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* Now that we have successfully created our IAM user, we
    can create a new VPC. This VPC will be used later when we create our Redshift
    Serverless endpoint.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new VPC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Virtual Private Cloud** (**VPC**) enables us to create and configure
    isolated virtual networks for our resources. In this section, we will create a
    new VPC from scratch even if we already have an existing one in the current region.
    This allows our Redshift Serverless instance to be launched in its own isolated
    network, which allows the network to be configured and secured separately from
    other existing VPCs.'
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to create and configure a VPC. One of the faster ways
    is to use the **VPC Wizard**, which lets us set up a new VPC in just a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are logged in as the `mle-ch4-user` IAM
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create a new VPC using the **VPC Wizard**:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the region of choice using the region drop-down in the menu bar.
    In this chapter, we’ll assume that we will create and manage our resources in
    the `us-west-2` region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the VPC console by doing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Typing `VPC` in the search bar of the AWS Management Console
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting the **VPC** service under the list of search results
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, click the **Launch VPC Wizard/Create VPC** button. This will redirect
    you to the **Create VPC** wizard, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The Create VPC wizard ](img/B18638_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The Create VPC wizard
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that we can create and configure the relevant VPC resources
    with just a few clicks using the VPC Wizard.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may want to customize and secure this VPC setup further, but this is outside
    the scope of this chapter. For more information, check out [https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the VPC Wizard, leave everything as-is except for the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`mle-ch4-vpc`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of Availability Zones (AZs)**: **3**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of public subnets**: **3**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of private subnets**: **0**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT gateways ($)**: **None**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you are done configuring the VPC, click the **Create VPC** button located
    at the bottom of the page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: VPC creation may take about 1 to 2 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Click **View VPC**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the VPC ID (for example, `vpc-abcdefghijklmnop`) into an editor on your
    local machine (for example, Visual Studio Code).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the required VPC resources have been created, we can proceed with the
    last set of prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading the dataset to S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering
    on AWS*, we used an **AWS Cloud9** environment to upload a sample dataset to **Amazon
    S3**. In this chapter, we will use **AWS CloudShell** instead to upload and download
    data to and from S3\. If this is your first time hearing about AWS CloudShell,
    it is a browser-based shell where we can run different commands to manage our
    resources. With CloudShell, we can run commands using the AWS CLI without having
    to worry about infrastructure management.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are using the same region where the VPC
    resources were created. This chapter assumes that we are using the `us-west-2`)
    region. At the same time, please make sure that you are logged in as the `mle-ch4-user`
    IAM user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to use CloudShell and the AWS CLI to upload our sample dataset
    to S3:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to **CloudShell** by clicking the button highlighted in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Launching CloudShell ](img/B18638_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Launching CloudShell
  prefs: []
  type: TYPE_NORMAL
- en: You can find this button in the top right-hand corner of the AWS Management
    Console. You may also use the search bar to navigate to the CloudShell console.
  prefs: []
  type: TYPE_NORMAL
- en: If you see the **Welcome to AWS CloudShell** popup window, click the **Close**
    button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It may take a minute or two for the `[cloudshell-user@ip-XX-XX-XX-XX ~]$`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following single-line `wget` command in the Terminal console (after
    the **$** sign) to download a CSV file containing 100,000 booking records:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, inspect the downloaded file using the `head` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield the first few lines of the CSV file, similar to what we have
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Results after using the head command ](img/B18638_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Results after using the head command
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the `head` command displayed the first 10 lines of the `synthetic.bookings.100000.csv`
    file. Here, we have the header with all the column names of the CSV file in the
    first line.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this dataset is similar to the hotel bookings dataset we used in [*Chapter
    1*](B18638_01.xhtml#_idTextAnchor017), *Introduction to ML Engineering on AWS*.
    The only major difference is that the CSV file we will use in this chapter contains
    100,000 records since we want to test how fast we can query data from our data
    warehouse and data lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new S3 bucket using the `aws s3 mb` command. Make sure to replace
    `<INSERT BUCKET NAME>` with a globally unique bucket name – an S3 bucket name
    that has never been used before by all other AWS users:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For more information on S3 bucket naming rules, check out [https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to remember the name of the bucket that was created in this step.
    We will use this S3 bucket in the different solutions and examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Copy the name of the S3 bucket you created into the text editor on your local
    machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload the `synthetic.bookings.100000.csv` file using the `aws s3 cp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that all the prerequisites are ready, we can use **Redshift Serverless**
    to load and query our data.
  prefs: []
  type: TYPE_NORMAL
- en: Running analytics at scale with Amazon Redshift Serverless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data warehouses play a crucial role in data management, data analysis, and data
    engineering. Data engineers and ML engineers spend time building data warehouses
    to work on projects involving **batch reporting** and **business intelligence**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Data warehouse ](img/B18638_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Data warehouse
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, a data warehouse contains combined data from
    different relational data sources such as PostgreSQL and MySQL databases. It generally
    serves as the single source of truth when querying data for reporting and business
    intelligence requirements. In ML experiments, a data warehouse can serve as the
    source of clean data where we can extract the dataset used to build and train
    ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When generating reports, businesses and start-ups may end up performing queries
    directly on the production databases used by running web applications. It is important
    to note that these queries may cause unplanned downtime for the web applications
    connected to the databases (since the databases might become “busy” processing
    the additional queries). To avoid these types of scenarios, it is recommended
    to join and load the data from the application databases to a central data warehouse,
    where queries can be run safely. This means that we can generate automated reports
    and perform read queries on a copy of the data without worrying about any unexpected
    downtime.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to set up a data warehouse on AWS, Amazon Redshift is one of the
    primary options available. With the announcement of **Amazon Redshift Serverless**,
    data engineers and ML engineers no longer need to worry about infrastructure management.
    Compared to its non-serverless counterparts and alternatives, there is no charge
    when the data warehouse is idle and not being used.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Redshift Serverless endpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting started and setting up Redshift Serverless is easy. All we need to do
    is navigate to the Redshift console and create a new Redshift Serverless endpoint.
    When creating a new Redshift Serverless endpoint, all we need to worry about is
    the VPC and the IAM user, which we prepared in the *Preparing the essential prerequisites*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to set up our Redshift Serverless endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the `redshift` in the search bar of the AWS Management Console,
    and then selecting the **Redshift** service from the list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Try Amazon Redshift Serverless** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `Customize settings`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dev`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Admin user credentials** > **Customize admin user credentials**: *[UNCHECKED]*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under **Permissions**, open the **Manage IAM roles** dropdown, and then select
    **Create IAM role** from the list of options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Create the default IAM role** popup window, select **Any S3 bucket**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Note that this configuration needs to be secured further once we need to configure
    our setup for production use. Ideally, Redshift is configured to access only a
    limited set of S3 buckets.
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Create IAM role as default** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should see a notification message similar to **The IAM role AmazonRedshift-CommandsAccessRole-XXXXXXXXXXXXXXX
    was successfully created and set as the default.** after clicking the **Create
    IAM role as default** button. Make sure that you copy the name of the created
    IAM role currently set as the default role into the text editor on your local
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, use the following configuration settings for **Network and security**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Virtual private cloud (VPC)**: Use the VPC you created in this chapter by
    selecting the appropriate VPC ID.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPC security groups**: Use the default VPC security group.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subnet**: Check all the subnets in the list of options available in the dropdown
    menu.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click the **Save configuration** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take 3 to 5 minutes to complete. You should see a popup window
    while you wait for the setup to complete. In the meantime, you may grab a cup
    of coffee or tea!
  prefs: []
  type: TYPE_NORMAL
- en: Once the setup is complete, click the **Continue** button to close the popup
    window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* At this point, you might be worried about the costs associated
    with the **Redshift Serverless** setup we have right now. The cool thing here
    is that there is no charge for the compute capacity when our serverless data warehouse
    is idle. Note that we’ll still be charged for storage, depending on the data stored.
    Once you have completed the hands-on solutions in this chapter, make sure to delete
    this setup and perform the relevant AWS resource cleanup steps to avoid any unexpected
    charges.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Redshift Serverless billing, feel free to check out
    [https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml](https://docs.amazonaws.cn/en_us/redshift/latest/mgmt/serverless-billing.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Opening Redshift query editor v2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different ways to access the Redshift Serverless endpoint we have
    configured and prepared. One of the more convenient ways is to use the **Redshift
    query editor**, which we can access using our web browser.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open the Redshift query editor and see what we can do with it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Serverless dashboard** area, click **Query data**, as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Locating the Query data button in the Serverless dashboard
    ](img/B18638_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Locating the Query data button in the Serverless dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the **Query data** button is located near the top right
    of the **Serverless dashboard** page. Clicking the **Query data** button will
    open the **Redshift query editor v2** service (in a new tab), as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Redshift query editor v2 ](img/B18638_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Redshift query editor v2
  prefs: []
  type: TYPE_NORMAL
- en: Using the Redshift query editor is straightforward. We can manage our resources
    using the options available in the left-hand sidebar and we can run SQL queries
    on the editor on the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you are having issues opening Redshift query editor v2 after clicking the
    **Query data** button, make sure that your browser is not blocking new windows
    or popups from being opened.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the arrow sign of the **Serverless** connection resource, as highlighted
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Connecting to the Serverless resource ](img/B18638_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Connecting to the Serverless resource
  prefs: []
  type: TYPE_NORMAL
- en: You should see a **Connecting to Serverless** notification while the Redshift
    query editor is connecting to the Redshift Serverless endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Once the connection has been made, we can proceed with creating a table.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different ways to create a table in Amazon Redshift. Follow these
    steps to create a table using a CSV file as a reference for the table schema:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the `synthetic.bookings.10.csv` file from the official GitHub repository
    to your local machine. You can access the CSV file, which contains 10 sample rows,
    here: [https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv](https://raw.githubusercontent.com/PacktPublishing/Machine-Learning-Engineering-on-AWS/main/chapter04/synthetic.bookings.10.csv).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In **Redshift query editor v2**, click the **+ Create** dropdown and then select
    **Table** from the list of options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Create table** popup window, set the **Schema** dropdown value to **public**
    and the **Table** field value to **bookings**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Schemas are used to manage and group database objects and tables together. A
    newly created database will have the `PUBLIC` schema by default. In this chapter,
    we will not create a new schema and simply use the default `PUBLIC` schema.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the `synthetic.bookings.10.csv` file from your local machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Load from CSV ](img/B18638_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Load from CSV
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the **Load from CSV** option used the records stored in
    the selected CSV file to infer the column names, data types, and encodings that
    will be used to configure and create the new table.
  prefs: []
  type: TYPE_NORMAL
- en: Click **Create table**. You should see a notification stating **bookings table
    is created successfully**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the CSV file used as a reference to create the table should be, ideally,
    a subset of the larger complete dataset. In our case, we used a CSV file that
    contains 10 records from the original CSV file with 100,000 records.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data from S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that our table is ready, we can load the data stored in S3 into our table.
    Follow these steps to load the data from S3 using **Redshift query editor v2**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **Load data** button (beside the **+ Create** dropdown). A popup
    window similar to the following should appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Loading data from S3 ](img/B18638_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Loading data from S3
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the different configuration options available for loading data
    from S3\.
  prefs: []
  type: TYPE_NORMAL
- en: Open the **S3 file location** dropdown under **S3 URI** and select **us-west-2**
    from the list of options available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This setup assumes that we are using the `us-west-2`) region when performing
    the hands-on solutions in this chapter. Feel free to change this if the S3 file
    is located in another region.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click the `synthetic.bookings.100000.csv` file inside the **input** folder
    of the S3 bucket we created in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Choose archive in S3 ](img/B18638_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Choose archive in S3
  prefs: []
  type: TYPE_NORMAL
- en: After selecting the `synthetic.bookings.100000.csv` file, click the **Choose**
    button.
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Choose an IAM role** dropdown and select the IAM role with the same
    name as what you have copied into the text editor on your local machine in the
    *Setting up a Redshift Serverless endpoint* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you were unable to copy the IAM role to a text editor in your local machine,
    you may open a new tab and navigate to the `default` **Namespace configuration**
    page (in the AWS Management Console). You should find the IAM role (tagged as
    **Default** under **Role type**) in the **Security and encryption** tab.
  prefs: []
  type: TYPE_NORMAL
- en: Under **Advanced settings**, click **Data conversion parameters**. Ensure that
    the checkbox for **Ignore header rows** is *checked*. Click **Done**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Select a schema** and then choose **public** from the list of dropdown
    options. Next, click **Select a table** and then select **bookings** from the
    list of dropdown options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Loading data from the S3 bucket ](img/B18638_04_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Loading data from the S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a set of configuration parameters similar to
    what’s shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Load data** button. This will close the **Load data** window and
    automatically run the load data operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may run the `SELECT * FROM sys_load_error_detail;` SQL statement in the
    query editor to troubleshoot any issues or errors you may have encountered.
  prefs: []
  type: TYPE_NORMAL
- en: The final step may take about 1 to 2 minutes to complete. If you did not encounter
    any issues after running the load data operation, you can proceed with querying
    the database!
  prefs: []
  type: TYPE_NORMAL
- en: Querying the database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have successfully loaded the CSV file from the S3 bucket to our
    Redshift Serverless table, let’s focus on performing queries using SQL statements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **+** button (located at the left of the first tab) and then select
    **Notebook**, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Creating a new SQL Notebook ](img/B18638_04_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Creating a new SQL Notebook
  prefs: []
  type: TYPE_NORMAL
- en: '**SQL Notebooks** help organize and document the results of multiple SQL queries
    run using the Redshift query editor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following SQL statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure to click the **Run** button, as highlighted in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Running the SQL query ](img/B18638_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Running the SQL query
  prefs: []
  type: TYPE_NORMAL
- en: 'This should return a set of results, similar to what we have in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – The Add SQL button ](img/B18638_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – The Add SQL button
  prefs: []
  type: TYPE_NORMAL
- en: Here, we should only get a maximum of 100 records after running the query since
    the **Limit 100** checkbox is toggled *to on*.
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Add SQL** button afterward to create a new SQL cell below the current
    set of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, run the following SQL statement in the new SQL cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We should get `66987` as the result after running the query.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You may run the `SELECT * FROM sys_load_error_detail;` SQL statement to troubleshoot
    and debug any issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try reviewing the bookings that have been canceled by guests with at
    least one previous cancellation. That said, let’s run the following SQL statement
    in a new SQL cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also review the bookings that have been canceled by guests where the
    number of days on the waiting list exceeds 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that we can also check for **data integrity issues** using queries similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this query, we should be able to list the records where the `booking_changes`
    column value does not match the `has_booking_changes` column value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can find other records with data integrity concerns using the
    following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this query, we should be able to list the records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These types of data integrity issues should be fixed before using the data to
    train an ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a materialized view containing a precomputed result set,
    which can help speed up repeated queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can query the precomputed data in the materialized view using the
    following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us the list of records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value, along with
    the records where the `booking_changes` column value does not match the `has_booking_changes`
    column value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information about this topic, feel free to check out [https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to run other SQL queries to explore the data stored in the *bookings*
    table.
  prefs: []
  type: TYPE_NORMAL
- en: Unloading data to S3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we will copy and unload the data stored in the *bookings* table to
    Amazon S3\. Here, we will configure and use the `UNLOAD` command to perform this
    operation in parallel, split the data, and store it in S3 across several files.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been unloaded to Amazon S3, we can perform other operations
    on this data using services, tools, and libraries that can load the data directly
    from S3\. In our case, we will use the unloaded data files in the next section,
    *Setting up Lake Formation*, and use **AWS Glue** along with **Amazon Athena**
    to process the data files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to unload the data stored in our Redshift Serverless table
    into an S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the menu (`mle-ch4-user@<ACCOUNT ALIAS>`) at the top right-hand side of
    the screen. Copy the account ID by clicking the boxes highlighted in the following
    screenshot. Save the copied account ID value to the text editor on your local
    machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – Copying the account ID ](img/B18638_04_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – Copying the account ID
  prefs: []
  type: TYPE_NORMAL
- en: Note that the account ID should have no dashes when copied to the text editor
    on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Redshift query editor v2**, click the **Add SQL** button and then run
    the following SQL statement in the new SQL cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure you replace the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<INSERT BUCKET NAME>` with the name of the bucket we created in the *Uploading
    the dataset to S3* section'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<ACCOUNT ID>` with the account ID of the AWS account'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<ROLE NAME>` with the IAM role name you copied into the text editor on your
    local machine in the *Setting up a Redshift Serverless endpoint* section'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since `PARALLEL ON` is specified when running the `UNLOAD` command, this `UNLOAD`
    operation will split the data stored in the *bookings* table and store these in
    multiple files in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to **AWS CloudShell** by clicking the button highlighted in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.23 – Launching CloudShell ](img/B18638_04_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Launching CloudShell
  prefs: []
  type: TYPE_NORMAL
- en: We can find this button in the top right-hand corner of the AWS Management Console.
    You may also use the search bar to navigate to the CloudShell console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to list the files inside the `unloaded` folder of
    our S3 bucket. Make sure to replace `<INSERT BUCKET NAME>` with the name of the
    bucket we created in the *Uploading the dataset to S3* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Move all the files in the current working directory to the `/tmp` directory
    using the `tmp` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `aws s3 cp` command to download a copy of the files stored inside the
    `unloaded` folder inside the S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `ls` command to check the filenames of the files that have been downloaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield a list of filenames, similar to what we have in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Results after using the ls command ](img/B18638_04_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Results after using the ls command
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the `UNLOAD` operation that was performed in the *Unloading
    data to S3* section divided and stored a copy of the *bookings* table in several
    files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `head` command to inspect the first few lines of each of the downloaded
    files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should yield an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Results after using the head command ](img/B18638_04_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Results after using the head command
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that each of the output files has a header with the corresponding
    names of each column.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have finished unloading data from the Redshift *bookings* table
    into our S3 bucket, we will proceed with setting up our data lake using AWS Lake
    Formation!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot more we can use in Amazon Redshift and Amazon Redshift Serverless.
    This includes performance tuning techniques (to significantly speed up slow queries),
    **Redshift ML** (which we can use to train and use ML models for inference using
    SQL statements), and **Redshift Spectrum** (which we can use to query data directly
    from files stored in S3 buckets). These topics are outside the scope of this book,
    so feel free to check out [https://docs.aws.amazon.com/redshift/index.xhtml](https://docs.aws.amazon.com/redshift/index.xhtml)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Lake Formation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it’s time to take a closer look at setting up our serverless data lake
    on AWS! Before we begin, let’s define what a data lake is and what type of data
    is stored in it. A **data lake** is a centralized data store that contains a variety
    of structured, semi-structured, and unstructured data from different data sources.
    As shown in the following diagram, data can be stored in a data lake without us
    having to worry about the structure and format. We can use a variety of file types
    such as JSON, CSV, and Apache Parquet when storing data in a data lake. In addition
    to these, data lakes may include both raw and processed (clean) data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Getting started with data lakes ](img/B18638_04_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Getting started with data lakes
  prefs: []
  type: TYPE_NORMAL
- en: ML engineers and data scientists can use data lakes as the source of the data
    used for building and training ML models. Since the data stored in data lakes
    may be a mixture of both raw and clean data, additional data processing, data
    cleaning, and data transformation steps are needed before it can be used in ML
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: If you are planning to set up and manage data lakes in AWS, **AWS Lake Formation**
    is the way to go! AWS Lake Formation is a service that helps set up and secure
    a data lake using a variety of services on AWS such as **Amazon S3**, **AWS Glue**,
    and **Amazon Athena**. Since we are utilizing *serverless* services with AWS Lake
    Formation, we won’t have to worry about managing any servers while we are setting
    up our data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to how databases work in Redshift and other services such as **Relational
    Database Service** (**RDS**), **AWS Lake Formation** databases can contain one
    or more tables. However, before we create a table, we will need to create a new
    database.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding, make sure that you are using the same region where the S3
    bucket and VPC resources were created. This chapter assumes that we are using
    the `us-west-2`) region. At the same time, please make sure that you are logged
    in as the `mle-ch4-user` IAM user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to create a new database in AWS Lake Formation:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Lake Formation console by typing `lake formation` in the
    search box of the AWS Management Console and then selecting **AWS Lake Formation**
    from the list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Welcome to Lake Formation** popup window, make sure that the **Add
    myself** checkbox is *checked*. Click the **Get started** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the sidebar, locate and click **Databases** under **Data catalog**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Create database** button located in the top right-hand corner of
    the **Databases** page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Under `mle-ch4-db` as the value for the **Name** field. Leave everything else
    as-is and then click the **Create database** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Creating a Lake Formation database ](img/B18638_04_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Creating a Lake Formation database
  prefs: []
  type: TYPE_NORMAL
- en: You should see a success notification stating that your database has been created
    successfully. You may ignore the **Unknown error** message notification shown
    in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The **Unknown error** message is most likely due to the limited permissions
    allowed with the current IAM user being used to perform the actions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have created our Lake Formation database, let’s proceed with creating
    a table using an AWS Glue Crawler.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table using an AWS Glue Crawler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS Glue** is a serverless **extract-transform-load** (**ETL**) service that
    provides different relevant components and capabilities for data integration.
    In this chapter, we will use one of the components of **AWS Glue** – the **AWS
    Glue Crawler**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – How an AWS Glue Crawler works ](img/B18638_04_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – How an AWS Glue Crawler works
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, an **AWS Glue Crawler** processes the files
    stored in the target data stores and then infers a schema based on the structure
    and content of the files processed. This schema is used to create a table or a
    set of tables in **AWS Glue Data Catalog**. These tables can then be used by services
    such as **Amazon Athena** when querying data directly in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these in mind, let’s proceed with creating an AWS Glue Crawler:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Tables** list page by clicking **Tables** from the sidebar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, click the **Create table using a crawler** button (located in the top-left
    corner of the page). This will open **AWS Glue Console** in a new tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Crawlers** (**legacy**), as highlighted in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Navigating to the Crawlers page ](img/B18638_04_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Navigating to the Crawlers page
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we can navigate to the **Crawlers** page using the sidebar on
    the left-hand side of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Create a new crawler by clicking the **Add crawler** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `mle-ch4-crawler` as the **Crawler name** field’s value. Then, click
    **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Specify crawler source type** page, choose **Data
    stores** for **Crawler source type**. Under **Repeat crawls of S3 data stores**,
    select **Crawl all folders**. Then, click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Add crawler** > **Add a data store** page, click the folder icon to
    set the S3 path location for the **Include path** field. This should open the
    **Choose S3 path** popup window, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Choose S3 path ](img/B18638_04_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Choose S3 path
  prefs: []
  type: TYPE_NORMAL
- en: Locate and toggle the checkbox for the `unloaded` folder inside the S3 bucket
    we created in the *Preparing the essential prerequisites* section of this chapter.
    Click the **Select** button afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: If you skipped the *Getting started with Redshift Serverless* section of this
    chapter, you may create an empty `unloaded` folder in the S3 bucket and then upload
    the `synthetic.bookings.100000.csv` file to the `unloaded` folder. You may manually
    do this using the AWS Management Console or by using `input` folder of the S3
    bucket with the AWS CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Set the `100`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure that the configuration you set on the **Add a data store** page is
    similar to what we have in the following screenshot before proceeding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.31 – Adding a data store ](img/B18638_04_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – Adding a data store
  prefs: []
  type: TYPE_NORMAL
- en: Once you have reviewed the data store configuration, click **Next**.
  prefs: []
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Add another data store** page, select the **No**
    option. Click the **Next** button afterward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the `ch4-iam` as the input field value under `AWSGlueServiceRole-ch4-iam`.
    After that, click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Create a schedule for this crawler** page, choose
    **Run on-demand** from the list of dropdown options under **Frequency**. Click
    the **Next** button afterward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Add crawler** > **Configure the crawler’s output** page, choose the
    database we have created (for example, **mle-ch4-db**) from the list of dropdown
    options under **Database**. Click the **Next** button afterward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Finish** to create the AWS Glue crawler using the specified configuration
    parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s run the Crawler by navigating to the **Crawlers** page (the new interface
    / NOT the **Legacy** pages), selecting the crawler, and then clicking the **Run**
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Running the crawler ](img/B18638_04_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Navigating to the Crawlers page
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step may take about 1 to 3 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the **Lake Formation** console (using the search box).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to the `unloaded` table that was generated by our AWS Glue crawler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Refreshing the Tables list page ](img/B18638_04_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – Refreshing the Tables list page
  prefs: []
  type: TYPE_NORMAL
- en: After clicking the refresh button, you should see the `unloaded` table in the
    list of tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click the **unloaded** link to navigate to the **Table details** page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.34 – Table details and Schema of the unloaded table ](img/B18638_04_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – Table details and Schema of the unloaded table
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, we should see the **Table details** and
    **Schema** information.
  prefs: []
  type: TYPE_NORMAL
- en: Open the **Actions** drop-down menu and select **View data** from the list of
    options. This should open the **Preview data** pop-up window, informing us that
    we will be taken to the Athena console. Click the **OK** button to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* Note that we are just scratching the surface of what we
    can do with **AWS Glue**. For more information, check out [https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml](https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Athena to query data in Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Athena** is a serverless query service that allows us to use SQL statements
    to query data from files stored in S3\. With Amazon Athena, we don’t have to worry
    about infrastructure management and it scales automatically to handle our queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.35 – How Amazon Athena works ](img/B18638_04_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.35 – How Amazon Athena works
  prefs: []
  type: TYPE_NORMAL
- en: If you were to set this up yourself, you may need to set up an EC2 instance
    cluster with an application such as **Presto**. In addition to this, you will
    need to manage the overall cost, security, performance, and stability of this
    EC2 cluster setup yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the query result location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the **Before you run your first query, you need to set up a query result
    location in Amazon S3** notification appears on the **Editor** page, this means
    that you must make a quick configuration change on the Amazon Athena **Settings**
    page so that Athena can store the query results in a specified S3 bucket location
    every time there’s a query. These query results are then displayed in the UI in
    the Athena console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to set up the query result location where our Amazon Athena
    queries will be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the **Before you run your first query, you need to set up a query
    result location in Amazon S3** notification, click **View settings** to navigate
    to the **Settings** page. Otherwise, you may click the **Settings** tab, as shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: f
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.36 – Navigating to the Settings tab ](img/B18638_04_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.36 – Navigating to the Settings tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Manage** located in the right-hand corner of the **Query result and
    encryption settings** pane:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.37 – Managing the query result and encryption settings ](img/B18638_04_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.37 – Managing the query result and encryption settings
  prefs: []
  type: TYPE_NORMAL
- en: Under **Query result location and encryption** in **Manage settings**, click
    **Browse S3** and locate the S3 bucket you created in this chapter. Toggle on
    the radio button and click the **Choose** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the **Save** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have finished configuring the query result location for Amazon Athena,
    we can start running our queries.
  prefs: []
  type: TYPE_NORMAL
- en: Running SQL queries using Athena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With everything ready, we can start using SQL statements to query the data stored
    in S3\. In this section, we’ll inspect our data and run a few queries to check
    for existing data integrity issues as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to query the data stored in the S3 bucket:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate back to the **Editor** page by clicking the **Editor** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Editor** tab, run the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make sure that you click the **Run** button, as highlighted in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.38 – Running the SQL query ](img/B18638_04_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.38 – Running the SQL query
  prefs: []
  type: TYPE_NORMAL
- en: 'This should return a set of results similar to what we have in the following
    screenshot. Note that Amazon Athena may return a different set of results every
    time the same query is run. You may add an `ORDER BY` clause in the query to ensure
    that there is consistency in the results that are returned when using the same
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.39 – Athena query results ](img/B18638_04_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.39 – Athena query results
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that our query was processed in less than half a second. If
    we were to run the same query without the `LIMIT` clause, the run time may be
    more than a second.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance tuning** is outside the scope of this book, but feel free to
    check out [https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)
    for more information on this topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following query to count the number of bookings that were not canceled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us a result of `66987`, which should be the same result we
    got when we performed a similar Redshift Serverless query (in the *Running analytics
    at scale with Amazon Redshift Serverless* section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s list the bookings that were canceled by guests with at least one
    previous cancellation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also review the bookings that were canceled by guests where the number
    of days on the waiting list exceeds 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that we can also check for **data integrity issues** using queries similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using this query, we should be able to list the records where the `booking_changes`
    column value does not match the `has_booking_changes` column value.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a similar note, we can find other records with data integrity concerns using
    the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this query, we should be able to list the records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also create a view that can be referenced by future queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that views do *NOT* contain any data – the query defined in the view runs
    every time the view is referenced by another query.
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, let’s run a sample query that references the view we prepared in
    the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This should give us the list of records where the `total_of_special_requests`
    column value does not match the `has_special_requests` column value, along with
    the records where the `booking_changes` column value does not match the `has_booking_changes`
    column value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are wondering if we can programmatically query our data in S3 using
    **boto3** (AWS SDK for Python), then the answer is *yes*. We can even generate
    predictions with a deployed ML model directly in SQL statements using Amazon Athena
    and Amazon SageMaker. For more information on this topic, check out [*Chapter
    4*](B18638_04.xhtml#_idTextAnchor079), *Serverless Data Management on AWS*, of
    the book *Machine Learning with Amazon SageMaker Cookbook*. You can also find
    a quick example of how to use Python and boto3 to run Athena and Athena ML queries
    here: [https://bit.ly/36AiPpR](https://bit.ly/36AiPpR).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Wasn’t that easy?* Setting up a **serverless data lake** on AWS is easy, so
    long as we use the right set of tools and services. Before continuing to the next
    chapter, make sure that you review and delete all the resources that you created
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were able to take a closer look at several AWS services
    that help enable serverless data management in organizations. When using **serverless**
    services, we no longer need to worry about infrastructure management, which helps
    us focus on what we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: We were able to utilize **Amazon Redshift Serverless** to prepare a serverless
    data warehouse. We were also able to use **AWS Lake Formation**, **AWS Glue**,
    and **Amazon Athena** to create and query data from a serverless data lake. With
    these *serverless* services, we were able to load and query data in just a few
    minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information on the topics that were covered in this chapter, feel
    free to check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Security best practices for your VPC* ([https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing Amazon Redshift Serverless* ([https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/](https://aws.amazon.com/blogs/aws/introducing-amazon-redshift-serverless-run-analytics-at-any-scale-without-having-to-manage-infrastructure/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security in AWS Lake Formation* ([https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml](https://docs.aws.amazon.com/lake-formation/latest/dg/security.xhtml))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
