<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer189">
<h1 class="chapterNumber">7</h1>
<h1 class="chapterTitle" id="_idParaDest-156">Deep Learning, Generative AI, and LLMOps</h1>
<p class="normal">The world is changing. Fast. At the time of writing in mid-2023, <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) and <strong class="keyWord">artificial intelligence</strong> (<strong class="keyWord">AI</strong>) have entered the public consciousness<a id="_idIndexMarker835"/> in a way that even a few months ago seemed<a id="_idIndexMarker836"/> impossible. With the rollout of ChatGPT in late 2022, as well as a wave of new tools from labs and organizations across the world, hundreds of millions of people are now using ML solutions every day to create, analyze, and develop. On top of this, innovation seems to only be speeding up, with what seems like a new announcement of a record-beating model or new tool every day. ChatGPT<a id="_idIndexMarker837"/> is only one example of a solution that uses what is now known as <strong class="keyWord">generative artificial intelligence</strong> (<strong class="keyWord">generative AI or GenAI</strong>). While ChatGPT, Bing AI, and Google Bard are examples of text-based generative AI tools, there is also DALL-E and Midjourney in the image space and now a whole suite of multi-modal models combining these and other types of data. Given the complexity of the ecosystem that is evolving and the models that are being developed by leading AI labs around the world, it would be easy to feel overwhelmed. But fear not, as this chapter is all about answering the question, “What does this mean for me as a budding ML engineer?”</p>
<p class="normal">In this chapter, we will take the same strategy as in the other chapters of the book and focus on the core concepts and on building solid foundations that you can use in your own projects for years to come. We will start with the fundamental algorithmic approach that has been at the heart of many cutting-edge<a id="_idIndexMarker838"/> developments in ML since the 2010s with a review of <strong class="keyWord">deep learning</strong>. We will then discuss how you can build and host your own deep learning models before moving on to GenAI, where we will explore the general landscape before going into a deep dive into the approach behind ChatGPT and other powerful text models, <strong class="keyWord">Large Language Models</strong> (<strong class="keyWord">LLMs</strong>).</p>
<p class="normal">This will then transition smoothly into an exploration<a id="_idIndexMarker839"/> of how ML engineering and MLOps can be applied to LLMs, including a discussion of the new challenges this brings. This is such a new area that much of what we will discuss in this chapter will reflect my views and understanding at the time of writing. As an ML community, we are only now starting to define what best practice means for these models, so we are going to be contributing to this brave new world together in the next few pages. I hope you enjoy the ride!</p>
<p class="normal">We will cover all of this in the following sections:</p>
<ul>
<li class="bulletList">Going deep with deep learning</li>
<li class="bulletList">Going big with LLMs </li>
<li class="bulletList">Building the future with LLMOps</li>
</ul>
<h1 class="heading-1" id="_idParaDest-157">Going deep with deep learning</h1>
<p class="normal">In this book, we have worked<a id="_idIndexMarker840"/> with relatively “classical” ML models so far, which rely on a variety of different mathematical and statistical approaches to learn from data. These algorithms in general are not modeled on any biological theory of learning and are at their heart motivated by finding procedures to explicitly optimize the loss function in different ways. A slightly different approach that the reader will likely be aware of, and that we met briefly in the section on <em class="italic">Learning about learning</em> in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, is that taken by <strong class="keyWord">Artificial Neural Networks</strong> (<strong class="keyWord">ANNs</strong>), which originated in the 1950s and were based<a id="_idIndexMarker841"/> on idealized models of neuronal activity in the brain. The core concept of an ANN is that through connecting relatively simple<a id="_idIndexMarker842"/> computational units called neurons or nodes (modeled on biological neurons), we can build systems that can effectively model any mathematical function (see the information box below for more details). The neuron in this case is a small component of the system that will return an output based on an input and the transformation of that input using some pre-determined mathematical formula. They are inherently non-linear and when acting in combination can very quickly begin to model quite complex data. Artificial neurons can be thought of as being arranged in layers, where neurons from one layer have connections to neurons in the next layer. At the level of small neural networks with not many neurons and not many layers, many of the techniques we have discussed in this book around retraining and drift detection still apply without modification. When we get to ANNs with many layers and neurons, so-called <strong class="keyWord">Deep Neural Networks</strong> (<strong class="keyWord">DNNs</strong>), then we have to consider some additional<a id="_idIndexMarker843"/> concepts, which we will cover in this section.</p>
<div class="note">
<p class="normal">The ability of neural networks to represent<a id="_idIndexMarker844"/> a huge variety of functions has a theoretical basis in what are known as <strong class="keyWord">Universal Approximation Theorems</strong>. These are rigorous mathematical results that prove that multilayer neural networks can approximate classes of mathematical functions to arbitrary levels of precision. These results don’t say which specific neural networks will do this, but they tell us that with enough hidden neurons or nodes, we can be sure that with enough data we should be able to represent our target function. Some of the most important results for these theorems were established in the late 1980s in papers like <em class="italic">Hornik, K., Stinchcombe, M. and White, H. (1989) “Multilayer feedforward networks are universal approximators”, Neural Networks, 2(5), pp. 359–366</em> and <em class="italic">Cybenko, G. (1989) “Approximation by superpositions of a sigmoidal function”, Mathematics of Control, Signals, and Systems, 2(4), pp. 303–314</em>.</p>
</div>
<p class="normal">DNNs have taken the world by storm in the last few years. From computer vision to natural language processing and from StableDiffusion to ChatGPT, there are now countless amazing examples of DNNs doing what was once considered the sole purview of humans. The in-depth mathematical details of deep learning models are covered in so much literature elsewhere, such as in the classic <em class="italic">Deep Learning</em> by Goodfellow, Bengio, Courville, MIT Press, 2016, that we would never be able to do them justice here. Although covering the detailed theory is far beyond the scope of this chapter, I will attempt to provide an overview of the main concepts and techniques you need in order to have a good working knowledge and to be able to start using these models in your ML engineering projects.</p>
<p class="normal">As mentioned, ANNs are based on ideas borrowed<a id="_idIndexMarker845"/> from biology, and just like in a biological brain, the ANN is built up of many individual <em class="italic">neurons</em>. Neurons can be thought of as providing the unit of computation in the ANN. The neuron works by taking multiple inputs and then combining them in a specified recipe to produce a single output, which can then act as one of the inputs for another neuron or as part of the overall model’s output. The inputs to the neurons in a biological setting flow along <em class="italic">dendrites</em> and the outputs are channeled along <em class="italic">axons</em>.</p>
<p class="normal">But how do the inputs get transformed into the outputs? There are a few concepts we need to bring together to understand this process.</p>
<ul>
<li class="bulletList"><strong class="keyWord">Weight</strong>: Assigned to each connection<a id="_idIndexMarker846"/> between the neurons in the network is a numerical value that can be thought of as the “strength” of the connection. During the training of the neural network, the weights are one of the sets of values that are altered to minimize the loss. This is in line with the explanation of model training provided in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>.</li>
<li class="bulletList"><strong class="keyWord">Bias</strong>: Every neuron in the network<a id="_idIndexMarker847"/> is given an another parameter that acts as an offset to the activation (defined below). This number is also updated during training and it gives the neural network more <em class="italic">degrees of freedom</em> to fit to the data. You can think of the bias as shifting the level at which the neuron will “fire” (or produce a certain output) and so having this as a variable value means there is more adaptability to the neuron.</li>
<li class="bulletList"><strong class="keyWord">Inputs</strong>: These can be thought of as the raw data<a id="_idIndexMarker848"/> points that are fed to the neuron before we take into account the weights or the bias. If the neuron is being fed features based on the data, then the inputs are the feature values; if the neuron is being fed outputs from other neurons, then those are the values in that case.</li>
<li class="bulletList"><strong class="keyWord">Activation</strong>: The neuron in an ANN receives multiple inputs; the activation<a id="_idIndexMarker849"/> is the linear combination of the inputs multiplied by the appropriate weights plus the bias term. This translates the multiple pieces of incoming data into a single number that can then be used to determine what the neuron’s output should be.</li>
<li class="bulletList"><strong class="keyWord">Activation function</strong>: The activation is just a number, but the activation function is how we decide<a id="_idIndexMarker850"/> what that number means for the neuron. There is a variety of activation functions that are very popular in deep learning today but the important characteristic is that when this function acts on the activation value, it produces a number that is the output of the neuron or node.</li>
</ul>
<p class="normal">These concepts are brought<a id="_idIndexMarker851"/> together diagrammatically in <em class="italic">Figure 7.1</em>. Deep learning models do not have a strict definition, but for our purposes, we can consider an ANN as deep as soon as it consists of three or more layers. This then means we must define some of the important characteristics of these layers, which we will do now:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Input layer</strong>: This is the first layer of neurons<a id="_idIndexMarker852"/> that has as its input the raw data or prepared features created from the data.</li>
<li class="bulletList"><strong class="keyWord">Hidden layers</strong>: These are the layers between the input<a id="_idIndexMarker853"/> and output layers, and can be thought of as where the main bulk of non-linear transformations of the data are performed. This is often simply because there are lots of hidden layers with lots of neurons! The way the neurons in the hidden layers are organized and connected are key parts of the <em class="italic">neural network architecture</em>.</li>
<li class="bulletList"><strong class="keyWord">Output layer</strong>: The output layer is the one responsible<a id="_idIndexMarker854"/> for translating the outcome of the transformations that have been carried out in the neural network into a result that can be interpreted appropriately for the problem at hand. As an example, if we are using a neural network to classify an image, we need the final layer to output either a 1 or 0 for the specified class or we could have it output a series of probabilities for different classes.</li>
</ul>
<p class="normal">These concepts are useful<a id="_idIndexMarker855"/> background, but how do we start working with them in Python? The two most popular deep learning frameworks in the world are Tensorflow, released by Google Brain in 2015, and PyTorch, released by Meta AI in 2016. In this chapter, we will focus on examples using PyTorch, but many of the concepts apply equally well to TensorFlow with some modifications.</p>
<p class="packt_figref"><img alt="" height="471" role="presentation" src="../Images/B19525_07_01.png" width="825"/></p>
<p class="packt_figref">Figure 7.1: A schematic representation of a “neuron” in an ANN and how this takes input data, x, and transforms it into output, y.</p>
<h2 class="heading-2" id="_idParaDest-158">Getting started with PyTorch</h2>
<p class="normal">First, if you haven’t already, install PyTorch. You can<a id="_idIndexMarker856"/> do this by following<a id="_idIndexMarker857"/> the PyTorch documentation at <a href="https://pytorch.org/get-started/locally/"><span class="url">https://pytorch.org/get-started/locally/</span></a>, for installing locally on Macbook, or use:</p>
<pre class="programlisting con"><code class="hljs-con">pip3 install torch 
</code></pre>
<p class="normal">There are some importance concepts and features of the PyTorch API that are useful to bear in mind when using PyTorch:</p>
<ul>
<li class="bulletList"><code class="inlineCode">torch.Tensor</code>: Tensors are mathematical objects <a id="_idIndexMarker858"/>that can be represented by multi-dimensional arrays and are core components of any modern deep learning framework. The data we feed into the network should be cast as a tensor, for example:
        <pre class="programlisting code"><code class="hljs-code">inputs = torch.tensor(X_train, dtype=torch.float32)
labels = torch.tensor(y_train, dtype=torch.long)
</code></pre>
</li>
<li class="bulletList"><code class="inlineCode">torch.nn</code>: This is the main module used to define<a id="_idIndexMarker859"/> our neural network models. For example, we can use this to define a basic classification neural network<a id="_idIndexMarker860"/> containing three hidden layers, each with a <strong class="keyWord">Rectified Linear Unit</strong> (<strong class="keyWord">ReLU</strong>) activation function. When defining a model in PyTorch using this method, you should<a id="_idIndexMarker861"/> also write a method called <code class="inlineCode">forward</code>, which defines how data is passed through the network during training. The following code shows how you can build a basic neural network inside a class that inherits from the <code class="inlineCode">torch.nn.Module</code> object. This network has four linear layers with ReLU activation functions and a simple forward-pass function:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-keyword">class</span> <span class="hljs-title">NeuralNetwork</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">super</span>(NeuralNetwork, self).__init__()
        self.sequential = nn.Sequential(
            nn.Linear(<span class="hljs-number">13</span>, <span class="hljs-number">64</span>),
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">32</span>),
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>),
            nn.ReLU(),
            nn.Linear(<span class="hljs-number">16</span>, <span class="hljs-number">3</span>)
        )
    <span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):
        x = self.sequential(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
</li>
<li class="bulletList"><strong class="keyWord">Loss functions</strong>: In the <code class="inlineCode">torch.nn</code> module, there is a series of loss functions that can<a id="_idIndexMarker862"/> be used for training the network. A popular choice is the cross-entropy loss, but there are many more to choose from in the documentation:
        <pre class="programlisting code"><code class="hljs-code">criterion = nn.CrossEntropyLoss()
</code></pre>
</li>
<li class="bulletList"><code class="inlineCode">torch.optim.Optimizer</code>: This is the base class for all optimizers<a id="_idIndexMarker863"/> in PyTorch. This allows for the implementation of most of the optimizers discussed in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>. 
    <p class="bulletList">When defining the optimizer in PyTorch, in most cases, you pass in the instantiated model’s parameters and then the relevant parameters for the particular optimizer. For example, if we define an Adam optimizer with a learning rate of <code class="inlineCode">0.001</code>, this is as simple as:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
model = NeuralNetwork()
optimizer = torch.optim.Adam(
        model.parameters(),
        lr=<span class="hljs-number">0.001</span>
)
</code></pre></li>
</ul>
<ul>
<li class="bulletList"><code class="inlineCode">torch.autograd</code>: Recall that training an ML model is really an optimization process that leverages a combination of linear algebra, calculus and some statistics. PyTorch performs model optimization<a id="_idIndexMarker864"/> using <em class="italic">automatic differentiation</em>, a method for converting the problem of finding partial derivatives of a function into the application of a series of primitives that is easy to compute but still results in calculating the differentiation to good precision. This is not to be confused with <em class="italic">finite differences or symbolic differentiation</em>. You call this implicitly by using a loss function and calling the <code class="inlineCode">backward</code> method, which uses autograd to calculate the gradients for the weight updates in each epoch; this is then used in the optimizer by calling <code class="inlineCode">optimizer.step()</code>. During a training run, it is important to reset any input tensors as tensors in PyTorch are mutable (operations change their data), and it is important to reset any gradients calculated in the optimizer as well using <code class="inlineCode">optimizer.zero_grad()</code>. Given this, an example training run with five hundred epochs<a id="_idIndexMarker865"/> would look like the following:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">500</span>):
    running_loss = <span class="hljs-number">0.0</span>
    optimizer.zero_grad()
    
    inputs = torch.tensor(X_train, dtype=torch.float32)
    labels = torch.tensor(y_train, dtype=torch.long)
    
    outputs = net(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
</code></pre>
</li>
<li class="bulletList"><code class="inlineCode">torch.save and torch.load</code>: You can probably guess<a id="_idIndexMarker866"/> what these methods<a id="_idIndexMarker867"/> do from their names! But it is still important to show how to save and load your PyTorch models. When training deep learning models, it is also important to save the model periodically during the training<a id="_idIndexMarker868"/> process, as this often takes a long time. This is called “checkpointing” and means that you can pick up where you left off if anything goes wrong during the training runs. To save a PyTorch checkpoint, we can add syntax like the following to the training loop:
        <pre class="programlisting code"><code class="hljs-code">model_path = <span class="hljs-string">"path/to/model/my_model.pt"</span>
torch.save({
            <span class="hljs-string">'epoch'</span>: epoch,
            <span class="hljs-string">'model_state_dict'</span>: model.state_dict(),
            <span class="hljs-string">'optimizer_state_dict'</span>: optimizer.state_dict(),
            <span class="hljs-string">'loss'</span>: loss,
            }, model_path)
</code></pre>
</li>
<li class="bulletList">To then load in the model, you need to initialize another instance of your neural network class and of an optimizer object before reading in their states from the <code class="inlineCode">checkpoint</code> object:
        <pre class="programlisting code"><code class="hljs-code">model = NeuralNetwork()
optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)
checkpoint = torch.load(model_path)
model.load_state_dict(checkpoint[<span class="hljs-string">'model_state_dict'</span>])
optimizer.load_state_dict(checkpoint[<span class="hljs-string">'optimizer_state_dict'</span>])
epoch = checkpoint[<span class="hljs-string">'epoch'</span>]
loss = checkpoint[<span class="hljs-string">'loss'</span>]
</code></pre>
</li>
<li class="bulletList"><code class="inlineCode">model.eval()</code>and<strong class="keyWord"> </strong><code class="inlineCode">model.train()</code>: Once you have loaded in a PyTorch checkpoint, you need<a id="_idIndexMarker869"/> to set the model to the appropriate mode<a id="_idIndexMarker870"/> for the task you want to perform or there may be downstream issues. For example, if you want to perform testing and validation or you want to perform inference on new data with your model, then you need to call <code class="inlineCode">model.eval()</code> before using it. This freezes any batch normalization or dropout layers you have included as they calculate statistics and perform updates during training that you do not want to be active during testing. Similarly, <code class="inlineCode">model.train()</code> ensures these layers are ready to continue performing updates as expected during a training run.
    <p class="bulletList">It should be noted that there is a more extreme setting than <code class="inlineCode">model.eval()</code> where you can entirely turn off any autograd functionality in your context by using the following syntax:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> torch.inference_mode():
</code></pre>
<p class="normal">This can give you added performance on inference but should only be used if you are certain that you do not need any gradient or tensor updates tracked or performed.</p> </li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Evaluation</strong>: If you wanted to test the model we have just<a id="_idIndexMarker871"/> trained in the example above you could calculate an accuracy using something like the syntax below, but any of the methods we have discussed in this book for model validation apply!
        <pre class="programlisting code"><code class="hljs-code">inputs = torch.tensor(X_test, dtype=torch.float32)
labels = torch.tensor(y_test, dtype=torch.long)
outputs = net(inputs)
_, predicted = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)
correct = (predicted == labels).<span class="hljs-built_in">sum</span>().item()
total = labels.size(<span class="hljs-number">0</span>)
accuracy = correct / total
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Accuracy on the test set: %.2f %%'</span> % (<span class="hljs-number">100</span> * accuracy))
</code></pre>
</li>
</ul>
<p class="normal">And with that, you can now build, train, save, load and evaluate your first PyTorch model. We will now discuss how we take this further by considering some of the challenges of taking deep learning models into production.</p>
<h2 class="heading-2" id="_idParaDest-159">Scaling and taking deep learning into production</h2>
<p class="normal">Now we will move<a id="_idIndexMarker872"/> on to how to run deep learning models<a id="_idIndexMarker873"/> in a production system. To do this, we need to consider a few specific points that mark out DNNs from the other classical ML algorithms:</p>
<ul>
<li class="bulletList"><strong class="keyWord">They are data-hungry</strong>: DNNs often require relatively large amounts of data compared to other ML algorithms, due to the fact that they are performing an extremely complex multi-dimensional optimization, with the parameters of each neuron adding degrees of freedom. This means that for you to consider training a DNN from scratch, you have to do the leg work upfront to make sure you have enough data and that it is of a suitable variety to adequately train the model. The data requirements also typically mean that you need to be able to store a lot of data in memory as well, so this often has to be thought about ahead of time.</li>
<li class="bulletList"><strong class="keyWord">Training is more complex</strong>: This point is related to the above but is subtly different. The very complex non-linear optimization problem we are solving means that during training, there are often many ways that the model can “get lost” and reach a sub-optimal local minimum. Techniques like the <em class="italic">checkpointing</em> example we described in the previous section are widespread in the deep learning community as you will often have to stop training at some step when the loss is not going in the right direction or stagnating, roll back, and try something different.</li>
<li class="bulletList"><strong class="keyWord">You have a new choice to make, the model architecture</strong>: DNNs are also very different from classical ML algorithms because now you do not just have to worry about a few hyperparameters, but you also need to decide the architecture or shape of your neural network. This is often a non-trivial exercise and can require detailed knowledge of neural networks. Even if you are working with a standard architecture like the Transformer architecture (see <em class="italic">Figure 7.2</em>), you should still have a solid grasp of what all the components are doing in order to effectively diagnose and resolve any issues. Techniques like automated architecture search as was discussed in <em class="chapterRef">Chapter 3</em> in the section on <em class="italic">Learning about learning</em> can help speed up architecture design but sound foundational knowledge is still important.</li>
<li class="bulletList"><strong class="keyWord">Explainability is inherently harder</strong>: A criticism that has been leveled at DNNs over the past few years is that their results can be very hard to explain. This is to be expected<a id="_idIndexMarker874"/> since the point is very much that DNN<a id="_idIndexMarker875"/> abstracts away a lot of the specifics of any problem into a more abstract approach. This can be fine in many scenarios but has now led to several high-profile cases of DNNs exhibiting undesired behavior like racial or gender bias, which can then be harder to explain and remediate. A challenge also arises in heavily regulated industries, like healthcare or finance, where your organization may have a legal duty to be able to evidence why a specific decision was made. If you used a DNN to help make this decision, this can often be quite challenging.</li>
</ul>
<figure class="mediaobject"><img alt="" height="759" role="presentation" src="../Images/B19525_07_02.png" width="522"/></figure>
<p class="packt_figref">Figure 7.2: The Transformer architecture as originally published in the paper “Attention is all you need” by Google Brain, https://arxiv.org/abs/1706.03762.</p>
<p class="normal">Given all of this, what are some <a id="_idIndexMarker876"/>of the main things we should consider<a id="_idIndexMarker877"/> when using deep learning models for our ML-engineered systems? Well, one of the first things you can do is use existing pre-trained models, rather than train your own. This obviously comes with some risks around ensuring that the model and the data it was fed were of sufficient quality for your application, so always proceed with caution and do your due diligence.</p>
<p class="normal">In many cases, however, this approach is absolutely fine as we may be using a model that has been put through its paces in quite a public way and it may be known to be performant on the tasks we wish to use it for. Furthermore, we may have a use case where we are willing to accept the operational risk of importing and using this pre-existing model, contingent on our own testing. Let us assume we are in such an example now, and we want to build a basic pipeline to summarize some text conversations between clients and employees of a fictional organization. We can do this using an off-the-shelf transformer model, like that shown in <em class="italic">Figure 7.2</em>, from the Hugging Face <code class="inlineCode">transformers</code> library.</p>
<p class="normal">All you need to get started is to know the name of the model you want to download from the Hugging Face model server; in this case, we will use the Pegasus text summarization model. Hugging Face provides a “<code class="inlineCode">pipeline</code>" API to wrap around the model and make it easy to use:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
summarizer = pipeline(<span class="hljs-string">"summarization"</span>, model= <span class="hljs-string">"google/pegasus-xsum"</span>)
</code></pre>
<p class="normal">Performing our first deep learning<a id="_idIndexMarker878"/> model inference is then as easy as just passing<a id="_idIndexMarker879"/> in some inputs to this pipeline. So, for the fictional bot-human interaction described above, we can just pass in some example text and see what this returns. Let’s do this to summarize a fictional conversation between a customer and a chatbot, where the customer is trying to get more information on an order they have placed. The conversation is shown below:</p>
<pre class="programlisting code"><code class="hljs-code">text = <span class="hljs-string">"Customer: Hi, I am looking for some help regarding my recent purchase of a bouquet of flowers. ChatBot: Sure, how can I help you today? Customer: I purchased a bouquet the other day, but it has not arrived. ChatBot: What is the order ID? Customer: 0123456. ChatBot: Please wait while I fetch the details of your order... It doesn't seem like there was an order placed as you described; are you sure of the details you have provided?"</span>
</code></pre>
<p class="normal">We will then feed this conversation into the summarizer <code class="inlineCode">pipeline</code> object, and print the result:</p>
<pre class="programlisting code"><code class="hljs-code">summary = summarizer(text)
<span class="hljs-built_in">print</span>(summary)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">[{'summary_text': 'This is a live chat conversation between a customer and a ChatBot.'}]
</code></pre>
<p class="normal">The result shows that the model has actually given a good summary of the nature of this interaction, highlighting how easy it is to start to do something that would probably have been very difficult or even impossible before the deep learning revolution.</p>
<p class="normal">We have just seen an example<a id="_idIndexMarker880"/> of using a pre-trained transformer model to perform<a id="_idIndexMarker881"/> some specific task, in this case text summarization, without any need for the model to be updated based on exposure to new data. In the next section, we will explore what to do when you want to update the model based on your own data.</p>
<h2 class="heading-2" id="_idParaDest-160">Fine-tuning and transfer learning</h2>
<p class="normal">In the previous section, we showed<a id="_idIndexMarker882"/> how easy it was to get started building solutions<a id="_idIndexMarker883"/> with existing deep learning models if ones could be found that were appropriate to your task. A good question to ask ourselves, however, is “What can I do if these models are not exactly right for my specific problem?” This is where the concepts of <strong class="keyWord">fine-tuning</strong> and <strong class="keyWord">transfer learning</strong> come in. Fine-tuning is when we take an existing deep learning model and then continue training the model on some new data. This means we are not starting from scratch and so can arrive at an optimized network far faster. Transfer learning is when we freeze most of the neural network’s state and retrain the last layer(s) with new data in order to perform some slightly different task or perform the same task in a way that is more appropriate to our problem. In both of these cases, this usually means that we can keep many of the powerful features of the original model, such as its feature representations, but start to adapt it for our specific use case.</p>
<p class="normal">To make this more concrete, we will now walk through an example of transfer learning in action. Fine-tuning can follow a similar process but just does not involve the adaptations to the neural network that we will implement. We will use the Hugging Face <code class="inlineCode">datasets</code> and <code class="inlineCode">evaluate</code> packages<a id="_idIndexMarker884"/> in this example, which will show how we can use a base <strong class="keyWord">Bidirectional Encoder Representations from Transformers</strong> (<strong class="keyWord">BERT</strong>) model and then use transfer learning to create a classifier that will estimate the star rating of reviews written in English on the Multilingual Amazon Reviews Corpus (<a href="https://registry.opendata.aws/amazon-reviews-ml/"><span class="url">https://registry.opendata.aws/amazon-reviews-ml/</span></a>). </p>
<p class="normal"><em class="italic">Figure 7.3</em> shows an example rating from this dataset:</p>
<figure class="mediaobject"><img alt="" height="303" role="presentation" src="../Images/B19525_07_03.png" width="767"/></figure>
<p class="packt_figref">Figure 7.3: This shows an example review and star rating from the Multilingual Amazon Reviews Corpus.</p>
<div class="note">
<p class="normal">Although we have used the BERT model in the following example, there are many variants that will work with the same example, such as DistilBERT or AlBERT, which are smaller models which aim to be quicker to train and to retain most of the performance of the original BERT model. You can play around with all of these, and may even find these models are faster to download due to their reduced size!</p>
</div>
<p class="normal">To start our transfer learning example:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, we can use the <code class="inlineCode">datasets</code> package<a id="_idIndexMarker885"/> to retrieve the dataset. We will use the concept<a id="_idIndexMarker886"/> of “configurations” and “splits” available for Hugging Face datasets, which specify specific subsets of the data and whether you want the train, test, or validate splits of the data. For this case, we want the English reviews and we will use the train split of the data initially. <em class="italic">Figure 7.3</em> shows an example record from the dataset. The data is retrieved with the following syntax:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-keyword">def</span> <span class="hljs-title">fetch_dataset</span>(<span class="hljs-params">dataset_name: </span><span class="hljs-built_in">str</span><span class="hljs-params">=</span><span class="hljs-string">"amazon_reviews_multi"</span><span class="hljs-params">,</span>
<span class="hljs-params">                  configuration: </span><span class="hljs-built_in">str</span><span class="hljs-params">=</span><span class="hljs-string">"en"</span><span class="hljs-params">, split: </span><span class="hljs-built_in">str</span><span class="hljs-params">=</span><span class="hljs-string">"train"</span>
                  ) -&gt; datasets.arrow_dataset.Dataset:
    <span class="hljs-string">'''</span>
<span class="hljs-string">    Fetch dataset from HuggingFace datasets server.</span>
<span class="hljs-string">    '''</span>
    dataset = load_dataset(dataset_name, configuration, split=split)
    <span class="hljs-keyword">return</span> dataset
</code></pre>
</li>
<li class="numberedList">The next step is to tokenize<a id="_idIndexMarker887"/> the dataset. To do this, we will use the <code class="inlineCode">AutoTokenizer</code> that pairs<a id="_idIndexMarker888"/> with the BERT model we will use. Before we pull in that specific tokenizer, let’s write a function that will use the selected tokenizer to transform the dataset. We will also define the logic to take the dataset and make it of the right form for use in later PyTorch processes. I have also added an option to downsample the data for testing:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> typing
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-keyword">def</span> <span class="hljs-title">tokenize_dataset</span>(<span class="hljs-params">tokenizer: AutoTokenizer, </span>
<span class="hljs-params">                     dataset: datasets.arrow_dataset.Dataset,</span>
<span class="hljs-params">                     sample=</span><span class="hljs-literal">True</span>) -&gt; datasets.arrow_dataset.Dataset:
    <span class="hljs-string">'''</span>
<span class="hljs-string">    Tokenize the HuggingFace dataset object and format for use in</span>
<span class="hljs-string">    later Pytorch logic.</span>
<span class="hljs-string">    '''</span>
    tokenized_dataset = dataset.<span class="hljs-built_in">map</span>(
        <span class="hljs-keyword">lambda</span> x: tokenizer(x[<span class="hljs-string">"</span><span class="hljs-string">review_body"</span>], padding=<span class="hljs-string">"max_length"</span>,
                            truncation=<span class="hljs-literal">True</span>),
        batched=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Torch needs the target column to be named "labels"</span>
    tokenized_dataset = tokenized_dataset.rename_column(<span class="hljs-string">"stars"</span>,
                                                         <span class="hljs-string">"labels"</span>)
    
    <span class="hljs-comment"># We can format the dataset for Torch using this method.</span>
    tokenized_dataset.set_format(
        <span class="hljs-built_in">type</span>=<span class="hljs-string">"torch"</span>, columns=[<span class="hljs-string">"</span><span class="hljs-string">input_ids"</span>, <span class="hljs-string">"token_type_ids"</span>,
                                <span class="hljs-string">"attention_mask"</span>, <span class="hljs-string">"labels"</span>]
    )
    <span class="hljs-comment"># Let's downsample to speed things up for testing</span>
    <span class="hljs-keyword">if</span> sample==<span class="hljs-literal">True</span>:
        tokenized_dataset_small = tokenized_dataset.\
                                  shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>))
        <span class="hljs-keyword">return</span> tokenized_dataset_small
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> tokenized_dataset
</code></pre>
</li>
<li class="numberedList">Next, we need to create the PyTorch <code class="inlineCode">dataloader</code> for feeding the data into the model:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-keyword">def</span> <span class="hljs-title">create_dataloader</span>(
<span class="hljs-params">    tokenized_dataset: datasets.arrow_dataset.Dataset,</span>
<span class="hljs-params">    batch_size: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">16</span><span class="hljs-params">,</span>
<span class="hljs-params">    shuffle: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">True</span>
    ):
    dataloader = DataLoader(tokenized_dataset,
                            shuffle=shuffle,
                            batch_size=batch_size)
    <span class="hljs-keyword">return</span> dataloader
</code></pre>
</li>
<li class="numberedList">Before we define<a id="_idIndexMarker889"/> the logic for training<a id="_idIndexMarker890"/> the model, it will be useful to write a helper function for defining the learning scheduler and the optimizer for the training run. This can then be called in our training function, which we will define in the next step. We will use the AdamW optimizer in this example:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

<span class="hljs-keyword">def</span> <span class="hljs-title">configure_scheduler_optimizer</span>(
<span class="hljs-params">    model: typing.</span><span class="hljs-type">Any</span><span class="hljs-params">,</span>
<span class="hljs-params">    dataloader: typing.</span><span class="hljs-type">Any</span><span class="hljs-params">,</span>
<span class="hljs-params">    learning_rate: </span><span class="hljs-built_in">float</span><span class="hljs-params">,</span>
<span class="hljs-params">    num_training_steps: </span><span class="hljs-built_in">int</span>) -&gt; <span class="hljs-built_in">tuple</span>[typing.<span class="hljs-type">Any</span>, typing.<span class="hljs-type">Any</span>]:
    <span class="hljs-string">'''</span>
<span class="hljs-string">    Return a learning scheduler for use in training using the AdamW</span>
<span class="hljs-string">    optimizer</span>
<span class="hljs-string">    '''</span>
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    lr_scheduler = get_scheduler(
        name=<span class="hljs-string">"</span><span class="hljs-string">linear"</span>, 
        optimizer=optimizer, 
        num_warmup_steps=<span class="hljs-number">0</span>, 
        num_training_steps=num_training_steps
    )
    <span class="hljs-keyword">return</span> lr_scheduler, optimizer
</code></pre>
</li>
<li class="numberedList">We can now define the model<a id="_idIndexMarker891"/> that we want to train using transfer<a id="_idIndexMarker892"/> learning. The <code class="inlineCode">transformers</code> library from Hugging Face provides a very helpful wrapper to help you alter the classification head of a neural network based on a core BERT model. We instantiate this model and pass in the number of classes, which implicitly creates an update to the neural network architecture to give the logits for each of the classes upon running a prediction. When running inference, we will then take the class corresponding to the maximum of these logits as the inferred class. First, let’s define the logic for training the model in a function:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-keyword">def</span> <span class="hljs-title">transfer_learn</span>(
<span class="hljs-params">    model: typing.</span><span class="hljs-type">Any</span><span class="hljs-params">, </span>
<span class="hljs-params">    dataloader: typing.</span><span class="hljs-type">Any</span><span class="hljs-params">,</span>
<span class="hljs-params">    learning_rate: </span><span class="hljs-built_in">float</span><span class="hljs-params"> = </span><span class="hljs-number">5e-5</span><span class="hljs-params">,</span>
<span class="hljs-params">    num_epochs: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">3</span><span class="hljs-params">,</span>
<span class="hljs-params">    progress_bar: </span><span class="hljs-built_in">bool</span><span class="hljs-params"> = </span><span class="hljs-literal">True</span><span class="hljs-params"> </span>)-&gt; typing.<span class="hljs-type">Any</span>:
    device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else\</span>
             torch.device(<span class="hljs-string">"cpu"</span>)
    model.to(device)
    
    num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(dataloader)
    lr_scheduler, optimizer = configure_scheduler_optimizer(
        model = model, 
        dataloader = dataloader,
        learning_rate = learning_rate,
        num_training_steps = num_training_steps
    )
    
    <span class="hljs-keyword">if</span> progress_bar:
        progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">pass</span>
    model.train()
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
        <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataloader:
            batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            <span class="hljs-keyword">if</span> progress_bar:
                progress_bar.update(<span class="hljs-number">1</span>)
            <span class="hljs-keyword">else</span>:
                <span class="hljs-keyword">pass</span>
    <span class="hljs-keyword">return</span> model
</code></pre>
</li>
<li class="numberedList">Finally, we can call all of these methods<a id="_idIndexMarker893"/> together to grab the tokenizer, pull in the dataset, transform<a id="_idIndexMarker894"/> it, define the model, configure the learning scheduler and optimizer, and finally perform the transfer learning to create the final model:
        <pre class="programlisting code"><code class="hljs-code">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)
tokenized_dataset = tokenize_dataset(tokenizer=tokenizer,
                                     dataset=dataset, sample=<span class="hljs-literal">True</span>)
dataloader = create_dataloader(tokenized_dataset=tokenized_dataset)
model = AutoModelForSequenceClassification.from_pretrained(
            <span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">6</span>) <span class="hljs-comment"># 0-5 stars</span>
transfer_learned_model = transfer_learn(
    model = model,
    dataloader=dataloader
)
</code></pre>
</li>
<li class="numberedList">We can then evaluate the performance of the model on the test split of the data using the Hugging Face <code class="inlineCode">evaluate</code> package or any method we like. Note that in the example below, we call <code class="inlineCode">model.eval()</code> so that the model is in evaluation mode as discussed previously:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> evaluate

device = torch.device(<span class="hljs-string">"</span><span class="hljs-string">cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else\</span>
<span class="hljs-keyword">   </span> torch.device(<span class="hljs-string">"cpu"</span>)
metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
model.<span class="hljs-built_in">eval</span>()
eval_dataset = fetch_dataset(split=<span class="hljs-string">"test"</span>)
tokenized_eval_dataset = tokenize_dataset(
    tokenizer=tokenizer,dataset=eval_dataset, sample=<span class="hljs-literal">True</span>)
eval_dataloader = create_dataloader(
    tokenized_dataset=tokenized_eval_dataset)
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs = model(**batch)
    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
    metric.add_batch(predictions=predictions,
                      references=batch[<span class="hljs-string">"labels"</span>])
metric.compute()
</code></pre>
<p class="normal">This will return a dictionary with the value of the calculated metric like that shown below:</p>
<pre class="programlisting con"><code class="hljs-con">{'accuracy': 0.8}
</code></pre></li>
</ol>
<p class="normal">And that is how you can use PyTorch and the Hugging Face <code class="inlineCode">transformers</code> library to perform transfer learning.</p>
<p class="normal">The Hugging Face <code class="inlineCode">transformers</code> library<a id="_idIndexMarker895"/> also now provides a very powerful Trainer API<a id="_idIndexMarker896"/> to help you perform fine-tuning in a more abstract way. If we take the same tokenizer and model from the previous examples, to use the Trainer API, we can just do the following:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">When using the Trainer API, you need to define a <code class="inlineCode">TrainingArguments</code> object, which can include hyperparameters and a few other flags. Let’s just accept all of the default values but supply a path for checkpoints to be outputted to:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments 
training_args = TrainingArguments(output_dir=<span class="hljs-string">"trainer_checkpoints"</span>)
</code></pre>
</li>
<li class="numberedList">We can then use the same <code class="inlineCode">evaluate</code> package<a id="_idIndexMarker897"/> we used in the previous example to define a function for calculating<a id="_idIndexMarker898"/> any specified metrics, which we will pass into the main <code class="inlineCode">trainer</code> object:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> evaluate

metric = evaluate.load(<span class="hljs-string">"accuracy"</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions,
                           references=labels)
</code></pre>
</li>
<li class="numberedList">You then define the <code class="inlineCode">trainer</code> object with all the relevant input objects:
        <pre class="programlisting code"><code class="hljs-code">trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
</code></pre>
</li>
<li class="numberedList">You train the model with these specified configurations and objects by calling
        <pre class="programlisting code"><code class="hljs-code">trainer.train().
</code></pre>
</li>
</ol>
<p class="normal">And that is how you perform your own training on an existing model hosted on Hugging Face.</p>
<p class="normal">It is also useful to note that the Trainer API provides<a id="_idIndexMarker899"/> a really nice way to use a tool like <strong class="keyWord">Optuna</strong>, which we met in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, in order to perform hyperparameter optimization. You can do this by specifying an Optuna trial search space:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">optuna_hp_space</span>(<span class="hljs-params">trial</span>):
    <span class="hljs-keyword">return</span> {
        <span class="hljs-string">"learning_rate"</span>: trial.suggest_float(<span class="hljs-string">"learning_rate"</span>, <span class="hljs-number">1e-6</span>, <span class="hljs-number">1e-4</span>,
                                              log=<span class="hljs-literal">True</span>)
    }
</code></pre>
<p class="normal">And then defining a function for initializing the neural network during every state of the hyperparameter search:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">model_init</span>():
    model = AutoModelForSequenceClassification.from_pretrained(
                <span class="hljs-string">"bert-base-cased"</span>, num_labels=<span class="hljs-number">6</span>)
    <span class="hljs-keyword">return</span> model
</code></pre>
<p class="normal">You then just need to pass this into the <code class="inlineCode">Trainer</code> object:</p>
<pre class="programlisting code"><code class="hljs-code">trainer = Trainer(
    model=<span class="hljs-literal">None</span>,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    model_init=model_init,
)
</code></pre>
<p class="normal">Finally, you can then run the hyperparameter search and retrieve the best run:</p>
<pre class="programlisting code"><code class="hljs-code">best_run = trainer.hyperparameter_search(
    n_trials=20, 
    direction="maximize", 
    hp_space=optuna_hp_space
)
</code></pre>
<p class="normal">And that concludes our example<a id="_idIndexMarker900"/> of transfer learning<a id="_idIndexMarker901"/> and fine-tuning of PyTorch deep learning models using the tools from Hugging Face. An important point to note is that both fine-tuning and transfer learning are still training processes and so can still be applied to the model factory methodology that was laid out in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>. For example, when we say “train” in the “train-run” process outlined in <em class="chapterRef">Chapter 3</em>, this may now refer to the fine-tuning or transfer learning of a pre-trained deep learning model.</p>
<p class="normal">As we have covered extensively already, deep learning models can be very powerful tools for solving a variety of problems. One of the trends that has been explored aggressively in recent years by many groups and organizations is the question of what is possible as these models get larger and larger. In the next section, we are going to start answering that question by exploring what happens when deep learning models get extremely large. It is time to enter<a id="_idIndexMarker902"/> the world<a id="_idIndexMarker903"/> of LLMs.</p>
<h1 class="heading-1" id="_idParaDest-161">Living it large with LLMs</h1>
<p class="normal">At the time of writing, GPT-4 has been released<a id="_idIndexMarker904"/> only a few months previously, in March 2023, by OpenAI. This model is potentially the largest ML model ever developed, with a reported one trillion parameters, although OpenAI has not confirmed the exact number. Since then, Microsoft and Google have announced advanced chat capabilities using similarly large models in their product suites and a raft of open-source packages and toolkits have been released. All of these solutions leverage some of the largest neural network models ever developed, LLMs. LLMs<a id="_idIndexMarker905"/> are part of an even wider class of models known as <strong class="keyWord">foundation models</strong>, which span not just text applications but video and audio as well. These models are roughly classified by the author as being too large for most organizations to consider training from scratch. This will mean organizations will either consume these models as third-party services or host and then fine-tune existing models. Solving this integration challenge in a safe and reliable way represents one of the main challenges in modern ML engineering. There is no time to lose, as new models and capabilities seem to be released every day; so let’s go!</p>
<h2 class="heading-2" id="_idParaDest-162">Understanding LLMs</h2>
<p class="normal">The main focus of LLM-based systems is to create human-like responses to a wide range of text-based inputs. LLMs are based on transformer architectures, which we have already met. This enables these models to process input in parallel, significantly reducing the amount of time for training on the same volume of data compared to other deep learning models.</p>
<p class="normal">The architecture of LLMs, as for any transformer, consists of a series of encoders and decoders that leverages self-attention and feed-forward neural networks. </p>
<p class="normal">At a high level, you can think of the encoders as being responsible for processing the input, transforming it into an appropriate<a id="_idIndexMarker906"/> numerical representation, and then feeding this into the decoders, from which the output can be generated. The magic of transformers comes from the use of <strong class="keyWord">self-attention</strong>, which is a mechanism for capturing the contextual relationships<a id="_idIndexMarker907"/> between words in a sentence. This results in <strong class="keyWord">attention vectors</strong> that represent this numerically, and when multiples<a id="_idIndexMarker908"/> of these are being calculated, it is called <strong class="keyWord">multi-headed attention</strong>. Both the encoder and decoder use self-attention mechanisms to capture the contextual dependencies of the input and output sequences.</p>
<p class="normal">One of the most popular transformer-based models used in LLMs is the BERT model. BERT was developed by Google and is a pre-trained model that can be fine-tuned for various natural language tasks.</p>
<p class="normal">Another popular architecture is the <strong class="keyWord">Generative Pre-trained Transformer</strong> (<strong class="keyWord">GPT</strong>), created by OpenAI. The ChatGPT system, released<a id="_idIndexMarker909"/> by OpenAI in November 2022, apparently utilized a third-generation GPT model when it took the world by storm. At the time of writing in March 2023, these models are up to their fourth generation and are incredibly powerful. Although GPT-4 is still relatively new, it is already sparking a heated debate<a id="_idIndexMarker910"/> about the future of AI and whether or not we have reached <strong class="keyWord">artificial general intelligence</strong> (<strong class="keyWord">AGI</strong>). The author does not believe we have, but what an exciting time to be in this space anyway!</p>
<p class="normal">The thing that makes LLMs<a id="_idIndexMarker911"/> infeasible to train anew in every new business context or organization is that they are trained on colossal datasets. GPT-3, which was released in 2020, was trained on almost 500 billion tokens of text. A token in this instance is a small fragment of a word used for the training and inferences process in LLMs, roughly around 4 characters in English. That is a lot of text! The costs for training these models are therefore concomitantly large and even inference can be hugely costly. This means that organizations whose sole focus is not producing these models will likely fail to see the economies of scale and the returns required to justify investing in them at this scale. This is before you even consider the need for specialized skill sets, optimized infrastructure, and the ability to grab all of that data. There are a lot of parallels with the advent of the public cloud several years ago, where organizations no longer had to invest in as much on-premises infrastructure or expertise and instead started to pay on a “what you use” basis. The same thing is now happening with the most sophisticated ML models. This is not to say that smaller, more domain-specific models have been ruled out. In fact, I think that this will remain one of the ways that organizations can leverage their own unique datasets to drive advantage over competitors and build out better products. The most successful teams will be those that combine this approach with the approach from the largest models in a robust way.</p>
<p class="normal">Scale is not the only important component though. ChatGPT and GPT-4 were not only trained on huge amounts of data but they were<a id="_idIndexMarker912"/> also then fine-tuned using a technique called <strong class="keyWord">Reinforcement Learning with Human Feedback</strong> (<strong class="keyWord">RLHF</strong>). During this process, the model is presented with a prompt, such as a conversational question, and generates a series of potential responses. The responses are then presented to a human evaluator who provides feedback on the quality of the<a id="_idIndexMarker913"/> response, usually by ranking them, which is then used to train a <strong class="keyWord">reward model</strong>. This model is then used to fine-tune<a id="_idIndexMarker914"/> the underlying language model through techniques like <strong class="keyWord">Proximal Policy Optimization</strong> (<strong class="keyWord">PPO</strong>). The details of all of this are well beyond the scope of this book but hopefully, you are gaining an intuition for how this is not run-of-the-mill data science that any team can quickly scale up. And since this is the case, we have to learn how to work with these tools as more of a “black box” and consume<a id="_idIndexMarker915"/> them as third-party solutions. We will cover this in the next section.</p>
<h2 class="heading-2" id="_idParaDest-163">Consuming LLMs via API</h2>
<p class="normal">As discussed in the previous<a id="_idIndexMarker916"/> sections, the main change<a id="_idIndexMarker917"/> in our way of thinking as ML engineers who want to interact with LLMs, and foundation models in general, is that we can no longer assume we have access to the model artifact, the training data, or testing data. We have to instead treat the model as a third-party service that we should call out to for consumption. Luckily, there are many tools and techniques for implementing this.</p>
<p class="normal">The next example will show you how<a id="_idIndexMarker918"/> to build a pipeline that leverages LLMs by using the popular <strong class="keyWord">LangChain</strong> package. The name comes from the fact that to leverage the power of LLMs, we often have to chain together many interactions with them with calls to other systems and sources of information. LangChain also provides a wide variety of functionality that is useful when dealing with NLP and text-based applications more generally. For example, there are utilities for text splitting, working with vector databases, document loading and retrieval, and conversational state persistence, among others. This makes it a worthwhile package to check out even in projects where you are not necessarily working with LLMs specifically.</p>
<p class="normal">First, we walk through a basic example of calling the OpenAI API:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Install the <code class="inlineCode">langchain</code> and <code class="inlineCode">openai</code> Python bindings:
        <pre class="programlisting con"><code class="hljs-con">pip install langchain
pip install openai
</code></pre>
</li>
<li class="numberedList">We assume the user has set up an OpenAI account and has access to an API key. You can set this as an environment variable or use a secrets manager for storage, like the one that GitHub provides. We will assume the key is accessible as an environment variable:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
openai_key = os.getenv(<span class="hljs-string">'OPENAI_API_KEY'</span>)
</code></pre>
</li>
<li class="numberedList">Now, in our Python script<a id="_idIndexMarker919"/> or module, we can define<a id="_idIndexMarker920"/> the model we will call using the OpenAI API as accessed via the <code class="inlineCode">langchain</code> wrapper. Here we will work with the <code class="inlineCode">gpt-3.5-turbo</code> model, which is the most advanced of the GPT-3.5 chat models:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> langchain.chat_models <span class="hljs-keyword">import</span> ChatOpenAI
gpt = ChatOpenAI(model_name=<span class="hljs-string">'''gpt-3.5-turbo'''</span>)
</code></pre>
</li>
<li class="numberedList">LangChain then facilitates the building up of pipelines using LLMs via prompt templates, which allow you to standardize how we will prompt and parse the response of the models:
        <pre class="programlisting code"><code class="hljs-code">template = <span class="hljs-string">'''Question: {question}</span>
<span class="hljs-string">              Answer: '''</span>
prompt = PromptTemplate(
  template=template,
  input_variables=[<span class="hljs-string">'question'</span>]
)
</code></pre>
</li>
<li class="numberedList">We can then create our first “chain,” which is the mechanism for pulling together related steps in <code class="inlineCode">langchain</code>. This first chain is a simple one that takes a prompt template and the input to create an appropriate prompt to the LLM API, before returning an appropriately formatted response:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># user question</span>
question = <span class="hljs-string">"Where does Andrew McMahon, author of 'Machine Learning</span>
<span class="hljs-string">            Engineering with Python', work?"</span>
<span class="hljs-comment"># create prompt template &gt; LLM chain</span>
llm_chain = LLMChain(
  prompt=prompt,
  llm=gpt
)
</code></pre>
</li>
<li class="numberedList">You can then run this question and print the result to the terminal as a test:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(llm_chain.run(question))
</code></pre>
<p class="normal">This returns:</p>
<pre class="programlisting con"><code class="hljs-con">As an AI language model, I do not have access to real-time information. However, Andrew McMahon is a freelance data scientist and software engineer based in Bristol, United Kingdom.
</code></pre> </li>
</ol>
<p class="normal">Given that the I am an ML engineer employed by a large bank and am based in Glasgow, United Kingdom, you can see that even the most sophisticated LLMs will get things wrong. This is an example of what we term a <em class="italic">hallucination</em>, where an LLM gives an incorrect but plausible answer. We will return to the topic of when LLMs get things wrong in the section on Building the future with <em class="italic">LLMOps</em>. This is still a good example of building a basic mechanism through which we can programmatically interact with LLMs in a standardized way.</p>
<p class="normal">LangChain also provides<a id="_idIndexMarker921"/> the ability to pull multiple prompts<a id="_idIndexMarker922"/> together using a method in the chain called <code class="inlineCode">generate</code>:</p>
<pre class="programlisting code"><code class="hljs-code">questions = [
  {<span class="hljs-string">'question'</span>: <span class="hljs-string">'''Where does Andrew McMahon, author of 'Machine Learning Engineering with Python', work?'''</span>},
  {<span class="hljs-string">'question'</span>: <span class="hljs-string">'What is MLOps?'</span>},
  {<span class="hljs-string">'question'</span>: <span class="hljs-string">'What is ML engineering?'</span>},
  {<span class="hljs-string">'question'</span>: <span class="hljs-string">'What's your favorite flavor of ice cream?'</span>}
]
<span class="hljs-built_in">print</span>(llm_chain.generate(questions))
</code></pre>
<p class="normal">The response from this series of questions is rather verbose, but here is the first element of the returned object:</p>
<pre class="programlisting con"><code class="hljs-con">generations=[[ChatGeneration(text='As an AI modeler and a data scientist, Andrew McMahon works at Cisco Meraki, a subsidiary of networking giant Cisco, in San Francisco Bay Area, USA.', generation_info=None, message=AIMessage(content='As an AI modeler and a data scientist, Andrew McMahon works at Cisco Meraki, a subsidiary of networking giant Cisco, in San Francisco Bay Area, USA.', additional_kwargs={}))], …]
</code></pre>
<p class="normal">Again, not <em class="italic">quite</em> right. You get the idea though! With some prompt engineering and better conversation design, this could quite easily be a lot better. I’ll leave you to play around and have some fun with it.</p>
<p class="normal">This quick introduction to LangChain and LLMs only scratches the surface, but hopefully gives you enough to fold in calls to these models into your ML workflows. </p>
<p class="normal">Let’s move on to discuss another important way that LLMs<a id="_idIndexMarker923"/> are becoming part of the ML engineering toolkit, as we explore software development <a id="_idIndexMarker924"/>using AI assistants.</p>
<h2 class="heading-2" id="_idParaDest-164">Coding with LLMs</h2>
<p class="normal">LLMs are not only useful for creating<a id="_idIndexMarker925"/> and analyzing natural language; they can also be applied to programming languages. This is the purpose of the OpenAI Codex family of models, which has been trained on millions of code repositories with the aim of being able to produce reasonable-looking and performing code when prompted. Since GitHub Copilot, an AI assistant for coding, was launched, the concept of an AI assistant helping you code has entered the mainstream. Many people have argued that these solutions provide massive productivity boosts and improved enjoyment when executing their own work. GitHub has published some of its own research suggesting that 60-75% of 2,000 developers asked reported less frustration and improved satisfaction when developing software. On a far smaller cohort of 95 developers, with 50 being in the control group, they also showed a speedup when developing an HTTP server in JavaScript with a given specification. I believe there should be much more work done on this topic before we declare that AI coding assistants are obviously making us all happier and faster, but the GitHub survey and test results definitely suggest they are a useful tool to try out. These results are published at <a href="https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/"><span class="url">https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/</span></a>. To this previous point, an interesting pre-print paper on the arXiv by researchers from Stanford University, <code class="inlineCode">arXiv:2211.03622</code> <strong class="keyWord">[cs.CR]</strong>, seems to show that developers using an AI coding assistant based on the OpenAI <code class="inlineCode">codex-davinci-002</code> model were more likely to introduce security vulnerabilities into their code and that users of the model would feel more confident in their work even though it contained these issues! It should be noted that the model they used is relatively old in terms of the family of LLMs that OpenAI offers now, so again more research is needed. This does raise the interesting possibility that AI coding assistants may provide a speed boost but also introduce more bugs. Time will tell. This area has also started to heat up with the introduction of powerful open-source contenders. A particular one to call out is StarCoder, which was developed through a collaboration between Hugging Face and ServiceNow <a href="https://huggingface.co/blog/starcoder"><span class="url">https://huggingface.co/blog/starcoder</span></a>. The one thing that is certain is that these assistants are not going anywhere and they are only going to improve with time. In this section, we will start to explore the possibilities of working with these AI assistants in a variety of guises. Learning to work with AI is likely going to be a critical part of the ML engineering workflow in the near future, so let’s get learning!</p>
<p class="normal">First, when would I want to use an AI coding assistant as an ML engineer? The consensus in the community, and in the GitHub research, looks to be that these assistants help with the development of boilerplate code on established languages, Python among them. They do not seem to be ideal for when you want to do something particularly innovative or different; however, we will explore this also.</p>
<p class="normal">So, how do you actually<a id="_idIndexMarker926"/> work with an AI to help you code? At the time of writing, there seem to be two main methods (but given the pace of innovation, it could be that you are working with an AI through a brain-computer interface in no time; who knows?), each of which has its own advantages and disadvantages:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Direct editor or IDE integration</strong>: In Copilot-supported code editors and IDEs, which at the time of writing include the PyCharm and VS Code environments we have used in this book, then you can enable Copilot to offer autocomplete suggestions for your code as you type it. You can also provide prompt information for the LLM model in your comments in the code. This mode of integration will likely always be there as long as developers are using these environments, but I can foresee a large number of AI assistant offerings in the future.</li>
<li class="bulletList"><strong class="keyWord">Chat interface</strong>: If you are not using Copilot but instead another LLM, for example, OpenAI’s GPT-4, then you will likely need to work in some sort of chat interface and copy and paste relevant information between your coding environment and the chat. This may seem a bit more clunky but is definitely far more versatile as it means you can easily switch between the models of your choice, or even combine multiples. You could actually build your own code to feed in your code with these models if you have the relevant access permissions and APIs to hit, but at that point, you are just redeveloping a tool like Copilot!</li>
</ul>
<p class="normal">We will walk through an example showing both and highlighting how this may potentially help you in your future ML engineering projects.</p>
<p class="normal">If you navigate to the GitHub Copilot web page, you can sign up for an individual subscription for a monthly fee and they offer a free trial. Once you have done that, you can follow the setup instructions for your chosen code editor here: <a href="https://docs.github.com/en/copilot/getting-started-with-github-copilot"><span class="url">https://docs.github.com/en/copilot/getting-started-with-github-copilot</span></a>.</p>
<p class="normal">Once you have this setup, as I have done<a id="_idIndexMarker927"/> for VS Code, then you can start to use Copilot straight away. For example, I opened a new Python file and started typing some typical imports. When I started to write my first function, Copilot kicked in with a suggestion to complete the entire function, as shown in <em class="italic">Figure 7.4</em>.</p>
<figure class="mediaobject"><img alt="" height="341" role="presentation" src="../Images/B19525_07_04.png" width="762"/></figure>
<p class="packt_figref">Figure 7.4: A suggested autocompletion from GitHub Copilot in VS Code.</p>
<p class="normal">As mentioned above, this is not the only way you can provide input to Copilot; you can also use comments to provide more information to the model. In <em class="italic">Figure 7.5</em>, we can see that providing some commentary in a leading comment line helps define the logic we want to be contained in the function.</p>
<figure class="mediaobject"><img alt="" height="241" role="presentation" src="../Images/B19525_07_05.png" width="788"/></figure>
<p class="packt_figref">Figure 7.5: By providing a leading comment, you can help Copilot suggest the logic you want for your code.</p>
<p class="normal">There are a few things that help<a id="_idIndexMarker928"/> get the best out of Copilot that are useful to bear in mind as you use it:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Be very modular</strong>: The more modular you can make your code, the better. We’ve already discussed why this benefits maintenance and quicker development, but here it also helps the Codex model create more appropriate auto-completion suggestions. If your functions are going to be long, complex objects, then the likelihood is that the Copilot suggestion wouldn’t be great.</li>
<li class="bulletList"><strong class="keyWord">Write clear comments</strong>: This is always a good practice of course but it really helps Copilot understand what code you need. It can help to write longer comments at the top of the file describing what you want the solution to do and then write shorter but very precise comments before your functions. The example in <em class="italic">Figure 7.5</em> shows a comment that specifies the way in which I wanted the function to perform the feature preparation, but if the comment simply said “<em class="italic">standardize features</em>,” the suggestion would likely not be as complete.</li>
<li class="bulletList"><strong class="keyWord">Write the interfaces and function signatures</strong>: As in <em class="italic">Figure 7.5</em>, it helps if you start the piece of code off by providing the function signature and the types or the first line of the class definition if this was a class. This acts to prime the model to complete the rest of the code block.</li>
</ul>
<p class="normal">Hopefully, this is enough to get you started on your journey working with AI to build your solutions. I think that as these tools become more ubiquitous, there are going to be many opportunities to use them to supercharge your development workflows.</p>
<p class="normal">Now that we know how to build some pipelines using LLMs and we know how to start leveraging them to aid our own development, we can turn to what I think is one of the most important topics in this field. I also think it is the one with the most unanswered questions and so it is a very exciting one to explore. This all relates to the question of the operational<a id="_idIndexMarker929"/> implications of leveraging LLMs, now being termed <strong class="keyWord">LLMOps</strong>.</p>
<h1 class="heading-1" id="_idParaDest-165">Building the future with LLMOps</h1>
<p class="normal">Given the rise in interest<a id="_idIndexMarker930"/> in LLMs recently, there has been no shortage of people expressing the desire to integrate these models into all sorts of software systems. For us as ML engineers, this should immediately trigger us to ask the question, “What will that mean operationally?” As discussed throughout this book, the marrying together of operations and development of ML systems is termed MLOps. Working with LLMs is likely to lead to its own interesting challenges, however, and so a new term, LLMOps, has arisen to give this sub-field of MLOps some good marketing.</p>
<p class="normal">Is this really any different? I don’t think it is <em class="italic">that</em> different, but should be viewed as a sub-field of MLOps with its own additional challenges. Some of the main challenges<a id="_idIndexMarker931"/> that I see in this area are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Larger infrastructure, even for fine-tuning</strong>: As discussed previously, these models are far too large for typical organizations or teams to consider training their own, so instead teams will have to leverage third-party models, be they open-source or proprietary, and fine-tune them. Fine-tuning models of this scale will still be very expensive and so there will be a higher premium on building very efficient data ingestion, preparation, and training pipelines.</li>
<li class="bulletList"><strong class="keyWord">Model management is different</strong>: When you train your own models, as we showed several times in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, effective ML engineering requires us to define good practices for versioning our models and storing metadata that provide lineage of the experiments and training runs we have gone through to produce these models. In a world where models are more often hosted externally, this is slightly harder to do, as we do not have access to the training data, to the core model artifacts, and probably not even to the detailed model architecture. Versioning metadata will then likely default to the publicly available metadata for the model, think along the lines of <code class="inlineCode">gpt-4-v1.3</code> and similar-sounding names. That is not a lot of information to go on, and so you will likely have to think of ways to enrich this metadata, perhaps with your own example runs and test results in order to understand how that model behaved in certain scenarios. This then also links to the next point.</li>
<li class="bulletList"><strong class="keyWord">Rollbacks become more challenging</strong>: If your model is hosted externally by a third party, you do not control the roadmap of that service. This means that if there is an issue with version 5 of a model and you want to roll back to version 4, that option might not be available to you. This is a different kind of “drift” from the model performance drift we’ve discussed at length in this book but it is going to become increasingly important. This will mean that you should have your own model, perhaps with nowhere near the same level of functionality or scale, ready as a last resort default to switch to in case of issues.</li>
<li class="bulletList"><strong class="keyWord">Model performance is more of a challenge</strong>: As mentioned in the previous point, with foundation models being served as externally hosted services, you are no longer in as much control as you were. This then means that if you do detect any issues with the model you are consuming, be they drift or some other bugs, you are very limited in what you can do and you will need to consider that default rollback we just discussed.</li>
<li class="bulletList"><strong class="keyWord">Applying your own guardrails will be key</strong>: LLMs hallucinate, they get things wrong, they can regurgitate training data, and they might even inadvertently offend the person interacting with them. All of this means that as these models are adopted by more organizations, there will be a growing need to develop methods for applying bespoke guardrails to systems utilizing them. As an example, if an LLM was being used to power a next-generation chatbot, you could envisage that between the LLM service and the chat interface, you could have a system layer that checked for abrupt sentiment changes and important keywords or data that should be obfuscated. This layer<a id="_idIndexMarker932"/> could utilize simpler ML models and a variety of other techniques. At its most sophisticated, it could try and ensure that the chatbot did not lead to a violation of ethical or other norms established by the organization. If your organization has made the climate crisis an area of focus, you may want to screen the conversation in real time for information that goes against critical scientific findings in this area as an example.</li>
</ul>
<p class="normal">Since the era of foundation models has only just begun, it is likely that more and more complex challenges will arise to keep us busy as ML engineers for a long time to come. To me, this is one of the most exciting challenges<a id="_idIndexMarker933"/> we face as a community, how we harness one of the most sophisticated and cutting-edge capabilities ever developed by the ML community in a way that still allows the software to run safely, efficiently, and robustly for users day in and day out. Are you ready to take on that challenge?</p>
<p class="normal">Let’s dive into some of these topics in a bit more detail, first with a discussion of LLM validation.</p>
<h2 class="heading-2" id="_idParaDest-166">Validating LLMs</h2>
<p class="normal">The validation of generative<a id="_idIndexMarker934"/> AI models is inherently different from and seemingly more complex than the same for other ML models. The main reasons for this are that when you are <em class="italic">generating</em> content, you are often creating very complex data in your results that has never existed! If an LLM returns a paragraph of generated text when asked to help summarize and analyze some document, how do you determine if the answer is “good”? If you ask an LLM to reformat some data into a table, how can you build a suitable metric that captures if it has done this correctly? In a generative context, what do “model performance” and “drift” really mean and how do I calculate them? Other questions may be more use case dependent, for example, if you are building an information retrieval or Retrieval-Augmented Generation (see <em class="italic">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>, <a href="https://arxiv.org/pdf/2005.11401.pdf"><span class="url">https://arxiv.org/pdf/2005.11401.pdf</span></a>) solution, how do you evaluate the truthfulness of the text generated by the LLM? </p>
<p class="normal">There are also important considerations around how we screen the LLM-generated outputs for any potential biased or toxic outputs that may cause harm or reputational damage to the organization running the model. The world of LLM validation is complex!</p>
<p class="normal">What can we do? Thankfully, this has not all happened in a vacuum and there have been several benchmarking tools and datasets released that can help us on our journey. Things are so young that there are not many worked examples of these tools yet, but we will discuss the key points so that you are aware of the landscape and can keep on top of how things are evolving. Let’s list some of the higher-profile evaluation frameworks and datasets for LLMs:</p>
<ul>
<li class="bulletList"><strong class="keyWord">OpenAI Evals</strong>: This is a framework whereby<a id="_idIndexMarker935"/> OpenAI allows for the crowdsourced development of tests against proposed text completions generated by LLMs. The core concept at the heart of evals is the “Completion Function Protocol,” which is a mechanism for standardizing the testing of the strings returned when interacting with an LLM. The framework<a id="_idIndexMarker936"/> is available on GitHub at <a href="https://github.com/openai/evals"><span class="url">https://github.com/openai/evals</span></a>.</li>
<li class="bulletList"><strong class="keyWord">Holistic Evaluation of Language Models</strong> (<strong class="keyWord">HELM</strong>): This project, from Stanford University, styles <a id="_idIndexMarker937"/>itself as a “living benchmark” for LLM performance. It gives you a wide variety of datasets, models, and metrics and shows the performance across these different combinations. It is a very powerful resource that you can use to base your own test scenarios on, or indeed just to use the information directly to understand the risks and potential benefits of using any specific LLM<a id="_idIndexMarker938"/> for your use case. The HELM benchmarks are available at <a href="https://crfm.stanford.edu/helm/latest/"><span class="url">https://crfm.stanford.edu/helm/latest/</span></a>.</li>
<li class="bulletList"><strong class="keyWord">Guardrails AI</strong>: This is a Python package<a id="_idIndexMarker939"/> that allows you to do validation on LLM outputs in the same style as <code class="inlineCode">pydantic</code>, which is a very powerful idea! You can also use it to build control flows with the LLM for when issues arise like a response to a prompt not meeting your set criteria; in this case, you can use Guardrails<a id="_idIndexMarker940"/> AI to re-prompt the LLM in the hope of getting a different response. To use Guardrails AI, you specify a <strong class="keyWord">Reliable AI Markup Language</strong> (<strong class="keyWord">RAIL</strong>) file that defines the prompt format and expected behavior in an XML-like file. Guardrails AI<a id="_idIndexMarker941"/> is available on GitHub at <a href="https://shreyar.github.io/guardrails/"><span class="url">https://shreyar.github.io/guardrails/</span></a>.</li>
</ul>
<p class="normal">There are several more of these frameworks<a id="_idIndexMarker942"/> being created all the time, but getting familiar with the core concepts and datasets out there will become increasingly important as more organizations want to take LLM-based systems from fun proofs-of-concept to production solutions. In the penultimate section of this chapter, we will briefly discuss some specific challenges I see around the management of “prompts” when building LLM applications.</p>
<h2 class="heading-2" id="_idParaDest-167">PromptOps</h2>
<p class="normal">When working with generative AI <a id="_idIndexMarker943"/>that takes text inputs, the data we input is often referred to as “prompts” to capture the conversational<a id="_idIndexMarker944"/> origin of working with these models and the concept that an input demands a response, the same way a prompt from a person would. For simplicity, we will call any input data that we feed to an LLM a prompt, whether this is in a user interface or via an API call and irrespective of the nature of the content we provide to the LLM.</p>
<p class="normal">Prompts are often quite different beasts from the data we typically feed into an ML model. They can be effectively freeform, have a variety of lengths, and, in most cases, express the intent for how we want the model to act. In other ML modeling problems, we can certainly feed in unstructured textual data, but this intent piece is missing. This all leads to some important considerations for us as ML engineers working with these models.</p>
<p class="normal">First, the shaping of prompts<a id="_idIndexMarker945"/> is important. The term <strong class="keyWord">prompt engineering</strong> has become popular in the data community recently and refers to the fact that there is often a lot of thought that goes into designing the content and format of these prompts. This is something we need to bear in mind when designing our ML systems with these models. We should be asking questions like “Can I standardize the prompt formats for my application or use case?”, “Can I provide appropriate additional formatting or content on top of what a user or input system provides to get a better outcome?”, and similar questions. I will stick with calling this prompt engineering.</p>
<p class="normal">Secondly, prompts are not your typical ML input, and tracking and managing them is a new, interesting challenge. This challenge is compounded by the fact that the same prompt may give very different outputs for different models, or even with different versions of the same model. We should think carefully<a id="_idIndexMarker946"/> about tracking the lineage of our prompts and the outputs they generate. I term this challenge <strong class="keyWord">prompt management</strong>.</p>
<p class="normal">Finally, we have a challenge that is not necessarily unique to prompts but definitely becomes a more pertinent one if we allow users of a system to feed in their own prompts, for example in chat interfaces. In this case, we need to apply some sort of screening and obfuscation rules to data coming in and coming out of the model to ensure that the model is not “jailbroken” in some way to evade any guardrails. We would also want to guard against adversarial attacks that may be designed to extract training data from these systems, thereby gaining personally identifiable or other critical information that we do not wish to be shared.</p>
<p class="normal">As you begin<a id="_idIndexMarker947"/> to explore this brave new world of LLMOps with the rest of the world, it will be important to keep these prompt-related challenges in mind. We will now conclude the chapter with a brief summary of what we have covered.</p>
<h1 class="heading-1" id="_idParaDest-168">Summary</h1>
<p class="normal">In this chapter, we focused on deep learning. In particular, we covered the key theoretical concepts behind deep learning, before moving on to discuss how to build and train your own neural networks. We walked through examples of using off-the-shelf models for inference and then adapting them to your specific use cases through fine-tuning and transfer learning. All of the examples shown were based on heavy use of the PyTorch deep learning framework and the Hugging Face APIs.</p>
<p class="normal">We then moved on to the topical question of the largest models ever built, LLMs, and what they mean for ML engineering. We explored a little of their important design principles and behaviors before showing how to interact with them in pipelines using the popular LangChain package and OpenAI APIs. We also explored the potential for using LLMs to help with improving software development productivity, and what this will mean for you as an ML engineer.</p>
<p class="normal">We finished the chapter with an exploration of the new topic of LLMOps, which is all about applying the principles of ML engineering and MLOps that we have been discussing throughout this book to LLMs. This covered the core components of LLMOps and also some new capabilities, frameworks, and datasets that can be used to validate your LLMs. We concluded with some pointers on managing your LLM prompts and how the concepts around experiment tracking we covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>, should be translated to apply in this case.</p>
<p class="normal">The next chapter will begin the final section of the book and will cover a detailed end-to-end example where we will build an ML microservice using Kubernetes. This will allow us to apply many of the skills we have learned throguh the book.</p>
<h1 class="heading-1" id="_idParaDest-169">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussion with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/mle"><span class="url">https://packt.link/mle</span></a></p>
<p class="normal"><img alt="" height="177" role="presentation" src="../Images/QR_Code102810325355484.png" width="177"/></p>
</div>
</div></body></html>