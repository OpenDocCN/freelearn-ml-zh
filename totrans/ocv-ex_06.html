<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Learning Object Classification</h1></div></div></div><p>In the previous chapter, we introduced you to the basic concepts of object segmentation and detection. This means isolating the objects that appear in an image for future processing and analysis.</p><p>This chapter covers how to classify each of these isolated objects. In order to allow us to classify each object, we need to train our system to be capable of learning the required parameters to decide which specific label should be assigned to the detected object (depending on the different categories taken into account during the training phase).</p><p>This chapter is going to introduce you to the basic concepts of machine learning to classify images with different labels.</p><p>We will create a basic application based on the segmentation algorithm, as discussed in <a class="link" href="ch05.html" title="Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>. This segmentation algorithm extracts parts of an image, which contains objects. For each object, we will extract the different features and analyze them using a machine learning algorithm. Using a machine learning algorithm, we are able to show, using our user interface, the labels of each object detected in the input image to the end user.</p><p>In this chapter, we will cover the different topics and algorithms, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An introduction to machine learning concepts</li><li class="listitem" style="list-style-type: disc">Common machine learning algorithms and processes</li><li class="listitem" style="list-style-type: disc">Feature extraction</li><li class="listitem" style="list-style-type: disc">Support vector machines</li><li class="listitem" style="list-style-type: disc">Training and prediction</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Introducing machine learning concepts</h1></div></div></div><p>Machine<a id="id239" class="indexterm"/> learning<a id="id240" class="indexterm"/> is an old concept that was defined in 1959 by Arthur Samuel as a <em>field of study that gives computers the ability to learn without being explicitly programmed</em>. Tom. M. Mitchel provided a more formal definition. In this<a id="id241" class="indexterm"/> definition, Tom links the concept of samples or experiences, labels, and performance measurements.</p><div><div><h3 class="title"><a id="note22"/>Note</h3><p>The machine learning definition by Arthur Samuel is referenced from <em>Some Studies in Machine Learning Using the Game of Checkers</em> in the <em>IBM Journal of Research and Development</em> (Volume: 3, Issue: 3), p. 210 and a phrase in <em>The New Yorker</em> and <em>Office Management</em> the same year.</p><p>The more formal definition by Tom. M. Mitchel is referenced from <em>Machine Learning Book</em>, McGray Hill 1997 (<a class="ulink" href="http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html">http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html</a>).</p></div></div><p>Machine learning <a id="id242" class="indexterm"/>involves pattern recognition and the learning theory in artificial intelligence and is related to computational statistics.</p><p>Machine learning is used in hundreds of applications such as <strong>OCR</strong> (<strong>Optical Character Recognition</strong>), spam filtering, search engines, and thousands of Computer Vision applications that we will develop in the current chapter, where a machine learning algorithm tries to classify the objects that appear in the input image.</p><p>Depending on how machine ML algorithms learn from the data or samples, we can divide them into three categories, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Supervised learning</strong>: The <a id="id243" class="indexterm"/>computer learns from a set of labeled data. The goal is to learn the parameters of the model and rules that allow computers to map the relation between data and output label results.</li><li class="listitem" style="list-style-type: disc"><strong>Unsupervised learning</strong>: No<a id="id244" class="indexterm"/> labels are given, and the computer tries to discover the input structure of the input data.</li><li class="listitem" style="list-style-type: disc"><strong>Reinforcement learning</strong>: The <a id="id245" class="indexterm"/>computer interacts with a dynamic environment that performs its goal and learns from its mistakes.</li></ul></div><p>Depending on the desired results that we obtain from our machine learning algorithm, we can categorize them into the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Classification</strong>: In <a id="id246" class="indexterm"/>classification, the space of the inputs can be divided into <em>N</em> classes, and the prediction results of a given sample are one of these training classes. This is one of the most used categories. A typical example is an e-mail spam filtering where there are only two classes: spam and non spam or OCR, where only <em>N</em> characters are available, and each character is one class.</li><li class="listitem" style="list-style-type: disc"><strong>Regression</strong>: The <a id="id247" class="indexterm"/>output is a continuous value instead of a discrete value such as a classification result. One example of regression can be the prediction of the house price by providing the house size, number of years, and location.</li><li class="listitem" style="list-style-type: disc"><strong>Clustering</strong>: The<a id="id248" class="indexterm"/> inputs are divided into <em>N</em> groups using unsupervised training.</li><li class="listitem" style="list-style-type: disc"><strong>Density estimation</strong>: This<a id="id249" class="indexterm"/> finds the (probability) distribution of inputs.</li></ul></div><p>In our example, we will use a supervised learning classification algorithm, where a training dataset (with labels) is used to train the model, and the result of our model is a prediction of one label.</p><p>Machine learning is a modern approach to artificial intelligence and statistics and involves both the techniques.</p><p>In machine learning, there are several approaches and methods, and some of them used are <a id="id250" class="indexterm"/>
<strong>SVM</strong> (<strong>support vector machines</strong>), <strong>ANNs</strong> (<strong>artificial neural networks</strong>), clustering <a id="id251" class="indexterm"/>such as<a id="id252" class="indexterm"/> <strong>K-Nearest Neighbors</strong>, <strong>decision trees,</strong> or deep learning, which is a <a id="id253" class="indexterm"/>big <a id="id254" class="indexterm"/>
<strong>neural network</strong> approach used in some cases that are convolutional, and so on.</p><p>All these methods and approaches are supported, implemented, and well-documented in OpenCV. We are going to explain one of them, SVM, in the next section.</p><p>OpenCV implements <a id="id255" class="indexterm"/>eight of these machine learning algorithms. They all inherit from the <code class="literal">StatModel</code> class:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Artificial neural networks</li><li class="listitem" style="list-style-type: disc">Boost</li><li class="listitem" style="list-style-type: disc">Random trees</li><li class="listitem" style="list-style-type: disc">Expectation maximization</li><li class="listitem" style="list-style-type: disc">K-Nearest Neighbours</li><li class="listitem" style="list-style-type: disc">Logistic regression</li><li class="listitem" style="list-style-type: disc">The Normal Bayes Classifier</li><li class="listitem" style="list-style-type: disc">Support vector machines</li></ul></div><div><div><h3 class="title"><a id="note23"/>Note</h3><p>To get more details of each<a id="id256" class="indexterm"/> algorithm, read the OpenCV document page of machine learning at <a class="ulink" href="http://docs.opencv.org/trunk/dc/dd6/ml_intro.html">http://docs.opencv.org/trunk/dc/dd6/ml_intro.html</a>.</p></div></div><p>In the following image, you can see the machine learning class hierarchy:</p><div><img src="img/B04283_06_01.jpg" alt="Introducing machine learning concepts"/></div><p>The <code class="literal">StatModel</code> class<a id="id257" class="indexterm"/> provides all the <code class="literal">read</code> and <code class="literal">write</code> functions that are very important to save our machine learning parameters and training data.</p><p>In machine learning, the most time-consuming part is the <code class="literal">training</code> method. Training can take from seconds to weeks or months for large datasets and complex machine learning structures; for example, in deep learning and a big neural network structure with more than 100,000 images. In deep learning algorithms, it is common to use parallel hardware processing; for example, GPUs or graphic cards with the CUDA technology used to decrease the computing time during training.</p><p>This means that we cannot train our algorithm each time we run our application, and it's recommended that we save our model after it is trained because all training/prediction parameters of machine learning are saved. Next, when we want to run it in the future, we only need to load/read from our saved model without training anymore if we need to update our model with more data.</p><p>The <code class="literal">StatModel</code> is an<a id="id258" class="indexterm"/> interface that is implemented by each of its implementations. The two key functions are <code class="literal">train</code> and <code class="literal">predict</code>.</p><p>The <code class="literal">train</code> method<a id="id259" class="indexterm"/> is responsible for learning the parameters of the model from a training dataset. The <code class="literal">train</code> function has the following four calls that can be called in four different ways:</p><div><pre class="programlisting">bool train(const Ptr&lt;TrainData&gt;&amp; trainData, int flags=0 );
bool train(InputArray samples, int layout, InputArray responses);
Ptr&lt;_Tp&gt; train(const Ptr&lt;TrainData&gt;&amp; data, const _Tp::Params&amp; p, int flags=0 );
Ptr&lt;_Tp&gt; train(InputArray samples, int layout, InputArray responses, const _Tp::Params&amp; p, int flags=0 );</pre></div><p>It has the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">trainData</code>: This <a id="id260" class="indexterm"/>is the training data that can be loaded or created from the <code class="literal">TrainData</code> class. This class is new in OpenCV 3 and helps developers to create training data because different algorithms require different types of structure of arrays for training and prediction, such as the ANN algorithm.</li><li class="listitem" style="list-style-type: disc"><code class="literal">samples</code>: This<a id="id261" class="indexterm"/> is the array of training array samples such as training data in the format required by the machine learning algorithm.</li><li class="listitem" style="list-style-type: disc"><code class="literal">layout</code>: There <a id="id262" class="indexterm"/>are two types of layouts: <code class="literal">ROW_SAMPLE</code> (training samples are the matrix rows) and <code class="literal">COL_SAMPLE</code> (training samples are the matrix columns).</li><li class="listitem" style="list-style-type: disc"><code class="literal">responses</code>: This<a id="id263" class="indexterm"/> is the vector of responses that is associated with the sample data.</li><li class="listitem" style="list-style-type: disc"><code class="literal">p</code>: This is the <a id="id264" class="indexterm"/><code class="literal">StatModel</code> parameter.</li><li class="listitem" style="list-style-type: disc"><code class="literal">flags</code>: These <a id="id265" class="indexterm"/>are optional flags defined by each method.</li></ul></div><p>The<a id="id266" class="indexterm"/> <code class="literal">predict</code> method<a id="id267" class="indexterm"/> is simpler and has only one call:</p><div><pre class="programlisting">float StatModel::predict(InputArray samples, OutputArray results=noArray(), int flags=0 )</pre></div><p>It has the following parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">samples</code>: These<a id="id268" class="indexterm"/> are the input samples to be predicted. There can be only one or multiple data to be predicted.</li><li class="listitem" style="list-style-type: disc"><code class="literal">results</code>: This<a id="id269" class="indexterm"/> is the result of each input row samples (computed by the algorithm from the previously trained model).</li><li class="listitem" style="list-style-type: disc"><code class="literal">flags</code>: These<a id="id270" class="indexterm"/> are optional flags that are model-dependent. Some models, such as Boost and SVM recognize the <code class="literal">StatModel::RAW_OUTPUT</code> flag, which makes the method return the raw results (the sum) and not the class label.</li></ul></div><p>The<code class="literal"> StatModel</code> class provides other very useful methods, which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">isTrained()</code>: This <a id="id271" class="indexterm"/>returns <code class="literal">true</code> if the model is trained</li><li class="listitem" style="list-style-type: disc"><code class="literal">isClassifier()</code>: This <a id="id272" class="indexterm"/>returns <code class="literal">true</code> if the model is a classifier or <code class="literal">false</code> in the case of regression</li><li class="listitem" style="list-style-type: disc"><code class="literal">getVarCount()</code>: This<a id="id273" class="indexterm"/> returns the number of variables in training samples</li><li class="listitem" style="list-style-type: disc"><code class="literal">save(const string&amp; filename)</code>: This <a id="id274" class="indexterm"/>saves the model in the filename</li><li class="listitem" style="list-style-type: disc"><code class="literal">Ptr&lt;_Tp&gt; load(const string&amp; filename)</code>: This <a id="id275" class="indexterm"/>loads the model from the filename, for example:<code class="literal"> Ptr&lt;SVM&gt; svm = StatModel::load&lt;SVM&gt;("my_svm_model.xml");</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">calcError(const Ptr&lt;TrainData&gt;&amp; data, bool test, OutputArray resp)</code>: This <a id="id276" class="indexterm"/>calculates the error from a test data, where the data is the training data. If the test is <code class="literal">true</code>, the method calculates the error from the test subset of all the training data, otherwise it's computed over the training subset of the data. Finally <code class="literal">resp</code> is the optional output results.</li></ul></div><p>Now, we will learn how to construct a basic application that uses machine learning in Computer Vision apps.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Computer Vision and the machine learning workflow</h1></div></div></div><p>The Computer Vision<a id="id277" class="indexterm"/> applications with machine learning have a common basic structure. This structure is divided into different steps that are repeated in almost all Computer Vision applications, and some others are omitted. In the following diagram, we show you the different steps involved:</p><div><img src="img/B04283_06_02.jpg" alt="Computer Vision and the machine learning workflow"/></div><p>Almost any Computer Vision application starts with a preprocessing stage that is applied to the input image. Preprocessing involves light removal conditions and noise, thresholding, blur, and so on.</p><p>After we apply all the preprocessing steps required to the input image, the second step is segmentation. In the segmentation step, we need to extract the regions of interest of an image and isolate each one as a unique object of interest. For example, in a face detection system, we need to separate the faces from the rest of the parts in the scene.</p><p>After getting <a id="id278" class="indexterm"/>the objects inside the image, we continue with the next step. We need to extract all the features of each one detected object; a feature is a vector of characteristics of objects. A characteristic describes our objects and can be the area of the object, contour, texture pattern, and so on.</p><p>Now, we have the descriptor of our object; a descriptor is a feature that describes an object, and we use these descriptors to train our model or predict one of them. To do this, we need to create a big dataset of features, where hundreds, thousands, and millions of images are preprocessed, and extracted features use all these features in a train model function that we choose:</p><div><img src="img/B04283_06_03.jpg" alt="Computer Vision and the machine learning workflow"/></div><p>When we train a dataset, the <a id="id279" class="indexterm"/>model learns all the parameters required to predict when a new vector of features with an unknown label is given:</p><div><img src="img/B04283_06_04.jpg" alt="Computer Vision and the machine learning workflow"/></div><p>After we get the <a id="id280" class="indexterm"/>prediction, sometimes, a post-processing of output data is required; for example, merging multiple classifications to decrease the prediction error or merging multiple labels. A sample case is <strong>OCR</strong> (<strong>Optical Character Recognition</strong>), where the classification result is per character, and by combining the results of character recognitions, we construct a word. This means that we can create a post-processing method to correct errors in detected words.</p><p>With this small introduction to machine learning for Computer Vision, we will learn how to implement our own application that uses machine learning to classify objects in a slide tape. We will use support vector machines as our classification methods, and see how to use them. The other machine learning algorithms have very similar uses. The OpenCV documentation has a detailed information about all machine learning algorithms.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Automatic object inspection classification example</h1></div></div></div><p>Continuing <a id="id281" class="indexterm"/>with the example of the previous chapter, the automatic object inspection segmentation, where a carrier tape contains three different types of objects (nuts, screws, and rings), and with Computer Vision, we will be able to recognize each one of them to send notifications to a robot or similar to put each one in different boxes.</p><div><img src="img/B04283_06_05.jpg" alt="Automatic object inspection classification example"/></div><p>In <a class="link" href="ch05.html" title="Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection,</em> we preprocessed the input images and extracted the regions of interest of images and isolated each object using different techniques. Now, we will apply all these concepts, as explained in previous sections, in this example to extract features and classify each object and allow to possible robot to put each one in different boxes. In our application, we are only going to show the labels of each image in an image, but we can send the positions in the image and the labels to other devices as a robot.</p><p>Then, our goal <a id="id282" class="indexterm"/>is from an input image with few objects to show the objects' names over each one, as per the following image. However, to learn all the steps of the complete process, we will train our system to show each image that is trained, create a plot to show each object the features that we are going to use with different colors, the preprocessed input image, and finally, the output classification result with the following result:</p><div><img src="img/B04283_06_06.jpg" alt="Automatic object inspection classification example"/></div><p>We will perform the following steps for our example application:</p><div><ol class="orderedlist arabic"><li class="listitem">For training each image:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Preprocess an image</li><li class="listitem" style="list-style-type: disc">Segment an image</li></ul></div></li><li class="listitem">For each object in an image:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extract the features</li><li class="listitem" style="list-style-type: disc">Add the object to the training feature vector with its label</li></ul></div></li><li class="listitem">Create an SVM model.</li><li class="listitem">Train our SVM model with the training feature vector.</li><li class="listitem">Preprocess an input image to be classified.</li><li class="listitem">Segment an input image.</li><li class="listitem">For each object detected:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extract the features</li><li class="listitem" style="list-style-type: disc">Predict with an SVM model</li><li class="listitem" style="list-style-type: disc">Paint the result in an output image</li></ul></div></li></ol></div><p>For the preprocessing <a id="id283" class="indexterm"/>and segmentation stage, we will use the code discussed in <a class="link" href="ch05.html" title="Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>, and we will explain how to extract the features and create the vectors required to train and predict our model.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Feature extraction</h1></div></div></div><p>Now, let's <a id="id284" class="indexterm"/>extract the features of each object. To understand the feature concept of a feature vector, we will extract very simple features, but it is enough to get good results. In other solutions, we can get more complex features, such as texture descriptors, contour descriptors, and so on.</p><p>In our example, we only <a id="id285" class="indexterm"/>have these three types of objects, <em>nuts</em>, <em>rings</em>, and <em>screws</em>, in different possible positions. All these possible objects and positions are shown in the following figure:</p><div><img src="img/B04283_06_07.jpg" alt="Feature extraction"/></div><p>We will explore the good characteristics that will help the computer to identify each object. The characteristics are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The area of an object</li><li class="listitem" style="list-style-type: disc">The aspect ratio, which is the width divided by the height of the bounding rectangle</li><li class="listitem" style="list-style-type: disc">The number of holes</li><li class="listitem" style="list-style-type: disc">The number of contour sides</li></ul></div><p>These <a id="id286" class="indexterm"/>characteristics can describe our objects very well, and if we use all of them, the classification error can be very small. However, in our implemented example, we will use only the first two characteristics, the area and aspect ratio, for learning purposes because we can plot these characteristics in 2D graphics, and we can show that these values describe our objects correctly. We can differentiate one kind of object from the others visually in the graphic plot.</p><p>To extract these features, we will use the black/white input ROI image as the input, where only one object appears in a white color with a black background. This input is the result of segmentation, as discussed in <a class="link" href="ch05.html" title="Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection</em>. We will use the <code class="literal">findCountours</code> algorithm for segmentation objects and create the <code class="literal">ExtractFeatures</code> function for this purpose:</p><div><pre class="programlisting">vector&lt; vector&lt;float&gt; &gt; ExtractFeatures(Mat img, vector&lt;int&gt;* left=NULL, vector&lt;int&gt;* top=NULL)
{
  vector&lt; vector&lt;float&gt; &gt; output;
  vector&lt;vector&lt;Point&gt; &gt; contours;
  Mat input= img.clone();
  
  vector&lt;Vec4i&gt; hierarchy;
  findContours(input, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE);
  // Check the number of objects detected
  if(contours.size() == 0 ){
    return output;
  }
  RNG rng( 0xFFFFFFFF );
  for(int i=0; i&lt;contours.size(); i++){
    
    Mat mask= Mat::zeros(img.rows, img.cols, CV_8UC1);
    drawContours(mask, contours, i, Scalar(1), FILLED, LINE_8, hierarchy, 1);
    Scalar area_s= sum(mask);
    float area= area_s[0];

    if(area&gt;500){ //if the area is greather than min.
      
      RotatedRect r= minAreaRect(contours[i]);
      float width= r.size.width;
      float height= r.size.height;
      float ar=(width&lt;height)?height/width:width/height;

      vector&lt;float&gt; row;
      row.push_back(area);
      row.push_back(ar);
      output.push_back(row);
      if(left!=NULL){
          left-&gt;push_back((int)r.center.x);
      }
      if(top!=NULL){
          top-&gt;push_back((int)r.center.y);
      }
      
      miw-&gt;addImage("Extract Features", mask*255);
      miw-&gt;render();
      waitKey(10);
    }
  }
  return output;
}</pre></div><p>Let's understand the code in detail.</p><p>We will <a id="id287" class="indexterm"/>create a function that has one image as the input and returns two vectors of left and top position for each object detected in the image as parameters; this will be used to draw its label over each object. The output of the function is a vector of vectors of floats; in other words, a matrix where each row contains the features of each object that is detected.</p><p>Let's create a function that draws a label over each other:</p><div><ol class="orderedlist arabic"><li class="listitem">Firstly, we need to create the output vector variable and contours variable that are to be used in our <code class="literal">FindContours</code> algorithm segmentation, and we need to create a copy of our input image because the <code class="literal">findContours</code> OpenCV functions modify the input image:<div><pre class="programlisting">  vector&lt; vector&lt;float&gt; &gt; output;
  vector&lt;vector&lt;Point&gt; &gt; contours;
  Mat input= img.clone();
  vector&lt;Vec4i&gt; hierarchy;
  findContours(input, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE);</pre></div></li><li class="listitem">Now, we can use the <code class="literal">findContours</code> function to retrieve each object in an image. If we don't detect any contour, we return an empty output matrix:<div><pre class="programlisting">if(contours.size() == 0 ){
    return output;
  }</pre></div></li><li class="listitem">For each object <code class="literal">contour</code> we are going to draw in a black image each object using <code class="literal">1</code> as the color value. This is our mask image to compute all features:<div><pre class="programlisting">for(int i=0; i&lt;contours.size(); i++){
Mat mask= Mat::zeros(img.rows, img.cols, CV_8UC1);
    drawContours(mask, contours, i, Scalar(1), FILLED, LINE_8, hierarchy, 1);</pre></div></li><li class="listitem">It's important to use the value <code class="literal">1</code> to draw inside the shape because we can calculate the area by summing all values inside the contour:<div><pre class="programlisting">    Scalar area_s= sum(mask);
    float area= area_s[0];</pre></div></li><li class="listitem">This area is our first feature. Now, we will use this area value as a filter to remove the small objects that we need to avoid. All objects with an area less than a minimum area are discarded. After we pass the filter, we create the second feature, that is, the aspect ratio of an object. This means that the maximum width or height is divided by the minimum width or height. This feature can differentiate the screw from other objects easily:<div><pre class="programlisting">if(area&gt;MIN_AREA){ //if the area is greather than min.
      RotatedRect r= minAreaRect(contours[i]);
      float width= r.size.width;
      float height= r.size.height;
      float ar=(width&lt;height)?height/width:width/height;</pre></div></li><li class="listitem">Now, we<a id="id288" class="indexterm"/> have the features, and we only need to add these features to the output vector. To do this, we create a row vector of floats and add these values, and later on, add this row to the output vector:<div><pre class="programlisting">vector&lt;float&gt; row;
      row.push_back(area);
      row.push_back(ar);
      output.push_back(row);</pre></div></li><li class="listitem">If the left and top <code class="literal">params</code> are passed, then add the top-left values to the <code class="literal">params</code> output:<div><pre class="programlisting">  if(left!=NULL){
          left-&gt;push_back((int)r.center.x);
      }
      if(top!=NULL){
          top-&gt;push_back((int)r.center.y);
      }</pre></div></li><li class="listitem">Finally, we will show the detected objects in a window for the user feedback, and when we finish processing all the objects in the image, we will return the output feature vector:<div><pre class="programlisting">      miw-&gt;addImage("Extract Features", mask*255);
      miw-&gt;render();
      waitKey(10);
    }
  }
  return output;</pre></div></li></ol></div><p>Now, we can extract the features of each input image, and we need to continue with the next step, which is to train our model.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec38"/>Training an SVM model</h2></div></div></div><p>We will <a id="id289" class="indexterm"/>use a supervised learning model, and then, we will require images of each object and their corresponding labels. There are no minimum number of images in the dataset. If we provide more images for the training process, we will get a better classification model (in most of the cases), but simple classifiers can be enough to train simple models. To do this, we create three folders (<code class="literal">screw</code>, <code class="literal">nut</code>, and <code class="literal">ring</code>), where all the images of each type are placed together.</p><p>For each image in the folder, we need to extract the features and add them to the train feature matrix, and at same time, we need to create a new vector with the labels for each row, corresponding to each training matrix.</p><p>To evaluate our system, we split each folder into a number of images for testing and training purposes. We leave around 20 images for testing and the others for training. Then, we need to create two vectors of labels and two matrices for train and test.</p><p>Then, let's understand the code. First, we need to create our model. We need to declare the model in order to be able access it as a global variable. OpenCV uses the <code class="literal">Ptr</code> template class for pointers:</p><div><pre class="programlisting">Ptr&lt;SVM&gt; svm;</pre></div><p>After we declare the pointer to the new SVM model, we need to create it and train it. We create the <code class="literal">trainAndTest</code> function for this purpose:</p><div><pre class="programlisting">void trainAndTest()
{
  vector&lt; float &gt; trainingData;
  vector&lt; int &gt; responsesData;
  vector&lt; float &gt; testData;
  vector&lt; float &gt; testResponsesData;

  int num_for_test= 20;

  // Get the nut images
  readFolderAndExtractFeatures("../data/nut/tuerca_%04d.pgm", 0, num_for_test, trainingData, responsesData, testData, testResponsesData);
  // Get and process the ring images
  readFolderAndExtractFeatures("../data/ring/arandela_%04d.pgm", 1, num_for_test, trainingData, responsesData, testData, testResponsesData);
  // get and process the screw images
  readFolderAndExtractFeatures("../data/screw/tornillo_%04d.pgm", 2, num_for_test, trainingData, responsesData, testData, testResponsesData);
  
  cout &lt;&lt; "Num of train samples: " &lt;&lt; responsesData.size() &lt;&lt; endl;

  cout &lt;&lt; "Num of test samples: " &lt;&lt; testResponsesData.size() &lt;&lt; endl;
  
  // Merge all data 
  Mat trainingDataMat(trainingData.size()/2, 2, CV_32FC1, &amp;trainingData[0]);
  Mat responses(responsesData.size(), 1, CV_32SC1, &amp;responsesData[0]);

  Mat testDataMat(testData.size()/2, 2, CV_32FC1, &amp;testData[0]);
  Mat testResponses(testResponsesData.size(), 1, CV_32FC1, &amp;testResponsesData[0]);
  
    svm = SVM::create();
        svm-&gt;setType(SVM::C_SVC);
        svm-&gt;setKernel(SVM::CHI2);
        svm-&gt;setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));

  svm-&gt;train(trainingDataMat, ROW_SAMPLE, responses);

  if(testResponsesData.size()&gt;0){
    cout &lt;&lt; "Evaluation" &lt;&lt; endl;
    cout &lt;&lt; "==========" &lt;&lt; endl;
    // Test the ML Model
    Mat testPredict;
    svm-&gt;predict(testDataMat, testPredict);
    cout &lt;&lt; "Prediction Done" &lt;&lt; endl;
    // Error calculation
    Mat errorMat= testPredict!=testResponses;
    float error= 100.0f * countNonZero(errorMat) /testResponsesData.size();
    cout &lt;&lt; "Error: " &lt;&lt; error &lt;&lt; "\%" &lt;&lt; endl;
    // Plot training data with error label
    plotTrainData(trainingDataMat, responses, &amp;error);

  }else{
    plotTrainData(trainingDataMat, responses);
  }
}</pre></div><p>Let's understand the code in detail.</p><p>First, we need to <a id="id290" class="indexterm"/>create the required variables to store the training and test data:</p><div><pre class="programlisting">  vector&lt; float &gt; trainingData;
  vector&lt; int &gt; responsesData;
  vector&lt; float &gt; testData;
  vector&lt; float &gt; testResponsesData;</pre></div><p>As mentioned earlier, we need to read all the images from each folder, extract the features, and save them in our training and test data. To do this, we will use the <code class="literal">readFolderAndExtractFeatures</code> function:</p><div><pre class="programlisting">  int num_for_test= 20;
  // Get the nut images
  readFolderAndExtractFeatures("../data/nut/tuerca_%04d.pgm", 0, num_for_test, trainingData, responsesData, testData, testResponsesData);
  // Get and process the ring images
  readFolderAndExtractFeatures("../data/ring/arandela_%04d.pgm", 1, num_for_test, trainingData, responsesData, testData, testResponsesData);
  // get and process the screw images
  readFolderAndExtractFeatures("../data/screw/tornillo_%04d.pgm", 2, num_for_test, trainingData, responsesData, testData, testResponsesData);</pre></div><p>The <code class="literal">readFolderAndExtractFeatures</code> function uses the <code class="literal">VideoCapture</code> OpenCV function to read all the images of a folder like a video or camera. For each image read, we extract the features and then add them to the corresponding output vector:</p><div><pre class="programlisting">bool readFolderAndExtractFeatures(string folder, int label, int num_for_test, 
  vector&lt;float&gt; &amp;trainingData, vector&lt;int&gt; &amp;responsesData, 
  vector&lt;float&gt; &amp;testData, vector&lt;float&gt; &amp;testResponsesData)
{
  VideoCapture images;
  if(images.open(folder)==false){
    cout &lt;&lt; "Can not open the folder images" &lt;&lt; endl;
    return false;
  }
  Mat frame;
  int img_index=0;
  while( images.read(frame) ){
    //// Preprocess image
    Mat pre= preprocessImage(frame);
    // Extract features
    vector&lt; vector&lt;float&gt; &gt; features= ExtractFeatures(pre);
    for(int i=0; i&lt; features.size(); i++){
      if(img_index &gt;= num_for_test){
        trainingData.push_back(features[i][0]);
        trainingData.push_back(features[i][1]);
        responsesData.push_back(label);
      }else{
        testData.push_back(features[i][0]);
        testData.push_back(features[i][1]);
        testResponsesData.push_back((float)label);
      }
    }
    img_index++;
  }
  return true;
}</pre></div><p>After filling all the<a id="id291" class="indexterm"/> vectors with features and labels, we need to convert them to the OpenCV <code class="literal">mat</code> format in order to send them to the <code class="literal">training</code> function:</p><div><pre class="programlisting">// Merge all data 
  Mat trainingDataMat(trainingData.size()/2, 2, CV_32FC1, &amp;trainingData[0]);
  Mat responses(responsesData.size(), 1, CV_32SC1, &amp;responsesData[0]);
  Mat testDataMat(testData.size()/2, 2, CV_32FC1, &amp;testData[0]);
  Mat testResponses(testResponsesData.size(), 1, CV_32FC1, &amp;testResponsesData[0]);</pre></div><p>We are now ready to create and train our machine learning model, as mentioned earlier, and we are going to use a support vector machine. First, we need to set up the basic model parameters:</p><div><pre class="programlisting">// Set up SVM's parameters
  svm = SVM::create();
svm-&gt;setType(SVM::C_SVC);
svm-&gt;setKernel(SVM::CHI2);
svm-&gt;setTermCriteria(TermCriteria(TermCriteria::MAX_ITER, 100, 1e-6));</pre></div><p>We need to define the SVM type and kernel to be used and the criteria to stop the learning process; in our case, we will use a maximum number of iterations, stopping at 100 iterations. For more information on each parameter and what it does, check out the OpenCV documentation. After we create the parameters of the setup, we need to create the model by calling the <code class="literal">train</code> method and using the <code class="literal">trainingDataMat</code> and response matrices:</p><div><pre class="programlisting">  // Train the SVM
  svm-&gt;train(trainingDataMat, ROW_SAMPLE, responses);</pre></div><p>We use the test <a id="id292" class="indexterm"/>vector (by setting the <code class="literal">num_for_test</code> variable greater than <code class="literal">0</code>) to obtain an approximation error of our model. To get the error estimation, we need to predict all the test vector features to obtain the SVM prediction results and then compare these results to the original labels:</p><div><pre class="programlisting">if(testResponsesData.size()&gt;0){
    cout &lt;&lt; "Evaluation" &lt;&lt; endl;
    cout &lt;&lt; "==========" &lt;&lt; endl;
    // Test the ML Model
    Mat testPredict;
    svm-&gt;predict(testDataMat, testPredict);
    cout &lt;&lt; "Prediction Done" &lt;&lt; endl;
    // Error calculation
    Mat errorMat= testPredict!=testResponses;
    float error= 100.0f * countNonZero(errorMat) / testResponsesData.size();
    cout &lt;&lt; "Error: " &lt;&lt; error &lt;&lt; "\%" &lt;&lt; endl;
    // Plot training data with error label
    plotTrainData(trainingDataMat, responses, &amp;error);
  }else{
    plotTrainData(trainingDataMat, responses);
  }</pre></div><p>We use the <code class="literal">predict</code> function using the <code class="literal">testDataMat</code> features and a new <code class="literal">mat</code> to predict results. The <code class="literal">predict</code> function allows you to do multiple predictions at the same time, giving a matrix instead of only one row.</p><p>After the prediction is done, we only need to get the difference of <code class="literal">testPredict</code> using our <code class="literal">testResponses</code> (the original labels). If there are differences, we only need to count the number of differences and divide them by the total number of tests to get the error.</p><div><div><h3 class="title"><a id="note24"/>Note</h3><p>We can use the new <code class="literal">TrainData</code> class to generate the feature vectors and samples and split out train data in test and train vectors.</p></div></div><p>Finally, we need to<a id="id293" class="indexterm"/> show the training data in a 2D plot, where the <em>y</em> axis is the aspect ratio feature and the <em>x</em> axis is the area of objects. Each point has a different color and shape (cross, square, and circle) that shows a different kind of object, and we can clearly see the groups of objects in the following figure:</p><div><img src="img/B04283_06_08.jpg" alt="Training an SVM model"/></div><p>Now, we are very <a id="id294" class="indexterm"/>close to finishing our application sample. We have a trained SVM model that we can use as a classification model to detect the type of a new incoming and unknown feature vector. Then, the next step is to predict an input image with unknown objects.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Input image prediction</h2></div></div></div><p>Now, we <a id="id295" class="indexterm"/>are ready to explain the main function, which loads the input image and predicts the objects that appear inside. We are going to use something like this, as shown in the following figure, as the input image where multiple and different objects appear:</p><div><img src="img/B04283_06_09.jpg" alt="Input image prediction"/></div><p>For all training images, we need to load and preprocess the input image:</p><div><ol class="orderedlist arabic"><li class="listitem">First, we load and convert the images to gray color values.</li><li class="listitem">We then apply the preprocessing tasks, as discussed in <a class="link" href="ch05.html" title="Chapter 5. Automated Optical Inspection, Object Segmentation, and Detection">Chapter 5</a>, <em>Automated Optical Inspection, Object Segmentation, and Detection,</em> using the <code class="literal">preprocessImage</code> function:<div><pre class="programlisting">Mat pre= preprocessImage(img);</pre></div></li><li class="listitem">Now, we extract the features of vectors of all the objects that appear in the image and the top-left positions of each one using the <code class="literal">ExtractFeatures</code> that we mentioned earlier:<div><pre class="programlisting">// Extract features
  vector&lt;int&gt; pos_top, pos_left;
  vector&lt; vector&lt;float&gt; &gt; features= ExtractFeatures(pre, &amp;pos_left, &amp;pos_top);</pre></div></li><li class="listitem">For each <a id="id296" class="indexterm"/>object that we detect, we store it as a feature row, and then, we convert each row as a <code class="literal">Mat</code> of one row and two features:<div><pre class="programlisting">for(int i=0; i&lt; features.size(); i++){
    Mat trainingDataMat(1, 2, CV_32FC1, &amp;features[i][0]);</pre></div></li><li class="listitem">Then, we predict the single object using the <code class="literal">predict</code> function of our <code class="literal">StatModel</code> SVM:<div><pre class="programlisting">float result= svm-&gt;predict(trainingDataMat);</pre></div><p>The float result of the prediction is the label of the object that is detected. Then, to complete the application, we only need to draw the label over each image in an output image.</p></li><li class="listitem">We will use a <code class="literal">stringstream</code> to store the text and a <code class="literal">Scalar</code> to store the color of each different label:<div><pre class="programlisting">stringstream ss;
    Scalar color;
    if(result==0){
      color= green; // NUT
      ss &lt;&lt; "NUT";
    }
    else if(result==1){
      color= blue; // RING
      ss &lt;&lt; "RING" ;
    }
    else if(result==2){
      color= red; // SCREW
      ss &lt;&lt; "SCREW";
    }</pre></div></li><li class="listitem">Draw the label text over each object using its detected position in the <code class="literal">ExtractFeatures</code> function:<div><pre class="programlisting">putText(img_output, 
      ss.str(), 
      Point2d(pos_left[i], pos_top[i]), 
      FONT_HERSHEY_SIMPLEX, 
      0.4, 
      color);</pre></div></li><li class="listitem">Finally, we will draw our results in the output window:<div><pre class="programlisting">  miw-&gt;addImage("Binary image", pre);
  miw-&gt;addImage("Result", img_output);
  miw-&gt;render();
  waitKey(0);</pre></div></li></ol></div><p>The final result <a id="id297" class="indexterm"/>of our application shows a window that is tiled with four screens, where the top-left image is the input training image, the top-right image is the plot training image, the bottom-left image is the input image to analyze preprocessed, and the bottom-right image is the final result of the prediction:</p><div><img src="img/B04283_06_10.jpg" alt="Input image prediction"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Summary</h1></div></div></div><p>In this chapter, we learned the basics of the machine learning model and how to apply a small sample application to understand all the basic tips required to create our own ML application.</p><p>Machine learning is complex and involves different techniques for each use case (supervised learning, unsupervised, clustering, and so on), and we learned how to create the most typical ML application and the supervised learning with an SVM.</p><p>The most important concepts in supervised machine learning are: first, we need to have an appropriate number of samples or datasets; and second, we need to correctly choose the features that describe our objects correctly. For more information on image features, refer to <a class="link" href="ch08.html" title="Chapter 8. Video Surveillance, Background Modeling, and Morphological Operations">Chapter 8</a>, <em>Video Surveillance, Background Modeling, and Morphological Operations</em>. Third, choose the best model that gives us the best predictions.</p><p>If we don't reach the correct predictions we have to check each one of these concepts to look for where the issue is.</p><p>In the next chapter, we will introduce background subtraction methods, which are very useful for video surveillance applications where the backgrounds don't give us any interesting information and must be discarded to allow the segmentation of the interested objects in which to analyze.</p></div></body></html>