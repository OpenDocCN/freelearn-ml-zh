<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer005">
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Preface</h1>
			<p>Self-driving cars will soon be among us. The improvements seen in this field have been nothing short of extraordinary. The first time I heard about self-driving cars, it was in 2010, when I tried one in the Toyota showroom in Tokyo. The ride cost around a dollar. The car was going very slowly, and it was apparently dependent on sensors embedded in the road.</p>
			<p>Fast forward a few years, lidar and advancements in computer vision and deep learning have made that technology look primitive and unnecessarily invasive and expensive.</p>
			<p>In the course of this book, we will use OpenCV for a variety of tasks, including pedestrian detection and lane detection; you will discover deep learning and learn how to leverage it for image classification, object detection, and semantic segmentation, using it to identify pedestrians, cars, roads, sidewalks, and crossing lights, while learning about some of the most influential neural networks.</p>
			<p>You will get comfortable using the CARLA simulator, which you will use to control a car using behavioral cloning and a PID controller; you will learn about network protocols, sensors, cameras, and how to use lidar to map the world around you and to find your position.</p>
			<p>But before diving into these amazing technologies, please take a moment and try to imagine the future in 20 years. What are the cars like? They can drive by themselves. But can they also fly? Are there still crossing lights? How fast, heavy, and expensive are those cars? How do we use them, and how often? What about self-driving buses and trucks?</p>
			<p>We cannot know the future, but it is conceivable that self-driving cars, and self-driving things in general, will shape our daily lives and our cities in new and exciting ways.</p>
			<p>Do you want to play an active role in defining this future? If so, keep reading. This book can be the first step of your journey.</p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>Who this book is for</h1>
			<p>The book covers several aspects of what is necessary to build a self-driving car and is intended for programmers with a basic knowledge of any programming language, preferably Python. No previous experience with deep learning is required; however, to fully understand the most advanced chapters, it might be useful to take a look at some of the suggested reading. The optional source code associated with <a href="B16322_11_Final_NM_ePUB.xhtml#_idTextAnchor250"><em class="italic">Chapter 11</em></a>, <em class="italic">Mapping Our Environments</em>, is in C++.</p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>What this book covers</h1>
			<p><a href="B16322_01_Final_NM_ePUB.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">OpenCV Basics and Camera Calibration</em>, is an introduction to OpenCV and NumPy; you will learn how to manipulate images and videos, and how to detect pedestrians using OpenCV; in addition, it explains how a camera works and how OpenCV can be used to calibrate it.</p>
			<p><a href="B16322_02_Final_NM_ePUB.xhtml#_idTextAnchor046"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding and Working with Signals</em>, describes the different types of signals: serial, parallel, digital, analog, single-ended, and differential, and explains some very important protocols: CAN, Ethernet, TCP, and UDP.</p>
			<p><a href="B16322_03_Final_NM_ePUB.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Lane Detection</em>, teaches you everything you need to know to detect the lanes in a road using OpenCV. It covers color spaces, perspective correction, edge detection, histograms, the sliding window technique, and the filtering required to get the best detection.</p>
			<p><a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, is a practical introduction to neural networks, designed to quickly teach how to write a neural network. It describes neural networks in general and convolutional neural networks in particular. It introduces Keras, a deep learning module, and it shows how to use it to detect handwritten digits and to classify some images.</p>
			<p><a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Deep Learning Workflow</em>, ideally complements <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, as it describes the theory of neural networks and the steps required in a typical workflow: obtaining or creating a dataset, splitting it into training, validation, and test sets, data augmentation, the main layers used in a classifier, and how to train, do inference, and retrain. The chapter also covers underfitting and overfitting and explains how to visualize the activations of the convolutional layers.</p>
			<p><a href="B16322_06_Final_JM_ePUB.xhtml#_idTextAnchor142"><em class="italic">Chapter 6</em></a>, <em class="italic">Improving Your Neural Network</em>, explains how to optimize a neural network, reducing its parameters, and how to improve its accuracy using batch normalization, early stopping, data augmentation, and dropout.</p>
			<p><a href="B16322_07_Final_NM_ePUB.xhtml#_idTextAnchor158"><em class="italic">Chapter 7</em></a>, <em class="italic">Detecting Pedestrians and Traffic Lights</em>, introduces you to CARLA, a self-driving car simulator, which we will use to create a dataset of traffic lights. Using a pre-trained neural network called SSD, we will detect pedestrians, cars, and traffic lights, and we will use a powerful technique called transfer learning to train a neural network to classify the traffic lights according to their colors.</p>
			<p><a href="B16322_08_Final_NM_ePUB.xhtml#_idTextAnchor182"><em class="italic">Chapter 8</em></a>, <em class="italic">Behavioral Cloning</em>, explains how to train a neural network to drive CARLA. It explains what behavioral cloning is, how to build a driving dataset using CARLA, how to create a network that's suitable for this task, and how to train it. We will use saliency maps to get an understanding of what the network is learning, and we will integrate it with CARLA to help it self-drive!</p>
			<p><a href="B16322_09_Final_JM_ePUB.xhtml#_idTextAnchor198"><em class="italic">Chapter 9</em></a>, <em class="italic">Semantic Segmentation</em>, is the final and most advanced chapter about deep learning, and it explains what semantic segmentation is. It details an extremely interesting architecture called DenseNet, and it shows how to adapt it to semantic segmentation.</p>
			<p><a href="B16322_10_Final_NM_ePUB.xhtml#_idTextAnchor221"><em class="italic">Chapter 10</em></a>, <em class="italic">Steering, Throttle, and Brake Control</em>, is about controlling a self-driving car. It explains what a controller is, focusing on PID controllers and covering the basics of MPC controllers. Finally, we will implement a PID controller in CARLA.</p>
			<p><a href="B16322_11_Final_NM_ePUB.xhtml#_idTextAnchor250"><em class="italic">Chapter 11</em></a>, <em class="italic">Mapping Our Environments</em>, is the final chapter. It discusses maps, localization, and lidar, and it describes some open source mapping tools. You will learn what Simultaneous Localization and Mapping (SLAM) is and how to implement it using the Ouster lidar and Google Cartographer.</p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>To get the most out of this book</h1>
			<p>We assume that you have basic knowledge of Python and that you are familiar with the shell of your operating system. You should install Python and possibly use a virtual environment to match the versions of the software used in the book. It is recommended to use a GPU, as training can be very demanding without one. Docker will be helpful for <a href="B16322_11_Final_NM_ePUB.xhtml#_idTextAnchor250"><em class="italic">Chapter 11</em></a>, <em class="italic">Mapping Our Environments</em>.</p>
			<p>Refer to the following table for the software used in the book:</p>
			<div>
				<div id="_idContainer004" class="IMG---Figure">
					<img src="Images/B16322_Preface_Table_NM.jpg" alt="" width="1650" height="317"/>
				</div>
			</div>
			<p>If you are using the digital version of this book, we advise you to type the code yourself or access the code via the GitHub repository (link available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.</p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Download the example code files</h1>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars</a>. In case there's an update to the code, it will be updated on the existing GitHub repository.</p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check them out!</p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Code in Action</h1>
			<p>Code in Action videos for this book can be viewed at <a href="https://bit.ly/2FeZ5dQ">https://bit.ly/2FeZ5dQ</a>.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Download the color images</h1>
			<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here:</p>
			<p><a href="_ColorImages.pdf">https://static.packt-cdn.com/downloads/9781800203587_ColorImages.pdf</a></p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Conventions used</h1>
			<p>There are a number of text conventions used throughout this book.</p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: "Keras offers a method in the model to get the probability, <strong class="source-inline">predict()</strong>, and one to get the label, <strong class="source-inline">predict_classes()</strong>."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">img_threshold = np.zeros_like(channel)</p>
			<p class="source-code">img_threshold [(channel &gt;= 180)] = 255</p>
			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
			<p class="source-code">[default]</p>
			<p class="source-code">exten =&gt; s,1,Dial(Zap/1|30)</p>
			<p class="source-code">exten =&gt; s,2,Voicemail(u100)</p>
			<p class="source-code"><strong class="bold">exten =&gt; s,102,Voicemail(b100)</strong></p>
			<p class="source-code">exten =&gt; i,1,Voicemail(s0)</p>
			<p>Any command-line input or output is written as follows:</p>
			<p class="source-code">/opt/carla-simulator/</p>
			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "The <strong class="bold">reference trajectory</strong> is the desired trajectory of the controlled variable; for example, the lateral position of the vehicle in the lane."</p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Get in touch</h1>
			<p>Feedback from our readers is always welcome.</p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, mention the book title in the subject of your message and email us at <a href="mailto:customercare@packtpub.com">customercare@packtpub.com</a>.</p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <a href="mailto:copyright@packt.com">copyright@packt.com</a> with a link to the material.</p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com">authors.packtpub.com</a>.</p>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Reviews</h1>
			<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
			<p>For more information about Packt, please visit <a href="http://packt.com">packt.com</a>.</p>
		</div>
	</div>



  </body></html>