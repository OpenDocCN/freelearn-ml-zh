<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Natural Language Processing</h1>
                </header>
            
            <article>
                
<p>Natural language processing is a set of machine learning techniques that allows working with text documents, considering their internal structure and the distribution of words. In this chapter, we're going to discuss all common methods to collect texts, split them into atoms, and transform them into numerical vectors. In particular, we'll compare different methods to tokenize documents (separate each word), to filter them, to apply special transformations to avoid inflected or conjugated forms, and finally to build a common vocabulary. Using the vocabulary, it will be possible to apply different vectorization approaches to build feature vectors that can easily be used for classification or clustering purposes. To show how to implement the whole pipeline, at the end of the chapter, we're going to set up a simple classifier for news lines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NLTK and built-in corpora</h1>
                </header>
            
            <article>
                
<p><strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) is a very powerful Python framework that implements most NLP algorithms and will be adopted in this chapter together with scikit-learn. Moreover, NLTK provides some built-in corpora that can be used to test algorithms. Before starting to work with NLTK, it's normally necessary to download all the additional elements (corpora, dictionaries, and so on) using a specific graphical interface. This can be done in the following way:</p>
<pre><strong>import nltk</strong><br/><br/><strong>&gt;&gt;&gt; nltk.download()</strong></pre>
<p>This command will launch the user interface, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="438" width="524" class="image-border" src="assets/b9adf74f-9ab4-4b3f-856e-7aeae1f3c586.png"/></div>
<p>It's possible to select every single feature or download all elements (I suggest this option if you have enough free space) to immediately <span>exploit</span><span> </span><span>all NLTK functionalities.</span></p>
<div class="packt_infobox">NLTK can be installed using pip (<kbd>pip install -U nltk</kbd>) or with one of the binary distributions available at <a href="http://www.nltk.org">http://www.nltk.org</a>. On the same website, there's complete documentation that can be useful for going deeper into each topic.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Corpora examples</h1>
                </header>
            
            <article>
                
<p>A subset of the Gutenberg project is provided and can be freely accessed in this way:</p>
<pre><strong>from nltk.corpus import gutenberg</strong><br/><br/><strong>&gt;&gt;&gt; print(gutenberg.fileids())</strong><br/><strong>[u'austen-emma.txt', u'austen-persuasion.txt', u'austen-sense.txt', u'bible-kjv.txt', u'blake-poems.txt', u'bryant-stories.txt', u'burgess-busterbrown.txt', u'carroll-alice.txt', u'chesterton-ball.txt', u'chesterton-brown.txt', u'chesterton-thursday.txt', u'edgeworth-parents.txt', u'melville-moby_dick.txt', u'milton-paradise.txt', u'shakespeare-caesar.txt', u'shakespeare-hamlet.txt', u'shakespeare-macbeth.txt', u'whitman-leaves.txt']</strong></pre>
<p>A single document can be accessed as a raw version or split into sentences or words:</p>
<pre><strong>&gt;&gt;&gt; print(gutenberg.raw('milton-paradise.txt'))</strong><br/><strong>[Paradise Lost by John Milton 1667] </strong><br/> <br/> <br/><strong>Book I </strong><br/> <br/> <br/><strong>Of Man's first disobedience, and the fruit </strong><br/><strong>Of that forbidden tree whose mortal taste </strong><br/><strong>Brought death into the World, and all our woe, </strong><br/><strong>With loss of Eden, till one greater Man </strong><br/><strong>Restore us, and regain the blissful seat, </strong><br/><strong>Sing, Heavenly Muse, that, on the secret top...</strong><br/><br/><strong>&gt;&gt;&gt; print(gutenberg.sents('milton-paradise.txt')[0:2])</strong><br/><strong>[[u'[', u'Paradise', u'Lost', u'by', u'John', u'Milton', u'1667', u']'], [u'Book', u'I']]</strong><br/><br/><strong>&gt;&gt;&gt; print(gutenberg.words('milton-paradise.txt')[0:20])</strong><br/><strong>[u'[', u'Paradise', u'Lost', u'by', u'John', u'Milton', u'1667', u']', u'Book', u'I', u'Of', u'Man', u"'", u's', u'first', u'disobedience', u',', u'and', u'the', u'fruit']</strong></pre>
<p>As we're going to discuss, in many cases, it can be useful to have the raw text so as to split it into words using a custom strategy. In many other situations, accessing <span>sentences</span><span> </span><span>directly allows working with the original structural subdivision. Other corpora include web texts, Reuters news lines, the Brown corpus, and many more. For example, the Brown corpus is a famous collection of documents divided by genre:</span></p>
<pre class="doctest"><strong><span class="pysrc-keyword">from</span> nltk.corpus <span class="pysrc-keyword">import</span> brown</strong><br/><br/><strong>&gt;&gt;&gt; print(brown.categories())</strong><br/><strong>[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']</strong><br/><br/><strong>&gt;&gt;&gt; print(brown.sents(categories='editorial')[0:100])</strong><br/><strong>[[u'Assembly', u'session', u'brought', u'much', u'good'], [u'The', u'General', u'Assembly', u',', u'which', u'adjourns', u'today', u',', u'has', u'performed', u'in', u'an', u'atmosphere', u'of', u'crisis', u'and', u'struggle', u'from', u'the', u'day', u'it', u'convened', u'.'], ...]</strong></pre>
<div class="packt_infobox">Further information about corpora can be found at <a href="http://www.nltk.org/book/ch02.html">http://www.nltk.org/book/ch02.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The bag-of-words strategy</h1>
                </header>
            
            <article>
                
<p>In NLP, a very common pipeline can be subdivided into the following steps:</p>
<ol>
<li>Collecting a document into a corpus.</li>
<li>Tokenizing, stopword (articles, prepositions and so on) removal, and stemming (reduction to radix-form).</li>
<li>Building a common vocabulary.</li>
<li>Vectorizing the documents.</li>
<li>Classifying or clustering the documents.</li>
</ol>
<p>The pipeline is called <strong>bag-of-words</strong> and will be discussed in this chapter. A fundamental assumption is that the order of each single word in a sentence is not important. In fact, when defining a feature vector, as we're going to see, the measures taken into account are always related to frequencies and therefore they are insensitive to the local positioning of all elements. From some viewpoints, this is a limitation because in a natural language the internal order of a sentence is necessary to preserve the meaning; however, there are many models that can work efficiently with texts without the complication of local sorting. When it's absolutely necessary to consider small sequences, it will be done by adopting groups of tokens (called n-grams) but considering them as a single atomic element during the vectorization step.</p>
<p>In the following figure, there's a schematic representation of this process (without the fifth step) for a sample document (sentence):</p>
<div class="CDPAlignCenter CDPAlign"><img height="208" width="355" class="image-border" src="assets/7a3fb850-b3c2-48b1-9d30-21db8e60009a.png"/></div>
<p class="mce-root">There are many different methods used to carry out each step and some of them are context-specific. However, the goal is always the same: maximizing the information of a document and reducing the size of the common vocabulary by removing terms that are too frequent or derived from the same radix (such as verbs). The information content of a document is in fact determined by the presence of specific terms (or group of terms) whose frequency in the corpus is limited. In the example shown in the previous figure, <strong>fox</strong> and <strong>dog</strong> are important terms, while <strong>the</strong> is useless (often called a <strong>stopword</strong>). Moreover, <strong>jumps</strong> can be converted to the standard form <strong>jump</strong>, which expresses a specific action when present in different forms (like jumping or jumped). The last step is transforming into a numerical vector, because our algorithms work with numbers, and it's important to limit the length of the vectors so as to improve the learning speed and the memory consumption. In the following sections, we're going to discuss each step in detail, and at the end, we're going to build a sample classifier for news lines.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tokenizing</h1>
                </header>
            
            <article>
                
<p>The first step in processing a piece of text or a corpus is splitting it into atoms (sentences, words, or parts of words), normally defined as <strong>tokens</strong>. Such a process is quite simple; however, there can be different strategies to solve particular problems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentence tokenizing</h1>
                </header>
            
            <article>
                
<p>In many cases, it's useful to split large text into sentences, which are normally delimited by a full stop or another equivalent mark. As every language has its own orthographic rules, NLTK offers a method called <kbd>sent_tokenize()</kbd><strong> </strong>that accepts a language (the default is English) and splits the text according to the specific rules. In the following example, we show the usage of this function with different languages:</p>
<pre><strong>from nltk.tokenize import sent_tokenize</strong><br/><br/><strong>&gt;&gt;&gt; generic_text = 'Lorem ipsum dolor sit amet, amet minim temporibus in sit. Vel ne impedit consequat intellegebat.'</strong><br/><br/><strong>&gt;&gt;&gt; print(sent_tokenize(generic_text))</strong><br/><strong>['Lorem ipsum dolor sit amet, amet minim temporibus in sit.',</strong><br/><strong> 'Vel ne impedit consequat intellegebat.']</strong><br/><br/><strong>&gt;&gt;&gt; english_text = 'Where is the closest train station? I need to reach London'</strong><br/><br/><strong>&gt;&gt;&gt; print(sent_tokenize(english_text, language='english'))</strong><br/><strong>['Where is the closest train station?', 'I need to reach London']</strong><br/><br/><strong>&gt;&gt;&gt; spanish_text = u'¿Dónde está la estación más cercana? Inmediatamente me tengo que ir a Barcelona.'</strong><br/><br/><strong>&gt;&gt;&gt; for sentence in sent_tokenize(spanish_text, language='spanish'):</strong><br/><strong>&gt;&gt;&gt;    print(sentence)</strong><br/><strong>¿Dónde está la estación más cercana?</strong><br/><strong>Inmediatamente me tengo que ir a Barcelona.</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word tokenizing</h1>
                </header>
            
            <article>
                
<p>The simplest way to tokenize a sentence into words is provided by the class <kbd>TreebankWordTokenizer</kbd>, which, however, has some limitations:</p>
<pre><strong>from nltk.tokenize import TreebankWordTokenizer</strong><br/><br/><strong>&gt;&gt;&gt; simple_text = 'This is a simple text.'</strong><br/><br/><strong>&gt;&gt;&gt; tbwt = TreebankWordTokenizer()</strong><br/><br/><strong>&gt;&gt;&gt; print(tbwt.tokenize(simple_text))</strong><br/><strong>['This', 'is', 'a', 'simple', 'text', '.']</strong><br/><br/><strong>&gt;&gt;&gt; complex_text = 'This isn\'t a simple text'</strong><br/><br/><strong>&gt;&gt;&gt; print(tbwt.tokenize(complex_text))</strong><br/><strong>['This', 'is', "n't", 'a', 'simple', 'text']</strong><br/><br/></pre>
<p>As you can see, in the first case the sentence has been correctly split into words, keeping the punctuation separate (this is not a real issue because it can be removed in a second step). However, in the complex example, the contraction <kbd>isn't</kbd> has been split into <kbd>is</kbd> and <kbd>n't</kbd>. Unfortunately, without a further processing step, it's not so easy converting a token with a contraction into its normal form (like <kbd>not</kbd> ), therefore, another strategy must be employed. A good way to solve the problem of separate punctuation is provided by the class <kbd>RegexpTokenizer</kbd>, which offers a flexible way to split words according to a regular expression:</p>
<pre><strong>from nltk.tokenize import RegexpTokenizer</strong><br/><br/><strong>&gt;&gt;&gt; complex_text = 'This isn\'t a simple text.'</strong><br/><br/><strong>&gt;&gt;&gt; ret = RegexpTokenizer('[a-zA-Z0-9\'\.]+')</strong><br/><strong>&gt;&gt;&gt; print(ret.tokenize(complex_text))</strong><br/><strong>['This', "isn't", 'a', 'simple', 'text.']</strong></pre>
<p>Most of the common problems can be easily solved using this class, so I suggest you learn how to write simple regular expressions that can match specific patterns. For example, we can remove all numbers, commas, and other punctuation marks from a sentence:</p>
<pre><strong>&gt;&gt;&gt; complex_text = 'This isn\'t a simple text. Count 1, 2, 3 and then go!'</strong><br/><br/><strong>&gt;&gt;&gt; ret = RegexpTokenizer('[a-zA-Z\']+')</strong><br/><strong>&gt;&gt;&gt; print(ret.tokenize(complex_text))</strong><br/><strong>['This', "isn't", 'a', 'simple', 'text', 'Count', 'and', 'the', 'go']</strong></pre>
<p>Even if there are other classes provided by NLTK, they can always be implemented using a customized <kbd>RegexpTokenizer</kbd>, which is powerful enough to solve almost every particular problem; so I prefer not to go deeper in this discussion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stopword removal</h1>
                </header>
            
            <article>
                
<p>Stopwords are part of a normal speech (articles, conjunctions, and so on), but their occurrence frequency is very high and they don't provide any useful semantic information. For these reasons, it's a good practice to filter sentences and corpora by removing them all. NLTK provides lists of stopwords for the most common languages and their usage is immediate:</p>
<pre><strong>from nltk.corpus import stopwords</strong><br/><br/><strong>&gt;&gt;&gt; sw = set(stopwords.words('english'))</strong></pre>
<p>A subset of English stopwords is shown in the following snippet:</p>
<pre><strong>&gt;&gt;&gt; print(sw)</strong><br/><strong>{u'a',</strong><br/><strong> u'about',</strong><br/><strong> u'above',</strong><br/><strong> u'after',</strong><br/><strong> u'again',</strong><br/><strong> u'against',</strong><br/><strong> u'ain',</strong><br/><strong> u'all',</strong><br/><strong> u'am',</strong><br/><strong> u'an',</strong><br/><strong> u'and',</strong><br/><strong> u'any',</strong><br/><strong> u'are',</strong><br/><strong> u'aren',</strong><br/><strong> u'as',</strong><br/><strong> u'at',</strong><br/><strong> u'be', ...</strong></pre>
<p>To filter a sentence, it's possible to adopt a functional approach:</p>
<pre><strong>&gt;&gt;&gt; complex_text = 'This isn\'t a simple text. Count 1, 2, 3 and then go!'</strong><br/><br/><strong>&gt;&gt;&gt; ret = RegexpTokenizer('[a-zA-Z\']+')</strong><br/><strong>&gt;&gt;&gt; tokens = ret.tokenize(complex_text)</strong><br/><strong>&gt;&gt;&gt; clean_tokens = [t for t in tokens if t not in sw]<br/>&gt;&gt;&gt;</strong> <strong>print(clean_tokens)</strong><br/><strong>['This', "isn't", 'simple', 'text', 'Count', 'go']</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Language detection</h1>
                </header>
            
            <article>
                
<p>Stopwords, like other important features, are strictly related to a specific language, so it's often necessary to detect the language before moving on to any other step. A simple, free, and reliable solution is provided by the <kbd>langdetect</kbd> <span>library,</span><span> </span><span>which has been ported from Google's language detection system. Its usage is immediate:</span></p>
<pre><strong>from langdetect import detect</strong><br/><br/><strong>&gt;&gt;&gt; print(detect('This is English'))</strong><br/><strong>en</strong><br/><br/><strong>&gt;&gt;&gt; print(detect('Dies ist Deutsch'))</strong><br/><strong>de</strong></pre>
<p>The function returns the <span>ISO 639-1 codes (<a href="https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes">https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes</a>), which can be used as keys in a dictionary to get the complete language name. Where the text is more complex, the detection can more difficult and it's useful to know whether there are any ambiguities. It's possible to get the probabilities for the expected languages through the <kbd>detect_langs()</kbd> method:</span></p>
<pre><strong>from langdetect import detect_langs</strong><br/><br/><strong>&gt;&gt;&gt; print(detect_langs('I really love you mon doux amour!'))</strong><br/><strong>[fr:0.714281321163, en:0.285716747181]</strong></pre>
<div class="packt_infobox">langdetect can be installed using pip (<kbd>pip install --upgrade langdetect</kbd>). Further information is available at <a href="https://pypi.python.org/pypi/langdetect">https://pypi.python.org/pypi/langdetect</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stemming</h1>
                </header>
            
            <article>
                
<p>Stemming is a process that is used to transform particular words (such as verbs or plurals) into their radical form so as to preserve the semantics without increasing the number of unique tokens. For example, if we consider the three expressions <kbd>I run</kbd>, <kbd>He runs</kbd>, and <kbd>Running</kbd>, they can be reduced into a useful (though grammatically incorrect) form: <kbd>I run</kbd>, <kbd>He run</kbd>, <kbd>Run</kbd>. In this way, we have a single token that defines the same concept (<kbd>run</kbd>), which, for clustering or classification purposes, can be used without any precision loss. There are many stemmer implementations provided by NLTK. The most common (and flexible) is <kbd>SnowballStemmer</kbd>, based on a multilingual algorithm:</p>
<pre><strong>from nltk.stem.snowball import SnowballStemmer</strong><br/><br/><strong>&gt;&gt;&gt; ess = SnowballStemmer('english', ignore_stopwords=True)</strong><br/><strong>&gt;&gt;&gt; print(ess.stem('flies'))</strong><br/><strong>fli</strong><br/><br/><strong>&gt;&gt;&gt; fss = SnowballStemmer('french', ignore_stopwords=True)</strong><br/><strong>&gt;&gt;&gt; print(fss.stem('courais'))</strong><br/><strong>cour</strong></pre>
<p>The <kbd>ignore_stopwords</kbd> <span>parameter</span><span> </span><span>informs the stemmer not to process the stopwords. Other implementations are</span> <kbd>PorterStemmer</kbd> <span>and</span> <kbd>LancasterStemmer</kbd><span>. Very often the result is the same, but in some cases, a stemmer can implement more selective rules. For example:</span></p>
<pre><strong>from nltk.stem.snowball import PorterStemmer</strong><br/><strong>from nltk.stem.lancaster import LancasterStemmer</strong><br/><br/><strong>&gt;&gt;&gt; print(ess.stem('teeth'))</strong><br/><strong>teeth</strong><br/><br/><strong>&gt;&gt;&gt; ps = PorterStemmer()</strong><br/><strong>&gt;&gt;&gt; print(ps.stem('teeth'))</strong><br/><strong>teeth</strong><br/><br/><strong>&gt;&gt;&gt; ls = LancasterStemmer()</strong><br/><strong>&gt;&gt;&gt; print(ls.stem('teeth'))</strong><br/><strong>tee</strong></pre>
<p>As you can see, Snowball and Porter algorithms keep the word unchanged, while Lancaster extracts a radix (which is meaningless). On the other hand, the latter algorithm implements many specific English rules, which can really reduce the number of unique tokens:</p>
<pre><strong>&gt;&gt;&gt; print(ps.stem('teen'))</strong><br/><strong>teen</strong><br/><br/><strong>&gt;&gt;&gt; print(ps.stem('teenager'))</strong><br/><strong>teenag</strong><br/><br/><strong>&gt;&gt;&gt; print(ls.stem('teen'))</strong><br/><strong>teen</strong><br/><br/><strong>&gt;&gt;&gt; print(ls.stem('teenager'))</strong><br/><strong>teen</strong></pre>
<p>Unfortunately, both Porter and Lancaster stemmers are available in NLTK only in English; so the default choice is often Snowball, which is available in many languages and can be used in conjunction with an appropriate stopword set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Vectorizing</h1>
                </header>
            
            <article>
                
<p>This is the last step of the bag-of-words pipeline and it is necessary for transforming text tokens into numerical vectors. The most common techniques are based on a count or frequency computation, and they are both available in scikit-learn with sparse matrix representations (this is a choice that can save a lot of space considering that many tokens appear only a few times while the vectors must have the same length).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Count vectorizing</h1>
                </header>
            
            <article>
                
<p>The algorithm is very simple and it's based on representing a token considering how many times it appears in a document. Of course, the whole corpus must be processed in order to determine how many unique tokens are present and their frequencies. Let's see an example of the <kbd>CountVectorizer</kbd> class on a simple corpus:</p>
<pre><strong>from sklearn.feature_extraction.text import CountVectorizer</strong><br/><br/><strong>&gt;&gt;&gt; corpus = [</strong><br/><strong>       'This is a simple test corpus',</strong><br/><strong>       'A corpus is a set of text documents',</strong><br/><strong>       'We want to analyze the corpus and the documents',</strong><br/><strong>       'Documents can be automatically tokenized'</strong><br/><strong>]</strong><br/><br/><strong>&gt;&gt;&gt; cv = CountVectorizer()</strong><br/><strong>&gt;&gt;&gt; vectorized_corpus = cv.fit_transform(corpus)</strong><br/><strong>&gt;&gt;&gt; print(vectorized_corpus.todense())<br/>[[0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0]<br/> [0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0]<br/> [1 1 0 0 0 1 1 0 0 0 0 0 0 2 0 1 0 1 1]<br/> [0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]</strong></pre>
<p>As you can see, each document has been transformed into a fixed-length vector, where 0 means that the corresponding token is not present, while a positive number represents the occurrences. If we need to exclude all tokens whose document frequency is less than a predefined value, we can set it through the parameter <kbd>min_df</kbd> (the default value is 1). Sometimes it can be useful to avoid terms that are very common; however, the next strategy will manage this problem in a more reliable and complete way.</p>
<p>The vocabulary can be accessed through the instance variable <kbd>vocabulary_</kbd>:</p>
<pre><strong>&gt;&gt;&gt; print(cv.vocabulary_)</strong><br/><strong>{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}</strong></pre>
<p>Given a generic vector, it's possible to retrieve the corresponding list of tokens with an inverse transformation:</p>
<pre><strong>&gt;&gt;&gt; vector = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]</strong><br/><strong>&gt;&gt;&gt; print(cv.inverse_transform(vector))</strong><br/><strong>[array([u'corpus', u'is', u'simple', u'test', u'this', u'want', u'we'], </strong><br/><strong>       dtype='&lt;U13')]</strong></pre>
<p>Both this and the following method can also use an external tokenizer (through the parameter <kbd>tokenizer</kbd>), it can be customized using the techniques discussed in previous sections:</p>
<pre><strong>&gt;&gt;&gt; ret = RegexpTokenizer('[a-zA-Z0-9\']+')</strong><br/><strong>&gt;&gt;&gt; sw = set(stopwords.words('english'))</strong><br/><strong>&gt;&gt;&gt; ess = SnowballStemmer('english', ignore_stopwords=True)</strong><br/><br/><strong>&gt;&gt;&gt; def tokenizer(sentence):</strong><br/><strong>&gt;&gt;&gt;    tokens = ret.tokenize(sentence)</strong><br/><strong>&gt;&gt;&gt;    return [ess.stem(t) for t in tokens if t not in sw]</strong><br/><br/><strong>&gt;&gt;&gt; cv = CountVectorizer(tokenizer=tokenizer)</strong><br/><strong>&gt;&gt;&gt; vectorized_corpus = cv.fit_transform(corpus)</strong><br/><strong>&gt;&gt;&gt; print(vectorized_corpus.todense())</strong><br/><strong>[[0 0 1 0 0 1 1 0 0 0]</strong><br/><strong> [0 0 1 1 1 0 0 1 0 0]</strong><br/><strong> [1 0 1 1 0 0 0 0 0 1]</strong><br/><strong> [0 1 0 1 0 0 0 0 1 0]]</strong></pre>
<p>With our tokenizer (using stopwords and stemming), the vocabulary is shorter and so are the vectors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">N-grams</h1>
                </header>
            
            <article>
                
<p>So far, we have considered only single tokens (also called unigrams), but in many contexts, it's useful to consider short sequences of words (bigrams or trigrams) as atoms for our classifiers, just like all the other tokens. For example, if we are analyzing the sentiment of some texts, it could be a good idea to consider bigrams such as <kbd>pretty good</kbd>, <kbd>very bad</kbd>, and so on. From a semantic viewpoint, in fact, it's important to consider <span>not</span><span> </span><span>just the adverbs but the whole compound form. It's possible to inform our vectorizers about the range of n-grams we want to consider. For example, if we need unigrams and bigrams, we can use this snippet:</span></p>
<pre><strong>&gt;&gt;&gt; cv = CountVectorizer(tokenizer=tokenizer, ngram_range=(1, 2))</strong><br/><strong>&gt;&gt;&gt; vectorized_corpus = cv.fit_transform(corpus)</strong><br/><strong>&gt;&gt;&gt; print(vectorized_corpus.todense())</strong><br/><strong>[[0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0]</strong><br/><strong> [0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0]</strong><br/><strong> [1 1 0 0 0 1 1 0 0 0 0 0 0 2 0 1 0 1 1]</strong><br/><strong> [0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]]</strong><br/><br/><strong>&gt;&gt;&gt; print(cv.vocabulary_)</strong><br/><strong>{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}</strong></pre>
<p>As you can see, the vocabulary now contains the bigrams, and the vectors include their relative frequencies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tf-idf vectorizing</h1>
                </header>
            
            <article>
                
<p>The most common limitation of count vectorizing is that the algorithm doesn't consider the whole corpus while considering the frequency of each token. The goal of vectorizing is normally preparing the data for a classifier; therefore it's necessary to avoid features that are present<span> </span><span>very often</span><span>, because their information decreases when the number of global occurrences increases. For example, in a corpus about a sport, the word</span> <kbd>match</kbd> <span>could be present in a huge number of documents; therefore it's almost useless as a classification feature. To address this issue, we need a different approach. If we have a corpus</span> <kbd>C</kbd><span> with</span> <kbd>n</kbd> <span>documents, we define</span> <strong>term-frequency</strong><span>, the number of times a token occurs in a document, as:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="26" width="175" src="assets/073280b0-172d-4084-b4c2-1d40623143df.png"/></div>
<p class="CDPAlignLeft CDPAlign">We define <strong>inverse-document-frequency</strong>, as the following measure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="50" width="391" src="assets/1cc49044-032f-48a3-920a-264f1e888436.png"/></div>
<p class="CDPAlignLeft CDPAlign">In other words, <kbd>idf(t,C)</kbd> measures how much information is provided by every single term. In fact, if <kbd>count(D,t) = n</kbd>, it means that a token is always present and <em>idf(t, C)</em> comes close to 0, and vice-versa. The term 1 in the denominator is a correction factor, which avoids null idf for count<kbd>(D,t) = n</kbd>. So, instead of considering only the term frequency, we weigh each token by defining a new measure:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="33" width="217" src="assets/4ab0e24e-ca65-432e-aac9-888fc5c68b2e.png"/></div>
<p class="CDPAlignLeft CDPAlign">scikit-learn provides the <kbd>TfIdfVectorizer</kbd> <span>class,</span><span> </span><span>which we can apply to the same toy corpus used in the previous paragraph:</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer</strong><br/><br/><strong>&gt;&gt;&gt; tfidfv = TfidfVectorizer()</strong><br/><strong>&gt;&gt;&gt; vectorized_corpus = tfidfv.fit_transform(corpus)</strong><br/><strong>&gt;&gt;&gt; print(vectorized_corpus.todense())</strong><br/><strong>[[ 0.          0.          0.          0.          0.          0.31799276</strong><br/><strong>   0.          0.39278432  0.          0.          0.49819711  0.49819711</strong><br/><strong>   0.          0.          0.49819711  0.          0.          0.          0.        ]</strong><br/><strong> [ 0.          0.          0.          0.          0.          0.30304005</strong><br/><strong>   0.30304005  0.37431475  0.4747708   0.4747708   0.          0.</strong><br/><strong>   0.4747708   0.          0.          0.          0.          0.          0.        ]</strong><br/><strong> [ 0.31919701  0.31919701  0.          0.          0.          0.20373932</strong><br/><strong>   0.20373932  0.          0.          0.          0.          0.          0.</strong><br/><strong>   0.63839402  0.          0.31919701  0.          0.31919701  0.31919701]</strong><br/><strong> [ 0.          0.          0.47633035  0.47633035  0.47633035  0.</strong><br/><strong>   0.30403549  0.          0.          0.          0.          0.          0.</strong><br/><strong>   0.          0.          0.          0.47633035  0.          0.        ]]</strong></pre>
<p>Let's now check the vocabulary to make a comparison with simple count vectorizing:</p>
<pre><strong>&gt;&gt;&gt; print(tfidfv.vocabulary_)</strong><br/><strong>{u'and': 1, u'be': 3, u'we': 18, u'set': 9, u'simple': 10, u'text': 12, u'is': 7, u'tokenized': 16, u'want': 17, u'the': 13, u'documents': 6, u'this': 14, u'of': 8, u'to': 15, u'can': 4, u'test': 11, u'corpus': 5, u'analyze': 0, u'automatically': 2}</strong></pre>
<p>The term <kbd>documents</kbd> is the sixth feature in both vectorizers and appears in the last three documents. As you can see, it's weight is about 0.3, while the term <kbd>the</kbd> is present twice only in the third document and its weight is about 0.64. The general rule is: if a term is representative of a document, its weight becomes close to 1.0, while it decreases if finding it in a sample document doesn't allow us to easily <span>determine</span><span> </span><span>its category.</span></p>
<p>Also in this case, it's possible to use an external tokenizer and specify the desired n-gram range. Moreover, it's possible to normalize the vectors (through the parameter <kbd>norm</kbd>) and decide whether to include or exclude the addend 1 to the denominator of idf (through the parameter <kbd>smooth_idf</kbd>). It's also possible to define the range of accepted document frequencies using the parameters <kbd>min_df</kbd> and <kbd>max_df</kbd> so as to exclude tokens whose occurrences are below or beyond a minimum/maximum threshold. They accept both integers (number of occurrences) or floats in the range of [0.0, 1.0] (proportion of documents). In the next example, we use some of these parameters:</p>
<pre><strong>&gt;&gt;&gt; tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')</strong><br/><strong>&gt;&gt;&gt; vectorized_corpus = tfidfv.fit_transform(corpus)</strong><br/><strong>&gt;&gt;&gt; print(vectorized_corpus.todense())</strong><br/><strong>[[ 0.          0.          0.          0.          0.30403549  0.          0.</strong><br/><strong>   0.          0.          0.          0.          0.47633035  0.47633035</strong><br/><strong>   0.47633035  0.47633035  0.          0.          0.          0.          0.        ]</strong><br/><strong> [ 0.          0.          0.          0.          0.2646963   0.</strong><br/><strong>   0.4146979   0.2646963   0.          0.4146979   0.4146979   0.          0.</strong><br/><strong>   0.          0.          0.4146979   0.4146979   0.          0.          0.        ]</strong><br/><strong> [ 0.4146979   0.4146979   0.          0.          0.2646963   0.4146979</strong><br/><strong>   0.          0.2646963   0.          0.          0.          0.          0.</strong><br/><strong>   0.          0.          0.          0.          0.          0.4146979</strong><br/><strong>   0.4146979 ]</strong><br/><strong> [ 0.          0.          0.47633035  0.47633035  0.          0.          0.</strong><br/><strong>   0.30403549  0.47633035  0.          0.          0.          0.          0.</strong><br/><strong>   0.          0.          0.          0.47633035  0.          0.        ]]</strong><br/><br/><strong>&gt;&gt;&gt; print(tfidfv.vocabulary_)</strong><br/><strong>{u'analyz corpus': 1, u'set': 9, u'simpl test': 12, u'want analyz': 19, u'automat': 2, u'want': 18, u'test corpus': 14, u'set text': 10, u'corpus set': 6, u'automat token': 3, u'corpus document': 5, u'text document': 16, u'token': 17, u'document automat': 8, u'text': 15, u'test': 13, u'corpus': 4, u'document': 7, u'simpl': 11, u'analyz': 0}</strong></pre>
<p>In particular, normalizing vectors is always a good choice if they must be used as input for a classifier, as we'll see in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A sample text classifier based on the Reuters corpus</h1>
                </header>
            
            <article>
                
<p>We are going to build a sample text classifier based on the NLTK Reuters corpus. This one is made of up thousands of news lines divided into 90 categories:</p>
<pre><strong>from nltk.corpus import reuters</strong><br/><br/><strong>&gt;&gt;&gt; print(reuters.categories())</strong><br/><strong>[u'acq', u'alum', u'barley', u'bop', u'carcass', u'castor-oil', u'cocoa', u'coconut', u'coconut-oil', u'coffee', u'copper', u'copra-cake', u'corn', u'cotton', u'cotton-oil', u'cpi', u'cpu', u'crude', u'dfl', u'dlr', u'dmk', u'earn', u'fuel', u'gas', u'gnp', u'gold', u'grain', u'groundnut', u'groundnut-oil', u'heat', u'hog', u'housing', u'income', u'instal-debt', u'interest', u'ipi', u'iron-steel', u'jet', u'jobs', u'l-cattle', u'lead', u'lei', u'lin-oil', u'livestock', u'lumber', u'meal-feed', u'money-fx', u'money-supply', u'naphtha', u'nat-gas', u'nickel', u'nkr', u'nzdlr', u'oat', u'oilseed', u'orange', u'palladium', u'palm-oil', u'palmkernel', u'pet-chem', u'platinum', u'potato', u'propane', u'rand', u'rape-oil', u'rapeseed', u'reserves', u'retail', u'rice', u'rubber', u'rye', u'ship', u'silver', u'sorghum', u'soy-meal', u'soy-oil', u'soybean', u'strategic-metal', u'sugar', u'sun-meal', u'sun-oil', u'sunseed', u'tea', u'tin', u'trade', u'veg-oil', u'wheat', u'wpi', u'yen', u'zinc']</strong></pre>
<p>To simplify the process, we'll take only two categories, which have a similar number of documents:</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; Xr = np.array(reuters.sents(categories=['rubber']))</strong><br/><strong>&gt;&gt;&gt; Xc = np.array(reuters.sents(categories=['cotton']))</strong><br/><strong>&gt;&gt;&gt; Xw = np.concatenate((Xr, Xc))</strong></pre>
<p>As each document is already split into tokens and we want to apply our custom tokenizer (with stopword removal and stemming), we need to rebuild the full sentences:</p>
<pre><strong>&gt;&gt;&gt; X = []</strong><br/><br/><strong>&gt;&gt;&gt; for document in Xw:</strong><br/><strong>&gt;&gt;&gt;    X.append(' '.join(document).strip().lower())</strong></pre>
<p>Now we need to prepare the label vector, by assigning 0 to <kbd>rubber</kbd> and 1 to <kbd>cotton</kbd>:</p>
<pre><strong>&gt;&gt;&gt; Yr = np.zeros(shape=Xr.shape)</strong><br/><strong>&gt;&gt;&gt; Yc = np.ones(shape=Xc.shape)</strong><br/><strong>&gt;&gt;&gt; Y = np.concatenate((Yr, Yc))</strong></pre>
<p>At this point, we can vectorize our corpus:</p>
<pre><strong>&gt;&gt;&gt; tfidfv = TfidfVectorizer(tokenizer=tokenizer, ngram_range=(1, 2), norm='l2')</strong><br/><strong>&gt;&gt;&gt; Xv = tfidfv.fit_transform(X)</strong></pre>
<p>Now the dataset is ready, and we can proceed by splitting it into train and test subsets and finally train our classifier. I've decided to adopt a random forest because it's particularly efficient for this kind of task, but the reader can try different classifiers and compare the results:</p>
<pre><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>from sklearn.ensemble import RandomForestClassifier</strong><br/><br/><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(Xv, Y, test_size=0.25)</strong><br/><br/><strong>&gt;&gt;&gt; rf = RandomForestClassifier(n_estimators=25)</strong><br/><strong>&gt;&gt;&gt; rf.fit(X_train, Y_train)</strong><br/><strong>&gt;&gt;&gt; score = rf.score(X_test, Y_test)</strong><br/><strong>&gt;&gt;&gt; print('Score: %.3f' % score)</strong><br/><strong>Score: 0.874</strong></pre>
<p>The score is about 88%, which is a quite good result, but let's try a prediction with a fake news line:</p>
<pre><strong>&gt;&gt;&gt; test_newsline = ['Trading tobacco is reducing the amount of requests for cotton and this has a negative impact on our economy']</strong><br/><br/><strong>&gt;&gt;&gt; yvt = tfidfv.transform(test_newsline)</strong><br/><strong>&gt;&gt;&gt; category = rf.predict(yvt)</strong><br/><strong>&gt;&gt;&gt; print('Predicted category: %d' % int(category[0]))</strong><br/><strong>Predicted category: 1</strong></pre>
<p>The classification result is correct; however, by adopting some techniques that we're going to discuss in the next chapter, it's <span>also</span><span> </span><span>possible to get better performance in more complex real-life problems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol>
<li>Perkins J., Python 3 Text Processing with NLTK 3 Cookbook, Packt.</li>
<li>Hardeniya N., NLTK Essentials, Packt</li>
<li>Bonaccorso G., BBC News classification algorithm comparison, <a href="https://github.com/giuseppebonaccorso/bbc_news_classification_comparison" target="_blank">https://github.com/giuseppebonaccorso/bbc_news_classification_comparison</a>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed all the basic NLP techniques, starting from the definition of a corpus up to the final transformation into feature vectors. We analyzed different tokenizing methods to address particular problems or situations of splitting a document into words. Then we introduced some filtering techniques that are necessary to remove all useless elements (also called stopwords) and to convert the inflected forms into standard tokens.</p>
<p>These steps are important in order to increase the information content by removing frequently used terms. When the documents have been successfully cleaned, it is possible to vectorize them using a simple approach such as the one implemented by the count-vectorizer, or a more complex one that takes into account the global distribution of terms, such as tf-idf. The latter was introduced to complete the work done by the stemming phase; in fact, it's purpose is to define vectors where each component will be close to 1 when the <span>amount</span><span> of </span><span>information is high and vice-versa. Normally a word that is present in many documents isn't a good marker for a classifier; therefore, if not already removed by the previous steps, tf-idf will automatically reduce its weight. At the end of the chapter, we built a simple text classifier that implements the whole bag-of-words pipeline and uses a random forest to classify news lines.</span></p>
<p>In the next chapter, we're going to complete this introduction with a brief discussion of advanced techniques such as topic modeling, latent semantic analysis, and sentiment analysis.</p>


            </article>

            
        </section>
    </body></html>