<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Regression</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first group of machine learning techniques that we will explore is generally referred to as <strong>regression</strong>. Regression is a process through which we can understand how one variable (for example, sales) changes with respect to another variable (for example, number of users). These techniques are useful on their own. However, they are also a good starting point to discuss machine learning techniques because they form the basis of other, more complicated, techniques that we will discuss later in the book.</p>
<p class="mce-root">Generally, regression techniques in machine learning are concerned with predicting continuous values (for example, stock price, temperature, or disease progression). <strong>Classification</strong>, which we will cover in the next chapter, is concerned with predicting discrete variables, or one of a discrete set of categories (for example, fraud/not fraud, sitting/standing/running, or hot dog/not hot dog). As mentioned, regression techniques are used throughout machine learning as part of classification algorithms, but in this chapter we will focus on their basic application to predict continuous values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding regression model jargon</h1>
                </header>
            
            <article>
                
<p>As already mentioned, regression itself is a process to analyze a relationship between one variable and another variable, but there are some terms used in machine learning to describe these variables along with various types of regression and processes associated with regression:</p>
<ul>
<li><strong>Response</strong> or <strong>dependent variable</strong>: These terms will be used interchangeably for the variable that we are trying to predict based on one or more other variables. This variable is often labeled <em>y</em>.</li>
<li><strong>Explanatory variables</strong>, <strong>independent variables</strong>, <strong>features</strong>, <strong>attributes</strong>, or <strong>regressors</strong>: These terms will be used interchangeably for the variables that we are using to predict the response. These variables are often labeled <em>x</em> or <em>x<sub>1</sub>, x<sub>2</sub>,</em> and so on.</li>
<li><strong>Linear regression</strong>: This type of regression assumes that the dependent variable depends on the independent variable linearly (that is, following the equation for a line).</li>
<li><strong>Nonlinear regression</strong>: This type of regression assumes that the dependent variable depends on the independent variable in a relationship that is not linear (for example, polynomial or exponential).</li>
<li><strong>Multiple regression</strong>: A regression with more than one independent variable.</li>
<li><strong>Fitting</strong> or <strong>training</strong>: The process of parameterizing a model, such as a regression model, so that it can predict a certain dependent variable.</li>
<li><strong>Prediction</strong>: The process of using a parameterized model, such as a regression model, to predict a certain dependent variable.</li>
</ul>
<p>Some of these terms will be used both in the context of regression and in other contexts throughout the rest of the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>Linear regression is one of the most simple machine learning models. However, you should not dismiss this model by any means. As mentioned previously, it is an essential building block that is utilized in other models, and it has some very important advantages.</p>
<p>As discussed throughout this book, integrity in machine learning applications is crucial, and the simpler and more interpretable a model is, the easier it is to maintain integrity. In addition, because the model is simple and interpretable, it allows you to understand inferred relationships between variables and check your work mentally as you develop. In the words of <span>Mike Lee Williams from Fast Forward Labs (in <a href="http://blog.fastforwardlabs.com/2017/08/02/interpretability.html">http://blog.fastforwardlabs.com/2017/08/02/interpretability.html</a>):</span></p>
<div class="packt_quote"><span>The future is algorithmic. Interpretable models offer a safer, more productive, and ultimately more collaborative relationship between humans and intelligent machines.</span></div>
<p>Linear regression models are interpretable, and thus, they can provide a safe and productive option for data scientists. When you are searching for a model to predict a continuous variable, you should consider and try linear regression (or even multiple linear regression) if your data and problem allows you to use it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of linear regression</h1>
                </header>
            
            <article>
                
<p>In linear regression, we attempt to model our dependent variable, <em>y</em>, by an independent variable, <em>x</em>, using the equation for a line:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="86" src="assets/f3e2dab9-8cd9-4682-add5-28573549128b.png"/></div>
<p>Here, <em>m</em> is the slope of the line and <em>b</em> is the intercept. For example, let's say that we want to model daily <em>sales</em> by the <em>number of users</em> on our website each day. To do this with linear regression, we would want to determine an <em>m</em> and <em>b</em> that would allow us to predict sales via the following formula:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="222" src="assets/6852719e-3947-4a09-80c9-48471fd9739c.png"/></div>
<p>Thus, our trained model is really just this parameterized function. We put in a <strong>Number of Users</strong> and we get the predicted <strong>Sales</strong>, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="211" width="443" class="image-border" src="assets/f368c62d-8816-4c7e-ae7a-4b3045c1bb70.png"/></div>
<p>The training or fitting of a linear regression model involves determining the values of <em>m</em> and <em>b,</em> such that the resulting formula has predictive power for our response. There are a variety of methods to determine <em>m</em> and <em>b</em>, but the most common and simple method is called <strong>ordinary least squares</strong> (<strong>OLS</strong>).</p>
<p>To find <em>m</em> and <em>b</em> with OLS, we first pick a value for <em>m</em> and <em>b</em> to create a first example line. We then measure the vertical distance between each of our known points (for example, from our training set) and the example line. These distances are called <strong>errors</strong> or <strong>residuals</strong>, similar to the errors that we discussed in <a href="64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml" target="_blank">Chapter 3</a>, <em>Evaluation and Validation,</em> and are illustrated in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="198" width="417" class="image-border" src="assets/1828093f-2bf5-4848-a6c0-056b7bb2de0b.png"/></div>
<p>Next, we add up the sum of the squares of these errors:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="36" width="169" src="assets/f616d895-8121-488b-baef-ca27d1da6751.png"/></div>
<p>We adjust <em>m</em> and <em>b</em> until we minimize this sum of the squares of the errors. In other words, our trained linear regression line is the line that minimizes this sum of the squares.</p>
<p>There are a variety of methods to find the line that minimizes the sum of the squared errors and, for OLS, the line can be found analytically. However, a very popular and general optimization technique that is used to minimize the sum of the squared error is called <strong>gradient descent</strong>. This method can be more efficient in terms of implementation, advantageous computationally (in terms of memory, for example), and more flexible than analytic solutions.</p>
<p>Gradient descent is discussed in more detail in the <a href="718ca26d-465a-47c9-91b9-14e749be0c30.xhtml" target="_blank">Appendix</a>, <em>Algorithms/Techniques Related to Machine Learning</em>, so we will avoid a lengthy discussion here. Suffice it to say that many implementations of linear and other regressions utilize gradient descent for the fitting or training of the linear regression line. In fact, gradient descent is ubiquitous in machine learning and also powers much more complicated modeling techniques such as deep learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression assumptions and pitfalls</h1>
                </header>
            
            <article>
                
<p>Like all machine learning models, linear regression does not work in all situations and it does make certain assumptions about your data and the relationships in your data. The assumptions of linear regression are as follows:</p>
<ul>
<li><strong>Linear relationship</strong>: This might seem obvious, but linear regression assumes that your dependent variable depends on your independent variable linearly (by means of the equation for a line). If this relationship is not linear, linear regression will likely perform poorly.</li>
<li><strong>Normality</strong>: This assumption means that your variables should be distributed according to a normal distribution (which looks like a bell shape). We will come back to this property later in the chapter and discuss some trade-offs and options when encountering non-normally distributed variables.</li>
<li><strong>No multicollinearity</strong>: Multicollinearity is a fancy term that means that independent variables are not really independent. They depend on each other in some fashion.</li>
<li><strong>No auto-correlation</strong>: Auto-correlation is another fancy term that means that a variable depends on itself or some shifted version of itself (like in some predictable time series).</li>
<li><strong>Homoscedasticity</strong>: This may be the fanciest word of this bunch of terms, but it means something relatively simple and is not really something you have to worry about very often. Linear regression assumes that the variance of your data is about the same around the regression line for all values of your independent variable.</li>
</ul>
<p>Technically, all of these assumptions need to be fulfilled for us to use linear regression. It's very important that we know how our data is distributed and how it behaves. We will look into these assumptions when we profile data in an example use of linear regression.</p>
<p>As a data scientist or analyst, you want to keep the following pitfalls in mind as you apply linear regression:</p>
<ul>
<li>You are training your linear regression model for a certain range of your independent variable. You should be careful making predictions for values outside of this range because your regression line might not be applicable (for example, your dependent variable may start behaving non-linearly at extreme values).</li>
<li>You can misspecify a linear regression model by finding some spurious relationship between two variables that really have nothing to do with one another. You should check to make sure that there is some logical reason why variables might be functionally related.</li>
<li>Outliers or extreme values in your data may throw off a regression line for certain types of fitting, such as OLS. There are ways to fit a regression line that is more immune to outliers, or behaves differently with respect to outliers, such as orthogonal least squares or ridge regression.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression example</h1>
                </header>
            
            <article>
                
<p>To illustrate linear regression, let's take an example problem and create our first machine learning model! The example data that we are going to use is example advertising data. It is in the <kbd>.csv</kbd> format and looks as follows:</p>
<pre><strong>$ head Advertising.csv 
TV,Radio,Newspaper,Sales
230.1,37.8,69.2,22.1
44.5,39.3,45.1,10.4
17.2,45.9,69.3,9.3
151.5,41.3,58.5,18.5
180.8,10.8,58.4,12.9
8.7,48.9,75,7.2
57.5,32.8,23.5,11.8
120.2,19.6,11.6,13.2
8.6,2.1,1,4.8</strong></pre>
<p>The dataset includes a set of attributes representing spend on advertising outlets (<kbd>TV</kbd>, <kbd>Radio</kbd>, and <kbd>Newspaper</kbd>) along with corresponding sales (<kbd>Sales</kbd>). Our goal in this example will be to model the sales (our dependent variable) by one of the attributes of advertising spend (our independent variable).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Profiling the data</h1>
                </header>
            
            <article>
                
<p>To make sure that we create a model, or at least process, that we understand, and to make sure that we can mentally check our results, we need to start every machine learning model building process with data profiling. We need to gain an understanding of how each of our variables are distributed and their range and variability.</p>
<p>To do this, we will calculate the summary statistics that we discussed earlier in <a href="5e7af3cb-ce60-4699-b2d3-7eeff8978a9e.xhtml" target="_blank">Chapter 2</a>, <em>Matrices, Probability, and Statistics</em>. Here, we will utilize a method built into the <kbd>github.com/kniren/gota/dataframe</kbd> package to calculate our summary statistics for all of the columns of our dataset in one operation:</p>
<pre>// Open the CSV file.
advertFile, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer advertFile.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(advertFile)

// Use the Describe method to calculate summary statistics
// for all of the columns in one shot.
advertSummary := advertDF.Describe()
<br/>// Output the summary statistics to stdout.
fmt.Println(advertSummary)</pre>
<p>Compiling and running this gives the following result:</p>
<pre><strong>$ go build
$ ./myprogram</strong><br/><strong>[7x5] DataFrame

    column   TV         Radio     Newspaper  Sales    
 0: mean     147.042500 23.264000 30.554000  14.022500
 1: stddev   85.854236  14.846809 21.778621  5.217457 
 2: min      0.700000   0.000000  0.300000   1.600000 
 3: 25%      73.400000  9.900000  12.600000  10.300000
 4: 50%      149.700000 22.500000 25.600000  12.900000
 5: 75%      218.500000 36.500000 45.100000  17.400000
 6: max      296.400000 49.600000 114.000000 27.000000
    &lt;string&gt; &lt;float&gt;    &lt;float&gt;   &lt;float&gt;    &lt;float&gt;</strong> </pre>
<p>As you can see, this prints out all of our summary statistics in a nice tabular form and includes mean, standard deviation, minimum value, maximum value, <em>25%/75%</em> percentiles, and median (or 50% percentile).</p>
<p>These values give us a good numerical reference for the numbers that we will be seeing as we train our linear regression model. However, this does not give us a very good visual understanding of our data. For this, we will create a histogram for the values in each of the columns:</p>
<pre>// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Create a histogram for each of the columns in the dataset.
for _, colName := range advertDF.Names() {

    // Create a plotter.Values value and fill it with the
    // values from the respective column of the dataframe.
    plotVals := make(plotter.Values, advertDF.Nrow())
    for i, floatVal := range advertDF.Col(colName).Float() {
        plotVals[i] = floatVal
    }

    // Make a plot and set its title.
    p, err := plot.New()
    if err != nil {
        log.Fatal(err)
    }
    p.Title.Text = fmt.Sprintf("Histogram of a %s", colName)

    // Create a histogram of our values drawn
    // from the standard normal.
    h, err := plotter.NewHist(plotVals, 16)
    if err != nil {
        log.Fatal(err)
    }        

    // Normalize the histogram.
    h.Normalize(1)

    // Add the histogram to the plot.
    p.Add(h)

    // Save the plot to a PNG file.
    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+"_hist.png"); err != nil {
        log.Fatal(err)
    }
}</pre>
<p>This program will create a <kbd>.png</kbd> image for each histogram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="471" width="478" class="alignnone size-full wp-image-521 image-border" src="assets/8f0052c8-b6ad-4933-8f2a-1959a33c2a0d.png"/></div>
<p>Now, looking at these histograms and the summary statistics that we calculated, we need to consider if we are working within the assumptions of linear regression. In particular, we can see that not all of our variables are normally distributed (that is, they are in a bell shape). The sales might be somewhat bell-shaped, but the others do not look to be normal.</p>
<p>We could use a statistical tool, such as a <strong>quantile-quantile</strong> (<strong>q-q</strong>) plot, to determine how close the distributions are to normal distributions, and we could even perform a statistical test to determine the probability of the variables following a normal distribution. However, most of the time, we can get a general idea from the histograms.</p>
<p>Now we have to make a decision. At least some of our data does not technically fit within the assumptions of our linear regression model. We could now do one of the following:</p>
<ul>
<li>Try to transform our variables (with, for example, a power transformation) that follow a normal distribution, and then use these transformed variables in our linear regression model. The advantage of this option is that we would be operating within the assumptions of the model. The disadvantage is that we would be making our model harder to understand, and less interpretable.</li>
<li>Get different data to solve our problem.</li>
<li>Ignore our issue with the linear regression assumptions and try to create the model.</li>
</ul>
<p>There may be other views on this, but my recommendation is that you try the third option first. There is not much harm in this option because you can train the linear regression model quickly. If you end up with a model that performs nicely, you have avoided further complications and have a nice simple model. If you end up with a model that performs poorly, you might need to resort to one of the other options.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing our independent variable</h1>
                </header>
            
            <article>
                
<p>So, now we have some intuition about our data and have come to terms with how our data fits within the assumptions of the linear regression model. Now, how do we choose which variable to use as our independent variable in trying to predict our dependent variable, and average points per game?</p>
<p>The easiest way to make this decision is by visually exploring the correlation between the dependent variable and all of the choices that you have for independent variables. In particular, you can make scatter plots (using <kbd>gonum.org/v1/plot</kbd>) of your dependent variable versus each of the other variables:</p>
<pre>// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Extract the target column.
yVals := advertDF.Col("Sales").Float()

// Create a scatter plot for each of the features in the dataset.
for _, colName := range advertDF.Names() {
    <br/>    // pts will hold the values for plotting
    pts := make(plotter.XYs, advertDF.Nrow())

    // Fill pts with data.
    for i, floatVal := range advertDF.Col(colName).Float() {
        pts[i].X = floatVal
        pts[i].Y = yVals[i]
    }

    // Create the plot.
    p, err := plot.New()
    if err != nil {
        log.Fatal(err)
    }
    p.X.Label.Text = colName
    p.Y.Label.Text = "y"
    p.Add(plotter.NewGrid())

    s, err := plotter.NewScatter(pts)
    if err != nil {
        log.Fatal(err)
    }
    s.GlyphStyle.Radius = vg.Points(3)

    // Save the plot to a PNG file.
    p.Add(s)
    if err := p.Save(4*vg.Inch, 4*vg.Inch, colName+"_scatter.png"); err != nil {
        log.Fatal(err)
    }
}</pre>
<p>This will create the following scatter plots:</p>
<div class="CDPAlignCenter CDPAlign"><img height="313" width="319" class="alignnone size-full wp-image-585 image-border" src="assets/926090dd-21c3-40a7-a44b-404163072193.png"/></div>
<p>As we look at these scatter plots, we want to deduce which of the attributes (<strong>TV</strong>, <strong>Radio</strong>, and/or <strong>Newspaper</strong>) have a linear relationship with our dependent variable, <strong>Sales</strong>. That is, could we draw a line on any of these scatter plots that would fit the trend of <strong>Sales</strong> versus the respective attribute? This is not always possible, and it likely will not be possible for all of the attributes that you have to work with for a given problem.</p>
<p>In this case, both <strong>Radio</strong> and <strong>TV</strong> appear to be somewhat linearly correlated with <strong>Sales</strong>. <strong>Newspaper</strong> may be slightly correlated with <strong>Sales</strong>, but the correlation is far from obvious. The linear relationship with <strong>TV</strong> seems most obvious, so let's start out with <strong>TV</strong> as our independent variable in our linear regression model. This would make our linear regression formula as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="140" src="assets/aed083c3-8921-4c5b-9dbb-77ff4a4af8cf.png"/></div>
<p class="mce-root">One other thing to note here is that the variable <strong>TV</strong> might not be strictly homoscedastic, which was discussed earlier as an assumption of linear regression. This is worth noting (and likely worth documenting in the project), but we will continue on to see if we can create the linear regression model with some predictive power. We can always revisit this assumption if our model is behaving poorly, as a possible explanation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating our training and test sets</h1>
                </header>
            
            <article>
                
<p>To avoid overfitting and make sure that our model can generalize, we are going to split our dataset into a training set and a test set, as was discussed in <a href="64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml" target="_blank">Chapter 3</a>, <em>Evaluation and Validation</em>. We will not bother with a holdout set here, because we are only going to make one pass through our model training without an iterative back and forth between training and testing. However, if you are experimenting with various dependent variables and/or iteratively adjusting any parameters of your model, you would want to create a holdout set that you save until the end of your model development process for validation.</p>
<p>We will use <kbd>github.com/kniren/gota/dataframe</kbd> to create our training and test datasets and then save them to respective <kbd>.csv</kbd> files. In this case, we</p>
<p>will use an 80/20 split for our training and test data:</p>
<pre>// Open the advertising dataset file.        <br/>f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
advertDF := dataframe.ReadCSV(f)

// Calculate the number of elements in each set.
trainingNum := (4 * advertDF.Nrow()) / 5
testNum := advertDF.Nrow() / 5
if trainingNum+testNum &lt; advertDF.Nrow() {
    trainingNum++
}

// Create the subset indices.
trainingIdx := make([]int, trainingNum)
testIdx := make([]int, testNum)

// Enumerate the training indices.
for i := 0; i &lt; trainingNum; i++ {
    trainingIdx[i] = i
}

// Enumerate the test indices.
for i := 0; i &lt; testNum; i++ {
    testIdx[i] = trainingNum + i
}

// Create the subset dataframes.
trainingDF := advertDF.Subset(trainingIdx)
testDF := advertDF.Subset(testIdx)

// Create a map that will be used in writing the data
// to files.
setMap := map[int]dataframe.DataFrame{
    0: trainingDF,
    1: testDF,
}

// Create the respective files.
for idx, setName := range []string{"training.csv", "test.csv"} {

    // Save the filtered dataset file.
    f, err := os.Create(setName)
    if err != nil {
        log.Fatal(err)
    }

    // Create a buffered writer.
    w := bufio.NewWriter(f)

    // Write the dataframe out as a CSV.
    if err := setMap[idx].WriteCSV(w); err != nil {
        log.Fatal(err)
    }
}</pre>
<p>This code will output the following training and test sets that we will use:</p>
<pre><strong>$ wc -l *.csv
  201 Advertising.csv
   41 test.csv
  161 training.csv
  403 total</strong></pre>
<div class="packt_tip">The data that we were using here was not sorted or ordered by data in any fashion. However, if you are dealing with data that is sorted by response, by date, or in any other way, it is important that you randomly split your data into training and test sets. If you do not do this, your training and test sets may include only certain ranges of the response, may be influenced artificially by time/date, and so on.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training our model</h1>
                </header>
            
            <article>
                
<p>Next, we are going to actually train, or fit, our linear regression model. If you remember, this just means that we are finding the slope (<em>m</em>) and intercept (<em>b</em>) for the line that minimizes the sum of the squared errors. To perform this training, we will use a really great package from Sajari: <kbd>github.com/sajari/regression</kbd>. Sajari is a web search company that relies heavily on Go and machine learning, and they use <a href="http://github.com/sajari/regression" target="_blank">github.com/sajari/regression</a> in production.</p>
<p>To train a regression model using <a href="http://github.com/sajari/regression" target="_blank">github.com/sajari/regression</a>, we need to initialize a <kbd>regression.Regression</kbd> value, set a couple of labels, and fill the <kbd>regression.Regression</kbd> value with labeled training data points. After this, training our linear regression model is as easy as calling the <kbd>Run()</kbd> method on the <kbd>regression.Regression</kbd> value:</p>
<pre>// Open the training dataset file.
f, err := os.Open("training.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
trainingData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// In this case we are going to try and model our Sales (y)
// by the TV feature plus an intercept. As such, let's create
// the struct needed to train a model using github.com/sajari/regression.
var r regression.Regression
r.SetObserved("Sales")
r.SetVar(0, "TV")

// Loop of records in the CSV, adding the training data to the regression value.
for i, record := range trainingData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales regression measure, or "y".
    yVal, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Add these points to the regression value.
    r.Train(regression.DataPoint(yVal, []float64{tvVal}))
}

// Train/fit the regression model.
r.Run()

// Output the trained model parameters.
fmt.Printf("\nRegression Formula:\n%v\n\n", r.Formula)</pre>
<p>Compiling and running this will result in the trained linear regression formula being printed to <kbd>stdout</kbd>:</p>
<pre><strong>$ go build
$ ./myprogram 

Regression Formula:
Predicted = 7.07 + TV*0.05</strong></pre>
<p>Here, we can see that the package determined our linear regression line with an intercept of <kbd>7.07</kbd> and a slope of <kbd>0.5</kbd>. We can perform a little mental check here, because we saw in the scatter plots how the correlation between <strong>TV</strong> and <strong>Sales</strong> was up and to the right (that is, a positive correlation). This means that the slope should be positive in the formula, which it is.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the trained model</h1>
                </header>
            
            <article>
                
<p>We now need to measure the performance of our model to see if we really have any power to predict <strong>Sales</strong> using <strong>TV</strong> as in independent variable. To do this, we can load in our test set, make predictions using our trained model for each test example, and then calculate one of the evaluation metrics discussed in <a href="64db3465-cd9a-42ca-b0fb-d54c59b87037.xhtml" target="_blank">Chapter 3</a>, <em>Evaluation and Validation</em>.</p>
<p>For this problem, let's use the Mean Absolute Error (MAE) as our evaluation metric. This seems reasonable, because it results in something directly comparable with our <kbd>Sales</kbd> values and we do not have to be too worried about outliers or extreme values.</p>
<p>To calculate the predicted <strong>Sales</strong> values using our trained <kbd>regression.Regression</kbd> value, we just need to parse the values in our test set and call the <kbd>Predict()</kbd> method on the <kbd>regression.Regression</kbd> value. We will then take the difference of these predicted values from the observed values, get the absolute value of the difference, and then add up all of the absolute values to get the MAE:</p>
<pre>// Open the test dataset file.
f, err = os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a CSV reader reading from the opened file.
reader = csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the test data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the observed Sales, or "y".
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted, err := r.Predict([]float64{tvVal})

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("MAE = %0.2f\n\n", mAE)</pre>
<p>Compiling and running this evaluation gives the following result:</p>
<pre><strong>$ go build
$ ./myprogram 

Regression Formula:
Predicted = 7.07 + TV*0.05

MAE = 3.01</strong></pre>
<p>How do we know if <kbd>MAE = 3.01</kbd> is good or bad? This is, again, why having a good mental model of your data is important. If you remember, we already computed the mean, range, and standard deviation of sales. The mean sales value was <kbd>14.02</kbd> and the standard deviation was <kbd>5.21</kbd>. Thus, our MAE is less than the standard deviations of our sales values and is about 20% of the mean value, and our model has some predictive power.</p>
<p>So, congratulations! We have built our first machine learning model that has predictive power!</p>
<p>To get better intuition about how our model is performing, we can also create a plot to help us visualize the linear regression line. This can be done with <kbd>gonum.org/v1/plot</kbd>. First, however, let's create a predict function that allows us to make our predictions without importing <kbd>github.com/sajari/regression</kbd>. This gives us a lightweight, in-memory version of the trained model:</p>
<pre>// predict uses our trained regression model to made a prediction.
func predict(tv float64) float64 {
    return 7.07 + tv*0.05
}</pre>
<p>Then, we can create the visualization of our regression line:</p>
<pre>// Open the advertising dataset file.
f, err := os.Open("Advertising.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
advertDF := dataframe.ReadCSV(f)

// Extract the target column.
yVals := advertDF.Col("Sales").Float()

// pts will hold the values for plotting.
pts := make(plotter.XYs, advertDF.Nrow())

// ptsPred will hold the predicted values for plotting.
ptsPred := make(plotter.XYs, advertDF.Nrow())

// Fill pts with data.
for i, floatVal := range advertDF.Col("TV").Float() {
    pts[i].X = floatVal
    pts[i].Y = yVals[i]
    ptsPred[i].X = floatVal
    ptsPred[i].Y = predict(floatVal)
}

// Create the plot.
p, err := plot.New()
if err != nil {
    log.Fatal(err)
}
p.X.Label.Text = "TV"
p.Y.Label.Text = "Sales"
p.Add(plotter.NewGrid())

// Add the scatter plot points for the observations.
s, err := plotter.NewScatter(pts)
if err != nil {
    log.Fatal(err)
}
s.GlyphStyle.Radius = vg.Points(3)

// Add the line plot points for the predictions.
l, err := plotter.NewLine(ptsPred)
if err != nil {
    log.Fatal(err)
}
l.LineStyle.Width = vg.Points(1)
l.LineStyle.Dashes = []vg.Length{vg.Points(5), vg.Points(5)}<br/><br/>// Save the plot to a PNG file.
p.Add(s, l)
if err := p.Save(4*vg.Inch, 4*vg.Inch, "regression_line.png"); err != nil {
    log.Fatal(err)
}</pre>
<p>It will produce this plot when compiled and run:</p>
<div class="CDPAlignCenter CDPAlign"><img height="217" width="217" class="image-border" src="assets/e6a05d14-8179-452e-bb8f-358348a02708.png"/></div>
<p>As you can see, our trained linear regression line follows the linear trend of the real data points. This is another visual confirmation that we are on the right track!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multiple linear regression</h1>
                </header>
            
            <article>
                
<p>Linear regression is not limited to simple formulas of lines that depend on only one independent variable. Multiple linear regression is similar to what we discussed previously, but here we have multiple independent variables (<em>x<sub>1</sub></em>, <em>x<sub>2</sub></em>, and so on). In this case, our simple equation of a line is as follows:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="255" src="assets/0f9f0a8c-4d78-47ad-9338-5b3775471f11.png"/></div>
<p>Here, the <em>x</em>'s are the various independent variables and the <em>m</em>'s are the various slopes associated with those independent variables. We also still have an intercept, <em>b</em>.</p>
<p>Multiple linear regression is a little harder to visualize and think about because this is no longer a line that can be visualized in two dimensions. It is a linear surface in two, three, or more dimensions. However, many of the same techniques that we used for our single linear regression will carry through.</p>
<p>Multiple linear regression has the same assumptions as regular linear regression. However, there are a few more pitfalls that we should keep in mind:</p>
<ul>
<li><strong>Overfitting</strong>: By adding more and more independent variables to our model, we are increasing our model complexity, which puts us at risk of overfitting. One technique to deal with this problem, which I would recommend looking into, is called <strong>regularization</strong>. Regularization creates a penalty term in your model that is a function of the complexity of your model, which helps keep this effect in check.</li>
<li><strong>Relative Scale</strong>: In some cases, one of your independent variables will be orders of magnitude different in scale than another independent variable. The larger of these could wash out any effect of the smaller, and you may need to consider normalizing your variables.</li>
</ul>
<p>With this in mind, let's try to expand our <strong>Sales</strong> model from a linear regression model to a multiple regression model. Looking back at our scatter plots from the previous section, we can see that <strong>Radio</strong> also appears to be linearly correlated with <strong>Sales</strong>, so let's try to create a multiple linear regression model that looks like the following:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="220" src="assets/9f6caa9b-e5e9-4918-820b-91db3369e2f1.png"/></div>
<p>To do this with <a href="http://github.com/sajari/regression" target="_blank">github.com/sajari/regression</a>, we just need to label another variable in the <kbd>regression.Regression</kbd> value and make sure that these values get paired in the training data points. We will then run the regression and see how the formula comes out:</p>
<pre>// Open the training dataset file.<br/>f, err := os.Open("training.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/>        <br/>// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
trainingData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// In this case we are going to try and model our Sales
// by the TV and Radio features plus an intercept.
var r regression.Regression
r.SetObserved("Sales")
r.SetVar(0, "TV")
r.SetVar(1, "Radio")

// Loop over the CSV records adding the training data.
for i, record := range trainingData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yVal, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Add these points to the regression value.
    r.Train(regression.DataPoint(yVal, []float64{tvVal, radioVal}))
}

// Train/fit the regression model.
r.Run()

// Output the trained model parameters.
fmt.Printf("\nRegression Formula:\n%v\n\n", r.Formula)</pre>
<p>Compiling and running this gives us the following regression formula:</p>
<pre><strong>$ go build
$ ./myprogram

Regression Formula:
Predicted = 2.93 + TV*0.05 + Radio*0.18</strong></pre>
<p>As you can see, the regression formula now includes an additional term for the <kbd>Radio</kbd> independent variable. The intercept value has also changed from our previous single regression model.</p>
<p>We can test this model similarly to the single regression model using the <kbd>Predict</kbd> method:</p>
<pre>// Open the test dataset file.
f, err = os.Open("test.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a CSV reader reading from the opened file.
reader = csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the test data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.ï¿¼
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted, err := r.Predict([]float64{tvVal, radioVal})

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("MAE = %0.2f\n\n", mAE)</pre>
<p>Running this reveals the following <kbd>MAE</kbd> for our new multiple regression model:</p>
<pre><strong>$ go build
$ ./myprogram

Regression Formula:
Predicted = 2.93 + TV*0.05 + Radio*0.18

MAE = 1.26</strong></pre>
<p>Our new multiple regression model has improved our MAE! Now we are definitely in pretty good shape to predict <kbd>Sales</kbd> based on our advertising spends. You could also try adding <kbd>Newspaper</kbd> to the model as a follow-up exercise to see how the model performance is influenced.</p>
<div class="packt_tip">Remember that as you add more complication to the model, you are sacrificing simplicity and you are potentially in danger of overfitting, so you should only add more complication if the gains in model performance actually create more value for your use case.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Nonlinear and other types of regression</h1>
                </header>
            
            <article>
                
<p>Although we have focused on linear regression in this chapter, you certainly are not limited to performing regression with linear formulas. You can model your dependent variable by one or more nonlinear terms such as powers, exponentials, or other transformations on your independent variables. For example, we could model <em>Sales</em> by a polynomial series of <em>TV</em> terms:</p>
<div style="padding-left: 120px" class="mce-root CDPAlignLeft CDPAlign"><img height="18" width="254" src="assets/7003a0ed-7126-4d23-b0a9-a68a28dd3a35.png"/></div>
<p>Keep in mind, however, that as you add this complexity, you are again putting yourself in danger of overfitting.</p>
<p>In terms of implementing non-linear regressions, you cannot use <kbd>github.com/sajari/regression</kbd>, which is limited to linear regression. However, <kbd>go-hep.org/x/hep/fit</kbd> <span>allows you to fit or train certain nonlinear models, and there are other various people in the Go community that have, or are, developing other tools for nonlinear modeling.</span></p>
<p>There are also other linear regression techniques, outside of OLS, that help overcome some of the assumptions and weaknesses associated with least squared linear regression. These include <strong>ridge regression</strong> and <strong>lasso regression</strong>. Both of these techniques penalize regression coefficients so as to mitigate the effects of multicollinearity and non-normality of independent variables.</p>
<p>In terms of Go implementations, ridge regression is implemented in <kbd>github.com/berkmancenter/ridge</kbd>. As opposed to <kbd>github.com/sajari/regression</kbd>, our independent variable and dependent variable data is input into <kbd>github.com/berkmancenter/ridge</kbd> via gonum matrices. Thus, to illustrate this method, let's first form a matrix containing our advertising spend features (<kbd>TV</kbd>, <kbd>Radio</kbd>, and <kbd>Newspaper</kbd>) and a matrix containing our <kbd>Sales</kbd> data. Note that in <kbd>github.com/berkmancenter/ridge</kbd>, we need to explicitly add a column to our input independent variable matrix for an intercept if we want to have an intercept in our model. Each value in this column is just <kbd>1.0</kbd>.</p>
<pre>// Open the training dataset file.<br/>f, err := os.Open("training.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/>        <br/>// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)
reader.FieldsPerRecord = 4

// Read in all of the CSV records
rawCSVData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}<br/><br/>// featureData will hold all the float values that will eventually be
// used to form our matrix of features.
featureData := make([]float64, 4*len(rawCSVData))
yData := make([]float64, len(rawCSVData))

// featureIndex and yIndex will track the current index of the matrix values.
var featureIndex int
var yIndex int

// Sequentially move the rows into a slice of floats.
for idx, record := range rawCSVData {
   <br/>    // Skip the header row.
    if idx == 0 {
        continue
    }

    // Loop over the float columns.
    for i, val := range record {

        // Convert the value to a float.
        valParsed, err := strconv.ParseFloat(val, 64)
        if err != nil {
            log.Fatal(err)
        }

        if i &lt; 3 {

            // Add an intercept to the model.
            if i == 0 {
                featureData[featureIndex] = 1
                featureIndex++
            }

            // Add the float value to the slice of feature floats.
            featureData[featureIndex] = valParsed
            featureIndex++
        }

        if i == 3 {

            // Add the float value to the slice of y floats.
            yData[yIndex] = valParsed
            yIndex++
        }<br/>    }
}

// Form the matrices that will be input to our regression.
features := mat64.NewDense(len(rawCSVData), 4, featureData)
y := mat64.NewVector(len(rawCSVData), yData)</pre>
<p>Next, we create a new <kbd>ridge.RidgeRegression</kbd> value with our independent and dependent variable matrices and call the <kbd>Regress()</kbd> method to train our model. We can then print out our trained regression formula:</p>
<pre>// Create a new RidgeRegression value, where 1.0 is the
// penalty value.
r := ridge.New(features, y, 1.0)

// Train our regression model.
r.Regress()

// Print our regression formula.
c1 := r.Coefficients.At(0, 0)
c2 := r.Coefficients.At(1, 0)
c3 := r.Coefficients.At(2, 0)
c4 := r.Coefficients.At(3, 0)
fmt.Printf("\nRegression formula:\n")
fmt.Printf("y = %0.3f + %0.3f TV + %0.3f Radio + %0.3f Newspaper\n\n", c1, c2, c3, c4)</pre>
<p>Compiling this program and running it gives the following regression formula:</p>
<pre><strong>$ go build
$ ./myprogram

Regression formula:
y = 3.038 + 0.047 TV + 0.177 Radio + 0.001 Newspaper</strong></pre>
<p>Here, you can that see the coefficients for <kbd>TV</kbd> and <kbd>Radio</kbd> are similar to what we got with least squares regression, but they are slightly different. Also, note that we went ahead and added a term for the <kbd>Newspaper</kbd> feature.</p>
<p>We can test this ridge regression formula by creating our own <kbd>predict</kbd> function:</p>
<pre>// predict uses our trained regression model to made a prediction based on a
// TV, Radio, and Newspaper value.
func predict(tv, radio, newspaper float64) float64 {
        return 3.038 + tv*0.047 + 0.177*radio + 0.001*newspaper
}</pre>
<p>Then, we use this <kbd>predict</kbd> function to test our ridge regression formula on our test examples:</p>
<pre>// Open the test dataset file.<br/>f, err := os.Open("test.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/>       <br/>// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// Read in all of the CSV records
reader.FieldsPerRecord = 4
testData, err := reader.ReadAll()
if err != nil {
    log.Fatal(err)
}

// Loop over the holdout data predicting y and evaluating the prediction
// with the mean absolute error.
var mAE float64
for i, record := range testData {

    // Skip the header.
    if i == 0 {
        continue
    }

    // Parse the Sales.
    yObserved, err := strconv.ParseFloat(record[3], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the TV value.
    tvVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Radio value.
    radioVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Parse the Newspaper value.
    newspaperVal, err := strconv.ParseFloat(record[2], 64)
    if err != nil {
        log.Fatal(err)
    }

    // Predict y with our trained model.
    yPredicted := predict(tvVal, radioVal, newspaperVal)

    // Add the to the mean absolute error.
    mAE += math.Abs(yObserved-yPredicted) / float64(len(testData))
}

// Output the MAE to standard out.
fmt.Printf("\nMAE = %0.2f\n\n", mAE)</pre>
<p>Compiling and running this gives us the following new <kbd>MAE</kbd>:</p>
<pre><strong>$ go build
$ ./myprogram

MAE = 1.26</strong></pre>
<p>Notice that adding <kbd>Newspaper</kbd> to the model did not actually improve our <kbd>MAE</kbd>. Thus, this would not be a good idea in this case, because it is adding further complications and not providing any significant changes in our model performance.</p>
<div class="packt_tip">Any complication or sophistication that you are adding to a model should be accompanied by a measurable justification for this added complication. Using a sophisticated model because it is intellectually interesting is a recipe for headaches.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Linear regression:</p>
<ul>
<li>
<p class="mce-root">Ordinary least squares regression explained visually: <a href="http://setosa.io/ev/ordinary-least-squares-regression/">http://setosa.io/ev/ordinary-least-squares-regression/</a></p>
</li>
<li><kbd>github.com/sajari/regression</kbd> docs: <a href="http://godoc.org/github.com/sajari/regression">http://godoc.org/github.com/sajari/regression</a></li>
</ul>
<p>Multiple regression:</p>
<ul>
<li>Multiple regression visualization: <a href="http://shiny.stat.calpoly.edu/3d_regression/">http://shiny.stat.calpoly.edu/3d_regression/</a></li>
</ul>
<p>Nonlinear and other regressions:</p>
<ul>
<li><kbd>go-hep.org/x/hep/fit</kbd> docs: <a href="https://godoc.org/go-hep.org/x/hep/fit">https://godoc.org/go-hep.org/x/hep/fit</a></li>
<li><kbd>github.com/berkmancenter/ridge</kbd> docs: <a href="https://godoc.org/github.com/berkmancenter/ridge">https://godoc.org/github.com/berkmancenter/ridge</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Congratulations! You have officially done machine learning using Go. In particular, you have learned about regression models, including linear regression, multiple regression, nonlinear regression, and ridge regression. You should be able to implement basic linear regressions and multiple regressions in Go.</p>
<p>Now that we have our feet wet with machine learning, we are going to move on to classification problems in the next chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>