- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Building and Using a Dataset
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和使用数据集
- en: The data collection and curation process is one of the most important stages
    in model building. It is also one of the most time-consuming. Typically, data
    can come from many sources; for example, customer records, transaction data, or
    stock lists. Nowadays, with the timely conjunction of big data, fast, high-capacity
    SSDs (to store big data), and GPUs (to process big data), it is easier for individuals
    to collect, store, and process data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集和整理过程是模型构建中最重要阶段之一。它也是耗时最长的。通常，数据可以来自许多来源；例如，客户记录、交易数据或股票清单。如今，随着大数据的及时结合、快速、高容量的SSD（用于存储大数据）和GPU（用于处理大数据），个人收集、存储和处理数据变得更加容易。
- en: In this chapter, you will learn about finding and accessing pre-existing, ready-made
    data sources that can be used to train your model. We will also look at ways to
    create your own datasets, transforming datasets so that they are useful for your
    problem, and we will also see how non-English datasets can be utilized.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解如何找到和访问可用于训练你的模型的现有、现成的数据源。我们还将探讨创建自己的数据集的方法，转换数据集以便它们对你的问题有用，以及我们将看到如何利用非英语数据集。
- en: In the remainder of this book, we will be using a selection of the datasets
    listed in this chapter to train and test a range of classifiers. When we do this,
    we will want to assess how well the classifiers work on each of the datasets—one
    of the major lessons of this book is that different classifiers work well with
    different datasets, and to see how well a classifier works with a given dataset,
    we will need ways of measuring performance. We will therefore end this chapter
    by looking at metrics for assessing the performance of a classifier on a dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，我们将使用本章中列出的数据集的一部分来训练和测试一系列分类器。当我们这样做时，我们希望评估分类器在每个数据集上的表现如何——本书的一个重要教训是不同的分类器与不同的数据集配合得很好，为了了解分类器与给定数据集配合得如何，我们需要衡量性能的方法。因此，我们将通过查看评估分类器在数据集上性能的指标来结束本章。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Ready-made data sources
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预制数据源
- en: Creating your own dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自己的数据集
- en: Other data sources
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他数据源
- en: Transforming data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Non-English datasets
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非英语数据集
- en: Evaluation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估
- en: Ready-made data sources
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预制数据源
- en: There are lots of places where ready-made data is available and usually freely
    downloadable. These are typically referred to as **public data sources** and are
    usually made available by companies, institutions, and organizations that are
    happy to either share their data (perhaps for publicity, or to entice others to
    share) or to act as a repository for others to make their data easily searchable
    and accessible. Clearly, these will only be useful to you if your need matches
    the data source, but if it does, it can be a great starting point or even a supplement
    to your own data. The good news is that these data sources usually cover a wide
    range of domains, so it’s likely you’ll find something useful.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多地方可以找到现成的数据，通常可以免费下载。这些通常被称为**公共数据源**，通常由愿意分享其数据（可能为了宣传，或者吸引他人分享）或作为其他人的数据存储库以使数据易于搜索和访问的公司、机构和组织提供。显然，这些数据源只有当你的需求与数据源相匹配时才有用，但如果确实如此，它可以是一个很好的起点，甚至可以补充你自己的数据。好消息是，这些数据源通常覆盖广泛的领域，因此你很可能找到有用的东西。
- en: 'We''ll now discuss some of these public data sources (in no particular order):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论一些这些公共数据源（不分先后顺序）：
- en: '**Kaggle**: Founded in 2010, Kaggle is a part of Google and, according to *Wikipedia*,
    is an “online community of data scientists and machine learning practitioners
    ". Kaggle has many features, but it is best known for its competitions in which
    anyone (for example, individuals and organizations) can publish a competition
    (typically a data science task) for participants to enter and compete to win prizes
    (sometimes in the form of cash!). However, Kaggle also allows users to find and
    publish datasets. Users can upload their datasets to Kaggle and also download
    datasets published by others.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kaggle**：成立于2010年，Kaggle是谷歌的一部分，根据*维基百科*，是一个“数据科学家和机器学习实践者的在线社区”。Kaggle有许多功能，但最出名的是其竞赛，任何人（例如个人和组织）都可以发布竞赛（通常是一个数据科学任务），让参与者进入并竞争奖品（有时以现金的形式！）。然而，Kaggle还允许用户查找和发布数据集。用户可以将他们的数据集上传到Kaggle，也可以下载其他人发布的数据集。'
- en: Everything on Kaggle is completely free, including datasets, and although the
    datasets are open source (open and free for download, modification, and reuse),
    for some datasets, it will be necessary to refer to the license to ascertain the
    purposes for which the dataset can be used. For example, some datasets may not
    be used for academic publications or commercial purposes. Users are also allowed
    to upload code to process the dataset, post comments against the dataset, and
    upvote the dataset so that others know that it is a reliable and useful dataset.
    There are also various other **Activity Overview** metrics, such as when the dataset
    was downloaded, how many times the dataset was downloaded, and how many times
    it was viewed. Since there are so many datasets (approximately 170,000 at the
    time of writing) these metrics can help you decide whether a dataset is worth
    downloading or not.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 上的所有内容都是完全免费的，包括数据集，尽管数据集是开源的（开放且免费下载、修改和重用），但对于某些数据集，可能需要参考许可证以确定数据集的使用目的。例如，某些数据集可能不能用于学术出版物或商业目的。用户还被允许上传处理数据集的代码，对数据集发表评论，并对数据集进行点赞，以便其他人知道这是一个可靠且有用的数据集。还有各种其他
    **活动概述** 指标，例如数据集何时被下载、数据集被下载了多少次以及数据集被查看了多少次。由于数据集数量众多（在撰写本文时约为 170,000 个），这些指标可以帮助您决定是否值得下载数据集。
- en: 'URL: https://www.kaggle.com/datasets'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)
- en: '**Hugging Face**: The Hugging Face Hub is a community-driven collection of
    datasets that span a variety of domains and tasks—for example, **natural language
    processing** (**NLP**), **computer vision** (**CV**), and audio. Each dataset
    is a Git repository that has the scripts required to download the data and generate
    splits for training, evaluation, and testing. There are approximately 10,000 datasets
    that can be filtered by the following criteria:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face**：Hugging Face Hub 是一个由社区驱动的数据集集合，涵盖了各种领域和任务——例如，**自然语言处理**（**NLP**）、**计算机视觉**（**CV**）和音频。每个数据集都是一个
    Git 仓库，包含下载数据和生成训练、评估和测试分割所需的脚本。大约有 10,000 个数据集可以通过以下标准进行筛选：'
- en: Task categories (text classification, QA, text generation, etc.)
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务类别（文本分类、问答、文本生成等）
- en: Tasks (language modeling, multi-class classification, language inference, etc.)
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务（语言建模、多类分类、语言推断等）
- en: Language (English, French, German, etc.)
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言（英语、法语、德语等）
- en: Multilinguality (monolingual, multilingual, translation, etc.)
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言性（单语、多语、翻译等）
- en: Size (10-100K, 1K-10K, 100K-1M, etc.)
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小（10-100K、1K-10K、100K-1M 等）
- en: License (CC by 4.0, MIT, others, etc.)
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许可证（CC by 4.0、MIT、其他等）
- en: Each dataset page includes a view of the first 100 rows of the dataset and also
    has a handy feature that allows you to copy the code to load a dataset. Some datasets
    also contain a loading script, which also allows you to easily load the dataset.
    Where the dataset does not include this loading script, the data is usually stored
    directly in the repository, in CSV, JSON, or Parquet format.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集页面都包含数据集前 100 行的视图，并且有一个方便的功能，允许您复制代码以加载数据集。一些数据集还包含加载脚本，这也允许您轻松加载数据集。如果数据集不包含此加载脚本，数据通常直接存储在仓库中，格式为
    CSV、JSON 或 Parquet。
- en: 'URL: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://huggingface.co/datasets](https://huggingface.co/datasets)
- en: '**TensorFlow Datasets**: TensorFlow Datasets offers a set of datasets that
    are suitable for use not just with TensorFlow but also with other Python **machine
    learning** (**ML**) frameworks. Each dataset is presented as a class to allow
    the building of efficient data input pipelines and user-friendly input processes.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Datasets**：TensorFlow Datasets 提供了一组适用于 TensorFlow 以及其他 Python
    **机器学习**（**ML**）框架的数据集。每个数据集都以类形式呈现，以便构建高效的数据输入管道和用户友好的输入过程。'
- en: 'URL: [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)
- en: '**Papers With Code**: This is a site that contains, as the name suggests, research
    papers along with their code implementations. At the time of writing, there are
    around 7,000 ML datasets available to freely download. These can be searched by
    using the following filters:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Papers With Code**：这是一个包含，正如其名所示，研究论文及其代码实现的网站。在撰写本文时，大约有 7,000 个机器学习数据集可供免费下载。可以使用以下筛选器进行搜索：'
- en: Modality (text, images, video, audio, etc.)
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模态（文本、图像、视频、音频等）
- en: Task (QA, object detection, image classification, text classification, etc.)
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务（问答、目标检测、图像分类、文本分类等）
- en: Language (English, Chinese, German, French, etc.)
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言（英语、中文、德语、法语等）
- en: According to its *About* page, all datasets are licensed under the CC BY-SA
    license, allowing anyone to use the datasets as long as the creator(s) are acknowledged.
    Usefully, each dataset lists papers that utilize the dataset, associated benchmarks,
    code, and similar datasets, and explains how the dataset can be loaded from within
    popular frameworks such as TensorFlow.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其“关于”页面，所有数据集均根据CC BY-SA许可进行授权，允许任何人在认可创作者（们）的情况下使用数据集。实用的是，每个数据集都列出了使用该数据集的论文、相关基准、代码和类似数据集，并解释了如何从流行的框架（如TensorFlow）中加载数据集。
- en: Papers With Code also encourages users to share their datasets with the community.
    The process is relatively simple and involves registering, uploading the dataset,
    and providing links and information (e.g. description, modality, task, language,
    etc.) about the dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Papers With Code还鼓励用户与社区分享他们的数据集。这个过程相对简单，包括注册、上传数据集，并提供有关数据集的链接和信息（例如，描述、模态、任务、语言等）。
- en: Note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Papers With Code’s *About* page states that although the core team is based
    at Meta AI Research no data is shared with any Meta Platforms product.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Papers With Code的“关于”页面声明，尽管核心团队位于Meta AI Research，但不会与任何Meta Platforms产品共享数据。
- en: 'URL: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://paperswithcode.com/datasets](https://paperswithcode.com/datasets)
- en: '**IEEE DataPort**: IEEE DataPort is an online data repository that was created,
    and is owned by, the **Institute of Electrical and Electronics Engineers** (**IEEE**),
    a professional association for electronic engineering, electrical engineering,
    and associated disciplines. At the time of writing, there are around 6,000 datasets
    available. These can be searched by using either free-text search terms (for example,
    title, author, or **digital object identifier** (**DOI**)) or filters, such as
    the following:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IEEE DataPort**：IEEE DataPort是一个由电气和电子工程师协会（**IEEE**）创建并拥有的在线数据存储库，**IEEE**是一个电子工程、电气工程和相关学科的行业协会。截至撰写本文时，大约有6,000个数据集可用。这些可以通过使用免费文本搜索词（例如，标题、作者或**数字对象标识符**（**DOI**））或以下筛选器进行搜索：'
- en: Category (**artificial intelligence** (**AI**), CV, ML, etc.)
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别（**人工智能**（**AI**）、CV、ML等）
- en: Type (Standard, Open Access)
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类型（标准、开放获取）
- en: Open Access datasets allow free access to all users, whereas accessing Standard
    datasets requires an IEEE paid subscription. IEEE DataPort also offers three options
    (Standard, Open Access, and Competition) for users to upload their datasets. Standard
    and Competition are free to upload and access; however, Open Access requires the
    purchase of an Open Access credit.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 开放获取数据集允许所有用户免费访问，而访问标准数据集则需要IEEE付费订阅。IEEE DataPort还提供三种选项（标准、开放获取和竞赛）供用户上传他们的数据集。标准和竞赛的上传和访问是免费的；然而，开放获取需要购买开放获取信用。
- en: 'URL: [https://ieee-dataport.org/datasets](https://ieee-dataport.org/datasets)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://ieee-dataport.org/datasets](https://ieee-dataport.org/datasets)
- en: '**Google Dataset Search**: Google Dataset Search is a search engine for datasets
    that features a simple keyword search engine (similar to the Google search page
    we all know and love) that allows users to find datasets that are themselves hosted
    in repositories (e.g. Kaggle) across the web. Results can then be filtered by
    the following criteria:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌数据集搜索**：谷歌数据集搜索是一个数据集搜索引擎，它具有一个简单的关键词搜索引擎（类似于我们所有人都熟悉和喜爱的谷歌搜索页面），允许用户找到自身托管在互联网上各个存储库（例如Kaggle）中的数据集。结果可以根据以下标准进行筛选：'
- en: Last updated (past month, year)
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后更新（过去一个月、一年）
- en: Download format (text, tabular, document, image, etc.)
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载格式（文本、表格、文档、图像等）
- en: Usage rights (commercial use allowed/not allowed)
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用权（允许/不允许商业用途）
- en: Topic (architecture and urban planning, computing, engineering, etc.)
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题（建筑和城市规划、计算、工程等）
- en: Free or paid
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 免费/付费
- en: The website states that the search engine only came out of beta in 2020, hence
    there may be more features added later. Being part of the Google ecosystem, it
    also allows users to easily bookmark datasets to return to later. As one would
    expect with Google, there is data available on a vast range of topics, from mobile
    apps to fast food and everything in between.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 网站声明，该搜索引擎仅在2020年从测试版中推出，因此以后可能还会添加更多功能。作为谷歌生态系统的一部分，它还允许用户轻松地将数据集添加到书签中以便稍后返回。正如人们所期望的那样，谷歌提供了关于广泛主题的数据，从移动应用到快餐以及所有介于其间的主题。
- en: 'URL: [https://datasetsearch.research.google.com](https://datasetsearch.research.google.com)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 网址：[https://datasetsearch.research.google.com](https://datasetsearch.research.google.com)
- en: '**BigQuery public datasets**: BigQuery is a **Google Cloud Platform** (**GCP**)
    product that was built to provide serverless, cost-effective, highly scalable
    data warehouse capabilities. Hence, BigQuery is used to host and access public
    datasets, making them publicly available for users to integrate into their applications
    via projects. Although the datasets are free, users must pay for the queries that
    are performed on the data. However, at the time of writing, the first 1 TB per
    month is free. There are many ways to access BigQuery public datasets: by using
    the Google Cloud console, by using the BigQuery REST API, or through Google Analytics
    Hub.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://cloud.google.com/bigquery/public-data](https://cloud.google.com/bigquery/public-data)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Public Data Explorer**: Google Public Data Explorer is a web-based
    tool that makes it easy to explore and visualize datasets as line graphs, bar
    graphs, plots, or on maps. It provides data from about 135 organizations and academic
    institutions such as The World Bank, The **World Trade Organization** (**WTO**),
    Eurostat, and the US Census Bureau. Users are also able to upload, visualize,
    and share their own data by making use of Google’s **Dataset Publishing Language**
    (**DSPL**) data format. Where the system really shines is when the charts are
    animated over time, making it easy even for non-scientists to understand the impact
    and gain insights.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://www.google.com/publicdata/directory](https://www.google.com/publicdata/directory)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '**UCI Machine Learning Repository**: The **University of California Irvine**
    (**UCI**) Machine Learning Repository was created as an FTP archive in 1987 by
    graduate students at UCI. It is a free (registration not required) collection
    of approximately 600 datasets that are available for the ML community. The main
    website is rudimentary and outdated with a Google-powered search and no filtering
    capabilities, but (at the time of writing) a new version is in beta testing and
    offers the ability to search using the following filters:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characteristics (text, tabular, sequential, time-series, image, etc.)
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject area (business, computer science, engineering, law, etc.)
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Associated tasks (classification, regression, clustering, etc.)
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of attributes (fewer than 10, 10-100, more than 100)
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of instances (fewer than 10, 10-100, more than 100)
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute types (numerical, categorical, mixed)
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets in the repository are donated by different authors and organizations,
    hence each dataset has individual license requirements. The site states that to
    use the datasets, citation information should be used, and usage policies and
    licenses should be checked.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://archive.ics.uci.edu](https://archive.ics.uci.edu)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Registry of Open Data on AWS**: The Registry of Open Data on AWS (short for
    Amazon Web Services) is a centralized repository that makes it easy to find publicly
    available datasets. These datasets are not provided by Amazon, as they are owned
    by government organizations, researchers, businesses, and individuals. The registry
    can be used to discover and share datasets. There are approximately 330 datasets
    available, and these are accessed via the AWS Data Exchange service (an online
    marketplace offering thousands of datasets). Being Amazon, much of this infrastructure
    is tied to the core AWS services; for example, datasets can be used with AWS resources
    and easily integrated into AWS cloud-based applications. As an example, it only
    takes minutes to provision an Amazon **Elastic Compute Cloud** (**EC2**) instance
    and start working with the data.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS 开放数据注册处**：AWS 开放数据注册处（简称 Amazon Web Services）是一个集中式存储库，便于查找公开可用的数据集。这些数据集不是由亚马逊提供的，因为它们由政府机构、研究人员、企业和个人拥有。该注册处可用于发现和共享数据集。大约有
    330 个数据集可用，这些数据集通过 AWS 数据交换服务（一个提供数千个数据集的在线市场）访问。作为亚马逊的一部分，大部分基础设施都与核心 AWS 服务相关联；例如，数据集可以与
    AWS 资源一起使用，并轻松集成到 AWS 基于云的应用程序中。例如，只需几分钟即可配置 Amazon **弹性计算云**（**EC2**）实例并开始处理数据。'
- en: 'URL: [https://registry.opendata.aws](https://registry.opendata.aws)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: URL：[https://registry.opendata.aws](https://registry.opendata.aws)
- en: '**US Government open data**: Launched in 2009, *Data.gov* is managed and hosted
    by the US General Services Administration and was created and launched by the
    US Government to provide access to federal, state, and local datasets. There are
    approximately 320,000 datasets that are made available in open, machine-readable
    formats, while continuing to maintain privacy and security, and can be searched
    by keyword or filtered by the following criteria:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**美国政府开放数据**：2009 年启动，*Data.gov* 由美国总务管理局管理和托管，并由美国政府创建和推出，以提供访问联邦、州和地方数据集。大约有
    320,000 个数据集以开放、机器可读的格式提供，同时继续维护隐私和安全，可以通过关键词搜索或按以下标准筛选：'
- en: Location
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置
- en: Topic (local government, climate, energy, etc.)
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题（地方政府、气候、能源等）
- en: Topic category (health, flooding water, etc.)
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题类别（健康、洪水水灾等）
- en: Dataset type (geospatial)
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集类型（地理空间）
- en: Format (CSV, HTML, XML, etc.)
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式（CSV、HTML、XML等）
- en: Organization type (federal, state, local, etc.)
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织类型（联邦、州、地方等）
- en: Organization (NASA, state, department, etc.)
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织（NASA、州、部门等）
- en: Publisher
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布者
- en: Bureau
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局
- en: The datasets are made available for free and without restriction, although they
    do advise that non-federal data available may have a different licensing method.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集免费且无限制提供，尽管他们建议非联邦数据可能具有不同的许可方式。
- en: 'URL: [https://data.gov](https://data.gov)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: URL：[https://data.gov](https://data.gov)
- en: '**data.gov.uk**: Similarly, the *data.gov.uk* site allows users to find public
    sector, non-personal data published by the UK central government, UK local authorities,
    and UK public bodies. The datasets are typically hosted on AWS. There are approximately
    52,000 datasets that can be filtered by the following criteria:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**data.gov.uk**：同样，*data.gov.uk* 网站允许用户查找由英国中央政府、英国地方当局和英国公共机构发布的公共部门、非个人信息。这些数据集通常托管在
    AWS 上。大约有 52,000 个数据集可以通过以下标准进行筛选：'
- en: Publisher (council)
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布者（议会）
- en: Topic (business and economy, crime and justice, education, etc.)
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题（商业和经济、犯罪和司法、教育等）
- en: Format (CSV, HTML, XLS, etc.)
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式（CSV、HTML、XLS等）
- en: The datasets are free (registration required), and licensing appears to be a
    mix, with some being **Open Government License** (**OGL**), which permits anyone
    to copy, distribute, or exploit the data, and others requiring **Freedom of Information**
    (**FOI**) requests for the dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集免费（需要注册），并且似乎许可方式是混合的，其中一些是**开放政府许可**（**OGL**），允许任何人复制、分发或利用数据，而其他则需要**信息自由**（**FOI**）请求以获取数据集。
- en: 'URL: [https://ukdataservice.ac.uk](https://ukdataservice.ac.uk)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: URL：[https://ukdataservice.ac.uk](https://ukdataservice.ac.uk)
- en: '**Microsoft Azure Open Datasets**: This is a curated repository of datasets
    that can be used to train models. However, there are only about 50 datasets, covering
    areas such as transport, health, and labor, as well as some common datasets. There
    are no charges for using most of the datasets.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Azure Open Datasets**：这是一个用于训练模型的精选数据集存储库。然而，只有大约 50 个数据集，涵盖交通、健康、劳动等领域，以及一些常见的数据集。使用大多数数据集无需付费。'
- en: 'URL: [https://azure.microsoft.com/en-us/products/open-datasets/](https://azure.microsoft.com/en-us/products/open-datasets/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: URL：[https://azure.microsoft.com/en-us/products/open-datasets/](https://azure.microsoft.com/en-us/products/open-datasets/)
- en: '**Microsoft Research Open Data**: This is another collection of free datasets
    from Microsoft and contains datasets useful for areas such as NLP and CV. Again,
    there are only about 100 datasets, which can be searched by text or can be filtered
    by the following criteria:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微软研究开放数据**：这是微软提供的另一组免费数据集，包含对NLP和CV等领域有用的数据集。同样，这里也只有大约100个数据集，可以通过文本搜索或通过以下标准进行筛选：'
- en: Category (computer science, math, physics, etc.)
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别（计算机科学、数学、物理等）
- en: Format (CSV, DOCX, JPG, etc.)
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式（CSV、DOCX、JPG等）
- en: License (Creative Commons, legacy Microsoft Research Data License Agreement,
    etc.)
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许可证（Creative Commons、旧版微软研究数据许可协议等）
- en: 'URL: [https://msropendata.com](https://msropendata.com)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: URL：[https://msropendata.com](https://msropendata.com)
- en: The preceding list is intended as an indicative, non-exhaustive guide for those
    who are unsure where to go to get data and provides examples of repositories from
    a number of organizations. There are also “repositories of repositories” where
    lists of dataset repositories are maintained, and these are good places to start
    searching for data. These include sites such as the DataCite Commons Repository
    Finder ([https://repositoryfinder.datacite.org](https://repositoryfinder.datacite.org))
    and the Registry of Research Data Repositories [https://re3data.org/](https://re3data.org/),
    which offers researchers an overview of existing repositories for research data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表旨在为不确定去哪里获取数据的人提供一个指示性的、非详尽的指南，并提供了一些组织的数据存储库的示例。还有“存储库的存储库”，其中维护了数据集存储库的列表，这些是开始搜索数据的好地方。这些包括DataCite
    Commons Repository Finder（[https://repositoryfinder.datacite.org](https://repositoryfinder.datacite.org)）和Research
    Data Repositories注册处[https://re3data.org/](https://re3data.org/)，它为研究人员提供了现有研究数据存储库的概述。
- en: It should also be noted that some of the most common popular datasets are also
    easily available from within Python packages such as TensorFlow, **scikit-learn**
    (**sklearn**), and the **Natural Language** **Toolkit** (**NLTK**).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，一些最常见的流行数据集也容易从Python包如TensorFlow、**scikit-learn**（**sklearn**）和**自然语言工具包**（**NLTK**）中获取。
- en: In this section, we saw how we can access ready-made data sources. However,
    sometimes these are inadequate, so let us next see how we can create our own data
    sources.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何访问现成的数据源。然而，有时这些数据源是不够的，因此接下来让我们看看我们如何可以创建自己的数据源。
- en: Creating your own dataset
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建您自己的数据集
- en: 'Although we have seen several sources where datasets can be obtained, sometimes
    it is necessary to build your own datasets either using your own data or using
    data from other sources. This may be because the available datasets are not adequate
    for our problem, and this approach also brings some additional benefits, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经看到了可以获取数据集的几个来源，但有时有必要使用您自己的数据或使用其他来源的数据来构建自己的数据集。这可能是因为可用的数据集不足以解决我们的问题，这种方法也带来了一些额外的益处，如下所述：
- en: Creating your own dataset can eliminate the challenges associated with third-party
    datasets that often have licensing terms or usage restrictions.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建您自己的数据集可以消除与第三方数据集相关的挑战，这些数据集通常具有许可条款或使用限制。
- en: There are no fees to pay (although building the dataset will incur costs).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有费用需要支付（尽管构建数据集会产生成本）。
- en: If the dataset is being created using your own data, there are no ownership
    issues. If not, then it is your responsibility to consider ownership issues, and
    appropriate steps should be taken.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集是使用您自己的数据创建的，则不存在所有权问题。如果不是这样，那么您有责任考虑所有权问题，并应采取适当的措施。
- en: You have complete ownership and flexibility in how you use the data.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您在使用数据方面拥有完全的所有权和灵活性。
- en: A fuller understanding of the data is gained as part of building the dataset.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建数据集的过程中，可以更全面地了解数据。
- en: Creating your own dataset also comes with increased responsibility; in other
    words, if there are any errors, issues, or biases, there will be only one person
    to blame!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 创建您自己的数据集也伴随着更大的责任；换句话说，如果存在任何错误、问题或偏见，那么将只有一个人要承担责任！
- en: Clearly, many types of data can be collected—for example, financial data, data
    from **Internet of Things** (**IoT**) devices, and data from databases. However,
    since the purpose of this book is the emotional analysis of text, we will demonstrate
    some ways to collect textual data to build datasets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，可以收集许多类型的数据——例如，财务数据、**物联网**（**IoT**）设备的数据和数据库中的数据。然而，由于本书的目的是文本的情感分析，我们将展示一些收集文本数据以构建数据集的方法。
- en: Data from PDF files
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PDF文件中的数据
- en: The **Portable Document Format** (**PDF**) format is one of the most popular
    and widely used digital file formats and is used to present and exchange documents.
    Many organizations use the PDF format to publish documentation, release notes,
    and other document types because files can be read anywhere,
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**便携式文档格式**（**PDF**）格式是最受欢迎和广泛使用的数字文件格式之一，用于展示和交换文档。许多组织使用PDF格式发布文档、发布说明和其他文档类型，因为文件可以在任何地方阅读，'
- en: 'on any device, as long as (free) tools such as Adobe Acrobat Reader are installed.
    Consequently, this makes PDF files a good place to look for data. Luckily for
    us, Python has a number of libraries to help us extract text from PDF files, as
    listed here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何设备上，只要（免费）工具如Adobe Acrobat Reader已安装。因此，这使得PDF文件成为寻找数据的好地方。幸运的是，Python有许多库可以帮助我们从PDF文件中提取文本，如下所示：
- en: PyPDF4
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyPDF4
- en: PDFMiner
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDFMiner
- en: PDFplumber
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PDFplumber
- en: There are many others, but these seem to be the most popular. For this example,
    due to our previous experiences, we will use PyPDF4.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多其他的，但这些似乎是最受欢迎的。由于我们之前的经验，我们将使用PyPDF4。
- en: 'Firstly, we need to ensure that the PyPDF4 module is installed. Here’s the
    command we run to achieve this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确保PyPDF4模块已安装。这是我们运行以实现此目的的命令：
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we need to import the package and set up a variable that contains the
    name of the file we wish to process. For this example, a sample PDF was downloaded
    from [https://www.jbc.org/article/S0021-9258(19)52451-6/pdf](https://www.jbc.org/article/S0021-9258(19)52451-6/pdf):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要导入包并设置一个变量，该变量包含我们希望处理的文件名。对于这个例子，从 [https://www.jbc.org/article/S0021-9258(19)52451-6/pdf](https://www.jbc.org/article/S0021-9258(19)52451-6/pdf)
    下载了一个样本PDF文件：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we need to set up some objects that will actually allow us to read the
    PDF file, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置一些对象，实际上允许我们读取PDF文件，如下所示：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'PyPDF4 can also extract metadata (data about the file) from the PDF. Here’s
    how to do that:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PyPDF4还可以从PDF中提取元数据（关于文件的数据）。以下是这样做的方法：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output shows the title, author, and subject of the document (there are
    also other fields available):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了文档的标题、作者和主题（还有其他可用字段）：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also get a count of the number of pages in the document by executing
    the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过执行以下代码来获取文档中页数的数量：
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output shows the number of pages in the document:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了文档中的页数：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now iterate through each page, extract the text, and write it to a database
    or a file, like so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以遍历每一页，提取文本，并将其写入数据库或文件，如下所示：
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That’s it! We have extracted the text from a PDF file and can use it to build
    a dataset and ultimately use it to train a model (after cleaning and preprocessing).
    Of course, in reality, we would wrap this into a function and iterate a folder
    of files to create a proper dataset, so let’s do that.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经从PDF文件中提取了文本，并可以使用它来构建数据集，最终用于训练模型（在清理和预处理之后）。当然，在现实中，我们会将其封装成一个函数，并迭代一个文件夹中的文件来创建合适的数据集，所以让我们来做这件事。
- en: 'First, we need to import the appropriate libraries and set up a folder where
    the PDF files are, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入适当的库并设置一个文件夹，其中包含PDF文件，如下所示：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can refactor the code we had originally and reengineer it in the form
    of some handy reusable functions:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重构我们最初编写的代码，并以一些方便的可重用函数的形式重新设计它：
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note how, in `save_content,` there is a placeholder where you would normally
    write the extracted content to a database.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 `save_content` 中有一个占位符，你通常会在这里将提取的内容写入数据库。
- en: 'And finally, here’s the main code where we iterate the folder and, for each
    PDF file, extract the content:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是主代码，我们迭代文件夹，并对每个PDF文件提取内容：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we have seen, extracting text from PDF files is pretty straightforward. Let’s
    now see how we can get data from the internet.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，从PDF文件中提取文本相当简单。现在，让我们看看我们如何从互联网获取数据。
- en: Data from web scraping
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络爬取的数据
- en: 'Nowadays, there is so much publicly available data on the web in the form of
    (for example) news, blogs, and social media that it makes sense to gather (“harvest”)
    and make use of this. The process of extracting data from a website is known as
    **web scraping**, and although this can be done manually, that would not be an
    efficient use of time and resources, especially when there are plenty of tools
    to help automate the process. The steps to do this are something like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，网络上公开可用的数据非常多，形式包括（例如）新闻、博客和社交媒体，因此收集（“收割”）并利用这些数据是有意义的。从网站中提取数据的过程称为**网络抓取**，尽管这可以手动完成，但这并不是有效利用时间和资源的方式，尤其是在有大量工具可以帮助自动化这个过程时。进行这一过程的步骤大致如下：
- en: Identify a root URL (a starting point).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定一个根URL（起点）。
- en: Download the page content.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载页面内容。
- en: Process/clean/format the downloaded text.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理/清理/格式化下载的文本。
- en: Save the cleaned text.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存清理后的文本。
- en: 'Although there are no hard and fast rules, there are some rules of etiquette
    that will stop your program from getting blocked and some rules that will make
    scraping easier that should be followed:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有硬性规定，但有一些礼仪规则可以防止你的程序被封锁，还有一些规则可以使抓取过程更加容易，这些规则应该遵循：
- en: Add a delay between each scrape request so the site does not get overloaded
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次抓取请求之间添加延迟，以防止网站过载
- en: Scrape during non-peak hours
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非高峰时段进行抓取
- en: Do note here that it is important that data is only scraped from sources that
    allow it, as unauthorized scraping can infringe terms of service and intellectual
    property rights and may even have legal ramifications. It is also a sensible idea
    to examine the metadata as it may provide guidance on whether the data is sensitive
    or private, data provenance, permissions, and restrictions on use. Being respectful
    of source permissions and data sensitivity are important considerations in responsible
    and ethical web scraping.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里重要的是只从允许抓取的数据源中抓取数据，因为未经授权的抓取可能会侵犯服务条款和知识产权，甚至可能产生法律后果。检查元数据也是一个明智的想法，因为它可能提供有关数据是否敏感或私密的指导，数据来源，权限和使用限制。尊重源权限和数据敏感性是负责任和道德网络抓取的重要考虑因素。
- en: Let’s begin!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 'Firstly, we need to ensure that the Beautiful Soup module is installed. We
    can do that using the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确保Beautiful Soup模块已安装。我们可以使用以下代码来完成：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To prevent any unforeseen errors, please ensure that the following versions
    are installed:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止任何意外错误，请确保以下版本已安装：
- en: Beautiful Soup 4.11.2
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Beautiful Soup 4.11.2
- en: lxml 4.9.3
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: lxml 4.9.3
- en: 'We then import the required libraries:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后导入所需的库：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also need a URL to start scraping from. Here’s an example:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个URL来开始抓取。以下是一个示例：
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We now need to separate interesting, relevant content from the non-useful elements
    of a web page, such as the menu, header, and footer. Every website has its own
    set of design styles and conventions and will display its content in its own unique
    manner. For the website we chose, we found that looking for three consecutive
    `<p>` tags homed in on the content part of the page. It’s very likely that this
    logic will be different for the website you are scraping from. To find these `<p>`
    tags, we define a **regular expression** (**regex**), as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要将网页中有趣、相关的内容与非有用元素（如菜单、页眉和页脚）分开。每个网站都有自己的设计风格和约定，并以独特的方式显示其内容。对于我们选择的网站，我们发现查找三个连续的`<p>`标签可以定位到页面内容部分。对于你要抓取的网站，这种逻辑可能不同。为了找到这些`<p>`标签，我们定义了一个**正则表达式**（**regex**），如下所示：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We now need to request the HTML for the website and extract the paragraphs
    using the regex. This text can then be cleaned (for example, any inline HTML removed)
    and saved to a database:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要请求网站的HTML并使用正则表达式提取段落。然后，可以将这些文本清理（例如，移除任何内联HTML）并保存到数据库中：
- en: '[PRE15]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'However, we can go one step further. By extracting the hyperlinks from this
    page, we can get our program to keep scraping deeper into the website. This is
    where the previous commentary on best practices should be applied:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以更进一步。通过从该页面上提取超链接，我们可以让我们的程序继续深入抓取网站。这就是之前关于最佳实践评论应该应用的地方：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we need some code to start the scrape:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一些代码来开始抓取：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To prevent the program from ending up in a loop, a list of visited URLs should
    be maintained and checked before scraping each URL—we have left this as an exercise
    for the reader.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止程序陷入循环，应该维护一个已访问的URL列表，并在抓取每个URL之前进行检查——我们将这个练习留给了读者。
- en: Note
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you get a `<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
    failed: unable to get local issuer certificate (_ssl.c:997)>` error, you can use
    this link to resolve it: [https://stackoverflow.com/a/70495761/5457712](https://stackoverflow.com/a/70495761/5457712).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Data from RSS feeds
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RSS** (short for **Really Simple Syndication**) is a relatively old technology.
    Once upon a time, it was used to collate all the latest news into a web browser.
    Nowadays, it is not as popular as it once was but is still used by many to stay
    up to date. Most news providers provide RSS feeds on their websites.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: An RSS feed is typically an **Extensible Markup Language** (**XML**) document
    that includes a URL to a web page (that can be scraped, as we have seen), full
    or summarized text, and metadata such as the publication date and the author’s
    name.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can create a dataset of news headlines.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, firstly we need to ensure that the module we need is installed. `feedparser`
    is a Python library that works with feeds in all known formats. You can install
    it using the following command:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we import it, like so:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We also need a feed URL to work with. Here’s an example:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, it is a simple matter of downloading the feed and extracting the relevant
    parts. For news headlines, we envisage that the summary contains more information,
    so it should be saved to a database:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we need some code to start the process:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output shows the URL, title, and summary from each element in the feed:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let us next take a look at how a more robust technology, APIs, can be used to
    download data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Data from APIs
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: X (formerly Twitter) is a fantastic place to obtain text data; it offers an
    easy-to-use API. It is free to start off with, and there are many Python libraries
    available that can be used to call the API.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the free X (Twitter) API is in a state of flux, and
    it may no longer be possible to use the `tweepy` API.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that, later on in this book, we work with tweets, it is sensible at this
    point to learn how to extract tweets from Twitter. For this, we need a package
    called `tweepy`. Use the following command to install `tweepy`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we need to sign up for an account and generate some keys, so proceed
    as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://developer.twitter.com/en](https://developer.twitter.com/en) and
    sign up for an account.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [https://developer.twitter.com/en/portal/projects-and-apps](https://developer.twitter.com/en/portal/projects-and-apps).
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create App** in the **Standalone** **Apps** section.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give your app a name, and make a note of the **API Key**, **API Key Secret**,
    and **Bearer** **Token** values.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **App Settings** and then click the **Keys and** **Tokens** tab.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this page, click **Generate** in the **Access Token and Secret** section
    and again make a note of these values.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are now ready to use these keys to get some tweets from Twitter! Let’s run
    the following code:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: You must replace the `YOUR_KEY_HERE` token with your own keys.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a class with a subclassed special method called `on_tweet` that
    is triggered when a tweet is received from this stream. The code is actually pretty
    simple and looks like this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Tweepy insists that “rules” are added to filter the stream, so let’s add a
    rule that states we are only interested in tweets that contain the `#``lfc` hashtag:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'See here for more about Tweepy rules: [https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Heavy use of the X (Twitter) API may need a paid package.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Other data sources
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have listed some of the commonly used sources of data in the previous section.
    However, there are probably many thousands of other free datasets available. You
    just need to know where to look. The following is a list of some of the interesting
    ones that we came across as part of our work on emotion analysis. There are probably
    many more available all over the internet:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Dr. Saif Mohammad is a Senior Research Scientist at the **National Research
    Council** (**NRC**) Canada. He has published many papers and has been heavily
    involved with *SemEval*, as one of the organizers, for many years. He has also
    published many different, free-for-research purposes datasets that have been used
    primarily for competition purposes. Many of these are listed on his website at
    http://saifmohammad.com/WebPages/SentimentEmotionLabeledData.xhtml, although some
    are better described on the associated competition page, as presented here:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Emotion Intensity** (**EmoInt**) dataset has four datasets for four emotions
    (http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.xhtml).
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Workshop on Computational Approaches to Subjectivity, Sentiment and Social
    Media Analysis** (**WASSA**) dataset is a total of 3,960 English tweets, each
    labeled with an emotion of anger, fear, joy, and sadness. Each tweet also has
    a real-valued score between 0 and 1, indicating the degree or intensity of the
    emotion felt by the speaker ([https://wt-public.emm4u.eu/wassa2017/](https://wt-public.emm4u.eu/wassa2017/)).
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SemEval* (Mohammad et al., 2018) is an annual competition in which teams of
    researchers from all over the world work on tasks where they develop systems to
    categorize datasets. The exact task varies from year to year. It has been running
    intermittently since 1998, but since 2012, it has become an annual event. A number
    of datasets have come about from this competition, as follows:'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task E-c**: A dataset containing tweets classified as “neutral or no
    emotion” or as 1, or more, of 11 given emotions that best represent the mental
    state of the tweeter.'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task EI-reg**: A dataset containing tweets labeled for emotion (anger,
    fear, joy, sadness), and for intensity, a real-valued score between 0 and 1, with
    a score of 1 indicating that the highest amount of emotion was inferred and a
    score of 0 indicating the lowest amount of emotion was inferred. The authors note
    that these scores have no inherent meaning; they are only used as a mechanism
    to convey that the instances with higher scores correspond to a greater degree
    of emotion than instances with lower scores.'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task EI-oc**: A dataset containing tweets labeled for emotion (anger,
    fear, joy, sadness) and one of four ordinal classes of the intensity of emotion
    that best represented the mental state of the tweeter. These datasets are all
    available from https://competitions.codalab.org/competitions/17751.'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Across the competition years, there also seem to be plenty of datasets labeled
    for sentiment, as follows:'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2014/task9/](https://alt.qcri.org/semeval2014/task9/)'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools](https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools)'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2017/task4/](https://alt.qcri.org/semeval2017/task4/)'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hashtag Emotion Corpus dataset contains tweets with emotion word hashtags
    ([http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip](http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip)).
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **International Survey On Emotion Antecedents And Reactions** (**ISEAR**)
    dataset contains reports of situations in which student respondents had experienced
    emotions (joy, fear, anger, sadness, disgust, shame, and guilt) ([https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular dataset labeled for sentiment (negative, neutral, positive) ([https://data.mendeley.com/datasets/z9zw7nt5h2](https://data.mendeley.com/datasets/z9zw7nt5h2)).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset labeled for six emotions (anger, fear, joy, love, sadness, surprise)
    ([https://github.com/dair-ai/emotion_dataset](https://github.com/dair-ai/emotion_dataset)).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextualized Affect Representations for Emotion Recognition** (**CARER**)
    is an emotion dataset collected through noisy labels, annotated for six emotions
    (anger, fear, joy, love, sadness, and surprise) ([https://paperswithcode.com/dataset/emotion](https://paperswithcode.com/dataset/emotion)).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: It is vitally important to consider data privacy and ethical concerns when using
    these datasets.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to access ready-made data sources and how to create your own
    data source. However, there may be occasions where these are good but not quite
    what we are looking for. Let’s see how we can tackle that problem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have seen that there are many sentiment and emotion datasets available,
    it is rare that a dataset meets all the exact requirements. However, there are
    ways to tackle this problem.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen some datasets labeled for sentiment, some for emotion, and also
    others labeled for more exotic things such as valence that do not seem to fit
    into what we are looking for: the emotion analysis problem. However, in certain
    circumstances, it is still possible to repurpose and use these. For example, if
    a dataset contains emotions, we can transform this into a sentiment dataset simply
    by assuming that the “anger” emotion is a negative sentiment and the “joy” emotion
    is a positive sentiment. A degree of subjectivity and manual analysis of the individual
    dataset is then required to determine which emotions would constitute a good substitute
    for the neutral sentiment. In our experience, this is typically not straightforward.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation** is the name given to the process of applying changes
    to a dataset to make that dataset more useful for your purpose. This could include
    adding data, removing data, or any process that makes that data more useful. Let
    us consider some examples from the *Other data sources* section and see how we
    can repurpose them.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The EI-reg dataset, as described previously, contains tweets that were annotated
    for emotion (anger, fear, joy, sadness) and for intensity with a score between
    0 and 1\. We can sensibly guess that scores around and under 0.5 are not going
    to be indicative of highly emotive tweets and hence tweets with scores under 0.5
    can be removed and the remaining tweets used to create a reduced, but potentially
    more useful dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The EI-oc dataset also contained tweets that were annotated for emotion (anger,
    fear, joy, sadness) and one of four ordinal classes of the intensity of emotion
    that best represented the mental state of the tweeter, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '*0*: No emotion can be inferred'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1*: Low amount of emotion can be inferred'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*2*: Moderate amount of emotion can be inferred'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*3*: High amount of emotion can be inferred'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, we can sensibly guess that by removing tweets with scores lower than
    3, we will get a dataset much better tuned to our needs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'These are relatively straightforward ideas of how datasets can be repurposed
    to extract data that is strongly emotive to create new datasets. However, any
    good dataset needs to be balanced, so let us now return to the problem of creating
    neutral tweets and see how this may be done. The following example assumes that
    the dataset is already downloaded and available; you can download it from here:
    [http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip](http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: You may get an **EI-reg-En-train.zip can’t be downloaded securely** error. In
    that case, simply click the **Keep** option.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is shown here:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A quick eyeball scan of the results shows tweets such as the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Tweet** | **Affect****Dimension** | **Intensity****Score** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-40665 | i love the word fret so much and im in heaven | anger | 0.127
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-11066 | I don’t like pineapple I only eat them on pizza, they lose
    the sting when they get cooked. | anger | 0.192 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-41007 | hate to see y’all frown but I’d rather see him smiling 💕✨
    | anger | 0.188 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: Figure 2.1 – Sample anger tweets with intensity scores < 0.2
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that there are tweets that, although they are low in anger intensity,
    are high in some other emotion and hence not neutral, and so we need some way
    to remove these. Clearly, it is the words themselves that tell us whether the
    tweet is neutral or not (for example, “love” or “hate”). There are plenty of lists
    of emotion words freely available on the internet; downloading one ([https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf](https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf))
    and printing the results shows the following output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It is then a trivial matter, as a first pass, to remove tweets that contain
    any of these words. However, we can see that while tweet 2017-En-40665 says “love,"
    the emotion words list says “Loved." This is problematic because this will prevent
    the tweet from being flagged as non-neutral. To address this, we simply have to
    stem or lemmatize (see [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment
    Lexicons and Vector Space Models* for further details) both the tweet and the
    emotion words list, as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When we look at the results, here’s what we see:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'These are the tweets that were dropped:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can see by and large this simplistic method has done a reasonable job. However,
    there are still cases that have slipped through—for example, where the hashtag
    contained an emotion word:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It is easy to update the code to catch this. We’ll leave that as an exercise
    for you, but the results must be examined carefully to catch edge cases such as
    this.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked solely at English datasets. However, there are many non-English
    datasets available. These may be useful when the dataset you need either does
    not exist or does exist but perhaps does not have enough data in it, or is imbalanced,
    and hence needs to be augmented. This is where we can turn to non-English datasets.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Non-English datasets
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, finding a dataset to train your model is the most challenging part of
    the project. There may be occasions where a dataset is available but it is in
    a different language—this is where translation can be used to make that dataset
    useful for your task. There are a number of different ways to translate a dataset,
    as listed here:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Ask someone you know, who knows the language
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employ a specialist translation company
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an online translation service (e.g. Google Translate) either through the
    GUI or via an API
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, the first two are the preferred options; however, they come with an
    associated cost in terms of effort, time, and money. The third option is also
    a good option, especially if there is a lot of data that needs translating. However,
    this option should be used with care because (as we will see) translation services
    have nuances, and each can produce different results.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of different translation services available (e.g. Google, Bing,
    Yandex, etc.) and lots of Python packages to utilize these (e.g. TextBlob, Googletrans,
    translate-api, etc.). We will use **translate-api** because it is easy to install,
    supports lots of translation services, and lets you start translating with only
    a few lines of code. Consider the following tweet:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: توتر فز قلبي
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the package, like so:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The code itself is deceptively simple:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will produce the following output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s see what happens when we try one of the other translation providers:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the results are not the same! This is why it is generally a
    good idea to get translation service results verified by a native speaker.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen, however, that the preceding code can easily be used to translate
    a complete dataset into English, thus generating new data for our model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have chosen a dataset, we will want to use it to train a classifier
    and see how well that classifier works. Assume that we have a dataset stored in
    the `dataset` variable and a classifier stored in `classifier`. The first thing
    we have to do is to split the dataset into two parts—one, stored in `training`,
    to be used for training the classifier, and one, stored in `testing`, for testing
    it. There are two obvious constraints on the way we do this split, as outlined
    here:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`training` and `testing` must be disjoint. **This is essential**. If they are
    not, then there is a trivial classifier that will get everything 100% correct—namely,
    just remember all the examples you have seen. Even ignoring this trivial case,
    classifiers will generally perform better on datasets that they have been trained
    on than on unseen cases, but when a classifier is deployed in the field, the vast
    majority of cases will be unknown to it, so testing should always be done on unseen
    data.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way that data is collected can often introduce bias. To take a simple example,
    tweets are often collected in chronological order—that is, tweets that were written
    on the same day will often occur together in the dataset. But if everyone was
    very happy about some topic on days 1 to 90 of the collection process and very
    unhappy about it on days 91 to 100, then there would be lots of happy tweets at
    the start of the dataset and lots of unhappy ones at the end. If we chose the
    first 90 days of the dataset for training and the final day for testing, our classifier
    would probably overstate the likelihood that a tweet in the test set is happy.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, it makes sense to randomize the data before splitting it into training
    and testing sections. However, if we do this, we should make sure that we always
    randomize it in the same way; otherwise, we will be unable to compare different
    classifiers on the same data since the randomization will mean that they are being
    trained and tested on different datasets.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: There is a third issue to be considered. If we do not have all that much data,
    we will want to use as much of it as possible for training—in general, the more
    training data we have, the better our classifier will perform, so we do not want
    to waste too much of the data on testing. On the other hand, if we do not have
    much data for testing, then our tests cannot be relied on to give reliable results.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to use **cross-fold validation** (sometimes called **X-fold
    validation**). We construct a series of **folds**, where each fold splits the
    data into N-T points for training and T points for testing (N is the size of the
    whole dataset, while T is the number of points we want to use for testing). If
    we do this N/T times, using a different set for testing in each fold, we will
    eventually use all the data points for testing.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: How big should T be? If we use very small sections for testing, we will have
    as much data as we can for training for each fold, but we will have to do a lot
    of rounds of training and testing. If we use large sections for testing, then
    we will have less data for training for each fold, but we will not have to do
    as many rounds. Suppose we have a dataset with 1000K data points. If we split
    it into two folds, using 500K points for training and 500K for testing, we will
    have quite substantial training sets (the scores for most of the classifiers we
    will be looking at in the remainder of the book flatten out well before we
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: have 500K training points) and we will use all the data for testing (half of
    it in the first fold and the other half in the second), and we will only have
    to do two rounds of training and testing. If, on the other hand, we just have
    1,000 data points, splitting it into two folds, each of which has 500 points for
    training and 500 for testing, will give us very small training sets. It would
    be better to split it into 10 folds, each with 900 points for training and 100
    for testing, or even into 100 folds, each with 990 points for training and 10
    for testing, or even 1,000 folds, each with 999 points for training and 1 for
    testing. Whichever we choose, we will use every point for testing exactly once,
    but if we use small test sets, we will maximize the size of the training set at
    the cost of carrying out more rounds of training and testing.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `makeSplits` function will divide a dataset into *f* folds, each
    with *N*(1-1/f)* points for training and *N/f* for testing, and apply a classifier
    to each:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the remainder of this book, we use 10 folds for datasets with fewer than
    20K data points and 5 folds for datasets with more than 20K data points. If we
    have 20K data points, using 5 folds will give us 16K points for training in each
    fold, which is typically enough to get a reasonable model, so since we will *always*
    eventually use every data point for testing, no matter how many or how few folds
    we use, this seems like a reasonable compromise.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding definition of `makeSplits`, `classifier.train(training)` trained
    the classifier and `clsf.test(test)` returned a score. For both of these tasks,
    we need to know which class each point in the dataset ought to belong to—we need
    a set of **Gold Standard** values. Without a set of Gold Standard values, the
    training phase does not know what it is supposed to be learning and the testing
    phase does not know whether the classifier is returning the right results. We
    will therefore assume that each data point has a Gold Standard label: how can
    we use these labels to assess the performance of a classifier?'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a classifier that is required to assign each data point to one of
    a set C1, C2, …, Cn of classes, and let `tweet.GS` and `tweet.predicted` be the
    Gold Standard value and the label assigned by the classifier. There are three
    possibilities: `tweet.GS` and `tweet.predicted` are the same, they are different,
    and the classifier simply fails to assign a value to `tweet.predicted`. If the
    classifier always assigns a value, then it is easy enough to calculate its accuracy
    since this is just the proportion of all cases that the classifier gets right:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'However, most classifiers allow for the third option—that is, in some cases,
    they can simply not provide an answer. This is a sensible thing to allow: if you
    ask a person whether one of their friends is happy or not, they might say yes,
    they might say no, but they might perfectly reasonably say they don’t know. If
    you don’t have any evidence that tells you whether something belongs to a given
    class, the only sensible thing to do is to say that you don’t know.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: This holds true for ML algorithms just as much as it does for people. It is
    always better for a classifier to say it is uncertain than for it to say the wrong
    thing. This does, however, make the task of comparing two classifiers less straightforward.
    Is a classifier that says “I don’t know” in 95% of cases but gets the remaining
    5% right better or is it worse than one that never admits it is uncertain but
    only gets 85% of cases right?
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: There is no single answer to this question. Suppose that giving a wrong answer
    would be disastrous—for example, if you thought someone might have taken some
    poison but the only known antidote is lethal for people who have not taken it.
    In that case, the classifier that frequently says it doesn’t know but is always
    right when it does say something is better than the one that always makes a decision
    but is quite often wrong. If, on the other hand, giving the wrong answer won’t
    really matter—for example, if you are considering giving someone statins because
    you think they might be prone to heart problems, then the one that always makes
    a decision but is sometimes wrong will be better. If you are likely to have heart
    problems, then taking statins is a good idea, and doing so when you don’t need
    to is unlikely to lead to problems. Therefore, we have to be able to combine scores
    flexibly to allow for different situations.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining four useful parameters, as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): The number of times that the classifier predicts
    that a tweet belongs to class C and the Gold Standard says that it does belong
    to C.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives** (**FP**): The number of times that the classifier predicts
    that a tweet belongs to C and the Gold Standard says that it does not belong to
    C.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives** (**FN**): The number of times that the classifier makes
    no prediction but the Gold Standard says it does belong to some class (so it is
    not right, but it is not really wrong).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives** (**TN**): The number of times that the classifier makes
    no prediction and the Gold Standard also says that the item in question does not
    belong to any of the available classes. In cases where each item belongs to exactly
    one class, this group is always empty, and it is typically not used in the assessment
    of classifiers.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these parameters, we can provide a number of metrics, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: How often the classifier is right when it makes a prediction—![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/1.png).'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: How many of the cases where it should make a prediction it makes
    the right one—![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/2.png)![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/3.png).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-measure**: As noted previously, sometimes precision matters more (diagnosing
    a poison when the antidote is potentially lethal), while sometimes recall matters
    more (prescribing statins to someone who may have heart problems). F-measure,
    defined as ![<mml:math  ><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/4.png)
    for some *a* between 0 and 1, allows us to balance these two: we choose *a* to
    be greater than 0.5 if precision matters more than recall and less than 0.5 if
    recall matters more than precision. Setting *a* equal to 0.5 provides a midway
    point, usually called F1-measure.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macro F1** and **micro F1**: When a task involves several classes, there
    are two ways of calculating F1\. You can take all the cases where the classifier
    makes a decision and use these to calculate P and R, and thence F1, or you can
    calculate F1 for each class and then take the average over all classes. The first
    of these is called micro F1 and the second is macro F1\. If one class contains
    many more cases than the others, then micro F1 can be misleading. Suppose that
    99% of people with symptoms of poisoning actually have indigestion. Then, a classifier
    that classifies all cases of people with symptoms of poisoning actually having
    indigestion will have overall scores of P = 0.99 and R = 0.99, for a micro F1
    score of 0.99\. But that means that no one will ever get treated with the antidote:
    the individual P and R scores would be 0.99 and 1 for indigestion and 0.0, 0.0
    for poisoning, for individual F1 scores of 0.99 and 0, averaging out at 0.495\.
    In general, micro F1 gives more weight to the majority classes and provides an
    overestimate of the scores for the minority cases.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Jaccard measure** provides an alternative way of combining TP, FP, and
    FN, using ![<mml:math  ><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/5.png)![<mml:math  ><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/6.png).
    Given that simple F1 is easily shown to be the same as ![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/7.png)![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/8.png)
    it is clear that the Jaccard measure and simple F1 will always provide the same
    ranking, with the Jaccard score always less than F1 (unless they are both 1).
    There is thus very little to choose between macro F1 and the Jaccard measure,
    but since some authors use one when comparing classifiers and others the other,
    we will give macro F1, micro F1, and Jaccard in all tables where we compare classifiers.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of the datasets we will be looking at allow tweets to have arbitrary
    numbers of labels. Many tweets express no emotions at all, so we need to allow
    for cases where the Gold Standard does not assign anything, and quite a few express
    multiple emotions. We will refer to datasets of this kind as **multi-label datasets**.
    This must be distinguished from datasets where there are several classes (**multi-class
    datasets**) and the task is to assign each tweet to exactly one of the options.
    As we will see, multi-label datasets are significantly harder to work with than
    single-label multi-class datasets. As far as metrics are concerned, true negatives
    (which are not used in any of the metrics given previously) become more significant
    for multi-label tasks, particularly for tasks where the Gold Standard may assign
    no labels at all to a tweet. We will look in detail at multi-label tasks in [*Chapter
    10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*. For now, we will just
    note that training and testing should be carried out using multiple folds to make
    sure that every data point gets used exactly once for testing, with smaller test
    sets (and hence more folds) for small datasets, and that macro F1 and Jaccard
    scores are the most useful metrics for comparing classifiers.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no doubt that finding suitable data can be a challenge, but there are
    ways and means to mitigate that. For example, there are plenty of repositories
    with comprehensive search features that allow you to find relevant datasets.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we started by looking at public data sources and went through
    some of the most popular ones. We saw that many datasets are free, but access
    to some required a subscription to the repository. Even with the existence of
    these repositories, there is still sometimes a need to “roll your own” dataset,
    so we looked at the benefit of doing that and some ways in which we might collect
    our own data and create our own datasets. We then discussed some niche places
    to find datasets specific to the emotion analysis problem—for example, from competition
    websites. Datasets often contain sensitive information about individuals, such
    as their personal beliefs, behaviors, and mental health status, hence we noted
    that it is crucial to consider data privacy and ethics concerns when using datasets.
    We also looked at how we could take datasets that were similar to what we required
    and how to transform them into something more useful. Finally, we looked at how
    we could take a non-English dataset and transform it into our target language,
    and also the problems of doing so.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'We also considered issues that arise when we are trying to evaluate classifiers,
    introducing the notion of cross-fold validation and looking at a number of metrics
    that can be used for assessing classifiers. We noted that splitting the data into
    a large number of folds, each with a small set for testing, is important when
    you have a fairly small dataset. Doing, for instance, 10-fold cross-validation
    is not necessarily more rigorous than using 5 folds: if we have a lot of data,
    then using a few folds, each with a large amount of test data, is a perfectly
    reasonable thing to do. We also considered the merits of the various metrics that
    are most commonly used and decided that since different authors use different
    metrics it makes sense to report all of them since that makes it possible to compare
    the scores for a given classifier with scores published elsewhere.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at labeling, key considerations, and some
    good practices that can improve the effectiveness and accuracy of the process.
    We will also explore techniques to improve outcomes and look at a simple architecture
    and UI for the data labeling task.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Mohammad, S. M., Bravo-Marquez, F., Salameh, M., and Kiritchenko, S. (2018).
    *SemEval-2018 Task 1: Affect in Tweets*. Proceedings of International Workshop
    on Semantic Evaluation (SemEval-2018).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
