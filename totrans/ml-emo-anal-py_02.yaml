- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building and Using a Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data collection and curation process is one of the most important stages
    in model building. It is also one of the most time-consuming. Typically, data
    can come from many sources; for example, customer records, transaction data, or
    stock lists. Nowadays, with the timely conjunction of big data, fast, high-capacity
    SSDs (to store big data), and GPUs (to process big data), it is easier for individuals
    to collect, store, and process data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about finding and accessing pre-existing, ready-made
    data sources that can be used to train your model. We will also look at ways to
    create your own datasets, transforming datasets so that they are useful for your
    problem, and we will also see how non-English datasets can be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this book, we will be using a selection of the datasets
    listed in this chapter to train and test a range of classifiers. When we do this,
    we will want to assess how well the classifiers work on each of the datasets—one
    of the major lessons of this book is that different classifiers work well with
    different datasets, and to see how well a classifier works with a given dataset,
    we will need ways of measuring performance. We will therefore end this chapter
    by looking at metrics for assessing the performance of a classifier on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Ready-made data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-English datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ready-made data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lots of places where ready-made data is available and usually freely
    downloadable. These are typically referred to as **public data sources** and are
    usually made available by companies, institutions, and organizations that are
    happy to either share their data (perhaps for publicity, or to entice others to
    share) or to act as a repository for others to make their data easily searchable
    and accessible. Clearly, these will only be useful to you if your need matches
    the data source, but if it does, it can be a great starting point or even a supplement
    to your own data. The good news is that these data sources usually cover a wide
    range of domains, so it’s likely you’ll find something useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll now discuss some of these public data sources (in no particular order):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kaggle**: Founded in 2010, Kaggle is a part of Google and, according to *Wikipedia*,
    is an “online community of data scientists and machine learning practitioners
    ". Kaggle has many features, but it is best known for its competitions in which
    anyone (for example, individuals and organizations) can publish a competition
    (typically a data science task) for participants to enter and compete to win prizes
    (sometimes in the form of cash!). However, Kaggle also allows users to find and
    publish datasets. Users can upload their datasets to Kaggle and also download
    datasets published by others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everything on Kaggle is completely free, including datasets, and although the
    datasets are open source (open and free for download, modification, and reuse),
    for some datasets, it will be necessary to refer to the license to ascertain the
    purposes for which the dataset can be used. For example, some datasets may not
    be used for academic publications or commercial purposes. Users are also allowed
    to upload code to process the dataset, post comments against the dataset, and
    upvote the dataset so that others know that it is a reliable and useful dataset.
    There are also various other **Activity Overview** metrics, such as when the dataset
    was downloaded, how many times the dataset was downloaded, and how many times
    it was viewed. Since there are so many datasets (approximately 170,000 at the
    time of writing) these metrics can help you decide whether a dataset is worth
    downloading or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: https://www.kaggle.com/datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging Face**: The Hugging Face Hub is a community-driven collection of
    datasets that span a variety of domains and tasks—for example, **natural language
    processing** (**NLP**), **computer vision** (**CV**), and audio. Each dataset
    is a Git repository that has the scripts required to download the data and generate
    splits for training, evaluation, and testing. There are approximately 10,000 datasets
    that can be filtered by the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task categories (text classification, QA, text generation, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks (language modeling, multi-class classification, language inference, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Language (English, French, German, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilinguality (monolingual, multilingual, translation, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Size (10-100K, 1K-10K, 100K-1M, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: License (CC by 4.0, MIT, others, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each dataset page includes a view of the first 100 rows of the dataset and also
    has a handy feature that allows you to copy the code to load a dataset. Some datasets
    also contain a loading script, which also allows you to easily load the dataset.
    Where the dataset does not include this loading script, the data is usually stored
    directly in the repository, in CSV, JSON, or Parquet format.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TensorFlow Datasets**: TensorFlow Datasets offers a set of datasets that
    are suitable for use not just with TensorFlow but also with other Python **machine
    learning** (**ML**) frameworks. Each dataset is presented as a class to allow
    the building of efficient data input pipelines and user-friendly input processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Papers With Code**: This is a site that contains, as the name suggests, research
    papers along with their code implementations. At the time of writing, there are
    around 7,000 ML datasets available to freely download. These can be searched by
    using the following filters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modality (text, images, video, audio, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Task (QA, object detection, image classification, text classification, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Language (English, Chinese, German, French, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: According to its *About* page, all datasets are licensed under the CC BY-SA
    license, allowing anyone to use the datasets as long as the creator(s) are acknowledged.
    Usefully, each dataset lists papers that utilize the dataset, associated benchmarks,
    code, and similar datasets, and explains how the dataset can be loaded from within
    popular frameworks such as TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Papers With Code also encourages users to share their datasets with the community.
    The process is relatively simple and involves registering, uploading the dataset,
    and providing links and information (e.g. description, modality, task, language,
    etc.) about the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Papers With Code’s *About* page states that although the core team is based
    at Meta AI Research no data is shared with any Meta Platforms product.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: '**IEEE DataPort**: IEEE DataPort is an online data repository that was created,
    and is owned by, the **Institute of Electrical and Electronics Engineers** (**IEEE**),
    a professional association for electronic engineering, electrical engineering,
    and associated disciplines. At the time of writing, there are around 6,000 datasets
    available. These can be searched by using either free-text search terms (for example,
    title, author, or **digital object identifier** (**DOI**)) or filters, such as
    the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Category (**artificial intelligence** (**AI**), CV, ML, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Type (Standard, Open Access)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Open Access datasets allow free access to all users, whereas accessing Standard
    datasets requires an IEEE paid subscription. IEEE DataPort also offers three options
    (Standard, Open Access, and Competition) for users to upload their datasets. Standard
    and Competition are free to upload and access; however, Open Access requires the
    purchase of an Open Access credit.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://ieee-dataport.org/datasets](https://ieee-dataport.org/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Dataset Search**: Google Dataset Search is a search engine for datasets
    that features a simple keyword search engine (similar to the Google search page
    we all know and love) that allows users to find datasets that are themselves hosted
    in repositories (e.g. Kaggle) across the web. Results can then be filtered by
    the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last updated (past month, year)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Download format (text, tabular, document, image, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage rights (commercial use allowed/not allowed)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic (architecture and urban planning, computing, engineering, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Free or paid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The website states that the search engine only came out of beta in 2020, hence
    there may be more features added later. Being part of the Google ecosystem, it
    also allows users to easily bookmark datasets to return to later. As one would
    expect with Google, there is data available on a vast range of topics, from mobile
    apps to fast food and everything in between.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://datasetsearch.research.google.com](https://datasetsearch.research.google.com)'
  prefs: []
  type: TYPE_NORMAL
- en: '**BigQuery public datasets**: BigQuery is a **Google Cloud Platform** (**GCP**)
    product that was built to provide serverless, cost-effective, highly scalable
    data warehouse capabilities. Hence, BigQuery is used to host and access public
    datasets, making them publicly available for users to integrate into their applications
    via projects. Although the datasets are free, users must pay for the queries that
    are performed on the data. However, at the time of writing, the first 1 TB per
    month is free. There are many ways to access BigQuery public datasets: by using
    the Google Cloud console, by using the BigQuery REST API, or through Google Analytics
    Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://cloud.google.com/bigquery/public-data](https://cloud.google.com/bigquery/public-data)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Public Data Explorer**: Google Public Data Explorer is a web-based
    tool that makes it easy to explore and visualize datasets as line graphs, bar
    graphs, plots, or on maps. It provides data from about 135 organizations and academic
    institutions such as The World Bank, The **World Trade Organization** (**WTO**),
    Eurostat, and the US Census Bureau. Users are also able to upload, visualize,
    and share their own data by making use of Google’s **Dataset Publishing Language**
    (**DSPL**) data format. Where the system really shines is when the charts are
    animated over time, making it easy even for non-scientists to understand the impact
    and gain insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://www.google.com/publicdata/directory](https://www.google.com/publicdata/directory)'
  prefs: []
  type: TYPE_NORMAL
- en: '**UCI Machine Learning Repository**: The **University of California Irvine**
    (**UCI**) Machine Learning Repository was created as an FTP archive in 1987 by
    graduate students at UCI. It is a free (registration not required) collection
    of approximately 600 datasets that are available for the ML community. The main
    website is rudimentary and outdated with a Google-powered search and no filtering
    capabilities, but (at the time of writing) a new version is in beta testing and
    offers the ability to search using the following filters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characteristics (text, tabular, sequential, time-series, image, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject area (business, computer science, engineering, law, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Associated tasks (classification, regression, clustering, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of attributes (fewer than 10, 10-100, more than 100)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of instances (fewer than 10, 10-100, more than 100)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute types (numerical, categorical, mixed)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets in the repository are donated by different authors and organizations,
    hence each dataset has individual license requirements. The site states that to
    use the datasets, citation information should be used, and usage policies and
    licenses should be checked.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://archive.ics.uci.edu](https://archive.ics.uci.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Registry of Open Data on AWS**: The Registry of Open Data on AWS (short for
    Amazon Web Services) is a centralized repository that makes it easy to find publicly
    available datasets. These datasets are not provided by Amazon, as they are owned
    by government organizations, researchers, businesses, and individuals. The registry
    can be used to discover and share datasets. There are approximately 330 datasets
    available, and these are accessed via the AWS Data Exchange service (an online
    marketplace offering thousands of datasets). Being Amazon, much of this infrastructure
    is tied to the core AWS services; for example, datasets can be used with AWS resources
    and easily integrated into AWS cloud-based applications. As an example, it only
    takes minutes to provision an Amazon **Elastic Compute Cloud** (**EC2**) instance
    and start working with the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://registry.opendata.aws](https://registry.opendata.aws)'
  prefs: []
  type: TYPE_NORMAL
- en: '**US Government open data**: Launched in 2009, *Data.gov* is managed and hosted
    by the US General Services Administration and was created and launched by the
    US Government to provide access to federal, state, and local datasets. There are
    approximately 320,000 datasets that are made available in open, machine-readable
    formats, while continuing to maintain privacy and security, and can be searched
    by keyword or filtered by the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic (local government, climate, energy, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic category (health, flooding water, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset type (geospatial)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Format (CSV, HTML, XML, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization type (federal, state, local, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization (NASA, state, department, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Publisher
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bureau
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets are made available for free and without restriction, although they
    do advise that non-federal data available may have a different licensing method.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://data.gov](https://data.gov)'
  prefs: []
  type: TYPE_NORMAL
- en: '**data.gov.uk**: Similarly, the *data.gov.uk* site allows users to find public
    sector, non-personal data published by the UK central government, UK local authorities,
    and UK public bodies. The datasets are typically hosted on AWS. There are approximately
    52,000 datasets that can be filtered by the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Publisher (council)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic (business and economy, crime and justice, education, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Format (CSV, HTML, XLS, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets are free (registration required), and licensing appears to be a
    mix, with some being **Open Government License** (**OGL**), which permits anyone
    to copy, distribute, or exploit the data, and others requiring **Freedom of Information**
    (**FOI**) requests for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'URL: [https://ukdataservice.ac.uk](https://ukdataservice.ac.uk)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microsoft Azure Open Datasets**: This is a curated repository of datasets
    that can be used to train models. However, there are only about 50 datasets, covering
    areas such as transport, health, and labor, as well as some common datasets. There
    are no charges for using most of the datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://azure.microsoft.com/en-us/products/open-datasets/](https://azure.microsoft.com/en-us/products/open-datasets/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Microsoft Research Open Data**: This is another collection of free datasets
    from Microsoft and contains datasets useful for areas such as NLP and CV. Again,
    there are only about 100 datasets, which can be searched by text or can be filtered
    by the following criteria:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Category (computer science, math, physics, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Format (CSV, DOCX, JPG, etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: License (Creative Commons, legacy Microsoft Research Data License Agreement,
    etc.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [https://msropendata.com](https://msropendata.com)'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding list is intended as an indicative, non-exhaustive guide for those
    who are unsure where to go to get data and provides examples of repositories from
    a number of organizations. There are also “repositories of repositories” where
    lists of dataset repositories are maintained, and these are good places to start
    searching for data. These include sites such as the DataCite Commons Repository
    Finder ([https://repositoryfinder.datacite.org](https://repositoryfinder.datacite.org))
    and the Registry of Research Data Repositories [https://re3data.org/](https://re3data.org/),
    which offers researchers an overview of existing repositories for research data.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted that some of the most common popular datasets are also
    easily available from within Python packages such as TensorFlow, **scikit-learn**
    (**sklearn**), and the **Natural Language** **Toolkit** (**NLTK**).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how we can access ready-made data sources. However,
    sometimes these are inadequate, so let us next see how we can create our own data
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although we have seen several sources where datasets can be obtained, sometimes
    it is necessary to build your own datasets either using your own data or using
    data from other sources. This may be because the available datasets are not adequate
    for our problem, and this approach also brings some additional benefits, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own dataset can eliminate the challenges associated with third-party
    datasets that often have licensing terms or usage restrictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no fees to pay (although building the dataset will incur costs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the dataset is being created using your own data, there are no ownership
    issues. If not, then it is your responsibility to consider ownership issues, and
    appropriate steps should be taken.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have complete ownership and flexibility in how you use the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A fuller understanding of the data is gained as part of building the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your own dataset also comes with increased responsibility; in other
    words, if there are any errors, issues, or biases, there will be only one person
    to blame!
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, many types of data can be collected—for example, financial data, data
    from **Internet of Things** (**IoT**) devices, and data from databases. However,
    since the purpose of this book is the emotional analysis of text, we will demonstrate
    some ways to collect textual data to build datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Data from PDF files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Portable Document Format** (**PDF**) format is one of the most popular
    and widely used digital file formats and is used to present and exchange documents.
    Many organizations use the PDF format to publish documentation, release notes,
    and other document types because files can be read anywhere,
  prefs: []
  type: TYPE_NORMAL
- en: 'on any device, as long as (free) tools such as Adobe Acrobat Reader are installed.
    Consequently, this makes PDF files a good place to look for data. Luckily for
    us, Python has a number of libraries to help us extract text from PDF files, as
    listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: PyPDF4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDFMiner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PDFplumber
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many others, but these seem to be the most popular. For this example,
    due to our previous experiences, we will use PyPDF4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to ensure that the PyPDF4 module is installed. Here’s the
    command we run to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to import the package and set up a variable that contains the
    name of the file we wish to process. For this example, a sample PDF was downloaded
    from [https://www.jbc.org/article/S0021-9258(19)52451-6/pdf](https://www.jbc.org/article/S0021-9258(19)52451-6/pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to set up some objects that will actually allow us to read the
    PDF file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'PyPDF4 can also extract metadata (data about the file) from the PDF. Here’s
    how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the title, author, and subject of the document (there are
    also other fields available):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also get a count of the number of pages in the document by executing
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the number of pages in the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now iterate through each page, extract the text, and write it to a database
    or a file, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We have extracted the text from a PDF file and can use it to build
    a dataset and ultimately use it to train a model (after cleaning and preprocessing).
    Of course, in reality, we would wrap this into a function and iterate a folder
    of files to create a proper dataset, so let’s do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the appropriate libraries and set up a folder where
    the PDF files are, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can refactor the code we had originally and reengineer it in the form
    of some handy reusable functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note how, in `save_content,` there is a placeholder where you would normally
    write the extracted content to a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, here’s the main code where we iterate the folder and, for each
    PDF file, extract the content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we have seen, extracting text from PDF files is pretty straightforward. Let’s
    now see how we can get data from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Data from web scraping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nowadays, there is so much publicly available data on the web in the form of
    (for example) news, blogs, and social media that it makes sense to gather (“harvest”)
    and make use of this. The process of extracting data from a website is known as
    **web scraping**, and although this can be done manually, that would not be an
    efficient use of time and resources, especially when there are plenty of tools
    to help automate the process. The steps to do this are something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify a root URL (a starting point).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the page content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process/clean/format the downloaded text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the cleaned text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although there are no hard and fast rules, there are some rules of etiquette
    that will stop your program from getting blocked and some rules that will make
    scraping easier that should be followed:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a delay between each scrape request so the site does not get overloaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scrape during non-peak hours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do note here that it is important that data is only scraped from sources that
    allow it, as unauthorized scraping can infringe terms of service and intellectual
    property rights and may even have legal ramifications. It is also a sensible idea
    to examine the metadata as it may provide guidance on whether the data is sensitive
    or private, data provenance, permissions, and restrictions on use. Being respectful
    of source permissions and data sensitivity are important considerations in responsible
    and ethical web scraping.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to ensure that the Beautiful Soup module is installed. We
    can do that using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent any unforeseen errors, please ensure that the following versions
    are installed:'
  prefs: []
  type: TYPE_NORMAL
- en: Beautiful Soup 4.11.2
  prefs: []
  type: TYPE_NORMAL
- en: lxml 4.9.3
  prefs: []
  type: TYPE_NORMAL
- en: 'We then import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a URL to start scraping from. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to separate interesting, relevant content from the non-useful elements
    of a web page, such as the menu, header, and footer. Every website has its own
    set of design styles and conventions and will display its content in its own unique
    manner. For the website we chose, we found that looking for three consecutive
    `<p>` tags homed in on the content part of the page. It’s very likely that this
    logic will be different for the website you are scraping from. To find these `<p>`
    tags, we define a **regular expression** (**regex**), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to request the HTML for the website and extract the paragraphs
    using the regex. This text can then be cleaned (for example, any inline HTML removed)
    and saved to a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we can go one step further. By extracting the hyperlinks from this
    page, we can get our program to keep scraping deeper into the website. This is
    where the previous commentary on best practices should be applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need some code to start the scrape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To prevent the program from ending up in a loop, a list of visited URLs should
    be maintained and checked before scraping each URL—we have left this as an exercise
    for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you get a `<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify
    failed: unable to get local issuer certificate (_ssl.c:997)>` error, you can use
    this link to resolve it: [https://stackoverflow.com/a/70495761/5457712](https://stackoverflow.com/a/70495761/5457712).'
  prefs: []
  type: TYPE_NORMAL
- en: Data from RSS feeds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**RSS** (short for **Really Simple Syndication**) is a relatively old technology.
    Once upon a time, it was used to collate all the latest news into a web browser.
    Nowadays, it is not as popular as it once was but is still used by many to stay
    up to date. Most news providers provide RSS feeds on their websites.'
  prefs: []
  type: TYPE_NORMAL
- en: An RSS feed is typically an **Extensible Markup Language** (**XML**) document
    that includes a URL to a web page (that can be scraped, as we have seen), full
    or summarized text, and metadata such as the publication date and the author’s
    name.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can create a dataset of news headlines.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, firstly we need to ensure that the module we need is installed. `feedparser`
    is a Python library that works with feeds in all known formats. You can install
    it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we import it, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need a feed URL to work with. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it is a simple matter of downloading the feed and extracting the relevant
    parts. For news headlines, we envisage that the summary contains more information,
    so it should be saved to a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need some code to start the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the URL, title, and summary from each element in the feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Let us next take a look at how a more robust technology, APIs, can be used to
    download data.
  prefs: []
  type: TYPE_NORMAL
- en: Data from APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: X (formerly Twitter) is a fantastic place to obtain text data; it offers an
    easy-to-use API. It is free to start off with, and there are many Python libraries
    available that can be used to call the API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the free X (Twitter) API is in a state of flux, and
    it may no longer be possible to use the `tweepy` API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that, later on in this book, we work with tweets, it is sensible at this
    point to learn how to extract tweets from Twitter. For this, we need a package
    called `tweepy`. Use the following command to install `tweepy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to sign up for an account and generate some keys, so proceed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://developer.twitter.com/en](https://developer.twitter.com/en) and
    sign up for an account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [https://developer.twitter.com/en/portal/projects-and-apps](https://developer.twitter.com/en/portal/projects-and-apps).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create App** in the **Standalone** **Apps** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give your app a name, and make a note of the **API Key**, **API Key Secret**,
    and **Bearer** **Token** values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **App Settings** and then click the **Keys and** **Tokens** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this page, click **Generate** in the **Access Token and Secret** section
    and again make a note of these values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are now ready to use these keys to get some tweets from Twitter! Let’s run
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You must replace the `YOUR_KEY_HERE` token with your own keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then create a class with a subclassed special method called `on_tweet` that
    is triggered when a tweet is received from this stream. The code is actually pretty
    simple and looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Tweepy insists that “rules” are added to filter the stream, so let’s add a
    rule that states we are only interested in tweets that contain the `#``lfc` hashtag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'See here for more about Tweepy rules: [https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule](https://developer.twitter.com/en/docs/twitter-api/tweets/filtered-stream/integrate/build-a-rule).'
  prefs: []
  type: TYPE_NORMAL
- en: Heavy use of the X (Twitter) API may need a paid package.
  prefs: []
  type: TYPE_NORMAL
- en: Other data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have listed some of the commonly used sources of data in the previous section.
    However, there are probably many thousands of other free datasets available. You
    just need to know where to look. The following is a list of some of the interesting
    ones that we came across as part of our work on emotion analysis. There are probably
    many more available all over the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dr. Saif Mohammad is a Senior Research Scientist at the **National Research
    Council** (**NRC**) Canada. He has published many papers and has been heavily
    involved with *SemEval*, as one of the organizers, for many years. He has also
    published many different, free-for-research purposes datasets that have been used
    primarily for competition purposes. Many of these are listed on his website at
    http://saifmohammad.com/WebPages/SentimentEmotionLabeledData.xhtml, although some
    are better described on the associated competition page, as presented here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Emotion Intensity** (**EmoInt**) dataset has four datasets for four emotions
    (http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.xhtml).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Workshop on Computational Approaches to Subjectivity, Sentiment and Social
    Media Analysis** (**WASSA**) dataset is a total of 3,960 English tweets, each
    labeled with an emotion of anger, fear, joy, and sadness. Each tweet also has
    a real-valued score between 0 and 1, indicating the degree or intensity of the
    emotion felt by the speaker ([https://wt-public.emm4u.eu/wassa2017/](https://wt-public.emm4u.eu/wassa2017/)).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SemEval* (Mohammad et al., 2018) is an annual competition in which teams of
    researchers from all over the world work on tasks where they develop systems to
    categorize datasets. The exact task varies from year to year. It has been running
    intermittently since 1998, but since 2012, it has become an annual event. A number
    of datasets have come about from this competition, as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task E-c**: A dataset containing tweets classified as “neutral or no
    emotion” or as 1, or more, of 11 given emotions that best represent the mental
    state of the tweeter.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task EI-reg**: A dataset containing tweets labeled for emotion (anger,
    fear, joy, sadness), and for intensity, a real-valued score between 0 and 1, with
    a score of 1 indicating that the highest amount of emotion was inferred and a
    score of 0 indicating the lowest amount of emotion was inferred. The authors note
    that these scores have no inherent meaning; they are only used as a mechanism
    to convey that the instances with higher scores correspond to a greater degree
    of emotion than instances with lower scores.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2018 Task EI-oc**: A dataset containing tweets labeled for emotion (anger,
    fear, joy, sadness) and one of four ordinal classes of the intensity of emotion
    that best represented the mental state of the tweeter. These datasets are all
    available from https://competitions.codalab.org/competitions/17751.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Across the competition years, there also seem to be plenty of datasets labeled
    for sentiment, as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2014/task9/](https://alt.qcri.org/semeval2014/task9/)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools](https://alt.qcri.org/semeval2015/task10/index.php?id=data-and-tools)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://alt.qcri.org/semeval2017/task4/](https://alt.qcri.org/semeval2017/task4/)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hashtag Emotion Corpus dataset contains tweets with emotion word hashtags
    ([http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip](http://saifmohammad.com/WebDocs/Jan9-2012-tweets-clean.txt.zip)).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The **International Survey On Emotion Antecedents And Reactions** (**ISEAR**)
    dataset contains reports of situations in which student respondents had experienced
    emotions (joy, fear, anger, sadness, disgust, shame, and guilt) ([https://www.unige.ch/cisa/research/materials-and-online-research/research-material/](https://www.unige.ch/cisa/research/materials-and-online-research/research-material/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A popular dataset labeled for sentiment (negative, neutral, positive) ([https://data.mendeley.com/datasets/z9zw7nt5h2](https://data.mendeley.com/datasets/z9zw7nt5h2)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dataset labeled for six emotions (anger, fear, joy, love, sadness, surprise)
    ([https://github.com/dair-ai/emotion_dataset](https://github.com/dair-ai/emotion_dataset)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contextualized Affect Representations for Emotion Recognition** (**CARER**)
    is an emotion dataset collected through noisy labels, annotated for six emotions
    (anger, fear, joy, love, sadness, and surprise) ([https://paperswithcode.com/dataset/emotion](https://paperswithcode.com/dataset/emotion)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is vitally important to consider data privacy and ethical concerns when using
    these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how to access ready-made data sources and how to create your own
    data source. However, there may be occasions where these are good but not quite
    what we are looking for. Let’s see how we can tackle that problem.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have seen that there are many sentiment and emotion datasets available,
    it is rare that a dataset meets all the exact requirements. However, there are
    ways to tackle this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen some datasets labeled for sentiment, some for emotion, and also
    others labeled for more exotic things such as valence that do not seem to fit
    into what we are looking for: the emotion analysis problem. However, in certain
    circumstances, it is still possible to repurpose and use these. For example, if
    a dataset contains emotions, we can transform this into a sentiment dataset simply
    by assuming that the “anger” emotion is a negative sentiment and the “joy” emotion
    is a positive sentiment. A degree of subjectivity and manual analysis of the individual
    dataset is then required to determine which emotions would constitute a good substitute
    for the neutral sentiment. In our experience, this is typically not straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation** is the name given to the process of applying changes
    to a dataset to make that dataset more useful for your purpose. This could include
    adding data, removing data, or any process that makes that data more useful. Let
    us consider some examples from the *Other data sources* section and see how we
    can repurpose them.'
  prefs: []
  type: TYPE_NORMAL
- en: The EI-reg dataset, as described previously, contains tweets that were annotated
    for emotion (anger, fear, joy, sadness) and for intensity with a score between
    0 and 1\. We can sensibly guess that scores around and under 0.5 are not going
    to be indicative of highly emotive tweets and hence tweets with scores under 0.5
    can be removed and the remaining tweets used to create a reduced, but potentially
    more useful dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The EI-oc dataset also contained tweets that were annotated for emotion (anger,
    fear, joy, sadness) and one of four ordinal classes of the intensity of emotion
    that best represented the mental state of the tweeter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*0*: No emotion can be inferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1*: Low amount of emotion can be inferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*2*: Moderate amount of emotion can be inferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*3*: High amount of emotion can be inferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, we can sensibly guess that by removing tweets with scores lower than
    3, we will get a dataset much better tuned to our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are relatively straightforward ideas of how datasets can be repurposed
    to extract data that is strongly emotive to create new datasets. However, any
    good dataset needs to be balanced, so let us now return to the problem of creating
    neutral tweets and see how this may be done. The following example assumes that
    the dataset is already downloaded and available; you can download it from here:
    [http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip](http://www.saifmohammad.com/WebDocs/AIT-2018/AIT2018-DATA/EI-reg/English/EI-reg-En-train.zip).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may get an **EI-reg-En-train.zip can’t be downloaded securely** error. In
    that case, simply click the **Keep** option.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick eyeball scan of the results shows tweets such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ID** | **Tweet** | **Affect****Dimension** | **Intensity****Score** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-40665 | i love the word fret so much and im in heaven | anger | 0.127
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-11066 | I don’t like pineapple I only eat them on pizza, they lose
    the sting when they get cooked. | anger | 0.192 |'
  prefs: []
  type: TYPE_TB
- en: '| 2017-En-41007 | hate to see y’all frown but I’d rather see him smiling 💕✨
    | anger | 0.188 |'
  prefs: []
  type: TYPE_TB
- en: Figure 2.1 – Sample anger tweets with intensity scores < 0.2
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that there are tweets that, although they are low in anger intensity,
    are high in some other emotion and hence not neutral, and so we need some way
    to remove these. Clearly, it is the words themselves that tell us whether the
    tweet is neutral or not (for example, “love” or “hate”). There are plenty of lists
    of emotion words freely available on the internet; downloading one ([https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf](https://www.ndapandas.org/wp-content/uploads/archive/Documents/News/FeelingsWordList.pdf))
    and printing the results shows the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'It is then a trivial matter, as a first pass, to remove tweets that contain
    any of these words. However, we can see that while tweet 2017-En-40665 says “love,"
    the emotion words list says “Loved." This is problematic because this will prevent
    the tweet from being flagged as non-neutral. To address this, we simply have to
    stem or lemmatize (see [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment
    Lexicons and Vector Space Models* for further details) both the tweet and the
    emotion words list, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'When we look at the results, here’s what we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the tweets that were dropped:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see by and large this simplistic method has done a reasonable job. However,
    there are still cases that have slipped through—for example, where the hashtag
    contained an emotion word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: It is easy to update the code to catch this. We’ll leave that as an exercise
    for you, but the results must be examined carefully to catch edge cases such as
    this.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have looked solely at English datasets. However, there are many non-English
    datasets available. These may be useful when the dataset you need either does
    not exist or does exist but perhaps does not have enough data in it, or is imbalanced,
    and hence needs to be augmented. This is where we can turn to non-English datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Non-English datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, finding a dataset to train your model is the most challenging part of
    the project. There may be occasions where a dataset is available but it is in
    a different language—this is where translation can be used to make that dataset
    useful for your task. There are a number of different ways to translate a dataset,
    as listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask someone you know, who knows the language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employ a specialist translation company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an online translation service (e.g. Google Translate) either through the
    GUI or via an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, the first two are the preferred options; however, they come with an
    associated cost in terms of effort, time, and money. The third option is also
    a good option, especially if there is a lot of data that needs translating. However,
    this option should be used with care because (as we will see) translation services
    have nuances, and each can produce different results.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of different translation services available (e.g. Google, Bing,
    Yandex, etc.) and lots of Python packages to utilize these (e.g. TextBlob, Googletrans,
    translate-api, etc.). We will use **translate-api** because it is easy to install,
    supports lots of translation services, and lets you start translating with only
    a few lines of code. Consider the following tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: توتر فز قلبي
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to install the package, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The code itself is deceptively simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what happens when we try one of the other translation providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the results are not the same! This is why it is generally a
    good idea to get translation service results verified by a native speaker.
  prefs: []
  type: TYPE_NORMAL
- en: It can be seen, however, that the preceding code can easily be used to translate
    a complete dataset into English, thus generating new data for our model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have chosen a dataset, we will want to use it to train a classifier
    and see how well that classifier works. Assume that we have a dataset stored in
    the `dataset` variable and a classifier stored in `classifier`. The first thing
    we have to do is to split the dataset into two parts—one, stored in `training`,
    to be used for training the classifier, and one, stored in `testing`, for testing
    it. There are two obvious constraints on the way we do this split, as outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`training` and `testing` must be disjoint. **This is essential**. If they are
    not, then there is a trivial classifier that will get everything 100% correct—namely,
    just remember all the examples you have seen. Even ignoring this trivial case,
    classifiers will generally perform better on datasets that they have been trained
    on than on unseen cases, but when a classifier is deployed in the field, the vast
    majority of cases will be unknown to it, so testing should always be done on unseen
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way that data is collected can often introduce bias. To take a simple example,
    tweets are often collected in chronological order—that is, tweets that were written
    on the same day will often occur together in the dataset. But if everyone was
    very happy about some topic on days 1 to 90 of the collection process and very
    unhappy about it on days 91 to 100, then there would be lots of happy tweets at
    the start of the dataset and lots of unhappy ones at the end. If we chose the
    first 90 days of the dataset for training and the final day for testing, our classifier
    would probably overstate the likelihood that a tweet in the test set is happy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, it makes sense to randomize the data before splitting it into training
    and testing sections. However, if we do this, we should make sure that we always
    randomize it in the same way; otherwise, we will be unable to compare different
    classifiers on the same data since the randomization will mean that they are being
    trained and tested on different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: There is a third issue to be considered. If we do not have all that much data,
    we will want to use as much of it as possible for training—in general, the more
    training data we have, the better our classifier will perform, so we do not want
    to waste too much of the data on testing. On the other hand, if we do not have
    much data for testing, then our tests cannot be relied on to give reliable results.
  prefs: []
  type: TYPE_NORMAL
- en: The solution is to use **cross-fold validation** (sometimes called **X-fold
    validation**). We construct a series of **folds**, where each fold splits the
    data into N-T points for training and T points for testing (N is the size of the
    whole dataset, while T is the number of points we want to use for testing). If
    we do this N/T times, using a different set for testing in each fold, we will
    eventually use all the data points for testing.
  prefs: []
  type: TYPE_NORMAL
- en: How big should T be? If we use very small sections for testing, we will have
    as much data as we can for training for each fold, but we will have to do a lot
    of rounds of training and testing. If we use large sections for testing, then
    we will have less data for training for each fold, but we will not have to do
    as many rounds. Suppose we have a dataset with 1000K data points. If we split
    it into two folds, using 500K points for training and 500K for testing, we will
    have quite substantial training sets (the scores for most of the classifiers we
    will be looking at in the remainder of the book flatten out well before we
  prefs: []
  type: TYPE_NORMAL
- en: have 500K training points) and we will use all the data for testing (half of
    it in the first fold and the other half in the second), and we will only have
    to do two rounds of training and testing. If, on the other hand, we just have
    1,000 data points, splitting it into two folds, each of which has 500 points for
    training and 500 for testing, will give us very small training sets. It would
    be better to split it into 10 folds, each with 900 points for training and 100
    for testing, or even into 100 folds, each with 990 points for training and 10
    for testing, or even 1,000 folds, each with 999 points for training and 1 for
    testing. Whichever we choose, we will use every point for testing exactly once,
    but if we use small test sets, we will maximize the size of the training set at
    the cost of carrying out more rounds of training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `makeSplits` function will divide a dataset into *f* folds, each
    with *N*(1-1/f)* points for training and *N/f* for testing, and apply a classifier
    to each:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In the remainder of this book, we use 10 folds for datasets with fewer than
    20K data points and 5 folds for datasets with more than 20K data points. If we
    have 20K data points, using 5 folds will give us 16K points for training in each
    fold, which is typically enough to get a reasonable model, so since we will *always*
    eventually use every data point for testing, no matter how many or how few folds
    we use, this seems like a reasonable compromise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding definition of `makeSplits`, `classifier.train(training)` trained
    the classifier and `clsf.test(test)` returned a score. For both of these tasks,
    we need to know which class each point in the dataset ought to belong to—we need
    a set of **Gold Standard** values. Without a set of Gold Standard values, the
    training phase does not know what it is supposed to be learning and the testing
    phase does not know whether the classifier is returning the right results. We
    will therefore assume that each data point has a Gold Standard label: how can
    we use these labels to assess the performance of a classifier?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a classifier that is required to assign each data point to one of
    a set C1, C2, …, Cn of classes, and let `tweet.GS` and `tweet.predicted` be the
    Gold Standard value and the label assigned by the classifier. There are three
    possibilities: `tweet.GS` and `tweet.predicted` are the same, they are different,
    and the classifier simply fails to assign a value to `tweet.predicted`. If the
    classifier always assigns a value, then it is easy enough to calculate its accuracy
    since this is just the proportion of all cases that the classifier gets right:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'However, most classifiers allow for the third option—that is, in some cases,
    they can simply not provide an answer. This is a sensible thing to allow: if you
    ask a person whether one of their friends is happy or not, they might say yes,
    they might say no, but they might perfectly reasonably say they don’t know. If
    you don’t have any evidence that tells you whether something belongs to a given
    class, the only sensible thing to do is to say that you don’t know.'
  prefs: []
  type: TYPE_NORMAL
- en: This holds true for ML algorithms just as much as it does for people. It is
    always better for a classifier to say it is uncertain than for it to say the wrong
    thing. This does, however, make the task of comparing two classifiers less straightforward.
    Is a classifier that says “I don’t know” in 95% of cases but gets the remaining
    5% right better or is it worse than one that never admits it is uncertain but
    only gets 85% of cases right?
  prefs: []
  type: TYPE_NORMAL
- en: There is no single answer to this question. Suppose that giving a wrong answer
    would be disastrous—for example, if you thought someone might have taken some
    poison but the only known antidote is lethal for people who have not taken it.
    In that case, the classifier that frequently says it doesn’t know but is always
    right when it does say something is better than the one that always makes a decision
    but is quite often wrong. If, on the other hand, giving the wrong answer won’t
    really matter—for example, if you are considering giving someone statins because
    you think they might be prone to heart problems, then the one that always makes
    a decision but is sometimes wrong will be better. If you are likely to have heart
    problems, then taking statins is a good idea, and doing so when you don’t need
    to is unlikely to lead to problems. Therefore, we have to be able to combine scores
    flexibly to allow for different situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining four useful parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positives** (**TP**): The number of times that the classifier predicts
    that a tweet belongs to class C and the Gold Standard says that it does belong
    to C.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positives** (**FP**): The number of times that the classifier predicts
    that a tweet belongs to C and the Gold Standard says that it does not belong to
    C.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negatives** (**FN**): The number of times that the classifier makes
    no prediction but the Gold Standard says it does belong to some class (so it is
    not right, but it is not really wrong).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True negatives** (**TN**): The number of times that the classifier makes
    no prediction and the Gold Standard also says that the item in question does not
    belong to any of the available classes. In cases where each item belongs to exactly
    one class, this group is always empty, and it is typically not used in the assessment
    of classifiers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given these parameters, we can provide a number of metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: How often the classifier is right when it makes a prediction—![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/1.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: How many of the cases where it should make a prediction it makes
    the right one—![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/2.png)![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/3.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-measure**: As noted previously, sometimes precision matters more (diagnosing
    a poison when the antidote is potentially lethal), while sometimes recall matters
    more (prescribing statins to someone who may have heart problems). F-measure,
    defined as ![<mml:math  ><mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>a</mml:mi><mml:mo>×</mml:mo><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/4.png)
    for some *a* between 0 and 1, allows us to balance these two: we choose *a* to
    be greater than 0.5 if precision matters more than recall and less than 0.5 if
    recall matters more than precision. Setting *a* equal to 0.5 provides a midway
    point, usually called F1-measure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macro F1** and **micro F1**: When a task involves several classes, there
    are two ways of calculating F1\. You can take all the cases where the classifier
    makes a decision and use these to calculate P and R, and thence F1, or you can
    calculate F1 for each class and then take the average over all classes. The first
    of these is called micro F1 and the second is macro F1\. If one class contains
    many more cases than the others, then micro F1 can be misleading. Suppose that
    99% of people with symptoms of poisoning actually have indigestion. Then, a classifier
    that classifies all cases of people with symptoms of poisoning actually having
    indigestion will have overall scores of P = 0.99 and R = 0.99, for a micro F1
    score of 0.99\. But that means that no one will ever get treated with the antidote:
    the individual P and R scores would be 0.99 and 1 for indigestion and 0.0, 0.0
    for poisoning, for individual F1 scores of 0.99 and 0, averaging out at 0.495\.
    In general, micro F1 gives more weight to the majority classes and provides an
    overestimate of the scores for the minority cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Jaccard measure** provides an alternative way of combining TP, FP, and
    FN, using ![<mml:math  ><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/5.png)![<mml:math  ><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/6.png).
    Given that simple F1 is easily shown to be the same as ![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/7.png)![<mml:math  ><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>×</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math>](img/8.png)
    it is clear that the Jaccard measure and simple F1 will always provide the same
    ranking, with the Jaccard score always less than F1 (unless they are both 1).
    There is thus very little to choose between macro F1 and the Jaccard measure,
    but since some authors use one when comparing classifiers and others the other,
    we will give macro F1, micro F1, and Jaccard in all tables where we compare classifiers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A number of the datasets we will be looking at allow tweets to have arbitrary
    numbers of labels. Many tweets express no emotions at all, so we need to allow
    for cases where the Gold Standard does not assign anything, and quite a few express
    multiple emotions. We will refer to datasets of this kind as **multi-label datasets**.
    This must be distinguished from datasets where there are several classes (**multi-class
    datasets**) and the task is to assign each tweet to exactly one of the options.
    As we will see, multi-label datasets are significantly harder to work with than
    single-label multi-class datasets. As far as metrics are concerned, true negatives
    (which are not used in any of the metrics given previously) become more significant
    for multi-label tasks, particularly for tasks where the Gold Standard may assign
    no labels at all to a tweet. We will look in detail at multi-label tasks in [*Chapter
    10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*. For now, we will just
    note that training and testing should be carried out using multiple folds to make
    sure that every data point gets used exactly once for testing, with smaller test
    sets (and hence more folds) for small datasets, and that macro F1 and Jaccard
    scores are the most useful metrics for comparing classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no doubt that finding suitable data can be a challenge, but there are
    ways and means to mitigate that. For example, there are plenty of repositories
    with comprehensive search features that allow you to find relevant datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we started by looking at public data sources and went through
    some of the most popular ones. We saw that many datasets are free, but access
    to some required a subscription to the repository. Even with the existence of
    these repositories, there is still sometimes a need to “roll your own” dataset,
    so we looked at the benefit of doing that and some ways in which we might collect
    our own data and create our own datasets. We then discussed some niche places
    to find datasets specific to the emotion analysis problem—for example, from competition
    websites. Datasets often contain sensitive information about individuals, such
    as their personal beliefs, behaviors, and mental health status, hence we noted
    that it is crucial to consider data privacy and ethics concerns when using datasets.
    We also looked at how we could take datasets that were similar to what we required
    and how to transform them into something more useful. Finally, we looked at how
    we could take a non-English dataset and transform it into our target language,
    and also the problems of doing so.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also considered issues that arise when we are trying to evaluate classifiers,
    introducing the notion of cross-fold validation and looking at a number of metrics
    that can be used for assessing classifiers. We noted that splitting the data into
    a large number of folds, each with a small set for testing, is important when
    you have a fairly small dataset. Doing, for instance, 10-fold cross-validation
    is not necessarily more rigorous than using 5 folds: if we have a lot of data,
    then using a few folds, each with a large amount of test data, is a perfectly
    reasonable thing to do. We also considered the merits of the various metrics that
    are most commonly used and decided that since different authors use different
    metrics it makes sense to report all of them since that makes it possible to compare
    the scores for a given classifier with scores published elsewhere.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at labeling, key considerations, and some
    good practices that can improve the effectiveness and accuracy of the process.
    We will also explore techniques to improve outcomes and look at a simple architecture
    and UI for the data labeling task.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mohammad, S. M., Bravo-Marquez, F., Salameh, M., and Kiritchenko, S. (2018).
    *SemEval-2018 Task 1: Affect in Tweets*. Proceedings of International Workshop
    on Semantic Evaluation (SemEval-2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
