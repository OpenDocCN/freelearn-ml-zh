["```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.feature_selection import SelectKBest,\\\n      mutual_info_classif, f_classif\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['gender','satverbal','satmath',\n      'gpascience', 'gpaenglish','gpamath','gpaoverall',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    X_train_enc = ohe.fit_transform(X_train)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    X_train_enc = \\\n      pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    ```", "```py\n    ksel = SelectKBest(score_func=mutual_info_classif, k=5)\n    ksel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[ksel.get_support()]\n    selcols\n    Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')\n    ```", "```py\n    pd.DataFrame({'score': ksel.scores_,\n      'feature': X_train_enc.columns},\n       columns=['feature','score']).\\\n       sort_values(['score'], ascending=False)\n            feature              score\n    5       gpaoverall           0.108\n    1       satmath              0.074\n    3       gpaenglish           0.072\n    0       satverbal            0.069\n    2       gpascience           0.047\n    4       gpamath              0.038\n    8       parentincome         0.024\n    7       fatherhighgrade      0.022\n    6       motherhighgrade      0.022\n    9       gender_Female        0.015\n    ```", "```py\nfrom functools import partial\nSelectKBest(score_func=partial(mutual_info_classif, \n                               random_state=0), k=5) \n```", "```py\n    X_train_analysis = X_train_enc[selcols] \n    X_train_analysis.dtypes\n    satverbal       float64\n    satmath         float64\n    gpascience      float64\n    gpaenglish      float64\n    gpaoverall      float64\n    dtype: object\n    ```", "```py\nksel = SelectKBest(score_func=f_classif, k=5)\n```", "```py\nksel.fit(X_train_enc, y_train.values.ravel())\n```", "```py\nselcols = X_train_enc.columns[ksel.get_support()]\n```", "```py\nselcols\n```", "```py\nIndex(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')\n```", "```py\npd.DataFrame({'score': ksel.scores_,\n```", "```py\n  'feature': X_train_enc.columns},\n```", "```py\n   columns=['feature','score']).\\\n```", "```py\n   sort_values(['score'], ascending=False)\n```", "```py\n       feature                score\n```", "```py\n5      gpaoverall           119.471\n```", "```py\n3      gpaenglish           108.006\n```", "```py\n2      gpascience            96.824\n```", "```py\n1      satmath               84.901\n```", "```py\n0      satverbal             77.363\n```", "```py\n4      gpamath               60.930\n```", "```py\n7      fatherhighgrade       37.481\n```", "```py\n6      motherhighgrade       29.377\n```", "```py\n8      parentincome          22.266\n```", "```py\n9      gender_Female         15.098\n```", "```py\n    import pandas as pd\n    import numpy as np\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.feature_selection import SelectKBest, f_regression\n    ```", "```py\n    nls97wages = pd.read_csv(\"data/nls97wages.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome',\n      'completedba']\n    ```", "```py\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97wages[feature_cols],\\\n      nls97wages[['wageincome']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    X_train_enc = ohe.fit_transform(X_train)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    X_train_enc = \\\n      pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Male']])\n    y_train = \\\n      pd.DataFrame(scaler.fit_transform(y_train),\n      columns=['wageincome'], index=y_train.index)\n    ```", "```py\n    ksel = SelectKBest(score_func=f_regression, k=5)\n    ksel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[ksel.get_support()]\n    selcols\n    Index(['satmath', 'gpascience', 'parentincome',\n     'completedba','gender_Male'],\n          dtype='object')\n    ```", "```py\n    pd.DataFrame({'score': ksel.scores_,\n      'feature': X_train_enc.columns},\n       columns=['feature','score']).\\\n       sort_values(['score'], ascending=False)\n\n                  feature              score\n    1             satmath              45\n    9             completedba          38\n    10            gender_Male          26\n    8             parentincome         24\n    2             gpascience           21\n    0             satverbal            19\n    5             gpaoverall           17\n    4             gpamath              13\n    3             gpaenglish           10\n    6             motherhighgrade       9\n    7             fatherhighgrade       8\n    ```", "```py\n    from functools import partial\n    ksel = SelectKBest(score_func=\\\n      partial(mutual_info_regression, random_state=0),\n      k=5)\n    ksel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[ksel.get_support()]\n    selcols\n    Index(['satmath', 'gpascience', 'fatherhighgrade', 'completedba','gender_Male'],dtype='object')\n    pd.DataFrame({'score': ksel.scores_,\n      'feature': X_train_enc.columns},\n       columns=['feature','score']).\\\n       sort_values(['score'], ascending=False)\n               feature               score\n    1          satmath               0.101\n    10         gender_Male           0.074\n    7          fatherhighgrade       0.047\n    2          gpascience            0.044\n    9          completedba           0.044\n    4          gpamath               0.016\n    8          parentincome          0.015\n    6          motherhighgrade       0.012\n    0          satverbal             0.000\n    3          gpaenglish            0.000\n    5          gpaoverall            0.000\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    from mlxtend.feature_selection import SequentialFeatureSelector\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    X_train_enc = ohe.fit_transform(X_train)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    X_train_enc = \\\n      pd.DataFrame(scaler.fit_transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n    sfs = SequentialFeatureSelector(rfc, k_features=5,\n      forward=True, floating=False, verbose=2,\n      scoring='accuracy', cv=5)\n    sfs.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[list(sfs.k_feature_idx_)]\n    selcols\n    Index(['satverbal', 'satmath', 'gpaoverall',\n    'parentincome', 'gender_Female'], dtype='object')\n    ```", "```py\nIndex(['satverbal', 'satmath', 'gpascience',\n 'gpaenglish', 'gpaoverall'], dtype='object')\n```", "```py\nrfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=0)\n```", "```py\nsfs = SequentialFeatureSelector(rfc, k_features=5,\n```", "```py\n  forward=False, floating=False, verbose=2,\n```", "```py\n  scoring='accuracy', cv=5)\n```", "```py\nsfs.fit(X_train_enc, y_train.values.ravel())\n```", "```py\nselcols = X_train_enc.columns[list(sfs.k_feature_idx_)]\n```", "```py\nselcols\n```", "```py\nIndex(['satverbal', 'gpascience', 'gpaenglish',\n```", "```py\n 'gpaoverall', 'gender_Female'], dtype='object')\n```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from mlxtend.feature_selection import ExhaustiveFeatureSelector\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=2,n_jobs=-1, random_state=0)\n    efs = ExhaustiveFeatureSelector(rfc, max_features=5,\n      min_features=1, scoring='accuracy', \n      print_progress=True, cv=5)\n    efs.fit(X_train_enc, y_train.values.ravel())\n    efs.best_feature_names_\n    ('satverbal', 'gpascience', 'gpamath', 'gender_Female')\n    ```", "```py\n    X_train_efs = efs.transform(X_train)\n    X_test_efs = efs.transform(X_test)\n    rfc.fit(X_train_efs, y_train.values.ravel())\n    y_pred = rfc.predict(X_test_efs)\n    confusion = pd.DataFrame(y_pred, columns=['pred'],\n      index=y_test.index).\\\n      join(y_test)\n    confusion.loc[confusion.pred==confusion.completedba].shape[0]\\\n      /confusion.shape[0]\n    0.6703296703296703\n    ```", "```py\n    accuracy_score(y_test, y_pred)\n    0.6703296703296703\n    ```", "```py\n    lr = LogisticRegression(solver='liblinear')\n    efs = ExhaustiveFeatureSelector(lr, max_features=5,\n      min_features=1, scoring='accuracy', \n      print_progress=True, cv=5)\n    efs.fit(X_train_enc, y_train.values.ravel())\n    efs.best_feature_names_\n    ('satmath', 'gpascience', 'gpaenglish', 'motherhighgrade', 'gender_Female')\n    ```", "```py\n    X_train_efs = efs.transform(X_train_enc)\n    X_test_efs = efs.transform(X_test_enc)\n    lr.fit(X_train_efs, y_train.values.ravel())\n    y_pred = lr.predict(X_test_efs)\n    accuracy_score(y_test, y_pred)\n    0.6923076923076923\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=2, \n      n_jobs=-1, random_state=0)\n    efs = ExhaustiveFeatureSelector(rfc, max_features=5,\n      min_features=1, scoring='accuracy', \n      print_progress=True, cv=5)\n    %timeit efs.fit(X_train_enc, y_train.values.ravel())\n    5min 8s ± 3 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n    lr = LogisticRegression(solver='liblinear')\n    efs = ExhaustiveFeatureSelector(lr, max_features=5,\n      min_features=1, scoring='accuracy', \n      print_progress=True, cv=5)\n    %timeit efs.fit(X_train_enc, y_train.values.ravel())\n    4.29 s ± 45.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.feature_selection import RFE\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.linear_model import LinearRegression\n    ```", "```py\n    nls97wages = pd.read_csv(\"data/nls97wages.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','motherhighgrade',\n      'fatherhighgrade','parentincome','gender','completedba']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97wages[feature_cols],\\\n      nls97wages[['weeklywage']], test_size=0.3, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = feature_cols[:-2]\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Male','completedba']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Male','completedba']])\n    scaler.fit(y_train)\n    y_train, y_test = \\\n      pd.DataFrame(scaler.transform(y_train),\n      columns=['weeklywage'], index=y_train.index),\\\n      pd.DataFrame(scaler.transform(y_test),\n      columns=['weeklywage'], index=y_test.index)\n    ```", "```py\n    rfr = RandomForestRegressor(max_depth=2)\n    treesel = RFE(estimator=rfr, n_features_to_select=5)\n    treesel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[treesel.get_support()]\n    selcols\n     Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')\n    ```", "```py\n    pd.DataFrame({'ranking': treesel.ranking_,\n      'feature': X_train_enc.columns},\n       columns=['feature','ranking']).\\\n       sort_values(['ranking'], ascending=True)\n               feature                ranking\n    1          satmath                1\n    5          gpaoverall             1\n    8          parentincome           1\n    9          gender_Male            1\n    10         completedba            1\n    6          motherhighgrade        2\n    2          gpascience             3\n    0          satverbal              4\n    3          gpaenglish             5\n    4          gpamath                6\n    7          fatherhighgrade        7\n    ```", "```py\n    rfr.fit(treesel.transform(X_train_enc), y_train.values.ravel())\n    rfr.score(treesel.transform(X_test_enc), y_test)\n    0.13612629794428466\n    ```", "```py\n    lr = LinearRegression()\n    lrsel = RFE(estimator=lr, n_features_to_select=5)\n    lrsel.fit(X_train_enc, y_train)\n    selcols = X_train_enc.columns[lrsel.get_support()]\n    selcols\n    Index(['satmath', 'gpaoverall', 'parentincome', 'gender_Male', 'completedba'], dtype='object')\n    ```", "```py\n    lr.fit(lrsel.transform(X_train_enc), y_train)\n    lr.score(lrsel.transform(X_test_enc), y_test)\n    0.17773742846314056\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_selection import RFE\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, \n      random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, max_depth=2, \n      n_jobs=-1, random_state=0)\n    treesel = RFE(estimator=rfc, n_features_to_select=5)\n    treesel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[treesel.get_support()]\n    selcols\n    Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpaoverall'], dtype='object')\n    ```", "```py\n    pd.DataFrame({'ranking': treesel.ranking_,\n      'feature': X_train_enc.columns},\n       columns=['feature','ranking']).\\\n       sort_values(['ranking'], ascending=True)\n\n               feature                 ranking\n    0          satverbal               1\n    1          satmath                 1\n    2          gpascience              1\n    3          gpaenglish              1\n    5          gpaoverall              1\n    4          gpamath                 2\n    8          parentincome            3\n    7          fatherhighgrade         4\n    6          motherhighgrade         5\n    9          gender_Female           6\n    ```", "```py\n    rfc.fit(treesel.transform(X_train_enc), y_train.values.ravel())\n    y_pred = rfc.predict(treesel.transform(X_test_enc))\n    accuracy_score(y_test, y_pred)\n    0.684981684981685\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.ensemble import RandomForestClassifier\n    from boruta import BorutaPy\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, \n      max_depth=2, n_jobs=-1, random_state=0)\n    borsel = BorutaPy(rfc, random_state=0, verbose=2)\n    borsel.fit(X_train_enc.values, y_train.values.ravel())\n    BorutaPy finished running.\n    Iteration:            100 / 100\n    Confirmed:            9\n    Tentative:            1\n    Rejected:             0\n    selcols = X_train_enc.columns[borsel.support_]\n    selcols\n    Index(['satverbal', 'satmath', 'gpascience', 'gpaenglish', 'gpamath', 'gpaoverall', 'motherhighgrade', 'fatherhighgrade', 'parentincome', 'gender_Female'], dtype='object')\n    ```", "```py\n    pd.DataFrame({'ranking': borsel.ranking_,\n      'feature': X_train_enc.columns},\n       columns=['feature','ranking']).\\\n       sort_values(['ranking'], ascending=True)\n               feature               ranking\n    0          satverbal             1\n    1          satmath               1\n    2          gpascience            1\n    3          gpaenglish            1\n    4          gpamath               1\n    5          gpaoverall            1\n    6          motherhighgrade       1\n    7          fatherhighgrade       1\n    8          parentincome          1\n    9          gender_Female         2\n    ```", "```py\n    rfc.fit(borsel.transform(X_train_enc.values), y_train.values.ravel())\n    y_pred = rfc.predict(borsel.transform(X_test_enc.values))\n    accuracy_score(y_test, y_pred)\n    0.684981684981685\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade','fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3, \n      random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, \n                        variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    lr = LogisticRegression(C=1, penalty=\"l1\", \n                            solver='liblinear')\n    regsel = SelectFromModel(lr, max_features=5)\n    regsel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[regsel.get_support()]\n    selcols\n    Index(['satmath', 'gpascience', 'gpaoverall', \n    'fatherhighgrade', 'gender_Female'], dtype='object')\n    ```", "```py\n    lr.fit(regsel.transform(X_train_enc), \n           y_train.values.ravel())\n    y_pred = lr.predict(regsel.transform(X_test_enc))\n    accuracy_score(y_test, y_pred)\n    0.684981684981685\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, \n      max_depth=2, n_jobs=-1, random_state=0)\n    rfcsel = SelectFromModel(rfc, max_features=5)\n    rfcsel.fit(X_train_enc, y_train.values.ravel())\n    selcols = X_train_enc.columns[rfcsel.get_support()]\n    selcols\n    Index(['satverbal', 'gpascience', 'gpaenglish', \n      'gpaoverall'], dtype='object')\n    ```", "```py\n    rfc.fit(rfcsel.transform(X_train_enc), \n            y_train.values.ravel())\n    y_pred = rfc.predict(rfcsel.transform(X_test_enc))\n    accuracy_score(y_test, y_pred)\n    0.673992673992674\n    ```", "```py\n    import pandas as pd\n    from feature_engine.encoding import OneHotEncoder\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.decomposition import PCA\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    nls97compba = pd.read_csv(\"data/nls97compba.csv\")\n    feature_cols = ['satverbal','satmath','gpascience',\n      'gpaenglish','gpamath','gpaoverall','gender',\n      'motherhighgrade', 'fatherhighgrade','parentincome']\n    X_train, X_test, y_train, y_test =  \\\n      train_test_split(nls97compba[feature_cols],\\\n      nls97compba[['completedba']], test_size=0.3,\n      random_state=0)\n    ```", "```py\n    ohe = OneHotEncoder(drop_last=True, \n                        variables=['gender'])\n    ohe.fit(X_train)\n    X_train_enc, X_test_enc = \\\n      ohe.transform(X_train), ohe.transform(X_test)\n    scaler = StandardScaler()\n    standcols = X_train_enc.iloc[:,:-1].columns\n    scaler.fit(X_train_enc[standcols])\n    X_train_enc = \\\n      pd.DataFrame(scaler.transform(X_train_enc[standcols]),\n      columns=standcols, index=X_train_enc.index).\\\n      join(X_train_enc[['gender_Female']])\n    X_test_enc = \\\n      pd.DataFrame(scaler.transform(X_test_enc[standcols]),\n      columns=standcols, index=X_test_enc.index).\\\n      join(X_test_enc[['gender_Female']])\n    ```", "```py\n    pca = PCA(n_components=5)\n    pca.fit(X_train_enc)\n    ```", "```py\npd.DataFrame(pca.components_,\n  columns=X_train_enc.columns).T\n                   0       1      2       3       4\nsatverbal         -0.34   -0.16  -0.61   -0.02   -0.19\nsatmath           -0.37   -0.13  -0.56    0.10    0.11\ngpascience        -0.40    0.21   0.18    0.03    0.02\ngpaenglish        -0.40    0.22   0.18    0.08   -0.19\ngpamath           -0.38    0.24   0.12    0.08    0.23\ngpaoverall        -0.43    0.25   0.23   -0.04   -0.03\nmotherhighgrade   -0.19   -0.51   0.24   -0.43   -0.59\nfatherhighgrade   -0.20   -0.51   0.18   -0.35    0.70\nparentincome      -0.16   -0.46   0.28    0.82   -0.08\ngender_Female     -0.02    0.08   0.12   -0.04   -0.11\n```", "```py\n    pca.explained_variance_ratio_\n    array([0.46073387, 0.19036089, 0.09295703, 0.07163009, 0.05328056])\n    np.cumsum(pca.explained_variance_ratio_)\n    array([0.46073387, 0.65109476, 0.74405179, 0.81568188, 0.86896244])\n    ```", "```py\n    X_train_pca = pca.transform(X_train_enc)\n    X_train_pca.shape\n    (634, 5)\n    np.round(X_train_pca[0:6],2)\n    array([[ 2.79, -0.34,  0.41,  1.42, -0.11],\n           [-1.29,  0.79,  1.79, -0.49, -0.01],\n           [-1.04, -0.72, -0.62, -0.91,  0.27],\n           [-0.22, -0.8 , -0.83, -0.75,  0.59],\n           [ 0.11, -0.56,  1.4 ,  0.2 , -0.71],\n           [ 0.93,  0.42, -0.68, -0.45, -0.89]])\n    X_test_pca = pca.transform(X_test_enc)\n    ```", "```py\n    rfc = RandomForestClassifier(n_estimators=100, \n      max_depth=2, n_jobs=-1, random_state=0)\n    rfc.fit(X_train_pca, y_train.values.ravel())\n    y_pred = rfc.predict(X_test_pca)\n    accuracy_score(y_test, y_pred)\n    0.7032967032967034\n    ```"]