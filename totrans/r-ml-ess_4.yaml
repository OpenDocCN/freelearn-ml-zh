- en: Chapter 4. Step 1 – Data Exploration and Feature Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different kinds of problems that require a machine learning solution.
    For instance, our target can be forecasting future outcomes or identifying patterns
    from the data. The starting point is a set of objects (for example, items) or
    people (for example, customers of a supermarket). In most situations, a machine
    learning technique identifies the solution, starting from some features that describe
    objects/people. The features are numeric and/or categorical attributes, and they
    are the base of the machine learning model. Having the right features will improve
    the performance and accuracy of the model, so it is extremely important to define
    some features that are relevant to the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Build machine learning solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a feature data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore the defined features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank the features using a filter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a machine learning solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In which stage of a machine learning solution are we defining the features?
    Let''s look at an overview of the whole procedure of building the solution. We
    can divide the approach into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the features that we will be using.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply one or more techniques to solve the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the result and optimize the performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the first step, we can evaluate the relevance of each feature by using a
    filter and selecting the most relevant feature. We can also define a combination
    of some features that are good for describing the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, when we build a model, we can use some techniques (embedded
    methods) that rank the features and identify the most relevant feature automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is very important since we have more information, allowing us
    to identify a more proper feature set. For instance, we can use the same model
    with different sets of features and evaluate which feature combination performs
    better. An option is to use a wrapper that consists of building a model with a
    chosen set of features, iteratively add (or remove) a feature, and retain the
    change if it improves the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, a feature selection is a cycle rather than a step, and it takes
    place in each part of the procedure. This chapter shows the feature engineering
    process, which consists of defining the features, transforming them, and identifying
    their ranking. The steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining/transforming new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the most relevant features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although exploring the data is always at the beginning, all the steps can be
    repeated until we find a satisfying solution, so they don't always follow the
    same order. For instance, after identifying the most relevant features, we can
    explore the data, identify new patterns, and consequently define some new features.
  prefs: []
  type: TYPE_NORMAL
- en: The process of feature selection is related to the model, and in this chapter,
    we identify some features that are suitable for many models.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows an example of flags. Based on the flag of a country, our
    target is to determine the country's language. Assuming that we know the flags
    of all the countries and the language of only some of them, the model will estimate
    the language of the others.
  prefs: []
  type: TYPE_NORMAL
- en: Building the feature data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section shows how we can structure the raw data to build the features.
    For each country, the data is:'
  prefs: []
  type: TYPE_NORMAL
- en: A picture of the flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some geographical data such as continent, geographic quadrant, area, and population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The language and religion of the country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target is to build a model that predicts a country language starting from
    its flag. Most of the models can deal with numeric and/or categorical data, so
    we can't use the image of the flag as a feature for the model. The solution is
    to define some features, for instance the number of colors, that describe each
    flag. In this way, we start from a table whose rows correspond to the countries
    and whose columns correspond to the flag features.
  prefs: []
  type: TYPE_NORMAL
- en: It would take a lot of time to build the matrix with the flag attributes based
    on the pictures. Fortunately, we can use a dataset that contains some features.
    The data that we have is still a bit messy, so we need to clean and transform
    it to build a feature table in the *right format*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features contained in the dataset display some information about:'
  prefs: []
  type: TYPE_NORMAL
- en: The colors in the flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The patterns in the flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some additional elements in the flag, such as text or some stars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some geographical data, such as continent, geographic quadrant, area, and population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The language and religion of the country
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The steps to lead the table in the right format are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset and the related information from [https://archive.ics.uci.edu/ml/machine-learning-databases/flags/](https://archive.ics.uci.edu/ml/machine-learning-databases/flags/)
    and download `flag.data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open RStudio and set the working directory to the folder that contains the
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data into the R environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can see the structure of `dfFlag` using `str`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dfFlag` object contains 30 columns whose names are not defined. We have
    documentation that describes the data contained in `flag.description.txt`, which
    allows us to define the column names. The first seven columns contain some attributes
    that are not related to the flag. Let''s start defining some vectors that contain
    the feature names. The first column is the name of the country. These are the
    steps to define the names:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the country name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the names of three geographic features: `continent`, `zone`, and `area`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the names of three features of the countries'' citizens, including their
    language:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a unique vector that contains the seven attributes in the right order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the names of the features defining the number of `bars`, `stripes`,
    and `colors`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For some colors, there is a variable that displays `1` if the flag contains
    the color and `0` otherwise. Define their names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the name of the predominant color:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the name of the attributes that display how many patterns/drawings (for
    instance, a shape, a picture, or a text) are contained in the flag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dangles: the color in two out of the four angles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `namesFlag` that contains all the names in the right order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the `dfFlag` column names that bind `namesAttributes` and `namesFlag`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the data frame has the right column names. However, some columns, such
    as `language` contain numbers instead of the attribute name, and the documentation
    shows what the numbers stand for. For instance, for language, 1 corresponds to
    English and 2 to Spanish. We can build a data table that has the data in the right
    format using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert `dfFlag` into the `dtFlag` data table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the `continent` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `continent` column contains a number between `1` and `6` and the documentation
    shows `1=N.America`, `2=S.America`, `3=Europe`, `4=Africa`, `5=Asia`, `6=Oceania`.
    Then, we define a vector that contains the continents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `continent` into `factor` whose levels are `vectorContinents`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to `continent`, convert `zone` into `factor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `language` into `factor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert `religion` into `factor`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a look at `dtFlag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The data is in the right format. Although we had to transform the data properly,
    it still took much less time than defining the features manually.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having defined the features, we can explore them and identify how they
    are related to the problem. In this section, you will see how to explore the data
    and define some simple charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a feature, for instance, `mainhue`, which displays the predominant
    color of a flag. We want to identify the most common predominant colors, and for
    that purpose, we can use `table` to count the number of occurrences of each possible
    value. We can extract the `mainhue` column from `dtFlag` and apply `table` to
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The three most common predominant colors are red, blue, and green. Please note
    that we could have put `table` inside the square brackets, obtaining the same
    result with cleaner code: `dtFlag[, table(mainhue)]`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we perform the same operation over any other column? First, let''s
    define a string called `nameCol` that contains the name of the column that we
    want to analyze. In order to access the column, we can use `get(nameCol)` inside
    the square brackets of `dtFlag`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This notation is very useful because we can easily include it inside a function
    using the name string, visualizing the same results for all the other columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we want to build a chart instead? We can build a histogram using `barplot`.
    Let''s first extract the table with each value of frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `freqValues` method contains the number of countries that speak any language
    in the list. We can extract a language vector using `names`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have all the necessary data to build a histogram (see the documentation
    of `barplot` if you haven''t read [Chapter 3](ch03.html "Chapter 3. A Simple Machine
    Learning Analysis"), *A Simple Machine Learning Analysis*). In addition, we can
    define the colors using `rainbow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exploring and visualizing the features](img/7740OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This chart is very useful if we want to explore an attribute. In order to do
    this using just one line of code, we can define a function that builds this chart
    for a generic column, `nameCol`. In addition, we can add `legend` that displays
    the percentage. In order to display `legend`, we compute `percValues`, which contains
    the percentage of rows that display that value, and use it as the `legend.text`
    argument, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s apply the function to another column, for instance `stripes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring and visualizing the features](img/7740OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a `for` loop, we can generate the same chart for each flag attribute.
    We need time to see the result between, each chart and the following, so we stop
    the program using `readline`. The script pauses until we press *Enter* in the
    console. In this way, we can explore all the features very quickly, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: With these few lines of code, we have observed how frequent the values of each
    feature are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another quick exploration is, given a color, counting the number of flags that
    contain the color. For instance, let''s count the flags that have a red part.
    There is an attribute called `red` whose value is `1` if the flag contains a red
    part and `0` otherwise. If we sum up all the column values, we will obtain the
    total number of flags that contain a red part, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have seen earlier, we can use `get` inside the square brackets. How can
    we do the same for all the common colors? The `namesColors` vector contains the
    name of all the color attributes, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The first element of `namesColors` is `red`, so we can use it to count the
    flags that contain red:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `sapply` (see the documentation) to apply a function over each element
    of `namesColors`. In this case, the function counts the number of flags that contain
    a specific color:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The most common colors are red and green.
  prefs: []
  type: TYPE_NORMAL
- en: Until now, we have explored the flag's features; the next step is to see how
    they are related to the country's language. A fast way is to use a decision tree
    (see [Chapter 3](ch03.html "Chapter 3. A Simple Machine Learning Analysis"), *A
    Simple Machine Learning Analysis*).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the packages to generate and visualize the decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The decision tree model requires a formula object that defines the relationship
    between the variables. In this case, the formula is *language ~ feature1 + feature2
    + …*. We can build the formula by adding all the names contained in `namesFlag`
    using a `for` loop, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We can build the model using `rpart` and visualize the tree using `prp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![Exploring and visualizing the features](img/7740OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Some nodes of the tree are not clear to read. For instance, `saltires` displays
    `1` if the flag has a saltire, and `0` otherwise. The first tree node expresses
    the **saltires >= 0.5** condition, so the flags on the left have a saltire. This
    reflects the fact that the features are not in the appropriate format, so the
    next step will be to transform the feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a new data table called `dtFeatures`, which contains the
    features and the outcome. From now, we will modify `dtFeatures` until all the
    features are in the right format, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function to visualize the table. We will reuse this function
    to keep track of progress during the feature transformation, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The chart is exactly the same as before.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen some techniques to explore the features. The data exploration
    allowed us to investigate the data nature, and it's the starting point to clean
    the current features and define some others. In addition, we have built some functions
    that allow us to generate some charts using just one line of code. We can use
    these functions to keep track of the feature transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our features are attributes that describe the flag, and some of them might not
    be in the right format. In this section, we will take a look at each feature and
    transform it if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to keep track of which features we have already processed, let''s
    start defining an empty vector `namesProcessed`, which contains the features that
    we have already processed. When we transform a feature, we add the feature name
    into `namesProcessed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with the numeric columns, such as `red`, which have two possible
    outcomes: `0`, in case the flag contains red and `1` otherwise. The `red` variable
    defines an attribute, so it should be categorical instead of numeric. Then, we
    can convert `red` into a feature that is `yes` if the color is red and `no` otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a look at the charts of each feature, we notice that some of them
    display only two values that are always `0` and `1`. In order to convert each
    of them into the `yes` and `no` format, we can use a `for` loop. For each feature
    in `namesFlag`, we check if there are only two possible values. If so, we convert
    the feature into a factor. Let''s start with `red`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check if `nameFeat` displays two values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the answer is `TRUE`, so we can generate a vector that contains
    the same column with `no` and `yes` for `0` and `1` respectively. For this purpose,
    we use `factor`, which specifies that the labels are `no` and `yes`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can convert each feature that displays two possible outcomes using
    a `for` loop. For each feature, we check whether it has only two values using
    `if`. After we generate `vectorFactor`, we override the old column using `eval`
    inside the square brackets. Doing `dtFeatures[, eval(''red'') := vectorFactor]`
    is the same as `dtFeatures[, red := vectorFactor]`, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the features that we haven''t transformed yet. The `namesFlag`
    feature contains all the initial features, and `namesProcessed` contains the ones
    that we have already transformed. In order to visualize the features that are
    not in `namesProcessed`, we can use `setdiff`, which is a function that gives
    the elements that are in the first vector and not in the second, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'There are still many features that we haven''t analyzed yet. For instance,
    `bars` is a numeric attribute that displays the number of vertical bars in a flag.
    If we use `bars` as a numeric feature, the model will identify a relationship
    between the language and the model. All flags of the Spanish-speaking countries
    contain zero or three bars, so model can learn something like "the language can
    be Spanish only if we have less than four bars." However, there is no Spanish-speaking
    country whose flag has 1 or 2 bars. A solution is to group the countries on the
    basis of the number of bars, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Modifying the features](img/7740OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The chart shows that the groups with a significant number of flags are the
    **0** and **3** bars. Therefore, the groups can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Flags with no bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flags with three bars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the other flags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can define a new column named `nBars0`, which is equal to `TRUE` if the
    flag has no vertical bars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we define `nBars3`, which is `TRUE` for the flags with three bars.
    We don''t need to define a column with the remaining flags because they can already
    be identified by checking that `nBars0` and `nBars3` are `FALSE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s remove the initial `bars` column and add `bars` to `namesProcessed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The operation that we performed is called **discretization** because we generated
    some discrete features, starting from a numeric one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can transform `stripes` and `colors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the features contained in `namesDrawings` that we haven''t
    processed yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Modifying the features](img/7740OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In all these features, most of the flags display `0`. Therefore, we can group
    the flags into two categories: `0` and the rest. We are defining a new categorical
    variable that is `yes` if the value is greater than `0` and `no` otherwise. The
    process is called **binarization** because we transform some numeric features
    into categorical features that display two values only, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explore the remaining features, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modifying the features](img/7740OS_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The three remaining features are `topleft`, `botright`, and `mainhue`. They
    are all categorical and display more than two possible values. For instance, there
    are eight options for `mainhue`. However, only a few flags have `black`, `brown`,
    or `orange` as their main color. We don''t have enough information to take into
    account the less common colors. In this case, we can define a new categorical
    variable called `dummy variable` for each of them. We can decide to define a dummy
    variable for each possible color with at least 15 flags. The situation is similar
    for `topleft` and `botright`, so we can transform all of them in the same way,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have transformed all the features. However, some of the new columns
    that we have defined are of the `logical` class. It''s better to visualize them
    as categorical attributes that display `yes` or `no`, so it''s best to transform
    them, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the decision tree has changed using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The chart obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modifying the features](img/7740OS_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The decision tree is similar to the earlier tree. However, each node of the
    decision tree is checking a condition whose outcomes are `yes` and `no`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have seen three ways of transforming the features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discretization**: Starting from a numeric variable, we group all the possible
    values in sets. Then, for each set, we define a new variable that displays `yes`
    if the numeric variable belongs to the set and `no` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Binarization**: Starting from a numeric variable, we discretize a numeric
    variable by defining two sets only. We define a threshold and we check whether
    the variable is above or below the threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dummy**: Starting from a categorical variable, we identify the most common
    outcomes. Then, for each common outcome, we define a new variable that displays
    `yes` if the variable is equal to the value and `no` otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking the features using a filter or a dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we defined different features. But are all of them
    really relevant to the problem? There are some techniques called **embedded models**
    that automatically select the most relevant features. We can also build the same
    machine learning model using different sets of features and pick the set whose
    performance is better. Both the options are good, although they require a lot
    of computational power.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to use filters that are techniques that identify the most
    relevant features. We use filters before applying any machine learning model,
    and in this way, we cut a lot of the computational cost of the algorithms. Some
    filters take account of each feature separately and are very computationally efficient.
  prefs: []
  type: TYPE_NORMAL
- en: A simple filter is the **Pearson correlation coefficient**, which is a measure
    of the linear relationship between variables. The correlation is a number between
    -1 and 1, and these two extreme values express a clear linear relationship between
    the two variables. When drawing a chart, all the points lie on the same line.
    A correlation of 0 expresses that there is no linear dependence between the two
    variables. The higher the correlation module, the stronger the linear relationship.
    In our case, we can measure the correlation between each flag attribute and the
    language, and pick the attributes whose correlation module is higher.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique that takes account of each feature separately is the information-gain
    ratio. Let's suppose that we want to build a model without knowing anything about
    the flags. In this case, the best we can do is to identify the most common language
    and assume that each country speaks that language. What if we know only which
    flags contain the color red? The model will definitely be better than not having
    any information. How much better? The information-gain ratio of a feature is an
    index that quantifies the improvement that comes from adding the feature.
  prefs: []
  type: TYPE_NORMAL
- en: The correlation and information-gain ratio take account of each feature separately,
    so they completely ignore the interaction between them. For instance, we can have
    two features that have a high impact on the language and are so strongly related
    with each other that they contain the same information. Let's suppose that we
    have already included one of the two features in the model. Adding the other won't
    provide any further information, although it would be highly relevant by itself.
    If the relationship between the two features is linear, we talk about **multicollinearity**.
  prefs: []
  type: TYPE_NORMAL
- en: In other situations, we have two features that have a little relevance if taken
    separately and have a big impact if taken together. If we rank the features using
    this kind of filter, we will exclude both of them, losing some useful information.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to ranking the features is identifying relevant feature combinations.
    A technique is the **Principal** **component analysis** (**PCA**), and it is based
    on the correlation between features. Starting from the features, the PCA defines
    a set of variables called principal components, which are linearly independent
    of each other. The number of principal components is equal to or less than the
    number of features and the components are ranked by variance. Then, it's possible
    to select a subset of components that have high variances. However, PCA has limitations
    since it is based on linear relationships only, and it doesn't take account of
    the attribute to predict things (language, in our example).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different techniques, and the one that we use in this chapter is
    the **information gain ratio** as it is simple and meaningful. R provides us with
    the `FSelector` package that contains different tools for the feature selection.
    The package requires you to have JRE installed on your computer, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s build the `namesFeatures` vector that contains the name of all the features.
    Then, we can compute their information gain ratio using the `information.gain`
    function, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `dfGains` method is a data frame with a field named `attr_importance`.
    The feature names are the row names, so let''s add another column that contains
    the names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s convert the data frame into a data table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to see the most relevant features, we can sort them by relevance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The `blue` and `saltires` features define very relevant attributes. In order
    to visualize the most relevant features, we can build a chart with the top 12
    attributes, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ranking the features using a filter or a dimensionality reduction](img/7740OS_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have defined feature ranking, we are able to build a model from
    the most relevant features. We can either include all the features whose relevance
    is above a chosen threshold, or pick a defined number of features starting from
    the top. However, we are still not taking into account the interaction between
    the features. For instance, among the top features, we have `the flag contains
    the blue`, `blue is the main color`, and `the bottom right is blue`. Although
    they are all very relevant, they are all about `blue`, so they are redundant and
    we can exclude one of them.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the filters are fast and useful methods to rank the features,
    but we have to be very careful about using them when we build the model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to perform feature selection. After having
    loaded and explored the features, you saw how to transform them using discretization
    and binarization. You also converted categoric features into dummy variables.
    You understood the importance of feature selection and ranked the features using
    the information gain ratio. In the next chapter, we will predict the language
    using machine learning techniques.
  prefs: []
  type: TYPE_NORMAL
