- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Attacking Models with Adversarial Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recent advances in **machine learning** (**ML**) and **artificial intelligence**
    (**AI**) have increased our reliance on intelligent algorithms and systems. ML
    systems are used to make decisions on the fly in several critical applications.
    For example, whether a credit card transaction should be authorized or not or
    whether a particular Twitter account is a bot or not is decided by a model within
    seconds, and this decision affects steps taken in the real world (such as the
    transaction or account being flagged as fraudulent). Attackers use the reduced
    human involvement to their advantage and aim to attack models deployed in the
    real world. **Adversarial ML** (**AML**) is a field of ML that focuses on detecting
    and exploiting flaws in ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks can come in several forms. Attackers may try to manipulate
    the features of a data point so that it is misclassified by the model. Another
    threat vector is data poisoning, where attackers introduce perturbations into
    the training data itself so that the model learns from incorrect data and thus
    performs poorly. An attacker may also attempt to run membership inference attacks
    to determine whether an individual was included in the training data or not. Protecting
    ML models from adversarial attacks and, therefore, understanding the nature and
    workings of such attacks is essential for data scientists in the cybersecurity
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to AML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacking image models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attacking text models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing robustness against adversarial attacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will help you understand how adversarial attacks can manifest themselves,
    which will then help you uncover gaps and vulnerabilities in your ML infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to AML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about what AML exactly is. We will begin by understanding
    the importance ML plays in today’s world, followed by the various kinds of adversarial
    attacks on models.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent times, our reliance on ML has increased. Automated systems and models
    are in every sphere of our life. These systems often allow for fast decision-making
    without the need for manual human intervention. ML is a boon to security tasks;
    a model can learn from historical behavior, identify and recognize patterns, extract
    features, and render a decision much faster and more efficiently than a human
    can. Examples of some ML systems handling security-critical decisions are given
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time fraud detection in credit card usage often uses ML. Whenever a transaction
    is made, the model looks at your location, the amount, the billing code, your
    past transactions, historical patterns, and other behavioral features. These are
    fed into an ML model, which will render a decision of *FRAUD* or *NOT FRAUD*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malware detection systems use ML models to detect malicious applications. The
    model uses API calls made, permissions requested, domains connected, and so on
    to classify the application as *MALWARE* or *BENIGN*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media platforms use ML to identify hate speech or toxic content. Models
    can extract text and image content, topics, keywords, and URLs to determine whether
    a post is *TOXIC* or *NON-TOXIC*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the goal behind listing these applications? In each case, you can see
    that ML plays a prominent role in detecting or identifying an adversary or attacker.
    The attacker, therefore, has an incentive to degrade the performance of the model.
    This leads us to the branch of AML and adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AML is a subfield of AI and ML concerned with the design and analysis of algorithms
    that can robustly and securely operate in adversarial environments. In these scenarios,
    an adversary with malicious intent can manipulate input data to disrupt the behavior
    of ML models, either by causing incorrect predictions or by compromising the confidentiality
    and privacy of the data.
  prefs: []
  type: TYPE_NORMAL
- en: AML attacks are intentionally crafted inputs to ML models that cause them to
    behave in unintended and potentially harmful ways. They can be used for malicious
    purposes, such as compromising the security and privacy of ML models, disrupting
    their normal operation, or undermining their accuracy and reliability. In these
    scenarios, attackers may use adversarial examples to trick ML models into making
    incorrect predictions, compromising the confidentiality and privacy of sensitive
    data, or causing harm to the system or the people who use it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we listed the applications of ML models in some critical tasks
    earlier. Here is how an attacker could manipulate them to their benefit:'
  prefs: []
  type: TYPE_NORMAL
- en: In a fraud detection system, a smart attacker may try to abuse the credit card
    with multiple small purchases instead of a large one. The model may be fooled
    by the purchase amounts and will not flag them as abnormal. Or, the attacker may
    use a **virtual private network** (**VPN**) connection to appear closer to the
    victim and purchase gift cards online, thus evading the model’s location-based
    features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A malware developer may know which features indicate malware presence. Therefore,
    they may try to mask that behavior by requesting some normal permissions or making
    redundant API calls so as to throw the classifier off in the predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A user who wants to post toxic content or hate speech knows which words indicate
    abusive content. They will try to misspell those words so that they are not flagged
    by the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using adversarial attacks, an attacker can potentially fool a system and escape
    undetected. It is, therefore, important for researchers and practitioners in the
    field of ML to understand adversarial attacks and to develop methods for detecting
    and defending against them. This requires a deep understanding of the underlying
    mechanisms of these attacks and the development of new algorithms and techniques
    to prevent them.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial tactics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The end goal of an adversarial attack is to degrade the performance of an ML
    model. Adversarial attacks generally employ one of three strategies: input perturbation,
    data poisoning, or model inversion attacks. We will cover these in detail next.'
  prefs: []
  type: TYPE_NORMAL
- en: Input perturbation attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In input perturbation attacks, the attacker maliciously crafts input examples
    so that they will be misclassified by the model. The attacker makes slight changes
    to the input that are neither discernible to the naked eye nor large enough to
    be detected as anomalous or noisy. Typically, this can include changing a few
    pixels in an image or altering some characters in a word. **Deep learning** (**DL**)
    systems, favored because of their power, are very susceptible to input perturbation
    attacks. Because of non-linear functions and transforms, a small change in the
    input can cause a significant unexpected change in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following screenshot from a 2017 study showing two
    images of a **STOP** sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – An actual image of a STOP sign (left) and the adversarially
    manipulated image (right)](img/B19327_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – An actual image of a STOP sign (left) and the adversarially manipulated
    image (right)
  prefs: []
  type: TYPE_NORMAL
- en: The one on the left is the actual one from the street, and the one on the right
    is an identical one with some pieces of tape. These pieces of tape represent an
    input perturbation on the original. The researchers found that the image on the
    left was correctly detected as a STOP sign, but the model was fooled by the right
    one and detected it as a 45 MPH speed limit sign.
  prefs: []
  type: TYPE_NORMAL
- en: Data poisoning attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data poisoning attacks are malicious attacks in which an adversary manipulates
    or corrupts training data in order to degrade the performance of an ML model or
    cause it to behave in unexpected ways. The goal of these attacks is to cause the
    model to make incorrect predictions or decisions, leading to security vulnerabilities
    or privacy breaches. If the quality of data (in terms of the correctness of labels
    presented to the model) is bad, naturally the resulting model will also be bad.
    Due to incorrect labels, the model will learn correlations and features incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a **supervised ML** (**SML**) scenario, an adversary may manipulate
    the labeled data used for training in order to cause a classifier to misclassify
    certain instances, leading to security vulnerabilities. In another scenario, an
    adversary may add malicious instances to the training data in order to cause the
    model to overfit, leading to a decrease in performance on unseen data. For example,
    if an attacker adds several requests from a malicious domain and marks them as
    safe or benign in the training data, the model may learn that this domain indicates
    safe behavior and, therefore, will not mark other requests from that domain as
    malicious.
  prefs: []
  type: TYPE_NORMAL
- en: These attacks can be particularly dangerous because ML models are becoming increasingly
    widely used in a variety of domains, including security and privacy-sensitive
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Model inversion attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model inversion attacks are a type of privacy attack in which an adversary tries
    to reverse-engineer an ML model to obtain sensitive information about the training
    data or the individuals represented by the data. The goal of these attacks is
    to reveal information about the training data that would otherwise be protected.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a fraud detection scenario, a financial institution might use
    an ML model to identify instances of fraud in financial transactions. An adversary
    might attempt to reverse-engineer the model in order to obtain information about
    the characteristics of fraudulent transactions, such as the types of goods or
    services that are commonly purchased in fraud cases. The attacker may discover
    the important features that are used to discern fraud and, therefore, know what
    to manipulate. This information could then be used to commit more sophisticated
    fraud in the future.
  prefs: []
  type: TYPE_NORMAL
- en: To carry out a model inversion attack, an adversary might start by submitting
    queries to the ML model with various inputs and observing the model’s predictions.
    Over time, the adversary could use this information to build an approximation
    of the model’s internal representation of the data. In some cases, the adversary
    might be able to obtain information about the training data itself, such as the
    values of sensitive features—for example, the age or address of individuals represented
    by the data.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of various kinds of adversarial attacks. In the
    next section, we will turn to implementing a few adversarial attacks on image-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Attacking image models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at two popular attacks on image classification
    systems: **Fast Gradient Sign Method** (**FGSM**) and the **Projected Gradient
    Descent** (**PGD**) method. We will first look at the theoretical concepts underlying
    each attack, followed by actual implementation in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: FGSM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FGSM is one of the earliest methods used for crafting adversarial examples for
    image classification models. Proposed by Goodfellow in 2014, it is a simple and
    powerful attack against **neural network** (**NN**)-based image classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: FGSM working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that NNs are layers of neurons placed one after the other, and there
    are connections from neurons in one layer to the next. Each connection has an
    associated weight, and the weights represent the model parameters. The final layer
    produces an output that can be compared with the available ground truth to calculate
    the loss, which is a measure of how far off the prediction is from the actual
    ground truth. The loss is *backpropagated*, and the model *learns* by adjusting
    the parameters based on the gradient of the loss. This process is known as *gradient
    descent*. If θ is the parameter and L is the loss, the adjusted parameter θ ′ 
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: θ′= θ − η  δL _ δθ
  prefs: []
  type: TYPE_NORMAL
- en: Here, the derivative term δL _ δθ  is known as the *gradient*.
  prefs: []
  type: TYPE_NORMAL
- en: The FGSM adversarial attack leverages the gradients to craft adversarial examples.
    While the learning algorithm *minimizes* the loss by adjusting the weights, the
    FGSM attack works to adjust the input data so as to *maximize* the loss. During
    backpropagation, a small perturbation is added to the image based on the sign
    of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally speaking, given an image X, a new (adversarial) image X ′  can be
    calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: X ′  = X+ ϵ ⋅ sign ( ∇ x ℒ(θ, x, y))
  prefs: []
  type: TYPE_NORMAL
- en: Here, ℒ represents the loss, θ represents the model parameters, and y refers
    to the ground-truth label. The term ℒ(θ, x, y) calculates the loss based on the
    model prediction and ground truth, and ∇ x calculates the gradient. The term ϵ
    is the perturbation added, which is either positive or negative depending on the
    sign of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'A popular example that demonstrates the effectiveness of the FGSM attack is
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Adding noise to an image with FGSM](img/B19327_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Adding noise to an image with FGSM
  prefs: []
  type: TYPE_NORMAL
- en: The original image is predicted to be a panda with a confidence of 57.7%, which
    indicates that the model made the correct prediction. Adversarial noise is added
    to the image depending on the sign of the gradient per pixel (so, either 0.007,
    0, or -0.007). The resulting image is identical to the original one—no difference
    can be seen to the naked eye. This is expected because the human eye is not sensitive
    to such small differences at the pixel level. However, the model now predicts
    the image to be a gibbon with a 99.3% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: FGSM implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us now implement the FGSM attack method using the PyTorch library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin, as usual, by importing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a function that performs the FGSM attack and generates
    an adversarial example. This function calculates the gradient, followed by the
    perturbation, and generates an adversarial image. The `epsilon` parameter is passed
    to it, which indicates the degree of perturbation to be added. In short, the function
    works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Takes in as input an image or an array of images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the predicted label by running it through the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the loss by comparing the predicted label with the actual ground
    truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagates the loss and calculates the gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the perturbation by multiplying `epsilon` with the sign of the gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds this to the image to obtain the adversarial image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to perform the FGSM attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need a basic image classifier to use as the model to attack. As the
    data at hand is images, we will use a **convolutional neural network** (**CNN**).
    For this, we define a class that has two functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The constructor**: This function defines the basic structure of the NN to
    be used for the classification. We define the convolutional and fully connected
    layers that we need. The number of neurons and the number of layers here are all
    design choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The forward function**: This function defines what happens during the forward
    pass of the NN. We take in the input data and pass it through the first convolutional
    layer. The output of this layer is processed through a ReLU activation function
    and then passed to the next layer. This continues for all convolutional layers
    we have. Finally, we flatten the output of the last convolutional layer and pass
    it through the fully connected layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process is illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now write a function that loads the required datasets. For our experiment,
    we will be using the *CIFAR-10* dataset. Developed by the **Canadian Institute
    for Advanced Research** (**CIFAR**), the dataset consists of 60,000 images from
    10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse,
    ship, and truck). Each image is color and of size 32 x 32 pixels. The dataset
    has been divided into 50,000 training and 10,000 test images. As a standardized
    dataset in the world of ML, it is well integrated with Python and PyTorch. The
    following function provides the code to load the train and test sets. If the data
    is not locally available, it will first download it and then load it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: PyTorch provides a standard functionality known as data loaders that facilitates
    easy data manipulation for ML. Data loaders can generate the data needed by applying
    specific transformations, and the generated data can be consumed by ML models.
    Note that when defining the data loader, we have specified a `batch_size` parameter.
    This defines the number of data instances that will be read at a time. In our
    case, it is set to `128`, which means that the forward pass, loss calculation,
    backpropagation, and gradient descent will happen one by one for batches where
    each batch is of 128 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will train a vanilla NN model for image classification on the CIFAR
    dataset. We first perform some boilerplate setup that includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the number of epochs to be used for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loading the train and test data loaders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing the model with the basic CNN model we defined earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the loss function to be cross-entropy and the optimizer to be Adam.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Moving the model to CUDA if it is available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we begin the training loop. In each iteration of the training loop, we
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load a batch of training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move it to the GPU (CUDA) if needed and available.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zero out the optimizer gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the predicted output by running inference on the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss based on prediction and ground truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a code snippet that executes these steps one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we evaluate our model. While evaluating, we evaluate the model on two
    sets of data—the original test data and the adversarial test data that we created
    using FGSM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first set the model to evaluation mode, which means that gradients are not
    computed and stored. This makes the operations more efficient, as we do not need
    the overhead of gradients during inference on the model. For every batch of the
    training data, we calculate the adversarial images using FGSM. Here, we have set
    the value of `epsilon` to 0.005\. Then, we run inference on the model using both
    the clean images (the original test set) and adversarial images (generated through
    FGSM). For every batch, we will calculate the number of examples for which the
    predicted and ground-truth labels match, which will give us the accuracy of the
    model. Comparing the accuracy of the clean and adversarial set shows us how effective
    our adversarial attack is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our discussion of the FGSM attack. You can compare the accuracy
    before and after the adversarial perturbation (`clean_accuracy` and `fgsm_accuracy`,
    respectively). The drop in accuracy indicates the effectiveness of the adversarial
    attack.
  prefs: []
  type: TYPE_NORMAL
- en: PGD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we discussed the FGSM attack method and how it can
    be used to generate adversarial images by adding small perturbations to the input
    image based on the sign of the gradient. The **PGD** method extends FGSM by applying
    it iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: PGD working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specifically, the PGD attack will, for an input image, calculate a perturbation
    based on the FGSM attack. Adding this to the image will give us the perturbed
    image. Whereas the FGSM attack stops here, the PGD attack goes a step further.
    Once an adversarial image has been generated, we clip the image. Clipping refers
    to adjusting the image so that it remains in the neighborhood of the original
    image. Clipping is done on a per-pixel basis. After the image has been clipped,
    we repeat the process multiple times iteratively to obtain the final adversarial
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally speaking, given an image X, a series of adversarial images can be
    calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: X N+1 ′  = Clip X,ϵ( X N ′ + α ⋅ sign (∇ x ℒ(θ, x, y)))
  prefs: []
  type: TYPE_NORMAL
- en: The notation here is slightly different from that for the FGSM attack. Here,
    α serves the same role as ϵ did in FGSM; it controls the amount of perturbation.
    Typically, it is set to 1, which means that each pixel is modified by at most
    one unit in each step. The process is repeated iteratively for a predetermined
    number of steps.
  prefs: []
  type: TYPE_NORMAL
- en: The function that implements this is quite straightforward. It simply uses FGSM
    iteratively and clips the generated images. The FGSM function must be modified
    to take in the predicted ground-truth label by the model, as it will change in
    every step and should not be recalculated by FGSM. So, we pass it the ground truth
    as a parameter and use that instead of recalculating it as a model prediction.
    In the FGSM function, we simply use the value that is passed in instead of running
    inference on the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified FGSM function is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For every image, the PGD method attack function completes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculates the predicted label by running inference on the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sets the original image as the initial adversarial image *X*0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the adversarial image using the FGSM attack method described in the
    previous section. In doing so, it passes the predicted value as a parameter so
    that FGSM does not recompute it in every cycle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computes the difference between the image and the adversarially generated image.
    This is the perturbation to be added.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clips this perturbation so that the adversarial image is within the neighborhood
    of the original image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adds the clipped perturbation to the image to obtain the adversarial image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeats *steps 2*-*6* for the desired number of iterations to obtain the final
    adversarial image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, the overarching idea remains the same as with FGSM, but only
    the process of generating the adversarial images changes.
  prefs: []
  type: TYPE_NORMAL
- en: PGD implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A code snippet for the PGD method is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This function can be used to generate adversarial images given an image using
    the PGD method. We will not repeat the experiment of model setup, training, and
    evaluation. Simply using the `Generate_PGDM_Image()` function instead of the `Generate_FGSM_Image()`
    function should allow you to run our analysis using this attack. How does the
    performance of this attack compare to the FGSM attack?
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of attacking image models. In the next section,
    we will discuss attacking text models.
  prefs: []
  type: TYPE_NORMAL
- en: Attacking text models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Please note that this section contains examples of hate speech and racist*
    *content online.*'
  prefs: []
  type: TYPE_NORMAL
- en: Just as with images, text models are also susceptible to adversarial attacks.
    Attackers can modify the text so as to trigger a misclassification by ML models.
    Doing so can allow an adversary to escape detection.
  prefs: []
  type: TYPE_NORMAL
- en: A good example of this can be seen on social media platforms. Most platforms
    have rules against abusive language and hate speech. Automated systems such as
    keyword-based filters and ML models are used to detect such content, flag it,
    and remove it. If something outrageous is posted, the platform will block it at
    the source (that is, not allow it to be posted at all) or remove it in the span
    of a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A malicious adversary can purposely manipulate the content in order to fool
    a model into thinking that the words are out of vocabulary or are not certain
    abusive words. For example, according to a study (*Poster | Proceedings of the
    2019 ACM SIGSAC Conference on Computer and Communications Security* ([https://dl.acm.org/doi/abs/10.1145/3319535.3363271](https://dl.acm.org/doi/abs/10.1145/3319535.3363271))),
    the attacker can manipulate their post as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A hateful tweet and the adversarially manipulated version](img/B19327_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – A hateful tweet and the adversarially manipulated version
  prefs: []
  type: TYPE_NORMAL
- en: The original tweet that says “*go back to where you came from -- these fucking
    immigrants are destroying America!!!*” is clearly hate speech and racism against
    immigrants. Originally, this was classified to be 95% toxic, that is, a toxicity
    classifier assigned it the label *toxic* with 95% probability. Obviously, this
    classification is correct.
  prefs: []
  type: TYPE_NORMAL
- en: In the obfuscated tweet, the attacker modified three words by eliminating one
    letter from three words. Note that we still very much recognize those words for
    what they are. The intent is clear, and this is still very much hate speech. An
    automated system, however, will not know this. Models and rules work by looking
    at words. To them, these new words are out of their vocabulary. They have not
    seen the words *imigrants*, *fuckng*, or *destroyin* in prior examples of hate
    speech during training. Therefore, the model misclassifies it and assigns it a
    label of not being toxic content with a probability of 63%. The attacker thus
    succeeded in fooling a classifier to pass off their toxic content as benign.
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle of adversarial attacks in text is the same as that of those in
    images: manipulating the input so as to confuse the model and not allowing it
    to recognize certain important features. However, two key differences set adversarial
    manipulation in text apart from adversarial manipulation in images.'
  prefs: []
  type: TYPE_NORMAL
- en: First, the adversarially generated input should be reasonably similar to the
    original input. For example, we saw in the preceding section that the two images
    of the panda were nearly identical. This will not be the case with text—the changes
    made through manipulation will be visible and discernible to the naked eye. Looking
    at the screenshot of tweets that we just discussed, we can clearly see that the
    words are different. Manipulation in text is obvious whereas that in images is
    not. As a result, there is a limit to how much we can manipulate. We cannot change
    every word—too many changes will clearly indicate the manipulation and lead to
    discovery, thus defeating the goal.
  prefs: []
  type: TYPE_NORMAL
- en: Second, images are robust to change as compared to text. Changing multiple pixels
    in an image will still leave the larger image mainly unchanged (that is, a panda
    will still be recognizable, maybe with some distortion). On the other hand, text
    depends on words for the meaning it provides. Changing a few words will change
    the meaning entirely, or render the text senseless. This would be unacceptable—the
    goal of an adversarial attack is to still have the original meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore techniques for manipulating text so that it
    may fool an ML classifier. We will show a few sample techniques and provide general
    guidelines at the end for further exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the example of hate speech online that we discussed earlier: attackers
    can manipulate text so as to escape detection and post toxic content online. In
    this section, we will attempt to build such techniques and examine whether we
    can beat ML models.'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For this experiment, we will use the *Toxic Tweets* dataset (*Toxic Tweets
    Dataset | Kaggle*) ([https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset](https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset)).
    This data is made available freely as part of a Kaggle challenge online. You will
    have to download the data, and then unzip it to extract the CSV file. The data
    can then be read as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you what the data looks like, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Hate speech dataset](img/B19327_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Hate speech dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also look at the distribution of the labels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will show you the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Distribution of tweets by toxicity label](img/B19327_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Distribution of tweets by toxicity label
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains approximately 24,000 tweets that are toxic and 32,500 that
    are not. In the next section, we will extract features from this data.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our earlier chapters, we discussed that there needs to be a method to extract
    features from text, and such features must be numeric in value. One such method,
    which we have already used, is the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) approach. Let us do a brief recap of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: 'TF-IDF is a commonly used technique in **natural language processing** (**NLP**)
    to convert text into numeric features. Every word in the text is assigned a score
    that indicates how important the word is in that text. This is done by multiplying
    two metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF**: How frequently does the word appear in the text sample? This can be
    normalized by the length of the text in words, as texts that differ in length
    by a large number can cause skews. The TF metric measures how common a word is
    in this particular text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IDF**: How frequently does the word appear in the rest of the corpus? First,
    the number of text samples containing this word is obtained. The total number
    of samples is divided by this number. Simply put, IDF is the inverse of the fraction
    of text samples containing the word. The IDF metric measures how common the word
    is in the rest of the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More details on TF-IDF can be found in [*Chapter 7*](B19327_07.xhtml#_idTextAnchor019),
    *Attributing Authorship and How to Evade It*. For now, here is a code snippet
    to extract the TF-IDF features for a list of sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Adversarial attack strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will attempt to evade our ML models using two adversarial strategies: appending
    a letter at the end of some words, and repeating certain vowels from some words.
    In each case, our end goal is to fool the classifier into thinking that there
    are words that it has not seen and, therefore, it does not recognize those words.
    Let us discuss these strategies one by one.'
  prefs: []
  type: TYPE_NORMAL
- en: Doubling the last letter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this strategy, we simply misspell the word by appending an additional letter
    at the end. We double the last letter so that the word appears to be unchanged,
    and is still recognizable. For example, *America* will become *Americaa*, and
    *immigrant* will become *immigrantt*. To a machine, these words are totally different
    from one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to implement this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Doubling vowels
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this attack, we will look for words with vowels, and on finding the first
    vowel, we will repeat it. For example, *Facebook* will become *Faacebook*, and
    *Coronavirus* will become *Cooronavirus*. It is fairly intuitive that repeated
    vowels often go unnoticed when reading text; this means that the text will appear
    to be unchanged to a quick reader. The following code snippet implements this
    attack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Executing the attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have defined the two attacks we will implement, it is time to actually
    execute them. To achieve this, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the data into training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a TF-IDF model on the training data and use it to extract features from
    the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a model based on the features extracted in *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same TF-IDF model to extract features on the test set and run inference
    on the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate metrics for classification—accuracy, precision, recall, and F-1 score.
    These are the baseline scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, apply the attack functions and derive the adversarial test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same TF-IDF model to extract features from the adversarial test set
    and run inference on the trained model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate metrics for classification—accuracy, precision, recall, and F-1 score.
    These are the scores upon adversarial attack.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparing the scores obtained in *steps 5* and *8* will tell us what the effectiveness
    of our attack was.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we split the data and extract features for our baseline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us also set up an evaluation function that takes in the actual and predicted
    values and prints out our metrics, such as accuracy, precision, recall, and F-1
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we build and evaluate our baseline model. This model has no adversarial
    perturbation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This should result in an output as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Metrics for classification of normal data](img/B19327_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Metrics for classification of normal data
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we actually execute the attack. We obtain adversarial samples but train
    the model on the clean data (as during training we do not have access to the attacker’s
    adversarial set). Here, we use the `double_last_letter()` adversarial function
    to compute our adversarial set. We then evaluate the model on the adversarial
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you another set of metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Metrics for classification of adversarial data](img/B19327_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Metrics for classification of adversarial data
  prefs: []
  type: TYPE_NORMAL
- en: 'Carefully note the differences between the scores obtained with the clean and
    adversarial data; we will compare them side by side for clarity, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Metric** | **Clean** | **Adversarial** |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | 0.93 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | 0.93 | 0.91 |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | 0.90 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| F-1 score | 0.92 | 0.85 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Comparing accuracy of normal versus adversarial data
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly see that our adversarial attack has been successful—while accuracy
    dropped only by 5%, the recall dropped by 11%, causing a 7% drop in the F-1 score.
    If you consider this at the scale of a social media network such as Twitter, it
    translates to significant performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: You can similarly evaluate the effect caused by the double-vowel attack. Simply
    generate adversarial examples using the double-vowel function instead of the double-last
    letter function. We will leave this as an exercise to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Further attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section covered only two basic attacks for attacking text-based
    models. As a data scientist, you must think of all possible attack surfaces and
    come up with potential new attacks. You are encouraged to develop and implement
    new attacks and examine how they affect model performance. Some potential attacks
    are presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: Combining two words from the sentence (for example, *Live and Let Live* will
    become *Liveand* *Let Live*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting a long word into two words (for example, *Immigrants will take away
    our jobs* will become *Immi grants will take away* *our jobs*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding hyphens or commas to long words (for example, *Immigrants will take away
    our jobs* will become *Im-migrants will take away* *our jobs*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, readers should also experiment with different feature extraction
    methods to examine whether any of them is more robust to adversarial attacks than
    the others, or our TF-IDF method. A few examples of such methods are set out here:'
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual embeddings (**Bidirectional Encoder Representations from Transformers**,
    or **BERT**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Character-level features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This completes our discussion on how text models can be fooled. In the next
    section, we will briefly discuss how models can be made robust against adversarial
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Developing robustness against adversarial attacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adversarial attacks can be a serious threat to the security and reliability
    of ML systems. Several techniques can be used to improve the robustness of ML
    models against adversarial attacks. Some of these are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adversarial training is a technique where the model is trained on adversarial
    examples in addition to the original training data. Adversarial examples are generated
    by perturbing the original input data in such a way that the perturbed input is
    misclassified by the model. By training the model on both the original and adversarial
    examples, the model learns to be more robust to adversarial attacks. The idea
    behind adversarial training is to simulate the types of attacks that the model
    is likely to face in the real world and make the model more resistant to them.
  prefs: []
  type: TYPE_NORMAL
- en: Defensive distillation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Defensive distillation is a technique that involves training a model on soft
    targets rather than hard targets. Soft targets are probability distributions over
    the classes, while hard targets are one-hot vectors indicating the correct class.
    By training on soft targets, the decision boundaries of the model become smoother
    and more difficult to attack. This is because the soft targets contain more information
    about the distribution of the classes, which makes it more difficult to create
    an adversarial example that will fool the model.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradient regularization is a technique that involves adding a penalty term to
    the loss function of the model that encourages the gradients of the model to be
    small. This helps to prevent an attacker from creating an adversarial example
    by perturbing the input in the direction of the gradient. The penalty term can
    be added to the loss function in various ways, such as through L1 or L2 regularization
    or by using adversarial training. Gradient regularization can be combined with
    other techniques to improve the robustness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Input preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Input preprocessing involves modifying the input data before it is fed into
    the model. This can include techniques such as data normalization, which helps
    to reduce the sensitivity of the model to small changes in the input. Other techniques
    include randomization of the input, which can help to disrupt the pattern of the
    adversarial attack, or filtering out anomalous input that may be indicative of
    an adversarial attack. Input preprocessing can be tailored to the specific model
    and the type of input data that it receives.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensemble methods involve combining multiple models to make a prediction. This
    can improve the robustness of the model by making it more difficult for an attacker
    to craft an adversarial example that will fool all the models in the ensemble.
    Ensemble methods can be used in conjunction with other techniques to further improve
    the robustness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Certified defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Certified defenses involve creating a provable guarantee that a model will be
    robust to a certain level of adversarial attack. This can be done using techniques
    such as interval-bound propagation or randomized smoothing. Interval-bound propagation
    involves computing a range of values that the model’s output can take given a
    certain range of inputs. This range can be used to create a provable bound on
    the robustness of the model. Randomized smoothing involves adding random noise
    to the input data to make the model more robust to adversarial attacks. Certified
    defenses are a relatively new area of research, but hold promise for creating
    more robust ML models.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that while these techniques can improve the robustness of
    ML models against adversarial attacks, they are not foolproof, and there is still
    ongoing research in this area. It’s important to use multiple techniques in combination
    to improve the robustness of the model.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have come to the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent times, human reliance on ML has grown exponentially. ML models are
    involved in several security-critical applications such as fraud, abuse, and other
    kinds of cybercrime. However, many models are susceptible to adversarial attacks,
    where attackers manipulate the input so as to fool the model. This chapter covered
    the basics of AML and the goals and strategies that attackers employ. We then
    discussed two popular adversarial attack methods, FGSM and PGD, along with their
    implementation in Python. Next, we learned about methods for manipulating text
    and their implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the importance and prevalence of ML in our lives, it is necessary
    for security data scientists to understand adversarial attacks and learn to defend
    against them. This chapter provides a solid foundation for AML and the kinds of
    attacks involved.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed multiple aspects of ML for security problems. In the
    next chapter, we will pivot to a closely related topic—privacy.
  prefs: []
  type: TYPE_NORMAL
