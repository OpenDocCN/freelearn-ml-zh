- en: Chapter 11. Ensembling Time Series Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All of the models developed in this book so far have dealt with situations that
    arise when observations are independent of each other. The example of overseas
    visitors explains a time series in which the observations are dependent on the
    previously observed data. In a brief discussion of this example, it was established
    that it is necessary to develop time series models. Since the time series is sequential
    in nature, the time stamp may be displayed in nanoseconds, seconds, minutes, hours,
    days, or months.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will open with a quick review of the important concepts of time
    series in autocorrelation and partial autocorrelation functions, as well as fitted
    model assessment measures. Much like the classification and regression models,
    a host of methods are available for analyzing time series data. An important class
    of time series models in seasonal decomposition includes LOESS (STL), exponential
    smoothing state space models (ets), Box-Jenkins (ARIMA) models, and autoregressive
    neural network models. These will be discussed and illustrated in the following
    section. The ensembling of time series models will be illustrated in the final
    section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main areas that will be covered in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Time series datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging time series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembling time series models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the following R libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`forecast`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`forecastHybrid`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data is structurally different from the data discussed up until
    now. A glimpse of time series data was seen in *Overseas Visitor* in *section
    1* of [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and the bootstrapping of the time series models was briefly touched on in [Chapter
    2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee "Chapter 2. Bootstrapping"),
    *Bootstrapping*. The complexity that arises in the analysis of time series data
    is that the observations are not independent and, consequently, we need to specify
    the dependence. Box et al. (2015) is the benchmark book for the statistical analysis
    of time series, and its first edition was published in 1970\. The class of models
    invented and popularized in Box and Jenkins is the popular autoregressive integrated
    moving average, famously abbreviated as ARIMA. This is also often known as the
    Box-Jenkins model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 1* summarizes twenty-one time series datasets. The Length column gives
    the number of observations/data points of the series, while the **Frequency**
    column gives the periodicity of the time series, and the remaining six columns
    are simply the summaries obtained by applying the summary function to a numeric
    object. The first column, of course, gives the names of the datasets as they are
    available in R, and hence we have not changed the cases, upper or lower. The numbers
    of observations in the datasets range from 19 to 7980.'
  prefs: []
  type: TYPE_NORMAL
- en: But what does frequency or periodicity mean? In a dataset which includes periodicity,
    the associated time index gets repeated. For instance, we might have yearly, quarterly,
    monthly, or weekly data, and thus, in the middle two cases, the frequency will
    be **4** and **12** respectively. The frequency need not be an integer and can
    also be a fractional value. For example, carcinogenesis tests will have values
    in nanoseconds. The summary of the time series data is simply the result of applying
    the summary function to a numeric dataset. Consequently, we implicitly assume
    that time series data is numeric. The variation as seen in the summaries also
    shows that different datasets will require different handling. A quick introduction
    to a time series application can be found in Chapter 10 of Tattar et al. (2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'A description of the datasets used in this chapter can be found in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Length | Frequency | Minimum | Q1 | Median | Mean | Q3 | Maximum
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | 144 | 12 | 104.00 | 180.00 | 265.50 | 280.30 | 360.50 |
    622.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | 150 | 1 | 198.60 | 212.58 | 220.65 | 229.98 | 254.68 | 263.30
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | 84 | 4 | 0.44 | 1.25 | 3.51 | 4.80 | 7.13 | 16.20 |'
  prefs: []
  type: TYPE_TB
- en: '| `LakeHuron` | 98 | 1 | 575.96 | 578.14 | 579.12 | 579.00 | 579.88 | 581.86
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Nile` | 100 | 1 | 456.00 | 798.50 | 893.50 | 919.35 | 1032.50 | 1370.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| `UKgas` | 108 | 4 | 84.80 | 153.30 | 220.90 | 337.63 | 469.90 | 1163.90 |'
  prefs: []
  type: TYPE_TB
- en: '| `UKDriverDeaths` | 192 | 12 | 1057.00 | 1461.75 | 1631.00 | 1670.31 | 1850.75
    | 2654.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `USAccDeaths` | 72 | 12 | 6892.00 | 8089.00 | 8728.50 | 8788.79 | 9323.25
    | 11317.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `WWWusage` | 100 | 1 | 83.00 | 99.00 | 138.50 | 137.08 | 167.50 | 228.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| `airmiles` | 24 | 1 | 412.00 | 1580.00 | 6431.00 | 10527.83 | 17531.50 |
    30514.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `austres` | 89 | 4 | 13067.30 | 14110.10 | 15184.20 | 15273.45 | 16398.90
    | 17661.50 |'
  prefs: []
  type: TYPE_TB
- en: '| `co2` | 468 | 12 | 313.18 | 323.53 | 335.17 | 337.05 | 350.26 | 366.84 |'
  prefs: []
  type: TYPE_TB
- en: '| `discoveries` | 100 | 1 | 0.00 | 2.00 | 3.00 | 3.10 | 4.00 | 12.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `lynx` | 114 | 1 | 39.00 | 348.25 | 771.00 | 1538.02 | 2566.75 | 6991.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| `nhtemp` | 60 | 1 | 47.90 | 50.58 | 51.20 | 51.16 | 51.90 | 54.60 |'
  prefs: []
  type: TYPE_TB
- en: '| `nottem` | 240 | 12 | 31.30 | 41.55 | 47.35 | 49.04 | 57.00 | 66.50 |'
  prefs: []
  type: TYPE_TB
- en: '| `presidents` | 120 | 4 | 23.00 | 46.00 | 59.00 | 56.31 | 69.00 | 87.00 |'
  prefs: []
  type: TYPE_TB
- en: '| `treering` | 7980 | 1 | 0.00 | 0.84 | 1.03 | 1.00 | 1.20 | 1.91 |'
  prefs: []
  type: TYPE_TB
- en: '| `gas` | 476 | 12 | 1646.00 | 2674.75 | 16787.50 | 21415.27 | 38628.50 | 66600.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| `uspop` | 19 | 0.1 | 3.93 | 15.00 | 50.20 | 69.77 | 114.25 | 203.20 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | 2820 | 12 | 0.00 | 15.70 | 42.00 | 51.27 | 74.93 | 253.80 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Time Series Datasets in R'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AirPassengers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `AirPassengers` dataset contains the monthly totals of international airline
    passengers from 1949 to 1960\. The monthly count numbers are in thousands. Over
    twelve years, the monthly data accumulated 144 observations. Since we have multiple
    observations across the years for the months, the seasonality aspect of the travelers
    count can be captured from the data. This was popularized in Box et al. (2015).
  prefs: []
  type: TYPE_NORMAL
- en: co2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `co2` time series data is related to atmospheric concentrations of carbon
    dioxide. The concentration is expressed in parts per million (ppm), and this dataset
    is reported in the preliminary 1997 SIO manometric mole fraction scale. This time
    series was captured on a monthly basis for the period of 1959–97\. On the help
    page of `co2`, it is noted that the missing values for the months of February,
    March, and April, 1964 are obtained by linearly interpolating between the values
    for January and May of 1964.
  prefs: []
  type: TYPE_NORMAL
- en: uspop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The US census population (in millions), `uspop`, was recorded by the decennial
    census between 1790 and 1970\. This was made available in a small time series
    dataset and, accordingly, it only consists of 19 data points. Seasonality is not
    captured in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: gas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `gas` time series data contains Australian monthly gas production. The data
    available here is for the period of 1956–95\. Consequently, we have 476 observations
    here. This dataset is drawn from the `forecast` package.
  prefs: []
  type: TYPE_NORMAL
- en: Car Sales
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `car sales` data is adapted from Abraham and Ledolter (1983). For more information,
    refer to Table 2.7, page 68 of the book. The sales and advertising data is available
    for a period of 36 months. Here, we have additional information on the amount
    spent on advertisements each week. This is the first instance of additional variables
    availability, and it calls for specialized treatment, which we will explore further
    later in the chapter. The data is available in the `Car_Sales.csv` file and this
    is available in the code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: austres
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `austres` time series dataset consists of a quarterly number of Australian
    residents for the period March 1971 to March 1994.
  prefs: []
  type: TYPE_NORMAL
- en: WWWusage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `WWWusage` time series dataset consists of the number of users connected
    to the internet through a server. The data is collected at a time interval of
    one minute. The time series values are collected for 100 observations.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization gives invaluable insights and we will plot some of the time series
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Time series visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main characteristic of time series data is that the observations are taken
    at regular intervals. A plot of the time series values (the *y* axis) against
    the time itself (*x* axis) is of great importance and gives away many structural
    insights. A time series plot is not merely a scatterplot with time as the *x*
    axis. The time is non-decreasing and hence it has more importance and meaning
    in a time series plot than the mere *x* axis in a scatterplot. For instance, lines
    can connect the points of a time series plot that will indicate the path of the
    time series, and such a connection would be meaningless in the scatterplot, which
    would be all over the place. The path will generally indicate the trend and as
    such, shows in which direction the series will go next. Changes in time series
    are easily depicted in the plot. We will now visualize the different time series.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plot.ts` function is central to the scheme of visualization here. An external
    graphical device of appropriate size is first invoked with the `windows` function.
    The `X11` function can be used in Ubuntu/Linux. Next, we run the `plot.ts` function
    on the `AirPassengers` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot shows an increase in the number of monthly passengers across
    the years, on average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00467.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Monthly airline passenger count'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see here that a pattern seems to be repeating itself after a cycle of
    12 months, which indicates a seasonal trend across the months. It would be nice
    to obtain a plot where we select the first year and look at the plot across the
    months, then move to the next year and impose the next year''s monthly data and
    visualize it, and so on until the entire data is displayed. A `plotts` function
    is created which achieves a plot of this description, and its structure is given
    in *Figure 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00468.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The time series frequency plot function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `plotts` function is now applied on the `AirPassengers` data. The function
    is available in the `Utilities.R` file of the companion chapter code bundle, and
    it is invoked in the beginning of the `C11.R` file using the `source` command.
    The data has 12 years of data, and thus we will have 12 curves on the resulting
    time series plots. The `legend` of the plot will require more than the usual area,
    and hence we plot it on the right-hand side of the graph. The required manipulations
    are accomplished with the `par`, `mar`, and `legend` functions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now clearly see the seasonal impact in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00469.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Seasonal plot for the AirPassengers dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The monthly passenger count visit hits a low in the months of February and November.
    The monthly passenger count increases steadily from February to July, remains
    at a similar level for August, and then decreases steeply until November. A slight
    increase can be seen for the months of December and January. Consequently, the
    seasonal plots give more insights, and they should be used complementarily with
    the `plot.ts` function. Hyndman's forecast package contains a function named `seasonalplot`
    which accomplishes the same result as the `plotts` function defined here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Australian residents dataset `austres` is covered next. The `plotts` function
    and legend will be used to enhance the display:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is a quarterly time series of the number of Australian residents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00470.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Quarterly time series of the number of Australian residents'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between the seasonal plots of *Figure 4* and *Figure
    5*? Of course, we are looking for differences other than the trivial monthly and
    quarterly periodicity. In *Figure 5*, we can see that, although there is an increase
    in the quarterly number of monthly residents, it is hardly a seasonal factor;
    it appears to be more of a trend factor than a seasonal one. Thus, the seasonal
    contribution appears less in comparison with the `AirPassengers` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time series plot for the carbon dioxide concentrations is visualized next.
    We use the `plot.ts` function on the `co2` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running the `plot.ts` function is the next output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00471.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Visualization of the Mauna Loa atmospheric CO2 concentration'
  prefs: []
  type: TYPE_NORMAL
- en: 'A seasonal impact is easily seen in the time series plot of the carbon dioxide
    concentrations. A seasonal plot might provide more insight, and we will use the
    `plotts` function next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `plotts` and `legend` functions have been used in the same way as previously.
    The result of the program is shown in *Figure 7*, and we can clearly see the seasonal
    impact in the time series display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00472.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Seasonal plot of the Mauna Loa Atmospheric CO2 concentration'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Use the `seasonalplot` function from the `forecast` package and
    replicate the seasonal plots. What is the difference here, if any?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The seasonality is an important aspect of the time series. It is important
    to identify it early so that an appropriate model is chosen for analysis of the
    time series. We will visualize three more time series datasets, `UKDriverDeaths`,
    `gas`, and `uspop`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The three displays in *Figure 7* are unlike any seen thus far. It seems unlikely
    that the models that fit well earlier will perform similarly here. We see a lot
    of variability for the `UKDriverDeaths` and `gas` datasets. For `UKDriverDeaths`,
    it appears that there was a decline in fatalities after the year `1983`. For the
    gas dataset, we can see that there was a regular seasonal impact until the year
    `1970`, and following that year, the `gas` productivity shot up drastically. It
    might be an indication of some technological breakthrough or some other phenomenological
    changes. The variability also increases, and barely appears constant across the
    time horizon. The `uspop` shows an exponential growth.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: Visually inspect if there is a seasonal impact for the `UKDriverDeaths`
    and `gas` datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Time series visualization](img/00473.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Three time series plots: UKDriverDeaths, gas, and uspop'
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The visualization of the time series data has conveyed a similar story in all
    the examples, from *Figure 1* to *Figure 7*. The trend and seasonality of the
    observations imply that the future values of the time series are dependent on
    the current values, and thus we can't assume that the observations are independent
    of each other. But what does this mean? To reiterate the point, consider the simpler
    `uspop` (US population) dataset, the third-right panel display of *Figure 7*.
    Here, we don't have a seasonal influence. Now, consider the census year 1900\.
    The population at the next census is certainly not less than in the year 1890,
    and it is not well beyond the same number of the same year. A similar narrative
    holds for most time series; for example, if we are recording the maximum temperature
    of the day. Here, if today's maximum temperature is 42°C, the next day's maximum
    temperature is highly influenced by this number and it is almost completely ruled
    out that the next day's maximum temperature will be either 55°C or 25°C. Although
    it is easily seen that the observations are dependent on each other, the formal
    specification is also a challenge in itself. Let us formally introduce a time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will denote a time series by ![Core concepts and metrics](img/00474.jpeg)that
    is observed at times ![Core concepts and metrics](img/00475.jpeg). An alternative
    notation for a time series observed up to time T is ![Core concepts and metrics](img/00476.jpeg).
    A time series may be conceptualized as a stochastic process Y observed at times
    t =1, 2, 3, …. Associated with the time series process ![Core concepts and metrics](img/00474.jpeg)
    is the error process ![Core concepts and metrics](img/00477.jpeg). The error process
    is generally assumed to be a white noise process with zero mean and some constant
    variance. The error process is often referred to as the innovation process. Note
    that the time series ![Core concepts and metrics](img/00478.jpeg) might depend
    on the past values of the process in ![Core concepts and metrics](img/00479.jpeg),
    as well as the values of error ![Core concepts and metrics](img/00480.jpeg) and
    the past values of the error process ![Core concepts and metrics](img/00481.jpeg).
    The value ![Core concepts and metrics](img/00482.jpeg) is also referred to as
    the first lag value of ![Core concepts and metrics](img/00478.jpeg), ![Core concepts
    and metrics](img/00483.jpeg) the second lag value of ![Core concepts and metrics](img/00478.jpeg),
    and so on. Now, if the observations are dependent on each other, the specification
    of the relationship between them is the biggest challenge. Of course, we can''t
    go into detail here. However, if we believe that the first-order lagged terms
    are dependent, there must be a relationship here, and we can obtain a scatterplot
    with the observations ![Core concepts and metrics](img/00478.jpeg) on the y-axis
    and the first-order lagged terms on the x-axis. The first-order lagged scatterplot
    for the `AirPassengers` dataset is obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The use of indexing changes the class of time series objects to a numeric object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00484.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Lag plot of the AirPassengers dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding graphical display, we can clearly see that there is (almost)
    a linear relationship between the lagged observations, and thus a model might
    be of the form ![Core concepts and metrics](img/00485.jpeg). Can the scatterplot
    help in determining the order of the dependency? Hardly! We will obtain a plot
    of the `WWWusage` dataset next and look at the lagged plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Following are the lagged plots for `WWWUsage`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00486.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Lagged plots for WWWUsage'
  prefs: []
  type: TYPE_NORMAL
- en: The first lagged plot might give the impression that the observations are correlated.
    However, the higher-order lagged plots barely make any sense, and going back to
    the first-order lagged plot creates more confusion. As a consequence, we need
    a more formal method of obtaining insights on the order of lags.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two measures that are useful for understanding the nature of dependency
    in time series data are **auto-correlation function** (**ACF**) and **partial
    auto-correlation function** (**PACF**). As the name suggests, ACF is the correlation
    of a time series with its own lagged values. The PACF''s partial nomenclature
    accounts for removing the impact of intermediate variables from the lagged one.
    In simple terms, the PACF for lag 3 will include only the first ![Core concepts
    and metrics](img/00478.jpeg); third lagged variables ![Core concepts and metrics](img/00487.jpeg)
    and the ![Core concepts and metrics](img/00488.jpeg) variables are not allowed
    to influence the PACF. The lag-k ACF is defined as the correlation between the
    random variable *Y* *t* and the k-th lagged variable *Y* *t-k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00489.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![Core concepts and metrics](img/00490.jpeg) is the variance of the time
    series. The partial autocorrelation function PACF between *Yt* and its k-th lag
    *Yt-k* is the partial correlation of the time series when controlling the values
    at shorter lags *Y* *t-1*, *Y* *t-2*, *…,* *Y* *t-k+1*. It is not possible to
    go into the mathematical details of the PACF concept; the reader may refer to
    Box et al. (2015) for more information. The sample ACF formula, based on *n* observations,
    is given using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00491.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For an explicit formula of the PACF, we refer the reader to the document available
    on the web at [http://mondi.web.elte.hu/spssdoku/algoritmusok/acf_pacf.pdf](http://mondi.web.elte.hu/spssdoku/algoritmusok/acf_pacf.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'In spite of the intimidating formula, we have an easy getaway by simply using
    the `acf` and `pacf` functions on two datasets, `austres` and `uspop`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will keep the interpretation of ACF and PACF plots simpler. The important
    guideline in ACF and PACF plots are the horizontal blue lines. Any lagged ACF
    and PACF plots beyond the two lines are significant, and those within the limits
    are insignificant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00492.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: ACF and PACF plots for austres and uspop datasets'
  prefs: []
  type: TYPE_NORMAL
- en: We can see from *Figure 10* for the `austres` time series that we need to extend
    the ACF plot to include more lags. This is because all plotted lags are beyond
    the horizontal blue lines. For the `uspop` time series, the first time series,
    the first four lags are significant and the rest are within the horizontal blue
    lines. The PACF plots can be interpreted in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: The ACF and PACF plots play an important role in the identification of the ARMA
    models. Even if the plots reveal information about the lag for the case of AR,
    it can be used to specify the number of previous values of the time series as
    part of the input vector for a neural network adapted for the time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many practical problems, we have additional variables, and we might refer
    to these as covariate time series or exogenous variables. Let us denote the covariate
    time series by ![Core concepts and metrics](img/00493.jpeg), where ![Core concepts
    and metrics](img/00494.jpeg) might be a scalar or vector time series. We adopt
    the convention that only the current and past values of ![Core concepts and metrics](img/00495.jpeg),
    will influence ![Core concepts and metrics](img/00478.jpeg) and that the future
    values of ![Core concepts and metrics](img/00496.jpeg) will not impact ![Core
    concepts and metrics](img/00478.jpeg) in any manner. That is, only the lagged
    values of the covariates will have influence, as opposed to their lead values.
    In the `Car Sales` dataset, the sales are the time series of interest, and it
    is the advertising aspect that we believe impacts the sales; sales can''t possibly
    explain the advertisement amount! The `ccf` function is used to obtain the cross-correlation
    coefficients as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the cross correlation between sales and advertising:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00497.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Cross-correlation coefficients for advertising spend and car sales'
  prefs: []
  type: TYPE_NORMAL
- en: Should we look at the positive lag values or the negative lag values? Note that
    the `ccf` plot is not symmetrical, and hence we can't be absolved for ignoring
    the signs of the lag values. On running `?ccf` at the R terminal, we get `The
    lag k value returned by ccf(x, y) from the help file, which estimates the correlation
    between x[t+k] and y[t]`. Consequently, the positive lags are the lead-ins, while
    the negative lags are of interest to us. In this example, only the previous lags
    of ![Core concepts and metrics](img/00494.jpeg), (that is, ![Core concepts and
    metrics](img/00494.jpeg) and ![Core concepts and metrics](img/00498.jpeg)) are
    significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'We close this section with a brief discussion of the accuracy measurements.
    As with earlier learning problems, we will have a slew of models available for
    us. This is the main topic of discussion in the next section, and we need to define
    certain assessment metrics accordingly. Let ![Core concepts and metrics](img/00474.jpeg)
    denote the time series and, as a consequence of using a certain model, the fitted
    values will be ![Core concepts and metrics](img/00499.jpeg). We can access the
    accuracy of the models through various methods; some of the accuracy measurements
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Core concepts and metrics](img/00500.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the moment, we will not concern ourselves with the model. Instead, we will
    use it as a main tool to extract fitted values and help in obtaining the defined
    metrics. Using the `subset` function, we will define the training dataset and
    fit a model using the `auto.arima` function from the `forecast` package. Using
    the `accuracy` and `forecast` functions, we will then obtain the different accuracies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `forecast` function is a very important one. Given a fitted time series,
    it will provide predictions for the future periods as requested, and the accuracy
    function computes the required accuracy for the seven different criteria. The
    mean error criteria is often useless, and for near unbiased models, its numerical
    value will be in the vicinity of 0\. The metrics RMSE, MAE, MPE, and MAPE are
    often useful, and the lower their values, the better the model fit is. Furthermore,
    the training set error and test set error must be comparable. If they are very
    different from one other, then the model is not useful for forecasts. In the following
    section, we will review a class of useful time series models.
  prefs: []
  type: TYPE_NORMAL
- en: Essential time series models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have encountered a set of models for the different regression models thus
    far. Time series data brings additional complexity, and hence we have even more
    models to choose from (or rather, ensemble from). A quick review of the important
    models is provided here. Most of the models discussed here deal with univariate
    time series ![Essential time series models](img/00474.jpeg), and we need even
    more specialized models and methods to incorporate ![Essential time series models](img/00493.jpeg).
    We will begin with the simplest possible time series model and then move up to
    the neural network implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that we have the data ![Naïve forecasting](img/00501.jpeg), and we
    need forecasts for the next *h* time points ![Naïve forecasting](img/00502.jpeg).
    The naïve forecast model does not require any modeling exercises or computations,
    it simply returns the current value as future predictions, and thus ![Naïve forecasting](img/00503.jpeg).
    It''s that simple. Even for this simple task, we will use the naïve function from
    the forecast package and ask it to provide the forecast for the next `25` observations
    with `h=25`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As anticipated, the forecast values remain the same throughout the period.
    They can be visualized easily, and the accuracies can also be easily computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The natural question that arises is whether the naïve forecasts are any good.
    An answer to this can be provided in other ways. Complex and sophisticated models
    will always claim to have advantages and merits. The models might indeed have
    advantages, but the reference and benchmarking should be clear. The naïve forecasts
    provide this important benchmark. Note that, for the training period, the accuracy
    values are different for the naïve forecasts, and it is important that the proposed
    models are at least better than the metrics of the naïve forecasts. This is the
    main purpose of the naïve forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Seasonal, trend, and loess fitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Season, trend, and loess are the three technical words that are combined to
    form the stl model. Earlier, we saw in the visual displays of the time series
    that some of them depict seasonal effects, some show a trend, some show a combination
    of both seasonal and trend, and some are simply irregular time series. The displays
    thus indicate the presence or absence of the specific nature of the underlying
    phenomenon. In this section, we will look at how to make use of the seasonal and
    trend part of the time series. The third component of the stl model in loess is
    not explained at all. Loess is a nonparametric regression technique, and stands
    for local polynomial regression, which generalizes the weighted least squares
    criteria to a p-th order polynomial. The loess method also consists of a vital
    component known as kernel. Kernel is a smoothing method, but we will not go into
    too much detail about this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cleveland et al. (1990) proposed a seasonal-trend decomposition based on the
    loess, and the full details of the procedure can be obtained from the following
    source: [http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/stl-a-seasonal-trend-decomposition-procedure-based-on-loess.pdf](http://www.scb.se/contentassets/ca21efb41fee47d293bbee5bf7be7fb3/stl-a-seasonal-trend-decomposition-procedure-based-on-loess.pdf).
    This paper is readable, intuitive, and insightful, and the reader should truly
    follow it. The stl model is a filtering method that decomposes a seasonal time
    series in three parts: trend, seasonal, and remainder. Let ![Seasonal, trend,
    and loess fitting](img/00501.jpeg) be the time series, and we denote trend, seasonal,
    and remainder parts by ![Seasonal, trend, and loess fitting](img/00504.jpeg),
    ![Seasonal, trend, and loess fitting](img/00505.jpeg), and ![Seasonal, trend,
    and loess fitting](img/00506.jpeg); then we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Seasonal, trend, and loess fitting](img/00507.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Refer to Cleveland et al.'s paper for complete details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `stl` function from the `stats` package, we decompose the `AirPassengers`
    data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'By executing the preceding code, the following graph plot is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Seasonal, trend, and loess fitting](img/00508.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: STL decompose of AirPassengers'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to specify the seasonality through the `s.window` option in
    the `stl` function. From the seasonal plot, we can see that each component increases
    with time. However, we get a clear picture of the different components of the
    passenger count over time. Though the seasonal part increases in magnitude, the
    pattern remains the same throughout the period. The trend shows a linear increase,
    which indicates the direction in which the series is headed. It is clear that
    seasonality plays an important role in this context.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: It has been previously remarked that seasonality does not appear
    to be a useful factor in the analysis of the `austres` dataset. Use the `stl`
    decomposition and check whether the observation holds true.'
  prefs: []
  type: TYPE_NORMAL
- en: A more parametric model will be considered next, in the form of an exponential
    smoothing model.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing state space model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The basic exponential smoothing model can be clearly defined. Denote the smoothing
    factor by ![Exponential smoothing state space model](img/00509.jpeg), and initialize
    ![Exponential smoothing state space model](img/00510.jpeg). The basic exponential
    smoothing model is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exponential smoothing state space model](img/00511.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The details of the exponential models can be found at [https://labs.omniti.com/people/jesus/papers/holtwinters.pdf](https://labs.omniti.com/people/jesus/papers/holtwinters.pdf).
    A more general model form is the **exponential smoothing state space model**.
    Here, a model is specified on three fronts, as in the stl model: the error component,
    the trend component, and the third is the seasonal component. In the `ets` function
    of the forecast package, the component can have **additive** effect denoted by
    **A**, it can have a **multiplicative** effect, denoted by M, or it might be asked
    to be automatically selected (Z), and this specification is possible for each
    of the components. The effect can be specified to be neither additive nor multiplicative
    with the letter N. A model in the `ets` function is thus specified with the first
    letter for error component, the second letter for the trend component, and the
    third letter for the seasonal component. Consequently, the notation `model="ZZZ"`
    means that each of the three components is selected automatically. `model="AMZ"`
    means that the error component is additive, the trend is multiplicative, the seasonal
    component is automatically chosen, and so on. Hyndman et al. (2008) provides a
    comprehensive account of the details of exponential smoothing methods. Next, we
    will use the `ets` function from the `forecast` package to fit the exponential
    smoothing model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ets` function has fit an additive error for the error and trend component,
    while choosing not to add any of it for the seasonal factor. This makes sense
    because there is no seasonal component for the `uspop` dataset. Using this fitted
    model, we will `forecast` the US population for the period of 1940–70, calculate
    the accuracies for the training and test dataset with the `accuracy` function,
    and also compare with the `naive` forecasts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is a plot depicting exponential smoothing for US population data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Exponential smoothing state space model](img/00512.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Exponential smoothing for US population data'
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy comparison is performed with the naïve forecasts, and we find that
    there is a significant improvement with the `ets` forecasts. Consequently, the
    `ets` forecasts are useful, and we can use them for future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will move on to the popular Box-Jenkins/ARIMA models.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive Integrated Moving Average (ARIMA) models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Box and Jenkins'' approach to time series with the ARIMA models changed the
    way analysis and forecasts of time series data is performed. The ARIMA model is
    a special case of the more general linear process model, and for the time series
    ![Auto-regressive Integrated Moving Average (ARIMA) models](img/00474.jpeg) with
    the innovation process ![Auto-regressive Integrated Moving Average (ARIMA) models](img/00477.jpeg),
    it is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00513.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Auto-regressive Integrated Moving Average (ARIMA) models](img/00514.jpeg)
    are the coefficients of the linear process. Note that there is no restriction
    on the lagged values of the innovation process and we indeed mean that there are
    infinite terms in this linear process model. In the popular autoregressive AR(p)
    model, p is the order of the AR model. This is given using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00515.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The AR model can be shown to be a particular case of the linear process model.
    When the time series is expressed in terms of the innovation process, another
    useful model is the moving average MA(q) model of order q:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00516.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A time series might depend on past errors as well as past values, and such
    a structural dependency is captured in an autoregressive moving average `ARMA(p,q)`
    model of order `(p,q)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00517.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The order of *p* and *q* can be decided by the rule of thumb related through
    ACF and PACF in *Table 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00518.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2: ACF and PACF for ARMA models'
  prefs: []
  type: TYPE_NORMAL
- en: The ARMA models work well with stationary time series data, and here stationary
    loosely means that the variability of the series is constant throughout. It is
    a restrictive assumption, and for many time series phenomena, it does not hold
    true. In many practical scenarios, it has been that stationary can be obtained
    by differencing the series ![Auto-regressive Integrated Moving Average (ARIMA)
    models](img/00474.jpeg), that is, we can consider the difference ![Auto-regressive
    Integrated Moving Average (ARIMA) models](img/00519.jpeg). The difference ![Auto-regressive
    Integrated Moving Average (ARIMA) models](img/00520.jpeg) is a first-order difference,
    and sometimes, one may require higher order difference. In most practical scenarios,
    differencing up to order 4 has been known to bring stationarity. The order of
    differencing is generally denoted by the letter d, and applying ARMA models on
    the difference is referred to as an autoregressive integrated moving average model,
    or ARIMA model. A succinct abbreviation is `ARIMA(p,d,q)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have come across the seasonal component too often in this chapter, and it
    is accommodated in ARIMA through seasonal ARIMA models. The motivated reader can
    go through Chapter 10 of Tattar et al. (2017) for more details. We will simply
    add here that we have further notation in capital letters (P, D, Q) for the seasonal
    parameters, along with the frequency term. We are now in a position to understand
    the fitted model at the end of the previous section. The `co2_arima` accuracy
    had been accessed, and we will now look at the summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The best fit ARIMA model order is `(2,1,2)(1,1,2)[12]`, which means that the
    seasonal frequency is at `12` (something we already knew), and that the seasonal
    order (P,D,Q) is `(1,1,2)` and the ARIMA order (p,d,q) is `(2,1,2)`. It is this
    differencing order that achieves stationarity. The forecasts are obtained and
    then visualized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Auto-regressive Integrated Moving Average (ARIMA) models](img/00521.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: CO2 forecast'
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, we have fitted the ARIMA models for the carbon dioxide concentration
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the nonlinear neural network time series models.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-regressive neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Neural networks have previously been seen in action for classification as well
    as regression problems. Since the time series are dependent observations, we need
    to tweak the architecture of neural networks to incorporate the dependency. The
    tweaking is to allow the lagged values of the time series as the members of the
    input layer. The rest of the architecture follows the same structure as the usual
    neural network. The `nnetar` function in the `forecast` stands for neural network
    autoregressive, and the `p=` option allows the lagged values of the time series,
    which we apply to the `gas` problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Auto-regressive neural networks](img/00522.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Gas forecast using auto-regressive neural network'
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen the autoregressive neural network in action.
  prefs: []
  type: TYPE_NORMAL
- en: Messing it all up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We began the chapter with a brief introduction of seven datasets and a mention
    of 21 datasets in *Table 1*. The data visualization provides moderate insight,
    and the accuracy metrics are useful in analyzing the usefulness of the models.
    A series of models have been introduced in this section up to now, and now we
    will mess it all up. A `get_Accuracy` function, which will fit six different time
    series models, is defined. The **LM**, which stands for **linear model**, has
    not been explained; neither has the TBATS model. The linear model is very simple,
    in that the time index is taken as a covariate. In general, if a time series has
    T observations, the covariate vector simply consists of the values 1, 2, 3, …,
    T. We expect the linear model to perform poorly. The TBATS model won''t be explained
    in further detail here and so it is recommended to do some extra reading in order
    to get more information on this. The `get_Accuracy` model fits each of the six
    models to the 21 datasets, names the model, and enlists its performance over the
    entire dataset. The following program gets the desired results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code block is the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| TSName | Model | ME | RMSE | MAE | MPE | MAPE | MASE |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | ETS | 1.5807 | 10.6683 | 7.7281 | 0.4426 | 2.8502 | 0.0164
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | STL | -0.1613 | 11.9379 | 8.5595 | -0.0662 | 3.4242 | 0.5515
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | LM | 0.0000 | 45.7362 | 34.4055 | -1.2910 | 12.3190 | 0.7282
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | ARIMA | 1.3423 | 10.8462 | 7.8675 | 0.4207 | 2.8005 | -0.0012
    |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | NNETAR | -0.0118 | 14.3765 | 11.4899 | -0.2964 | 4.2425
    | 0.5567 |'
  prefs: []
  type: TYPE_TB
- en: '| `AirPassengers` | TBATS | 0.4655 | 10.6614 | 7.7206 | 0.2468 | 2.8519 | 0.0215
    |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | ETS | 0.1466 | 1.3272 | 1.0418 | 0.0657 | 0.4587 | -0.0110 |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | STL | NA | NA | NA | NA | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | LM | 0.0000 | 9.1504 | 7.1133 | -0.1563 | 3.1686 | 0.9872 |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | ARIMA | 0.1458 | 1.3281 | 1.0447 | 0.0651 | 0.4601 | -0.0262
    |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | NNETAR | -0.0001 | 1.4111 | 1.0849 | -0.0040 | 0.4798 | 0.2888
    |'
  prefs: []
  type: TYPE_TB
- en: '| `BJsales` | TBATS | 0.1622 | 1.3566 | 1.0666 | 0.0732 | 0.4712 | -0.0113
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | ETS | 0.0495 | 0.4274 | 0.2850 | 1.0917 | 7.0339 | -0.2948
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | STL | -0.0088 | 0.1653 | 0.1080 | -0.5953 | 2.8056 | -0.4155
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | LM | 0.0000 | 1.6508 | 1.3287 | 22.6663 | 66.3896 | 0.6207
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | ARIMA | 0.0677 | 0.4074 | 0.2676 | 2.0526 | 6.5007 | 0.0101
    |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | NNETAR | 0.0003 | 0.3501 | 0.2293 | -0.6856 | 5.8778 |
    -0.0347 |'
  prefs: []
  type: TYPE_TB
- en: '| `JohnsonJohnson` | TBATS | 0.0099 | 0.4996 | 0.3115 | 0.9550 | 7.5277 | -0.1084
    |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | ETS | -0.0153 | 15.9356 | 11.2451 | #NAME? | Inf | 0.0615 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | STL | 0.0219 | 12.2612 | 8.7973 | NA | Inf | 0.18 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | LM | 0 | 42.9054 | 34.1212 | #NAME? | Inf | 0.9196 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | ARIMA | -0.0267 | 15.6006 | 11.0258 | NA | Inf | -0.0106 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | NNETAR | -0.0672 | 10.3105 | 7.6878 | NA | Inf | 0.0108 |'
  prefs: []
  type: TYPE_TB
- en: '| `sunspots` | TBATS | -0.0514 | 15.5788 | 11.0119 | NA | Inf | -0.0013 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Accuracy for six models across 21 datasets'
  prefs: []
  type: TYPE_NORMAL
- en: The overall message is the same as what was obtained in the introductory chapter
    for the classification problem. Since it is not always possible for us to carry
    out the inspection of the results all the time, it is desirable to combine the
    results of multiple models to put across a unified story of greater accuracy.
    We begin this task with the simple idea of bagging the exponential time series
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging and time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will only illustrate the bagging technique for the ETS
    model. The main purpose of bagging is to stabilize the predictions or forecasts.
    Here, we will base the bagging on the Box-Cox and Loess-based decomposition. Using
    500 such bootstrap samples, the bagging model for ETS will be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Is there an advantage to using the bagging method? We can quickly check this
    using the confidence intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The confidence intervals of the bagged ETS are clearly shorter, and hence reflect
    the decrease in the variance, which is the main purpose of bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging and time series](img/00523.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Bagging forecasts for US population'
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy comparison is also easily performed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of ensembling homogeneous base learners is clearly seen here.
    Next, we move to the heterogeneous base learners and their ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble time series models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `forecastHybrid` R package gives a platform to ensemble heterogeneous time
    series models. The main function that enables this task is the `hybridModel` function.
    The core function provides the option referred to as `models`. It takes as input
    a string of up to six characters, and the characters are representatives of the
    models: `a` for the `auto.arima` model, `e` for `ets`, `f` for `thetam`, `n` denoting
    `nnetar`, `s` for `stlm`, and finally, `t` represents `tbats`. Consequently, if
    we give a character string of `ae` to models, it will combine results from the
    ARIMA and ets models. This is illustrated on the `co2` dataset for different combinations
    of the time series models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Though the discussion of the ensemble is very brief here, time series literature
    has only recently begun to adapt to ensembling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**: The options of `weights` and `errorMethod` are crucial to put
    different time series models together. Explore these options for the different
    datasets introduced and discussed in the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series data poses new challenges and complexities. The chapter began with
    an introduction to important and popular datasets. We looked at different time
    series and their intricacies. Visualization of time series provides great insight,
    and the time series plots, along with the seasonal plot, are complementarily used
    for clear ideas and niche implementations. Accuracy metrics are different for
    the time series, and we looked at more than a handful of these. The concepts of
    ACF and PACF are vital in model identification, and seasonal components are also
    important to the modeling of time series. We also saw that different models express
    different datasets, and the degree of variation is something similar to the usual
    regression problems. The bagging of time series (ets only) reduces the variance
    of the forecasts. Combining heterogeneous base learners was discussed in the concluding
    section. The next chapter is the concluding chapter. We will summarize the main
    takeaways from the first eleven chapters and outline some shortcomings and further
    scope.
  prefs: []
  type: TYPE_NORMAL
