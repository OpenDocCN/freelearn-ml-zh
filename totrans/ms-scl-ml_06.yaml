- en: Chapter 6. Working with Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。处理无结构数据
- en: 'I am very excited to introduce you to this chapter. Unstructured data is what,
    in reality, makes big data different from the old data, it also makes Scala to
    be the new paradigm for processing the data. To start with, unstructured data
    at first sight seems a lot like a derogatory term. Notwithstanding, every sentence
    in this book is unstructured data: it does not have the traditional record / row
    / column semantics. For most people, however, this is the easiest thing to read
    rather than the book being presented as a table or spreadsheet.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常激动地向您介绍这一章。无结构数据在现实中使得大数据与旧数据不同，它也使得Scala成为处理数据的新范式。首先，无结构数据乍一看似乎像是一个贬义词。尽管如此，这本书中的每一句话都是无结构数据：它没有传统的记录/行/列语义。然而，对于大多数人来说，这比将书籍呈现为表格或电子表格更容易阅读。
- en: In practice, the unstructured data means nested and complex data. An XML document
    or a photograph are good examples of unstructured data, which have very rich structure
    to them. My guess is that the originators of the term meant that the new data,
    the data that engineers at social interaction companies such as Google, Facebook,
    and Twitter saw, had a different structure to it as opposed to a traditional flat
    table that everyone used to see. These indeed did not fit the traditional RDBMS
    paradigm. Some of them can be flattened, but the underlying storage would be too
    inefficient as the RDBMSs were not optimized to handle them and also be hard to
    parse not only for humans, but for the machines as well.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，无结构数据意味着嵌套和复杂的数据。一个XML文档或一张照片都是无结构数据的良好例子，它们具有非常丰富的结构。我的猜测是，这个术语的创造者原本的意思是，新的数据，工程师在像Google、Facebook和Twitter这样的社交互动公司看到的数据，与传统大家熟悉的扁平表格结构不同。这些数据确实不符合传统的RDBMS范式。其中一些可以被展平，但作为RDBMS没有针对它们进行优化，存储效率低下，而且不仅对人类，对机器也很难解析。
- en: A lot of techniques introduced in this chapter were created as an emergency
    Band-Aid to deal with the need to just process the data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的大量技术都是作为应急措施创建的，以应对仅处理数据的需要。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the serialization, popular serialization frameworks, and language
    in which the machines talk to each other
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解序列化、流行的序列化框架以及机器之间交流的语言
- en: Learning about Avro-Parquet encoding for nested data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解嵌套数据的Avro-Parquet编码
- en: Learning how RDBMs try to incorporate nested structures in modern SQL-like languages
    to work with them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习RDBMs如何尝试在类似SQL的现代语言中结合嵌套结构来处理它们
- en: Learning how you can start working with nested structures in Scala
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在Scala中开始处理嵌套结构
- en: Seeing a practical example of sessionization—one of the most frequent use cases
    for unstructured data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看一个会话化的实际例子——无结构数据最常见用例之一
- en: Seeing how Scala traits and match/case statements can simplify path analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看Scala特性和match/case语句如何简化路径分析
- en: Learning where the nested structures can benefit your analysis
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习嵌套结构如何有助于您的分析
- en: Nested data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌套数据
- en: 'You already saw unstructured data in the previous chapters, the data was an
    array of **LabeledPoint**, which is a tuple **(label: Double, features: Vector)**.
    The label is just a number of type **Double**. **Vector** is a sealed trait with
    two subclasses: **SparseVector** and **DenseVector**. The class diagram is as
    follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '您已经在前面的章节中看到了无结构数据，数据是一个**LabeledPoint**数组，它是一个**(label: Double, features:
    Vector**)的元组。标签只是一个**Double**类型的数字。**Vector**是一个密封特质，有两个子类：**SparseVector**和**DenseVector**。类图如下：'
- en: '![Nested data](img/B04935_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![嵌套数据](img/B04935_06_01.jpg)'
- en: 'Figure 1: The LabeledPoint class structure is a tuple of label and features,
    where features is a trait with two inherited subclasses {Dense,Sparse}Vector.
    DenseVector is an array of double, while SparseVector stores only size and non-default
    elements by index and value.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LabeledPoint类结构是一个标签和特征的元组，其中特征是一个具有两个继承子类{Dense,Sparse}Vector的特质。DenseVector是一个double类型的数组，而SparseVector通过索引和值存储大小和非默认元素。
- en: 'Each observation is a tuple of label and features, and features can be sparse.
    Definitely, if there are no missing values, the whole row can be represented as
    vector. A dense vector representation requires (*8 x size + 8*) bytes. If most
    of the elements are missing—or equal to some default value—we can store only the
    non-default elements. In this case, we would require (*12 x non_missing_size +
    20*) bytes, with small variations depending on the JVM implementation. So, the
    threshold for switching between one or another, from the storage point of view,
    is when the size is greater than *1.5 x* ( *non_missing_size + 1* ), or if roughly
    at least 30% of elements are non-default. While the computer languages are good
    at representing the complex structures via pointers, we need some convenient form
    to exchange these data between JVMs or machines. First, let''s see first how Spark/Scala
    does it, specifically persisting the data in the Parquet format:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个观测值都是一个标签和特征的元组，特征可以是稀疏的。当然，如果没有缺失值，整个行可以表示为向量。密集向量表示需要 (*8 x size + 8*) 字节。如果大多数元素缺失或等于某个默认值，我们只需存储非默认元素。在这种情况下，我们需要
    (*12 x non_missing_size + 20*) 字节，具体取决于 JVM 实现的小幅变化。因此，从存储的角度来看，在大小大于 *1.5 x*
    ( *non_missing_size + 1* ) 或大约至少 30% 的元素是非默认值时，我们需要在一种或另一种表示之间切换。虽然计算机语言擅长通过指针表示复杂结构，但我们还需要一种方便的形式来在
    JVM 或机器之间交换这些数据。首先，让我们看看 Spark/Scala 是如何做的，特别是将数据持久化到 Parquet 格式：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: What we did was create a new RDD dataset from command line, or we could use
    `org.apache.spark.mllib.util.MLUtils` to load a text file, converted it to a DataFrames
    and create a serialized representation of it in the Parquet file under the `points`
    directory.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是从命令行创建一个新的 RDD 数据集，或者我们可以使用 `org.apache.spark.mllib.util.MLUtils` 来加载一个文本文件，将其转换为
    DataFrames，并在 `points` 目录下创建其 Parquet 文件的序列化表示。
- en: Note
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**What Parquet stands for?**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet 代表什么？**'
- en: Apache Parquet is a columnar storage format, jointly developed by Cloudera and
    Twitter for big data. Columnar storage allows for better compression of values
    in the datasets and is more efficient if only a subset of columns need to be retrieved
    from the disk. Parquet was built from the ground up with complex nested data structures
    in mind and uses the record shredding and assembly algorithm described in the
    Dremel paper ([https://blog.twitter.com/2013/dremel-made-simple-with-parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)).
    Dremel/Parquet encoding uses definition/repetition fields to denote the level
    in the hierarchy the data is coming from, which covers most of the immediate encoding
    needs, as it is sufficient to store optional fields, nested arrays, and maps.
    Parquet stores the data by chunks, thus probably the name Parquet, which means
    flooring composed of wooden blocks arranged in a geometric pattern. Parquet can
    be optimized for reading only a subset of blocks from disk, depending on the subset
    of columns to be read and the index used (although it very much depends on whether
    the specific implementation is aware of these features). The values in the columns
    can use dictionary and **Run-Length Encoding** (**RLE**), which provides exceptionally
    good compression for columns with many duplicate entries, a frequent use case
    in big data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet 是一种列式存储格式，由 Cloudera 和 Twitter 联合开发，用于大数据。列式存储允许对数据集中的值进行更好的压缩，如果只需要从磁盘检索部分列，则效率更高。Parquet
    是从底层构建的，考虑到复杂嵌套数据结构，并使用了 Dremel 论文中描述的记录切割和组装算法（[https://blog.twitter.com/2013/dremel-made-simple-with-parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)）。Dremel/Parquet
    编码使用定义/重复字段来表示数据来自层次结构中的哪个级别，这覆盖了大多数直接的编码需求，因为它足以存储可选字段、嵌套数组和映射。Parquet 通过块存储数据，因此可能得名
    Parquet，意为由按几何图案排列的木块组成的地面。Parquet 可以优化为只从磁盘读取部分块，这取决于要读取的列子集和使用的索引（尽管这很大程度上取决于特定实现是否了解这些功能）。列中的值可以使用字典和**运行长度编码**（**RLE**），这对于具有许多重复条目的列提供了非常好的压缩效果，这在大数据中是一个常见的用例。
- en: 'Parquet file is a binary format, but you might look at the information in it
    using `parquet-tools`, which are downloadable from [http://archive.cloudera.com/cdh5/cdh/5](http://archive.cloudera.com/cdh5/cdh/5):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 文件是一种二进制格式，但您可以使用 `parquet-tools` 来查看其中的信息，这些工具可以从 [http://archive.cloudera.com/cdh5/cdh/5](http://archive.cloudera.com/cdh5/cdh/5)
    下载：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s look at the schema, which is very close to the structure depicted in
    *Figure 1*: first member is the label of type double and the second and last one
    is features of composite type. The keyword optional is another way of saying that
    the value can be null (absent) in the record for one or another reason. The lists
    or arrays are encoded as a repeated field. As the whole array may be absent (it
    is possible for all features to be absent), it is wrapped into optional groups
    (indices and values). Finally, the type encodes whether it is a sparse or dense
    representation:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看模式，它与*图1*中描述的结构非常相似：第一个成员是类型为double的标签，第二个和最后一个成员是复合类型特征。关键字optional是另一种表示值可以在记录中为null（缺失）的方式，这可能是因为一个或另一个原因。列表或数组被编码为重复字段。由于整个数组可能缺失（所有特征都可能缺失），它被包裹在可选组（索引和值）中。最后，类型编码表示它是一个稀疏表示还是密集表示：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You are probably a bit confused about the `R`: and `D`: in the output. These
    are the repetition and definition levels as described in the Dremel paper and
    they are necessary to efficiently encode the values in the nested structures.
    Only repeated fields increment the repetition level and only non-required fields
    increment the definition level. Drop in `R` signifies the end of the list(array).
    For every non-required level in the hierarchy tree, one needs a new definition
    level. Repetition and definition level values are small by design and can be efficiently
    stored in a serialized form.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对输出中的`R:`和`D:`有点困惑。这些是重复和定义级别，如Dremel论文中所述，并且它们是高效编码嵌套结构中的值所必需的。只有重复字段会增加重复级别，只有非必需字段会增加定义级别。`R`的下降表示列表（数组）的结束。对于层次树中的每个非必需级别，都需要一个新的定义级别。重复和定义级别值设计得较小，可以有效地以序列化形式存储。
- en: What is best, if there are many duplicate entries, they will all be placed together.
    The case for which the compression algorithm (by default, it is gzip) are optimized.
    Parquet also implements other algorithms exploiting repeated values such as dictionary
    encoding or RLE compression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有大量重复条目，它们都会放在一起。这是压缩算法（默认为gzip）优化的情况。Parquet还实现了其他利用重复值的算法，例如字典编码或RLE压缩。
- en: This is a simple and efficient serialization out of the box. We have been able
    to write a set of complex objects to a file, each column stored in a separate
    block, representing all values in the records and nested structures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单且高效的默认序列化。我们已经能够将一组复杂对象写入文件，每个列存储在一个单独的块中，代表记录中的所有值和嵌套结构。
- en: 'Let''s now read the file and recover RDD. The Parquet format does not know
    anything about the `LabeledPoint` class, so we''ll have to do some typecasting
    and trickery here. When we read the file, we''ll see a collection of `org.apache.spark.sql.Row`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来读取文件并恢复RDD。Parquet格式对`LabeledPoint`类一无所知，所以我们必须在这里进行一些类型转换和技巧。当我们读取文件时，我们会看到一个`org.apache.spark.sql.Row`的集合：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Personally, I think that this is pretty cool: without any compilation, we can
    encode and decide complex objects. One can easily create their own objects in
    REPL. Let''s consider that we want to track user''s behavior on the web:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 个人认为，这相当酷：无需任何编译，我们就可以编码和决定复杂对象。在REPL中，人们可以轻松地创建自己的对象。让我们考虑我们想要跟踪用户在网上的行为：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As a matter of good practice, we need to register the newly created classes
    with the `Kryo` `serializer`—Spark will use another serialization mechanism to
    pass the objects between tasks and executors. If the class is not registered,
    Spark will use default Java serialization, which might be up to *10 x* slower:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为良好的实践，我们需要将新创建的类注册到`Kryo`序列化器中——Spark将使用另一种序列化机制在任务和执行器之间传递对象。如果未注册类，Spark将使用默认的Java序列化，这可能会慢上*10倍*：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you are deploying the code on a cluster, the recommendation is to put this
    code in a jar on the classpath.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在集群上部署代码，建议将此代码放在类路径上的jar文件中。
- en: I've certainly seen examples of up to 10 level deep nesting in production. Although
    this might be an overkill for performance reasons, nesting is required in more
    and more production business use cases. Before we go into the specifics of constructing
    a nested object in the example of sessionization, let's get an overview of serialization
    in general.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实在生产中见过10层深嵌套的例子。虽然这可能在性能上可能有些过度，但在越来越多的生产业务用例中，嵌套是必需的。在我们深入到构建嵌套对象的细节之前，比如在会话化的例子中，让我们先对序列化有一个整体的了解。
- en: Other serialization formats
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他序列化格式
- en: I do recommend the Parquet format for storing the data. However, for completeness,
    I need to at least mention other serialization formats, some of them like Kryo
    will be used implicitly for you during Spark computations without your knowledge
    and there is obviously a default Java serialization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实推荐使用Parquet格式来存储数据。然而，为了完整性，我至少需要提及其他序列化格式，其中一些，如Kryo，将在Spark计算过程中不为人知地为你使用，并且显然存在默认的Java序列化。
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Object-oriented approach versus functional approach**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**面向对象方法与函数式方法**'
- en: Objects in object-oriented approach are characterized by state and behavior.
    Objects are the cornerstone of object-oriented programming. A class is a template
    for objects with fields that represent the state, and methods that may represent
    the behavior. Abstract method implementation may depend on the instance of the
    class. In functional approach, the state is usually frowned upon; in pure programming
    languages, there should be no state, no side effects, and every invocation should
    return the same result. The behaviors may be expressed though additional function
    parameters and higher order functions (functions over functions, such as currying),
    but should be explicit unlike the abstract methods. Since Scala is a mix of object-oriented
    and functional language, some of the preceding constraints are violated, but this
    does not mean that you have to use them unless absolutely necessary. It is best
    practice to store the code in jar packages while storing the data, particularly
    for the big data, separate from code in data files (in a serialized form); but
    again, people often store data/configurations in jar files, and it is less common,
    but possible to store code in the data files.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 面向对象方法中的对象以状态和行为为特征。对象是面向对象编程的基石。一个类是具有表示状态的字段和可能表示行为的方法的对象的模板。抽象方法实现可能依赖于类的实例。在函数式方法中，状态通常是不受欢迎的；在纯编程语言中，不应该有状态，没有副作用，并且每次调用都应该返回相同的结果。行为可以通过额外的函数参数和高级函数（如柯里化函数）来表示，但应该像抽象方法一样明确。由于Scala是一种面向对象和函数式的语言，一些先前的约束被违反了，但这并不意味着你必须在绝对必要时才使用它们。最佳实践是在存储数据的同时将代码存储在jar包中，尤其是对于大数据，将数据文件（以序列化形式）与代码文件分开；但再次强调，人们经常将数据/配置存储在jar文件中，而将代码存储在数据文件中则较少见，但也是可能的。
- en: The serialization has been an issue since the need to persist data on disk or
    transfer object from one JVM or machine to another over network appeared. Really,
    the purpose of serialization is to make complex nested objects be represented
    as a series of bytes, understandable by machines, and as you can imagine, this
    might be language-dependent. Luckily, serialization frameworks converge on a set
    of common data structures they can handle.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化问题自从需要在磁盘上持久化数据或通过网络将对象从一个JVM或机器传输到另一个机器以来就出现了。实际上，序列化的目的是使复杂的嵌套对象以一系列字节的形式表示，这些字节对机器来说是可理解的，并且正如你可以想象的那样，这可能是语言相关的。幸运的是，序列化框架在可以处理的一组常见数据结构上达成了一致。
- en: 'One of the most popular serialization mechanisms, but not the most efficient,
    is to dump an object in an ASCII file: CSV, XML, JSON, YAML, and so on. They do
    work for more complex nested data like structures, arrays, and maps, but are inefficient
    from the storage space perspective. For example, a Double represents a continuous
    number with 15-17 significant digits that will, without rounding or trivial ratios,
    take 15-17 bytes to represent in US ASCII, while the binary representation takes
    only 8 bytes. Integers may be stored even more efficiently, particularly if they
    are small, as we can compress/remove zeroes.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的序列化机制之一，但并非最有效的，是将对象转储到ASCII文件中：CSV、XML、JSON、YAML等等。它们可以处理更复杂的嵌套数据结构，如结构、数组和映射，但从存储空间的角度来看效率不高。例如，一个Double表示一个具有15-17位有效数字的连续数字，在不进行舍入或简单比例的情况下，在US
    ASCII中表示将占用15-17个字节，而二进制表示只需8个字节。整数可能存储得更有效率，尤其是如果它们很小，因为我们可以压缩/删除零。
- en: One advantage of text encoding is that they are much easier to visualize with
    simple command-line tools, but any advanced serialization framework now comes
    with a set of tools to work with raw records such as `avro` *-* or `parquet-tools`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码的一个优点是它们可以用简单的命令行工具轻松可视化，但现在任何高级序列化框架都附带了一套用于处理原始记录的工具，例如`avro` *-* 或 `parquet-tools`。
- en: 'The following table provides an overview for most common serialization frameworks:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了大多数常见的序列化框架：
- en: '| Serialization Format | When developed | Comments |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 序列化格式 | 开发时间 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| XML, JSON, YAML | This was a direct response to the necessity to encode nested
    structures and exchange the data between machines. | While grossly inefficient,
    these are still used in many places, particularly in web services. The only advantage
    is that they are relatively easy to parse without machines. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| XML, JSON, YAML | 这是对编码嵌套结构和在机器之间交换数据的必要性的直接回应。虽然效率低下，但它们仍然被许多地方使用，尤其是在网络服务中。唯一的优点是它们相对容易解析，无需机器。|'
- en: '| Protobuf | Developed by Google in the early 2000s. This implements the Dremel
    encoding scheme and supports multiple languages (Scala is not officially supported
    yet, even though some code exists). | The main advantage is that Protobuf can
    generate native classes in many languages. C++, Java, and Python are officially
    supported. There are ongoing projects in C, C#, Haskell, Perl, Ruby, Scala, and
    more. Run-time can call native code to inspect/serialize/deserialize the objects
    and binary representations. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Protobuf | 由 Google 在 2000 年代初开发。它实现了 Dremel 编码方案并支持多种语言（Scala 尚未官方支持，尽管存在一些代码）。|
    主要优点是 Protobuf 可以在许多语言中生成原生类。C++、Java 和 Python 是官方支持的。有 C、C#、Haskell、Perl、Ruby、Scala
    等语言的持续项目。运行时可以调用原生代码来检查/序列化/反序列化对象和二进制表示。|'
- en: '| Avro | Avro was developed by Doug Cutting while he was working at Cloudera.
    The main objective was to separate the encoding from a specific implementation
    and language, allowing better schema evolution. | While the arguments whether
    Protobuf or Avro are more efficient are still ongoing, Avro supports a larger
    number of complex structures, say unions and maps out of the box, compared to
    Protobuf. Scala support is still to be strengthened to the production level. Avro
    files have schema encoded with every file, which has its pros and cons. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Avro | Avro 是由 Doug Cutting 在 Cloudera 工作期间开发的。其主要目标是将编码与特定的实现和语言分离，从而实现更好的模式演进。尽管关于
    Protobuf 或 Avro 哪个更高效的争论仍在继续，但与 Protobuf 相比，Avro 支持更多的复杂结构，例如开箱即用的联合和映射。Scala
    的支持仍需加强以达到生产级别。Avro 文件中每个文件都包含编码的模式，这既有优点也有缺点。|'
- en: '| Thrift | The Apache Thrift was developed at Facebook for the same purpose
    Protobuf was developed. It probably has the widest selection of supported languages:
    C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js,
    Smalltalk, OCaml, Delphi, and other languages. Again, Twitter is hard at work
    for making the Thrift code generation in Scala ([https://twitter.github.io/scrooge/](https://twitter.github.io/scrooge/)).
    | Apache Thrift is often described as a framework for cross-language services
    development and is most frequently used as **Remote Procedure Call** (**RPC**).
    Even though it can be used directly for serialization/deserialization, other frameworks
    just happen to be more popular. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Thrift | Apache Thrift 是在 Facebook 为与 Protobuf 相同的目的开发的。它可能拥有最广泛的语言支持选择：C++、Java、Python、PHP、Ruby、Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml、Delphi
    以及其他语言。Twitter 也在努力为 Scala 中的 Thrift 代码生成器（[https://twitter.github.io/scrooge/](https://twitter.github.io/scrooge/))
    进行开发。| Apache Thrift 通常被描述为跨语言服务开发的框架，并且最常用于 **远程过程调用** (**RPC**)。尽管它可以直接用于序列化/反序列化，但其他框架却更受欢迎。|'
- en: '| Parquet | Parquet was developed in a joint effort between Twitter and Cloudera.
    Compared to the Avro format, which is row-oriented, Parquet is columnar storage
    that results in better compression and performance if only a few columns are to
    be selected. The interval encoding is Dremel or Protobuf-based, even though the
    records are presented as Avro records; thus, it is often called **AvroParquet**.
    | Advances features such as indices, dictionary encoding, and RLE compression
    potentially make it very efficient for pure disk storage. Writing the files may
    be slower as Parquet requires some preprocessing and index building before it
    can be committed to the disk. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | Parquet 是由 Twitter 和 Cloudera 联合开发的。与以行为导向的 Avro 格式相比，Parquet 是列式存储，如果只选择少量列，则可以实现更好的压缩和性能。区间编码基于
    Dremel 或 Protobuf，尽管记录以 Avro 记录的形式呈现；因此，它通常被称为 **AvroParquet**。| 索引、字典编码和 RLE
    压缩等高级功能可能使其对于纯磁盘存储非常高效。由于 Parquet 需要进行一些预处理和索引构建才能将其提交到磁盘，因此写入文件可能会更慢。|'
- en: '| Kryo | This is a framework for encoding arbitrary classes in Java. However,
    not all built-in Java collection classes can be serialized. | If one avoids non-serializable
    exceptions, such as priority queues, Kryo can be very efficient. Direct support
    in Scala is also under way. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Kryo | 这是一个用于在Java中编码任意类的框架。然而，并非所有内置的Java集合类都可以序列化。 | 如果避免非序列化异常，例如优先队列，Kryo可以非常高效。Scala的直接支持也在进行中。|'
- en: Certainly, Java has a built-in serialization framework, but as it has to support
    all Java cases, and therefore is overly general, the Java serialization is far
    less efficient than any of the preceding methods. I have certainly seen other
    companies implement their own proprietary serialization earlier, which would beat
    any of the preceding serialization for the specific cases. Nowadays, it is no
    longer necessary, as the maintenance costs definitely overshadow the converging
    inefficiency of the existing frameworks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Java有一个内置的序列化框架，但由于它必须支持所有Java情况，因此过于通用，Java序列化比任何先前的方法都要低效得多。我确实看到其他公司更早地实现了他们自己的专有序列化，这会优于先前的任何序列化针对特定情况。如今，这已不再必要，因为维护成本肯定超过了现有框架的收敛低效。
- en: Hive and Impala
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive和Impala
- en: 'One of the design considerations for a new framework is always the compatibility
    with the old frameworks. For better or worse, most data analysts still work with
    SQL. The roots of the SQL go to an influential relational modeling paper (*Codd,
    Edgar F* (June 1970). *A Relational Model of Data for Large Shared Data Banks*.
    *Communications of the ACM (Association for Computing Machinery) 13 (6): 377–87*).
    All modern databases implement one or another version of SQL.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 新框架的设计考虑因素之一总是与旧框架的兼容性。不论好坏，大多数数据分析师仍在使用SQL。SQL的根源可以追溯到一篇有影响力的关系建模论文（*Codd,
    Edgar F*（1970年6月）。*大型共享数据银行的数据关系模型*。《ACM通讯》（计算机制造协会）13（6）：377–87）。所有现代数据库都实现了SQL的一个或多个版本。
- en: 'While the relational model was influential and important for bringing the database
    performance, particularly for **Online Transaction Processing** (**OLTP**) to
    the competitive levels, the significance of normalization for analytic workloads,
    where one needs to perform aggregations, and for situations where relations themselves
    change and are subject to analysis, is less critical. This section will cover
    the extensions of standard SQL language for analysis engines traditionally used
    for big data analytics: Hive and Impala. Both of them are currently Apache licensed
    projects. The following table summarizes the complex types:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关系模型对提高数据库性能有影响，尤其是对于**在线事务处理**（**OLTP**）的竞争力水平，但对于需要执行聚合的查询负载以及关系本身发生变化并受到分析的情况，规范化的重要性较低。本节将介绍用于大数据分析的传统分析引擎的标准SQL语言的扩展：Hive和Impala。它们目前都是Apache许可项目。以下表格总结了复杂类型：
- en: '| Type | Hive support since version | Impala support since version | Comments
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | Hive支持版本 | Impala支持版本 | 备注 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `ARRAY` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | This can be an array of any type, including complex. The
    index is `int` in Hive (`bigint` in Impala) and access is via array notation,
    for example, `element[1]` only in Hive (`array.pos` and `item pseudocolumns` in
    Impala). |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `ARRAY` | 自0.1.0版本起支持，但非常量索引表达式的使用仅限于0.14版本。 | 自2.3.0版本起支持（仅限于Parquet表）。
    | 可以是任何类型的数组，包括复杂类型。在Hive中索引是`int`（在Impala中是`bigint`），通过数组符号访问，例如，`element[1]`仅在Hive中（在Impala中通过`array.pos`和`item`伪列访问）。|'
- en: '| `MAP` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | The key should be of primitive type. Some libraries support
    keys of the string type only. Fields are accessed using array notation, for example,
    `map["key"]` only in Hive (map key and value pseudocolumns in Impala). |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `MAP` | 自0.1.0版本起支持，但非常量索引表达式的使用仅限于0.14版本。 | 自2.3.0版本起支持（仅限于Parquet表）。 |
    键应该是原始类型。一些库只支持字符串类型的键。字段通过数组符号访问，例如，`map["key"]`仅在Hive中（在Impala中通过map键和值伪列访问）。|'
- en: '| `STRUCT` | This is supported since 0.5.0. | This is supported since 2.3.0
    (only for Parquet tables). | Access is using dot notation, for example, `struct.element`.
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `STRUCT` | 自0.5.0版本起支持。 | 自2.3.0版本起支持（仅限于Parquet表）。 | 使用点符号访问，例如，`struct.element`。|'
- en: '| `UNIONTYPE` | This is supported since 0.7.0. | This is not supported in Impala.
    | Support is incomplete: queries that reference `UNIONTYPE` fields in `JOIN` (HIVE-2508),
    `WHERE`, and `GROUP BY` clauses will fail, and Hive does not define the syntax
    to extract the tag or value fields of `UNIONTYPE`. This means that `UNIONTYPEs`
    are effectively look-at-only. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `UNIONTYPE` | 自0.7.0版本起支持。 | 在Impala中不支持。 | 支持不完整：引用`JOIN`（HIVE-2508）、`WHERE`和`GROUP
    BY`子句中的`UNIONTYPE`字段的查询将失败，并且Hive没有定义提取`UNIONTYPE`的标签或值字段的语法。这意味着`UNIONTYPEs`实际上只能查看。
    |'
- en: 'While Hive/Impala tables can be created on top of many underlying file formats
    (Text, Sequence, ORC, Avro, Parquet, and even custom format) and multiple serializations,
    in most practical instances, Hive is used to read lines of text in ASCII files.
    The underlying serialization/deserialization format is `LazySimpleSerDe` (**Serialization**/**Deserialization**
    (**SerDe**)). The format defines several levels of separators, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Hive/Impala表可以建立在多种底层文件格式（文本、序列、ORC、Avro、Parquet以及甚至自定义格式）和多种序列化之上，但在大多数实际情况下，Hive用于读取ASCII文件的文本行。底层的序列化/反序列化格式是`LazySimpleSerDe`（**序列化**/**反序列化**（**SerDe**））。该格式定义了多个分隔符级别，如下所示：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The default for separators are `''\001''` or `^A`, `''\002''` or `^B`, and
    `''\003''` or `^B`. In other words, it''s using the new separator at each level
    of the hierarchy as opposed to the definition/repetition indicator in the Dremel
    encoding. For example, to encode the `LabeledPoint` table that we used before,
    we need to create a file, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符的默认值是`'\001'`或`^A`，`'\002'`或`^B`，以及`'\003'`或`^B`。换句话说，它是在层次结构的每一级使用新的分隔符，而不是Dremel编码中的定义/重复指示符。例如，为了编码我们之前使用的`LabeledPoint`表，我们需要创建一个文件，如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Download Hive from [http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz)
    and perform the follow:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz)下载Hive并执行以下操作：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In Spark, select from a relational table is supported via the `sqlContext.sql`
    method, but unfortunately the Hive union types are not directly supported as of
    Spark 1.6.1; it does support maps and arrays though. The supportability of complex
    objects in other BI and data analysis tools still remains the biggest obstacle
    to their adoption. Supporting everything as a rich data structure in Scala is
    one of the options to converge on nested data representation.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，可以通过`sqlContext.sql`方法从关系型表中选择，但遗憾的是，截至Spark 1.6.1版本，Hive联合类型并不直接支持；尽管如此，它支持map和数组。在其他BI和数据分析工具中复杂对象的可用性仍然是它们被采用的最大障碍。将所有内容作为Scala中的丰富数据结构支持是一种收敛到嵌套数据表示的选项。
- en: Sessionization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 会话化
- en: I will demonstrate the use of the complex or nested structures in the example
    of sessionization. In sessionization, we want to find the behavior of an entity,
    identified by some ID over a period of time. While the original records may come
    in any order, we want to summarize the behavior over time to derive trends.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过会话化的例子来演示复杂或嵌套结构的使用。在会话化中，我们想要找到某个ID在一段时间内的实体行为。虽然原始记录可能以任何顺序到来，但我们想要总结时间上的行为以推导出趋势。
- en: We already analyzed web server logs in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*. We found out how often different
    web pages are accessed over a period of time. We could dice and slice this information,
    but without analyzing the sequence of pages visited, it would be hard to understand
    each individual user interaction with the website. In this chapter, I would like
    to give this analysis more individual flavor by tracking the user navigation throughout
    the website. Sessionization is a common tool for website personalization and advertising,
    IoT tracking, telemetry, and enterprise security, in fact anything to do with
    entity behavior.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](ch01.xhtml "第1章。探索性数据分析") *探索性数据分析* 中分析了Web服务器日志。我们发现了不同网页在一段时间内被访问的频率。我们可以对这个信息进行切块和切片，但没有分析页面访问的顺序，就很难理解每个用户与网站的交互。在这一章中，我想要通过跟踪用户在整个网站中的导航来给这个分析增加更多的个性化特点。会话化是网站个性化、广告、物联网跟踪、遥测和企业安全等与实体行为相关的常见工具。
- en: 'Let''s assume the data comes as tuples of three elements (fields `1`, `5`,
    `11` in the original dataset in [Chapter 1](ch01.xhtml "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据以三个元素的元组形式出现（原始数据集中的字段`1`、`5`、`11`，见[第1章](ch01.xhtml "第1章. 探索性数据分析")，*探索性数据分析*）：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, `id` is a unique entity ID, timestamp is an event `timestamp` (in any
    sortable format: Unix timestamp or an ISO8601 date format), and `path` is some
    indication of the location on the web server page hierarchy.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`id`是一个唯一的实体ID，时间戳是事件的时间戳（任何可排序的格式：Unix时间戳或ISO8601日期格式），而`path`是关于Web服务器页面层级结构的某种指示。
- en: 'For people familiar with SQL, sessionization, or at least a subset of it, is
    better known as a windowing analytics function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉SQL或至少熟悉其子集的人来说，会话化（sessionization）更知名为一个窗口分析函数：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here `ANALYTIC_FUNCTION` is some transformation on the sequence of paths for
    a given `id`. While this approach works for a relatively simple function, such
    as first, last, lag, average, expressing a complex function over a sequence of
    paths is usually very convoluted (for example, nPath from Aster Data ([https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf](https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf))).
    Besides, without additional preprocessing and partitioning, these approaches usually
    result in big data transfers across multiple nodes in a distributed setting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`ANALYTIC_FUNCTION`是对给定`id`的路径序列的某种转换。虽然这种方法适用于相对简单的函数，如first、last、lag、average，但在路径序列上表达复杂函数通常非常复杂（例如，Aster
    Data中的nPath ([https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf](https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf)))）。此外，没有额外的预处理和分区，这些方法通常会导致在分布式设置中跨多个节点的大数据传输。
- en: While in a pure functional approach, one would just have to design a function—or
    a sequence of function applications—to produce the desired answers from the original
    set of tuples, I will create two helper objects that will help us to simplify
    working with the concept of a user session. As an additional benefit, the new
    nested structures can be persisted on a disk to speed up getting answers on additional
    questions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯函数式方法中，人们只需设计一个函数或一系列函数应用来从原始的元组集中生成所需的答案，我将创建两个辅助对象，这将帮助我们简化与用户会话概念的工作。作为额外的优势，新的嵌套结构可以持久化到磁盘上，以加快对额外问题的答案获取。
- en: 'Let''s see how it''s done in Spark/Scala using case classes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在Spark/Scala中使用case类是如何实现的：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The first class will represent a single page view with a timestamp, which, in
    this case, is an ISO8601 `String`, while the second a sequence of page views.
    Could we do it by encoding both members as a `String` with a object separator?
    Absolutely, but representing the fields as members of a class gives us nice access
    semantics, together with offloading some of the work that we need to perform on
    the compiler, which is always nice.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个类将代表一个带有时间戳的单个页面视图，在这个例子中，它是一个ISO8601的`String`，而第二个是一个页面视图的序列。我们能通过将这两个成员编码为一个带有对象分隔符的`String`来实现吗？绝对可以，但将字段表示为类的成员提供了良好的访问语义，同时将一些需要在编译器上执行的工作卸载掉，这总是件好事。
- en: 'Let''s read the previously described log files and construct the objects:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们阅读之前描述的日志文件并构建对象：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Bingo! We have an RDD of Sessions, one per each unique IP address. The IP `189.248.74.238`
    has a session that lasted from `23:09:16` to `23:15:10`, and seemingly ended after
    browsing for men''s shoes. The session for IP `82.166.130.148` contains only one
    hit. The last session concentrated on sports watch and lasted for over three minutes
    from `2015-08-23 22:36:10` to `2015-08-23 22:39:26`. Now, we can easily ask questions
    involving specific navigation path patterns. For example, we want analyze all
    the sessions that resulted in checkout (the path contains `checkout`) and see
    the number of hits and the distribution of times after the last hit on homepage:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Bingo！我们得到了一个包含会话的RDD，每个会话对应一个唯一的IP地址。IP地址`189.248.74.238`有一个从`23:09:16`到`23:15:10`的会话，看起来是在浏览男士鞋子后结束的。IP地址`82.166.130.148`的会话只有一个点击。最后一个会话专注于运动手表，从`2015-08-23
    22:36:10`持续了超过三分钟到`2015-08-23 22:39:26`。现在，我们可以轻松地询问涉及特定导航路径模式的问题。例如，我们想要分析所有导致结账（路径中包含`checkout`）的会话，并查看点击次数以及主页最后点击后的时间分布：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The sessions last from 1 to 121 hits with a mode at 8 hits and from 15 to 2653
    seconds (or about 45 minutes). Why would you be interested in this information?
    Long sessions might indicate that there was a problem somewhere in the middle
    of the session: a long delay or non-responsive call. It does not have to be: the
    person might just have taken a long lunch break or a call to discuss his potential
    purchase, but there might be something of interest here. At least one should agree
    that this is an outlier and needs to be carefully analyzed.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些会话的持续时间从1到121次点击，平均点击次数为8次，从15到2653秒（或大约45分钟）。你为什么会对这个信息感兴趣呢？长时间的会话可能表明会话中间出现了问题：长时间的延迟或无响应的电话。但这并不一定：这个人可能只是吃了个漫长的午餐休息，或者打了个电话讨论他的潜在购买，但这里可能有一些有趣的东西。至少应该同意这一点是一个异常值，需要仔细分析。
- en: Let's talk about persisting this data to the disk. As you've seen, our transformation
    is written as a long pipeline, so there is nothing in the result that one could
    not compute from the raw data. This is a functional approach, the data is immutable.
    Moreover, if there is an error in our processing, let's say I want to change the
    homepage to some other anchor page, I can always modify the function as opposed
    to data. You may be content or not with this fact, but there is absolutely no
    additional piece of information in the result—transformations only increase the
    disorder and entropy. They might make it more palatable for humans, but this is
    only because humans are a very inefficient data-processing apparatus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈将数据持久化到磁盘的问题。正如你所看到的，我们的转换被写成了一个长的管道，所以结果中没有任何东西是不能从原始数据计算出来的。这是一个功能性的方法，数据是不可变的。此外，如果我们的处理过程中出现了错误，比如说我想将主页更改为其他锚定页面，我总是可以修改函数而不是数据。你可能对这个事实感到满意或不满意，但结果中绝对没有额外的信息——转换只会增加无序和熵。它们可能使人类更容易接受，但这仅仅是因为人类是一个非常低效的数据处理装置。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Why rearranging the data makes the analysis faster?**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么重新排列数据会使分析更快？**'
- en: Sessionization seems just a simple rearranging of data—we just put the pages
    that were accessed in sequence together. Yet, in many cases, it makes practical
    data analysis run 10 to 100 times faster. The reason is data locality. The analysis,
    like filtering or path matching, most often tends to happen on the pages in one
    session at a time. Deriving user features requires all page views or interactions
    of the user to be in one place on disk and memory. This often beats other inefficiencies
    such as the overhead of encoding/decoding the nested structures as this can happen
    in local L1/L2 cache as opposed to data transfers from RAM or disk, which are
    much more expensive in modern multithreaded CPUs. This very much depends on the
    complexity of the analysis, of course.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 会话化似乎只是简单的数据重新排列——我们只是将按顺序访问的页面放在一起。然而，在许多情况下，它使实际数据分析的速度提高了10到100倍。原因是数据局部性。分析，如过滤或路径匹配，通常一次只发生在会话中的一个页面上。推导用户特征需要用户的全部页面浏览或交互都在磁盘和内存的一个地方。这通常比其他低效性更好，比如编码/解码嵌套结构的开销，因为这可以在本地L1/L2缓存中发生，而不是从RAM或磁盘进行数据传输，这在现代多线程CPU中要昂贵得多。当然，这很大程度上取决于分析的复杂性。
- en: There is a reason to persist the new data to the disk, and we can do it with
    either CSV, Avro, or Parquet format. The reason is that we do not want to reprocess
    the data if we want to look at them again. The new representation might be more
    compact and more efficient to retrieve and show to my manager. Really, humans
    like side effects and, fortunately, Scala/Spark allows you to do this as was described
    in the previous section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有必要将新数据持久化到磁盘，我们可以使用CSV、Avro或Parquet格式来完成。原因是我们不希望在再次查看数据时重新处理数据。新的表示可能更紧凑，更高效地检索和展示给我的经理。实际上，人类喜欢副作用，幸运的是，Scala/Spark允许你像前一个部分所描述的那样做。
- en: 'Well, well, well...will say the people familiar with sessionization. This is
    only a part of the story. We want to split the path sequence into multiple sessions,
    run path analysis, compute conditional probabilities for page transitions, and
    so on. This is exactly where the functional paradigm shines. Write the following
    function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，嗯，嗯...让我们来说说熟悉会话化的人。这只是故事的一部分。我们想要将路径序列分割成多个会话，运行路径分析，计算页面转换的条件概率等等。这正是函数式范式大放异彩的地方。编写以下函数：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then run the following code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下代码：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Bingo! The result is the session's split. I intentionally left the implementation
    out; it's the implementation that is user-dependent, not the data, and every analyst
    might have it's own way to split the sequence of page visits into sessions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 哈哈！结果是会话的分割。我故意省略了实现部分；实现是用户依赖的，而不是数据，每个分析师可能有自己将页面访问序列分割成会话的方式。
- en: 'Another use case to apply the function is feature generation for applying machine
    learning…well, this is already hinting at the side effect: we want to modify the
    state of the world to make it more personalized and user-friendly. I guess one
    cannot avoid it after all.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个应用函数的用例是用于机器学习的特征生成……这已经暗示了副作用：我们希望修改世界的状态，使其更加个性化和用户友好。我想无论如何也无法避免。
- en: Working with traits
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用特性
- en: As we saw, case classes significantly simplify handling of new nested data structures
    that we want to construct. The case class definition is probably the most convincing
    reason to move from Java (and SQL) to Scala. Now, what about the methods? How
    do we quickly add methods to a class without expensive recompilation? Scala allows
    you to do this transparently with traits!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，案例类显著简化了我们构造新嵌套数据结构的方式。案例类的定义可能是从 Java（和 SQL）迁移到 Scala 的最有说服力的理由。那么，关于方法呢？我们如何快速向类中添加方法而不需要昂贵的重新编译？Scala
    允许您通过特性透明地做到这一点！
- en: A fundamental feature of functional programming is that functions are a first
    class citizen on par with objects. In the previous section, we defined the two
    `EpochSeconds` functions that transform the ISO8601 format to epoch time in seconds.
    We also suggested the `splitSession` function that provides a multi-session view
    for a given IP. How do we associate this or other behavior with a given class?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式编程的一个基本特征是函数与对象处于同等地位，是一等公民。在前一节中，我们定义了两个将 ISO8601 格式转换为秒的纪元时间的 `EpochSeconds`
    函数。我们还建议了 `splitSession` 函数，它为给定的 IP 提供多会话视图。我们如何将这种行为或其它行为与给定的类关联起来？
- en: 'First, let''s define a desired behavior:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个期望的行为：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This basically creates a `PageView`-specific function that converts a string
    representation for datetime to epoch time in seconds. Now, if we just make the
    following transformation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上创建了一个 `PageView` 特定的函数，它将日期时间的字符串表示转换为秒的纪元时间。现在，如果我们只是进行以下转换：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now have a new RDD of page views with additional behavior. For example,
    if we want to find out what is the time spent on each individual page in a session
    is, we will run a pipeline, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含额外行为的页面视图的新 RDD。例如，如果我们想找出会话中每个单独页面花费的时间，我们将运行以下管道：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Multiple traits can be added at the same time without affecting either the original
    class definitions or original data. No recompilation is required.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可以同时添加多个特性，而不会影响原始类定义或原始数据。无需重新编译。
- en: Working with pattern matching
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模式匹配
- en: 'No Scala book would be complete without mentioning the match/case statements.
    Scala has a very rich pattern-matching mechanism. For instance, let''s say we
    want to find all instances of a sequence of page views that start with a homepage
    followed by a products page—we really want to filter out the determined buyers.
    This may be accomplished with a new function, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一本 Scala 书会不提到匹配/案例语句。Scala 有一个非常丰富的模式匹配机制。例如，假设我们想找出所有以主页开始，随后是产品页面的页面视图序列的实例——我们真正想过滤出确定的买家。这可以通过以下新函数实现：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that we explicitly put `PageView` constructors in the case statement!
    Scala will traverse the `visits` sequence and generate new sessions that match
    the specified two `PageViews`, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在案例语句中明确地放置了 `PageView` 构造函数！Scala 将遍历 `visits` 序列，并生成与指定的两个 `PageViews`
    匹配的新会话，如下所示：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: I leave it to the reader to write a function that also filters only those sessions
    where the user spent less than 10 seconds before going to the products page. The
    epoch trait or the previously defined to the `EpochSeconds` function may be useful.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我留给读者编写一个函数，该函数只过滤那些用户在转到产品页面之前花费少于 10 秒的会话。纪元特性或先前定义的 `EpochSeconds` 函数可能很有用。
- en: The match/case function can be also used for feature generation and return a
    vector of features over a session.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配/案例函数也可以用于特征生成，并在会话中返回特征向量。
- en: Other uses of unstructured data
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化数据的其它用途
- en: The personalization and device diagnostic obviously are not the only uses of
    unstructured data. The preceding case is a good example as we started from structured
    record and quickly converged on the need to construct an unstructured data structure
    to simplify the analysis.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化设备和诊断显然不是非结构化数据的唯一用途。前面的例子是一个很好的例子，因为我们从结构化记录开始，迅速转向构建非结构化数据结构以简化分析的需求。
- en: In fact, there are many more unstructured data than there are structured; it
    is just the convenience of having the flat structure for the traditional statistical
    analysis that makes us to present the data as a set of records. Text, images,
    and music are the examples of semi-structured data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，非结构化数据比结构化数据要多得多；只是传统统计分析中具有平坦结构的便利性让我们将数据呈现为记录集。文本、图像和音乐是半结构化数据的例子。
- en: One example of non-structured data is denormalized data. Traditionally the record
    data are normalized mostly for performance reasons as the RDBMSs have been optimized
    to work with structured data. This leads to foreign key and lookup tables, but
    these are very hard to maintain if the dimensions change. Denormalized data does
    not have this problem as the lookup table can be stored with each record—it is
    just an additional table object associated with a row, but may be less storage-efficient.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据的一个例子是非规范化数据。传统上，记录数据主要是为了性能原因而规范化的，因为关系型数据库管理系统已经优化了处理结构化数据。这导致了外键和查找表，但如果维度发生变化，这些表非常难以维护。非规范化数据没有这个问题，因为查找表可以与每个记录一起存储——它只是一个与行相关联的附加表对象，但可能不太节省存储空间。
- en: Probabilistic structures
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率结构
- en: Another use case is the probabilistic structures. Usually people assume that
    answering a question is deterministic. As I showed in [Chapter 2](ch02.xhtml "Chapter 2. Data
    Pipelines and Modeling"), *Data Pipelines and Modeling*, in many cases, the true
    answer has some uncertainty associated with it. One of the most popular ways to
    encode uncertainty is probability, which is a frequentist approach, meaning that
    the simple count of when the answer does happen to be the true answer, divided
    by the total number of attempts—the probability also can encode our beliefs. I
    will touch on probabilistic analysis and models in the following chapters, but
    probabilistic analysis requires storing each possible outcome with some measure
    of probability, which happens to be a nested structure.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用例是概率结构。通常人们认为回答问题具有确定性。正如我在 [第 2 章](ch02.xhtml "第 2 章. 数据管道和建模") *数据管道和建模*
    中所展示的，在许多情况下，真正的答案与一些不确定性相关联。编码不确定性的最流行的方法之一是概率，这是一种频率论方法，意味着当答案确实发生时，简单的计数除以总尝试次数——概率也可以编码我们的信念。我将在接下来的章节中涉及到概率分析和模型，但概率分析需要存储每个可能的输出及其概率度量，这恰好是一个嵌套结构。
- en: Projections
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投影
- en: 'One way to deal with high dimensionality is projections on a lower dimensional
    space. The fundamental basis for why projections might work is Johnson-Lindenstrauss
    lemma. The lemma states that a small set of points in a high-dimensional space
    can be embedded into a space of much lower dimension in such a way that distances
    between the points are nearly preserved. We will touch on random and other projections
    when we talk about NLP in [Chapter 9](ch09.xhtml "Chapter 9. NLP in Scala"), *NLP
    in Scala*, but the random projections work well for nested structures and functional
    programming language, as in many cases, generating a random projection is the
    question of applying a function to a compactly encoded data rather than flattening
    the data explicitly. In other words, the Scala definition for a random projection
    may look like functional paradigm shines. Write the following function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 处理高维度的方法之一是将投影到低维空间。投影可能奏效的基本依据是 Johnson-Lindenstrauss 引理。该引理指出，高维空间中的一小部分点可以嵌入到一个维度远低的空間中，这样点与点之间的距离几乎得到保留。当我们谈到第
    9 章 [NLP in Scala](ch09.xhtml "第 9 章. Scala 中的 NLP") 中的 NLP 时，我们将涉及到随机和其他投影，但随机投影对于嵌套结构和函数式编程语言来说效果很好，因为在许多情况下，生成随机投影的问题在于对一个紧凑编码的数据应用一个函数，而不是显式地展平数据。换句话说，Scala
    中随机投影的定义可能看起来像是函数式范式闪耀。请编写以下函数：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, `Vector` is in low dimensional space.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Vector` 位于低维空间中。
- en: The map used for embedding is at least Lipschitz, and can even be taken to be
    an orthogonal projection.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 用于嵌入的映射至少是 Lipschitz 的，甚至可以被认为是正交投影。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw examples of how to represent and work with complex and
    nested data in Scala. Obviously, it would be hard to cover all the cases as the
    world of unstructured data is much larger than the nice niche of structured row-by-row
    simplification of the real world and is still under construction. Pictures, music,
    and spoken and written language have a lot of nuances that are hard to capture
    in a flat representation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何在Scala中表示和处理复杂和嵌套数据的示例。显然，要涵盖所有情况是困难的，因为非结构化数据的领域比现实世界中结构化数据按行逐行简化的美好领域要大得多，而且仍在建设中。图片、音乐以及口语和书面语言有很多细微差别，这些在平面的表示中很难捕捉到。
- en: While for ultimate data analysis, we eventually convert the datasets to the
    record-oriented flat representation, at least at the time of collection, one needs
    to be careful to store that data as it is and not throw away useful information
    that might be contained in data or metadata. Extending the databases and storage
    with a way to record this useful information is the first step. The next one is
    to use languages that can effectively analyze this information; which is definitely
    Scala.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在最终的数据分析中，我们最终会将数据集转换为面向记录的平面表示，但在收集数据时，至少需要小心地存储数据，不要丢弃可能包含在数据或元数据中的有用信息。通过扩展数据库和存储，以记录这些有用信息的方式是第一步。接下来的一步是使用能够有效分析这些信息的语言；这无疑是Scala。
- en: In the next chapter we'll look at somewhat related topic of working with graphs,
    a specific example of non-structured data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨与处理图相关的话题，这是非结构化数据的一个具体例子。
