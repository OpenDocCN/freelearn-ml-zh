- en: Introduction to Machine Learning with Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explain some basic concepts of **machine learning**
    (**ML**) that will be used in all subsequent chapters. We will start with a brief
    introduction to ML including basic learning workflow, ML rule of thumb, and different
    learning tasks. Then we will gradually cover most important ML tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we will discuss getting started with Scala and Scala-based ML libraries
    for getting a quick start for the next chapter. Finally, we get started with ML
    with Scala and Spark ML by solving a real-life problem. The chapter will briefly
    cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala ML libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with ML with Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll be required to have basic knowledge of Scala and Java. Since Scala is
    also a JVM-based language, make sure both Java JRE and JDK are installed and configured
    on your machine. To be more specific, you'll need Scala 2.11.x and Java 1.8.x
    version installed. Also, you need an IDE, such as Eclipse, IntelliJ IDEA, or Scala
    IDE, with the necessary plugins. However, if you're using IntelliJ IDEA, Scala
    will already be integrated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code files of this chapter can be found on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter01](https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter01)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2V3Id08](http://bit.ly/2V3Id08)'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML approaches are based on a set of statistical and mathematical algorithms
    in order to carry out tasks such as classification, regression analysis, concept
    learning, predictive modeling, clustering, and mining of useful patterns. Using
    ML, we aim to improve the whole learning process automatically such that we may
    not need complete human interactions, or we can at least reduce the level of such
    interactions as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Working principles of a learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tom M. Mitchell explained what learning really means from a computer science
    perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P, if its performance at tasks in T,
    as measured by P, improves with experience E."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this definition, we can conclude that a computer program or machine
    can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn from data and histories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve with experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteratively enhance a model that can be used to predict outcomes of questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the preceding points are at the core of predictive analytics, almost
    every ML algorithm we use can be treated as an optimization problem. This is about
    finding parameters that minimize an objective function, for example, a weighted
    sum of two terms such as a cost function and regularization. Typically, an objective
    function has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: A regularizer, which controls the complexity of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss, which measures the error of the model on the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the regularization parameter defines the trade-off between
    minimizing the training error and the model's complexity, in an effort to avoid
    overfitting problems. Now, if both of these components are convex, then their
    sum is also convex. So, when using an ML algorithm, the goal is to obtain the
    best hyperparameters of a function that return the minimum error when making predictions.
    Therefore, by using a convex optimization technique, we can minimize the function
    until it converges toward the minimum error.
  prefs: []
  type: TYPE_NORMAL
- en: Given that a problem is convex, it is usually easier to analyze the asymptotic
    behavior of the algorithm, which shows how fast it converges as the model observes
    more and more training data. The task of ML is to train a model so that it can
    recognize complex patterns from the given input data and can make decisions in
    an automated way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, inferencing is all about testing the model against new (that is, unobserved)
    data and evaluating the performance of the model itself. However, in the whole
    process and for making the predictive model a successful one, data acts as the
    first-class citizen in all ML tasks. In reality, the data that we feed to our
    machine learning systems must be made up of mathematical objects, such as vectors,
    so that they can consume such data. For example, in the following diagram, raw
    images are embedded into numeric values called feature vectors before feeding
    in to the learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b259b7b-104b-42f6-b03f-e6e686e14938.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the available data and feature types, the performance of your predictive
    model can vacillate dramatically. Therefore, selecting the right features is one
    of the most important steps before the inferencing takes place. This is called
    feature engineering, where the domain knowledge about the data is used to create
    only selective or useful features that help prepare the feature vectors to be
    used so that a machine learning algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: For example, comparing hotels is quite difficult unless we already have a personal
    experience of staying in multiple hotels. However, with the help of an ML model,
    which is already trained with quality features out of thousands of reviews and
    features (for example, how many stars does a hotel have, size of the room, location,
    room service, and so on), it is pretty feasible now. We'll see several examples
    throughout the chapters. However, before developing such an ML model, knowing
    some ML concepts is also important.
  prefs: []
  type: TYPE_NORMAL
- en: General machine learning rule of thumb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The general machine learning rule of thumb is that the more data there is,
    the better the predictive model. However, having more features often creates a
    mess, to the extent that the performance degrades drastically, especially if the
    dataset is high-dimensional. The entire learning process requires input datasets
    that can be split into three types (or are already provided as such):'
  prefs: []
  type: TYPE_NORMAL
- en: A **training set** is the knowledge base coming from historical or live data
    that is used to fit the parameters of the ML algorithm. During the training phase,
    the ML model utilizes the training set to find optimal weights of the network
    and reach the objective function by minimizing the training error. Here, the back-prop
    rule or an optimization algorithm is used to train the model, but all the hyperparameters
    are needed to be set before the learning process starts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **validation set** is a set of examples used to tune the parameters of an
    ML model. It ensures that the model is trained well and generalizes toward avoiding
    overfitting. Some ML practitioners refer to it as a development set or dev set
    as well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **test set** is used for evaluating the performance of the trained model on
    unseen data. This step is also referred to as model inferencing. After assessing
    the final model on the test set (that is, when we're fully satisfied with the
    model's performance), we do not have to tune the model any further, but the trained
    model can be deployed in a production-ready environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common practice is splitting the input data (after necessary pre-processing
    and feature engineering) into 60% for training, 10% for validation, and 20% for
    testing, but it really depends on use cases. Sometimes, we also need to perform
    up-sampling or down-sampling on the data based on the availability and quality
    of the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This rule of thumb of learning on different types of training sets can differ
    across machine learning tasks, as we will cover in the next section. However,
    before that, let's take a quick look at a few common phenomena in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: General issues in machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we use this input data for the training, validation, and testing, usually
    the learning algorithms cannot learn 100% accurately, which involves training,
    validation, and test error (or loss). There are two types of error that one can
    encounter in a machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: Irreducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducible error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The irreducible error cannot be reduced even with the most robust and sophisticated
    model. However, the reducible error, which has two components, called bias and
    variance, can be reduced**.** Therefore, to understand the model (that is, prediction
    errors), we need to focus on bias and variance only:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias means how far the predicted value are from the actual values. Usually,
    if the average predicted values are very different from the actual values (labels),
    then the bias is higher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An ML model will have a high bias because it can't model the relationship between
    input and output variables (can't capture the complexity of data well) and becomes
    very simple. Thus, a too-simple model with high variance causes underfitting of
    the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram gives some high-level insights and also shows what a
    just-right fit model should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/643e8299-1ebf-4072-a764-c87d8d90aab3.png)'
  prefs: []
  type: TYPE_IMG
- en: Variance signifies the variability between the predicted values and the actual
    values (how scattered they are).
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying high bias and high variance**: If the model has a high training
    error as well as the validation error or test error is the same as the training
    error, the model has high bias. On the other hand, if the model has low training
    error but has high validation or high test error, the model has a high variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML model usually performs very well on the training set but doesn''t work
    well on the test set (because of high error rates). Ultimately, it results in
    an underfit model. We can recap the overfitting and underfitting once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting**: If your training and validation error are both relatively
    equal and very high, then your model is most likely underfitting your training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: If your training error is low and your validation error is
    high, then your model is most likely overfitting your training data. The just-rightfit
    model learns very well and performs better on unseen data too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias-variance trade-off**: The high bias and high variance issue is often
    called bias-variance trade-off, because a model cannot be too complex or too simple
    at the same time. Ideally, we would strive for the best model that has both low
    bias and low variance.'
  prefs: []
  type: TYPE_NORMAL
- en: Now we know the basic working principle of an ML algorithm. However, based on
    problem type and the method used to solve a problem, ML tasks can be different,
    for example, supervised learning, unsupervised learning, and reinforcement learning.
    We'll discuss these learning tasks in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ML tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although every ML problem is more or less an optimization problem, the way they
    are solved can vary. In fact, learning tasks can be categorized into three types: supervised
    learning, unsupervised learning, and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning is the simplest and most well-known automatic learning
    task. It is based on a number of predefined examples, in which the category to
    which each of the inputs should belong is already known, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5c163b5-9617-43ef-916f-edd20c1a9bc1.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows a typical workflow of supervised learning. An actor
    (for example, a data scientist or data engineer) performs **Extraction Transformation
    Load** (**ETL**) and the necessary feature engineering (including feature extraction,
    selection, and so on) to get the appropriate data with features and labels so
    that they can be fed in to the model. Then he would split the data into training,
    development, and test sets. The training set is used to train an ML model, the
    validation set is used to validate the training against the overfitting problem
    and regularization, and then the actor would evaluate the model's performance
    on the test set (that is, unseen data).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the performance is not satisfactory, he can perform additional
    tuning to get the best model based on hyperparameter optimization. Finally, he
    would deploy the best model in a production-ready environment. The following diagram
    summarizes these steps in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfa111da-e783-452e-a17f-32ccb53902de.png)'
  prefs: []
  type: TYPE_IMG
- en: In the overall life cycle, there might be many actors involved (for example,
    a data engineer, data scientist, or an ML engineer) to perform each step independently
    or collaboratively. The supervised learning context includes classification and
    regression tasks; classification is used to predict which class a data point is
    a part of (discrete value). It is also used for predicting the label of the class
    attribute. On the other hand, regression is used for predicting continuous values
    and making a numeric prediction of the class attribute.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of supervised learning, the learning process required for the
    input dataset is split randomly into three sets, for example, 60% for the training
    set, 10% for the validation set, and the remaining 30% for the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How would you summarize and group a dataset if the labels were not given? Probably,
    you''ll try to answer this question by finding the underlying structure of a dataset
    and measuring the statistical properties such as frequency distribution, mean,
    standard deviation, and so on. If the question is *how would you effectively represent
    data in a compressed format?* You''ll probably reply saying that you''ll use some
    software for doing the compression, although you might have no idea how that software
    would do it. The following diagram shows the typical workflow of an unsupervised
    learning task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69e15120-3013-419e-ac6d-66248ab3c9a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These are exactly two of the main goals of unsupervised learning, which is
    largely a data-driven process. We call this type of learning *unsupervised* because
    you will have to deal with unlabeled data. The following quote comes from Yann
    LeCun, director of AI research (source: Predictive Learning, NIPS 2016, Yann LeCun,
    Facebook Research):'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Most of human and animal learning is unsupervised learning. If intelligence
    was a cake, unsupervised learning would be the cake, supervised learning would
    be the icing on the cake, and reinforcement learning would be the cherry on the
    cake. We know how to make the icing and the cherry, but we don''t know how to
    make the cake. We need to solve the unsupervised learning problem before we can
    even think of getting to true AI".*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two most widely used unsupervised learning tasks include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: Grouping data points based on similarity (or statistical properties).
    For example, a company such as Airbnb often groups its apartments and houses into
    neighborhoods so that customers can navigate the listed ones more easily.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality** **reduction**: Compressing the data with the structure and
    statistical properties preserved as much as possible. For example, often the number
    of dimensions of the dataset needs to be reduced for the modeling and visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Useful in several applications such as identification
    of credit card fraud detection, identifying faulty pieces of hardware in an industrial engineering
    process, and identifying outliers in large-scale datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Association rule mining**: Often used in market basket analysis, for example,
    asking which items are brought together and frequently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning is an artificial intelligence approach that focuses
    on the learning of the system through its interactions with the environment. In
    reinforcement learning, the system''s parameters are adapted based on the feedback
    obtained from the environment, which in turn provides feedback on the decisions
    made by the system. The following diagram shows a person making decisions in order
    to arrive at their destination. Let''s take an example of the route you take from
    home to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/529a2d39-87ec-450b-8fb1-23e795d276f0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, you take the same route to work every day. However, out of the
    blue, one day you get curious and decide to try a different route with a view
    to finding the shortest path. Similarly, based on your experience and the time
    taken with the different route, you'd decide whether you should take a specific
    route more often. We can take a look at one more example in terms of a system
    modeling a chess player. In order to improve its performance, the system utilizes
    the result of its previous moves; such a system is said to be a system learning
    with reinforcement.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned the basic working principles of ML and different learning
    tasks. However, a summarized view of each learning task with some example use
    cases is a mandate, which we will see in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing learning types with applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen the basic working principles of ML algorithms. Then we have seen
    what the basic ML tasks are and how they formulate domain-specific problems. However,
    each of these learning tasks can be solved using different algorithms. The following
    diagram provides a glimpse into this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69724fda-a4e8-4d3f-b8ac-522d883c906f.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of learning and related problems
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram summarizes the previously mentioned ML tasks and some
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a2cf2df-a148-4609-8eb0-af0f975bdda8.png)'
  prefs: []
  type: TYPE_IMG
- en: ML tasks and some use cases from different application domains
  prefs: []
  type: TYPE_NORMAL
- en: However, the preceding diagram lists only a few use cases and applications using
    different ML tasks. In practice, ML is used in numerous use cases and applications.
    We will try to cover a few of those throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scala is a scalable, functional, and object-oriented programming language that
    is most closely related to Java. However, Scala is designed to be more concise
    and have features of functional programming languages. For example, Apache Spark,
    which is written in Scala, is a fast and general engine for large-scale data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scala''s success is due to many factors: it has many tools that enable succinct
    expression, it is very concise because you need less typing, and it therefore
    requires less reading, and it offers very good performance as well. This is why
    Spark has more support for Scala in the sense that more APIs are available that
    are written in Scala compared to R, Python, and Java. Scala''s symbolic operators
    are easy to read and, compared to Java, most of the Scala codes are comparatively
    concise and easy to read; Java is too verbose. Functional programming concepts
    such as pattern matching and higher-order functions are also available in Scala.'
  prefs: []
  type: TYPE_NORMAL
- en: The best way to get started with Scala is either using Scala through the **Scala
    build tool** (**SBT**) or to use Scala through an **integrated development environment**
    (**IDE**). Either way, the first important step is downloading, installing, and
    configuring Scala. However, since Scala runs on **Java Virtual Machine** (**JVM**),
    having Java installed and configured on your machine is a prerequisite. Therefore,
    I'm not going to cover how to do that. Instead, I will provide some useful links
    ([https://en.wikipedia.org/wiki/Integrated_development_environment](https://en.wikipedia.org/wiki/Integrated_development_environment)).
  prefs: []
  type: TYPE_NORMAL
- en: Just follow the instructions on how to set up both Java and an IDE (for example,
    IntelliJ IDEA) or build tool (for example, SBT) at [https://www.scala-lang.org/download/](https://www.scala-lang.org/download/).
    If you're using Windows (for example, Windows 10) or Linux (for example, Ubuntu),
    visit [https://www.journaldev.com/7456/download-install-scala-linux-unix-windows](https://www.journaldev.com/7456/download-install-scala-linux-unix-windows).
    Finally, here are some macOS instructions: [http://sourabhbajaj.com/mac-setup/Scala/README.html](http://sourabhbajaj.com/mac-setup/Scala/README.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Java programmers normally prefer Scala when they need to add some functional
    programming flavor to their codes as Scala runs on JVM. There are various other
    options when it comes to editors. The following are some options to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: Scala IDE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala plugin for Eclipse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IntelliJ IDEA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emacs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eclipse has several advantages using numerous beta plugins and local, remote,
    and high-level debugging facilities with semantic highlighting and code completion
    for Scala.
  prefs: []
  type: TYPE_NORMAL
- en: ML libraries in Scala
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Scala is a relatively new programming language compared to Java and
    Python, the question will arise as to why we need to consider learning it while
    we have Python and R. Well, Python and R are two leading programming languages
    for rapid prototyping and data analytics including building, exploring, and manipulating
    powerful models.
  prefs: []
  type: TYPE_NORMAL
- en: But Scala is becoming the key language too in the development of functional
    products, which are well suited for big data analytics. Big data applications
    often require stability, flexibility, high speed, scalability, and concurrency.
    All of these requirements can be fulfilled with Scala because Scala is not only
    a general-purpose language but also a powerful choice for data science (for example,
    Spark MLlib/ML). I've been using Scala for the last couple of years and I found
    that more and more Scala ML libraries are in development. Up next, we will discuss
    available and widely used Scala libraries that can be used for developing ML applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interested readers can take a quick look at this, which lists the 15 most popular
    Scala libraries for ML and data science:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.datasciencecentral.com/profiles/blogs/top-15-scala-libraries-for-data-science-in-2018-1](https://www.datasciencecentral.com/profiles/blogs/top-15-scala-libraries-for-data-science-in-2018-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib and ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib is a library that provides user-friendly ML algorithms that are implemented
    using Scala. The same API is then exposed to provide support for other languages
    such as Java, Python, and R. Spark MLlib provides support for local vectors and
    matrix data types stored on a single machine, as well as distributed matrices
    backed by one or multiple **resilient distributed datasets** (**RDDs**).
  prefs: []
  type: TYPE_NORMAL
- en: RDD is the primary data abstraction of Apache Spark, often called Spark Core,
    that represents an immutable, partitioned collection of elements that can be operated
    on in parallel. The resiliency makes RDD fault-tolerant (based on RDD lineage
    graph). RDD can help in distributed computing even when data is stored on multiple
    nodes in a Spark cluster. Also, RDD can be converted into a dataset as a collection
    of partitioned data with primitive values such as tuples or other objects.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML is a new set of ML APIs that allows users to quickly assemble and configure
    practical machine learning pipelines on top of datasets, which makes it easier
    to combine multiple algorithms into a single pipeline. For example, an ML algorithm
    (called estimator) and a set of transformers (for example, a `StringIndexer`,
    a `StandardScalar`, and a `VectorAssembler`) can be chained together to perform
    the ML task as stages without needing to run them sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Interested readers can take a look at the Spark MLlib and ML guide at [https://spark.apache.org/docs/latest/ml-guide.html](https://spark.apache.org/docs/latest/ml-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, I have to inform you of something very useful. Since we will
    be using Spark MLlib and ML APIs in upcoming chapters too. Therefore, it would
    be worth fixing some issues in advance. If you're a Windows user, then let me
    tell you about a very weird issue that you will experience while working with
    Spark. The thing is that Spark works on Windows, macOS, and Linux. While using
    Eclipse or IntelliJ IDEA to develop your Spark applications on Windows, you might
    face an I/O exception error and, consequently, your application might not compile
    successfully or may be interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark needs a runtime environment for Hadoop on Windows too. Unfortunately,
    the binary distribution of Spark (v2.4.0, for example) does not contain Windows-native
    components such as `winutils.exe` or `hadoop.dll`. However, these are required
    (not optional) to run Hadoop on Windows if you cannot ensure the runtime environment,
    an I/O exception saying the following will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two ways to tackle this issue on Windows and from IDEs such as Eclipse
    and IntelliJ IDEA:'
  prefs: []
  type: TYPE_NORMAL
- en: Download `winutls.exe` from [https://github.com/steveloughran/ winutils/tree/
    master/hadoop-2\. 7\. 1/bin/](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download and copy it inside the `bin` folder in the Spark distribution—for example,
    `spark-2.2.0-bin-hadoop2.7/bin/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Project | Run Configurations... | Environment | New | and create a variable
    named `HADOOP_HOME`, then put the path in the Value field. Here is an example: `c:/spark-2.2.0-bin-hadoop2.7/bin/`
    | OK | Apply | Run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ScalNet and DynaML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ScalNet is a wrapper around Deeplearning4J intended to emulate a Keras-like
    API for developing deep learning applications. If you''re already familiar with
    neural network architectures and are coming from a JVM background, it would be
    worth exploring the Scala-based ScalNet library:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub ([https://github.com/deeplear…/deeplearning4j/…/master/scalnet](https://github.com/deeplearning4j/deeplearning4j/tree/master/scalnet?fbclid=IwAR01enpe_dySCpU1aPkMorznm6k31cDmQ49wE52_jAGQzcr-3CZs9NNSVas))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example ([https://github.com/…/sc…/org/deeplearning4j/scalnet/examples](https://github.com/deeplearning4j/ScalNet/tree/master/src/test/scala/org/deeplearning4j/scalnet/examples?fbclid=IwAR2uMjTESm9KHAIZ_mZCHckZhRuZJByhmAbQDoUAn1vCVC1SoE0KmKDmQ9M))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DynaML is a Scala and JVM ML toolbox for research, education, and industry.
    This library provides an interactive, end-to-end, and enterprise-friendly way
    of developing ML applications. If you're interested, see more at [https://transcendent-ai-labs.github.io/DynaML/](https://transcendent-ai-labs.github.io/DynaML/).
  prefs: []
  type: TYPE_NORMAL
- en: ScalaNLP, Vegas, and Breeze
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Breeze is one of the primary scientific computing libraries for Scala, which
    provides a fast and efficient way of data manipulation operations such as matrix
    and vector operations for creating, transposing, filling with numbers, conducting
    element-wise operations, and calculating determinants.
  prefs: []
  type: TYPE_NORMAL
- en: Breeze enables basic operations based on the `netlib-java` library, which enables
    extremely fast algebraic computations. In addition, Breeze provides a way to perform
    signal-processing operations**,** necessary for working with digital signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the GitHub links:'
  prefs: []
  type: TYPE_NORMAL
- en: Breeze ([https://github.com/scalanlp/breeze/](https://github.com/scalanlp/breeze/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breeze examples ([https://github.com/scalanlp/breeze-examples](https://github.com/scalanlp/breeze-examples))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breeze quickstart ([https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, ScalaNLP is a suite of scientific computing, ML, and natural
    language processing, which also acts as an umbrella project for several libraries,
    including Breeze and Epic. Vegas is another Scala library for data visualization,
    which allows plotting specifications such as filtering, transformations, and aggregations.
    Vegas is more functional than the other numerical processing library, Breeze.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information and examples of using Vegas and Breeze, refer to GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: Vegas ([https://github.com/vegas-viz/Vegas](https://github.com/vegas-viz/Vegas))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whereas the visualization library of Breeze is backed by Breeze and JFreeChart,
    Vegas can be considered a missing Matplotlib for Scala and Spark, because it provides
    several options for rendering plots through and within interactive notebook environments,
    such as Jupyter and Zeppelin.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to Zeppelin notebook solutions of each chapter in the GitHub repository
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll see a real-life example of a classification problem.
    The idea is to develop a classifier that, given the values for sex, age, time,
    number of warts, type, and area, will predict whether a patient has to go through
    the cryotherapy.
  prefs: []
  type: TYPE_NORMAL
- en: Description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use a recently added cryotherapy dataset from the UCI machine learning
    repository. The dataset can be downloaded from [http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+#](http://archive.ics.uci.edu/ml/datasets/Cryotherapy+Dataset+#).
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains information about wart treatment results of 90 patients
    using cryotherapy. In case you don't know, a wart is a kind of skin problem caused
    by infection with a type of human papillomavirus. Warts are typically small, rough,
    and hard growths that are similar in color to the rest of the skin.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two available treatments for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Salicylic acid**: A type of gel containing salicylic acid used in medicated
    band-aids.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cryotherapy**: A freezing liquid (usually nitrogen) is sprayed onto the wart.
    It will destroy the cells in the affected area. After the cryotherapy, usually,
    a blister develops, which eventually turns into a scab and falls off after a week
    or so.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are 90 samples or instances that were either recommended to go through
    cryotherapy or be discharged without cryotherapy. There are seven attributes in
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sex`: Patient gender, characterized by `1` (male) or `0` (female).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: Patient age.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`: Observation and treatment time in hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number_of_Warts`: Number of warts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Type`: Types of warts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Area`: The amount of affected area.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Result_of_Treatment`: The recommended result of the treatment, characterized
    by either `1` (yes) or `0` (no). It is also the target column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can understand, it is a classification problem because we will have to
    predict discrete labels. More specifically, it is a binary classification problem.
    Since this is a small dataset with only six features, we can start with a very
    basic classification algorithm called logistic regression, where the logistic
    function is applied to the regression to get the probabilities of it belonging
    in either class. We will learn more details about logistic regression and other
    classification algorithms in [Chapter 3](51712107-c5bc-4d7d-a84a-3039aafc8c0a.xhtml),
    *Scala for Learning Classification*. For this, we use the Spark ML-based implementation
    of logistic regression in Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the programming environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I am assuming that Java is already installed on your machine and `JAVA_HOME`
    is set too. Also, I''m assuming that your IDE has the Maven plugin installed.
    If so, then just create a Maven project and add the project properties as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding `properties` tag, I specified the Spark version (that is,
    `2.3.0`), but you can adjust it. Then add the following dependencies in the `pom.xml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then, if everything goes smoothly, all the JAR files will be downloaded in the
    project home as Maven dependencies. Alright! Then we can start writing the code.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since you're here to learn how to solve a real-life problem in Scala, exploring
    available Scala libraries would be worthwhile. Unfortunately, we don't have many
    options except for the Spark MLlib and ML, which can be used for the regression
    analysis very easily and comfortably. Importantly, it has every regression analysis
    algorithm implemented as high-level interfaces. I assume that Scala, Java, and
    your favorite IDE such as Eclipse or IntelliJ IDEA are already configured on your
    machine. We will introduce some concepts of Spark without providing much detail,
    but we will continue learning in upcoming chapters too.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, I''ll introduce `SparkSession`, which is a unified entry point of a
    Spark application introduced from Spark 2.0\. Technically, `SparkSession` is the
    gateway to interact with some of Spark''s functionality with a few constructs
    such as `SparkContext`, `HiveContext`, and `SQLContext`, which are all encapsulated
    in a `SparkSession`. Previously, you have seen how to create such a session, probably
    without knowing it. Well, a `SparkSession` can be created as a builder pattern
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding builder will try to get an existing `SparkSession` or create a
    new one. Then the newly created `SparkSession` will be assigned as the global
    default.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, when using `spark-shell`, you don't need to create a `SparkSession`
    explicitly, because it's already created and accessible with the `spark` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creating a DataFrame is probably the most important task in every data analytics
    task. Spark provides a `read()` method that can be used to read data from numerous
    sources in various formats such as CSV, JSON, Avro, and JDBC. For example, the
    following code snippet shows how to read a CSV file and create a Spark DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a DataFrame is created, we can see a few samples (that is, rows) by invoking
    the `show()` method, as well as print the schema using the `printSchema()` method.
    Invoking `describe().show()` will show the statistics about the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In many cases, we have to use the `spark.implicits._` package*,* which is one
    of the most useful imports. It is handy, with a lot of implicit methods for converting
    Scala objects to datasets and vice versa. Once we have created a DataFrame, we
    can create a view (temporary or global) for performing SQL using either the `ceateOrReplaceTempView()` method
    or the `createGlobalTempView()` method, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now a SQL query can be issued to see the data in tabular format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To drop these views, `spark.catalog.dropTempView("myTempDataFrame")` or `spark.catalog.dropGlobalTempView("myGloDataFrame")`,
    respectively, can be invoked. By the way, once you're done simply invoking the `spark.stop()`
    method, it will destroy the `SparkSession` and all the resources allocated by
    the Spark application. Interested readers can read detailed API documentation
    at [https://](https://spark.apache.org/)[spark.apache.org/](https://spark.apache.org/)
    to get more information.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the training dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a `Cryotherapy.xlsx` Excel file, which contains data as well as data
    usage agreement texts. So, I just copied the data and saved it in a CSV file named
    `Cryotherapy.csv`. Let''s start by creating `SparkSession`—the gateway to access
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s read the training set and see a glimpse of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look to see if the preceding CSV reader managed to read the data
    properly, including header and types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen from the following screenshot, the schema of the Spark DataFrame has
    been correctly identified. Also, as expected, all the features of my ML algorithms
    are numeric (in other words, in integer or double format):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a21026d-dda3-4de9-b779-610209849628.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A snapshot of the dataset can be seen using the `show()` method. We can limit
    the number of rows; here, let''s say `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding line of code shows the first five samples of the
    DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca5ade58-f80c-4354-b9ad-a630145e1087.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing and feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As per the dataset description on the UCI machine learning repository, there
    are no null values. Also, the Spark ML-based classifiers expect numeric values
    to model them. The good thing is that, as seen in the schema, all the required
    fields are numeric (that is, either integers or floating point values). Also, the
    Spark ML algorithms expect a `label` column, which in our case is `Result_of_Treatment`.
    Let''s rename it to `label` using the Spark-provided `withColumnRenamed()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'All the Spark ML-based classifiers expect training data containing two objects
    called `label` (which we already have) and `features`. We have seen that we have
    six features. However, those features have to be assembled to create a feature
    vector. This can be done using the `VectorAssembler()` method. It is one kind
    of transformer from the Spark ML library. But first we need to select all the
    columns except the `label` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we instantiate a `VectorAssembler()` transformer and transform as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the last line of the preceding code segment shows the assembled
    DataFrame having `label` and `features`, which are needed to train an ML algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/144b0868-c2b9-4a88-b5f4-9f29ed0b43ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Preparing training data and training a classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we separate the training set and test sets. Let''s say that 80% of the
    training set will be used for the training and the other 20% will be used to evaluate
    the trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate a decision tree classifier by specifying impurity, max bins, and
    the max depth of the trees. Additionally, we set the `label` and `feature` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data and the classifier are ready, we can perform the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since it''s a binary classification problem, we need the `BinaryClassificationEvaluator()`
    estimator to evaluate the model''s performance on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the training is completed and we have a trained decision tree model,
    we can evaluate the trained model on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute the classification accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You should experience about 96% classification accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we stop the `SparkSession` by invoking the `stop()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We have managed to achieve about 96% accuracy with minimum effort. However,
    there are other performance metrics such as precision, recall, and F1 measure.
    We will discuss them in upcoming chapters. Also, if you're a newbie to ML and
    haven't understood all the steps in this example, don't worry. We'll recap all
    of these steps in other chapters with various other examples.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned some basic concepts of ML, which is used to
    solve a real-life problem. We started with a brief introduction to ML including
    a basic learning workflow, the ML rule of thumb, and different learning tasks,
    and then we gradually covered important ML tasks such as supervised learning,
    unsupervised learning, and reinforcement learning. Additionally, we discussed
    Scala-based ML libraries. Finally, we have seen how to get started with machine
    learning with Scala and Spark ML by solving a simple classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know basic ML and Scala-based ML libraries, we can start learning
    in a more structured way. In the next chapter, we will learn about regression
    analysis techniques. Then we will develop a predictive analytics application for
    predicting slowness in traffic using linear regression and generalized linear
    regression algorithms.
  prefs: []
  type: TYPE_NORMAL
