<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;4.&#xA0;Generalized Linear Models" id="11C3M1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Generalized Linear Models</h1></div></div></div><p class="calibre8">For regression tasks where the goal is to predict a numerical output, such as a price or temperature, we've seen that linear regression can potentially be a good starting point. It is simple to train and easy to interpret even though, as a model, it makes strict assumptions about the data and the underlying target function. Before studying more advanced techniques <a id="id302" class="calibre1"/>to tackle regression problems, we'll introduce <span class="strong"><strong class="calibre2">logistic regression</strong></span>. Despite its somewhat misleading name, this is actually our first model for performing classification. As we learned in <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, in classification problems, our output is qualitative and thus comprises a finite set of values, which we call classes. We'll begin by thinking about the binary classification scenario, where we are trying to distinguish between two classes, which we'll arbitrarily label as 0 and 1, and later on we'll extend this to distinguishing between multiple classes. Finally, we'll finish up by touching on additional regression methods, Poisson regression and Negative Binomial regression.</p></div>

<div class="book" title="Chapter&#xA0;4.&#xA0;Generalized Linear Models" id="11C3M1-c6198d576bbb4f42b630392bd61137d7">
<div class="book" title="Classifying with linear regression"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec28" class="calibre1"/>Classifying with linear regression</h1></div></div></div><p class="calibre8">Even though <a id="id303" class="calibre1"/>we know classification problems involve qualitative outputs, it seems natural to ask whether we could use our existing knowledge of linear regression and apply it to the classification setting. We could do this by training a linear regression model to predict a value in the interval [0, 1], remembering that we've chosen to label our two classes as 0 and 1. Then, we could apply a threshold to the output of our model in such a way that, if the model outputs a value below 0.5, we would predict class 0; otherwise, we would predict class 1. </p><p class="calibre8">The following graph demonstrates this concept for a simple linear regression with a single input feature X1 and for a binary classification problem.</p><div class="mediaobject"><img src="../images/00063.jpeg" alt="Classifying with linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Our output <a id="id304" class="calibre1"/>variable <span class="strong"><em class="calibre9">y</em></span> is either 0 or 1, so all the data lies on two horizontal lines. The solid line shows the output of the model, and the dashed line shows the decision boundary, which arises when we put a threshold on the model's predicted output at the value 0.5. Points to the left of the dashed line are predicted as belonging to class 0, and points to the right are predicted as belonging to class 1.</p><p class="calibre8">The model is clearly not perfect, but it does seem to correctly classify a large proportion of the data.</p><p class="calibre8">While a good approximation in this case, this approach doesn't feel right for a number of reasons. Firstly, although we know before hand that our output variable is limited to the interval [0, 1] because we have just two classes, the raw output from the linear regression predicts values outside this range. We can see this from the graph for values of input feature <span class="strong"><em class="calibre9">X1</em></span> that are either very low or very high. Secondly, linear regression is designed to solve the problem of minimizing the MSE, which does not seem appropriate for us in this case. Our goal is really to find a way to separate the two classes, not to minimize the mean squared error against a line of best fit. As a consequence of this fact, the location of the decision <a id="id305" class="calibre1"/>boundary is very sensitive to the presence of high leverage points. As we discussed in <a class="calibre1" title="Chapter 2. Tidying Data and Measuring Performance" href="part0019_split_000.html#I3QM1-c6198d576bbb4f42b630392bd61137d7">Chapter 2</a>, <span class="strong"><em class="calibre9">Linear Regression</em></span>, high leverage points are points that lie far away from most of the data because they have extreme values for at least one of their input features.</p><p class="calibre8">The following plot demonstrates the effect of high leverage points on our classifier:</p><div class="mediaobject"><img src="../images/00064.jpeg" alt="Classifying with linear regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the data is exactly the same as before, except that we have added two new observations for class 1 that have relatively high values for feature <span class="strong"><em class="calibre9">X1</em></span> and thus appear far to the right of the graph. Now ideally, because these two newly added observations are well into the area of the graph where we predict class 1, they should not impact our decision boundary so heavily. Due to the fact that we are minimizing the MSE, the old linear regression line (shown as a solid line) has now shifted to the right (shown as a dashed line). Consequently, the point at which our new linear regression line crosses 0.5 on the <span class="strong"><em class="calibre9">y</em></span> axis has moved to the right. Thus, our decision boundary has noticeably moved to the right as a result <a id="id306" class="calibre1"/>of adding only two new points.</p><p class="calibre8">Logistic regression addresses all of these points by providing an output that is bounded by the interval [0,1] and is trained using an entirely different optimization criterion from linear regression, so we are no longer fitting a function by minimizing the MSE, as we'll now see.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Introduction to logistic regression"><div class="book" id="12AK82-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec29" class="calibre1"/>Introduction to logistic regression</h1></div></div></div><p class="calibre8">In logistic <a id="id307" class="calibre1"/>regression, input features are linearly scaled just <a id="id308" class="calibre1"/>as with linear regression; however, the result is then fed as an input to the <span class="strong"><strong class="calibre2">logistic function</strong></span>. This function provides a nonlinear transformation on its input and ensures that the range of the output, which is interpreted as the probability of the input belonging to class 1, lies in the interval [0,1]. The form of the logistic function is as follows:</p><div class="mediaobject"><img src="../images/00065.jpeg" alt="Introduction to logistic regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here is a plot of the logistic function:</p><div class="mediaobject"><img src="../images/00066.jpeg" alt="Introduction to logistic regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When <span class="strong"><em class="calibre9">x = 0</em></span>, the logistic function takes the value 0.5. As <span class="strong"><em class="calibre9">x</em></span> tends to <span class="strong"><em class="calibre9">+∞</em></span>, the exponential in the <a id="id309" class="calibre1"/>denominator vanishes and the function approaches the value 1. As <span class="strong"><em class="calibre9">x</em></span> tends to <span class="strong"><em class="calibre9">-∞</em></span>, the exponential, and hence the denominator, tends to move towards infinity and the function approaches the value 0. Thus, our output is guaranteed to be in the interval [0,1], which is necessary for it to be a probability.</p></div>

<div class="book" title="Introduction to logistic regression">
<div class="book" title="Generalized linear models"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec42" class="calibre1"/>Generalized linear models</h2></div></div></div><p class="calibre8">Logistic <a id="id310" class="calibre1"/>regression belongs to <a id="id311" class="calibre1"/>a class of models known as <span class="strong"><strong class="calibre2">generalized linear models</strong></span> (<span class="strong"><strong class="calibre2">GLMs</strong></span>). Generalized linear models have three unifying characteristics. The first of these is that they all involve a linear combination of the input features, thus explaining part of their name. The second characteristic is that the output is considered to have an underlying probability distribution belonging to the family of exponential distributions. These include the normal distribution, the Poisson, and the binomial distribution. Finally, the mean of the output distribution is related to the linear combination of input <a id="id312" class="calibre1"/>features by way of a function, known as the <span class="strong"><strong class="calibre2">link function</strong></span>. Let's see how this all ties in with logistic regression, which <a id="id313" class="calibre1"/>is just one of many examples of a GLM. We know that we begin with a linear combination of input features, so for example, in the case of one input feature, we can build up an <span class="strong"><em class="calibre9">x</em></span> term as follows:</p><div class="mediaobject"><img src="../images/00067.jpeg" alt="Generalized linear models" class="calibre10"/></div><p class="calibre11"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note15" class="calibre1"/>Note</h3><p class="calibre8">Note that, in the case of logistic regression, we are modeling the probability that the output belongs to class 1, rather than modeling the output directly as we were in linear regression. As a result, we do not need to model the error term because our output, which is a probability, directly incorporates the inherent randomness of our model.</p></div><p class="calibre8">Next, we apply the logistic function to this term in order to produce our model's output:</p><div class="mediaobject"><img src="../images/00068.jpeg" alt="Generalized linear models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the left term tells us directly that we are computing the probability that our output belongs to class 1, based on our evidence of seeing the value of the input feature <span class="strong"><em class="calibre9">X1</em></span>. For logistic regression, the underlying probability distribution of the output is the Bernoulli distribution. This is the same as the binomial distribution with a single trial, and is the distribution we would obtain in an experiment with only two possible outcomes having constant <a id="id314" class="calibre1"/>probability, such as a coin flip. </p><p class="calibre8">The mean of the Bernoulli distribution, <span class="strong"><em class="calibre9">μy</em></span>, is the probability of the (arbitrarily chosen) outcome for success, in this case, class 1. Consequently, the left-hand side in the previous equation is also the mean of our underlying output distribution. For this reason, the function that transforms our linear combination of input features is sometimes known as the <span class="strong"><strong class="calibre2">mean function</strong></span>, and we just saw that this function is the logistic function for logistic regression.</p><p class="calibre8">Now, to determine the link function for logistic regression, we can perform some simple algebraic manipulations in order to isolate our linear combination of input features.</p><div class="mediaobject"><img src="../images/00069.jpeg" alt="Generalized linear models" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The term <a id="id315" class="calibre1"/>on the left-hand side is <a id="id316" class="calibre1"/>known as the <span class="strong"><strong class="calibre2">log-odds</strong></span> or <span class="strong"><strong class="calibre2">logit function</strong></span> and is the link function for <a id="id317" class="calibre1"/>logistic regression. The denominator of the fraction inside the logarithm is the probability of the output being class 0 given the data. Consequently, this fraction <a id="id318" class="calibre1"/>represents the ratio of probability between class 1 and class 0, which is also known as the <span class="strong"><strong class="calibre2">odds ratio</strong></span>:</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip11" class="calibre1"/>Tip</h3><p class="calibre8">A good reference for logistic regression along with examples of other GLMs such as Poisson regression is <span class="strong"><em class="calibre9">Extending the Linear Model with R</em></span>, <span class="strong"><em class="calibre9">Julian J. Faraway</em></span>, <span class="strong"><em class="calibre9">CRC Press</em></span>.</p></div></div></div>

<div class="book" title="Introduction to logistic regression">
<div class="book" title="Interpreting coefficients in logistic regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec43" class="calibre1"/>Interpreting coefficients in logistic regression</h2></div></div></div><p class="calibre8">Looking <a id="id319" class="calibre1"/>at the right-hand side of the last equation, we can see that we have almost exactly the same form that we had for simple linear regression, barring the error term. The fact that we have the logit function on the left-hand side, however, means we cannot interpret our regression coefficients in the same way that we did with linear regression. In logistic regression, a unit increase in feature <span class="strong"><em class="calibre9">Xi</em></span> results in multiplying the odds ratio by an amount,<span class="strong"><img src="../images/00070.jpeg" alt="Interpreting coefficients in logistic regression" class="calibre26"/></span>. When a coefficient <span class="strong"><em class="calibre9">βi</em></span> is positive, then we multiply the odds ratio by a number greater than 1, so we know that increasing the feature <span class="strong"><em class="calibre9">Xi</em></span> will effectively increase the probability of the output being labeled as class 1. </p><p class="calibre8">Similarly, increasing a feature with a negative coefficient shifts the balance toward predicting class 0. Finally, note that when we change the value of an input feature, the effect is a multiplication on the odds ratio and not on the model output itself, which we saw is the probability <a id="id320" class="calibre1"/>of predicting class 1. In absolute terms, the change in the output of our model as a result of a change in the input is not constant throughout, but depends on the current value of our input features. This is, again, different from linear regression, where irrespective of the values of the input features, the regression coefficients always represent a fixed increase in the output per unit increase of an input feature.</p></div></div>

<div class="book" title="Introduction to logistic regression">
<div class="book" title="Assumptions of logistic regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec44" class="calibre1"/>Assumptions of logistic regression</h2></div></div></div><p class="calibre8">Logistic <a id="id321" class="calibre1"/>regression makes fewer assumptions about the input than linear regression. In particular, the nonlinear transformation of the logistic function means that we can model more complex input-output relationships. We still have a linearity assumption, but in this case, it is between the features and the log-odds. We no longer require a normality assumption for residuals and nor do we need the homoscedastic assumption. On the other hand, our error terms still need to be independent. Strictly speaking, the features themselves no longer need to be independent but, in practice, our model will still face issues if the features exhibit a high degree of multicollinearity. Finally, we'll note that, just as with unregularized linear regression, feature scaling does not affect the logistic regression model. This means that centering and scaling a particular input feature will simply result in an adjusted coefficient in the output model, without <a id="id322" class="calibre1"/>any repercussions on the model performance. It turns out that, for logistic regression, this is the result of a property known as the <span class="strong"><strong class="calibre2">invariance property of maximum likelihood</strong></span>. Maximum likelihood is the method used to select the coefficients and will be the focus of the next section. It should be noted, however, that centering and scaling features might still be a good idea if they are on very different scales. This is done to assist the optimization procedure during training. In short, we should turn to feature scaling only if we run into model convergence issues.</p></div></div>

<div class="book" title="Introduction to logistic regression">
<div class="book" title="Maximum likelihood estimation"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch04lvl2sec45" class="calibre1"/>Maximum likelihood estimation</h2></div></div></div><p class="calibre8">When we <a id="id323" class="calibre1"/>studied linear regression, we found our coefficients by minimizing the sum of squared error terms. For logistic <a id="id324" class="calibre1"/>regression, we do this by maximizing the <span class="strong"><strong class="calibre2">likelihood</strong></span> of the data. The likelihood of an observation is the probability of seeing that observation under a particular model.</p><p class="calibre8">In our case, the likelihood of seeing an observation <span class="strong"><em class="calibre9">X</em></span> for class 1 is simply given by the probability <span class="strong"><em class="calibre9">P(Y=1|X)</em></span>, the form of which was given earlier in this chapter. As we only have two classes, the likelihood of seeing an observation for class 0 is given by <span class="strong"><em class="calibre9">1 - P(Y=1|X)</em></span>. The overall likelihood of seeing our entire dataset of observations is the product of all the individual likelihoods for each data point as we consider our observations to be independently obtained. As the likelihood of each observation is parameterized by the regression coefficients <span class="strong"><em class="calibre9">βi</em></span>, the likelihood function for our entire dataset is also, therefore, parameterized by these coefficients. We can express our likelihood function as an equation, as shown in the following:</p><div class="mediaobject"><img src="../images/00071.jpeg" alt="Maximum likelihood estimation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Now, this equation simply computes the probability that a logistic regression model with a particular <a id="id325" class="calibre1"/>set of regression coefficients could have generated our training data. The idea is to choose our regression coefficients so that this likelihood function is maximized. We can see that the form of the likelihood function is a product of two large products from the two big <span class="strong"><em class="calibre9">π</em></span> symbols. The first product contains the likelihood of all our observations for class 1, and the second product contains <a id="id326" class="calibre1"/>the likelihood of all our observations for class 0. We often refer to the <span class="strong"><strong class="calibre2">log likelihood</strong></span> of the data, which is computed by taking the logarithm of the likelihood function. Using the fact that the logarithm of a product of terms is the sum of the logarithm of each term we can write:</p><div class="mediaobject"><img src="../images/00072.jpeg" alt="Maximum likelihood estimation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We can simplify this even further using a classic trick to form just a single sum:</p><div class="mediaobject"><img src="../images/00073.jpeg" alt="Maximum likelihood estimation" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To see why this is true, note that for the observations where the actual value of the output variable <span class="strong"><em class="calibre9">y</em></span> is 1, the right term inside the summation is zero, so we are effectively left with the first sum from the previous equation. Similarly, when the actual value of <span class="strong"><em class="calibre9">y</em></span> is 0, then we are left with the second summation from the previous equation. Understanding the form of the log likelihood is important, and we'll get some practice with this when we start working with R to train a logistic regression model in the next section. Note that maximizing the likelihood is equivalent to maximizing the log likelihood; both approaches will yield the same parameters.</p><p class="calibre8">Maximum likelihood estimation is a fundamental technique of parameter fitting, and we will encounter it in other models in this book. Despite its popularity, it should be noted that maximum likelihood is not a panacea. Alternative training criteria on which to build a model <a id="id327" class="calibre1"/>do exist, and there are some well-known scenarios under which this approach does not lead to a good model, as we shall see in subsequent chapters. Finally, note that the details of the actual optimization procedure, which finds the values of the regression coefficients for maximum likelihood, are beyond the scope of this book and in general, we can rely on R to implement this for us.</p></div></div>
<div class="book" title="Predicting heart disease"><div class="book" id="1394Q2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec30" class="calibre1"/>Predicting heart disease</h1></div></div></div><p class="calibre8">We'll put <a id="id328" class="calibre1"/>logistic regression for the binary classification task to the test with a real-world dataset from the UCI Machine Learning Repository. This time, we will be working with the <span class="strong"><em class="calibre9">Statlog (Heart) dataset</em></span>, which we will refer <a id="id329" class="calibre1"/>to as the <span class="strong"><em class="calibre9">heart dataset</em></span> henceforth for brevity. The dataset can be downloaded from the UCI Machine Repository's website at <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29">http://archive.ics.uci.edu/ml/datasets/Statlog+%28Heart%29</a>. The data contains 270 observations for patients with potential heart problems. Of these, 120 patients were shown to have heart problems, so the split between the two classes is fairly even. The task is to predict whether a patient has a heart disease based on their profile and a series of medical tests. First, we'll load the data into a data frame and rename the columns according to the website:</p><div class="informalexample"><pre class="programlisting">&gt; heart &lt;- read.table("heart.dat", quote = "\"")
&gt; names(heart) &lt;- c("AGE", "SEX", "CHESTPAIN", "RESTBP", "CHOL", "SUGAR", "ECG", "MAXHR", "ANGINA", "DEP", "EXERCISE", "FLUOR", "THAL", "OUTPUT")</pre></div><p class="calibre8">The following table contains the definitions of our input features and the output:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">AGE</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Age (years)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">SEX</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Gender</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CHESTPAIN</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">4-valued chest pain type</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">RESTBP</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Resting blood pressure (beats per minute)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">CHOL</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Serum cholesterol (mg/dl)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">SUGAR</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Is the fasting blood sugar level &gt; 120 mg/dl?</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ECG</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">3-valued resting electrocardiographic results</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">MAXHR</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Maximum heart rate achieved (beats per minute)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">ANGINA</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Was angina induced by exercise?</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">DEP</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">ST depression induced by exercise relative to rest</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">EXERCISE</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Ordered categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Slope of the peak exercise ST segment</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">FLUOR</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">The number of major vessels colored by fluoroscopy</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">THAL</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">3-valued Thal</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">OUTPUT</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Binary</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Presence or absence of a heart disease</p>
</td></tr></tbody></table></div><p class="calibre8">Before we <a id="id330" class="calibre1"/>train a logistic regression model for these data, there are a couple of preprocessing steps that we should perform. A common pitfall when working with numerical data is the failure to notice when a feature is actually a categorical variable and not a numerical variable when the levels are coded as numbers. In the heart dataset, we have four such features. The <code class="email">CHESTPAIN</code>, <code class="email">THAL</code>, and <code class="email">ECG</code> features are all categorical features. The <code class="email">EXERCISE</code> variable, although an ordered categorical variable, is nonetheless a categorical variable, so it will have to be coded as a factor as well:</p><div class="informalexample"><pre class="programlisting">&gt; heart$CHESTPAIN = factor(heart$CHESTPAIN)
&gt; heart$ECG = factor(heart$ECG)
&gt; heart$THAL = factor(heart$THAL)
&gt; heart$EXERCISE = factor(heart$EXERCISE)</pre></div><p class="calibre8">In <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, we saw how we can transform categorical features with many levels into a series of binary valued indicator variables. By doing this, we can use them in a model such as linear or logistic regression, which requires all the inputs to be numerical. As long as the relevant categorical variables in a data frame have been coded as factors, R will automatically apply a coding scheme when performing logistic regression. Concretely, R will treat one of the <span class="strong"><em class="calibre9">k</em></span> factor levels as a reference level and create <span class="strong"><em class="calibre9">k-1</em></span> binary features from the other factor levels. We'll see visual evidence of this when we study the summary output of the logistic regression model that we'll train.</p><p class="calibre8">Next, we should observe that the <code class="email">OUTPUT</code> variable is coded so that class 1 corresponds to the absence of heart disease and class 2 corresponds to the presence of heart disease. As a final change, we'll want to recode the <code class="email">OUTPUT</code> variable so that we will have the familiar class labels of 0 and 1, respectively. This is done by simply subtracting <code class="email">1</code>:</p><div class="informalexample"><pre class="programlisting">&gt; heart$OUTPUT = heart$OUTPUT - 1</pre></div><p class="calibre8">Our data <a id="id331" class="calibre1"/>frame is now ready. Before we train our model, however, we will split our data frame into two parts, for training and testing, exactly as we did for linear regression. Once again, we'll use an 85-15 split:</p><div class="informalexample"><pre class="programlisting">&gt; library(caret)
&gt; set.seed(987954)
&gt; heart_sampling_vector &lt;- 
  createDataPartition(heart$OUTPUT, p = 0.85, list = FALSE)
&gt; heart_train &lt;- heart[heart_sampling_vector,]
&gt; heart_train_labels &lt;- heart$OUTPUT[heart_sampling_vector]
&gt; heart_test &lt;- heart[-heart_sampling_vector,]
&gt; heart_test_labels &lt;- heart$OUTPUT[-heart_sampling_vector]</pre></div><p class="calibre8">We now have 230 observations in our training set and 40 observations in our test set. To train a logistic regression model in R, we use the <code class="email">glm()</code> function, which stands for generalized linear model. This function can be used to train various generalized linear models, but we'll focus on the syntax and usage for logistic regression here. The call is as follows:</p><div class="informalexample"><pre class="programlisting">&gt; heart_model &lt;- 
  glm(OUTPUT ~ ., data = heart_train, family = binomial("logit"))</pre></div><p class="calibre8">Note that the format is very similar to what we saw with linear regression. The first parameter is the model formula, which identifies the output variable and which features we want to use (in this case, all of them). The second parameter is the data frame and the final <code class="email">family</code> parameter is used to specify that we want to perform logistic regression. We can use the <code class="email">summary()</code> function to find out more about the model we just trained, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; summary(heart_model)

Call:
glm(formula = OUTPUT ~ ., family = binomial("logit"), data = heart_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7137  -0.4421  -0.1382   0.3588   2.8118  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -7.946051   3.477686  -2.285 0.022321 *  
AGE         -0.020538   0.029580  -0.694 0.487482    
SEX          1.641327   0.656291   2.501 0.012387 *  
CHESTPAIN2   1.308530   1.000913   1.307 0.191098    
CHESTPAIN3   0.560233   0.865114   0.648 0.517255    
CHESTPAIN4   2.356442   0.820521   2.872 0.004080 ** 
RESTBP       0.026588   0.013357   1.991 0.046529 *  
CHOL         0.008105   0.004790   1.692 0.090593 .  
SUGAR       -1.263606   0.732414  -1.725 0.084480 .  
ECG1         1.352751   3.287293   0.412 0.680699    
ECG2         0.563430   0.461872   1.220 0.222509    
MAXHR       -0.013585   0.012873  -1.055 0.291283    
ANGINA       0.999906   0.525996   1.901 0.057305 .  
DEP          0.196349   0.282891   0.694 0.487632    
EXERCISE2    0.743530   0.560700   1.326 0.184815    
EXERCISE3    0.946718   1.165567   0.812 0.416655    
FLUOR        1.310240   0.308348   4.249 2.15e-05 ***
THAL6        0.304117   0.995464   0.306 0.759983    
THAL7        1.717886   0.510986   3.362 0.000774 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 315.90  on 229  degrees of freedom
Residual deviance: 140.36  on 211  degrees of freedom
AIC: 178.36

Number of Fisher Scoring iterations: 6</pre></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Assessing logistic regression models"><div class="book" id="147LC2-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec31" class="calibre1"/>Assessing logistic regression models</h1></div></div></div><p class="calibre8">The summary <a id="id332" class="calibre1"/>of the logistic regression model produced with the <code class="email">glm()</code> function has a similar format to that of the linear regression model produced with the <code class="email">lm()</code> function. This shows us that, for our categorical variables, we have one fewer binary features than the number of levels in the original variable, so for example, the three-valued <code class="email">THAL</code> input feature produced two binary variables labeled <code class="email">THAL6</code> and <code class="email">THAL7</code>. We'll begin by looking first at the regression coefficients that are predicted <a id="id333" class="calibre1"/>with our model. These are presented with their corresponding <span class="strong"><strong class="calibre2">z-statistic</strong></span>. This is analogous to the t-statistic that we saw in linear regression, and again, the higher the absolute value of the z-statistic, the more likely it is that this particular feature is significantly related to our output variable. The p-values next to the z-statistic express this notion as a probability and are annotated with stars and dots, as they were in linear regression, indicating the smallest confidence interval that includes the corresponding p-value. </p><p class="calibre8">Due to the fact that logistic regression models are trained with the maximum likelihood criterion, we use the standard normal distribution to perform significance tests on our coefficients. For example, to reproduce the p-value for the <code class="email">THAL7</code> feature that corresponds to the listed z-value of 3.362, we can write the following (set the <code class="email">lower.tail</code> parameter to <code class="email">T</code> when testing negative coefficients):</p><div class="informalexample"><pre class="programlisting">&gt; pnorm(3.362 , lower.tail = F) * 2
[1] 0.0007738012</pre></div><div class="informalexample" title="Note"><h3 class="title2"><a id="note16" class="calibre1"/>Note</h3><p class="calibre8">An excellent reference for learning about the essential concepts of distributions in statistics is <span class="strong"><em class="calibre9">All of Statistics</em></span>, <span class="strong"><em class="calibre9">Larry Wasserman</em></span>, <span class="strong"><em class="calibre9">Springer</em></span>.</p></div><p class="calibre8">From the <a id="id334" class="calibre1"/>model summary, we see that <code class="email">FLUOR</code>, <code class="email">CHESTPAIN4</code>, and <code class="email">THAL7</code> are the strongest feature predictors for heart diseases. A number of input features have relatively high p-values. This indicates that they are probably not good indicators of heart disease in the presence of the other features. We'll stress once again the importance of interpreting this table correctly. The table does not say that heart age, for example, is not a good indicator for heart disease; rather, it says that, in the presence of the other input features, age does not really add much to the model. Furthermore, note that we almost definitely have some degree of collinearity in our features as the regression coefficient of age is negative, whereas we would expect that the likelihood of heart disease increases with age. Of course, this assumption is valid only in the absence of all other input features. Indeed, if we retrain a logistic regression model with only the <code class="email">AGE</code> variable, we get a positive regression coefficient as well as a low p-value, both of which support our belief that the features are collinear:</p><div class="informalexample"><pre class="programlisting">&gt; heart_model2 &lt;- glm(OUTPUT ~ AGE, data = heart_train, family = binomial("logit"))
&gt; summary(heart_model2)

Call:
glm(formula = OUTPUT ~ AGE, family = binomial("logit"), data = heart_train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5027  -1.0691  -0.8435   1.2061   1.6759  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -2.71136    0.86348  -3.140  0.00169 **
AGE          0.04539    0.01552   2.925  0.00344 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 315.90  on 229  degrees of freedom
Residual deviance: 306.89  on 228  degrees of freedom
AIC: 310.89

Number of Fisher Scoring iterations: 4</pre></div><p class="calibre8">Note that <a id="id335" class="calibre1"/>the AIC value of this simpler model is higher than what we obtained with the full model, so we would expect this simple model to be worse.</p></div>

<div class="book" title="Assessing logistic regression models">
<div class="book" title="Model deviance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec46" class="calibre1"/>Model deviance</h2></div></div></div><p class="calibre8">To understand <a id="id336" class="calibre1"/>the remainder of the model <a id="id337" class="calibre1"/>summary, we need to introduce an important concept known as <span class="strong"><strong class="calibre2">deviance</strong></span>. In linear regression, our residuals were defined simply as the difference between the predicted value and the actual value of the output that we are trying to predict. Logistic regression is trained using maximum likelihood, so it is natural to expect that an analogous concept to the residual would involve the likelihood. There are several closely-related definitions of the concept of deviance. Here, we will use the definitions that the <code class="email">glm()</code> function uses in order to explain the model's output. The deviance of an observation can be computed as the -2 times the log likelihood of that observation. The deviance of a dataset is just the <a id="id338" class="calibre1"/>sum of all the observation deviances. </p><p class="calibre8">The <span class="strong"><strong class="calibre2">deviance residual</strong></span> of an observation is derived from the deviance itself and is analogous to the residual of a linear regression. It can be computed as follows:</p><div class="mediaobject"><img src="../images/00074.jpeg" alt="Model deviance" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">For an observation <span class="strong"><em class="calibre9">i</em></span>, <span class="strong"><em class="calibre9">dr<sub class="calibre14">i</sub></em></span> represents the deviance residual and <span class="strong"><em class="calibre9">di</em></span> represents the deviance. Note that squaring a deviance residual effectively eliminates the sign function and produces just the deviance of the observation. Consequently, the sum of squared deviance residuals is the deviance of the dataset, which is just the log likelihood of the dataset scaled by the constant -2. Consequently, maximizing the log likelihood of the data is the same as minimizing the sum of the squared deviance residuals, so our analogy with linear regression is complete.</p><p class="calibre8">In order to reproduce the results that are shown in the model summary, and to understand how deviance is computed, we'll write some of our own functions in R. We'll begin by computing the log likelihood for our dataset using the equation for the log likelihood that we saw earlier on in this chapter. From the equation, we'll create two functions. The <code class="email">log_likelihoods()</code> function computes a vector of log likelihoods for all the observations in a dataset, given the probabilities that the model predicts and the actual target labels, and <code class="email">dataset_log_likelihood()</code> sums these up to produce the log likelihood of a dataset:</p><div class="informalexample"><pre class="programlisting"> log_likelihoods &lt;- function(y_labels, y_probs) {
     y_a &lt;- as.numeric(y_labels)
     y_p &lt;- as.numeric(y_probs)
     y_a * log(y_p) + (1 - y_a) * log(1 - y_p)
 }
 
 dataset_log_likelihood &lt;- function(y_labels, y_probs) {
     sum(log_likelihoods(y_labels, y_probs))
 }</pre></div><p class="calibre8">Next, we <a id="id339" class="calibre1"/>can use the definition of deviance to compute two analogous functions: <code class="email">deviances()</code> and <code class="email">dataset_deviance()</code>. The first of these computes a vector of observation deviances, and the second sums these up for the whole dataset:</p><div class="informalexample"><pre class="programlisting"> deviances &lt;- function(y_labels, y_probs) {
     -2 * log_likelihoods(y_labels, y_probs)
 }

dataset_deviance &lt;- function(y_labels, y_probs) {
     sum(deviances(y_labels, y_probs))
 }</pre></div><p class="calibre8">Given these functions, we can now create a function that will compute the deviance of a model. To do this, we need to use the <code class="email">predict()</code> function in order to compute the model's probability predictions for the observations in the training data. This works just as with linear regression, except that by default it returns probabilities on the logit scale. To ensure that we get actual probabilities, we need to specify the value of <code class="email">response</code> for the <code class="email">type</code> parameter:</p><div class="informalexample"><pre class="programlisting">model_deviance &lt;- function(model, data, output_column) {
  y_labels = data[[output_column]]
  y_probs = predict(model, newdata = data, type = "response")
  dataset_deviance(y_labels, y_probs)
}</pre></div><p class="calibre8">To check whether our function is working, let's compute the model deviance, also known as the residual deviance, for our heart model:</p><div class="informalexample"><pre class="programlisting">&gt; model_deviance(heart_model, data = heart_train, output_column = 
                 "OUTPUT")
[1] 140.3561</pre></div><p class="calibre8">Reassuringly, this is the same value as that listed in our model summary. One way to evaluate a logistic regression model is to compute the difference between the model deviance and the deviance of the null model, which is the model trained without any features. The deviance <a id="id340" class="calibre1"/>of the null model is known as <span class="strong"><strong class="calibre2">null deviance</strong></span>. The null model predicts class 1 via a constant probability, as it has no features. This probability is estimated via the proportion of the observations of class 1 in the <a id="id341" class="calibre1"/>training data, which we can obtain by simply averaging the <code class="email">OUTPUT</code> column:</p><div class="informalexample"><pre class="programlisting"> null_deviance &lt;- function(data, output_column) {
     y_labels &lt;- data[[output_column]]
     y_probs &lt;- mean(data[[output_column]])
     dataset_deviance(y_labels, y_probs)
 }

&gt; null_deviance(data = heart_training, output_column = "OUTPUT")
[1] 314.3811</pre></div><p class="calibre8">Once again, we see that we have reproduced the value that R computes for us in the model summary. The residual deviance and null deviance are analogous to the <span class="strong"><strong class="calibre2">Residual Sum of Squares</strong></span> (<span class="strong"><strong class="calibre2">RSS</strong></span>) and the <span class="strong"><strong class="calibre2">True Sum of Squares</strong></span> (<span class="strong"><strong class="calibre2">TSS</strong></span>) that we saw in linear regression. If the <a id="id342" class="calibre1"/>difference between these two is high, the interpretation is <a id="id343" class="calibre1"/>similar to the notion of the residual sum of squares in linear regression <span class="strong"><em class="calibre9">explaining away</em></span> the variance observed by the output variable. </p><p class="calibre8">Continuing <a id="id344" class="calibre1"/>with this analogy, we can define a <span class="strong"><strong class="calibre2">pseudo R2</strong></span> value for our model using the same equation that we used to compute the R2 for linear regression, but substituting in the deviances. We implement this in R as follows:</p><div class="informalexample"><pre class="programlisting"> model_pseudo_r_squared &lt;- function(model, data, output_column) {
     1 - ( model_deviance(model, data, output_column) / 
           null_deviance(data, output_column) )
 }

&gt; model_pseudo_r_squared(heart_model, data = heart_train, 
                         output_column = "OUTPUT")
[1] 0.5556977</pre></div><p class="calibre8">Our logistic regression model is said to explain roughly 56 % of the null deviance. This is not particularly high; most likely, we don't have a rich enough feature set to make accurate predictions with a logistic model. Unlike linear regression, it is possible for the pseudo R2 to exceed 1, but this only happens under problematic circumstances where the residual deviance exceeds the null deviance. If this happens, we should not trust the model and proceed with feature selection methods, or try out alternative models.</p><p class="calibre8">Besides the pseudo R2, we may also want a statistical test to check whether the difference between the null deviance and the residual deviance is significant. The absence of a p-value next to the residual deviance in the model summary indicates that R has not created any test. It turns out that the difference between the residual and null deviances is approximately and asymptotically distributed with a <span class="strong"><em class="calibre9">χ2</em></span> (pronounced <span class="strong"><em class="calibre9">CHI squared</em></span>) distribution. We'll define a function to compute a p-value for this difference, but this is only an approximation.</p><p class="calibre8">First, we need <a id="id345" class="calibre1"/>the difference between the null deviance and the residual deviance. We also need the degrees of freedom for this difference, which are computed simply by subtracting the number of degrees of freedom of our model from those of the null model. The null model only has an intercept, so the number of degrees of freedom is the total number of observations in our dataset minus 1. For the residual deviance, we are computing a number of regression coefficients, including the intercept, so we need to subtract this number from the total number of observations. Finally, we use the <code class="email">pchisq()</code> function to obtain a <code class="email">p-value</code>, noting that we are creating an upper tail computation and hence need to set the <code class="email">lower.tail</code> parameter to <code class="email">FALSE</code>. The code is as follows:</p><div class="informalexample"><pre class="programlisting">model_chi_squared_p_value &lt;-  function(model, data, output_column) {
     null_df &lt;- nrow(data) - 1
     model_df &lt;- nrow(data) - length(model$coefficients)
     difference_df &lt;- null_df - model_df
     null_deviance &lt;- null_deviance(data, output_column)
     m_deviance &lt;- model_deviance(model, data, output_column)
     difference_deviance &lt;- null_deviance - m_deviance
     pchisq(difference_deviance, difference_df,lower.tail = F)
}

&gt; model_chi_squared_p_value(heart_model, data = heart_train, 
                            output_column = "OUTPUT")
[1] 7.294219e-28</pre></div><p class="calibre8">The <code class="email">p-value</code> that we obtain is tiny, so we feel certain that our model produces predictions that are better than average guessing. In our original model summary we also saw a summary of the deviance residuals. Using the definition of a deviance residual that we gave earlier, we'll define a function to compute the vector of deviance residuals:</p><div class="informalexample"><pre class="programlisting">model_deviance_residuals &lt;- function(model, data, output_column) {
     y_labels = data[[output_column]]
     y_probs = predict(model, newdata = data, type = "response")
     residual_sign = sign(y_labels - y_probs)
     residuals = sqrt(deviances(y_labels, y_probs))
     residual_sign * residuals
 }</pre></div><p class="calibre8">Finally, we can use the <code class="email">summary()</code> function on the deviance residuals that we obtain with our <code class="email">model_deviance_residuals()</code> function to obtain a table:</p><div class="informalexample"><pre class="programlisting">&gt; summary(model_deviance_residuals(heart_model, data = 
          heart_train, output_column = "OUTPUT"))
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-2.71400 -0.44210 -0.13820 -0.02765  0.35880  2.81200 </pre></div><p class="calibre8">Once again, we can verify that we obtain the correct result. Our model summary provides us with <a id="id346" class="calibre1"/>one final diagnostic fisher scoring iterations, which we have not yet discussed. This number is typically in the range of 4 to 8 and is a convergence diagnostic. If the optimization procedure that R uses to train the logistic model has not converged, we expect to see a number that is high. If this happens, our model is suspect and we may not be able to use it to make predictions. In our case, we are within the expected range.</p></div></div>

<div class="book" title="Assessing logistic regression models">
<div class="book" title="Test set performance"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec47" class="calibre1"/>Test set performance</h2></div></div></div><p class="calibre8">We've seen <a id="id347" class="calibre1"/>how we can use the <code class="email">predict()</code> function to compute the output of our model. This output is the probability of the input belonging to class 1. We can perform binary classification by applying a threshold. We'll do this with both our training and test data and compare them with our expected outputs to measure the classification accuracy:</p><div class="informalexample"><pre class="programlisting">&gt; train_predictions &lt;- predict(heart_model, newdata = heart_train, 
                               type = "response")
&gt; train_class_predictions &lt;- as.numeric(train_predictions &gt; 0.5)
&gt; mean(train_class_predictions == heart_train$OUTPUT)
[1] 0.8869565
&gt; test_predictions = predict(heart_model, newdata = heart_test, 
                             type = "response")
&gt; test_class_predictions = as.numeric(test_predictions &gt; 0.5)
&gt; mean(test_class_predictions == heart_test$OUTPUT)
[1] 0.9</pre></div><p class="calibre8">The classification accuracies on the training and test sets are very similar and are close to 90 %. This is a very good starting point for a modeler to work from. The coefficients table in our model showed us that several features did not seem to be significant, and we also saw a degree of collinearity that means we could now proceed with variable selection and possibly look for more features, either through computation or by obtaining additional data about our patients. The pseudo R2 computation showed us that we did not explain enough of the deviance in our model, which also supports this.</p></div></div>
<div class="book" title="Regularization with the lasso" id="1565U1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec32" class="calibre1"/>Regularization with the lasso</h1></div></div></div><p class="calibre8">In the <a id="id348" class="calibre1"/>previous chapter on linear regression, we used the <code class="email">glmnet</code> package to perform regularization with ridge regression and the lasso. As we've seen that, it might be a good idea to remove some of our features, we'll try applying lasso to our dataset and assess the results. First, we'll train a series of regularized models with <code class="email">glmnet()</code> and then we will use <code class="email">cv.glmnet()</code> to estimate a suitable value for <span class="strong"><em class="calibre9">λ</em></span>. Then, we'll examine the coefficients of our regularized model using this <span class="strong"><em class="calibre9">λ</em></span>:</p><div class="informalexample"><pre class="programlisting">&gt; library(glmnet)
&gt; heart_train_mat &lt;- model.matrix(OUTPUT ~ ., heart_train)[,-1]
&gt; lambdas &lt;- 10 ^ seq(8, -4, length = 250)
&gt; heart_models_lasso &lt;- glmnet(heart_train_mat, 
  heart_train$OUTPUT, alpha = 1, lambda = lambdas, family = "binomial")
&gt; lasso.cv &lt;- cv.glmnet(heart_train_mat, heart_train$OUTPUT, alpha = 1,lambda = lambdas, family = "binomial")
&gt; lambda_lasso &lt;- lasso.cv$lambda.min
&gt; lambda_lasso
[1] 0.01057052

&gt; predict(heart_models_lasso, type = "coefficients", s = lambda_lasso)
19 x 1 sparse Matrix of class "dgCMatrix"
                       1
(Intercept) -4.980249537
AGE          .          
SEX          1.029146139
CHESTPAIN2   0.122044733
CHESTPAIN3   .          
CHESTPAIN4   1.521164330
RESTBP       0.013456000
CHOL         0.004190012
SUGAR       -0.587616822
ECG1         .          
ECG2         0.338365613
MAXHR       -0.010651758
ANGINA       0.807497991
DEP          0.211899820
EXERCISE2    0.351797531
EXERCISE3    0.081846313
FLUOR        0.947928099
THAL6        0.083440880
THAL7        1.501844677</pre></div><p class="calibre8">We see that a number of our features have effectively been removed from the model because their coefficients are zero. If we now use this model to measure the classification accuracy <a id="id349" class="calibre1"/>on our training and test sets, we observe that in both cases, we get slightly better performance. Even if this difference is small, remember that we have achieved this using three fewer features:</p><div class="informalexample"><pre class="programlisting">&gt; lasso_train_predictions &lt;- predict(heart_models_lasso, s = lambda_lasso, newx = heart_train_mat, type = "response")
&gt; lasso_train_class_predictions &lt;- 
  as.numeric(lasso_train_predictions &gt; 0.5)
&gt; mean(lasso_train_class_predictions == heart_train$OUTPUT)
[1] 0.8913043
&gt; heart_test_mat &lt;- model.matrix(OUTPUT ~ ., heart_test)[,-1]
&gt; lasso_test_predictions &lt;- predict(heart_models_lasso, s = lambda_lasso, newx = heart_test_mat, type = "response")
&gt; lasso_test_class_predictions &lt;- 
  as.numeric(lasso_test_predictions &gt; 0.5)
&gt; mean(lasso_test_class_predictions == heart_test$OUTPUT)
[1] 0.925</pre></div></div>
<div class="book" title="Classification metrics" id="164MG1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec33" class="calibre1"/>Classification metrics</h1></div></div></div><p class="calibre8">Although we <a id="id350" class="calibre1"/>looked at the test set accuracy for our model, we know from <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, that the binary confusion matrix can be used to compute a number of other useful performance metrics for our data, such as precision, recall, and the <span class="strong"><em class="calibre9">F</em></span> measure.</p><p class="calibre8">We'll compute these for our training set now:</p><div class="informalexample"><pre class="programlisting">&gt; (confusion_matrix &lt;- table(predicted = train_class_predictions, actual = heart_train$OUTPUT))
         actual
predicted   0   1
        0 118  16
        1  10  86
&gt; (precision &lt;- confusion_matrix[2, 2] / sum(confusion_matrix[2,]))
[1] 0.8958333
&gt; (recall &lt;- confusion_matrix[2, 2] / sum(confusion_matrix[,2]))
[1] 0.8431373
&gt; (f = 2 * precision * recall / (precision + recall))
[1] 0.8686869</pre></div><p class="calibre8">Here, we used the trick of bracketing our assignment statements to simultaneously assign the result of an expression to a variable and print out the value assigned. Now, recall is the ratio of correctly identified instances of class 1, divided by the total number of observations that actually belong to class 1. In a medical context such as ours, this is also known as <span class="strong"><strong class="calibre2">sensitivity</strong></span>, as it is an effective measure of a model's ability to detect or be sensitive to a particular condition. Recall is also known as the true positive rate. There is an analogous measure <a id="id351" class="calibre1"/>known as <span class="strong"><strong class="calibre2">specificity</strong></span>, which is the false negative rate. This involves the mirror computation of recall for class 0, that is, the correctly identified members <a id="id352" class="calibre1"/>of class 0 over all the observations of class 0 in our dataset. In our medical context, for example, the interpretation of specificity is that it measures the model's ability to reject observations that do not have the condition represented by class 1 (in our case, heart disease). We can compute the specificity of our model as follows:</p><div class="informalexample"><pre class="programlisting">&gt; (specificity &lt;- confusion_matrix[1,1]/sum(confusion_matrix[1,]))
[1] 0.880597</pre></div><p class="calibre8">In computing <a id="id353" class="calibre1"/>these metrics, we begin to see the importance of setting the threshold at <code class="email">0.5</code>. If we were to choose a different threshold, it should be clear that all of the preceding metrics would change. In particular, there are many circumstances, our current medical context being a prime example, in which we may want to adjust our threshold to be biased towards identifying members of class 1. For example, suppose our model was being used by a clinician to determine whether to have a patient undergo a more detailed and expensive examination for heart disease. We would probably consider that mislabeling a patient with a heart condition as healthy is a more serious mistake to make than asking a healthy patient to undergo further tests because they were deemed unhealthy. To achieve this bias, we could lower our classification threshold to <code class="email">0.3</code> or <code class="email">0.2</code>, for example.</p><p class="calibre8">Ideally, what we would like is a visual way to assess the effect of changing the threshold on our performance metrics, and the precision recall curve is one such useful plot. In R, we can use the <code class="email">ROCR</code> package to obtain precision-recall curves:</p><div class="informalexample"><pre class="programlisting">&gt; library(ROCR)
&gt; train_predictions &lt;- predict(heart_model, newdata = heart_train, type = "response")
&gt; pred &lt;- prediction(train_predictions, heart_training$OUTPUT)
&gt; perf &lt;- performance(pred, measure = "prec", x.measure = "rec")</pre></div><p class="calibre8">We can then plot the <code class="email">perf</code> object to obtain our precision recall curve.</p><div class="mediaobject"><img src="../images/00075.jpeg" alt="Classification metrics" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The graph <a id="id354" class="calibre1"/>shows us, for example, that, to obtain values of recall above 0.8, we'll have to sacrifice precision quite abruptly. To fine-tune our threshold, we'll want to see the individual thresholds that were used to compute this graph. A useful exercise is to create a data frame of cutoff values, which are the threshold values for which precision and recall change in our data, along with their corresponding precision and recall values. We can then subset this data frame to inspect individual thresholds that interest us.</p><p class="calibre8">For example, suppose we want to find a suitable threshold so that we have at least 90 % recall and 80 % precision. We can do this as follows:</p><div class="informalexample"><pre class="programlisting">&gt; thresholds &lt;- data.frame(cutoffs = perf@alpha.values[[1]], recall = perf@x.values[[1]], precision = perf@y.values[[1]])
&gt; subset(thresholds,(recall &gt; 0.9) &amp; (precision &gt; 0.8))
      cutoffs    recall precision
112 0.3491857 0.9019608 0.8288288
113 0.3472740 0.9019608 0.8214286
114 0.3428354 0.9019608 0.8141593
115 0.3421438 0.9019608 0.8070175</pre></div><p class="calibre8">As we <a id="id355" class="calibre1"/>can see, a threshold of roughly 0.35 will satisfy our requirements.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip12" class="calibre1"/>Tip</h3><p class="calibre8">You may have noticed that we used the <code class="email">@</code> symbol to access some of the attributes of the <code class="email">perf</code> object. This is because this object is a special type of object known as an S4 class. S4 classes are used to provide object-oriented features in R. A good reference to learn about S4 classes and object-orientated programming in R more generally is <span class="strong"><em class="calibre9">Advanced R</em></span>, <span class="strong"><em class="calibre9">Hadley Wickham</em></span>, <span class="strong"><em class="calibre9">Chapman and Hall</em></span>.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Extensions of the binary logistic classifier"><div class="book" id="173722-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec34" class="calibre1"/>Extensions of the binary logistic classifier</h1></div></div></div><p class="calibre8">So far, the <a id="id356" class="calibre1"/>focus of this chapter has been on the binary classification task where we have two classes. We'll now turn to the problem of multiclass prediction. In <a class="calibre1" title="Chapter 1. Gearing Up for Predictive Modeling" href="part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7">Chapter 1</a>, <span class="strong"><em class="calibre9">Gearing Up for Predictive Modeling</em></span>, we studied the iris dataset where the goal is to distinguish between three different species of iris, based on features that describe the external appearance of iris flower samples. Before presenting additional examples of multiclass problems, we'll state an important caveat. The caveat is that several other methods for classification that we will study in this book, such as neural networks and decision trees, are both more natural and more commonly used than logistic regression for classification problems involving more than two classes. With that in mind, we'll turn to multinomial logistic regression, our first extension of the binary logistic classifier.</p></div>

<div class="book" title="Extensions of the binary logistic classifier">
<div class="book" title="Multinomial logistic regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec48" class="calibre1"/>Multinomial logistic regression</h2></div></div></div><p class="calibre8">Suppose our <a id="id357" class="calibre1"/>target variable <a id="id358" class="calibre1"/>comprises <span class="strong"><em class="calibre9">K</em></span> classes. For example, in the iris dataset, <span class="strong"><em class="calibre9">K = 3</em></span>. <span class="strong"><strong class="calibre2">Multinomial logistic regression</strong></span> tackles the multiclass problem by fitting <span class="strong"><em class="calibre9">K-1</em></span> independent binary logistic classifier models. This is done by arbitrarily choosing one of the output classes as a reference class and fitting <span class="strong"><em class="calibre9">K-1</em></span> regression models that compare each of the remaining classes to this one. For example, if we have two features, <span class="strong"><em class="calibre9">X1</em></span> and <span class="strong"><em class="calibre9">X2</em></span>, and three classes, which we could call 0, 1, and 2, we construct the following two models:</p><div class="mediaobject"><img src="../images/00076.jpeg" alt="Multinomial logistic regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here we used class 0 as the baseline and built two binary regression models. In the first, we compared class 1 against class 0 and in the second, we compare class 2 against class 0. Note that <a id="id359" class="calibre1"/>because we now have more than one binary regression model, our model coefficients have two subscripts. The first <a id="id360" class="calibre1"/>subscript identifies the model and the second subscript pairs the coefficient with a feature. For example, <span class="strong"><em class="calibre9">β<sub class="calibre14">12</sub></em></span> is the coefficient of feature <span class="strong"><em class="calibre9">X<sub class="calibre14">2</sub></em></span> in the first model. We can write a general expression for the probability that our <a id="id361" class="calibre1"/>combined model predicts class <span class="strong"><em class="calibre9">k</em></span> when there are <span class="strong"><em class="calibre9">K</em></span> classes in total, numbered from <span class="strong"><em class="calibre9">0</em></span> to <span class="strong"><em class="calibre9">K-1</em></span>, and class 0 is chosen as the reference class:</p><div class="mediaobject"><img src="../images/00077.jpeg" alt="Multinomial logistic regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The <a id="id362" class="calibre1"/>reader should verify that the sum of all the output class probabilities is 1, as required. This particular mathematical form of an exponential divided by a sum of exponentials is known as the <span class="strong"><strong class="calibre2">softmax</strong></span> function. For our three-class problem discussed previously, we simply substitute <span class="strong"><em class="calibre9">K=3</em></span> in the preceding equations. At this point, we should mention some important characteristics of this approach.</p><p class="calibre8">To begin with, we are training one fewer models than the total number of classes in our output variable, and as a result, it should be easy to see that this approach does not scale very well when we have a large number of output classes from which to choose. The fact that we are building and training so many models also means that we tend to need a much larger dataset to produce results with reasonable accuracy. Finally, as we independently compare each output class to a reference class, we make an assumption, known as the <span class="strong"><strong class="calibre2">Independence of Irrelevant Alternatives</strong></span> (<span class="strong"><strong class="calibre2">IIA</strong></span>) assumption.</p><p class="calibre8">The IIA <a id="id363" class="calibre1"/>assumption, in a nutshell, states that the odds of predicting one particular output class over another do not depend on whether we increase the number of possible output classes <span class="strong"><em class="calibre9">k</em></span> by adding new classes. To illustrate this, suppose for simplicity that we model our iris dataset using multinomial logistic regression, and the odds of the output classes are 0.33 : 0.33 : 0.33 for the three different species so that every species is in a 1 : 1 ratio with every other species. The IIA assumption states that if we refit a model that includes samples of a new type of iris, for example, ensata (the Japanese iris), the odds ratio between the previous three iris species is maintained. A new overall odds ratio of 0.2 : 0.2 : 0.2 : 0.4 between the four species (where the 0.4 corresponds to the ensata) would be valid, for example, because the 1 : 1 ratios between the old three species are maintained.</p><div class="book" title="Predicting glass type"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch04lvl3sec08" class="calibre1"/>Predicting glass type</h3></div></div></div><p class="calibre8">In this <a id="id364" class="calibre1"/>section, we'll demonstrate how we can train multinomial logistic regression models in R by way of an example dataset. The data we'll examine is from the field of forensic science. Here, our goal is to examine properties of glass fragments found in crime scenes and predict the source of these fragments, for example, headlamps. The <span class="strong"><em class="calibre9">glass identification dataset</em></span> is hosted <a id="id365" class="calibre1"/>by the UCI Machine Learning Repository at <a class="calibre1" href="http://archive.ics.uci.edu/ml/datasets/Glass+Identification">http://archive.ics.uci.edu/ml/datasets/Glass+Identification</a>. We'll first load the data in a data frame, rename the columns using information from the website, and throw away the first column (a unique identifier for each sample), as this has been arbitrarily assigned and is not needed by our model:</p><div class="informalexample"><pre class="programlisting">&gt; glass &lt;- read.csv("glass.data", header = FALSE)
&gt; names(glass) &lt;- c("id","RI","Na", "Mg", "Al", "Si", "K", "Ca", 
                    "Ba", "Fe", "Type")
&gt; glass &lt;- glass[,-1]</pre></div><p class="calibre8">Next, we'll look at a table showing what each column in our data frame represents:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">RI</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Refractive index</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Na</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Sodium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Mg</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Magnesium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Al</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Aluminium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Si</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Silicon Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">K</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Potassium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Ca</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Calcium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Ba</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Barium Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Fe</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Percentage of Iron Oxide by weight</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">Type</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Type of glass (1: float processed building windows, 2: nonfloat processed building windows, 3: float processed vehicle windows, 4: nonfloat processed vehicle windows, 5: containers, 6: tableware, 7: headlamps)</p>
</td></tr></tbody></table></div><p class="calibre8">As usual, we'll proceed by preparing a training and test set for our glass data:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(4365677)
&gt; glass_sampling_vector 
   &lt;- createDataPartition(glass$Type, p = 0.80, list = FALSE)
&gt; glass_train &lt;- glass[glass_sampling_vector,]
&gt; glass_test &lt;- glass[-glass_sampling_vector,]</pre></div><p class="calibre8">Now, to perform multinomial logistic regression, we will use the <code class="email">nnet</code> package. This package <a id="id366" class="calibre1"/>also contains functions that work with neural networks, so we will revisit this package in the next chapter as well. The <code class="email">multinom()</code>function is used for multinomial logistic regression. This works by specifying a formula and a data frame, so it has a familiar interface. In addition, we can also specify the <code class="email">maxit</code> parameter that determines the maximum number of iterations for which the underlying optimization procedure will run. Sometimes, we may find that training a model returns an error to the effect that convergence was not reached. In this case, one possible approach is to increase this parameter and allow the model to train over a larger number of iterations. In doing so, however, we should be aware of the fact that the model may take longer to train:</p><div class="informalexample"><pre class="programlisting">&gt; library(nnet)
&gt; glass_model &lt;- multinom(Type ~ ., data = glass_train, maxit = 1000)
&gt; summary(glass_model)
Call:
multinom(formula = Type ~ ., data = glass_train, maxit = 1000)

Coefficients:
  (Intercept)         RI         Na         Mg          Al
2   52.259841  229.29126 -3.3704788  -5.975435  0.07372541
3  596.591193 -237.75997 -1.2230210  -2.435149 -0.65752347
5   -1.107583  -22.94764 -0.7434635  -4.244450  8.39355868
6   -7.493074  -11.83462 11.7893062  -6.383788 35.54561277
7  -55.888124  442.23590 -2.5269178 -10.479849  1.35983136
          Si            K         Ca          Ba           Fe
2 -4.0428142   -3.4934439 -4.6096363   -6.319183    3.2295218
3 -2.6703131   -4.1221815 -1.7952780   -3.910554    0.2818498
5  0.6992306   -0.2149109 -0.8790202   -4.642283    4.3379314
6 -2.2672275 -138.1047925  0.9011624 -161.700857 -200.9598019
7 -6.5363409   -7.5444163 -8.5710078   -4.087614  -67.9907347



Std. Errors:
  (Intercept)         RI         Na        Mg       Al        Si
2  0.03462075 0.08068713  0.5475710 0.7429120 1.282725 0.1392131
3  0.05425817 0.08750688  0.7339134 0.9173184 1.544409 0.1805758
5  0.06674926 0.11759231  1.0866157 1.4062285 2.738635 0.3225212
6  0.17049665 0.28791033 17.2280091 4.9726046 2.622643 4.3385330
7  0.06432732 0.10522206  2.2561142 1.5246356 3.244288 0.4733835
           K        Ca           Ba         Fe
2 1.98021049 0.4897356 1.473156e+00 2.45881312
3 2.35233054 0.5949799 4.222783e+00 3.45835575
5 2.78360034 0.9807043 5.471887e+00 5.52299959
6 0.02227295 7.2406622 1.656563e-08 0.01779519
7 3.25038195 1.7310334 4.381655e+00 0.28562065

Residual Deviance: 219.2651 
AIC: 319.2651</pre></div><p class="calibre8">Our model <a id="id367" class="calibre1"/>summary shows us that we have five sets of coefficients. This is because our <code class="email">TYPE</code> output variable has six levels, which is to say that we are choosing to predict one of six different sources of glass. There are no examples in the data where <code class="email">Type</code> takes the value 4. The model also shows us standard errors, but no significance tests. In general, testing for coefficient significance is a lot trickier than with binary logistic regression, and this is one of the weaknesses of this approach. Often, we resort to independently testing the significance of coefficients for each of the binary models that we trained.</p><p class="calibre8">We won't dwell on this any further, but will instead check the overall accuracy on our training data to give us a sense of the overall quality of fit:</p><div class="informalexample"><pre class="programlisting">&gt; glass_predictions &lt;- predict(glass_model, glass_train)
&gt; mean(glass_predictions == glass_train$Type)
[1] 0.7209302</pre></div><p class="calibre8">Our training accuracy is 72 %, which is not especially high. Here is the confusion matrix:</p><div class="informalexample"><pre class="programlisting">&gt; table(predicted = glass_predictions, actual = glass_train$Type)
         actual
predicted  1  2  3  5  6  7
        1 46 17  8  0  0  0
        2 13 40  6  2  0  1
        3  0  0  0  0  0  0
        5  0  1  0  7  0  0
        6  0  0  0  0  7  0
        7  0  0  0  0  0 24</pre></div><p class="calibre8">The confusion <a id="id368" class="calibre1"/>matrix reveals certain interesting facts. The first of these is that it seems that the model does not distinguish well between the first two classes, as many of the errors that are made involve these two. Part of the reason for this, however, is that these two classes are the most frequent in the data. The second problem that we are seeing is that the model never predicts class 3. In fact, it completely confuses this class with the first two classes. The seven examples of class 6 are perfectly distinguished, and accuracy for class 7 is also near perfect, with only 1 mistake out of 25. Overall, 72 % accuracy on training data is considered mediocre, but given the fact that we have six output classes and only 172 observations in our training data, this is to be expected with this type of model. Let's repeat this for the test dataset:</p><div class="informalexample"><pre class="programlisting">&gt; glass_test_predictions &lt;- predict(glass_model, glass_test)
&gt; mean(glass_test_predictions == glass_test$Type)
[1] 0.6428571
&gt; table(predicted = glass_test_predictions, actual = 
        glass_test$Type)
         actual
predicted  1  2  3  5  6  7
        1  7  2  2  0  0  0
        2  4 15  1  2  0  0
        3  0  0  0  0  0  0
        5  0  0  0  1  0  2
        6  0  0  0  0  2  0
        7  0  1  0  1  0  2</pre></div><p class="calibre8">As we can see, the confusion matrix paints a fairly similar picture to what we saw in training. Again, our model never predicts class 3 and the first two classes are still hard to distinguish. The number of observations in our test set is only 42, so this is very small. The test set accuracy is only 64 %, somewhat less than we saw in training. If our sample sizes were larger, we might suspect that our model suffers from overfitting, but in this case the variance of our test set performance is high due to the small sample size.</p><p class="calibre8">With multinomial logistic regression, we assumed that there was no natural ordering to the output <a id="id369" class="calibre1"/>classes. If our output <a id="id370" class="calibre1"/>variable is an ordinal, also known as an <span class="strong"><strong class="calibre2">ordered factor</strong></span>, we can train <a id="id371" class="calibre1"/>a different model known as <span class="strong"><strong class="calibre2">ordinal logistic regression</strong></span>. This is our second extension of the binary logistic regression model and is presented in the next section.</p></div></div></div>

<div class="book" title="Extensions of the binary logistic classifier">
<div class="book" title="Ordinal logistic regression"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec49" class="calibre1"/>Ordinal logistic regression</h2></div></div></div><p class="calibre8">Ordered <a id="id372" class="calibre1"/>factors are very common in a number of scenarios. For example, human responses to surveys are often on subjective scales with scores ranging from 1 to 5 or using qualitative labels with an intrinsic ordering such as <span class="strong"><em class="calibre9">disagree</em></span>, <span class="strong"><em class="calibre9">neutral</em></span>, and <span class="strong"><em class="calibre9">agree</em></span>. We can try to treat these problems as regression problems, but we will still face similar issues to those we experienced when treating the binary classification problem as a regression problem. Instead of trying to train <span class="strong"><em class="calibre9">K-1</em></span> binary logistic regression models as multinomial logistic regression, ordinal logistic regression trains a single model with multiple thresholds on the output. In order to achieve this, it makes an important assumption known <a id="id373" class="calibre1"/>as the assumption of <span class="strong"><strong class="calibre2">proportional odds</strong></span>. If we have <span class="strong"><em class="calibre9">K</em></span> classes and want to put a threshold on the output of a single binary logistic regression model, we will need <span class="strong"><em class="calibre9">K-1</em></span> thresholds or cutoff points. The proportional odds assumption is that, in the logit scale, all of these thresholds lie on a straight line. Put differently, the model uses a single set of <span class="strong"><em class="calibre9">βi</em></span> coefficients determining the slope of the straight line, but there are <span class="strong"><em class="calibre9">K-1</em></span> intercept terms. For a model with <span class="strong"><em class="calibre9">p</em></span> features and an output variable with <span class="strong"><em class="calibre9">K</em></span> classes numbered from 0 to <span class="strong"><em class="calibre9">K-1</em></span>, our model predicts:</p><div class="mediaobject"><img src="../images/00078.jpeg" alt="Ordinal logistic regression" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This assumption may be a little hard to visualize and is perhaps best understood by way of an example. Suppose that we are trying to predict the results of a survey on of public opinion about a particular government policy, based on demographic data about survey participants.</p><p class="calibre8">The output <a id="id374" class="calibre1"/>variable is an ordered factor that ranges from <span class="strong"><em class="calibre9">strongly disagree</em></span> to <span class="strong"><em class="calibre9">strongly agree</em></span> on a five-point scale (also known as a <span class="strong"><strong class="calibre2">Likert</strong></span> scale). Suppose that <span class="strong"><em class="calibre9">l<sub class="calibre14">0</sub></em></span> is the log-odds of the probabilities of strongly disagreeing versus disagreeing or better, <span class="strong"><em class="calibre9">l<sub class="calibre14">1</sub></em></span> is the log-odds of the probabilities of disagreeing or strongly disagreeing versus at least being neutral, and so on until <span class="strong"><em class="calibre9">l<sub class="calibre14">3</sub></em></span>. These four log-odds <span class="strong"><em class="calibre9">l<sub class="calibre14">0</sub></em></span> to <span class="strong"><em class="calibre9">l<sub class="calibre14">3</sub></em></span> form an arithmetic sequence, which means that the distance between consecutive numbers is a constant.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note17" class="calibre1"/>Note</h3><p class="calibre8">Even though the proportional odds model is the most frequently cited logistic regression model that handles ordered factors, there are alternative approaches. A good reference that discusses the proportional odds model as well as other related models, such as the adjacent-category logistic model, is <span class="strong"><em class="calibre9">Applied Logistic Regression Third Edition</em></span>, <span class="strong"><em class="calibre9">Hosmer Jr.</em></span>, <span class="strong"><em class="calibre9">Lemeshow</em></span>, and <span class="strong"><em class="calibre9">Sturdivant</em></span>, published by <span class="strong"><em class="calibre9">Wiley</em></span>.</p></div><div class="book" title="Predicting wine quality"><div class="book"><div class="book"><div class="book"><h3 class="title2"><a id="ch04lvl3sec09" class="calibre1"/>Predicting wine quality</h3></div></div></div><p class="calibre8">The dataset <a id="id375" class="calibre1"/>for our ordinal logistic regression example is the <span class="strong"><em class="calibre9">wine quality dataset</em></span> from the <span class="strong"><em class="calibre9">UCI Machine Learning Repository</em></span>. The observations in this dataset consist of wine samples taken from both red and white wines of the Portuguese Vinho Verde variety. The wine samples have been rated on a scale from 1 to 10 by a number of wine experts. The goal of the dataset is to predict <a id="id376" class="calibre1"/>the rating that an expert will give to a wine sample, using a range of physiochemical properties, such as acidity and alcohol composition. The website is <a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>. The data is split into two files, one for red wines and one for white wines. We will use the white wine dataset, as it contains a larger number of samples. In addition, for simplicity and because the distribution of wine samples by score is sparse, we will contract our original output variable to a three point scale from 0 to 2. First, let's load and process our data:</p><div class="informalexample"><pre class="programlisting">&gt; wine &lt;- read.csv("winequality-white.csv", sep = ";")
&gt; wine$quality &lt;- factor(ifelse(wine$quality &lt; 5, 0,                     
                         ifelse(wine$quality &gt; 6, 2, 1)))</pre></div><p class="calibre8">The following table shows our input features and output variables:</p><div class="informalexample"><table border="1" class="calibre17"><colgroup class="calibre18"><col class="calibre19"/><col class="calibre19"/><col class="calibre19"/></colgroup><thead class="calibre20"><tr class="calibre21"><th valign="bottom" class="calibre22">
<p class="calibre23">Column name</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Type</p>
</th><th valign="bottom" class="calibre22">
<p class="calibre23">Definition</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">fixed.acidity</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Fixed Acidity (g(tartaric acid)/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">volatile.acidity</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Volatile acidity (g(acetic acid)/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">citric.acid</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Citric acid (g/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">residual.sugar</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Residual sugar (g/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">chlorides</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Chlorides (g(sodium chloride)/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">free.sulfur.dioxide</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Free Sulfur Dioxide (mg/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">total.sulfur.dioxide</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Total Sulfur Dioxide (mg/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">density</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Density (g/cm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">pH</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">PH</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">sulphates</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Sulphates (g(potassium sulphate)/dm3)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">alcohol</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Numerical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Alcohol (% vol.)</p>
</td></tr><tr class="calibre21"><td valign="top" class="calibre25">
<p class="calibre23">
<code class="literal">quality</code>
</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Categorical</p>
</td><td valign="top" class="calibre25">
<p class="calibre23">Wine quality (1 = Poor, 2 = Average, 3 = Good)</p>
</td></tr></tbody></table></div><p class="calibre8">First, we'll prepare a training and test set:</p><div class="informalexample"><pre class="programlisting">&gt; set.seed(7644)
&gt; wine_sampling_vector &lt;- createDataPartition(wine$quality, p = 
                          0.80, list = FALSE)
&gt; wine_train &lt;- wine[wine_sampling_vector,]
&gt; wine_test &lt;- wine[-wine_sampling_vector,]</pre></div><p class="calibre8">Next, we'll use the <code class="email">polr()</code> function from the <code class="email">MASS</code> package to train a proportional odds logistic <a id="id377" class="calibre1"/>regression model. Just as with the other model functions we have seen so far, we first need to specify a formula and a data frame with our training data. In addition, we must specify the <code class="email">Hess</code> parameter to <code class="email">TRUE</code> in order to obtain a model that includes additional information, such as standard errors on the coefficients:</p><div class="informalexample"><pre class="programlisting">&gt; library(MASS)
&gt; wine_model &lt;- polr(quality ~ ., data = wine_train, Hess = T)
&gt; summary(wine_model)
Call:
polr(formula = quality ~ ., data = wine_train, Hess = T)

Coefficients:
                          Value Std. Error    t value
fixed.acidity         4.728e-01   0.055641     8.4975
volatile.acidity     -4.211e+00   0.435288    -9.6741
citric.acid           9.896e-02   0.353466     0.2800
residual.sugar        3.386e-01   0.009835    34.4248
chlorides            -2.891e+00   0.116025   -24.9162
free.sulfur.dioxide   1.176e-02   0.003234     3.6374
total.sulfur.dioxide -1.618e-04   0.001384    -0.1169
density              -7.534e+02   0.625157 -1205.1041
pH                    3.107e+00   0.301434    10.3087
sulphates             2.199e+00   0.338923     6.4873
alcohol               2.883e-02   0.041479     0.6951

Intercepts:
    Value      Std. Error t value   
1|2  -736.9784     0.6341 -1162.3302
2|3  -731.4177     0.6599 -1108.4069

Residual Deviance: 4412.75 
AIC: 4438.75 </pre></div><p class="calibre8">Our model <a id="id378" class="calibre1"/>summary shows us that we have three output classes, and we have two intercepts. Now, in this dataset we have many wines that were rated average (either 5 or 6) and as a result, this class is the most frequent. We'll use the <code class="email">table()</code> function to count the number of samples by the output score and then apply <code class="email">prop.table()</code> to express these as relative frequencies:</p><div class="informalexample"><pre class="programlisting">&gt; prop.table(table(wine$quality))

         1          2          3 
0.03736219 0.74622295 0.21641486</pre></div><p class="calibre8">Class 2, which corresponds to average wines, is by far the most frequent. In fact, a simple baseline model that always predicts this category would be correct 74.6 % of the time. Let's see whether our model does better than this. We'll begin by looking at the fit on the training data and the corresponding confusion matrix:</p><div class="informalexample"><pre class="programlisting">&gt; wine_predictions &lt;- predict(wine_model, wine_train)
&gt; mean(wine_predictions == wine_train$quality)
[1] 0.7647359
&gt; table(predicted = wine_predictions,actual = wine_train$quality)
         actual
predicted    1    2    3
        1    4    1    0
        2  141 2764  619
        3    2  159  229</pre></div><p class="calibre8">Our model performs only marginally better on the training data than our baseline model. We can see why this is the case—it predicts the average class (2) very often and almost never predicts class 1. Repeating with the test set reveals a similar situation:</p><div class="informalexample"><pre class="programlisting">&gt; wine_test_predictions &lt;- predict(wine_model, wine_test)
&gt; mean(wine_test_predictions == wine_test$quality)
[1] 0.7681307
&gt; table(predicted = wine_test_predictions, 
           actual = wine_test$quality)
         actual predicted   
         1   2   3
        1   2   2   0
        2  33 693 155
        3   1  36  57</pre></div><p class="calibre8">It seems that our model is not a particularly good choice for this dataset. As we know, there are a <a id="id379" class="calibre1"/>number of possible reasons ranging from having chosen the wrong type of model to having insufficient features or the wrong kind of features. One aspect of the ordinal logistic regression model that we should always try to check is whether the proportional odds assumption is valid. There is no universally accepted way to do this, but a number of different statistical tests have been proposed in the literature. Unfortunately, it is very difficult to find reliable implementations of these tests in R. One simple test that is easy to do, however, is to train a second model using multinomial logistic regression. Then, we can compare the AIC value of our two models. Let's do this:</p><div class="informalexample"><pre class="programlisting">&gt; wine_model2 &lt;- multinom(quality ~ ., data = wine_train, 
                          maxit = 1000)
&gt; wine_predictions2 &lt;- predict(wine_model2, wine_test)
&gt; mean(wine_predictions2 == wine_test$quality)
[1] 0.7630235
&gt; table(predicted = wine_predictions2, actual = wine_test$quality)
         actual
predicted   1   2   3
        1   2   2   0
        2  32 682 149
        3   2  47  63</pre></div><p class="calibre8">The two models have virtually no difference in the quality of the fit. Let's check their <code class="email">AIC</code> values:</p><div class="informalexample"><pre class="programlisting">&gt; AIC(wine_model)
[1] 4438.75
&gt; AIC(wine_model2)
[1] 4367.448</pre></div><p class="calibre8">The AIC is lower in the multinomial logistic regression model, which suggests that we might be better off working with that model. Another possible avenue for improvement on this dataset would be to carry out feature selection. The <code class="email">step()</code> function that we saw in the previous chapter, for example, also works on models trained with the <code class="email">polr()</code> function. We'll leave this as an exercise for the reader to verify that we can, in fact, get practically the same level of performance by removing some of the features. Dissatisfied with the results of logistic regression on this latest dataset, we will revisit it in subsequent chapters in order to see whether more sophisticated classification models can do better.</p></div></div></div>
<div class="book" title="Poisson regression" id="181NK1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec35" class="calibre1"/>Poisson regression</h1></div></div></div><p class="calibre8">Another <a id="id380" class="calibre1"/>form of regression analysis is <span class="strong"><em class="calibre9">Poisson</em></span> regression. This type of analysis is a generalized linear model or GLM, used to model <span class="strong"><em class="calibre9">count</em></span> data.</p><p class="calibre8">Unlike the example in the previous section where wine samples had been rated (or ranked) on a scale from 1 to 10, count data is a (statistical) data type in which the observations can take only the non-negative integer values <span class="strong"><em class="calibre9">{0, 1, 2, 3, ...}</em></span>, and where these integers arise from <span class="strong"><em class="calibre9">counting</em></span> rather than <span class="strong"><em class="calibre9">ranking</em></span>.</p><p class="calibre8">Poisson regression assumes the outcome of your analysis has a <span class="strong"><em class="calibre9">Poisson distribution</em></span> – in that it expresses: the probability of a number of events occurring in a fixed interval of time if these events occur with a known average rate and independently of the time since the last event.</p><p class="calibre8">An example model might be of the number of phone calls received by a software support center each hour. Predictors of the number of calls received include the number of days after a new version (of the software) has been released and the number of years the customer has been a user of the software (in keeping with our wine example, you could use Poisson regression to analyze the <span class="strong"><em class="calibre9">number of bottles of wine sold during a month</em></span>, with perhaps <span class="strong"><em class="calibre9">store location</em></span> and <span class="strong"><em class="calibre9">the month of the year</em></span> as predictors).</p><p class="calibre8">A data scientist may also use a Poisson distribution for the number of events in other specified intervals, such as distance, area, or volume.</p></div>
<div class="book" title="Negative Binomial regression" id="190861-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec36" class="calibre1"/>Negative Binomial regression</h1></div></div></div><p class="calibre8">While <a id="id381" class="calibre1"/>Poisson regression assumes a (known) average, <span class="strong"><em class="calibre9">Negative Binomial regression</em></span> is implemented using what is referred to as <span class="strong"><em class="calibre9">maximum likelihood estimation</em></span>.</p><p class="calibre8">Remember that, although Poisson distribution assumes that the <span class="strong"><em class="calibre9">mean and variance are the same</em></span>, sometimes data will show greater variability or <span class="strong"><em class="calibre9">extra variation that is greater than the mean</em></span>. When this occurs, Negative Binomial regression is a better choice because of its greater flexibility in that regard.</p><p class="calibre8">To illustrate, what if we consider that a university wants to predict the average number of days a student athlete may miss each year. Predictors (of the number of days of absence from class) include <a id="id382" class="calibre1"/>the type of sport the student athlete is a member of and their average GPA score. The variable <span class="strong"><strong class="calibre2">sport</strong></span> is a four-level nominal variable indicating which sport the athlete participates in (in this case it's either <code class="email">Football</code>, <code class="email">Track</code>, <code class="email">Field</code> <code class="email">Hockey</code>, or <code class="email">Volleyball</code>).</p><p class="calibre8">If we profile our data, suppose we find the following statistics:</p><div class="informalexample"><pre class="programlisting">Football: M (SD) = 10.65 (8.20)
Track: M (SD) = 6.93 (7.45)
Field Hockey: M (SD) = 2.67 (3.73)
Volleyball: M (SD) = 1.67 (1.73)</pre></div><p class="calibre8">We may <a id="id383" class="calibre1"/>look at the preceding statistics and see that the average numbers of days absent by sport seems to suggest that the variable sport is a good candidate for predicting the number of days absent because the mean value of the outcome appears to vary by the athlete chosen sport. However, looking at the standard deviations, we see that the variances within each type of sport are higher than the means within each level.</p><p class="calibre8">These are the conditional means and variances. These differences suggest that over-dispersion (greater variability) is present and that a Negative Binomial model would be perhaps more appropriate. You could still use Poisson regression, but the standard errors could be biased.</p><p class="calibre8">Negative Binomial regression takes advantage of one additional parameter (over Poisson regression) to fine-tune the variance independently from the mean (in this example it is the student athlete GPA score).</p></div>
<div class="book" title="Summary" id="19UOO1-c6198d576bbb4f42b630392bd61137d7"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec37" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Logistic regression is the prototypical method for solving classification problems, just as linear regression was the prototypical example of a model to solve regression problems. In this chapter, we demonstrated why logistic regression offers a better way of approaching classification problems compared to linear regression with a threshold, by showing that the least squares criterion is not the most appropriate criterion to use when trying to separate two classes. We presented the notion of likelihood and its maximization as the basis for training a model. This is a very important concept that features time and again in various machine learning contexts. Logistic regression is an example of a generalized linear model. This is a model that relates the output variable to a linear combination of input features via a link function, which we saw was the logit function in this case. For the binary classification problem, we used R's <code class="email">glm()</code> function to perform logistic regression on a real-world dataset and studied the model diagnostics to evaluate our model's performance. We discovered parallels with linear regression, in that the model produces deviance residuals that are analogous to least squared error residuals and that we can compute a pseudo R2 statistic that is analogous to the R2 statistic, which measures the goodness of fit in linear regression. </p><p class="calibre8">We also saw that we can apply regularization techniques to logistic regression models. Our tour of binary classification with the logistic regression model ended by studying precision-recall curves in order to choose appropriate model thresholds, an exercise that is very important when the cost of misclassifying an observation is not symmetric for the two classes involved. We then investigated two possible extensions of the binary logistic regression model to handle outputs with many class labels. These were the multinomial logistic regression model and the ordinal logistic regression model, which can be useful when the output classes are ordered. Lastly, we touched on the use of Poisson regression and, for models with greater variability, Negative Binomial regression.</p><p class="calibre8">It turns out that logistic regression is not a great choice for multiclass settings in general. In the next chapter, we'll introduce neural networks, which are a nonlinear model used to solve both regression and classification problems. We'll also see how neural networks are able to handle multiple class labels in a natural way.</p></div></body></html>