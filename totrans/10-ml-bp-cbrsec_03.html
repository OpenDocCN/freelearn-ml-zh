<html><head></head><body>
		<div id="_idContainer041">
			<h1 id="_idParaDest-28" class="chapter-number"><a id="_idTextAnchor014"/>2</h1>
			<h1 id="_idParaDest-29">Detecting Suspicious Activity</h1>
			<p>Many problems in cybersecurity are constructed as anomaly detection tasks, as attacker behavior is generally deviant from good actor behavior. An anomaly is anything that is out of the ordinary—an event that doesn’t fit in with normal behavior and hence is considered suspicious. For example, if a person has been consistently using their credit card in Bangalore, a transaction using the same card in Paris might be an anomaly. If a website receives roughly 10,000 visits every day, a day when it receives 2 million visits might <span class="No-Break">be anomalous.</span></p>
			<p>Anomalies are few and rare and indicate behavior that is strange and suspicious. Anomaly detection algorithms are <em class="italic">unsupervised</em>; we do not have labeled data to train a model. We learn what the normal expected behavior is and flag anything that deviates from it as abnormal. Because labeled data is very rarely available in security-related areas, anomaly detection methods are crucial in identifying attacks, fraud, <span class="No-Break">and intrusions.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Basics of <span class="No-Break">anomaly detection</span></li>
				<li>Statistical algorithms for <span class="No-Break">intrusion detection</span></li>
				<li><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) algorithms for <span class="No-Break">intrusion detection</span></li>
			</ul>
			<p>By the end of this chapter, you will know how to detect outliers and anomalies using statistical and <span class="No-Break">ML methods.</span></p>
			<h1 id="_idParaDest-30">Technical requirements</h1>
			<p>All of the implementation in this chapter (and the book too) is using the Python programming language. Most standard computers will allow you to run all of the code without any memory or <span class="No-Break">runtime issues.</span></p>
			<p>There are two options for you to run the code in this book, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Jupyter Notebook</strong>: An interactive code and text notebook with a GUI that will allow you to run code locally. <em class="italic">Real Python</em> has a very good introductory tutorial on getting started at <em class="italic">Jupyter Notebook: An </em><span class="No-Break"><em class="italic">Introduction</em></span><span class="No-Break"> (</span><a href="https://realpython.com/jupyter-notebook-introduction/"><span class="No-Break">https://realpython.com/jupyter-notebook-introduction/</span></a><span class="No-Break">).</span></li>
				<li><strong class="bold">Google Colab</strong>: This is simply the online version of Jupyter Notebook. You can use the free tier as this is sufficient. Be sure to download any data or files that you create, as they disappear after the runtime is <span class="No-Break">cleaned up.</span></li>
			</ul>
			<p>You will need to install a few libraries that we need for our experiments and analysis. A list of libraries is provided as a text file, and all the libraries can be installed using the <strong class="source-inline">pip</strong> utility with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
pip install –r requirements.txt</pre>
			<p>In case you get an error for a particular library not found or installed, you can simply install it with <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install &lt;library_name&gt;</strong></span><span class="No-Break">.</span></p>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202"><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%202</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-31">Basics of anomaly detection</h1>
			<p>In this section, we will look at anomaly <a id="_idIndexMarker086"/>detection, which forms the foundation for detecting intrusions and <span class="No-Break">suspicious activity.</span></p>
			<h2 id="_idParaDest-32">What is anomaly detection?</h2>
			<p>The word <em class="italic">anomaly</em> means <em class="italic">something that deviates from what is standard, normal, or expected</em>. Anomalies are events <a id="_idIndexMarker087"/>or data points that do not fit in with the rest of the data. They represent deviations from the expected trend in data. Anomalies are rare occurrences and, therefore, few <span class="No-Break">in number.</span></p>
			<p>For example, consider a bot or fraud detection model used in a social media website such as Twitter. If we examine the number of follow requests sent to a user per day, we can get a general sense of the trend and plot this data. Let’s say that we plotted this data for a month, and ended up with the <span class="No-Break">following trend:</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B19327_02_01.jpg" alt="Figure 2.1 – Trend for the number of follow requests over a month"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Trend for the number of follow requests over a month</p>
			<p>What do you notice? The <a id="_idIndexMarker088"/>user seems to have roughly 30-40 follow requests per day. On the 8<span class="superscript">th</span> and 18<span class="superscript">th</span> days, however, we see a spike that clearly stands out from the daily trend. These two days <span class="No-Break">are anomalies.</span></p>
			<p>Anomalies can also be visually observed in a two-dimensional space. If we plot all the points in the dataset, the anomalies should stand out as being different from the others. For instance, continuing with the same example, let us say we have a number of features such as the number of messages sent, likes, retweets, and so on by a user. Using all of the features together, we can construct an <em class="italic">n</em>-dimensional feature vector for a user. By applying a dimensionality reduction algorithm such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) (at a high<a id="_idIndexMarker089"/> level, this algorithm can convert data to lower dimensions and still retain the properties), we can reduce it to two dimensions and plot the data. Say we get a plot as follows, where each point represents a user, and the dimensions represent principal components of the original data. The points colored in red clearly stand out from the rest of the data—these <span class="No-Break">are outliers:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B19327_02_02.jpg" alt="Figure 2.2 – A 2D representation of data with anomalies in red"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – A 2D representation of data with anomalies in red</p>
			<p>Note that anomalies do not necessarily represent a malicious event—they simply indicate that the trend deviates from what was normally expected. For example, a user suddenly receiving increased amounts of friend requests is anomalous, but this may have been because they posted some very engaging content. Anomalies, when flagged, must be investigated to determine whether they are malicious <span class="No-Break">or benign.</span></p>
			<p>Anomaly detection is<a id="_idIndexMarker090"/> considered an important problem in the field of cybersecurity. Unusual or abnormal events can often indicate security breaches or attacks. Furthermore, anomaly detection does not need labeled data, which is hard to come by in <span class="No-Break">security problems.</span></p>
			<h2 id="_idParaDest-33">Introducing the NSL-KDD dataset</h2>
			<p>Now that we have introduced what anomaly detection is in sufficient detail, we will look at a real-world dataset that will help us observe and detect anomalies <span class="No-Break">in action.</span></p>
			<h3>The data</h3>
			<p>Before we jump into <a id="_idIndexMarker091"/>any algorithms for anomaly detection, let us talk about the dataset we will be using in this chapter. The dataset that is popularly used for anomaly and intrusion<a id="_idIndexMarker092"/> detection tasks is the <strong class="bold">Network Security Laboratory-Knowledge Discovery in Databases</strong> (<strong class="bold">NSL-KDD</strong>) dataset. This was originally created in 1999 for use in a competition at the <em class="italic">5</em><span class="superscript">th</span><em class="italic"> International Conference on Knowledge Discovery and Data Mining (KDD)</em>. The task in the competition was to develop a network intrusion detector, which is a predictive model that can distinguish between bad connections, called intrusions or attacks, and benign normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated<a id="_idIndexMarker093"/> in a military <span class="No-Break">network environment.</span></p>
			<h3>Exploratory data analysis (EDA)</h3>
			<p>This activity consists of a few <a id="_idIndexMarker094"/>steps, which we will look <a id="_idIndexMarker095"/>at in the <span class="No-Break">next subsections.</span></p>
			<h4>Downloading the data</h4>
			<p>The actual NSL-KDD dataset is<a id="_idIndexMarker096"/> fairly large (nearly 4 million records). We will be using a smaller version of the data that is a 10% subset randomly sampled from the whole data. This will make our analysis feasible. You can, of course, experiment by downloading the full data and rerunning <span class="No-Break">our experiments.</span></p>
			<p>First, we import the necessary <span class="No-Break">Python libraries:</span></p>
			<pre class="source-code">
import pandas as pd
import numpy as np
import os
from requests import get</pre>
			<p>Then, we set the paths for the locations of training and test data, as well as the paths to a label file that holds a header (names of features) for <span class="No-Break">the data:</span></p>
			<pre class="source-code">
train_data_page = "http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"
test_data_page = "http://kdd.ics.uci.edu/databases/kddcup99/kddcup.testdata.unlabeled_10_percent.gz"
labels ="http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names"
datadir = "data"</pre>
			<p>Next, we download the data and column names using the <strong class="source-inline">wget</strong> command through Python. As these files are <a id="_idIndexMarker097"/>zipped (compressed), we have to first extract the contents using the <strong class="source-inline">gunzip</strong> command. The following Python code snippet does that <span class="No-Break">for us:</span></p>
			<pre class="source-code">
# Download training data
print("Downloading Training Data")
os.system("wget " + train_data_page)
training_file_name = train_data_page.split("/")[-1].replace(".gz","")
os.system("gunzip " + training_file_name )
with open(training_file_name, "r+") as ff:
  lines = [i.strip().split(",") for i in ff.readlines()]
ff.close()
# Download training column labels
print("Downloading Training Labels")
response = get(labels)
labels = response.text
labels = [i.split(",")[0].split(":") for i in labels.split("\n")]
labels = [i for i in labels if i[0]!='']
final_labels = labels[1::]</pre>
			<p>Finally, we construct a DataFrame from the <span class="No-Break">downloaded streams:</span></p>
			<pre class="source-code">
data = pd.DataFrame(lines)
labels = final_labels
data.columns = [i[0] for i in labels]+['target']
for i in range(len(labels)):
  if labels[i][1] == ' continuous.':
    data.iloc[:,i] = data.iloc[:,i].astype(float)</pre>
			<p>This completes our step of downloading the data and creating a DataFrame from it. A DataFrame is a tabular <a id="_idIndexMarker098"/>data structure that will allow us to manipulate, slice and dice, and filter the data <span class="No-Break">as needed.</span></p>
			<h4>Understanding the data</h4>
			<p>Once the data is<a id="_idIndexMarker099"/> downloaded, you can have a look at the DataFrame simply by printing the top <span class="No-Break">five rows:</span></p>
			<pre class="source-code">
data.head()</pre>
			<p>This should give you an output just <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B19327_02_03.jpg" alt="Figure 2.3 – Top five rows from the NSL-KDD dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Top five rows from the NSL-KDD dataset</p>
			<p>As you can see, the top five rows of the data are displayed. The dataset has 42 columns. The last column, named <strong class="source-inline">target</strong>, identifies the kind of network attack for every row in the data. To examine the distribution of network attacks (that is, how many examples of each kind of attack are present), we can run the <span class="No-Break">following statement:</span></p>
			<pre class="source-code">
data['target'].value_counts()</pre>
			<p>This will list all network attacks and the count (number of rows) for each attack, <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B19327_02_04.jpg" alt="Figure 2.4 – Distribution of data by label (attack type)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Distribution of data by label (attack type)</p>
			<p>We can see that there are a variety of attack types present in the data, with the <strong class="source-inline">smurf</strong> and <strong class="source-inline">neptune</strong> types<a id="_idIndexMarker100"/> accounting for the largest part. Next, we will look at how to model this data using <span class="No-Break">statistical algorithms.</span></p>
			<h1 id="_idParaDest-34">Statistical algorithms for intrusion detection</h1>
			<p>Now that we have taken a look <a id="_idIndexMarker101"/>at the data, let us look at<a id="_idIndexMarker102"/> basic statistical algorithms that can help us isolate<a id="_idIndexMarker103"/> anomalies and thus <span class="No-Break">identify intrusions.</span></p>
			<h2 id="_idParaDest-35">Univariate outlier detection</h2>
			<p>In the most basic form of <a id="_idIndexMarker104"/>anomaly detection, known as <em class="italic">univariate anomaly detection</em>, we build a model that considers the trends<a id="_idIndexMarker105"/> and detects anomalies based on a single feature at a time. We can build multiple such models, each operating on a single feature of <span class="No-Break">the data.</span></p>
			<h3>z-score</h3>
			<p>This is the most fundamental method to detect outliers and a cornerstone of statistical anomaly detection. It is based <a id="_idIndexMarker106"/>on the <strong class="bold">central limit theorem</strong> (<strong class="bold">CLT</strong>), which says that in most observed distributions, data is clustered around the mean. For every data point, we calculate a <em class="italic">z-score</em> that indicates how far it is from the mean. Because absolute distances would depend on the scale and nature of data, we measure how many <a id="_idIndexMarker107"/>standard deviations away from the mean the point falls. If the mean and standard deviation of a feature are μ and σ respectively, the <em class="italic">z-score</em> for a point <em class="italic">x</em> is calculated <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">μ</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>The value of <em class="italic">z</em> is the number of standard deviations away from the mean that <em class="italic">x</em> falls. The CLT says that most data (99%) falls within two standard deviations (on either side) of the mean. Thus, the higher the value of <em class="italic">z</em>, the higher the chances of the point being <span class="No-Break">an anomaly.</span></p>
			<p>Recall our defining characteristic of anomalies: they are rare and few in number. To simulate this setup, we sample from our dataset. We choose only those rows for which the target is either <strong class="source-inline">normal</strong> or <strong class="source-inline">teardrop</strong>. In this new dataset, the examples labeled <strong class="source-inline">teardrop</strong> are anomalies. We assign a label of <strong class="source-inline">0</strong> to the normal data points and <strong class="source-inline">1</strong> to the <span class="No-Break">anomalous ones:</span></p>
			<pre class="source-code">
data_resampled = data.loc[data["target"].isin(["normal.","teardrop."])]
def map_label(target):
  if target == "normal.":
    return 0
  return 1
data_resampled["Label"] = data_resampled ["target"].apply(map_label)</pre>
			<p>As univariate outlier detection operates on only one feature at a time, let us choose <strong class="source-inline">wrong_fragment</strong> as the feature for demonstration. To calculate the <em class="italic">z-score</em> for every data point, we first calculate the mean and standard deviation of <strong class="source-inline">wrong_fragment</strong>. We then subtract the mean of the entire group from the <strong class="source-inline">wrong_fragment</strong> value in each row and divide it by the <span class="No-Break">standard deviation:</span></p>
			<pre class="source-code">
mu = data_resampled ['wrong_fragment'].mean()
sigma = data_resampled ['wrong_fragment'].std()
data_resampled["Z"] = (data_resampled ['wrong_fragment'] – mu) / sigma</pre>
			<p>We can plot the distribution<a id="_idIndexMarker108"/> of the <em class="italic">z-score</em> to visually discern the nature of the distribution. The following line of code can generate a <span class="No-Break">density plot:</span></p>
			<pre class="source-code">
data_resampled["Z"].plot.density()</pre>
			<p>It should give you something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B19327_02_05.jpg" alt="Figure 2.5 – Density plot of z-scores"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Density plot of z-scores</p>
			<p>We see a sharp spike in the density around <strong class="source-inline">0</strong>, which indicates that most of the data points have a <em class="italic">z-score</em> around 0. Also, notice the very small blip around <strong class="source-inline">10</strong>; these are the small number of outlier points that have <span class="No-Break">high </span><span class="No-Break"><em class="italic">z-scores</em></span><span class="No-Break">.</span></p>
			<p>Now, all we have to do is filter out those rows that have a <em class="italic">z-score</em> of more than <strong class="source-inline">2</strong> or less than <strong class="source-inline">-2</strong>. We want to assign a label of <strong class="source-inline">1</strong> (predicted anomalies) to these rows, and <strong class="source-inline">0</strong> (predicted normal) to <span class="No-Break">the others:</span></p>
			<pre class="source-code">
def map_z_to_label(z):
    if z &gt; 2 or z &lt; -2:
      return 1
    return 0
data_resampled["Predicted_Label"] = data_resampled["Z"].apply(map_z_to_label)</pre>
			<p>Now we have the actual<a id="_idIndexMarker109"/> and predicted labels for each row, we can evaluate the performance of our model using the confusion matrix (described earlier in <a href="B19327_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">On Cybersecurity and Machine Learning</em>). Fortunately, the <strong class="source-inline">scikit-learn</strong> package in Python provides a very convenient built-in method that allows us to compute the matrix, and another package called <strong class="source-inline">seaborn</strong> allows us to quickly plot it. The code is illustrated in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
from sklearn.metrics import confusion_matrix
from matplotlib import pyplot as plt
import seaborn as sns
confusion = confusion_matrix(data_resampled["Label"],
                data_resampled["Predicted_Label"])
plt.figure(figsize = (10,8))
sns.heatmap(confusion, annot = True, fmt = 'd', cmap="YlGnBu")</pre>
			<p>This will produce a confusion matrix <span class="No-Break">as shown:</span></p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B19327_02_06.jpg" alt="Figure 2.6 – Confusion matrix"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Confusion matrix</p>
			<p>Observe the confusion matrix carefully, and compare it with the skeleton confusion matrix from <a href="B19327_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">On Cybersecurity and Machine Learning</em>. We can see that our model is able to<a id="_idIndexMarker110"/> perform very well; all of the data points have been classified correctly. We have only true positives and negatives, and no false positives or <span class="No-Break">false negatives.</span></p>
			<h2 id="_idParaDest-36">Elliptic envelope</h2>
			<p>Elliptic envelope is<a id="_idIndexMarker111"/> an algorithm to detect anomalies in Gaussian data. At a high level, the algorithm models the data as a high-dimensional <a id="_idIndexMarker112"/>Gaussian distribution. The goal is to construct an ellipse covering most of the data—points falling outside the ellipse are anomalies or outliers. Statistical methods such as covariance matrices of features are used to estimate the size and shape of <span class="No-Break">the ellipse.</span></p>
			<p>The concept of elliptic envelope is easier to visualize in a two-dimensional space. A very idealized representation is shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.7</em>. The points colored blue are within the boundary of the ellipse and hence considered normal or benign. Points in red fall outside, and hence <span class="No-Break">are anomalies:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B19327_02_07.jpg" alt="Figure 2.7 – How elliptic envelope detects anomalies"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – How elliptic envelope detects anomalies</p>
			<p>Note that the axes<a id="_idIndexMarker113"/> are labeled <strong class="bold">Dimension 1</strong> and <strong class="bold">Dimension 2</strong>. These dimensions can be features you have extracted from your data; or, in the case of high-dimensional data, they might represent <a id="_idIndexMarker114"/>principal <span class="No-Break">component features.</span></p>
			<p>Implementing elliptic envelope as an anomaly detector in Python is straightforward. We will use the resampled data (consisting of only <strong class="source-inline">normal</strong> and <strong class="source-inline">teardrop</strong> data points) and drop the categorical features, <span class="No-Break">as before:</span></p>
			<pre class="source-code">
from sklearn.covariance import EllipticEnvelope
actual_labels = data4["Label"]
X = data4.drop(["Label", "target",
                "protocol_type", "service",
                "flag"], axis=1)
clf = EllipticEnvelope(contamination=.1,random_state=0)
clf.fit(X)
predicted_labels = clf.predict(X)</pre>
			<p>The implementation of the algorithm is such that it produces <strong class="source-inline">-1</strong> if a point is outside the ellipse, and <strong class="source-inline">1</strong> if it is within. To be consistent with our ground truth labeling, we will reassign <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong> and <strong class="source-inline">1</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
predicted_labels_rescored =
[1 if pred == -1 else 0 for pred in predicted_labels]</pre>
			<p>Plot the confusion <a id="_idIndexMarker115"/>matrix as described previously. The result should be something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B19327_02_08.jpg" alt="Figure 2.8 – Confusion matrix for elliptic envelope"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Confusion matrix for elliptic envelope</p>
			<p>Note that this <a id="_idIndexMarker116"/>model has significant false positives and negatives. While the number may appear small, recall that our data had very few examples of the positive class (labeled 1) to begin with. The confusion matrix tells us that there were 931 false negatives and only 48 true positives. This indicates that the model has extremely low precision and is unable to isolate <span class="No-Break">anomalies properly.</span></p>
			<h2 id="_idParaDest-37">Local outlier factor</h2>
			<p><strong class="bold">Local outlier factor</strong> (also known as <strong class="bold">LOF</strong>) is <a id="_idIndexMarker117"/>a density-based <a id="_idIndexMarker118"/>anomaly detection algorithm. It examines points in the local neighborhood of a point to detect whether that point is anomalous. While other algorithms consider a point with respect to the<a id="_idIndexMarker119"/> global data distribution, LOF considers only the local neighborhood and determines whether the point fits in. This is particularly<a id="_idIndexMarker120"/> useful to identify hidden outliers, which may be part of a cluster of points that is not an anomaly globally. Look at <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.9</em>, <span class="No-Break">for instance:</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B19327_02_09.jpg" alt="Figure 2.9 – Local outliers"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Local outliers</p>
			<p>In this figure, the yellow point is clearly an outlier. The points marked in red are not really outliers if you consider the entirety of the data. However, observe the neighborhood of the points; they are far away and stand apart from the local clusters they are in. Therefore, they are anomalies local to the area. LOF can detect such local anomalies in addition to <span class="No-Break">global ones.</span></p>
			<p>In brief, the algorithm works as follows. For every point <em class="italic">P</em>, we do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Compute the distances from <em class="italic">P</em> to every other point in the data. This distance can be computed using a<a id="_idIndexMarker121"/> metric called <em class="italic">Manhattan distance</em>. If (x<span class="subscript">1</span>, y<span class="subscript">1</span>) and (x<span class="subscript">2</span>, y<span class="subscript">2</span>) are two distinct points, the Manhattan distance between them is |x<span class="subscript">1</span> – x<span class="subscript">2</span>| + |y<span class="subscript">1</span> – y<span class="subscript">2</span>|. This can be generalized to <span class="No-Break">multiple dimensions.</span></li>
				<li>Based on the distance, calculate the <em class="italic">K</em> closest points. This is the neighborhood of <span class="No-Break">point </span><span class="No-Break"><em class="italic">P</em></span><span class="No-Break">.</span></li>
				<li>Calculate the local reachability density, which is nothing but the inverse of the average distance between <em class="italic">P</em> and the <em class="italic">K</em> closest points. The local reachability density measures <a id="_idIndexMarker122"/>how close the neighborhood points are to <em class="italic">P</em>. A smaller value of density indicates that <em class="italic">P</em> is far away from <span class="No-Break">its neighbors.</span></li>
				<li>Finally, calculate the LOF. This is the sum of distances from <em class="italic">P</em> to the neighboring points weighted by the sum of densities of the <span class="No-Break">neighborhood points.</span></li>
				<li>Based on the<a id="_idIndexMarker123"/> LOF, we can determine whether <em class="italic">P</em> represents an anomaly in the data <span class="No-Break">or not.</span></li>
			</ol>
			<p>A high LOF value indicates that <em class="italic">P</em> is far from its neighbors and the neighbors have high densities (that is, they are close to their neighbors). This means that <em class="italic">P</em> is a local outlier in <span class="No-Break">its neighborhood.</span></p>
			<p>A low LOF value indicates that either <em class="italic">P</em> is far from the neighbors or that neighbors possibly have low densities themselves. This means that it is not an outlier in <span class="No-Break">the neighborhood.</span></p>
			<p>Note that the performance of our model here depends on the selection of <em class="italic">K</em>, the number of neighbors to form the neighborhood. If we set <em class="italic">K</em> to be too high, we would basically be looking for outliers at the global dataset level. This would lead to false positives because points that are in a cluster (so not locally anomalous) far away from the high-density regions would also be classified as anomalies. On the other hand, if we set <em class="italic">K</em> to be very small, our neighborhood would be very sparse and we would be looking for anomalies with respect to very small regions of points, which would also lead <span class="No-Break">to misclassification.</span></p>
			<p>We can try this out in Python using the built-in off-the-shelf implementation. We will use the same data and features that we <span class="No-Break">used before:</span></p>
			<pre class="source-code">
from sklearn.neighbors import LocalOutlierFactor
actual_labels = data["Label"]
X = data.drop(["Label", "target",
                "protocol_type", "service",
                "flag"], axis=1)
k = 5
clf = LocalOutlierFactor(n_neighbors=k, contamination=.1)
predicted_labels = clf.fit_predict(X)</pre>
			<p>After rescoring the <a id="_idIndexMarker124"/>predicted labels <a id="_idIndexMarker125"/>for consistency as described before, we can plot the confusion matrix. You should see something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B19327_02_10.jpg" alt="Figure 2.10 – Confusion matrix for LOF with K = 5"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Confusion matrix for LOF with K = 5</p>
			<p>Clearly, the model has an extremely high number of false negatives. We can examine how the performance changes by changing the value of <em class="italic">K</em>. Note that we first picked a very small value. If we rerun the same code with <em class="italic">K</em> = 250, we get the following <span class="No-Break">confusion matrix:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B19327_02_11.jpg" alt="Figure 2.11 – Confusion matrix for LOF with K = 250"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Confusion matrix for LOF with K = 250</p>
			<p>This second model is <a id="_idIndexMarker126"/>slightly better than the first. To find the best <em class="italic">K</em> value, we can try doing this over all possible values of <em class="italic">K</em>, and observe how our metrics change. We will vary <em class="italic">K</em> from 100 to 10,000, and for each iteration, we will calculate the accuracy, precision, and recall. We can then plot the trends in metrics with increasing <em class="italic">K</em> to check which one shows the <a id="_idIndexMarker127"/><span class="No-Break">best performance.</span></p>
			<p>The complete code listing for this is shown next. First, we define empty lists that will hold our measurements (accuracy, precision, and recall) for each value of <em class="italic">K</em> that we test. We then fit an LOF model and compute the confusion matrix. Recall the definition of a confusion matrix from <a href="B19327_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">On Cybersecurity and Machine Learning</em>, and note which entries of the matrix define the true positives, false positives, and <span class="No-Break">false negatives.</span></p>
			<p>Using the matrix, we compute accuracy, precision, and recall, and record them in the arrays. Note the calculation of the precision and recall; we deviate from the formula slightly by adding <strong class="source-inline">1</strong> to the denominator. Why do we do this? In extreme cases, we will have zero true or false positives, and we do not want the denominator to be <strong class="source-inline">0</strong> in order to avoid a <span class="No-Break">division-by-zero error:</span></p>
			<pre class="source-code">
from sklearn.neighbors import LocalOutlierFactor
actual_labels = data4["Label"]
X = data4.drop(["Label", "target","protocol_type", "service","flag"], axis=1)
all_accuracies = []
all_precision = []
all_recall = []
all_k = []
total_num_examples = len(X)
start_k = 100
end_k = 3000
for k in range(start_k, end_k,100):
  print("Checking for k = {}".format(k))
  # Fit a model
  clf = LocalOutlierFactor(n_neighbors=k, contamination=.1)
  predicted_labels = clf.fit_predict(X)
  predicted_labels_rescored = [1 if pred == -1 else 0 for pred in predicted_labels]
  confusion = confusion_matrix(actual_labels, predicted_labels_rescored)
  # Calculate metrics
  accuracy = 100 * (confusion[0][0] + confusion[1][1]) / total_num_examples
  precision = 100 * (confusion[1][1])/(confusion[1][1] + confusion[1][0] + 1)
  recall = 100 * (confusion[1][1])/(confusion[1][1] + confusion[0][1] + 1)
  # Record metrics
  all_k.append(k)
  all_accuracies.append(accuracy)
  all_precision.append(precision)
  all_recall.append(recall)</pre>
			<p>Once complete, we<a id="_idIndexMarker128"/> can plot the three series to show <a id="_idIndexMarker129"/>how the value of <em class="italic">K</em> affects the metrics. We can do this using <strong class="source-inline">matplotlib</strong>, <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
plt.plot(all_k, all_accuracies, color='green', label = 'Accuracy')
plt.plot(all_k, all_precision, color='blue', label = 'precision')
plt.plot(all_k, all_recall, color='red', label = 'Recall')
plt.show()</pre>
			<p>This is what the output <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B19327_02_12.jpg" alt="Figure 2.12 – Accuracy, precision, and recall trends with K"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Accuracy, precision, and recall trends with K</p>
			<p>We see that while accuracy<a id="_idIndexMarker130"/> and recall have remained more or less similar, the value of precision shows a declining trend as we <span class="No-Break">increase </span><span class="No-Break"><em class="italic">K</em></span><span class="No-Break">.</span></p>
			<p>This completes our discussion <a id="_idIndexMarker131"/>of statistical measures and methods for anomaly detection and their application to intrusion detection. In the next section, we will look at a few advanced unsupervised methods for doing <span class="No-Break">the same.</span></p>
			<h1 id="_idParaDest-38">Machine learning algorithms for intrusion detection</h1>
			<p>This section will cover ML <a id="_idIndexMarker132"/>methods such as clustering<a id="_idIndexMarker133"/>, autoencoders, SVM, and isolation forests, which can be used for <span class="No-Break">anomaly detection.</span></p>
			<h2 id="_idParaDest-39">Density-based scan (DBSCAN)</h2>
			<p>In the previous chapter<a id="_idIndexMarker134"/> where we introduced <strong class="bold">unsupervised ML</strong> (<strong class="bold">UML</strong>), we studied the concept of clustering via the<a id="_idIndexMarker135"/> K-Means clustering algorithm. However, recall that <em class="italic">K</em> is a hyperparameter that has to be set manually; there is no good way to know the ideal number of <a id="_idIndexMarker136"/>clusters in advance. <strong class="bold">DBSCAN</strong> is a density-based clustering algorithm that does not need a pre-specified number <span class="No-Break">of clusters.</span></p>
			<p>DBSCAN hinges on two parameters: the minimum number of points required to call a group of points a cluster and ξ (which specifies the minimum distance between two points to call them neighbors). Internally, the algorithm classifies every data point as being from one of the following <span class="No-Break">three categories:</span></p>
			<ul>
				<li><strong class="bold">Core points</strong> are those that<a id="_idIndexMarker137"/> have at least the minimum number of points in the neighborhood defined by a circle of <span class="No-Break">radius ξ</span></li>
				<li><strong class="bold">Border points</strong>, which are not<a id="_idIndexMarker138"/> core points, but in the neighborhood area or cluster of a core point <span class="No-Break">described previously</span></li>
				<li>An <strong class="bold">anomaly point</strong>, which is neither a core <a id="_idIndexMarker139"/>point nor reachable from one (that is, not a border <span class="No-Break">point either)</span></li>
			</ul>
			<p>In a nutshell, the process<a id="_idIndexMarker140"/> works <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Set the values for the parameters, the minimum number of points, <span class="No-Break">and ξ.</span></li>
				<li>Choose a starting point (say, <em class="italic">A</em>) at random, and find all points at a distance of ξ or less from <span class="No-Break">this point.</span></li>
				<li>If the number of points meets the threshold for the minimum number of points, then <em class="italic">A</em> is a core point, and a cluster can form around it. All the points at a distance of ξ or less are border points and are added to the cluster centered <span class="No-Break">on </span><span class="No-Break"><em class="italic">A</em></span><span class="No-Break">.</span></li>
				<li>Repeat these steps for each point. If a point (say, <em class="italic">B</em>) added to a cluster also turns out to be a core point, we first form its own cluster and then merge it with the original cluster <span class="No-Break">around </span><span class="No-Break"><em class="italic">A</em></span><span class="No-Break">.</span></li>
			</ol>
			<p>Thus, DBSCAN is able to identify high-density regions in the feature space and group points into clusters. A point that does not fall within a cluster is determined to be anomalous or an outlier. DBSCAN offers two powerful advantages over the K-Means clustering <span class="No-Break">discussed earlier:</span></p>
			<ul>
				<li>We do not need to specify the number of clusters. Oftentimes, when analyzing data, and especially in unsupervised settings, we may not be aware of the number of classes in the data. We, therefore, do not know what the number of clusters should be for anomaly detection. DBSCAN eliminates this problem and forms clusters <a id="_idIndexMarker141"/>appropriately based <span class="No-Break">on density.</span></li>
				<li>DBSCAN is able to find clusters that have arbitrary shapes. Because it is based on the density of <a id="_idIndexMarker142"/>regions around the core points, the shape of the clusters need not be circular. K-Means clustering, on the other hand, cannot detect overlapping or <span class="No-Break">arbitrary-shaped clusters.</span></li>
			</ul>
			<p>As an example of the arbitrary shape clusters that DBSCAN forms, consider the following figure. The color of the points represents the ground-truth labels of the classes they belong to. If K-Means is used, circularly shaped clusters are formed that do not separate the two classes. On the other hand, if we use DBSCAN, it is able to properly identify clusters based on density and separate the two classes in a much <span class="No-Break">better manner:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B19327_02_13.jpg" alt="Figure 2.13 – How K-Means (left) and DBSCAN (right) would perform for irregularly shaped clusters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – How K-Means (left) and DBSCAN (right) would perform for irregularly shaped clusters</p>
			<p>The Python implementation <a id="_idIndexMarker143"/>of DBSCAN is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from sklearn.neighbors import DBSCAN
actual_labels = data4["Label"]
X = data4.drop(["Label", "target","protocol_type", "service","flag"], axis=1)
epsilon = 0.2
minimum_samples = 5
clf = DBSCAN( eps = epsilon,  min_samples = minimum_samples)
predicted_labels = clf.fit_predict(X)</pre>
			<p>After rescoring <a id="_idIndexMarker144"/>the labels, we can plot the confusion matrix. You should see something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B19327_02_14.jpg" alt="Figure 2.14 – Confusion matrix for DBSCAN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Confusion matrix for DBSCAN</p>
			<p>This presents an interesting <a id="_idIndexMarker145"/>confusion matrix. We see that all of the outliers are being detected—but along with it, a large number of inliers are also being wrongly classified as outliers. This is a classic case of low precision and <span class="No-Break">high recall.</span></p>
			<p>We can experiment <a id="_idIndexMarker146"/>with how this model performs as we vary its two parameters. For example, if we run the same block of code with <strong class="source-inline">minimum_samples = 1000</strong> and <strong class="source-inline">epsilon = 0.8</strong>, we will get the following <span class="No-Break">confusion matrix:</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B19327_02_15.jpg" alt="Figure 2.15 – Confusion matrix for DBSCAN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Confusion matrix for DBSCAN</p>
			<p>This model is worse than<a id="_idIndexMarker147"/> the previous one and is an extreme case of low precision and high recall. Everything is predicted to be <span class="No-Break">an outlier.</span></p>
			<p>What happens if you set <strong class="source-inline">epsilon</strong> to a high value—say, <strong class="source-inline">35</strong>? You end up with the following <span class="No-Break">confusion matrix:</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B19327_02_16.jpg" alt="Figure 2.16 – Confusion matrix for improved DBSCAN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Confusion matrix for improved DBSCAN</p>
			<p>Somewhat better<a id="_idIndexMarker148"/> than before! You can experiment with other values of the parameters to find out what <span class="No-Break">works best.</span></p>
			<h2 id="_idParaDest-40">One-class SVM</h2>
			<p><strong class="bold">Support vector machine</strong> (<strong class="bold">SVM</strong>) is an algorithm widely used for classification tasks. <strong class="bold">One-class SVM</strong> (<strong class="bold">OC-SVM</strong>) is the version <a id="_idIndexMarker149"/>used for anomaly detection. However, before<a id="_idIndexMarker150"/> we turn to OC-SVM, it would be helpful to have a primer into what SVM is and how it <span class="No-Break">actually works.</span></p>
			<h3>Support vector machines</h3>
			<p>The fundamental goal of SVM is to <a id="_idIndexMarker151"/>calculate the optimal <a id="_idIndexMarker152"/>decision boundary between two classes, also known as<a id="_idIndexMarker153"/> the separating hyperplane. Data points are classified into a category depending on which side of the hyperplane or decision boundary they <span class="No-Break">fall on.</span></p>
			<p>For example, if we consider points in two dimensions, the orange and yellow lines as shown in the following figure are possible hyperplanes. Note that the visualization here is straightforward <a id="_idIndexMarker154"/>because we have only two dimensions. For 3D data, the decision boundary will be a plane. For <em class="italic">n</em>-dimensional data, the decision boundary will be an <span class="No-Break"><em class="italic">n-1</em></span><span class="No-Break">-dimensional hyperplane:</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B19327_02_17.jpg" alt="Figure 2.17 – OC-SVM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – OC-SVM</p>
			<p>We can see that in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.17</em>, both the orange and yellow lines are possible hyperplanes. But<a id="_idIndexMarker155"/> how does the SVM choose the best hyperplane? It evaluates all possible hyperplanes for the <span class="No-Break">following criteria:</span></p>
			<ul>
				<li>How well does this hyperplane separate the <span class="No-Break">two classes?</span></li>
				<li>What is the smallest distance between data points (on either side) and <span class="No-Break">the hyperplane?</span></li>
			</ul>
			<p>Ideally, we want a hyperplane that best separates the two classes and has the largest distance to the data points in <span class="No-Break">either class.</span></p>
			<p>In some cases, points may not be linearly separable. SVM employs the <em class="italic">kernel trick</em>; using a kernel function, the points are projected to a higher-dimension space. Because of the complex transformations, points that are not linearly separable in their original low-dimensional space<a id="_idIndexMarker156"/> may become separable in a higher-dimensional space. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.18</em> (drawn from <a href="https://sebastianraschka.com/faq/docs/select_svm_kernels.html">https://sebastianraschka.com/faq/docs/select_svm_kernels.html</a>) shows an example of a kernel transformation. In the original two-dimensional space, the classes are not linearly separable by a hyperplane. However, when projected into three dimensions, they<a id="_idIndexMarker157"/> become linearly separable. The hyperplane calculated in three dimensions is mapped back to the two-dimensional space in order to <span class="No-Break">make predictions:</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B19327_02_18.jpg" alt="Figure 2.18 – Kernel transformations"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18 – Kernel transformations</p>
			<p>SVMs are effective in high-dimensional feature spaces and are also memory efficient since they use a small number of points <span class="No-Break">for training.</span></p>
			<h3>The OC-SVM algorithm</h3>
			<p>Now that we have a sufficient background into what SVM is and how it works, let us discuss OC-SVM and how it can be used for <span class="No-Break">anomaly detection.</span></p>
			<p>OC-SVM has its foundations in the concepts of <strong class="bold">Support Vector Data Description</strong> (also known as <strong class="bold">SVDD</strong>). While SVM takes a<a id="_idIndexMarker158"/> planar approach, the goal in SVDD is to build a <em class="italic">hypersphere</em> enclosing the data points. Note that as this is anomaly detection, we have no labels. We construct the<a id="_idIndexMarker159"/> hypersphere and optimize it to be<a id="_idIndexMarker160"/> as small as possible. The hypothesis behind this is that outliers will be removed from the regular points, and will hence fall outside the <span class="No-Break">spherical boundary.</span></p>
			<p>For every point, we calculate the distance to the center of the sphere. If the distance is less than the radius, the point falls inside the sphere and is benign. If it is greater, the point falls outside the sphere and is hence classified as <span class="No-Break">an anomaly.</span></p>
			<p>The OC-SVM algorithm can be implemented in Python <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from sklearn import svm
actual_labels = data4["Label"]
X = data4.drop(["Label", "target","protocol_type", "service","flag"], axis=1)
clf = svm.OneClassSVM(kernel="rbf")
clf.fit(X)
predicted_labels = clf.predict(X)</pre>
			<p>Plotting the confusion matrix, this is what <span class="No-Break">we see:</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B19327_02_19.jpg" alt="Figure 2.19 – Confusion matrix for OC-SVM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19 – Confusion matrix for OC-SVM</p>
			<p>Okay—this seems <a id="_idIndexMarker161"/>to be an improvement. While our true positives have decreased, so have our false positives—and by <span class="No-Break">a lot.</span></p>
			<h2 id="_idParaDest-41">Isolation forest</h2>
			<p>In order to understand what an isolation<a id="_idIndexMarker162"/> forest is, it is <a id="_idIndexMarker163"/>necessary to have an overview of decision trees and <span class="No-Break">random forests.</span></p>
			<h3>Decision trees</h3>
			<p>A decision tree is an ML algorithm that<a id="_idIndexMarker164"/> creates a hierarchy of rules in order to classify a data point. The leaf nodes of a tree represent the labels for classification. All internal nodes (non-leaf) represent rules. For every possible result of the rule, a different child is defined. Rules are such that outputs are generally binary in nature. For continuous features, the rule compares the feature with some value. For example, in our fraud detection decision tree, <strong class="source-inline">Amount &gt; 10,000</strong> is a rule that has outputs as <strong class="source-inline">1</strong> (yes) or <strong class="source-inline">0</strong> (no). In the case of categorical variables, there is no ordering, and so a greater-than or less-than comparison is meaningless. Rules for categorical features check for membership in sets. For example, if there is a rule involving the day of the week, it can check whether the transaction fell on a weekend using the <em class="italic">Day € {Sat, </em><span class="No-Break"><em class="italic">Sun}</em></span><span class="No-Break"> rule.</span></p>
			<p>The tree defines the set of rules to be used for classification; we start at the root node and traverse the tree depending on the output of <span class="No-Break">our rules.</span></p>
			<p>A random forest is a <a id="_idIndexMarker165"/>collection of multiple decision trees, each trained on a different subset of data and hence having a different structure and different rules. While making a prediction for a data point, we run the rules in each tree independently and choose the predicted label by a <span class="No-Break">majority vote.</span></p>
			<h3>The isolation forest algorithm</h3>
			<p>Now that we have set a fair bit of background on random forests, let us turn to isolation forests. An isolation forest is an unsupervised anomaly detection algorithm. It is based on the underlying definition of <a id="_idIndexMarker166"/>anomalies; anomalies are rare occurrences and deviate significantly from normal data points. Because of this, if we process the data in a tree-like structure (similar to what we do for decision trees), the non-anomalous points will require more and more rules (which means traversing more deeply into the tree) to be classified, as they are all similar to each other. Anomalies can be detected based on the path length a data point takes from the root of <span class="No-Break">the tree.</span></p>
			<p>First, we construct a set of <em class="italic">isolation trees</em> similar to decision trees, <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Perform a random sampling over the data to obtain a subset to be used for training an <span class="No-Break">isolation tree.</span></li>
				<li>Select a feature from the set of <span class="No-Break">features available.</span></li>
				<li>Select a random threshold for the feature (if it is continuous), or a random membership test (if it <span class="No-Break">is categorical).</span></li>
				<li>Based on the rule created in <em class="italic">step 3</em>, data points will be assigned to either the left branch or the right branch of <span class="No-Break">the tree.</span></li>
				<li>Repeat <em class="italic">steps 2-4</em> recursively until each data point is isolated (in a leaf node <span class="No-Break">by itself).</span></li>
			</ol>
			<p>After the isolation trees have been constructed, we have a trained isolation forest. In order to run inferencing, we use an ensemble approach to examine the path length required to isolate a particular point. Points that can be isolated with the fewest number of rules (that is, closer<a id="_idIndexMarker167"/> to the root node) are more likely to be anomalies. If we have <em class="italic">n </em>training data points, the anomaly score for a point <em class="italic">x</em> is calculated <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>Here, <em class="italic">h(x) </em>represents the path length (number of edges of the tree traversed until the point <em class="italic">x</em> is isolated). <em class="italic">E(h(x))</em>, therefore, represents the expected value or mean of all the path lengths across multiple trees in the isolation forest. The constant <em class="italic">c(n)</em> is the average path length of an unsuccessful search in a binary search tree; we use it to normalize the expected value of <em class="italic">h(x)</em>. It is dependent on the number of training examples and can be calculated using the harmonic number <em class="italic">H(n)</em> <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">H</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">/</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span></p>
			<p>The value of  <span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable_v-normal">, </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable_v-normal">)</span> is used to determine whether the point <em class="italic">x</em> is anomalous or not. Higher values closer to <strong class="source-inline">1</strong> indicate that the points are anomalies, and smaller values indicate that they <span class="No-Break">are normal.</span></p>
			<p>The <strong class="source-inline">scikit-learn</strong> package provides us with an efficient implementation for an isolation forest so that we don’t have to do any of the hard work ourselves. We simply fit a model and use it to make predictions. For simplicity of our analysis, let us use only the continuous-valued variables, and ignore the categorical string variables <span class="No-Break">for now.</span></p>
			<p>First, we will use the original DataFrame we constructed and select the columns of interest to us. Note that we must record the labels in a list beforehand since labels cannot be used during model training. We then fit an isolation forest model on our features and use it to <span class="No-Break">make predictions:</span></p>
			<pre class="source-code">
from sklearn.ensemble import IsolationForest
actual_labels = data["Label"]
X = data.drop(["Label", "target","protocol_type", "service","flag"], axis=1)
clf = IsolationForest(random_state=0).fit(X)
predicted_labels = clf.predict(X)</pre>
			<p>The <strong class="source-inline">predict</strong> function here calculates the anomaly score and returns a prediction based on that score. A prediction of <strong class="source-inline">-1</strong> indicates that the example is determined to be anomalous, and that of <strong class="source-inline">1</strong> indicates that it is not. Recall that our actual labels are in the form of <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, not <strong class="source-inline">-1</strong> and <strong class="source-inline">1</strong>. For an apples-to-apples comparison, we will recode the predicted labels, replacing <strong class="source-inline">1</strong> with <strong class="source-inline">0</strong> and <strong class="source-inline">-1</strong> <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
predicted_labels_rescored =
[1 if pred == -1 else 0 for pred in predicted_labels]</pre>
			<p>Now, we can plot a<a id="_idIndexMarker168"/> confusion matrix using the actual and predicted labels, in the same way that we did before. On doing so, you should end up with the <span class="No-Break">following plot:</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B19327_02_20.jpg" alt="Figure 2.20 – Confusion matrix for isolation forest"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.20 – Confusion matrix for isolation forest</p>
			<p>We see that while this model predicts the majority of benign classes correctly, it also has a significant chunk of false positives <span class="No-Break">and negatives.</span></p>
			<h2 id="_idParaDest-42">Autoencoders</h2>
			<p><strong class="bold">Autoencoders</strong> (<strong class="bold">AEs</strong>) are <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>) that <a id="_idIndexMarker169"/>can be used for anomaly detection. As this is<a id="_idIndexMarker170"/> not an introductory book, we expect you to have some background and a preliminary <a id="_idIndexMarker171"/>understanding of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) and how<a id="_idIndexMarker172"/> neural networks work. As a refresher, we will present some basic concepts here. This is not meant to be an exhaustive tutorial on <span class="No-Break">neural networks.</span></p>
			<h3>A primer on neural networks</h3>
			<p>Let us now go through a<a id="_idIndexMarker173"/> few basics of neural networks, how they are formed, and how <span class="No-Break">they work.</span></p>
			<h4>Neural networks – structure</h4>
			<p>The fundamental building block <a id="_idIndexMarker174"/>of a neural network is a <em class="italic">neuron</em>. This is a <a id="_idIndexMarker175"/>computational unit that takes in multiple numeric inputs and applies a mathematical transformation on it to produce an output. Each input to a neuron has a <em class="italic">weight</em> associated with it. The neuron first calculates a weighted sum of the inputs and<a id="_idIndexMarker176"/> then applies an <em class="italic">activation function</em> that transforms this sum into an output. Weights represent the parameters of our neural network; training a model means essentially finding the optimum values of the weights such that the classification error <span class="No-Break">is reduced.</span></p>
			<p>A sample neuron is depicted in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.21</em>. It can be generalized to a neuron with any number of inputs, each having its own weight. Here, σ represents the activation function. This determines how the weighted sum of inputs is transformed into <span class="No-Break">an output:</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B19327_02_21.jpg" alt="Figure 2.21 – Basic structure of a neuron"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.21 – Basic structure of a neuron</p>
			<p>A group of neurons together form a <em class="italic">layer</em> of neurons, and multiple such layers connected together form a neural network. The more the number of layers, the <em class="italic">deeper</em> the network is said to be. Input data (in the form of features) is fed to the first layer. In the simplest form of neural networks, every neuron in a layer is connected to every neuron in the next layer; this is<a id="_idIndexMarker177"/> known as a <strong class="bold">fully connected neural </strong><span class="No-Break"><strong class="bold">network</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FCNN</strong></span><span class="No-Break">).</span></p>
			<p>The final layer is the output layer. In the case of binary classification, the output layer has only one neuron with a <em class="italic">sigmoid</em> activation function. The output of this neuron indicates the probability of the data point belonging to the positive class. If it is a multiclass classification <a id="_idIndexMarker178"/>problem, then the final layer contains as many neurons as the number of classes, each with a <em class="italic">softmax</em> activation. The outputs are normalized such that each one represents the probability of the input belonging to a particular class, and they all add up to 1. All of the layers other than the input and output are not visible from the outside; they are known as <span class="No-Break"><em class="italic">hidden layers</em></span><span class="No-Break">.</span></p>
			<p>Putting all of this together, <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.22</em> shows the structure of a <span class="No-Break">neural network:</span></p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B19327_02_22.jpg" alt="Figure 2.22 – A neural network"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.22 – A neural network</p>
			<p>Let us take a quick look at four commonly<a id="_idIndexMarker179"/> used activation functions: sigmoid, tanh, <strong class="bold">rectified linear unit</strong> (<strong class="bold">ReLU</strong>), <span class="No-Break">and softmax.</span></p>
			<h4>Neural networks – activation functions</h4>
			<p>The <em class="italic">sigmoid</em> function normalizes a<a id="_idIndexMarker180"/> real-valued input into a value between 0 and 1. It is defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>If we plot the function, we end up with a graph <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B19327_02_23.jpg" alt="Figure 2.23 – sigmoid activation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.23 – sigmoid activation</p>
			<p>As we can see, any number is <a id="_idIndexMarker181"/>squashed into a range from 0 to 1, which makes the sigmoid function an ideal candidate for <span class="No-Break">outputting probabilities.</span></p>
			<p>The <em class="italic">hyperbolic tangent</em> function, also<a id="_idIndexMarker182"/> known as the <em class="italic">tanh</em> function, is defined <span class="No-Break">as</span><span class="No-Break"><a id="_idIndexMarker183"/></span><span class="No-Break"> follows:</span></p>
			<p><span class="_-----MathTools-_Math_Function_v-normal">tanh</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span></p>
			<p>Plotting this function, we see a graph <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B19327_02_24.jpg" alt="Figure 2.24 – tanh activation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.24 – tanh activation</p>
			<p>This looks very similar to <a id="_idIndexMarker184"/>the sigmoid graph, but note the subtle differences: here, the output of the function ranges from <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong> instead of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>. Negative numbers get mapped to negative values, and positive numbers get mapped to positive values. As the function is centered on <strong class="source-inline">0</strong> (as opposed to sigmoid centered on <strong class="source-inline">0.5</strong>), it provides better mathematical conveniences. Generally, we use <em class="italic">tanh</em> as an activation function for hidden layers and <em class="italic">sigmoid</em> for the <span class="No-Break">output layer.</span></p>
			<p>The <em class="italic">ReLU</em> activation function is defined <a id="_idIndexMarker185"/><span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Variable">U</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Function_v-normal">max</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Operator">;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">≥</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">&lt;</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Number"> </span></p>
			<p>Basically, the output is the same as the input if it is positive; else, it is 0. The graph looks <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B19327_02_25.jpg" alt="Figure 2.25 – ReLU activation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.25 – ReLU activation</p>
			<p>The <em class="italic">softmax</em> function can <a id="_idIndexMarker186"/>be considered to be a normalized version of sigmoid. While sigmoid operates on a single input, softmax will operate on a vector of inputs and produce a vector as output. Each value in the output vector will be between 0 and 1. If the vector <strong class="source-inline">Z</strong> is defined as [z<span class="subscript">1</span>, z<span class="subscript">2</span>, z<span class="subscript">3</span> ….. z<span class="subscript">K</span>], softmax is defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">σ</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Variable">K</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">j</span><span class="_-----MathTools-_Math_Base"> </span></p>
			<p>Basically, we first calculate the sigmoid values for each element in the vector and form a sigmoid vector. Then, we normalize this vector by the total so that each element is between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> and they add up to <strong class="source-inline">1</strong> (representing probabilities). This can be better illustrated with a <span class="No-Break">concrete example.</span></p>
			<p>Say that our output vector <strong class="source-inline">Z</strong> has three elements: <strong class="source-inline">Z</strong> <strong class="source-inline">=</strong> <strong class="source-inline">[2,</strong> <strong class="source-inline">0.9,</strong> <strong class="source-inline">0.1]</strong>. Then, we have <em class="italic">z</em><span class="subscript">1</span>= 2, <em class="italic">z</em><span class="subscript">2</span> = 0.9, and <em class="italic">z</em><span class="subscript">3 </span>= 0.1; we therefore have <span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">7.3890</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.9</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2.4596</span>, and <span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1.1051</span>. Applying the previous equation, the denominator is the sum of these three, which is 10.9537. Now, the output vector is simply the ratio of each element to this summed-up value—that is, <span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">7.3890</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10.9537</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2.4596</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10.9537</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1.1051</span><span class="_-----MathTools-_Math_Number"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">10.9537</span><span class="_-----MathTools-_Math_Base">]</span>, which comes out to be [0.6745, 0.2245, 0.1008]. These values represent probabilities of the input belonging to each class respectively (they do not add up to 1 because of <span class="No-Break">rounding errors).</span></p>
			<p>Now that we have an <a id="_idIndexMarker187"/>understanding of activation functions, let us discuss how a neural network actually functions from end <span class="No-Break">to end.</span></p>
			<h4>Neural networks – operation</h4>
			<p>When a data point is passed through a<a id="_idIndexMarker188"/> neural network, it undergoes a series of transformations through the neurons in each layer. This phase is known as the forward pass. As<a id="_idIndexMarker189"/> the weights are assigned randomly at the beginning, the output at each neuron is different. The final layer will give us the probability of the point belonging to a particular class; we compare this with our ground-truth labels (<strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>) and calculate a loss. Just <a id="_idIndexMarker190"/>as <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) in linear regression is a loss function indicating the error, classification in neural networks <a id="_idIndexMarker191"/>uses <em class="italic">binary cross-entropy</em> and <em class="italic">categorical cross-entropy</em> as the loss for <a id="_idIndexMarker192"/>binary and multiclass <span class="No-Break">classification respectively.</span></p>
			<p>Once the loss is calculated, it is passed back through the network in a process called <em class="italic">backpropagation</em>. Every weight <a id="_idIndexMarker193"/>parameter is adjusted based on how it contributes to the loss. This <a id="_idIndexMarker194"/>phase is known as the backward pass. The same gradient descent that we learned before applies here too! Once all the data points have been passed through the network once, we say that a training <em class="italic">epoch</em> has completed. We continue this process multiple times, with the weights changing and the loss (hopefully) decreasing in each iteration. The training can be stopped either after a fixed number of epochs or if we reach a point where the loss changes <span class="No-Break">only minimally.</span></p>
			<h3>Autoencoders – a special class of neural networks</h3>
			<p>While a standard neural <a id="_idIndexMarker195"/>network aims to learn a decision function that will predict the class of an input data point (that is, classification), the goal of autoencoders is to simply <em class="italic">reconstruct</em> a data point. While training autoencoders, both the input and output that we provide to the autoencoder are the same, which is the feature vector of the data point. The rationale behind it is that because we train only on normal (non-anomalous) data, the neural network will learn how to reconstruct it. On the anomalous data, however, we expect it to fail; remember—the model was never exposed to these data points, and we expect them to be significantly different than the normal ones. As a result, the error in reconstruction is expected to be high for anomalous data points, and we use this error as a metric to determine whether a point is <span class="No-Break">an anomaly.</span></p>
			<p>An autoencoder has two components: an encoder and a decoder. The encoder takes in input data and reduces it to a lower-dimensional space. The output of the encoder can be considered to be a dimensionally reduced version of the input. This output is fed to the decoder, which then <a id="_idIndexMarker196"/>projects it into a higher-dimensional subspace (similar to that of <span class="No-Break">the input).</span></p>
			<p>Here is what an autoencoder <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B19327_02_26.jpg" alt="Figure 2.26 – Autoencoder"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.26 – Autoencoder</p>
			<p>We will implement this in Python using a framework known as <strong class="source-inline">keras</strong>. This is a library built on top of the TensorFlow framework that allows us to easily and intuitively design, build, and customize neural <span class="No-Break">network models.</span></p>
			<p>First, we import the necessary libraries and divide our data into training and test sets. Note that while we train the autoencoder only on normal or non-anomalous data, in the real world it is impossible to have a dataset that is 100% clean; there will be some contamination involved. To simulate this, we train on both the normal and abnormal examples, with the abnormal ones being very small in proportion. The <strong class="source-inline">stratify</strong> parameter ensures that the<a id="_idIndexMarker197"/> training and testing data has a similar distribution of labels so as to avoid an imbalanced dataset. The code is illustrated in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
                        actual_labels,
                        test_size=0.33,
                        random_state=42,
                        stratify=actual_labels)
X_train = np.array(X_train, dtype=np.float)
X_test = np.array(X_test, dtype=np.float)</pre>
			<p>We then build our neural network using <strong class="source-inline">keras</strong>. For each layer, we specify the number of neurons. If our feature vector has <em class="italic">N</em> dimensions, the input layer would have to have <em class="italic">N</em> layers. Similarly, because this is an autoencoder, the output layer would also have <span class="No-Break"><em class="italic">N</em></span><span class="No-Break"> layers.</span></p>
			<p>We first build the encoder part. We start with an input layer, followed by three fully connected layers of decreasing dimensions. To each layer, we feed the output of the previous layer <span class="No-Break">as input:</span></p>
			<pre class="source-code">
input = keras.Input(shape=(X_train.shape[1],))
encoded = layers.Dense(30, activation='relu')(input)
encoded = layers.Dense(16, activation='relu')(encoded)
encoded = layers.Dense(8, activation='relu')(encoded)</pre>
			<p>Now, we work our way back to higher dimensions and build the <span class="No-Break">decoder part:</span></p>
			<pre class="source-code">
decoded = layers.Dense(16, activation='relu')(encoded)
decoded = layers.Dense(30, activation='relu')(decoded)
decoded = layers.Dense(X_train.shape[1], activation='sigmoid')(encoded)</pre>
			<p>Now, we put it all together to form <span class="No-Break">the autoencoder:</span></p>
			<pre class="source-code">
autoencoder = keras.Model(input, decoded)</pre>
			<p>We also define the <a id="_idIndexMarker198"/>encoder and decoder models separately, to make <span class="No-Break">predictions later:</span></p>
			<pre class="source-code">
# Encoder
encoder = keras.Model(input_img, encoded)
# Decoder
encoded_input = keras.Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]
decoder = keras.Model(encoded_input,decoder_layer(encoded_input))</pre>
			<p>If we want to examine the structure of the autoencoder, we can compile it and print out a summary of <span class="No-Break">the model:</span></p>
			<pre class="source-code">
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.summary()</pre>
			<p>You should see which layers are in the network and what their dimensions are <span class="No-Break">as well:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B19327_02_27.jpg" alt="Figure 2.27 – Autoencoder model summary"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.27 – Autoencoder model summary</p>
			<p>Finally, we actually<a id="_idIndexMarker199"/> train the model. Normally, we provide features and the ground truth for training, but in this case, our input and output are <span class="No-Break">the same:</span></p>
			<pre class="source-code">
autoencoder.fit(X_train, X_train,
                epochs=10,
                batch_size=256,
                shuffle=True)</pre>
			<p>Let’s see <span class="No-Break">the result:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B19327_02_28.jpg" alt="Figure 2.28 – Training phase"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.28 – Training phase</p>
			<p>Now that the<a id="_idIndexMarker200"/> model is fit, we can use it to make predictions. To evaluate this model, we will predict the output for each input data point, and calculate the <span class="No-Break">reconstruction error:</span></p>
			<pre class="source-code">
from sklearn.metrics import mean_squared_error
predicted = autoencoder.predict(X_test)
errors = [np.linalg.norm(X_test[idx] - k[idx]) for idx in range(X_test.shape[0])]</pre>
			<p>Now, we need to set a threshold for the reconstruction error, after which we will call data points as anomalies. The easiest way to do this is to leverage the distribution of the reconstruction error. Here, we say that when the error is above the 99<span class="superscript">th</span> percentile, it represents <span class="No-Break">an anomaly:</span></p>
			<pre class="source-code">
thresh = np.percentile(errors, 99)
predicted_labels = [1 if errors[idx] &gt; thresh else 0 for idx in range(X_test.shape[0])]</pre>
			<p>When we generate<a id="_idIndexMarker201"/> the confusion matrix, we see something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B19327_02_29.jpg" alt="Figure 2.29 – Confusion matrix for Autoencoder"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.29 – Confusion matrix for Autoencoder</p>
			<p>As we can see, this model is a very poor one; there are absolutely no true <span class="No-Break">positives here.</span></p>
			<h1 id="_idParaDest-43">Summary</h1>
			<p>This chapter delved into the details of anomaly detection. We began by learning what anomalies are and what their occurrence can indicate. Using NSL-KDD, a benchmark dataset, we first explored statistical methods used to detect anomalies, such as the z-score, elliptical envelope, LOF, and DBSCAN. Then, we examined ML methods for the same task, including isolation forests, OC-SVM, and <span class="No-Break">deep autoencoders.</span></p>
			<p>Using the techniques introduced in this chapter, you will be able to examine a dataset and detect anomalous data points. Identifying anomalies is key in many security problems such as intrusion and <span class="No-Break">fraud detection.</span></p>
			<p>In the next chapter, we will learn about malware, and how to detect it using state-of-the-art models known <span class="No-Break">as transformers.</span></p>
		</div>
	</body></html>