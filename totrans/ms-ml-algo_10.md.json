["```py\nimport numpy as np\n\nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\n\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\nwidth = height = X_train.shape[1]\n\nX_train = X_train.reshape((X_train.shape[0], width, height, 1)).astype(np.float32) / 255.0\n X_test = X_test.reshape((X_test.shape[0], width, height, 1)).astype(np.float32) / 255.0\n\nY_train = to_categorical(Y_train, num_classes=10)\nY_test = to_categorical(Y_test, num_classes=10)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Conv2D, AveragePooling2D, Flatten\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Dropout(0.25, input_shape=(width, height, 1), seed=1000))\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\nmodel.add(Activation('relu'))\n\nmodel.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5, seed=1000))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nmodel.compile(optimizer=Adam(lr=0.001, decay=1e-5),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n```", "```py\nhistory = model.fit(X_train, Y_train,\n                    epochs=200,\n                    batch_size=256,\n                    validation_data=(X_test, Y_test))\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/200\n60000/60000 [==============================] - 30s 496us/step - loss: 0.4474 - acc: 0.8531 - val_loss: 0.0993 - val_acc: 0.9693\nEpoch 2/200\n60000/60000 [==============================] - 20s 338us/step - loss: 0.1497 - acc: 0.9530 - val_loss: 0.0682 - val_acc: 0.9780\nEpoch 3/200\n60000/60000 [==============================] - 21s 346us/step - loss: 0.1131 - acc: 0.9647 - val_loss: 0.0598 - val_acc: 0.9839\n\n...\n\nEpoch 199/200\n60000/60000 [==============================] - 21s 349us/step - loss: 0.0083 - acc: 0.9974 - val_loss: 0.0137 - val_acc: 0.9950\nEpoch 200/200\n60000/60000 [==============================] - 22s 373us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0143 - val_acc: 0.9950\n```", "```py\nfrom keras.datasets import fashion_mnist\n\n(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n```", "```py\nimport numpy as np\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import to_categorical\n\nnb_classes = 10\ntrain_batch_size = 256\ntest_batch_size = 100\n\ntrain_idg = ImageDataGenerator(rescale=1.0 / 255.0,\n                               samplewise_center=True,\n                               samplewise_std_normalization=True,\n                               horizontal_flip=True,\n                               rotation_range=10.0,\n                               shear_range=np.pi / 12.0,\n                               zoom_range=0.25)\n\ntrain_dg = train_idg.flow(x=np.expand_dims(X_train, axis=3),\n                          y=to_categorical(Y_train, num_classes=nb_classes),\n                          batch_size=train_batch_size,\n                          shuffle=True,\n                          seed=1000)\n\ntest_idg = ImageDataGenerator(rescale=1.0 / 255.0,\n                              samplewise_center=True,\n                              samplewise_std_normalization=True)\n\ntest_dg = train_idg.flow(x=np.expand_dims(X_test, axis=3),\n                         y=to_categorical(Y_test, num_classes=nb_classes),\n                         shuffle=False,\n                         batch_size=test_batch_size,\n                         seed=1000)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Flatten, LeakyReLU, Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32,\n                 kernel_size=(3, 3),\n                 padding='same',\n                 input_shape=(X_train.shape[1], X_train.shape[2], 1)))\n\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Conv2D(filters=64,\n                 kernel_size=(3, 3),\n                 padding='same'))\n\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64,\n                 kernel_size=(3, 3),\n                 padding='same'))\n\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Conv2D(filters=128,\n                 kernel_size=(3, 3),\n                 padding='same'))\n\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Conv2D(filters=128,\n                 kernel_size=(3, 3),\n                 padding='same'))\n\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=1024))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Dense(units=1024))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.1))\n\nmodel.add(Dense(units=nb_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(lr=0.0001, decay=1e-5),\n              metrics=['accuracy'])\n```", "```py\nfrom keras.callbacks import ReduceLROnPlateau\n\nnb_epochs = 100\nsteps_per_epoch = 1500\n\nhistory = model.fit_generator(generator=train_dg,\n                              epochs=nb_epochs,\n                              steps_per_epoch=steps_per_epoch,\n                              validation_data=test_dg,\n                              validation_steps=int(X_test.shape[0] / test_batch_size),\n                              callbacks=[\n                                 ReduceLROnPlateau(factor=0.1, patience=1, cooldown=1, min_lr=1e-6)\n                              ])\n\nEpoch 1/100\n1500/1500 [==============================] - 471s 314ms/step - loss: 0.3457 - acc: 0.8722 - val_loss: 0.2863 - val_acc: 0.8952\nEpoch 2/100\n1500/1500 [==============================] - 464s 309ms/step - loss: 0.2325 - acc: 0.9138 - val_loss: 0.2721 - val_acc: 0.8990\nEpoch 3/100\n1500/1500 [==============================] - 460s 307ms/step - loss: 0.1929 - acc: 0.9285 - val_loss: 0.2522 - val_acc: 0.9112\n\n...\n\nEpoch 99/100\n1500/1500 [==============================] - 449s 299ms/step - loss: 0.0438 - acc: 0.9859 - val_loss: 0.2142 - val_acc: 0.9323\nEpoch 100/100\n1500/1500 [==============================] - 449s 299ms/step - loss: 0.0443 - acc: 0.9857 - val_loss: 0.2136 - val_acc: 0.9339\n```", "```py\nimport numpy as np\n\ndataset_filename = '<YOUR_PATH>\\dataset.csv'\n\nn_samples = 2820\ndata = np.zeros(shape=(n_samples, ), dtype=np.float32)\n\nwith open(dataset_filename, 'r') as f:\n    lines = f.readlines()\n\nfor i, line in enumerate(lines):\n    if i == 0:\n        continue\n\n    if i == n_samples + 1:\n        break\n\n    _, value = line.split(',')\n    data[i-1] = float(value)\n```", "```py\nimport pandas as pd\n\ndataset_filename = '<YOUR_PATH>\\dataset.csv'\n\ndf = pd.read_csv(dataset_filename, index_col=0, header=0).dropna()\ndata = df.values.astype(np.float32).squeeze()\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\n\nmmscaler = MinMaxScaler((-1.0, 1.0))\ndata = mmscaler.fit_transform(data.reshape(-1, 1))\n```", "```py\nsequence_length = 15\n\nX_ts = np.zeros(shape=(n_samples - sequence_length, sequence_length, 1), dtype=np.float32)\nY_ts = np.zeros(shape=(n_samples - sequence_length, 1), dtype=np.float32)\n\nfor i in range(0, data.shape[0] - sequence_length):\n    X_ts[i] = data[i:i + sequence_length]\n    Y_ts[i] = data[i + sequence_length]\n\nX_ts_train = X_ts[0:2300, :]\nY_ts_train = Y_ts[0:2300]\n\nX_ts_test = X_ts[2300:2800, :]\nY_ts_test = Y_ts[2300:2800]\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Activation\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(LSTM(4, stateful=True, batch_input_shape=(20, sequence_length, 1)))\n\nmodel.add(Dense(1))\nmodel.add(Activation('tanh'))\n\nmodel.compile(optimizer=Adam(lr=0.001, decay=0.0001),\n              loss='mse',\n              metrics=['mse'])\n```", "```py\nmodel.fit(X_ts_train, Y_ts_train,\n          batch_size=20,\n          epochs=100,\n          shuffle=False,\n          validation_data=(X_ts_test, Y_ts_test))\n\nTrain on 2300 samples, validate on 500 samples\nEpoch 1/100\n2300/2300 [==============================] - 11s 5ms/step - loss: 0.4905 - mean_squared_error: 0.4905 - val_loss: 0.1827 - val_mean_squared_error: 0.1827\nEpoch 2/100\n2300/2300 [==============================] - 4s 2ms/step - loss: 0.1214 - mean_squared_error: 0.1214 - val_loss: 0.1522 - val_mean_squared_error: 0.1522\nEpoch 3/100\n2300/2300 [==============================] - 4s 2ms/step - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.1154 - val_mean_squared_error: 0.1154\n\n...\n\nEpoch 99/100\n2300/2300 [==============================] - 4s 2ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\nEpoch 100/100\n2300/2300 [==============================] - 4s 2ms/step - loss: 0.0139 - mean_squared_error: 0.0139 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n```"]