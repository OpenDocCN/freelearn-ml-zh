- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intermediate Linear Algebra in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter covered the basics of linear algebra and its calculations
    in R. This chapter will go a step further by extending to intermediate linear
    algebra and cover topics such as the determinant, rank, and trace of a matrix,
    eigenvalues and eigenvectors, and **principal component analysis** (**PCA**).
    Besides providing an intuitive understanding of these abstract yet important mathematical
    concepts, we’ll cover the practical implementations of calculating these quantities
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have grasped important matrix properties,
    such as determinant and rank, and gained hands-on experience in calculating these
    quantities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the matrix determinant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the matrix trace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the matrix norm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting to know eigenvalues and eigenvectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of the `Matrix` package, which is 1.5.1 at the time of writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of the `factoextra` package, which is 1.0.7 at the time of
    writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the matrix determinant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **determinant** of a matrix is a special scalar value that can be calculated
    from a matrix. Here, the matrix needs to be square, meaning it has an equal number
    of rows and columns. For a 2x2 square matrix, the determinant is simply calculated
    as the difference between the product of the diagonal elements and the off-diagonal
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, suppose our 2x2 matrix is A = [a b c d ]. Its determinant,
    |A|, is thus calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: det(A) = |A| = ad − bc
  prefs: []
  type: TYPE_NORMAL
- en: Please do not confuse these vertical lines with the absolute operation sign.
    They represent the determinant in the context of a matrix, and the determinant
    of a matrix can be negative as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say our 2x2 matrix is A = [2 6 1 8]. We can find its determinant like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '|A| = 2 * 8 − 6 * 1 = 10'
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the determinant of a matrix is the easy part, but understanding
    its use is of equal importance. Before we cover its properties, first, we’ll review
    the calculation in R to get a straightforward understanding of the scalar output
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we are creating a matrix, A, from a vector with
    proper configurations (two rows, filling by row). As usual, we verify the content
    in the matrix by printing it out to the console. We then call the `det()` function
    to calculate its determinant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There is also a corresponding formula for calculating the determinant of a 3x3
    matrix or even higher dimension. We will not entertain these cases here as understanding
    the properties of the determinant is more important at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the determinant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that any matrix can be thought of as a transformation or projection
    that changes the input from one space to another. There are two things to note
    for such a change: *quantity* and *direction*. Quantity measures the percentage
    change in the magnitude of the original size of the matrix, while the direction
    indicates the sign of the transformation, taking either a positive or a negative
    value. Here, the matrix size can be considered as the area of a 2x2 matrix or
    the volume of a 3x3 matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: The columns in the matrix represent the collection of linear transformation
    that either stretches or squishes the space of the original input, thus changing
    the size of the matrix. So, the determinant measures how much the collection of
    linear transformations stretches or squishes the input. It gives a factor by which
    the area or volume of a region increases or decreases. Also, since directionality
    matters, the change may flip the input, as indicated by a negative determinant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. Suppose we have a 2x2 input matrix, [1 0 0 1], and
    we would like to transform it via another 2x2 matrix, [3 0 0 2]. A direct multiplication
    gives an output of [3 0 0 2], which can be verified by carrying out the matrix
    multiplication rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1 0 0 1][3 0 0 2] = [3 0 0 2]'
  prefs: []
  type: TYPE_NORMAL
- en: There is no change to the output since the input matrix is essentially an identity
    matrix, and we know that any matrix multiplied by an identity matrix remains unchanged.
    No surprise here. However, when viewing [1 0 0 1] as the input matrix, with [3 0 0 2]
    on the left as the transformation matrix and [3 0 0 2] on the right as the output
    matrix, we can see that the transformation matrix increases the area of the input
    matrix by a factor of six.
  prefs: []
  type: TYPE_NORMAL
- en: To see this, imagine the input matrix, [1 0 0 1], on a two-dimensional coordinate
    system. The area of the input matrix is 1 * 1 = 1, while the area of the output
    matrix is 3 * 2 = 6, which happens to be the determinant of the transformation
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not a coincidence. The net effect of the transformation matrix is thus
    to magnify the area of the input matrix by 6, maintaining the same direction.
    And it is not difficult to obtain the same increase in area in a negative direction
    when changing the transformation matrix to [− 3 0 0 2] or [3 0 0 − 2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Illustrating the effect of the matrix determinant in determining
    the change in the area of the input matrix](img/B18680_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Illustrating the effect of the matrix determinant in determining
    the change in the area of the input matrix
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.1* summarizes this important property.'
  prefs: []
  type: TYPE_NORMAL
- en: Connection to the matrix rank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rank of a matrix, A, is the maximal number of linearly independent columns
    in the matrix. This number has a connection to the determinant of a matrix. Specifically,
    the rank is the number of rows (or columns) of the largest square submatrix of
    A such that its determinant is nonzero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. Suppose A is a 2x3 matrix, [1 2 3 3 2 4]. First,
    we find the largest square submatrix, which is [1 2 3 2] or [2 3 2 4]. Both matrices
    have a nonzero determinant. Thus, the rank of A is. This means we can use this
    technique to find the rank of a matrix. *Figure 8**.2* summarizes this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Deriving the rank of a matrix using the determinant](img/B18680_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Deriving the rank of a matrix using the determinant
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how to obtain the rank of a matrix computationally:'
  prefs: []
  type: TYPE_NORMAL
- en: For this, we need to load the `Matrix` package in R and call the `rankMatrix()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As shown in the following code snippet, first, we create the 3x2 matrix, A,
    and print it out. When designing this matrix, we simply fill in a vector that
    consists of the row-wise concatenation of A:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we call the `rankMatrix()` function to obtain its rank:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Multiple attributes are returned. We can access the first attribute as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the next section, we will look at another important property of a matrix:
    the trace.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the matrix trace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **trace** is a quantity that only applies to square matrices, such as the
    covariance matrix often encountered in ML. It is denoted as tr(A) for a square
    matrix, A, and is calculated as the sum of the diagonal elements in a square matrix.
    Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we are creating a 3x3 matrix, A, and using the
    `diag()` function to extract the diagonal elements and sum them up to obtain the
    trace of the matrix. Note that we first create a DataFrame consisting of three
    columns, each having three elements, and then convert it into a matrix format
    to store in `A`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since there is no built-in function that can calculate the trace in one shot,
    we can build a customized one to perform this task. As shown in the following
    code snippet, the customized function named `trace()` essentially loops through
    all the diagonal elements of the input square matrix and adds them up as the return
    value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Testing out this function gives us the same trace as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are some interesting properties regarding the trace of the matrix, as
    we’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Special properties of the matrix trace
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate these properties, we’ll first create another matrix, B:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will introduce five properties that are commonly used in statistical modeling.
    All of these properties will be verified in our example with matrices A and B:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Property 1**: The trace of the sum of two square matrices is the sum of the
    traces of the two matrices:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tr(A + B) = tr(A) + tr(B)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we use the equality sign to check whether the left-hand side is equal
    to the right-hand side. A return of `TRUE` means that the property has been verified.
  prefs: []
  type: TYPE_NORMAL
- en: '**Property 2**: The trace of a matrix is equal to the trace of the matrix’s
    transpose:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tr(A) = tr(A T)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we used the `t()` function to obtain the transpose of a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Property 3**: For a matrix multiplied by a scalar value, its trace is the
    same as the original trace multiplied by the same scalar:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tr(αA) = αtr(A)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we chose a scalar coefficient of `2`. Feel free to change this value and
    verify whether the property still holds.
  prefs: []
  type: TYPE_NORMAL
- en: '**Property 4**: The trace is cyclical:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tr(AB) = tr(BA)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This property says that when multiplying A by B, the trace of the resulting
    matrix is the same as the one from multiplying B by A.
  prefs: []
  type: TYPE_NORMAL
- en: Note the use of the `%*%` notation when performing matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: '**Property 5**: The trace is invariant:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tr(A) = tr(BA B −1)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This property says that if we multiply B by A, and then multiply the inverse,
    B −1, the trace of the resulting matrix is the same as the trace of matrix A.
    *Figure 8**.3* summarizes these five properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Five properties of matrix trace](img/B18680_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Five properties of matrix trace
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will cover another important summary measure of a matrix:
    the matrix norm.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the matrix norm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **norm** of a matrix is a scalar value that measures the magnitude of the
    matrix. Therefore, the norm is a way to measure the size or length of a vector
    or a matrix. For example, the weights of a deep neural network are stored in matrices,
    and we would typically constrain the norm of the weights to be small to prevent
    overfitting. This allows us to quantify the magnitude, which is useful when comparing
    different vectors or matrices, which often consist of multiple elements. As it
    generalizes from the vector norm, we will first go through the basics of the vector
    norm.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the vector norm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a vector, a = [1,0, − 1], and another vector, b = [1,2,0]. To
    assess the similarity between these two vectors, we can argue that they are the
    same in the first element only and different for the remaining two elements. To
    compare these two vectors holistically, we need a single metric – one that summarizes
    the whole vector. The norm is one way to go forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different norms for an arbitrary vector of length n. All forms come
    from the following generalized form of L p-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖x‖ p = (∑ i=1 n |x i| p) 1/p
  prefs: []
  type: TYPE_NORMAL
- en: Here, the double vertical bars denote the norm. This is called the L p-norm
    because p is used as a placeholder to represent the specific type of norm. Common
    values of p include 1, 2, and ∞, although theoretically, it could take on any
    positive integer value. For example, to calculate the L 1-norm of the vector,
    we can simply plug p = 1 into the formula.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll go through these common norms in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the L 1-norm of a vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Substituting p = 1 in the previous equation gives us the L 1-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖x‖ 1 = ∑ i=1 n |x i|
  prefs: []
  type: TYPE_NORMAL
- en: This can be considered as the total length of the vector, x, which is calculated
    as the sum of the absolute values of all entries in the vector. When we have a
    two-element vector, x = [x 1, x 2], the L 1-norm will be ‖x‖ 1 = |x 1| + |x 2|.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a 3x1 matrix to represent a column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call the `norm()` function to calculate the L 1-norm in R, as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `norm()` function calculates the L 1-norm by default. To set
    the type of norm explicitly, we can pass in the `type="1"` argument, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will move on to the L 2-norm.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the L 2-norm of a vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The L 2-norm is the most common type of norm we usually work with. Also called
    the Euclidean norm, the L 2-norm measures the usual distance between two points.
    Plugging p = 2 into the previous formula gives us the following definition of
    the L 2-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖x‖ 2 = √ _ ∑ i=1 n |x i| 2  = √ _ ∑ i=1 n x i 2
  prefs: []
  type: TYPE_NORMAL
- en: The calculation process involves squaring each entry, adding them up, and then
    taking the square root. Similarly, when we have a two-element vector, x = [x 1,
    x 2], the L 2-norm will be ‖x‖ 2 = √ _ x 1 2 + x 2 2 .
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the L 2-norm of a vector by specifying `type="2"`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next comes the max norm.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the L ∞-norm of a vector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The L ∞-norm, or **max norm**, finds the largest absolute value of all the
    elements within the vector. This norm is often used in worse-case scenarios –
    for example, representing the maximum noise injected into a signal. Its definition
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖x‖ ∞ = max 1≤i≤n | x i|
  prefs: []
  type: TYPE_NORMAL
- en: So, the calculation process involves pairwise comparisons of absolute values
    in search of the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the L ∞-norm, we can specify `type="2"`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll move on to the matrix norm.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the matrix norm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `norm()` function to do the job. We will use X to denote a general matrix,
    m × n, where X ij denotes the element located at the i th row and j th column.
    All forms of matrix norm come from the following generalized form of L p-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖X‖ p = (∑ i=1 m ∑ j=1 n |X ij| p) 1/p
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a 3x3 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will look at the L 1-norm of the X matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the L 1-norm of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The L 1-norm for a matrix is similar to its vector form but slightly different.
    As shown here, to calculate the L 1-norm for a matrix, we must first sum the absolute
    values of each column, then take the largest summation as the L 1-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖X‖ 1 = max 1≤j≤n ∑ i=1 m | X ij|
  prefs: []
  type: TYPE_NORMAL
- en: Since the summation is performed column-wise, the L 1-norm for a matrix is also
    called the column-sum norm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the L 1-norm using the same command that we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'A visual inspection shows that the third column gives the maximum summation
    of absolute values of 12\. We can also quickly check the column-wise summation
    of the absolute values of the matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Calculating the Frobenius norm of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The L 2-norm of a matrix is more involved at this stage, so we will focus on
    a similar kin that is widely used in practice: the **Frobenius norm**. The Frobenius
    norm is calculated by summing all squared entries of the matrix and taking the
    square root:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖X‖ F = √ _ ∑ i=1 m ∑ j=1 n |X ij| 2
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the Frobenius norm by setting `type="f"`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify the calculations by carrying out the manual process of squaring
    all entries, summing them up, and taking the square root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will look at the infinity norm of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the infinity norm of a matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **infinity norm** of a matrix works similarly to the L 1-norm of a matrix,
    although the order of sequence is different. In particular, we would sum up the
    absolute values for each row and return the largest summation:'
  prefs: []
  type: TYPE_NORMAL
- en: ‖X‖ ∞ = max 1≤j≤m ∑ i=1 n | X ij|
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the infinity norm is also referred to as the `type="I"` in the `norm()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, it is a good habit to verify the result by manually carrying out the
    calculation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 8**.4* summarizes these three norms for both vectors and matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Common norms for vectors and matrices](img/B18680_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Common norms for vectors and matrices
  prefs: []
  type: TYPE_NORMAL
- en: 'Having covered these fundamentals, let’s move on to the next important topic:
    eigenvalues and eigenvectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know eigenvalues and eigenvectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **eigenvalue**, often denoted by a scalar value of λ, and the **eigenvector**,
    often denoted by v, are essential properties of a square matrix, A. Two central
    ideas are required to understand the purpose of eigenvalues and eigenvectors.
    The first is that the matrix, A, is a transformation that maps one input vector
    to another output vector, which possibly changes the direction. The second is
    that the eigenvector is a special vector that does not change direction after
    going through the transformation induced by A. Instead, the eigenvector gets scaled
    along the same original direction by a multiple of the corresponding scalar eigenvalue.
    The following equation sums this up:'
  prefs: []
  type: TYPE_NORMAL
- en: Av = λv
  prefs: []
  type: TYPE_NORMAL
- en: 'These two points capture the essence of **eigendecomposition**, which represents
    the original matrix, A, in terms of its eigenvalues and eigenvectors and thus
    allows easier matrix operations in many cases. Let’s start by understanding a
    simple case: **scalar-vector multiplication**.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding scalar-vector multiplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix-vector multiplication can result in many forms of transformations, such
    as rotation, reflection, dilation, contraction, projection, and a combination
    of these operations. With eigenvalues and eigenvectors, we can decompose these
    operations into a series of simpler ones. For the case of scalar-vector multiplication,
    when the scalar value is in the range of 0 and 1, this will make the elements
    of the vector smaller, thus *contracting* the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to multiply a vector, v, by a scalar, λ, giving us λv. Since
    v contains one or more elements, multiplication essentially applies to each element
    in the vector. The following code snippet shows the result of multiplying a scalar
    by a vector, where each of the elements gets doubled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes a key technique: introducing the identity matrix, I. Since the vector
    has three elements, we can introduce a 3x3 identity matrix into the equation.
    In the previous chapter, we learned that multiplying an identity matrix will not
    change the result, so this is something we can proceed with:'
  prefs: []
  type: TYPE_NORMAL
- en: λv = λIv
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we first create a 3x3 identity matrix, I, using the `diag()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Multiplying it by the scalar, λ, changes all diagonal elements to 2 in λI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Since λI is a 3x3 matrix, the previous scalar-vector multiplication becomes
    matrix-vector multiplication now. This means that we need to switch to the `%*%`
    sign to perform the inner multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The result is the same as the previous scalar-vector product, although it is
    now expressed as a column vector instead of a row vector. From the perspective
    of matrix-vector multiplication, the matrix *transforms* the vector by doubling
    every element in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will formally define the notion of eigenvalues and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Defining eigenvalues and eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By introducing the identity matrix, we managed to convert a scalar-vector multiplication,
    λv, into a matrix-vector multiplication, λIv. This makes all the difference in
    understanding the key equation: Av = λv, where the left is a matrix-vector multiplication
    and the right is a scalar-vector multiplication. By writing Av = λIv, this equation
    suddenly makes more sense as it has the same type of operation on both sides.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we must define λ and v to give them proper names. For a square matrix,
    A, we say that the scalar, λ, is an eigenvalue of A, together with an associated
    eigenvector, v ≠ 0, if Av = λv is true. This equation says that the matrix-vector
    multiplication in Av produces the same vector as the scalar-vector multiplication.
    λ and v together are called an eigenpair.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly verify the equality mentioned in Av = λv. Suppose A = [2 3 0 1],
    λ = 2, and v T = [1,0]. The calculation on the left-hand side gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Av = [2 3 0 1][1 0] = [2 0]
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation on the right-hand side gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: λv = 2[1 0] = [2 0]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the equality checks.
  prefs: []
  type: TYPE_NORMAL
- en: Geometrically, the eigenvector is a vector that stays fixed in its original
    direction when we apply a matrix transformation by A. It stays on the same line
    and remains invariant upon multiplication. Also, there is often a collection of
    such eigenvectors (with the corresponding eigenvalues) for a square matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet verifies the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the eigenvector focuses entirely on the *direction* of the transformation-invariant
    vector, rather than the magnitude. To see this, we can double the eigenvector
    and find that the equality still checks:'
  prefs: []
  type: TYPE_NORMAL
- en: Av = [2 3 0 1][2 0] = [4 0]
  prefs: []
  type: TYPE_NORMAL
- en: λv = 2[2 0] = [4 0]
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the equality by taking their difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 8**.5* summarizes our understanding so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Summarizing our understanding of eigenvalues and eigenvectors](img/B18680_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Summarizing our understanding of eigenvalues and eigenvectors
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at how to compute the eigenvalues and eigenvectors of a square
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Computing eigenvalues and eigenvectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous examples assume that we have access to the eigenvalues and eigenvectors.
    In practice, these need to be computed from the original square matrix. This section
    focuses on how to obtain the solutions to the eigenvalues and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start from where we left off. By introducing the identity matrix, we managed
    to obtain Av = λIv. Moving things around, we have Av − λIv = 0\. Combining similar
    terms gives us (A − λI)v = 0\. We know that v ≠ 0 by definition. Based on the
    invertibility of a matrix, if there is a non-zero vector in the null space (the
    set of all vectors that end up as zero) of a matrix, then this matrix is not invertible.
    Therefore, A − λI is not invertible.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one convenient property that connects the determinant of a matrix
    with the invertibility – that is, when a matrix is not invertible, its determinant
    has to be zero. Similarly, if a matrix is invertible, its determinant cannot be
    zero. Thus, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: det(A − λI) = 0
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a system of linear equations, which we can use to solve the values
    of λ, as shown in the previous chapter. Let’s look at a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a 2x2 square matrix, A = [ 0 1 − 2 − 3]. Plugging this into
    the previous equation gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: det([ 0 1 − 2 − 3] − λ[1 0 0 1]) = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'By multiplying the scalar, λ, into the identity matrix, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: det([ 0 1 − 2 − 3] − [ λ 0 0 λ ]) = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining the two matrices gives us this:'
  prefs: []
  type: TYPE_NORMAL
- en: det([ − λ 1 − 2 − 3 − λ]) = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'By applying the definition of the matrix determinant, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: − λ(− 3 − λ) − 1 *(− 2) = 0
  prefs: []
  type: TYPE_NORMAL
- en: λ 2 + 3λ + 2 = 0
  prefs: []
  type: TYPE_NORMAL
- en: (λ + 1)(λ + 2) = 0
  prefs: []
  type: TYPE_NORMAL
- en: λ = − 1, − 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have two solutions: λ = − 1 and λ = − 2\. The next step is to find
    the corresponding eigenvectors for both; we will focus on λ = − 1 in the following
    exposition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the square matrix, A, is 2x2, we know that the eigenvector, v, needs
    to be two-dimensional. By denoting v T = [ v 1, v 2] and plugging λ = − 1 and
    A = [ 0 1 − 2 − 3] into Av = λIv, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ 0 1 − 2 − 3][v 1 v 2] = [− v 1 − v 2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives us the following system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '{ v 2 = − v 1  − 2 v 1 − 3 v 2 = − v 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving this system of equations gives us v 2 = − v 1, which corresponds to
    an infinite number of solutions. This makes sense as the eigenvector focuses more
    on direction instead of absolute magnitude. In this case, the direction is represented
    by a line, y = − x, in a two-dimensional coordinate system. *Figure 8**.6* summarizes
    the process of finding the eigenvalues and eigenvectors of a square matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – The process of deriving the eigenvalues and eigenvectors of
    a square matrix](img/B18680_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – The process of deriving the eigenvalues and eigenvectors of a square
    matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we take v 1 = 1\. The resulting eigenvector becomes v T = [1, − 1].
    When v 1 = 2, we get v T = [2, − 2]. To calculate the eigenvalues and eigenvectors
    via eigen-decomposition, we can simply call the `eigen()` function in R, as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two entries in the `values` attribute and two column vectors in the
    `vectors` attribute, indicating a total of two eigenpairs in the matrix. We can
    access the first eigenvalue as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the eigenvectors are returned as a set of column vectors. Therefore,
    we can access the first eigenvector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We can draw a few useful properties from eigen-decomposition. Take an n × n
    square matrix, A, for example. The number of distinct eigenvalues is, at most,
    n.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also verify the correctness of the resulting eigenvalues and eigenvectors
    by making use of the condition derived earlier: det(A − λI) = 0\. For the first
    eigenvalue and eigenvector, the following code snippet does the verification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also verify the eigen-decomposition based on the original equation –
    that is, Av = λv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, the second command returns a very small number, which is due to numerical
    approximation and can be considered zero. Again, note the use of the scalar-vector
    multiplication sign, `*`, and the matrix-vector multiplication sign, `%*%`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of eigenvalues and eigenvectors, let’s
    look at a popular application: PCA.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building an ML model, the dataset that’s used to train the model may have
    redundant information in the predictors. The redundancy in the predictors/columns
    of the dataset arises from correlated features in the dataset and needs to be
    taken care of when using a certain class of models. In such cases, PCA is a popular
    technique to address such challenges as it reduces the feature dimension of the
    dataset and thus shrinks the redundancy. The problem of **collinearity**, which
    says that two or more predictors are linearly correlated in a model, could thus
    be relieved via dimension reduction using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity among the predictors is often considered a big problem when building
    an ML model. Using the Pearson correlation coefficient, it is a number between
    -1 and 1, where a coefficient near 0 indicates two variables are linearly independent,
    and a coefficient near -1 or 1 indicates that two variables are linearly related.
  prefs: []
  type: TYPE_NORMAL
- en: When two independent variables are linearly correlated, such as x 2 = 2 x 1,
    no extra information is provided by x 2\. The perfect correlation between x 1
    and x 2 makes x 2 useless in terms of explaining the outcome variable. A natural
    choice is to remove x 2 from the set of independent variables. However, when the
    correlation is not perfect, we will lose some amount of information due to the
    removal.
  prefs: []
  type: TYPE_NORMAL
- en: PCA provides us with another way to combat correlation among the predictors.
    It allows us to extract meaningful and uncorrelated information from the original
    dataset. Specifically, it uncovers the hidden and low-dimensional features that
    underlie the dataset. These low-dimensional hidden features make it convenient
    for both visualization and interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: To help us understand this technique, we’ll start by covering the notion of
    the variance-covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the variance-covariance matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All ML models sit on a training dataset. In the context of supervised learning,
    the dataset consists of input-output pairs. The input is also called the design
    matrix, holding n rows of observations and p columns of features. This n × p design
    matrix, X, is our focus of study in PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we would like to know the correlation between each pair of features.
    The correlation coefficient, ranging from -1 to 1, is calculated based on the
    covariance of two variables. The covariance is a scalar value that measures the
    strength of co-movement between two variables. Therefore, the covariance matrix
    of the design matrix measures the strength of co-movement between each unique
    pair of features. It is a p × p square matrix, cov(X), where the entry is located
    at the i th row, and the j th column represents the covariance value between the
    x i and x j features.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain this covariance matrix, we must de-mean all features – that is, subtract
    the column-wise mean from each element in a given column. This removes the central
    tendency and indicates the relative amount of deviation from the mean. This results
    in the de-meaned n × p design matrix, (X − _ X). We must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Transpose the de-meaned design matrix to obtain a p × n matrix, (X − _ X) T.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply the transposed design matrix with the original design matrix, both
    de-meaned, to obtain a p × p square matrix, (X − _ X) T(X − _ X).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the result by n − 1 to normalize the entries in the matrix (instead of
    n used in population covariance).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we’ve done this, a p × p variance-covariance matrix, (X − _ X) T(X − _ X) _ n
    − 1 , is generated, where the i th diagonal element is the variance of the i th
    column in the original design matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s shed some light on the contents of the variance-covariance matrix, (X
    − _ X) T(X − _ X) _ n − 1 . We will start by exposing the entries in this matrix,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (X − _ X) T(X − _ X) ____________ n − 1  = ⎡ ⎢ ⎣ ∑ (x 1 − _ x 1) 2 / (n − 1)
     ⋯ ∑ ( x 1 − _ x 1)( x p − _ x p) / (n − 1)    ⋮ ⋱ ⋮    ∑ ( x p − _ x p)( x 1
    − _ x 1) / (n − 1) ⋯ ∑ (x p − _ x p) 2 / (n − 1)  ⎤ ⎥ ⎦
  prefs: []
  type: TYPE_NORMAL
- en: The first entry, namely ∑ (x 1 − _ x 1) 2 / (n − 1), is the definition of the
    sample variance of the x 1 feature. This comes from the dot product between two
    length-n variables, later divided by n − 1\. So, the summation works on all n
    elements in both column vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving to the rightmost entry on the first row, we have ∑ ( x 1 − _ x 1)( x p
    − _ x p) / (n − 1) as the covariance between the x 1 and x p variables. This is
    exactly how we would calculate the sample covariance between these two variables,
    where the summation is applied to all n elements in both column vectors. *Figure
    8**.7* illustrates the calculation process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Summarizing the calculation process for the variance-covariance
    matrix based on the design matrix in a training dataset](img/B18680_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Summarizing the calculation process for the variance-covariance
    matrix based on the design matrix in a training dataset
  prefs: []
  type: TYPE_NORMAL
- en: Note that although the n × p design matrix, X, is not necessarily square, we
    managed to obtain a p × p square matrix by multiplying the transposed de-meaned
    p × n matrix, (X − _ X) T, with just the de-meaned n × p matrix, (X − _ X).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the variance-covariance of a given matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we first generate a dummy matrix, where the
    second column is produced by doubling the first column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we de-mean both columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, we can calculate the variance-covariance matrix, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is a 2x2 matrix, which aligns with our previous discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also manually calculate the particular variance or covariance
    entries. For example, the following command calculates the variance of the second
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate the covariance between the two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we will connect the variance-covariance matrix to
    PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The variance-covariance matrix, (X − _ X) T(X − _ X) _ n − 1 , from the previous
    section can be used in eigendecomposition, which generates a set of eigenvalues
    along with the associated eigenvectors. Note that these eigenvalues will be real
    scalars, and the associated eigenvectors will be orthogonal to each other, each
    pointing in a distinct direction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few properties of PCA in connection to these eigenvalues and
    eigenvectors. First, the total variance of the dataset is the sum of these eigenvalues.
    Thus, we can rank these eigenvalues in decreasing order, keep only the first few,
    and take the associated eigenvectors as a reduced set of hidden features for downstream
    modeling. By doing this, we can achieve dimension reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, these eigenvectors are called **principal components** and point
    in a particular direction. So, for a specific eigenvector, v i, the direction
    it points in can explain λ i of the total variance of the dataset. Thus, we can
    explain the cumulative variance (as a percentage) by adding up the corresponding
    eigenvalues and deciding the trade off between the number of hidden variables
    to keep and the percentage of the total variance explained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run eigendecomposition on the previous variance-covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The result shows that the first eigenvalue significantly dominates the second
    one in terms of the values that are output, showing that the first eigenvector
    is sufficient to explain the total variance of the original design matrix. This
    makes sense as the second variable is simply double the first variable and thus
    delivers no additional information. Although the dataset has two columns, it only
    has one column’s worth of information and the second column is completely redundant.
    The second eigenvector also lies along the same line as the first one, thus having
    no variability in the eigenspace.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll introduce a function for performing PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Performing PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One function we can use to perform PCA is `prcomp()`. For our exercise, we
    will use the first four columns of the Iris dataset to perform PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `prcomp()` function will automatically perform all the aforementioned steps
    involved in eigendecomposition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the variances that are explained – in other words, eigenvalues – are
    expressed in terms of standard deviations instead of variance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have access to the PCA results, we can visualize these results to
    make them more intuitive. For this purpose, we will use the `factoextra` package.
    Remember to install and load this package first.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first function we will use is the `fviz_eig()` function, which shows the
    percentage of variance explained by each principal component in a scree plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this command generates *Figure 8**.8*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Visualizing the PCA results in a scree plot](img/B18680_08_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Visualizing the PCA results in a scree plot
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also show a graph for the individual observations using the `fviz_pca_ind()`
    function, where individuals with similar profiles are grouped:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this command generates *Figure 8**.9*. Note that observations with
    lower quality (less variance explained) are assigned with colors toward the lower
    half of the palette:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Visualizing individual contributions in explaining the total
    variance](img/B18680_08_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Visualizing individual contributions in explaining the total variance
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can also visualize the directions of the variables. Positively correlated
    variables will point to the same side of the plot, while negatively correlated
    variables will point to opposite sides of the graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Running this command generates *Figure 8**.10*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Visualizing variable directions](img/B18680_08_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Visualizing variable directions
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that `Petal.Width` and `Petal.Length` point in almost overlapping
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered intermediate linear algebra and its implementations
    in R. We started by introducing the matrix determinant, a widely used property
    in numerical analysis. We highlighted the intuition behind the matrix determinant
    and its connection to matrix rank.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also covered additional properties, including matrix trace and norm. In
    particular, we introduced three popular norms: L 1-norm, L 2-norm, and L ∞-norm.
    We detailed their mathematical constructs and calculation process.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we covered eigendecomposition, which leads to a set of eigenvalues and
    eigenvectors of a square matrix. We provided a step-by-step derivation and analysis
    of the core equation, as well as the approach to compute them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered PCA, a popular technique that’s used for dimension reduction.
    Specifically, we highlighted its role in removing collinearity in the dataset
    and provided a few ways to compute and visualize PCA results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will switch gears and cover another key branch of mathematics:
    calculus.'
  prefs: []
  type: TYPE_NORMAL
