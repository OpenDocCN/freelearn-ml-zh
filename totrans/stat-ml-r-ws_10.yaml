- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Intermediate Linear Algebra in R
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的中级线性代数
- en: The previous chapter covered the basics of linear algebra and its calculations
    in R. This chapter will go a step further by extending to intermediate linear
    algebra and cover topics such as the determinant, rank, and trace of a matrix,
    eigenvalues and eigenvectors, and **principal component analysis** (**PCA**).
    Besides providing an intuitive understanding of these abstract yet important mathematical
    concepts, we’ll cover the practical implementations of calculating these quantities
    in R.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章介绍了线性代数的基础及其在R中的计算。本章将进一步扩展到中级线性代数，并涵盖行列式、秩和迹、特征值和特征向量以及**主成分分析**（**PCA**）等主题。除了提供对这些抽象但重要的数学概念的直观理解外，我们还将涵盖在R中计算这些量的实际应用。
- en: By the end of this chapter, you will have grasped important matrix properties,
    such as determinant and rank, and gained hands-on experience in calculating these
    quantities.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将掌握重要的矩阵性质，如行列式和秩，并在计算这些量方面获得实践经验。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the matrix determinant
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍矩阵行列式
- en: Introducing the matrix trace
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍矩阵迹
- en: Understanding the matrix norm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解矩阵范数
- en: Getting to know eigenvalues and eigenvectors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解特征值和特征向量
- en: Introducing principal component analysis
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍主成分分析
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To run the code in this chapter, you will need to have the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码，你需要以下要求：
- en: The latest version of the `Matrix` package, which is 1.5.1 at the time of writing
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Matrix`包的最新版本，写作时为1.5.1'
- en: The latest version of the `factoextra` package, which is 1.0.7 at the time of
    writing
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factoextra`包的最新版本，写作时为1.0.7'
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码和数据均可在[https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_8/working.R)找到。
- en: Introducing the matrix determinant
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍矩阵行列式
- en: The **determinant** of a matrix is a special scalar value that can be calculated
    from a matrix. Here, the matrix needs to be square, meaning it has an equal number
    of rows and columns. For a 2x2 square matrix, the determinant is simply calculated
    as the difference between the product of the diagonal elements and the off-diagonal
    elements.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的**行列式**是一个可以从矩阵中计算出的特殊标量值。在这里，矩阵必须是方阵，即行数和列数相等。对于一个2x2的方阵，行列式简单地计算为对角线元素乘积与非对角线元素乘积的差。
- en: 'Mathematically, suppose our 2x2 matrix is A = [a b c d ]. Its determinant,
    |A|, is thus calculated as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，假设我们的2x2矩阵是A = [a b c d ]。因此，其行列式|A|的计算如下：
- en: det(A) = |A| = ad − bc
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: det(A) = |A| = ad − bc
- en: Please do not confuse these vertical lines with the absolute operation sign.
    They represent the determinant in the context of a matrix, and the determinant
    of a matrix can be negative as well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要将这些垂直线与绝对值操作符混淆。它们代表矩阵上下文中的行列式，矩阵的行列式也可以是负数。
- en: 'Let’s say our 2x2 matrix is A = [2 6 1 8]. We can find its determinant like
    so:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的2x2矩阵是A = [2 6 1 8]。我们可以这样找到它的行列式：
- en: '|A| = 2 * 8 − 6 * 1 = 10'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|A| = 2 * 8 − 6 * 1 = 10'
- en: Calculating the determinant of a matrix is the easy part, but understanding
    its use is of equal importance. Before we cover its properties, first, we’ll review
    the calculation in R to get a straightforward understanding of the scalar output
    value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算矩阵的行列式是容易的部分，但理解其用途同样重要。在我们介绍其性质之前，首先，我们将回顾在R中的计算方法，以获得对标量输出值的直观理解。
- en: 'In the following code snippet, we are creating a matrix, A, from a vector with
    proper configurations (two rows, filling by row). As usual, we verify the content
    in the matrix by printing it out to the console. We then call the `det()` function
    to calculate its determinant:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们从一个向量创建矩阵A，并使用适当的配置（两行，按行填充）。像往常一样，我们通过在控制台打印矩阵内容来验证其内容。然后我们调用`det()`函数来计算其行列式：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There is also a corresponding formula for calculating the determinant of a 3x3
    matrix or even higher dimension. We will not entertain these cases here as understanding
    the properties of the determinant is more important at this stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the determinant
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall that any matrix can be thought of as a transformation or projection
    that changes the input from one space to another. There are two things to note
    for such a change: *quantity* and *direction*. Quantity measures the percentage
    change in the magnitude of the original size of the matrix, while the direction
    indicates the sign of the transformation, taking either a positive or a negative
    value. Here, the matrix size can be considered as the area of a 2x2 matrix or
    the volume of a 3x3 matrix.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The columns in the matrix represent the collection of linear transformation
    that either stretches or squishes the space of the original input, thus changing
    the size of the matrix. So, the determinant measures how much the collection of
    linear transformations stretches or squishes the input. It gives a factor by which
    the area or volume of a region increases or decreases. Also, since directionality
    matters, the change may flip the input, as indicated by a negative determinant.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example. Suppose we have a 2x2 input matrix, [1 0 0 1], and
    we would like to transform it via another 2x2 matrix, [3 0 0 2]. A direct multiplication
    gives an output of [3 0 0 2], which can be verified by carrying out the matrix
    multiplication rule:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[1 0 0 1][3 0 0 2] = [3 0 0 2]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: There is no change to the output since the input matrix is essentially an identity
    matrix, and we know that any matrix multiplied by an identity matrix remains unchanged.
    No surprise here. However, when viewing [1 0 0 1] as the input matrix, with [3 0 0 2]
    on the left as the transformation matrix and [3 0 0 2] on the right as the output
    matrix, we can see that the transformation matrix increases the area of the input
    matrix by a factor of six.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: To see this, imagine the input matrix, [1 0 0 1], on a two-dimensional coordinate
    system. The area of the input matrix is 1 * 1 = 1, while the area of the output
    matrix is 3 * 2 = 6, which happens to be the determinant of the transformation
    matrix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not a coincidence. The net effect of the transformation matrix is thus
    to magnify the area of the input matrix by 6, maintaining the same direction.
    And it is not difficult to obtain the same increase in area in a negative direction
    when changing the transformation matrix to [− 3 0 0 2] or [3 0 0 − 2]:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Illustrating the effect of the matrix determinant in determining
    the change in the area of the input matrix](img/B18680_08_001.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Illustrating the effect of the matrix determinant in determining
    the change in the area of the input matrix
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 8**.1* summarizes this important property.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Connection to the matrix rank
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rank of a matrix, A, is the maximal number of linearly independent columns
    in the matrix. This number has a connection to the determinant of a matrix. Specifically,
    the rank is the number of rows (or columns) of the largest square submatrix of
    A such that its determinant is nonzero.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵A的秩是矩阵中线性无关列的最大数量。这个数字与矩阵的行列式有关。具体来说，秩是A中最大的平方子矩阵的行数（或列数），其行列式不为零。
- en: 'Let’s look at an example. Suppose A is a 2x3 matrix, [1 2 3 3 2 4]. First,
    we find the largest square submatrix, which is [1 2 3 2] or [2 3 2 4]. Both matrices
    have a nonzero determinant. Thus, the rank of A is. This means we can use this
    technique to find the rank of a matrix. *Figure 8**.2* summarizes this approach:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。假设A是一个2x3矩阵，[1 2 3 3 2 4]。首先，我们找到最大的平方子矩阵，即[1 2 3 2]或[2 3 2 4]。这两个矩阵的行列式都不为零。因此，A的秩是。这意味着我们可以使用这种技术来找到矩阵的秩。*图8**.2*总结了这种方法：
- en: '![Figure 8.2 – Deriving the rank of a matrix using the determinant](img/B18680_08_002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 使用行列式推导矩阵的秩](img/B18680_08_002.jpg)'
- en: Figure 8.2 – Deriving the rank of a matrix using the determinant
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 使用行列式推导矩阵的秩
- en: 'Let’s look at how to obtain the rank of a matrix computationally:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算矩阵的秩：
- en: For this, we need to load the `Matrix` package in R and call the `rankMatrix()`
    function.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要在R中加载`Matrix`包并调用`rankMatrix()`函数。
- en: 'As shown in the following code snippet, first, we create the 3x2 matrix, A,
    and print it out. When designing this matrix, we simply fill in a vector that
    consists of the row-wise concatenation of A:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如以下代码片段所示，首先，我们创建3x2矩阵A并打印出来。在设计这个矩阵时，我们只是简单地填充一个由A的行拼接而成的向量：
- en: '[PRE1]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we call the `rankMatrix()` function to obtain its rank:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们调用`rankMatrix()`函数来获取它的秩：
- en: '[PRE2]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Multiple attributes are returned. We can access the first attribute as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回多个属性。我们可以如下访问第一个属性：
- en: '[PRE3]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next section, we will look at another important property of a matrix:
    the trace.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨矩阵的另一个重要属性：迹。
- en: Introducing the matrix trace
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵迹的介绍
- en: 'The **trace** is a quantity that only applies to square matrices, such as the
    covariance matrix often encountered in ML. It is denoted as tr(A) for a square
    matrix, A, and is calculated as the sum of the diagonal elements in a square matrix.
    Let’s take a look:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**迹**只适用于方阵，例如在机器学习中经常遇到的协方差矩阵。它用tr(A)表示一个方阵A，并计算为方阵中所有对角元素的和。让我们看看：'
- en: 'In the following code snippet, we are creating a 3x3 matrix, A, and using the
    `diag()` function to extract the diagonal elements and sum them up to obtain the
    trace of the matrix. Note that we first create a DataFrame consisting of three
    columns, each having three elements, and then convert it into a matrix format
    to store in `A`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们创建了一个3x3矩阵A，并使用`diag()`函数提取对角元素并将它们相加以获得矩阵的迹。请注意，我们首先创建一个包含三个列，每列有三个元素的DataFrame，然后将它转换为矩阵格式存储在`A`中：
- en: '[PRE4]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since there is no built-in function that can calculate the trace in one shot,
    we can build a customized one to perform this task. As shown in the following
    code snippet, the customized function named `trace()` essentially loops through
    all the diagonal elements of the input square matrix and adds them up as the return
    value:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于没有内置函数可以一次性计算迹，我们可以构建一个自定义函数来执行此任务。如下面的代码片段所示，自定义函数`trace()`本质上遍历输入方阵的所有对角元素并将它们相加作为返回值：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Testing out this function gives us the same trace as before:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试这个函数给出了与之前相同的迹：
- en: '[PRE6]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There are some interesting properties regarding the trace of the matrix, as
    we’ll see in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于矩阵的迹有一些有趣的属性，我们将在下一节中看到。
- en: Special properties of the matrix trace
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵迹的特殊属性
- en: 'To illustrate these properties, we’ll first create another matrix, B:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些属性，我们首先创建另一个矩阵B：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will introduce five properties that are commonly used in statistical modeling.
    All of these properties will be verified in our example with matrices A and B:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍五个在统计建模中常用到的属性。所有这些属性都将通过我们用矩阵A和B的例子进行验证：
- en: '**Property 1**: The trace of the sum of two square matrices is the sum of the
    traces of the two matrices:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性1**：两个平方矩阵之和的迹是这两个矩阵迹的和：'
- en: tr(A + B) = tr(A) + tr(B)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: tr(A + B) = tr(A) + tr(B)
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we use the equality sign to check whether the left-hand side is equal
    to the right-hand side. A return of `TRUE` means that the property has been verified.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用等号来检查左边是否等于右边。返回 `TRUE` 表示属性已被验证。
- en: '**Property 2**: The trace of a matrix is equal to the trace of the matrix’s
    transpose:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性 2**：矩阵的迹等于矩阵转置的迹：'
- en: tr(A) = tr(A T)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: tr(A) = tr(A T)
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that we used the `t()` function to obtain the transpose of a matrix:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在使用 `t()` 函数来获取矩阵的转置时：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Property 3**: For a matrix multiplied by a scalar value, its trace is the
    same as the original trace multiplied by the same scalar:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性 3**：对于一个乘以标量值的矩阵，其迹与原始迹乘以相同的标量相同：'
- en: tr(αA) = αtr(A)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: tr(αA) = αtr(A)
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, we chose a scalar coefficient of `2`. Feel free to change this value and
    verify whether the property still holds.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们选择了一个标量系数 `2`。您可以随意更改此值并验证属性是否仍然成立。
- en: '**Property 4**: The trace is cyclical:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性 4**：迹是循环的：'
- en: tr(AB) = tr(BA)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: tr(AB) = tr(BA)
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This property says that when multiplying A by B, the trace of the resulting
    matrix is the same as the one from multiplying B by A.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性表明，当我们乘以 A 和 B 时，结果矩阵的迹与乘以 B 和 A 的迹相同。
- en: Note the use of the `%*%` notation when performing matrix multiplication.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在执行矩阵乘法时使用 `%*%` 符号。
- en: '**Property 5**: The trace is invariant:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**属性 5**：迹是不变的：'
- en: tr(A) = tr(BA B −1)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: tr(A) = tr(BA B −1)
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This property says that if we multiply B by A, and then multiply the inverse,
    B −1, the trace of the resulting matrix is the same as the trace of matrix A.
    *Figure 8**.3* summarizes these five properties:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个属性表明，如果我们先乘以 B 和 A，然后乘以逆矩阵 B −1，结果矩阵的迹与矩阵 A 的迹相同。*图 8**.3* 总结了这五个属性：
- en: '![Figure 8.3 – Five properties of matrix trace](img/B18680_08_003.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 矩阵迹的五个属性](img/B18680_08_003.jpg)'
- en: Figure 8.3 – Five properties of matrix trace
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 矩阵迹的五个属性
- en: 'In the next section, we will cover another important summary measure of a matrix:
    the matrix norm.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍矩阵的另一个重要汇总度量：矩阵范数。
- en: Understanding the matrix norm
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解矩阵范数
- en: The **norm** of a matrix is a scalar value that measures the magnitude of the
    matrix. Therefore, the norm is a way to measure the size or length of a vector
    or a matrix. For example, the weights of a deep neural network are stored in matrices,
    and we would typically constrain the norm of the weights to be small to prevent
    overfitting. This allows us to quantify the magnitude, which is useful when comparing
    different vectors or matrices, which often consist of multiple elements. As it
    generalizes from the vector norm, we will first go through the basics of the vector
    norm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的**范数**是一个标量值，用于衡量矩阵的大小。因此，范数是衡量向量或矩阵的大小或长度的方法。例如，深度神经网络的权重存储在矩阵中，我们通常会约束权重的范数以保持较小，以防止过拟合。这使我们能够量化大小，这在比较由多个元素组成的不同向量或矩阵时很有用。由于它从向量范数推广而来，我们将首先介绍向量范数的基本知识。
- en: Understanding the vector norm
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解向量范数
- en: Suppose we have a vector, a = [1,0, − 1], and another vector, b = [1,2,0]. To
    assess the similarity between these two vectors, we can argue that they are the
    same in the first element only and different for the remaining two elements. To
    compare these two vectors holistically, we need a single metric – one that summarizes
    the whole vector. The norm is one way to go forward.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个向量，a = [1,0, − 1]，另一个向量，b = [1,2,0]。为了评估这两个向量之间的相似性，我们可以认为它们在第一个元素上是相同的，而在剩余的两个元素上不同。为了全面比较这两个向量，我们需要一个单一的度量标准——一个可以总结整个向量的度量标准。范数是前进的一种方式。
- en: 'There are different norms for an arbitrary vector of length n. All forms come
    from the following generalized form of L p-norm:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于长度为 n 的任意向量，有不同的范数。所有形式都来自以下 L p-范数的广义形式：
- en: ‖x‖ p = (∑ i=1 n |x i| p) 1/p
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ‖x‖ p = (∑ i=1 n |x i| p) 1/p
- en: Here, the double vertical bars denote the norm. This is called the L p-norm
    because p is used as a placeholder to represent the specific type of norm. Common
    values of p include 1, 2, and ∞, although theoretically, it could take on any
    positive integer value. For example, to calculate the L 1-norm of the vector,
    we can simply plug p = 1 into the formula.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，双竖线表示范数。这被称为 L p-范数，因为 p 用作占位符来表示特定的范数类型。p 的常见值包括 1、2 和 ∞，尽管理论上它可以取任何正整数值。例如，为了计算向量的
    L 1-范数，我们可以简单地将在公式中 p = 1。
- en: We’ll go through these common norms in the following sections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中介绍这些常见的范数。
- en: Calculating the L 1-norm of a vector
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算向量的 L 1-范数
- en: 'Substituting p = 1 in the previous equation gives us the L 1-norm:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将前一个方程中的p替换为1，我们得到L1-范数：
- en: ‖x‖ 1 = ∑ i=1 n |x i|
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \(||x||_1 = \sum_{i=1}^{n} |x_i|\)
- en: This can be considered as the total length of the vector, x, which is calculated
    as the sum of the absolute values of all entries in the vector. When we have a
    two-element vector, x = [x 1, x 2], the L 1-norm will be ‖x‖ 1 = |x 1| + |x 2|.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被认为是向量x的总长度，它是向量中所有项的绝对值之和。当我们有一个二维向量，\(x = [x_1, x_2]\)，L1-范数将是\(||x||_1
    = |x_1| + |x_2|\)。
- en: 'First, let’s create a 3x1 matrix to represent a column vector:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个3x1矩阵来表示一个列向量：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can call the `norm()` function to calculate the L 1-norm in R, as shown
    in the following code snippet:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在R中使用`norm()`函数来计算L1-范数，如下所示：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that the `norm()` function calculates the L 1-norm by default. To set
    the type of norm explicitly, we can pass in the `type="1"` argument, as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`norm()`函数默认计算L1-范数。要明确设置范数类型，我们可以传递`type="1"`参数，如下所示：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, we will move on to the L 2-norm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续讨论L2-范数。
- en: Calculating the L 2-norm of a vector
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算向量的L2-范数
- en: 'The L 2-norm is the most common type of norm we usually work with. Also called
    the Euclidean norm, the L 2-norm measures the usual distance between two points.
    Plugging p = 2 into the previous formula gives us the following definition of
    the L 2-norm:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: L2-范数是我们通常使用的最常见范数。也称为欧几里得范数，L2-范数衡量两点之间的通常距离。将p替换为2到前一个公式中，我们得到以下L2-范数的定义：
- en: ‖x‖ 2 = √ _ ∑ i=1 n |x i| 2  = √ _ ∑ i=1 n x i 2
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \(||x||_2 = \sqrt{\sum_{i=1}^{n} |x_i|^2} = \sqrt{\sum_{i=1}^{n} x_i^2}\)
- en: The calculation process involves squaring each entry, adding them up, and then
    taking the square root. Similarly, when we have a two-element vector, x = [x 1,
    x 2], the L 2-norm will be ‖x‖ 2 = √ _ x 1 2 + x 2 2 .
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 计算过程涉及对每个元素进行平方，将它们相加，然后取平方根。同样，当我们有一个二维向量，\(x = [x_1, x_2]\)，L2-范数将是\(||x||_2
    = \sqrt{x_1^2 + x_2^2}\)。
- en: 'We can calculate the L 2-norm of a vector by specifying `type="2"`, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过指定`type="2"`来计算向量的L2-范数，如下所示：
- en: '[PRE17]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next comes the max norm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是最大范数。
- en: Calculating the L ∞-norm of a vector
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算向量的L∞-范数
- en: 'The L ∞-norm, or **max norm**, finds the largest absolute value of all the
    elements within the vector. This norm is often used in worse-case scenarios –
    for example, representing the maximum noise injected into a signal. Its definition
    is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: L∞-范数，或称为**最大范数**，找出向量中所有元素的最大绝对值。这种范数常用于最坏情况场景——例如，表示信号中注入的最大噪声。其定义如下：
- en: ‖x‖ ∞ = max 1≤i≤n | x i|
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \(||x||_{\infty} = \max_{1 \leq i \leq n} |x_i|\)
- en: So, the calculation process involves pairwise comparisons of absolute values
    in search of the maximum.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，计算过程涉及成对比较绝对值以寻找最大值。
- en: 'To calculate the L ∞-norm, we can specify `type="2"`, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算L∞-范数，我们可以指定`type="2"`，如下所示：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we’ll move on to the matrix norm.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续讨论矩阵范数。
- en: Understanding the matrix norm
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解矩阵范数
- en: 'The `norm()` function to do the job. We will use X to denote a general matrix,
    m × n, where X ij denotes the element located at the i th row and j th column.
    All forms of matrix norm come from the following generalized form of L p-norm:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`norm()`函数来完成这项工作。我们将使用X表示一个m×n的一般矩阵，其中\(X_{ij}\)表示位于第i行第j列的元素。所有矩阵范数形式都来源于以下Lp-范数的推广形式：'
- en: ‖X‖ p = (∑ i=1 m ∑ j=1 n |X ij| p) 1/p
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \(||X||_p = \left(\sum_{i=1}^{m} \sum_{j=1}^{n} |X_{ij}|^p\right)^{1/p}\)
- en: 'First, let’s create a 3x3 matrix:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个3x3矩阵：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, we will look at the L 1-norm of the X matrix.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看X矩阵的L1-范数。
- en: Calculating the L 1-norm of a matrix
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算矩阵的L1-范数
- en: 'The L 1-norm for a matrix is similar to its vector form but slightly different.
    As shown here, to calculate the L 1-norm for a matrix, we must first sum the absolute
    values of each column, then take the largest summation as the L 1-norm:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的L1-范数与其向量形式类似，但略有不同。如图所示，为了计算矩阵的L1-范数，我们必须首先对每一列的绝对值进行求和，然后将最大的求和值作为L1-范数：
- en: ‖X‖ 1 = max 1≤j≤n ∑ i=1 m | X ij|
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: \(||X||_1 = \max_{1 \leq j \leq n} \sum_{i=1}^{m} |X_{ij}|\)
- en: Since the summation is performed column-wise, the L 1-norm for a matrix is also
    called the column-sum norm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于求和是按列进行的，因此矩阵的L1-范数也称为列和范数。
- en: 'We can calculate the L 1-norm using the same command that we used earlier:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用之前使用的相同命令来计算L1-范数：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A visual inspection shows that the third column gives the maximum summation
    of absolute values of 12\. We can also quickly check the column-wise summation
    of the absolute values of the matrix, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过视觉检查，我们可以看到第三列给出了绝对值求和的最大值12。我们也可以快速检查矩阵的列绝对值求和，如下所示：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Calculating the Frobenius norm of a matrix
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算矩阵的Frobenius范数
- en: 'The L 2-norm of a matrix is more involved at this stage, so we will focus on
    a similar kin that is widely used in practice: the **Frobenius norm**. The Frobenius
    norm is calculated by summing all squared entries of the matrix and taking the
    square root:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的L2范数在这个阶段更为复杂，所以我们将关注一个在实践中广泛使用的类似概念：**Frobenius范数**。Frobenius范数是通过计算矩阵中所有元素的平方和然后取平方根得到的：
- en: ‖X‖ F = √ _ ∑ i=1 m ∑ j=1 n |X ij| 2
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ‖X‖F = √∑ i=1 m ∑ j=1 n |Xij|²
- en: 'We can calculate the Frobenius norm by setting `type="f"`, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置`type="f"`来计算Frobenius范数，如下所示：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s verify the calculations by carrying out the manual process of squaring
    all entries, summing them up, and taking the square root:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过手动平方所有项、求和并取平方根的过程来验证计算：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we will look at the infinity norm of a matrix.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将研究矩阵的无限范数。
- en: Calculating the infinity norm of a matrix
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算矩阵的无限范数
- en: 'The **infinity norm** of a matrix works similarly to the L 1-norm of a matrix,
    although the order of sequence is different. In particular, we would sum up the
    absolute values for each row and return the largest summation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的**无限范数**与矩阵的L1范数类似，尽管序列的顺序不同。特别是，我们将对每一行的绝对值进行求和，然后返回最大的求和值：
- en: ‖X‖ ∞ = max 1≤j≤m ∑ i=1 n | X ij|
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ‖X‖∞ = max 1≤j≤m ∑ i=1 n |Xij|
- en: 'Thus, the infinity norm is also referred to as the `type="I"` in the `norm()`
    function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，无限范数也被称为`norm()`函数中的`type="I"`：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Again, it is a good habit to verify the result by manually carrying out the
    calculation process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，手动执行计算过程以验证结果是一个好习惯：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*Figure 8**.4* summarizes these three norms for both vectors and matrices:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.4* 总结了这些三种范数，包括向量和矩阵：'
- en: '![Figure 8.4 – Common norms for vectors and matrices](img/B18680_08_004.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 向量和矩阵的常见范数](img/B18680_08_004.jpg)'
- en: Figure 8.4 – Common norms for vectors and matrices
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 向量和矩阵的常见范数
- en: 'Having covered these fundamentals, let’s move on to the next important topic:
    eigenvalues and eigenvectors.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了这些基础知识之后，让我们继续下一个重要主题：特征值和特征向量。
- en: Getting to know eigenvalues and eigenvectors
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解特征值和特征向量
- en: 'The **eigenvalue**, often denoted by a scalar value of λ, and the **eigenvector**,
    often denoted by v, are essential properties of a square matrix, A. Two central
    ideas are required to understand the purpose of eigenvalues and eigenvectors.
    The first is that the matrix, A, is a transformation that maps one input vector
    to another output vector, which possibly changes the direction. The second is
    that the eigenvector is a special vector that does not change direction after
    going through the transformation induced by A. Instead, the eigenvector gets scaled
    along the same original direction by a multiple of the corresponding scalar eigenvalue.
    The following equation sums this up:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征值**，通常用标量λ表示，和**特征向量**，通常用v表示，是方阵A的基本属性。理解特征值和特征向量的目的需要两个核心思想。第一个思想是矩阵A是一个将一个输入向量映射到另一个输出向量的变换，这可能会改变方向。第二个思想是特征向量是一个特殊的向量，在经过A诱导的变换后不会改变方向。相反，特征向量沿着原始方向被相应的标量特征值的倍数缩放。以下方程总结了这一点：'
- en: Av = λv
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Av = λv
- en: 'These two points capture the essence of **eigendecomposition**, which represents
    the original matrix, A, in terms of its eigenvalues and eigenvectors and thus
    allows easier matrix operations in many cases. Let’s start by understanding a
    simple case: **scalar-vector multiplication**.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这两点概括了**特征分解**的精髓，它用特征值和特征向量来表示原始矩阵A，从而在许多情况下简化矩阵运算。让我们从一个简单的案例开始理解：**标量-向量乘法**。
- en: Understanding scalar-vector multiplication
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解标量-向量乘法
- en: Matrix-vector multiplication can result in many forms of transformations, such
    as rotation, reflection, dilation, contraction, projection, and a combination
    of these operations. With eigenvalues and eigenvectors, we can decompose these
    operations into a series of simpler ones. For the case of scalar-vector multiplication,
    when the scalar value is in the range of 0 and 1, this will make the elements
    of the vector smaller, thus *contracting* the vector.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法可以导致多种形式的变换，如旋转、反射、膨胀、收缩、投影以及这些操作的组合。有了特征值和特征向量，我们可以将这些操作分解成一系列更简单的操作。对于标量-向量乘法的案例，当标量值在0和1之间时，这将使向量的元素变小，从而*收缩*向量。
- en: 'Suppose we want to multiply a vector, v, by a scalar, λ, giving us λv. Since
    v contains one or more elements, multiplication essentially applies to each element
    in the vector. The following code snippet shows the result of multiplying a scalar
    by a vector, where each of the elements gets doubled:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要将一个向量v乘以一个标量λ，得到λv。由于v包含一个或多个元素，乘法本质上应用于向量的每个元素。以下代码片段显示了标量乘以向量的结果，其中每个元素都加倍：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now comes a key technique: introducing the identity matrix, I. Since the vector
    has three elements, we can introduce a 3x3 identity matrix into the equation.
    In the previous chapter, we learned that multiplying an identity matrix will not
    change the result, so this is something we can proceed with:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在介绍一个关键技术：引入单位矩阵I。由于向量有三个元素，我们可以将一个3x3的单位矩阵引入方程中。在上一章中，我们了解到乘以单位矩阵不会改变结果，因此我们可以继续这样做：
- en: λv = λIv
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: λv = λIv
- en: 'Here, we first create a 3x3 identity matrix, I, using the `diag()` function,
    as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先使用`diag()`函数创建一个3x3的单位矩阵I，如下所示：
- en: '[PRE27]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Multiplying it by the scalar, λ, changes all diagonal elements to 2 in λI:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以标量λ，将λI的对角线元素都变为2：
- en: '[PRE28]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since λI is a 3x3 matrix, the previous scalar-vector multiplication becomes
    matrix-vector multiplication now. This means that we need to switch to the `%*%`
    sign to perform the inner multiplication:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于λI是一个3x3矩阵，之前的标量-向量乘法现在变成了矩阵-向量乘法。这意味着我们需要切换到`%*%`符号来执行内积运算：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The result is the same as the previous scalar-vector product, although it is
    now expressed as a column vector instead of a row vector. From the perspective
    of matrix-vector multiplication, the matrix *transforms* the vector by doubling
    every element in the vector.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与之前的标量-向量积相同，尽管现在它被表示为一个列向量而不是行向量。从矩阵-向量乘法的角度来看，矩阵通过将向量中的每个元素加倍来*变换*向量。
- en: Next, we will formally define the notion of eigenvalues and eigenvectors.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将正式定义特征值和特征向量的概念。
- en: Defining eigenvalues and eigenvectors
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义特征值和特征向量
- en: 'By introducing the identity matrix, we managed to convert a scalar-vector multiplication,
    λv, into a matrix-vector multiplication, λIv. This makes all the difference in
    understanding the key equation: Av = λv, where the left is a matrix-vector multiplication
    and the right is a scalar-vector multiplication. By writing Av = λIv, this equation
    suddenly makes more sense as it has the same type of operation on both sides.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入单位矩阵，我们成功地将标量-向量乘法λv转换成了矩阵-向量乘法λIv。这使理解关键方程Av = λv变得至关重要，其中左侧是矩阵-向量乘法，右侧是标量-向量乘法。通过写成Av
    = λIv，这个方程突然在两侧具有相同类型的操作，因此更有意义。
- en: Now, we must define λ and v to give them proper names. For a square matrix,
    A, we say that the scalar, λ, is an eigenvalue of A, together with an associated
    eigenvector, v ≠ 0, if Av = λv is true. This equation says that the matrix-vector
    multiplication in Av produces the same vector as the scalar-vector multiplication.
    λ and v together are called an eigenpair.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须定义λ和v，给它们合适的名字。对于一个方阵A，我们说标量λ是A的特征值，与一个相关的特征向量v ≠ 0相关联，如果Av = λv成立。这个方程说明矩阵-向量乘法在Av中产生的向量与标量-向量乘法相同。λ和v一起被称为特征对。
- en: 'We can quickly verify the equality mentioned in Av = λv. Suppose A = [2 3 0 1],
    λ = 2, and v T = [1,0]. The calculation on the left-hand side gives us the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速验证Av = λv中提到的等式。假设A = [2 3 0 1]，λ = 2，v T = [1,0]。左侧的计算给出以下结果：
- en: Av = [2 3 0 1][1 0] = [2 0]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Av = [2 3 0 1][1 0] = [2 0]
- en: 'The calculation on the right-hand side gives us the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的计算给出以下结果：
- en: λv = 2[1 0] = [2 0]
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: λv = 2[1 0] = [2 0]
- en: Therefore, the equality checks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，等式成立。
- en: Geometrically, the eigenvector is a vector that stays fixed in its original
    direction when we apply a matrix transformation by A. It stays on the same line
    and remains invariant upon multiplication. Also, there is often a collection of
    such eigenvectors (with the corresponding eigenvalues) for a square matrix.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 几何上，特征向量是在我们对矩阵A应用矩阵变换时保持其原始方向的向量。它在同一直线上，并且在乘法后保持不变。对于方阵，通常有一组这样的特征向量（以及相应的特征值）。
- en: 'The following code snippet verifies the same:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段验证了相同的结果：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that the eigenvector focuses entirely on the *direction* of the transformation-invariant
    vector, rather than the magnitude. To see this, we can double the eigenvector
    and find that the equality still checks:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征向量完全关注于变换不变向量的**方向**，而不是其大小。为了看到这一点，我们可以将特征向量加倍，并发现等式仍然成立：
- en: Av = [2 3 0 1][2 0] = [4 0]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Av = [2 3 0 1][2 0] = [4 0]
- en: λv = 2[2 0] = [4 0]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: λv = 2[2 0] = [4 0]
- en: 'We can verify the equality by taking their difference:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过取它们的差来验证等式：
- en: '[PRE31]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*Figure 8**.5* summarizes our understanding so far:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8**.5* 总结了我们到目前为止的理解：'
- en: '![Figure 8.5 – Summarizing our understanding of eigenvalues and eigenvectors](img/B18680_08_005.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 总结我们对特征值和特征向量的理解](img/B18680_08_005.jpg)'
- en: Figure 8.5 – Summarizing our understanding of eigenvalues and eigenvectors
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 总结我们对特征值和特征向量的理解
- en: Next, we’ll look at how to compute the eigenvalues and eigenvectors of a square
    matrix.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨如何计算方阵的特征值和特征向量。
- en: Computing eigenvalues and eigenvectors
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算特征值和特征向量
- en: The previous examples assume that we have access to the eigenvalues and eigenvectors.
    In practice, these need to be computed from the original square matrix. This section
    focuses on how to obtain the solutions to the eigenvalues and eigenvectors.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子假设我们能够访问特征值和特征向量。在实践中，这些需要从原始方阵中计算出来。本节重点介绍如何获得特征值和特征向量的解。
- en: Let’s start from where we left off. By introducing the identity matrix, we managed
    to obtain Av = λIv. Moving things around, we have Av − λIv = 0\. Combining similar
    terms gives us (A − λI)v = 0\. We know that v ≠ 0 by definition. Based on the
    invertibility of a matrix, if there is a non-zero vector in the null space (the
    set of all vectors that end up as zero) of a matrix, then this matrix is not invertible.
    Therefore, A − λI is not invertible.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们上次停止的地方开始。通过引入单位矩阵，我们设法获得了 Av = λIv。重新排列这些项，我们得到 Av − λIv = 0。根据定义，我们知道
    v ≠ 0。根据矩阵的可逆性，如果一个矩阵的零空间（所有最终变为零的向量的集合）中存在非零向量，那么这个矩阵是不可逆的。因此，A − λI是不可逆的。
- en: 'There is one convenient property that connects the determinant of a matrix
    with the invertibility – that is, when a matrix is not invertible, its determinant
    has to be zero. Similarly, if a matrix is invertible, its determinant cannot be
    zero. Thus, we have the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 存在一个方便的性质将矩阵的行列式与可逆性联系起来——那就是，当一个矩阵不可逆时，它的行列式必须为零。同样，如果一个矩阵是可逆的，它的行列式不能为零。因此，我们有以下结果：
- en: det(A − λI) = 0
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: det(A − λI) = 0
- en: This gives us a system of linear equations, which we can use to solve the values
    of λ, as shown in the previous chapter. Let’s look at a concrete example.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个线性方程组，我们可以使用它来求解λ的值，正如前一章所示。让我们看一个具体的例子。
- en: 'Suppose we have a 2x2 square matrix, A = [ 0 1 − 2 − 3]. Plugging this into
    the previous equation gives us the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个2x2的方阵，A = [ 0 1 − 2 − 3]。将这个值代入前面的方程，我们得到以下结果：
- en: det([ 0 1 − 2 − 3] − λ[1 0 0 1]) = 0
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: det([ 0 1 − 2 − 3] − λ[1 0 0 1]) = 0
- en: 'By multiplying the scalar, λ, into the identity matrix, we get the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将标量λ乘以单位矩阵，我们得到以下结果：
- en: det([ 0 1 − 2 − 3] − [ λ 0 0 λ ]) = 0
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: det([ 0 1 − 2 − 3] − [ λ 0 0 λ ]) = 0
- en: 'Combining the two matrices gives us this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 将两个矩阵相加得到以下结果：
- en: det([ − λ 1 − 2 − 3 − λ]) = 0
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: det([ − λ 1 − 2 − 3 − λ]) = 0
- en: 'By applying the definition of the matrix determinant, we get the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用矩阵行列式的定义，我们得到以下结果：
- en: − λ(− 3 − λ) − 1 *(− 2) = 0
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: − λ(− 3 − λ) − 1 *(− 2) = 0
- en: λ 2 + 3λ + 2 = 0
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: λ 2 + 3λ + 2 = 0
- en: (λ + 1)(λ + 2) = 0
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: (λ + 1)(λ + 2) = 0
- en: λ = − 1, − 2
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: λ = − 1, − 2
- en: 'Thus, we have two solutions: λ = − 1 and λ = − 2\. The next step is to find
    the corresponding eigenvectors for both; we will focus on λ = − 1 in the following
    exposition.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个解：λ = − 1 和 λ = − 2。下一步是找到这两个对应的特征向量；在接下来的阐述中，我们将关注 λ = − 1。
- en: 'Since the square matrix, A, is 2x2, we know that the eigenvector, v, needs
    to be two-dimensional. By denoting v T = [ v 1, v 2] and plugging λ = − 1 and
    A = [ 0 1 − 2 − 3] into Av = λIv, we get the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 由于方阵A是2x2的，我们知道特征向量v需要是二维的。通过表示 v T = [ v 1, v 2] 并将 λ = − 1 和 A = [ 0 1 − 2 −
    3] 代入 Av = λIv，我们得到以下结果：
- en: '[ 0 1 − 2 − 3][v 1 v 2] = [− v 1 − v 2]'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[ 0 1 − 2 − 3][v 1 v 2] = [− v 1 − v 2]'
- en: 'This gives us the following system of equations:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下方程组：
- en: '{ v 2 = − v 1  − 2 v 1 − 3 v 2 = − v 2'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '{ v 2 = − v 1  − 2 v 1 − 3 v 2 = − v 2'
- en: 'Solving this system of equations gives us v 2 = − v 1, which corresponds to
    an infinite number of solutions. This makes sense as the eigenvector focuses more
    on direction instead of absolute magnitude. In this case, the direction is represented
    by a line, y = − x, in a two-dimensional coordinate system. *Figure 8**.6* summarizes
    the process of finding the eigenvalues and eigenvectors of a square matrix:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 解这个方程组给出v2 = − v1，这对应于无限多个解。这是有意义的，因为特征向量更关注方向而不是绝对大小。在这种情况下，方向由二维坐标系中的一条线，y
    = − x，表示。*图8**.6*总结了寻找方阵特征值和特征向量的过程：
- en: '![Figure 8.6 – The process of deriving the eigenvalues and eigenvectors of
    a square matrix](img/B18680_08_006.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 推导方阵的特征值和特征向量的过程](img/B18680_08_006.jpg)'
- en: Figure 8.6 – The process of deriving the eigenvalues and eigenvectors of a square
    matrix
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 推导方阵的特征值和特征向量的过程
- en: 'Let’s say we take v 1 = 1\. The resulting eigenvector becomes v T = [1, − 1].
    When v 1 = 2, we get v T = [2, − 2]. To calculate the eigenvalues and eigenvectors
    via eigen-decomposition, we can simply call the `eigen()` function in R, as shown
    in the following code snippet:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们取v1 = 1。得到的特征向量变为vT = [1, − 1]。当v1 = 2时，我们得到vT = [2, − 2]。要使用特征值分解计算特征值和特征向量，我们可以在R中简单地调用`eigen()`函数，如下面的代码片段所示：
- en: '[PRE32]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are two entries in the `values` attribute and two column vectors in the
    `vectors` attribute, indicating a total of two eigenpairs in the matrix. We can
    access the first eigenvalue as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`values`属性中有两个条目，`vectors`属性中有两个列向量，表明矩阵中总共有两个特征对。我们可以如下访问第一个特征值：'
- en: '[PRE33]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Similarly, the eigenvectors are returned as a set of column vectors. Therefore,
    we can access the first eigenvector as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，特征向量作为一组成列向量返回。因此，我们可以如下访问第一个特征向量：
- en: '[PRE34]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can draw a few useful properties from eigen-decomposition. Take an n × n
    square matrix, A, for example. The number of distinct eigenvalues is, at most,
    n.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从特征值分解中得出一些有用的性质。以一个n × n的方阵A为例。不同的特征值的数量最多是n。
- en: 'We can also verify the correctness of the resulting eigenvalues and eigenvectors
    by making use of the condition derived earlier: det(A − λI) = 0\. For the first
    eigenvalue and eigenvector, the following code snippet does the verification:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过使用之前推导出的条件来验证所得特征值和特征向量的正确性：det(A − λI) = 0。对于第一个特征值和特征向量，以下代码片段进行了验证：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can also verify the eigen-decomposition based on the original equation –
    that is, Av = λv:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以根据原始方程验证特征值分解 – 即，Av = λv：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, the second command returns a very small number, which is due to numerical
    approximation and can be considered zero. Again, note the use of the scalar-vector
    multiplication sign, `*`, and the matrix-vector multiplication sign, `%*%`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第二个命令返回一个非常小的数字，这是由于数值近似造成的，可以认为是零。再次注意，标量-向量乘法符号`*`和矩阵-向量乘法符号`%*%`的使用。
- en: 'Now that we have a better understanding of eigenvalues and eigenvectors, let’s
    look at a popular application: PCA.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对特征值和特征向量有了更好的理解，让我们看看一个流行的应用：主成分分析（PCA）。
- en: Introducing principal component analysis
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍主成分分析
- en: When building an ML model, the dataset that’s used to train the model may have
    redundant information in the predictors. The redundancy in the predictors/columns
    of the dataset arises from correlated features in the dataset and needs to be
    taken care of when using a certain class of models. In such cases, PCA is a popular
    technique to address such challenges as it reduces the feature dimension of the
    dataset and thus shrinks the redundancy. The problem of **collinearity**, which
    says that two or more predictors are linearly correlated in a model, could thus
    be relieved via dimension reduction using PCA.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当构建机器学习模型时，用于训练模型的训练集可能包含预测变量中的冗余信息。数据集中预测变量/列的冗余来自特征的相关性，在使用某些类模型时需要加以注意。在这种情况下，主成分分析（PCA）是一种流行的技术，可以解决这类挑战，因为它减少了数据集的特征维度，从而减少了冗余。**共线性**问题，即模型中两个或多个预测变量线性相关，可以通过使用PCA进行降维来缓解。
- en: Collinearity among the predictors is often considered a big problem when building
    an ML model. Using the Pearson correlation coefficient, it is a number between
    -1 and 1, where a coefficient near 0 indicates two variables are linearly independent,
    and a coefficient near -1 or 1 indicates that two variables are linearly related.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习模型时，预测变量之间的共线性通常被认为是一个大问题。使用皮尔逊相关系数，它是一个介于 -1 和 1 之间的数字，其中接近 0 的系数表示两个变量线性独立，而接近
    -1 或 1 的系数表示两个变量线性相关。
- en: When two independent variables are linearly correlated, such as x 2 = 2 x 1,
    no extra information is provided by x 2\. The perfect correlation between x 1
    and x 2 makes x 2 useless in terms of explaining the outcome variable. A natural
    choice is to remove x 2 from the set of independent variables. However, when the
    correlation is not perfect, we will lose some amount of information due to the
    removal.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个独立变量线性相关时，例如 x₂ = 2x₁，x₂ 不提供额外信息。x₁ 和 x₂ 之间的完美相关性使得 x₂ 在解释结果变量方面变得无用。一个自然的选择是从独立变量集中删除
    x₂。然而，当相关性不是完美的时候，由于删除，我们将失去一些信息。
- en: PCA provides us with another way to combat correlation among the predictors.
    It allows us to extract meaningful and uncorrelated information from the original
    dataset. Specifically, it uncovers the hidden and low-dimensional features that
    underlie the dataset. These low-dimensional hidden features make it convenient
    for both visualization and interpretation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: PCA为我们提供了一种对抗预测变量之间相关性的另一种方法。它允许我们从原始数据集中提取有意义的、不相关的信息。具体来说，它揭示了数据集背后的隐藏的低维特征。这些低维隐藏特征使得可视化解释变得方便。
- en: To help us understand this technique, we’ll start by covering the notion of
    the variance-covariance matrix.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们理解这一技术，我们将首先介绍方差-协方差矩阵的概念。
- en: Understanding the variance-covariance matrix
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解方差-协方差矩阵
- en: All ML models sit on a training dataset. In the context of supervised learning,
    the dataset consists of input-output pairs. The input is also called the design
    matrix, holding n rows of observations and p columns of features. This n × p design
    matrix, X, is our focus of study in PCA.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习模型都建立在训练数据集之上。在监督学习的背景下，数据集由输入-输出对组成。输入也称为设计矩阵，包含 n 行观测值和 p 列特征。这个 n ×
    p 的设计矩阵 X 是我们在主成分分析（PCA）中的研究对象。
- en: Suppose we would like to know the correlation between each pair of features.
    The correlation coefficient, ranging from -1 to 1, is calculated based on the
    covariance of two variables. The covariance is a scalar value that measures the
    strength of co-movement between two variables. Therefore, the covariance matrix
    of the design matrix measures the strength of co-movement between each unique
    pair of features. It is a p × p square matrix, cov(X), where the entry is located
    at the i th row, and the j th column represents the covariance value between the
    x i and x j features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想了解每对特征之间的相关性。相关系数的范围在 -1 到 1 之间，它是基于两个变量的协方差计算的。协方差是一个标量值，用于衡量两个变量共同运动的强度。因此，设计矩阵的协方差矩阵衡量了每对独特特征之间共同运动的强度。它是一个
    p × p 的方阵，cov(X)，其中位于第 i 行、第 j 列的项表示 x_i 和 x_j 特征之间的协方差值。
- en: 'To obtain this covariance matrix, we must de-mean all features – that is, subtract
    the column-wise mean from each element in a given column. This removes the central
    tendency and indicates the relative amount of deviation from the mean. This results
    in the de-meaned n × p design matrix, (X − _ X). We must do the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这个协方差矩阵，我们必须对所有特征进行均值化处理——也就是说，从给定列的每个元素中减去列均值。这消除了中心趋势，并表明了相对于均值的相对偏差量。这导致了一个均值化后的
    n × p 设计矩阵 (X − μ_X)。我们必须执行以下操作：
- en: Transpose the de-meaned design matrix to obtain a p × n matrix, (X − _ X) T.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将均值化后的设计矩阵转置，以获得一个 p × n 的矩阵，(X − μ_X)ᵀ。
- en: Multiply the transposed design matrix with the original design matrix, both
    de-meaned, to obtain a p × p square matrix, (X − _ X) T(X − _ X).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将均值化后的设计矩阵的转置与原始设计矩阵相乘，两者都进行了均值化处理，以获得一个 p × p 的方阵，(X − μ_X)ᵀ(X − μ_X)。
- en: Divide the result by n − 1 to normalize the entries in the matrix (instead of
    n used in population covariance).
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果除以 n - 1 以对矩阵中的项进行归一化（而不是在总体协方差中使用的 n）。
- en: Once we’ve done this, a p × p variance-covariance matrix, (X − _ X) T(X − _ X) _ n
    − 1 , is generated, where the i th diagonal element is the variance of the i th
    column in the original design matrix.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些操作后，将生成一个 p × p 的方差-协方差矩阵 (X − μ_X)ᵀ(X − μ_X) / (n − 1)，其中第 i 个对角元素是原始设计矩阵中第
    i 列的方差。
- en: 'Let’s shed some light on the contents of the variance-covariance matrix, (X
    − _ X) T(X − _ X) _ n − 1 . We will start by exposing the entries in this matrix,
    as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: (X − _ X) T(X − _ X) ____________ n − 1  = ⎡ ⎢ ⎣ ∑ (x 1 − _ x 1) 2 / (n − 1)
     ⋯ ∑ ( x 1 − _ x 1)( x p − _ x p) / (n − 1)    ⋮ ⋱ ⋮    ∑ ( x p − _ x p)( x 1
    − _ x 1) / (n − 1) ⋯ ∑ (x p − _ x p) 2 / (n − 1)  ⎤ ⎥ ⎦
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The first entry, namely ∑ (x 1 − _ x 1) 2 / (n − 1), is the definition of the
    sample variance of the x 1 feature. This comes from the dot product between two
    length-n variables, later divided by n − 1\. So, the summation works on all n
    elements in both column vectors.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving to the rightmost entry on the first row, we have ∑ ( x 1 − _ x 1)( x p
    − _ x p) / (n − 1) as the covariance between the x 1 and x p variables. This is
    exactly how we would calculate the sample covariance between these two variables,
    where the summation is applied to all n elements in both column vectors. *Figure
    8**.7* illustrates the calculation process:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Summarizing the calculation process for the variance-covariance
    matrix based on the design matrix in a training dataset](img/B18680_08_007.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Summarizing the calculation process for the variance-covariance
    matrix based on the design matrix in a training dataset
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Note that although the n × p design matrix, X, is not necessarily square, we
    managed to obtain a p × p square matrix by multiplying the transposed de-meaned
    p × n matrix, (X − _ X) T, with just the de-meaned n × p matrix, (X − _ X).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the variance-covariance of a given matrix:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we first generate a dummy matrix, where the
    second column is produced by doubling the first column:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we de-mean both columns:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Lastly, we can calculate the variance-covariance matrix, like so:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The result is a 2x2 matrix, which aligns with our previous discussion.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can also manually calculate the particular variance or covariance
    entries. For example, the following command calculates the variance of the second
    variable:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also calculate the covariance between the two variables:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the following section, we will connect the variance-covariance matrix to
    PCA.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to PCA
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The variance-covariance matrix, (X − _ X) T(X − _ X) _ n − 1 , from the previous
    section can be used in eigendecomposition, which generates a set of eigenvalues
    along with the associated eigenvectors. Note that these eigenvalues will be real
    scalars, and the associated eigenvectors will be orthogonal to each other, each
    pointing in a distinct direction.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a few properties of PCA in connection to these eigenvalues and
    eigenvectors. First, the total variance of the dataset is the sum of these eigenvalues.
    Thus, we can rank these eigenvalues in decreasing order, keep only the first few,
    and take the associated eigenvectors as a reduced set of hidden features for downstream
    modeling. By doing this, we can achieve dimension reduction.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: In addition, these eigenvectors are called **principal components** and point
    in a particular direction. So, for a specific eigenvector, v i, the direction
    it points in can explain λ i of the total variance of the dataset. Thus, we can
    explain the cumulative variance (as a percentage) by adding up the corresponding
    eigenvalues and deciding the trade off between the number of hidden variables
    to keep and the percentage of the total variance explained.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s run eigendecomposition on the previous variance-covariance matrix:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The result shows that the first eigenvalue significantly dominates the second
    one in terms of the values that are output, showing that the first eigenvector
    is sufficient to explain the total variance of the original design matrix. This
    makes sense as the second variable is simply double the first variable and thus
    delivers no additional information. Although the dataset has two columns, it only
    has one column’s worth of information and the second column is completely redundant.
    The second eigenvector also lies along the same line as the first one, thus having
    no variability in the eigenspace.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll introduce a function for performing PCA.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Performing PCA
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One function we can use to perform PCA is `prcomp()`. For our exercise, we
    will use the first four columns of the Iris dataset to perform PCA:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the dataset:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `prcomp()` function will automatically perform all the aforementioned steps
    involved in eigendecomposition:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that the variances that are explained – in other words, eigenvalues – are
    expressed in terms of standard deviations instead of variance.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have access to the PCA results, we can visualize these results to
    make them more intuitive. For this purpose, we will use the `factoextra` package.
    Remember to install and load this package first.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first function we will use is the `fviz_eig()` function, which shows the
    percentage of variance explained by each principal component in a scree plot:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Running this command generates *Figure 8**.8*:'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Visualizing the PCA results in a scree plot](img/B18680_08_008.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Visualizing the PCA results in a scree plot
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also show a graph for the individual observations using the `fviz_pca_ind()`
    function, where individuals with similar profiles are grouped:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Running this command generates *Figure 8**.9*. Note that observations with
    lower quality (less variance explained) are assigned with colors toward the lower
    half of the palette:'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Visualizing individual contributions in explaining the total
    variance](img/B18680_08_009.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Visualizing individual contributions in explaining the total variance
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we can also visualize the directions of the variables. Positively correlated
    variables will point to the same side of the plot, while negatively correlated
    variables will point to opposite sides of the graph:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Running this command generates *Figure 8**.10*:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Visualizing variable directions](img/B18680_08_010.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Visualizing variable directions
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that `Petal.Width` and `Petal.Length` point in almost overlapping
    directions.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered intermediate linear algebra and its implementations
    in R. We started by introducing the matrix determinant, a widely used property
    in numerical analysis. We highlighted the intuition behind the matrix determinant
    and its connection to matrix rank.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'We also covered additional properties, including matrix trace and norm. In
    particular, we introduced three popular norms: L 1-norm, L 2-norm, and L ∞-norm.
    We detailed their mathematical constructs and calculation process.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Next, we covered eigendecomposition, which leads to a set of eigenvalues and
    eigenvectors of a square matrix. We provided a step-by-step derivation and analysis
    of the core equation, as well as the approach to compute them.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered PCA, a popular technique that’s used for dimension reduction.
    Specifically, we highlighted its role in removing collinearity in the dataset
    and provided a few ways to compute and visualize PCA results.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will switch gears and cover another key branch of mathematics:
    calculus.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
