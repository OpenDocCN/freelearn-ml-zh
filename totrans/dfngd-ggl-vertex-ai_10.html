<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer115">
			<h1 id="_idParaDest-136" class="chapter-number"><a id="_idTextAnchor136"/>10</h1>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Vertex AI Deployment and Automation Tools – Orchestration through Managed Kubeflow Pipelines</h1>
			<p>In a typical <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) solution, we often have lots of applications and services as part of the end-to-end workflow. If we try to stitch these services and applications together using some custom scripts with cron jobs, it becomes super tricky to manage the workflows. Thus, it becomes important to make use of some orchestration services to carefully manage, scale, and monitor complex workflows. Orchestration<a id="_idIndexMarker632"/> is the process of stitching multiple applications or services together to build an end-to-end solution workflow. Google Cloud provides multiple orchestration services, such as Cloud Scheduler, Workflows, and Cloud Composer, to manage complex workflows at scale. Cloud Scheduler<a id="_idIndexMarker633"/> is ideal for single, repetitive tasks, Workflows<a id="_idIndexMarker634"/> is more suitable for complex multi-service orchestration, and Cloud Composer<a id="_idIndexMarker635"/> is ideal for <span class="No-Break">data-driven workloads.</span></p>
			<p>ML workflows have a lot of steps, from data preparation to model training, evaluation, and more. On top of that, monitoring and version tracking become even more challenging. In this chapter, we will learn about GCP tooling for orchestrating ML workflows effectively. The main topics covered in this chapter are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Orchestrating ML workflows using Vertex AI Pipelines (managed <span class="No-Break">Kubeflow pipelines)</span></li>
				<li>Orchestrating ML workflows using Cloud Composer (<span class="No-Break">managed Airflow)</span></li>
				<li>Vertex AI Pipelines versus <span class="No-Break">Cloud Composer</span></li>
				<li>Getting predictions on <span class="No-Break">Vertex AI</span></li>
				<li>Managing deployed models on <span class="No-Break">Vertex AI</span></li>
			</ul>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>Technical requirements</h1>
			<p>The code examples shown in this chapter can be found in the following GitHub <span class="No-Break">repo:</span><span class="No-Break"/><a href="https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10"><span class="No-Break">https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter10</span></a></p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor139"/>Orchestrating ML workflows using Vertex AI Pipelines (managed Kubeflow pipelines)</h1>
			<p>ML solutions are <a id="_idIndexMarker636"/>complex and involve lots of<a id="_idIndexMarker637"/> steps, including data preparation, feature engineering, model selection, model training, testing, evaluation, and deployment. On top of these, it is really important to track and version control lots of aspects related to the ML model while in production. Vertex AI Pipelines on GCP lets us codify our ML workflows in such a way that they are easily composable, shareable, and reproducible. Vertex AI Pipelines can run Kubeflow as well as <strong class="bold">TensorFlow Extended</strong> (<strong class="bold">TFX</strong>)-based ML pipelines<a id="_idIndexMarker638"/> in a fully managed way. In this section, we will learn about developing Kubeflow pipelines for ML development as Vertex <span class="No-Break">AI Pipelines.</span></p>
			<p>Kubeflow<a id="_idIndexMarker639"/> is a Kubernetes-native solution that simplifies the orchestration of ML pipelines and makes experimentation easy and reproducible. Also, the pipelines are sharable. It comes with framework support for things such as execution monitoring, workflow scheduling, metadata logging, and versioning. A Kubeflow pipeline is a description of an ML workflow that combines multiple small components of the workflow <a id="_idIndexMarker640"/>into a <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>). Behind the scenes, it runs the pipeline components on containers, which provide portability, reproducibility, and encapsulation. Each pipeline component is one step in the ML workflow that does a specific task. The output of one component may become the input of another component and so forth. Each pipeline component is made up of code, packaged as a Docker image that performs one step in the pipeline and runs on one or more Kubernetes Pods. Kubeflow pipelines can be leveraged for ETL and CI/CD tasks but they are more popularly used to run <span class="No-Break">ML workflows.</span></p>
			<p>The Vertex AI SDK<a id="_idIndexMarker641"/> lets us create and upload Kubeflow pipelines programmatically from within the<a id="_idIndexMarker642"/> Jupyter Notebook itself, but <a id="_idIndexMarker643"/>we can also use the console UI to work on pipelines. The Vertex AI UI lets us<a id="_idIndexMarker644"/> visualize the pipeline execution graph. It also lets us track, monitor, and compare different <span class="No-Break">pipeline executions.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Developing Vertex AI Pipeline using Python</h2>
			<p>In this section, we <a id="_idIndexMarker645"/>will develop and launch a simple <a id="_idIndexMarker646"/>Kubeflow-based Vertex Pipeline using the Vertex AI SDK within a Jupyter Notebook. In this example, we will work on an open source wine quality dataset. Let’s <span class="No-Break">get started!</span></p>
			<p>Open a Jupyter Notebook and install some <span class="No-Break">useful libraries:</span></p>
			<pre class="source-code">
!pip3 install google-cloud-aiplatform
!pip3 install kfp --upgrade
!pip install google_cloud_pipeline_components</pre>			<p>In a new cell, import useful libraries for Vertex <span class="No-Break">Pipeline development:</span></p>
			<pre class="source-code">
from typing import NamedTuple
import typing
import pandas as pd
from kfp.v2 import dsl
from kfp.v2.dsl import (Artifact, Dataset, Input, Model, Output, Metrics, ClassificationMetrics, component, OutputPath, InputPath)
from kfp.v2 import compiler
from google.cloud import bigquery
from google.cloud import aiplatform
from google.cloud.aiplatform import pipeline_jobs
from google_cloud_pipeline_components import aiplatform as gcc_aip</pre>			<p>Create a timestamp variable. It will be useful in creating unique names for <span class="No-Break">pipeline objects:</span></p>
			<pre class="source-code">
from datetime import datetime
TIMESTAMP =datetime.now().strftime("%Y%m%d%H%M%S")</pre>			<p>Now, we will set some project-related configurations, such as <strong class="source-inline">project_id</strong>, region, staging bucket, and <span class="No-Break">service account:</span></p>
			<pre class="source-code">
PROJECT_ID='417xxxxxxx97'
REGION='us-west2'
SERVICE_ACCOUNT='417xxxxxxx97-compute@developer.gserviceaccount.com'
BUCKET_URI='gs://my-training-artifacts'</pre>			<p>In this section we will use the Wine Quality dataset. The Wine Quality dataset was created by Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009). You can check it out at OR You can download the dataset from the following link: <a href="https://doi.org/10.24432/C56S3T">https://doi.org/10.24432/C56S3T</a>. (UCI Machine <span class="No-Break">Learning Repository</span><span class="No-Break">.)</span></p>
			<p>Next, we load and check the wine quality dataset in a notebook cell to understand the data <span class="No-Break">and columns:</span></p>
			<pre class="source-code">
df_wine = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", delimiter=";")
df_wine.head()</pre>			<p>The output of this snippet is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17792_10_1.jpg" alt="Figure 10.1 – Overview of the wine quality dataset" width="1187" height="189"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Overview of the wine quality dataset</p>
			<p>Here is a quick overview of the <span class="No-Break">feature columns:</span></p>
			<ul>
				<li><strong class="source-inline">volatile acidity</strong>: The <strong class="source-inline">volatile acidity</strong> column represents the amount of <span class="No-Break">gaseous acids</span></li>
				<li><strong class="source-inline">fixed acidity</strong>: The amount of fixed acids found in wine, which can be tartaric, succinic, citric, malic, and <span class="No-Break">so on</span></li>
				<li><strong class="source-inline">residual sugar</strong>: This column represents the amount of sugar left after the fermentation <span class="No-Break">of wine</span></li>
				<li><strong class="source-inline">citric acid</strong>: The amount of citric acid, which is naturally found <span class="No-Break">in fruits</span></li>
				<li><strong class="source-inline">chlorides</strong>: The amount of salt in <span class="No-Break">the wine</span></li>
				<li><strong class="source-inline">free sulfur dioxide</strong>: Sulpher dioxide, or SO<span class="subscript">2</span>, prevents wine oxidation <span class="No-Break">and spoilage</span></li>
				<li><strong class="source-inline">total sulfur dioxide</strong>: The total amount of SO<span class="subscript">2</span> in <span class="No-Break">a wine</span></li>
				<li><strong class="source-inline">pH</strong>: pH is used for checking acidity in <span class="No-Break">a wine</span></li>
				<li><strong class="source-inline">density</strong>: Represents the density of <span class="No-Break">the wine</span></li>
				<li><strong class="source-inline">sulphates</strong>: Sulphates help preserve the freshness of wine and also protect it from oxidation <span class="No-Break">and bacteria</span></li>
				<li><strong class="source-inline">alcohol</strong>: The percentage of alcohol present in <span class="No-Break">the wine</span></li>
			</ul>
			<p>The idea is to <a id="_idIndexMarker647"/>predict the wine quality given all the preceding <a id="_idIndexMarker648"/>parameters. We will convert it into a classification problem and call a wine <em class="italic">best quality</em> if its quality indicator value <span class="No-Break">is &gt;=7.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>Pipeline components</h2>
			<p>In this exercise, we will define <a id="_idIndexMarker649"/>four pipeline components for <span class="No-Break">our task:</span></p>
			<ul>
				<li>Data <span class="No-Break">loading component</span></li>
				<li>Model <span class="No-Break">training component</span></li>
				<li>Model <span class="No-Break">evaluation component</span></li>
				<li>Model <span class="No-Break">deploying component</span></li>
			</ul>
			<p>Here, the first <a id="_idIndexMarker650"/>component loads the data and the<a id="_idIndexMarker651"/> second component uses that data to train a <a id="_idIndexMarker652"/>model. The third component evaluates the trained model on the test dataset. The fourth component automatically<a id="_idIndexMarker653"/> deploys the trained model as a Vertex AI endpoint. We will put a condition on automatic model deployment, such as if model ROC &gt;= 0.8, then deploy the model, <span class="No-Break">otherwise don’t.</span></p>
			<p>Now, let’s define these components one by one. The following is the first component that loads and splits the data into training and <span class="No-Break">testing partitions.</span></p>
			<p>To create a Kubeflow component, we can wrap our function with an <strong class="source-inline">@component</strong> decorator. Here, we can<a id="_idIndexMarker654"/> define the base image, and also the dependencies <span class="No-Break">to install:</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In a real project or production pipeline, it is advisable to write package versions along with  their names to avoid any version <span class="No-Break">realated conflicts.</span></p>
			<pre class="source-code">
@component(
    packages_to_install=["pandas", "pyarrow", "scikit-learn==1.0.0"],
    base_image="python:3.9",
    output_component_file="load_data_component.yaml"
)</pre>			<p>Here, we define the function that loads and splits the data into train and <span class="No-Break">test sets:</span></p>
			<pre class="source-code">
def get_wine_data(
    url: str,
    dataset_train: Output[Dataset],
    dataset_test: Output[Dataset]
):
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split as tts
    df_wine = pd.read_csv(url, delimiter=";")
    df_wine['best_quality'] = [1 if x&gt;=7 else 0 for x in df_wine.quality]
    df_wine['target'] = df_wine.best_quality
    df_wine = df_wine.drop(
        ['quality', 'total sulfur dioxide', 'best_quality'],
         axis=1,
    )</pre>			<p>We will keep<a id="_idIndexMarker655"/> about 30% of the data for testing and the remaining for training and save them as <span class="No-Break">CSV files:</span></p>
			<pre class="source-code">
    train, test = tts(df_wine, test_size=0.3)
    train.to_csv(
        dataset_train.path + ".csv",
        index=False,
        encoding='utf-8-sig',
    )
    test.to_csv(
        dataset_test.path + ".csv",
        index=False,
        encoding='utf-8-sig',
    )</pre>			<p>To define a <a id="_idIndexMarker656"/>component, we can wrap our Python functions with an <strong class="source-inline">@component</strong> decorator. It allows us to pass the base image path, packages to install, and a YAML file path if we wish to write the component into a file. The YAML file definition of a component makes it portable and reusable. We can simply create a YAML file with the component definition and load this component anywhere in the project. Note that we can use our custom container image with all our custom dependencies <span class="No-Break">as well.</span></p>
			<p>The first component essentially loads the wine quality dataset table, creates the binary classification output, as discussed previously, drops unnecessary columns, and finally divides it into train and test files. Here, the train and test dataset files are output artifacts of this component that can be reused by subsequently <span class="No-Break">running components.</span></p>
			<p>Now, let’s define the second component, which trains a random forest classifier over the training dataset generated by the <span class="No-Break">first component.</span></p>
			<p>The first step is the decorator, <span class="No-Break">with dependencies:</span></p>
			<pre class="source-code">
@component(
    packages_to_install = [
        "pandas",
        "scikit-learn"
    ],
    base_image="python:3.9",
    output_component_file="model_training_component.yml",
)</pre>			<p>Next, we define our training function, which fits our model on training data and saves it as a Pickle file. Here, our output artifact would be a model and we can associate it with some metadata as well, as shown in the following function. Inside this function, we can associate the model artifact with metadata by putting the metadata key and value within <span class="No-Break"><strong class="source-inline">model.metadata</strong></span><span class="No-Break"> dictionary.</span></p>
			<pre class="source-code">
def train_winequality(
    dataset:  Input[Dataset],
    model: Output[Model],
):
    from sklearn.ensemble import RandomForestClassifier
    import pandas as pd
    import pickle
    data = pd.read_csv(dataset.path+".csv")
    model_rf = RandomForestClassifier(n_estimators=10)
    model_rf.fit(
        data.drop(columns=["target"]),
        data.target,
    )
    model.metadata["framework"] = "RF"
    file_name = model.path + f".pkl"
    with open(file_name, 'wb') as file:
        pickle.dump(model_rf, file)</pre>			<p>This component <a id="_idIndexMarker657"/>trains a random forest classifier model on the training dataset and saves the model as a <span class="No-Break">Pickle file.</span></p>
			<p>Next, let’s define the third component for model evaluation. We start with the <strong class="source-inline">@</strong><span class="No-Break"><strong class="source-inline">component</strong></span><span class="No-Break"> decorator:</span></p>
			<pre class="source-code">
@component(
    packages_to_install = [
        "pandas",
        "scikit-learn"
    ],
    base_image="python:3.9",
    output_component_file="model_evaluation_component.yml",
)</pre>			<p>Now, we define <a id="_idIndexMarker658"/>the actual Python function for <span class="No-Break">model evaluation:</span></p>
			<pre class="source-code">
def winequality_evaluation(
    test_set:  Input[Dataset],
    rf_winequality_model: Input[Model],
    thresholds_dict_str: str,
    metrics: Output[ClassificationMetrics],
    kpi: Output[Metrics]
) -&gt; NamedTuple("output", [("deploy", str)]):
    from sklearn.ensemble import RandomForestClassifier
    import pandas as pd
    import logging
    import pickle
    from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score
    import json
    import typing</pre>			<p>Here is a small<a id="_idIndexMarker659"/> function that controls the deployment of the model. We only deploy a new model if its accuracy is above a <span class="No-Break">certain threshold:</span></p>
			<pre class="source-code">
    def threshold_check(val1, val2):
        cond = "false"
        if val1 &gt;= val2 :
            cond = "true"
        return cond
    data = pd.read_csv(test_set.path+".csv")
    model = RandomForestClassifier()
    file_name = rf_winequality_model.path + ".pkl"
    with open(file_name, 'rb') as file:
        model = pickle.load(file)
    y_test = data.drop(columns=["target"])
    y_target=data.target
    y_pred = model.predict(y_test)</pre>			<p>Now that we have the model outputs, we can calculate accuracy scores and <strong class="source-inline">roc_curve</strong>, and log them <span class="No-Break">as metadata:</span></p>
			<pre class="source-code">
    y_scores =  model.predict_proba(
        data.drop(columns=["target"])
    )[:, 1]
    fpr, tpr, thresholds = roc_curve(
         y_true=data.target.to_numpy(),
        y_score=y_scores, pos_label=True
    )
    metrics.log_roc_curve(
        fpr.tolist(),
        tpr.tolist(),
        thresholds.tolist()
    )
    metrics.log_confusion_matrix(
       ["False", "True"],
       confusion_matrix(
           data.target, y_pred
       ).tolist(),
    )</pre>			<p>Finally, we check the <a id="_idIndexMarker660"/>model accuracy and see whether it satisfies the deployment condition. We return the deployment condition flag <span class="No-Break">from here:</span></p>
			<pre class="source-code">
    accuracy = accuracy_score(data.target, y_pred.round())
    thresholds_dict = json.loads(thresholds_dict_str)
    rf_winequality_model.metadata["accuracy"] = float(accuracy)
    kpi.log_metric("accuracy", float(accuracy))
    deploy = threshold_check(float(accuracy), int(thresholds_dict['roc']))
    return (deploy,)</pre>			<p>This component uses the<a id="_idIndexMarker661"/> outputs of component 1 (test dataset) and component 2 (trained model) as input and performs model evaluation. This component performs the <span class="No-Break">following operations:</span></p>
			<ul>
				<li>Loads the <span class="No-Break">test dataset</span></li>
				<li>Loads the trained model from a <span class="No-Break">Pickle file</span></li>
				<li>Logs the ROC curve and confusion matrix as an <span class="No-Break">output artifact</span></li>
				<li>Checks whether the model accuracy is greater than <span class="No-Break">the threshold</span></li>
			</ul>
			<p>Finally, we define the model deployment component. This component automatically deploys the trained model as a Vertex AI endpoint if the deployment condition <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">true</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
@component(
    packages_to_install=["google-cloud-aiplatform", "scikit-learn",  "kfp"],
    base_image="python:3.9",
    output_component_file="model_winequality_component.yml"
)</pre>			<p>Next, we define the function that deploys the wine quality model when the deployment condition is true. This function will be wrapped around by the previously defined <strong class="source-inline">@component</strong> decorator so that we can later use it in the final <span class="No-Break">pipeline definition:</span></p>
			<pre class="source-code">
def deploy_winequality(
    model: Input[Model],
    project: str,
    region: str,
    serving_container_image_uri : str,
    vertex_endpoint: Output[Artifact],
    vertex_model: Output[Model]
):
    from google.cloud import aiplatform
    aiplatform.init(project=project, location=region)
    DISPLAY_NAME  = "winequality"
    MODEL_NAME = "winequality-rf"
    ENDPOINT_NAME = "winequality_endpoint"</pre>			<p>Here, we define a<a id="_idIndexMarker662"/> function to create the endpoint for our model so that we can use it <span class="No-Break">for inference:</span></p>
			<pre class="source-code">
    def create_endpoint():
        endpoints = aiplatform.Endpoint.list(
        filter='display_name="{}"'.format(ENDPOINT_NAME),
        order_by='create_time desc',
        project=project,
        location=region,
        )
        if len(endpoints) &gt; 0:
            endpoint = endpoints[0]  # most recently created
        else:
            endpoint = aiplatform.Endpoint.create(
            display_name=ENDPOINT_NAME, project=project, location=region
        )
    endpoint = create_endpoint()</pre>			<p>Here, we import our saved <span class="No-Break">model programmatically:</span></p>
			<pre class="source-code">
    #Import a model programmatically
    model_upload = aiplatform.Model.upload(
        display_name = DISPLAY_NAME,
        artifact_uri = model.uri.replace("model", ""),
        serving_container_image_uri =  serving_container_image_uri,
        serving_container_health_route=f"/v1/models/{MODEL_NAME}",
        serving_container_predict_route=f"/v1/models/{MODEL_NAME}:predict",
        serving_container_environment_variables={
        "MODEL_NAME": MODEL_NAME,
    },
    )</pre>			<p>Finally, we <a id="_idIndexMarker663"/>deploy the uploaded model on the desired machine type with the desired <span class="No-Break">traffic split:</span></p>
			<pre class="source-code">
    model_deploy = model_upload.deploy(
        machine_type="n1-standard-4",
        endpoint=endpoint,
        traffic_split={"0": 100},
        deployed_model_display_name=DISPLAY_NAME,
    )
    # Save data to the output params
    vertex_model.uri = model_deploy.resource_name</pre>			<p>Now that the<a id="_idIndexMarker664"/> core components of our pipeline are ready, we can go ahead and define our <span class="No-Break">Vertex Pipeline.</span></p>
			<p>First, we need to provide a unique name for <span class="No-Break">our pipe:</span></p>
			<pre class="source-code">
DISPLAY_NAME = 'pipeline-winequality job{}'.format(TIMESTAMP)</pre>			<p>Pipeline definition is the part where we stitch these components together to define our ML workflow (or execution graph). Here, we can control which components run first and the output of which component should be fed to another component. The following scripts define a simple pipeline for <span class="No-Break">our experiment.</span></p>
			<p>We can use the <strong class="source-inline">@dsl.pipeline</strong> decorator to define a Kubeflow pipeline. We can pass here a <strong class="bold">Google Cloud Storage</strong> (<strong class="bold">GCS</strong>) location where it will store any execution-related artifacts. This GCS location is passed through the <strong class="source-inline">pipeline_root</strong> parameter inside the decorator, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
@dsl.pipeline(
    pipeline_root=BUCKET_URI,
    name="pipeline-winequality",
)
def pipeline(
    url: str = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv",
    project: str = PROJECT_ID,
    region: str = REGION,
    display_name: str = DISPLAY_NAME,
    api_endpoint: str = REGION+"-aiplatform.googleapis.com",
    thresholds_dict_str: str = '{"roc":0.8}',
    serving_container_image_uri: str = "us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest"
    ):</pre>			<p>We can create the<a id="_idIndexMarker665"/> execution DAG here and define the order of execution for our predefined components. Some components can be dependent, where the output of one component is the input for another. Dependent components execute sequentially, while independent ones can be executed <span class="No-Break">in parallel:</span></p>
			<pre class="source-code">
    # adding first component
    data_op = get_wine_data(url)
    # second component uses output of first component as input
    train_model_op = train_winequality(data_op.outputs["dataset_train"])
    # add third component (uses outputs of comp1 and comp2 as input)
    model_evaluation_op = winequality_evaluation(
        test_set=data_op.outputs["dataset_test"],
        rf_winequality_model=train_model_op.outputs["model"],
        # We deploy the model only if the model performance is above the threshold
        thresholds_dict_str = thresholds_dict_str,
    )</pre>			<p>Here is our condition that decides whether to deploy this model <span class="No-Break">or not:</span></p>
			<pre class="source-code">
    # condition to deploy the model
    with dsl.Condition(
        model_evaluation_op.outputs["deploy"]=="true",
        name="deploy-winequality",
    ):
        deploy_model_op = deploy_winequality(
        model=train_model_op.outputs['model'],
        project=project,
        region=region,
        serving_container_image_uri = serving_container_image_uri,
        )</pre>			<p>Here, we use the <strong class="source-inline">@dsl.pipeline</strong> decorator to define our pipeline. Note that in the preceding definition, the first three components are simple, but the fourth component has been defined using <strong class="source-inline">dsl.Condition()</strong>. We only run the model deployment component if this<a id="_idIndexMarker666"/> condition is satisfied. So, this is how we can control when to deploy the model. If our model meets the business criteria, we can choose to <span class="No-Break">auto-deploy it.</span></p>
			<p>Next, we can compile <span class="No-Break">our pipeline:</span></p>
			<pre class="source-code">
compiler.Compiler().compile(
    pipeline_func=pipeline,
    package_path='ml_winequality.json',
)</pre>			<p>Finally, we can submit our pipeline job to <span class="No-Break">Vertex AI:</span></p>
			<pre class="source-code">
pipeline_job = pipeline_jobs.PipelineJob(
    display_name="winequality-pipeline",
    template_path="ml_winequality.json",
    enable_caching=False,
    location=REGION,
)
pipeline_job.run()</pre>			<p>This script will launch our pipeline in Vertex AI. It will also provide us with a console URL to monitor the <span class="No-Break">pipeline job.</span></p>
			<p>We can <a id="_idIndexMarker667"/>also locate the pipeline run by going to the <strong class="bold">Vertex AI</strong> tab in the console and clicking on the <strong class="bold">pipelines</strong> tab. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em> is a screenshot of the execution graph present in Vertex AI for our <span class="No-Break">example job.</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17792_10_2.jpg" alt="Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google Cloud console UI" width="1210" height="995"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Execution graph of our example Vertex Pipeline from the Google Cloud console UI</p>
			<p>As we can see, this execution graph has all four components defined by us. It also has all the artifacts generated by the components. If we click on the <strong class="bold">metrics</strong> artifact, we can see the output values in the right pane of the console UI. It looks something similar to <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B17792_10_3.jpg" alt="Figure 10.3 – Metadata and artifacts related to our pipeline execution" width="1211" height="1427"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Metadata and artifacts related to our pipeline execution</p>
			<p>This is how we<a id="_idIndexMarker668"/> can use the Google Cloud console UI to track the execution and metrics of our ML-related workflows. Once we have our <a id="_idIndexMarker669"/>pipeline ready, we can also schedule its execution using services such as the native scheduler for Vertex AI Pipelines, Cloud Scheduler (we can define a schedule), Cloud Functions (event-based trigger), and <span class="No-Break">so on.</span></p>
			<p>Now, we have a good understanding of how Kubeflow pipelines can be developed on Google Cloud as Vertex AI Pipelines. We should be able to develop and launch our custom pipelines from scratch now. In the next section, we will learn about Cloud Composer as another solution for <span class="No-Break">workflow orchestration.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor142"/>Orchestrating ML workflows using Cloud Composer (managed Airflow)</h1>
			<p>Cloud Composer<a id="_idIndexMarker670"/> is a workflow orchestration service on Google Cloud that is built upon the open source project of Apache Airflow. The key difference is that Composer is fully managed and also integrates with other GCP tooling very easily. With Cloud Composer, we can write, execute, schedule, or monitor our workflows that are also supported across multi-cloud and hybrid environments. Composer pipelines are DAGs that can be easily defined and configured using Python. It comes with a rich library of connectors that let us deploy our workflows instantly with one click. Graphical representations of workflows on the Google Cloud console make monitoring and troubleshooting quite convenient. Automatic synchronization of our DAGs ensures that our jobs always stay <span class="No-Break">on schedule.</span></p>
			<p>Cloud Composer is <a id="_idIndexMarker671"/>commonly used by data scientists <a id="_idIndexMarker672"/>and data engineers to build complex data pipelines (ETL or ELT pipelines). It can also be used as an orchestrator for ML workflows. Cloud Composer is pretty convenient for data-related workflows as the Apache project comes with hundreds of operators and sensors that make it easy to communicate across multiple cloud environments with very little code. It also lets us define failure handling mechanisms such as sending emails or Slack notifications on <span class="No-Break">pipeline failure.</span></p>
			<p>Now let’s understand how to develop Cloud <span class="No-Break">Composer-based pipelines.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor143"/>Creating a Cloud Composer environment</h2>
			<p>We can follow these steps to create a<a id="_idIndexMarker673"/> Cloud Composer environment using the Google Cloud <span class="No-Break">console UI:</span></p>
			<ol>
				<li>Enable the Cloud <span class="No-Break">Composer API.</span></li>
				<li>From the left pane of the console, select <strong class="bold">Composer</strong> and click on <strong class="bold">Create</strong> to start creating a Composer environment (see <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B17792_10_4.jpg" alt=" Figure 10.4 – Creating a Composer environment on the Google Cloud console" width="880" height="1314"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">	Figure 10.4 – Creating a Composer environment on the Google Cloud console</p>
			<ol>
				<li value="3">Click on <strong class="bold">Create</strong>. It will<a id="_idIndexMarker674"/> take about 15–20 minutes to create the environment. Once it is complete, the environment page will look like the following (see <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">).</span></li>
			</ol>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B17792_10_5.jpg" alt="Figure 10.5 – Ready-to-use Cloud Composer environment" width="1209" height="109"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Ready-to-use Cloud Composer environment</p>
			<ol>
				<li value="4">Click on <strong class="bold">Airflow</strong> to see the Airflow web UI. The Airflow web UI is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17792_10_6.jpg" alt=" Figure 10.6 – Airflow web UI with our workflows" width="1211" height="269"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">	Figure 10.6 – Airflow web UI with our workflows</p>
			<p>As we can see in the preceding <a id="_idIndexMarker675"/>screenshot, there is already one DAG running – <strong class="bold">airflow_monitoring</strong>. If we go to the Cloud Storage bucket for Composer, we will find a <strong class="bold">dags</strong> folder there, and we will be able to see one <strong class="source-inline">airflow_monitoring.py</strong> file. See <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17792_10_7.jpg" alt="Figure 10.7 – GCS location where we can put our Python-based DAGs for execution" width="1175" height="524"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – GCS location where we can put our Python-based DAGs for execution</p>
			<p>Now that our Composer setup is ready, we can quickly check whether it is working as expected. To test things fast, we will use one demo DAG from the Airflow tutorials and put it inside the <strong class="bold">dags</strong> folder of this bucket. If everything is working fine, any DAG that we put inside this bucket should automatically get synced <span class="No-Break">with Airflow.</span></p>
			<p>The following is the <a id="_idIndexMarker676"/>code for a demo DAG from the <span class="No-Break">Airflow tutorials:</span></p>
			<pre class="source-code">
from datetime import timedelta
from textwrap import dedent
# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG
# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
# These args will get passed on to each operator
# You can override them on a per-task basis during operator initialization</pre>			<p>The following dictionary with some default arguments will be used when creating the operators later. By default, these arguments will be passed to each operator, but we can also override some of these arguments in some operators as per <span class="No-Break">the requirements:</span></p>
			<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    # 'queue': 'bash_queue',
    # 'execution_timeout': timedelta(seconds=300),
    # 'on_failure_callback': some_function,
    # 'on_success_callback': some_other_function,
    # 'on_retry_callback': another_function,
    # 'sla_miss_callback': yet_another_function,
}</pre>			<p>This is where we define<a id="_idIndexMarker677"/> our DAG, with execution steps in the desired or <span class="No-Break">required order:</span></p>
			<pre class="source-code">
with DAG(
    'composer-test-dag',
    default_args=default_args,
    description='A simple composer DAG',
    schedule_interval=timedelta(days=1),
    start_date=days_ago(2),
    tags=['example'],
) as dag:</pre>			<p>Here, we define different tasks that our code will <span class="No-Break">be performing:</span></p>
			<pre class="source-code">
    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id='print_date',
        bash_command='date',
    )
    t2 = BashOperator(
        task_id='sleep',
        depends_on_past=False,
        bash_command='sleep 5',
        retries=3,
    )</pre>			<p>You can document your task <a id="_idIndexMarker678"/>using the following attributes: <strong class="source-inline">doc_md</strong> (Markdown), <strong class="source-inline">doc</strong> (plain text), <strong class="source-inline">doc_rst</strong>, <strong class="source-inline">doc_json</strong>, and <strong class="source-inline">doc_yaml</strong>, which gets rendered on the UI’s <strong class="bold">Task Instance </strong><span class="No-Break"><strong class="bold">Details</strong></span><span class="No-Break"> page:</span></p>
			<pre class="source-code">
    t1.doc_md = dedent(
    )
    dag.doc_md = __doc__
    dag.doc_md = """a documentation placed anywhere"""
    templated_command = dedent(
        """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
        echo "{{ params.my_param }}"
    {% endfor %}
    """
    )</pre>			<p>Now let’s define the <span class="No-Break"><strong class="source-inline">t3</strong></span><span class="No-Break"> task:</span></p>
			<pre class="source-code">
    t3 = BashOperator(
        task_id='templated',
        depends_on_past=False,
        bash_command=templated_command,
        params={'my_param': 'Parameter I passed in'},
    )</pre>			<p>Here, we define the execution order of our tasks. <strong class="source-inline">t1</strong> needs to be executed before <strong class="source-inline">t2</strong> and <strong class="source-inline">t3</strong>, but <strong class="source-inline">t2</strong> and <strong class="source-inline">t3</strong> can execute <span class="No-Break">in parallel:</span></p>
			<pre class="source-code">
    t1 &gt;&gt; [t2, t3]</pre>			<p>As soon as we <a id="_idIndexMarker679"/>upload this <strong class="source-inline">.py</strong> file to a GCS bucket inside the dags folder, Airflow will automatically sync it. If you refresh the Airflow web UI, it should show another DAG, as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17792_10_8.jpg" alt="Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS location" width="1199" height="237"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Airflow web UI with all the DAGs that are present in the GCS location</p>
			<p>If we are able to see our DAG running in the Airflow UI, it verifies that our installation is working fine. Now, let’s open this DAG to check the actual execution graph. It should look something similar to what is shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17792_10_9.jpg" alt="Figure 10.9 – Execution graph of our workflow within the Airflow web UI" width="1210" height="456"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Execution graph of our workflow within the Airflow web UI</p>
			<p>Although it is a very simple <a id="_idIndexMarker680"/>DAG, it gives an idea of how easy it is to work with Airflow using Cloud Composer. The level of logging and monitoring we get with Cloud Composer is quite amazing. Cloud Composer makes the lives of data engineers really easy so that they can focus on defining complex data pipelines without worrying about infrastructure and <span class="No-Break">Airflow management.</span></p>
			<p>We now have a good idea of how Vertex AI Pipelines and Cloud Composer (managed Airflow service) can be used as an orchestrator for ML workflows. Now let’s summarize some of the similarities and differences between <span class="No-Break">these two.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor144"/>Vertex AI Pipelines versus Cloud Composer</h1>
			<p>In this section, we will talk about some of the key similarities and differences between Vertex AI Pipelines and <a id="_idIndexMarker681"/>Cloud Composer when it comes to <a id="_idIndexMarker682"/>orchestrating ML workflows. Based on this comparison, we can choose the best solution for our next ML project. The following is a list of points that summarize the important aspects of both orchestrators for <span class="No-Break">ML-related tasks:</span></p>
			<ul>
				<li>Both are easy to use and divide the overall ML workflow into smaller execution units in terms of tasks (Composer) or containerized components (Vertex <span class="No-Break">AI Pipelines).</span></li>
				<li>Passing data between components is similar, and it may require an intermediate storage system if the data size <span class="No-Break">is large.</span></li>
				<li>Vertex AI Pipelines have an extensive list of prebuilt components available open source and thus developers can avoid writing a lot of boilerplate code. On the other hand, in the case of a Composer-based pipeline, we need to write the <span class="No-Break">entire workflow.</span></li>
				<li>Based on the ease of setting up environments, Vertex AI Pipelines is a little <span class="No-Break">bit easier.</span></li>
				<li>Both run on Kubernetes, but in the case of Vertex AI Pipelines, there is no need to worry about clusters, Pods, and <span class="No-Break">so on.</span></li>
				<li>Cloud Composer is ideal for data-related tasks. We can also implement ML pipelines as a data task but we lose a lot of ML-related functionalities, such as lineage tracking, metrics, experiment comparisons, and distributed training. These features come out of the box with Vertex <span class="No-Break">AI Pipelines.</span></li>
				<li>Data engineers might feel more comfortable with Composer pipelines, while ML engineers might be more comfortable with Vertex <span class="No-Break">AI Pipelines.</span></li>
				<li>In many cases, Vertex AI Pipelines can be cheaper to use as here we pay for what we use. On the other hand, in the case of Composer, some Pods are <span class="No-Break">always running.</span></li>
				<li>If needed, some of the Vertex AI Pipelines capabilities can be used with Composer <span class="No-Break">as well.</span></li>
				<li>Working with<a id="_idIndexMarker683"/> Vertex AI Pipelines requires<a id="_idIndexMarker684"/> zero knowledge about Kubernetes, but with Cloud Composer, it is important to know common aspects <span class="No-Break">of Kubernetes.</span></li>
			</ul>
			<p>After reading these comparison points, we might find it easy to choose the best orchestrator for our next ML use case. Nevertheless, both orchestrators are easy to use and are commonly used across organizations to manage their complex <span class="No-Break">data/ML-related workflows.</span></p>
			<p>Now that we have a good understanding of ML orchestration tools on Google Cloud with their pros and cons, we are ready to start developing production-grade ML pipelines. Next, let’s learn how to get predictions on <span class="No-Break">Vertex AI.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor145"/>Getting predictions on Vertex AI</h1>
			<p>In this section, we will <a id="_idIndexMarker685"/>learn how to get predictions from our ML <a id="_idIndexMarker686"/>models on Vertex AI. Depending on the use case, prediction requests can be of two types – online predictions (real time) and batch predictions. Online predictions are synchronous requests made to a model endpoint. Online predictions are needed by applications that keep requesting outputs for given inputs in a timely manner via an API call in order to update information for end users in near real time. For example, the Google Maps API gives us near real-time traffic updates and requires online prediction requests. Batch predictions, on the other hand, are asynchronous requests. If our use case only requires batch prediction, we might not need to deploy the model to an endpoint as the Vertex AI <strong class="source-inline">batchprediciton</strong> service also allows us to perform batch prediction from a saved model that is present in a GCS location without even needing to create an endpoint. Batch predictions are suitable for use cases where the response is not time sensitive and we can afford to get a delayed response (for example, an e-commerce company may wish to forecast sales for the next six months or so). Using batch predictions, we can make predictions of a large amount of data with just a <span class="No-Break">single request.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>Getting online predictions</h2>
			<p>We <a id="_idIndexMarker687"/>must deploy our model to an endpoint before<a id="_idIndexMarker688"/> that model can be used to serve online prediction requests. Model deployment essentially means keeping the model in memory with the required infrastructure (memory and compute) so that it can serve predictions with low latency. We can deploy multiple models to a single endpoint as well as a single model to multiple endpoints based on the use case and <span class="No-Break">scaling requirements.</span></p>
			<p>When you deploy a model using the Vertex AI API, you complete the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Create <span class="No-Break">an endpoint.</span></li>
				<li>Get the <span class="No-Break">endpoint ID.</span></li>
				<li>Deploy the model to <span class="No-Break">the endpoint.</span></li>
			</ol>
			<p>We can use the following Python sample function to create a Vertex AI endpoint. This function is taken from official <span class="No-Break">documentation (</span><a href="https://cloud.google.com/vertex-ai/docs/general/deployment#api"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/general/deployment#api</span></a><span class="No-Break">):</span></p>
			<pre class="source-code">
def create_endpoint_sample(
    project: str,
    display_name: str,
    location: str,
):
    aiplatform.init(project=project, location=location)
    endpoint = aiplatform.Endpoint.create(
        display_name=display_name,
        project=project,
        location=location,
    )
    print(endpoint.display_name)
    print(endpoint.resource_name)
    return endpoint</pre>			<p>The second step is to get the endpoint ID so that we can use it to deploy our model. The following shell command will give us a list of all the endpoints within our project and location. We can filter it with the endpoint name if we <span class="No-Break">have it:</span></p>
			<pre class="source-code">
gcloud ai endpoints list \
  --region=LOCATION \
  --filter=display_name=ENDPOINT_NAME</pre>			<p>Now that we <a id="_idIndexMarker689"/>have the endpoint ID, we can deploy our model<a id="_idIndexMarker690"/> to this endpoint. While deploying the model, we can specify parameters for a number of replicas, the accelerator count, accelerator types, and so on. The following is a sample Python function that can be used to deploy the model to a given endpoint. This sample has been taken from the Google Cloud <span class="No-Break">documentation (</span><a href="https://cloud.google.com/vertex-ai/docs/general/deployment#api"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/general/deployment#api</span></a><span class="No-Break">):</span></p>
			<pre class="source-code">
def deploy_model_with_dedicated_resources_sample(
    project,
    location,
    model_name: str,
    machine_type: str,
    endpoint: Optional[aiplatform.Endpoint] = None,
    deployed_model_display_name: Optional[str] = None,
    traffic_percentage: Optional[int] = 0,
    traffic_split: Optional[Dict[str, int]] = None,
    min_replica_count: int = 1,
    max_replica_count: int = 1,
    accelerator_type: Optional[str] = None,
    accelerator_count: Optional[int] = None,
    explanation_metadata: Optional[explain.ExplanationMetadata] = None,
    explanation_parameters: Optional[explain.ExplanationParameters] = None,
    metadata: Optional[Sequence[Tuple[str, str]]] = (),
    sync: bool = True,
):</pre>			<p>Here, we<a id="_idIndexMarker691"/> initialize the Vertex AI SDK and deploy our<a id="_idIndexMarker692"/> model to <span class="No-Break">an endpoint:</span></p>
			<pre class="source-code">
    aiplatform.init(project=project, location=location)
    model = aiplatform.Model(model_name=model_name)
    model.deploy(
        endpoint=endpoint, deployed_model_display_name=deployed_model_display_name,
        traffic_percentage=traffic_percentage,
        traffic_split=traffic_split,
        machine_type=machine_type,
        min_replica_count=min_replica_count,
        max_replica_count=max_replica_count,
        accelerator_type=accelerator_type,
        accelerator_count=accelerator_count,
        explanation_metadata=explanation_metadata,
        explanation_parameters=explanation_parameters,
        metadata=metadata,
        sync=sync,
    )
    model.wait()
    print(model.display_name)
    print(model.resource_name)
    return model</pre>			<p>Once our model<a id="_idIndexMarker693"/> is deployed to an endpoint, it is ready<a id="_idIndexMarker694"/> to serve online predictions. We can now make online prediction requests to this endpoint. See the following <span class="No-Break">sample request:</span></p>
			<pre class="source-code">
def endpoint_predict_sample(
    project: str, location: str, instances: list, endpoint: str
):
    aiplatform.init(project=project, location=location)
    endpoint = aiplatform.Endpoint(endpoint)
    prediction = endpoint.predict(instances=instances)
    print(prediction)
    return prediction</pre>			<p>The <strong class="source-inline">instances[]</strong> object is required and must contain the list of instances to get predictions for. See the <span class="No-Break">following example:</span></p>
			<pre class="source-code">
{
  "instances": [
    [0.0, 1.1, 2.2],
    [3.3, 4.4, 5.5],
    ...
  ]
}</pre>			<p>The response body is also similar. It may look something like the following example. This example is not related to the earlier model; it is just for <span class="No-Break">understanding purposes:</span></p>
			<pre class="source-code">
{
  "predictions": [
    {
      "label": "tree",
      "scores": [0.2, 0.8]
    },
    {
      "label": "bike",
      "scores": [0.85, 0.15]
    }
  ],
  "deployedModelId": 123456789012345678
}</pre>			<p>The response<a id="_idIndexMarker695"/> when there is an error in processing the input<a id="_idIndexMarker696"/> looks <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
{"error": "Divide by zero"}</pre>			<p>We now have a good idea of how to get online predictions using Vertex AI endpoints. But not every use case requires on-demand or online predictions. There are times when we want to make predictions on a large amount of data but the results are not immediately required. In such cases, we can utilize batch predictions. Let’s discuss more about getting batch predictions using <span class="No-Break">Vertex AI.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>Getting batch predictions</h2>
			<p>As discussed before, batch prediction<a id="_idIndexMarker697"/> requests are asynchronous<a id="_idIndexMarker698"/> and do not require a model to be deployed to an endpoint all the time. To make a batch prediction request, we specify an input source and an output location (either Cloud Storage or BigQuery), where Vertex AI stores prediction results. The input source location must contain our input instances in one of the accepted formats: TFRecord, JSON Lines, CSV, BigQuery, and <span class="No-Break">so on.</span></p>
			<p>TFRecord input instances may look something like the <span class="No-Break">following example:</span></p>
			<pre class="source-code">
{"instances": [
    { "b64": "b64EncodedASCIIString" },
    { "b64": "b64EncodedASCIIString" }
  ]}</pre>			<p>Batch prediction can be requested through Vertex AI API programatically or also with Google Cloud console UI. As we can pass lots of data to batch prediction requests, they may take a long time to complete depending upon the size of data <span class="No-Break">and model.</span></p>
			<p>A sample <a id="_idIndexMarker699"/>batch prediction request using the <a id="_idIndexMarker700"/>Vertex AI API with Python may look something like the following Python function. This sample code has been taken from the official <span class="No-Break">documentation (</span><a href="https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions"><span class="No-Break">https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions</span></a><span class="No-Break">):</span></p>
			<pre class="source-code">
def create_batch_prediction_job_dedicated_resources_sample(
    project: str,
    location: str,
    model_resource_name: str,
    job_display_name: str,
    gcs_source: Union[str, Sequence[str]],
    gcs_destination: str,
    instances_format: str = "jsonl",
    machine_type: str = "n1-standard-2",
    accelerator_count: int = 1,
    accelerator_type: Union[str, aiplatform_v1.AcceleratorType] = "NVIDIA_TESLA_K80",
    starting_replica_count: int = 1,
    max_replica_count: int = 1,
    sync: bool = True,
):</pre>			<p>Here, we initialize the Vertex AI SDK and call batch predictions on our <span class="No-Break">deployed model:</span></p>
			<pre class="source-code">
    aiplatform.init(project=project, location=location)
    my_model = aiplatform.Model(model_resource_name)
    batch_prediction_job = my_model.batch_predict(
        job_display_name=job_display_name,
        gcs_source=gcs_source,
        gcs_destination_prefix=gcs_destination,
        instances_format=instances_format,
        machine_type=machine_type,
        accelerator_count=accelerator_count,
        accelerator_type=accelerator_type,
        starting_replica_count=starting_replica_count,
        max_replica_count=max_replica_count,
        sync=sync,
    )
    batch_prediction_job.wait()
    print(batch_prediction_job.display_name)
    print(batch_prediction_job.resource_name)
    print(batch_prediction_job.state)
    return batch_prediction_job</pre>			<p>Once the <a id="_idIndexMarker701"/>batch prediction request is complete, the <a id="_idIndexMarker702"/>output is saved in the specified Cloud Storage or <span class="No-Break">BigQuery location.</span></p>
			<p>A <strong class="source-inline">jsonl</strong> output file might look something like the following <span class="No-Break">example output:</span></p>
			<pre class="source-code">
{ "instance": [1, 2, 3, 4], "prediction": [0.1,0.9]}
{ "instance": [5, 6, 7, 8], "prediction": [0.7,0.3]}</pre>			<p>We now have a fair idea of how online and batch prediction work on Vertex AI. The idea of separating batch prediction from online prediction (eliminating the need for deployment) saves a lot of resources and costs. Next, let’s discuss some important considerations related to deployed models on Google <span class="No-Break">Vertex AI.</span></p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Managing deployed models on Vertex AI</h1>
			<p>When <a id="_idIndexMarker703"/>we deploy an ML model to an endpoint, we associate it <a id="_idIndexMarker704"/>with physical resources (compute) so that it can serve online predictions at low latency. Depending on the requirements, we might want to deploy multiple models to a single endpoint or a single model to multiple endpoints as well. Let’s learn about these <span class="No-Break">two scenarios.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>Multiple models – single endpoint</h2>
			<p>Suppose we already<a id="_idIndexMarker705"/> have one model deployed to an endpoint in production and we have found some interesting ideas to improve that model. Now, suppose we have already trained an improved model that we want to deploy but we also don’t want to make any sudden changes to our application. In this situation, we can add our latest model to the existing endpoint and start serving a very small percentage of traffic with the new model. If everything looks great, we can gradually increase the traffic until it is serving the full 100% of <span class="No-Break">the traffic.</span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>Single model – multiple endpoints</h2>
			<p>This is useful<a id="_idIndexMarker706"/> when we want to deploy our model with different resources for different application environments, such as testing and production. Secondly, if one of our applications has high-performance needs, we can serve it using an endpoint with high-performance machines, while we can serve other applications with lower-performance machines to optimize <span class="No-Break">operationalization costs.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>Compute resources and scaling</h2>
			<p>Vertex AI allocates <a id="_idIndexMarker707"/>compute nodes to handle online and batch predictions. When <a id="_idIndexMarker708"/>we deploy our ML model to an endpoint, we can customize the type of virtual machines to be used for serving the model. We can choose accelerators such as GPUs or TPUs if needed. A machine configuration with more computing resources can serve predictions with lower latency, hence handling more prediction requests at the same time. But such a machine will cost more than a machine with low compute resources. Thus, it is important to choose the best-suited machine depending on the use case <span class="No-Break">and requirements.</span></p>
			<p>When we deploy a model for online predictions, we can also configure a prediction node to automatically scale. But the prediction nodes for batch prediction do not automatically scale. By default, if we deploy a model with or without dedicated GPU resources, Vertex AI will automatically scale the number of replicas up or down so that CPU or GPU usage (whichever is higher) matches the default 60% target value. Given these conditions, Vertex AI will scale up, even if this may not have been needed to achieve <strong class="bold">queries per second</strong> (<strong class="bold">QPS</strong>) and <a id="_idIndexMarker709"/>latency targets. We can monitor the endpoint to track metrics such as CPU and accelerator usage, the number of requests, and latency, as well as the current and target number <span class="No-Break">of replicas.</span></p>
			<p>To determine the ideal <a id="_idIndexMarker710"/>machine type for a prediction container from a<a id="_idIndexMarker711"/> cost perspective, we can deploy it to a virtual machine instance and benchmark the instance by making prediction requests until the virtual machine hits about 90% of the CPU usage. By doing this experiment a few times on different machines, we can identify the cost of the prediction service based on the <span class="No-Break">QPS values.</span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor152"/>Summary</h1>
			<p>In this chapter, we have learned about two popular ML workflow orchestration tools – Vertex AI Pipelines (managed Kubeflow) and Cloud Composer (managed Airflow). We have also implemented a Vertex Pipeline for an example use case, and similarly, we have also developed and executed an example DAG with Cloud Composer. Both Vertex AI Pipelines and Cloud Composer are managed services on GCP and make it really easy to set up and launch complex ML and data-related workflows. Finally, we have learned about getting online and batch predictions on Vertex AI for our custom models, including some best practices related to <span class="No-Break">model deployments.</span></p>
			<p>After reading this chapter, you should have a good understanding of different ways of carrying out ML workflow orchestration on GCP and their similarities and differences. Now, you should be able to write your own ML workflows and orchestrate them on GCP via either Vertex AI Pipelines or Cloud Composer. Finally, you should also be confident in getting online and batch predictions using <span class="No-Break">Vertex AI.</span></p>
			<p>Now that we have a good understanding of deploying ML models on GCP, and also orchestrating ML workflows, we can start developing production-grade pipelines for different use cases. Along similar lines, we will learn about some ML governance best practices and tools in the <span class="No-Break">upcoming chapter.</span></p>
		</div>
	</div>
</div>
</body></html>