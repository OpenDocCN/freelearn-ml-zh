<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;5.&#xA0;Classification (I) &#x2013; Tree, Lazy, and Probabilistic"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Classification (I) – Tree, Lazy, and Probabilistic</h1></div></div></div><p class="calibre7">In this chapter, we will cover the following recipes:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Preparing the training and testing datasets</li><li class="listitem">Building a classification model with recursive partitioning trees</li><li class="listitem">Visualizing a recursive partitioning tree</li><li class="listitem">Measuring the prediction performance of a recursive partitioning tree</li><li class="listitem">Pruning a recursive partitioning tree</li><li class="listitem">Building a classification model with a conditional inference tree</li><li class="listitem">Visualizing a conditional inference tree</li><li class="listitem">Measuring the prediction performance of a conditional inference tree</li><li class="listitem">Classifying data with a k-nearest neighbor classifier</li><li class="listitem">Classifying data with logistic regression</li><li class="listitem">Classifying data with the Naïve Bayes classifier</li></ul></div></div>

<div class="book" title="Chapter&#xA0;5.&#xA0;Classification (I) &#x2013; Tree, Lazy, and Probabilistic">
<div class="book" title="Introduction"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec54" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre7">Classification is used to identify a category of new observations (testing datasets) based on a <a id="id431" class="calibre1"/>classification model built from the training dataset, of which the categories are already known. Similar to regression, classification is categorized as a supervised learning method as it employs known answers (label) of a training dataset to<a id="id432" class="calibre1"/> predict the answer (label) of the testing <a id="id433" class="calibre1"/>dataset. The main difference between regression and classification is that regression is used to predict continuous values. </p><p class="calibre7">In contrast to this, classification is used to identify the category of a given observation. For example, one may use regression to predict the future price of a given stock based on historical prices. However, one should use the classification method to predict whether the stock price will rise or fall.</p><p class="calibre7">In this chapter, we will illustrate how to use R to perform classification. We first build a training dataset and a testing dataset from the churn dataset, and then apply different classification methods to classify the churn dataset. In the following recipes, we will introduce the tree-based classification method using a traditional classification tree and a conditional inference tree, lazy-based algorithm, and a probabilistic-based method using the training dataset to build up a classification model, and then use the model to predict the category (class label) of the testing dataset. We will also use a confusion matrix to measure the performance.</p></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Preparing the training and testing datasets"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec55" class="calibre1"/>Preparing the training and testing datasets</h1></div></div></div><p class="calibre7">Building a<a id="id434" class="calibre1"/> classification model requires a<a id="id435" class="calibre1"/> training dataset to train the classification model, and testing data is needed to then validate the prediction performance. In the following recipe, we will demonstrate how to split the telecom churn dataset into  training and testing datasets, respectively.</p></div>

<div class="book" title="Preparing the training and testing datasets">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec177" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">In this recipe, we will use the telecom churn dataset as the input data source, and split the data into training and testing datasets.</p></div></div>

<div class="book" title="Preparing the training and testing datasets">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec178" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to split the churn dataset into training and testing datasets:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can retrieve the churn dataset from the <code class="email">C50</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("C50")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(C50)</strong></span>
<span class="strong"><strong class="calibre2">&gt; data(churn)</strong></span>
</pre></div></li><li class="listitem" value="2">Use <code class="email">str</code> to read the structure of the dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; str(churnTrain)</strong></span>
</pre></div></li><li class="listitem" value="3">We can remove the <code class="email">state</code>, <code class="email">area_code</code>, and <code class="email">account_length</code> attributes, which are not appropriate for classification features:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churnTrain = churnTrain[,! names(churnTrain) %in% c("state", "area_code", "account_length") ]</strong></span>
</pre></div></li><li class="listitem" value="4">Then, split 70 percent of the data into the training dataset and 30 percent of the data into the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; set.seed(2)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ind = sample(2, nrow(churnTrain), replace = TRUE, prob=c(0.7, 0.3))</strong></span>
<span class="strong"><strong class="calibre2">&gt; trainset = churnTrain[ind == 1,]</strong></span>
<span class="strong"><strong class="calibre2">&gt; testset = churnTrain[ind == 2,]</strong></span>
</pre></div></li><li class="listitem" value="5">Lastly, use <code class="email">dim</code> to explore the dimensions of both the training and testing datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; dim(trainset)</strong></span>
<span class="strong"><strong class="calibre2">[1] 2315   17</strong></span>
<span class="strong"><strong class="calibre2">&gt; dim(testset)</strong></span>
<span class="strong"><strong class="calibre2">[1] 1018   17</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Preparing the training and testing datasets">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec179" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id436" class="calibre1"/> recipe, we use the telecom churn <a id="id437" class="calibre1"/>dataset as our example data source. The dataset contains 20 variables with 3,333 observations. We would like to build a classification model to predict whether a customer will churn, which is very important to the telecom company as the cost of acquiring a new customer is significantly more than retaining one.</p><p class="calibre7">Before building the classification model, we need to preprocess the data first. Thus, we load the churn data from the <code class="email">C50</code> package into the R session with the variable name as <code class="email">churn</code>. As we determined that attributes such as <code class="email">state</code>, <code class="email">area_code</code>, and <code class="email">account_length</code> are not useful features for building the classification model, we remove these attributes.</p><p class="calibre7">After preprocessing the data, we split it into training and testing datasets, respectively. We then use a sample function to randomly generate a sequence containing 70 percent of the training dataset and 30 percent of the testing dataset with a size equal to the number of observations. Then, we use a generated sequence to split the churn dataset into the training dataset, <code class="email">trainset</code>, and the testing dataset, <code class="email">testset</code>. Lastly, by using the <code class="email">dim</code> function, we found that 2,315 out of the 3,333 observations are categorized into the training dataset, <code class="email">trainset</code>, while the other 1,018 are categorized into the testing dataset, <code class="email">testset</code>.</p></div></div>

<div class="book" title="Preparing the training and testing datasets">
<div class="book" title="There's more..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec180" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre7">You can combine the split process of the training and testing datasets into the <code class="email">split.data</code> function. Therefore, you can easily split the data into the two datasets by calling this function and specifying the proportion and seed in the parameters:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; split.data = function(data, p = 0.7, s = 666){</strong></span>
<span class="strong"><strong class="calibre2">+   set.seed(s)</strong></span>
<span class="strong"><strong class="calibre2">+   index = sample(1:dim(data)[1])</strong></span>
<span class="strong"><strong class="calibre2">+   train = data[index[1:floor(dim(data)[1] * p)], ]</strong></span>
<span class="strong"><strong class="calibre2">+   test = data[index[((ceiling(dim(data)[1] * p)) + 1):dim(data)[1]], ]</strong></span>
<span class="strong"><strong class="calibre2">+   return(list(train = train, test = test))</strong></span>
<span class="strong"><strong class="calibre2">+ } </strong></span>
</pre></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Building a classification model with recursive partitioning trees"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec56" class="calibre1"/>Building a classification model with recursive partitioning trees</h1></div></div></div><p class="calibre7">A classification tree uses a split condition to predict class labels based on one or multiple input <a id="id438" class="calibre1"/>variables. The <a id="id439" class="calibre1"/>classification process starts from the root node of the tree; at each node, the process will check whether the input value should recursively continue to the right or left sub-branch according to the split condition, and stops when meeting any leaf (terminal) nodes of the decision tree. In this recipe, we will introduce how to apply a recursive partitioning tree on the customer churn dataset.</p></div>

<div class="book" title="Building a classification model with recursive partitioning trees">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec181" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the previous recipe by splitting the churn dataset into the training dataset (<code class="email">trainset</code>) and testing dataset (<code class="email">testset</code>), and each dataset should contain exactly 17 variables.</p></div></div>

<div class="book" title="Building a classification model with recursive partitioning trees">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec182" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to split the churn dataset into training and testing datasets:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Load the <code class="email">rpart</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(rpart)</strong></span>
</pre></div></li><li class="listitem" value="2">Use the <code class="email">rpart</code> function to build a classification tree model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.rp = rpart(churn ~ ., data=trainset)</strong></span>
</pre></div></li><li class="listitem" value="3">Type <code class="email">churn.rp</code> to retrieve the node detail of the classification tree:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.rp </strong></span>
</pre></div></li><li class="listitem" value="4">Next, use <a id="id440" class="calibre1"/>the <code class="email">printcp</code> function to examine the complexity parameter:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; printcp(churn.rp)</strong></span>

<span class="strong"><strong class="calibre2">Classification tree:</strong></span>
<span class="strong"><strong class="calibre2">rpart(formula = churn ~ ., data = trainset)</strong></span>

<span class="strong"><strong class="calibre2">Variables actually used in tree construction:</strong></span>
<span class="strong"><strong class="calibre2">[1] international_plan            number_customer_service_calls</strong></span>
<span class="strong"><strong class="calibre2">[3] total_day_minutes             total_eve_minutes            </strong></span>
<span class="strong"><strong class="calibre2">[5] total_intl_calls              total_intl_minutes           </strong></span>
<span class="strong"><strong class="calibre2">[7] voice_mail_plan              </strong></span>

<span class="strong"><strong class="calibre2">Root node error: 342/2315 = 0.14773</strong></span>

<span class="strong"><strong class="calibre2">n= 2315 </strong></span>

<span class="strong"><strong class="calibre2">        CP nsplit rel error  xerror     xstd</strong></span>
<span class="strong"><strong class="calibre2">1 0.076023      0   1.00000 1.00000 0.049920</strong></span>
<span class="strong"><strong class="calibre2">2 0.074561      2   0.84795 0.99708 0.049860</strong></span>
<span class="strong"><strong class="calibre2">3 0.055556      4   0.69883 0.76023 0.044421</strong></span>
<span class="strong"><strong class="calibre2">4 0.026316      7   0.49415 0.52632 0.037673</strong></span>
<span class="strong"><strong class="calibre2">5 0.023392      8   0.46784 0.52047 0.037481</strong></span>
<span class="strong"><strong class="calibre2">6 0.020468     10   0.42105 0.50877 0.037092</strong></span>
<span class="strong"><strong class="calibre2">7 0.017544     11   0.40058 0.47076 0.035788</strong></span>
<span class="strong"><strong class="calibre2">8 0.010000     12   0.38304 0.47661 0.035993</strong></span>
</pre></div></li><li class="listitem" value="5">Next, use <a id="id441" class="calibre1"/>the <code class="email">plotcp</code> function to plot the cost complexity parameters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plotcp(churn.rp)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00101.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 1: The cost complexity parameter plot</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Lastly, use the <code class="email">summary</code> function to examine the built model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(churn.rp)</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Building a classification model with recursive partitioning trees">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec183" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id442" class="calibre1"/> recipe, we use a <a id="id443" class="calibre1"/>recursive partitioning tree from the <code class="email">rpart</code> package to build a tree-based classification model. The recursive portioning tree includes two processes: recursion and partitioning. During the process of decision induction, we have to consider a statistic evaluation question (or simply a yes/no question) to partition the data into different partitions in accordance with the assessment result. Then, as we have determined the child node, we can repeatedly perform the splitting until the stop criteria is satisfied.</p><p class="calibre7">For example, the data (shown in the following figure) in the root node can be partitioned into two groups with regard to the question of whether <span class="strong"><strong class="calibre2">f1</strong></span> is smaller than <span class="strong"><strong class="calibre2">X</strong></span>. If so, the data is divided into the left-hand side. Otherwise, it is split into the right-hand side. Then, we can continue to partition the left-hand side data with the question of whether <span class="strong"><strong class="calibre2">f2</strong></span> is smaller than <span class="strong"><strong class="calibre2">Y</strong></span>:</p><div class="mediaobject"><img src="../images/00102.jpeg" alt="How it works..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 2: Recursive partioning tree</p></div></div><p class="calibre10"> </p><p class="calibre7">In the<a id="id444" class="calibre1"/> first step, we load <a id="id445" class="calibre1"/>the <code class="email">rpart</code> package with the <code class="email">library</code> function. Next, we build a classification model using the <code class="email">churn</code> variable as a classification category (class label) and the remaining variables as input features.</p><p class="calibre7">After the model is built, you can type the variable name of the built model, <code class="email">churn.rp</code>, to display the tree node details. In the printed node detail, <code class="email">n</code> indicates the sample size, <code class="email">loss</code> indicates the misclassification cost, <code class="email">yval</code> stands for the classified membership (<code class="email">no</code> or <code class="email">yes</code>, in this case), and <code class="email">yprob</code> stands for the probabilities of two classes (the left value refers to the probability reaching label <code class="email">no</code>, and the right value refers to the probability reaching label, <code class="email">yes</code>).</p><p class="calibre7">Then, we use the <code class="email">printcp</code> function to print the complexity parameters of the built tree model. From the output of <code class="email">printcp</code>, one should find the value of CP, a complexity parameter, which serves as a penalty to control the size of the tree. In short, the greater the CP value, the fewer the number of splits there are (<code class="email">nsplit</code>). The output value (the <code class="email">rel</code> error) represents the average deviance of the current tree divided by the average deviance of the null tree. A <code class="email">xerror</code> value represents the relative error estimated by a 10-fold classification. <code class="email">xstd</code> stands for the standard error of the relative error.</p><p class="calibre7">To make <a id="id446" class="calibre1"/>the <span class="strong"><strong class="calibre2">CP</strong></span> (<span class="strong"><strong class="calibre2">cost complexity parameter</strong></span>) table more readable, we use <code class="email">plotcp</code> to generate an information graphic of the CP table. As per the screenshot (step 5), the x-axis at the bottom illustrates the <code class="email">cp</code> value, the y-axis illustrates the relative error, and the upper x-axis displays the size of the tree. The dotted line indicates the upper limit of a standard deviation. From the screenshot, we can determine that minimum cross-validation error occurs when the tree is at a size of 12.</p><p class="calibre7">We<a id="id447" class="calibre1"/> can also use the <code class="email">summary</code> function to display the function call, complexity parameter table for the fitted<a id="id448" class="calibre1"/> tree model, variable importance, which helps identify the most important variable for the tree classification (summing up to 100), and detailed information of each node.</p><p class="calibre7">The advantage of using the decision tree is that it is very flexible and easy to interpret. It works on both classification and regression problems, and more; it is nonparametric. Therefore, one does not have to worry about whether the data is linear separable. As for the disadvantage of using the decision tree, it is that it tends to be biased and over-fitted. However, you can conquer the bias problem through the use of a conditional inference tree, and solve the problem of over-fitting through a random forest method or tree pruning.</p></div></div>

<div class="book" title="Building a classification model with recursive partitioning trees">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec184" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For more information about the <code class="email">rpart</code>, <code class="email">printcp</code>, and <code class="email">summary</code> functions, please use the <code class="email">help</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?rpart</strong></span>
<span class="strong"><strong class="calibre2">&gt; ?printcp</strong></span>
<span class="strong"><strong class="calibre2">&gt; ?summary.rpart</strong></span>
</pre></div></li><li class="listitem"><code class="email">C50</code> is another <a id="id449" class="calibre1"/>package that provides a decision tree and a rule-based <a id="id450" class="calibre1"/>model. If you are interested in the package, you may refer to the document at <a class="calibre1" href="http://cran.r-project.org/web/packages/C50/C50.pdf">http://cran.r-project.org/web/packages/C50/C50.pdf</a>.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing a recursive partitioning tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec57" class="calibre1"/>Visualizing a recursive partitioning tree</h1></div></div></div><p class="calibre7">From the <a id="id451" class="calibre1"/>last recipe, we learned how to print the classification tree in a text format. To make the tree more readable, we can use the <code class="email">plot</code> function to obtain the graphical display of a built classification tree.</p></div>

<div class="book" title="Visualizing a recursive partitioning tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec185" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">One needs to have the previous recipe completed by generating a classification model, and assign the model into the <code class="email">churn.rp</code> variable.</p></div></div>

<div class="book" title="Visualizing a recursive partitioning tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec186" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform <a id="id452" class="calibre1"/>the following steps to visualize the classification tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Use the <code class="email">plot</code> function and the <code class="email">text</code> function to plot the classification tree:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(churn.rp, margin= 0.1)</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(churn.rp, all=TRUE, use.n = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00103.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 3: The graphical display of a classification tree</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">You can also specify the <code class="email">uniform</code>, <code class="email">branch</code>, and <code class="email">margin</code> parameter to adjust the layout:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(churn.rp, uniform=TRUE, branch=0.6, margin=0.1)</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(churn.rp, all=TRUE, use.n = TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00104.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 4: Adjust the layout of the classification tree</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing a recursive partitioning tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec187" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Here, we <a id="id453" class="calibre1"/>demonstrate how to use the <code class="email">plot</code> function to graphically display a classification tree. The <code class="email">plot</code> function can simply visualize the classification tree, and you can then use the <code class="email">text</code> function to add text to the plot.</p><p class="calibre7">In <span class="strong"><em class="calibre8">Figure 3</em></span>, we assign margin = 0.1 as a parameter to add extra white space around the border to prevent the displayed text being truncated by the margin. It shows that the length of the branches displays the relative magnitude of the drop in deviance. We then use the text function to add labels for the nodes and branches. By default, the text function will add a split condition on each split, and add a category label in each terminal node. In order to add extra information on the tree plot, we set the parameter as all equal to <code class="email">TRUE</code> to add a label to all the nodes. In addition to this, we add a parameter by specifying <code class="email">use.n = TRUE</code> to add extra information, which shows that the actual number of observations fall into two different categories (no and yes).</p><p class="calibre7">In <span class="strong"><em class="calibre8">Figure 4</em></span>, we set the option branch to 0.6 to add a shoulder to each plotted branch. In addition to this, in order to display branches of an equal length rather than relative magnitude of the drop in deviance, we set the option uniform to <code class="email">TRUE</code>. As a result, <span class="strong"><em class="calibre8">Figure 4</em></span> shows a classification tree with short shoulders and branches of equal length.</p></div></div>

<div class="book" title="Visualizing a recursive partitioning tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec188" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">You may use <code class="email">?plot.rpart</code> to read more about the plotting of the classification tree. This document also includes information on how to specify the <a id="id454" class="calibre1"/>parameters, <code class="email">uniform</code>, <code class="email">branch</code>, <code class="email">compress</code>, <code class="email">nspace</code>, <code class="email">margin</code>, and <code class="email">minbranch</code>, to adjust the layout of the classification tree.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring the prediction performance of a recursive partitioning tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec58" class="calibre1"/>Measuring the prediction performance of a recursive partitioning tree</h1></div></div></div><p class="calibre7">Since we have built a classification tree in the previous recipes, we can use it to predict the<a id="id455" class="calibre1"/> category (class label) of new observations. Before making a prediction, we first validate the prediction power of the classification tree, which can be done by generating a classification table on the testing dataset. In this recipe, we will introduce how to generate a predicted label versus a real label table with the <code class="email">predict</code> function and the <code class="email">table</code> function, and explain how to generate a confusion matrix to measure the performance.</p></div>

<div class="book" title="Measuring the prediction performance of a recursive partitioning tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec189" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the previous recipe completed by generating the classification model, <code class="email">churn.rp</code>. In addition to this, you have to prepare the training dataset, <code class="email">trainset</code>, and the testing dataset, <code class="email">testset</code>, generated in the first recipe of this chapter.</p></div></div>

<div class="book" title="Measuring the prediction performance of a recursive partitioning tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec190" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to validate the prediction performance of a classification tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can use the <code class="email">predict</code> function to generate a predicted label of testing the dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predictions = predict(churn.rp, testset, type="class")</strong></span>
</pre></div></li><li class="listitem" value="2">Use the <code class="email">table</code> function to generate a classification table for the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(testset$churn, predictions)</strong></span>
<span class="strong"><strong class="calibre2">     predictions</strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes 100  41</strong></span>
<span class="strong"><strong class="calibre2">  no   18 859</strong></span>
</pre></div></li><li class="listitem" value="3">One can further generate a confusion matrix using the <code class="email">confusionMatrix</code> function provided in the <code class="email">caret</code> package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(caret)</strong></span>
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(table(predictions, testset$churn))</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">           </strong></span>
<span class="strong"><strong class="calibre2">predictions yes  no</strong></span>
<span class="strong"><strong class="calibre2">        yes 100  18</strong></span>
<span class="strong"><strong class="calibre2">        no   41 859</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.942           </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.9259, 0.9556)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.7393          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 0.004181        </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.70922         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.97948         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.84746         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.95444         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.09823         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.11591         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.84435         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes             </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring the prediction performance of a recursive partitioning tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec191" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this <a id="id456" class="calibre1"/>recipe, we use a <code class="email">predict</code> function and built up classification model, <code class="email">churn.rp</code>, to predict the possible class labels of the testing dataset, <code class="email">testset</code>. The predicted categories (class labels) are coded as either no or yes. Then, we use the <code class="email">table</code> function to generate a classification table on the testing dataset. From the table, we discover that there are 859 correctly predicted as no, while 18 are misclassified as yes. 100 of the yes predictions are correctly predicted, but 41 observations are misclassified into no. Further, we use the <code class="email">confusionMatrix</code> function from the <code class="email">caret</code> package to produce a measurement of the classification model.</p></div></div>

<div class="book" title="Measuring the prediction performance of a recursive partitioning tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec192" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">You <a id="id457" class="calibre1"/>may use <code class="email">?confusionMatrix</code> to read more about the performance measurement using the confusion matrix</li><li class="listitem">For those who are interested in the definition output by the confusion matrix, please<a id="id458" class="calibre1"/> refer to the Wikipedia entry, <span class="strong"><strong class="calibre2">Confusion_matrix</strong></span> (<a class="calibre1" href="http://en.wikipedia.org/wiki/Confusion_matrix">http://en.wikipedia.org/wiki/Confusion_matrix</a>)</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Pruning a recursive partitioning tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec59" class="calibre1"/>Pruning a recursive partitioning tree</h1></div></div></div><p class="calibre7">In previous recipes, we have built a complex decision tree for the churn dataset. However, sometimes we have to remove sections that are not powerful in classifying instances<a id="id459" class="calibre1"/> to avoid over-fitting, and to improve the prediction accuracy. Therefore, in this recipe, we introduce the cost complexity pruning method to prune the classification tree.</p></div>

<div class="book" title="Pruning a recursive partitioning tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec193" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the previous recipe completed by generating a classification model, and assign the model into the <code class="email">churn.rp</code> variable.</p></div></div>

<div class="book" title="Pruning a recursive partitioning tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec194" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to prune the classification tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Find the minimum cross-validation error of the classification tree model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; min(churn.rp$cptable[,"xerror"])</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.4707602</strong></span>
</pre></div></li><li class="listitem" value="2">Locate the record with the minimum cross-validation errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; which.min(churn.rp$cptable[,"xerror"])</strong></span>
<span class="strong"><strong class="calibre2">7 </strong></span>
</pre></div></li><li class="listitem" value="3">Get the cost complexity parameter of the record with the minimum cross-validation errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.cp = churn.rp$cptable[7,"CP"]</strong></span>
<span class="strong"><strong class="calibre2">&gt; churn.cp</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.01754386</strong></span>
</pre></div></li><li class="listitem" value="4">Prune the tree by setting the <code class="email">cp</code> parameter to the CP value of the record with minimum cross-validation errors:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; prune.tree = prune(churn.rp, cp= churn.cp)</strong></span>
</pre></div></li><li class="listitem" value="5">Visualize <a id="id460" class="calibre1"/>the classification tree by using the <code class="email">plot</code> and <code class="email">text</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(prune.tree, margin= 0.1)</strong></span>
<span class="strong"><strong class="calibre2">&gt; text(prune.tree, all=TRUE , use.n=TRUE)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00105.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 5: The pruned classification tree</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="6">Next, you can generate a classification table based on the pruned classification tree model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; predictions = predict(prune.tree, testset, type="class")</strong></span>
<span class="strong"><strong class="calibre2">&gt; table(testset$churn, predictions)</strong></span>
<span class="strong"><strong class="calibre2">     predictions</strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes  95  46</strong></span>
<span class="strong"><strong class="calibre2">  no   14 863</strong></span>
</pre></div></li><li class="listitem" value="7">Lastly, you can generate a confusion matrix based on the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(table(predictions, testset$churn))</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">   </strong></span>
<span class="strong"><strong class="calibre2">        </strong></span>
<span class="strong"><strong class="calibre2">predictions yes  no</strong></span>
<span class="strong"><strong class="calibre2">        yes  95  14</strong></span>
<span class="strong"><strong class="calibre2">        no   46 863</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9411          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.9248, 0.9547)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 2.786e-16       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.727           </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 6.279e-05       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.67376         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.98404         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.87156         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.94939         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.09332         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.10707         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.82890         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes             </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Pruning a recursive partitioning tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec195" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this<a id="id461" class="calibre1"/> recipe, we discussed pruning a classification tree to avoid over-fitting and producing a more robust classification model. We first located the record with the minimum cross-validation errors within the <code class="email">cptable</code>, and we then extracted the CP of the record and assigned the value to <code class="email">churn.cp</code>. Next, we used the <code class="email">prune</code> function to prune the classification tree with <code class="email">churn.cp</code> as the parameter. Then, by using the <code class="email">plot</code> function, we graphically displayed the pruned classification tree. From <span class="strong"><em class="calibre8">Figure 5</em></span>, it is clear that the split of the tree is less than the original classification tree (<span class="strong"><em class="calibre8">Figure 3</em></span>). Lastly, we produced a classification table and used the confusion matrix to validate the performance of the pruned tree. The result shows that the accuracy (0.9411) is slightly lower than the original model (0.942), and also suggests that the pruned tree may not perform better than the original classification tree as we have pruned some split conditions (Still, one should examine the change in sensitivity and specificity). However, the pruned tree model is more robust as it removes some split conditions that may lead to over-fitting.</p></div></div>

<div class="book" title="Pruning a recursive partitioning tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec196" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For<a id="id462" class="calibre1"/> those who would like to know more <a id="id463" class="calibre1"/>about cost complexity pruning, please refer to the Wikipedia article for <span class="strong"><strong class="calibre2">Pruning (decision_trees)</strong></span>: <a class="calibre1" href="http://en.wikipedia.org/wiki/Pruning_(decision_trees">http://en.wikipedia.org/wiki/Pruning_(decision_trees</a></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Building a classification model with a conditional inference tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec60" class="calibre1"/>Building a classification model with a conditional inference tree</h1></div></div></div><p class="calibre7">In addition to traditional decision trees (<code class="email">rpart</code>), conditional inference trees (<code class="email">ctree</code>) are another<a id="id464" class="calibre1"/> popular tree-based<a id="id465" class="calibre1"/> classification method. Similar to traditional decision trees, conditional inference trees also recursively partition the data by performing a univariate split on the dependent variable. However, what makes conditional inference trees different from traditional decision trees is that conditional inference trees adapt the significance test procedures to select variables rather than selecting variables by maximizing information measures (<code class="email">rpart</code> employs a Gini coefficient). In this recipe, we will introduce how to adapt a conditional inference tree to build a classification model.</p></div>

<div class="book" title="Building a classification model with a conditional inference tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec197" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the first recipe completed by generating the training dataset, <code class="email">trainset</code>, and the testing dataset, <code class="email">testset</code>.</p></div></div>

<div class="book" title="Building a classification model with a conditional inference tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec198" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to build the conditional inference tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, we use <code class="email">ctree</code> from the <code class="email">party</code> package to build the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(party)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ctree.model = ctree(churn ~ . , data = trainset)</strong></span>
</pre></div></li><li class="listitem" value="2">Then, we examine the built tree model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ctree.model</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Building a classification model with a conditional inference tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec199" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we used a conditional inference tree to build a classification tree. The use of <code class="email">ctree</code> is similar to <code class="email">rpart</code>. Therefore, you can easily test the classification power using either a traditional decision tree or a conditional inference tree while confronting classification problems. Next, we obtain the node details of the classification tree by examining the built model. Within the model, we discover that <code class="email">ctree</code> provides information<a id="id466" class="calibre1"/> similar to a split<a id="id467" class="calibre1"/> condition, criterion (1 – p-value), statistics (test statistics), and weight (the case weight corresponding to the node). However, it does not offer as much information as <code class="email">rpart</code> does through the use of the <code class="email">summary</code> function.</p></div></div>

<div class="book" title="Building a classification model with a conditional inference tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec200" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">You may <a id="id468" class="calibre1"/>use the <code class="email">help</code> function to refer to the definition of <span class="strong"><strong class="calibre2">Binary</strong></span> <span class="strong"><strong class="calibre2">Tree</strong></span> <span class="strong"><strong class="calibre2">Class</strong></span> and read more about the properties of binary trees:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2"> &gt; help("BinaryTree-class")</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Visualizing a conditional inference tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec61" class="calibre1"/>Visualizing a conditional inference tree</h1></div></div></div><p class="calibre7">Similar to <code class="email">rpart</code>, the <code class="email">party</code> package also provides a visualization method for users to plot <a id="id469" class="calibre1"/>conditional inference trees. In the following recipe, we will introduce how to use the <code class="email">plot</code> function to visualize conditional inference trees.</p></div>

<div class="book" title="Visualizing a conditional inference tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec201" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the first recipe completed by generating the conditional inference tree model, <code class="email">ctree.model</code>. In addition to this, you need to have both, <code class="email">trainset</code> and <code class="email">testset</code>, loaded in an R session.</p></div></div>

<div class="book" title="Visualizing a conditional inference tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec202" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to visualize the conditional inference tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Use the <code class="email">plot</code> function to plot <code class="email">ctree.model</code> built in the last recipe:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; plot(ctree.model)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00106.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 6: A conditional inference tree of churn data</p></div></div><p class="calibre13"> </p></li><li class="listitem" value="2">To obtain<a id="id470" class="calibre1"/> a simple conditional inference tree, one can reduce the built model with less input features, and redraw the classification tree:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; daycharge.model = ctree(churn ~ total_day_charge, data = trainset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; plot(daycharge.model)</strong></span>
</pre></div><div class="mediaobject"><img src="../images/00107.jpeg" alt="How to do it..." class="calibre9"/><div class="caption"><p class="calibre12">Figure 7: A conditional inference tree using the total_day_charge variable as only split condition</p></div></div><p class="calibre13"> </p></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Visualizing a conditional inference tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec203" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">To<a id="id471" class="calibre1"/> visualize the node detail of the conditional inference tree, we can apply the <code class="email">plot</code> function on a built classification model. The output figure reveals that every intermediate node shows the dependent variable name and the p-value. The split condition is displayed on the left and right branches. The terminal nodes show the number of categorized observations, <span class="strong"><em class="calibre8">n</em></span>, and the probability of a class label of either 0 or 1.</p><p class="calibre7">Taking <span class="strong"><em class="calibre8">Figure 7</em></span> as an example, we first build a classification model using <code class="email">total_day_charge</code> as the only feature and <code class="email">churn</code> as the class label. The built classification tree shows that when <code class="email">total_day_charge</code> is above 48.18, the lighter gray area is greater than the darker gray in node 9, which indicates that the customer with a day charge of over 48.18 has a greater likelihood to churn (label = yes).</p></div></div>

<div class="book" title="Visualizing a conditional inference tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec204" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">The visualization of the conditional inference tree comes from the <code class="email">plot.BinaryTree</code> function. If you are interested in adjusting the layout of the classification tree, you may use the <code class="email">help</code> function to read the following document:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ?plot.BinaryTree</strong></span>
</pre></div></li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Measuring the prediction performance of a conditional inference tree"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec62" class="calibre1"/>Measuring the prediction performance of a conditional inference tree</h1></div></div></div><p class="calibre7">After building a conditional inference tree as a classification model, we can use the <code class="email">treeresponse</code> and <code class="email">predict</code> functions to predict categories of the testing dataset, <code class="email">testset</code>, and further validate the prediction power with a classification table and a confusion<a id="id472" class="calibre1"/> matrix.</p></div>

<div class="book" title="Measuring the prediction performance of a conditional inference tree">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec205" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the previous recipe completed by generating the conditional inference tree model, <code class="email">ctree.model</code>. In addition to this, you need to have both <code class="email">trainset</code> and <code class="email">testset</code> loaded in an R session.</p></div></div>

<div class="book" title="Measuring the prediction performance of a conditional inference tree">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec206" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to measure the prediction performance of a conditional inference tree:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">You can use the <code class="email">predict</code> function to predict the category of the testing dataset, <code class="email">testset</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; ctree.predict = predict(ctree.model ,testset)</strong></span>
<span class="strong"><strong class="calibre2">&gt; table(ctree.predict, testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">             </strong></span>
<span class="strong"><strong class="calibre2">ctree.predict yes  no</strong></span>
<span class="strong"><strong class="calibre2">          yes  99  15</strong></span>
<span class="strong"><strong class="calibre2">          no   42 862</strong></span>
</pre></div></li><li class="listitem" value="2">Furthermore, you can use <code class="email">confusionMatrix</code> from the caret package to generate the performance measurements of the prediction result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(table(ctree.predict, testset$churn))</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">             </strong></span>
<span class="strong"><strong class="calibre2">ctree.predict yes  no</strong></span>
<span class="strong"><strong class="calibre2">          yes  99  15</strong></span>
<span class="strong"><strong class="calibre2">          no   42 862</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.944           </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.9281, 0.9573)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.7449          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 0.0005736       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.70213         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.98290         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.86842         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.95354         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.09725         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.11198         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.84251         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes             </strong></span>
</pre></div></li><li class="listitem" value="3">You<a id="id473" class="calibre1"/> can also use the <code class="email">treeresponse</code> function, which will tell you the list of class probabilities:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tr = treeresponse(ctree.model, newdata = testset[1:5,])</strong></span>
<span class="strong"><strong class="calibre2">&gt; tr</strong></span>
<span class="strong"><strong class="calibre2">[[1]]</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.03497409 0.96502591</strong></span>

<span class="strong"><strong class="calibre2">[[2]]</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.02586207 0.97413793</strong></span>

<span class="strong"><strong class="calibre2">[[3]]</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.02586207 0.97413793</strong></span>

<span class="strong"><strong class="calibre2">[[4]]</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.02586207 0.97413793</strong></span>

<span class="strong"><strong class="calibre2">[[5]]</strong></span>
<span class="strong"><strong class="calibre2">[1] 0.03497409 0.96502591</strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Measuring the prediction performance of a conditional inference tree">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec207" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">In this recipe, we first demonstrate that one can use the <code class="email">prediction</code> function to predict the<a id="id474" class="calibre1"/> category (class label) of the testing dataset, <code class="email">testset</code>, and then employ a <code class="email">table</code> function to generate a classification table. Next, you can use the <code class="email">confusionMatrix</code> function built into the caret package to determine the performance measurements.</p><p class="calibre7">In addition to the <code class="email">predict</code> function, <code class="email">treeresponse</code> is also capable of estimating the class probability, which will often classify labels with a higher probability. In this example, we demonstrated how to obtain the estimated class probability using the top five records of the testing dataset, <code class="email">testset</code>. The <code class="email">treeresponse</code> function returns a list of five probabilities. You can use the list to determine the label of instance.</p></div></div>

<div class="book" title="Measuring the prediction performance of a conditional inference tree">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec208" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For the <code class="email">predict</code> function, you can specify the type as <code class="email">response</code>, <code class="email">prob</code>, or <code class="email">node</code>. If you specify the type as <code class="email">prob</code> when using the <code class="email">predict</code> function (for example, <code class="email">predict(… type="prob")</code>), you will get exactly the same result as what <code class="email">treeresponse</code> returns.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with the k-nearest neighbor classifier"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec63" class="calibre1"/>Classifying data with the k-nearest neighbor classifier</h1></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">K-nearest neighbor</strong></span> (<span class="strong"><strong class="calibre2">knn</strong></span>) is a <a id="id475" class="calibre1"/>nonparametric lazy learning method. From a nonparametric view, it does not make any assumptions <a id="id476" class="calibre1"/>about data distribution. In terms of lazy learning, it does not require an explicit learning phase for generalization. The following recipe will introduce how to apply the k-nearest neighbor algorithm<a id="id477" class="calibre1"/> on the churn dataset.</p></div>

<div class="book" title="Classifying data with the k-nearest neighbor classifier">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec209" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the previous recipe completed by generating the training and testing datasets.</p></div></div>

<div class="book" title="Classifying data with the k-nearest neighbor classifier">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec210" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to classify the churn data with the k-nearest neighbor algorithm:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">First, one has to install the <code class="email">class</code> package and have it loaded in an R session:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; install.packages("class")</strong></span>
<span class="strong"><strong class="calibre2">&gt; library(class)</strong></span>
</pre></div></li><li class="listitem" value="2">Replace <code class="email">yes</code> and <code class="email">no</code> of the <code class="email">voice_mail_plan</code> and <code class="email">international_plan</code> attributes<a id="id478" class="calibre1"/> in both the<a id="id479" class="calibre1"/> training dataset and testing dataset to 1 and 0:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; levels(trainset$international_plan) = list("0"="no", "1"="yes")</strong></span>
<span class="strong"><strong class="calibre2">&gt; levels(trainset$voice_mail_plan) = list("0"="no", "1"="yes")</strong></span>
<span class="strong"><strong class="calibre2">&gt; levels(testset$international_plan) = list("0"="no", "1"="yes")</strong></span>
<span class="strong"><strong class="calibre2">&gt; levels(testset$voice_mail_plan) = list("0"="no", "1"="yes")</strong></span>
</pre></div></li><li class="listitem" value="3">Use the <code class="email">knn</code> classification method on the training dataset and the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.knn  = knn(trainset[,! names(trainset) %in% c("churn")], testset[,! names(testset) %in% c("churn")], trainset$churn, k=3)</strong></span>
</pre></div></li><li class="listitem" value="4">Then, you can use the <code class="email">summary</code> function to retrieve the number of predicted labels:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(churn.knn)</strong></span>
<span class="strong"><strong class="calibre2">yes  no</strong></span>
<span class="strong"><strong class="calibre2"> 77 941</strong></span>
</pre></div></li><li class="listitem" value="5">Next, you can generate the classification matrix using the <code class="email">table</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; table(testset$churn, churn.knn)</strong></span>
<span class="strong"><strong class="calibre2">     churn.knn</strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes  44  97</strong></span>
<span class="strong"><strong class="calibre2">  no   33 844</strong></span>
</pre></div></li><li class="listitem" value="6">Lastly, you can generate a confusion matrix by using the <code class="email">confusionMatrix</code> function:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(table(testset$churn, churn.knn))</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">     churn.knn</strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes  44  97</strong></span>
<span class="strong"><strong class="calibre2">  no   33 844</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.8723          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8502, 0.8922)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.9244          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 1               </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.339           </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 3.286e-08       </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.57143         </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.89692         </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.31206         </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.96237         </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.07564         </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.04322         </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.13851         </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.73417         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes   </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with the k-nearest neighbor classifier">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec211" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">
<span class="strong"><strong class="calibre2">knn</strong></span> trains <a id="id480" class="calibre1"/>all samples and classifies <a id="id481" class="calibre1"/>new instances based on a <a id="id482" class="calibre1"/>similarity (distance) measure. For example, the similarity measure can be formulated as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre2">Euclidian Distance</strong></span>: <span class="strong"><img src="../images/00108.jpeg" alt="How it works..." class="calibre24"/></span></li><li class="listitem"><span class="strong"><strong class="calibre2">Manhattan Distance</strong></span>:<span class="strong"><img src="../images/00109.jpeg" alt="How it works..." class="calibre24"/></span></li></ul></div><p class="calibre7">In knn, a new instance is classified to a label (class) that is common among the k-nearest neighbors. If <span class="strong"><em class="calibre8">k = 1</em></span>, then the new instance is assigned to the class where its nearest neighbor belongs. The only required input for the algorithm is k. If we give a small k input, it may lead to over-fitting. On the other hand, if we give a large k input, it may result in under-fitting. To choose a proper k-value, one can count on cross-validation.</p><p class="calibre7">The advantages<a id="id483" class="calibre1"/> of knn are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">The<a id="id484" class="calibre1"/> cost of the <a id="id485" class="calibre1"/>learning process is zero</li><li class="listitem">It is nonparametric, which means that you do not have to make the assumption of data distribution</li><li class="listitem">You can classify any data whenever you can find similarity measures of given instances</li></ul></div><p class="calibre7">The main <a id="id486" class="calibre1"/>disadvantages of knn are:</p><div class="book"><ul class="itemizedlist"><li class="listitem">It is hard to interpret the classified result.</li><li class="listitem">It is an expensive computation for a large dataset.</li><li class="listitem">The performance relies on the number of dimensions. Therefore, for a high dimension problem, you should reduce the dimension first to increase the process performance.</li></ul></div><p class="calibre7">The use of knn does not vary significantly from applying a tree-based algorithm mentioned in the previous recipes. However, while a tree-based algorithm may show you the decision tree model, the output produced by knn only reveals classification category factors. However, before building a classification model, one should replace the attribute with a string type to an integer since the k-nearest neighbor algorithm needs to calculate the distance between observations. Then, we build up a classification model by specifying <span class="strong"><em class="calibre8">k=3</em></span>, which means choosing the three nearest neighbors. After the classification model is built, we can generate a classification table using predicted factors and the testing dataset label as the input. Lastly, we can generate a confusion matrix from the classification table. The confusion matrix output reveals an accuracy result of (0.8723), which suggests that both the tree-based methods mentioned in previous recipes outperform the accuracy of the k-nearest neighbor classification method in this case. Still, we cannot determine which model is better depending merely on accuracy, one should also examine the specificity and sensitivity from the output.</p></div></div>

<div class="book" title="Classifying data with the k-nearest neighbor classifier">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec212" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">There is another package named <code class="email">kknn</code>, which provides a weighted k-nearest neighbor classification, regression, and clustering. You can learn more about the <a id="id487" class="calibre1"/>package by reading this document: <a class="calibre1" href="http://cran.r-project.org/web/packages/kknn/kknn.pdf">http://cran.r-project.org/web/packages/kknn/kknn.pdf</a>.</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with logistic regression"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec64" class="calibre1"/>Classifying data with logistic regression</h1></div></div></div><p class="calibre7">Logistic regression is a form of probabilistic statistical classification model, which can be used to <a id="id488" class="calibre1"/>predict class labels based on one or <a id="id489" class="calibre1"/>more features. The classification is done by using the <code class="email">logit</code> function to estimate the outcome probability. One can use logistic regression by specifying the family as a binomial while using the <code class="email">glm</code> function. In this recipe, we will introduce how to classify data using logistic regression.</p></div>

<div class="book" title="Classifying data with logistic regression">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec213" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have completed the first recipe by generating training and testing datasets.</p></div></div>

<div class="book" title="Classifying data with logistic regression">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec214" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to classify the churn data with logistic regression:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">With the specification of family as a binomial, we apply the <code class="email">glm</code> function on the dataset, <code class="email">trainset</code>, by using churn as a class label and the rest of the variables as input features:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; fit = glm(churn ~ ., data = trainset, family=binomial)</strong></span>
</pre></div></li><li class="listitem" value="2">Use the <code class="email">summary</code> function to obtain summary information of the built logistic regression model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(fit)</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">glm(formula = churn ~ ., family = binomial, data = trainset)</strong></span>

<span class="strong"><strong class="calibre2">Deviance Residuals: </strong></span>
<span class="strong"><strong class="calibre2">    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong class="calibre2">-3.1519   0.1983   0.3460   0.5186   2.1284  </strong></span>

<span class="strong"><strong class="calibre2">Coefficients:</strong></span>
<span class="strong"><strong class="calibre2">                                Estimate Std. Error z value Pr(&gt;|z|)</strong></span>
<span class="strong"><strong class="calibre2">(Intercept)                    8.3462866  0.8364914   9.978  &lt; 2e-16</strong></span>
<span class="strong"><strong class="calibre2">international_planyes         -2.0534243  0.1726694 -11.892  &lt; 2e-16</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_planyes             1.3445887  0.6618905   2.031 0.042211</strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages         -0.0155101  0.0209220  -0.741 0.458496</strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes              0.2398946  3.9168466   0.061 0.951163</strong></span>
<span class="strong"><strong class="calibre2">total_day_calls               -0.0014003  0.0032769  -0.427 0.669141</strong></span>
<span class="strong"><strong class="calibre2">total_day_charge              -1.4855284 23.0402950  -0.064 0.948592</strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes              0.3600678  1.9349825   0.186 0.852379</strong></span>
<span class="strong"><strong class="calibre2">total_eve_calls               -0.0028484  0.0033061  -0.862 0.388928</strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge              -4.3204432 22.7644698  -0.190 0.849475</strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes            0.4431210  1.0478105   0.423 0.672367</strong></span>
<span class="strong"><strong class="calibre2">total_night_calls              0.0003978  0.0033188   0.120 0.904588</strong></span>
<span class="strong"><strong class="calibre2">total_night_charge            -9.9162795 23.2836376  -0.426 0.670188</strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes             0.4587114  6.3524560   0.072 0.942435</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls               0.1065264  0.0304318   3.500 0.000464</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge             -2.0803428 23.5262100  -0.088 0.929538</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls -0.5109077  0.0476289 -10.727  &lt; 2e-16</strong></span>
<span class="strong"><strong class="calibre2">                                 </strong></span>
<span class="strong"><strong class="calibre2">(Intercept)                   ***</strong></span>
<span class="strong"><strong class="calibre2">international_planyes         ***</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_planyes            *  </strong></span>
<span class="strong"><strong class="calibre2">number_vmail_messages            </strong></span>
<span class="strong"><strong class="calibre2">total_day_minutes                </strong></span>
<span class="strong"><strong class="calibre2">total_day_calls                  </strong></span>
<span class="strong"><strong class="calibre2">total_day_charge                 </strong></span>
<span class="strong"><strong class="calibre2">total_eve_minutes                </strong></span>
<span class="strong"><strong class="calibre2">total_eve_calls                  </strong></span>
<span class="strong"><strong class="calibre2">total_eve_charge                 </strong></span>
<span class="strong"><strong class="calibre2">total_night_minutes              </strong></span>
<span class="strong"><strong class="calibre2">total_night_calls                </strong></span>
<span class="strong"><strong class="calibre2">total_night_charge               </strong></span>
<span class="strong"><strong class="calibre2">total_intl_minutes               </strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls              ***</strong></span>
<span class="strong"><strong class="calibre2">total_intl_charge                </strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls ***</strong></span>
<span class="strong"><strong class="calibre2">---</strong></span>
<span class="strong"><strong class="calibre2">Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong class="calibre2">(Dispersion parameter for binomial family taken to be 1)</strong></span>

<span class="strong"><strong class="calibre2">    Null deviance: 1938.8  on 2314  degrees of freedom</strong></span>
<span class="strong"><strong class="calibre2">Residual deviance: 1515.3  on 2298  degrees of freedom</strong></span>
<span class="strong"><strong class="calibre2">AIC: 1549.3</strong></span>

<span class="strong"><strong class="calibre2">Number of Fisher Scoring iterations: 6</strong></span>
</pre></div></li><li class="listitem" value="3">Then, we<a id="id490" class="calibre1"/> find that the built model <a id="id491" class="calibre1"/>contains insignificant variables, which would lead to misclassification. Therefore, we use significant variables only to train the classification model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; fit = glm(churn ~ international_plan + voice_mail_plan+total_intl_calls+number_customer_service_calls, data = trainset, family=binomial)</strong></span>
<span class="strong"><strong class="calibre2">&gt; summary(fit)</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">glm(formula = churn ~ international_plan + voice_mail_plan + </strong></span>
<span class="strong"><strong class="calibre2">    total_intl_calls + number_customer_service_calls, family = binomial, </strong></span>
<span class="strong"><strong class="calibre2">    data = trainset)</strong></span>

<span class="strong"><strong class="calibre2">Deviance Residuals: </strong></span>
<span class="strong"><strong class="calibre2">    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong class="calibre2">-2.7308   0.3103   0.4196   0.5381   1.6716  </strong></span>

<span class="strong"><strong class="calibre2">Coefficients:</strong></span>
<span class="strong"><strong class="calibre2">                              Estimate Std. Error z value</strong></span>
<span class="strong"><strong class="calibre2">(Intercept)                    2.32304    0.16770  13.852</strong></span>
<span class="strong"><strong class="calibre2">international_planyes         -2.00346    0.16096 -12.447</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_planyes             0.79228    0.16380   4.837</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls               0.08414    0.02862   2.939</strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls -0.44227    0.04451  -9.937</strong></span>
<span class="strong"><strong class="calibre2">                              Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong class="calibre2">(Intercept)                    &lt; 2e-16 ***</strong></span>
<span class="strong"><strong class="calibre2">international_planyes          &lt; 2e-16 ***</strong></span>
<span class="strong"><strong class="calibre2">voice_mail_planyes            1.32e-06 ***</strong></span>
<span class="strong"><strong class="calibre2">total_intl_calls               0.00329 ** </strong></span>
<span class="strong"><strong class="calibre2">number_customer_service_calls  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong class="calibre2">---</strong></span>
<span class="strong"><strong class="calibre2">Signif. codes:  </strong></span>
<span class="strong"><strong class="calibre2">0  es:    des:  **rvice_calls  &lt; '.  es:    de</strong></span>

<span class="strong"><strong class="calibre2">(Dispersion parameter for binomial family taken to be 1)</strong></span>

<span class="strong"><strong class="calibre2">    Null deviance: 1938.8  on 2314  degrees of freedom</strong></span>
<span class="strong"><strong class="calibre2">Residual deviance: 1669.4  on 2310  degrees of freedom</strong></span>
<span class="strong"><strong class="calibre2">AIC: 1679.4</strong></span>

<span class="strong"><strong class="calibre2">Number of Fisher Scoring iterations: 5</strong></span>
</pre></div></li><li class="listitem" value="4">Then, you<a id="id492" class="calibre1"/> can then use a fitted <a id="id493" class="calibre1"/>model, <code class="email">fit</code>, to predict the outcome of <code class="email">testset</code>. You can also determine the class by judging whether the probability is above 0.5:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; pred = predict(fit,testset, type="response")</strong></span>
<span class="strong"><strong class="calibre2">&gt; Class = pred &gt;.5</strong></span>
</pre></div></li><li class="listitem" value="5">Next, the use of the <code class="email">summary</code> function will show you the binary outcome count, and reveal whether the probability is above 0.5:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; summary(Class)</strong></span>
<span class="strong"><strong class="calibre2">   Mode   FALSE    TRUE    NA's </strong></span>
<span class="strong"><strong class="calibre2">logical      29     989       0 </strong></span>
</pre></div></li><li class="listitem" value="6">You <a id="id494" class="calibre1"/>can generate the counting<a id="id495" class="calibre1"/> statistics based on the testing dataset label and predicted result:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; tb = table(testset$churn,Class)</strong></span>
<span class="strong"><strong class="calibre2">&gt; tb      Class</strong></span>
<span class="strong"><strong class="calibre2">      FALSE TRUE</strong></span>
<span class="strong"><strong class="calibre2">  yes    18  123</strong></span>
<span class="strong"><strong class="calibre2">  no     11  866</strong></span>
</pre></div></li><li class="listitem" value="7">You can turn the statistics of the previous step into a classification table, and then generate the confusion matrix:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; churn.mod = ifelse(testset$churn == "yes", 1, 0)</strong></span>
<span class="strong"><strong class="calibre2">&gt; pred_class = churn.mod</strong></span>
<span class="strong"><strong class="calibre2">&gt; pred_class[pred&lt;=.5] = 1- pred_class[pred&lt;=.5]</strong></span>
<span class="strong"><strong class="calibre2">&gt; ctb = table(churn.mod, pred_class)</strong></span>
<span class="strong"><strong class="calibre2">&gt; ctb</strong></span>
<span class="strong"><strong class="calibre2">         pred_class</strong></span>
<span class="strong"><strong class="calibre2">churn.mod   0   1</strong></span>
<span class="strong"><strong class="calibre2">        0 866  11</strong></span>
<span class="strong"><strong class="calibre2">        1  18 123</strong></span>
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(ctb)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">         pred_class</strong></span>
<span class="strong"><strong class="calibre2">churn.mod   0   1</strong></span>
<span class="strong"><strong class="calibre2">        0 866  11</strong></span>
<span class="strong"><strong class="calibre2">        1  18 123</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.9715          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.9593, 0.9808)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8684          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : &lt;2e-16          </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.8781          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 0.2652          </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.9796          </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.9179          </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.9875          </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.8723          </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.8684          </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.8507          </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.9488          </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : 0  </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with logistic regression">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec215" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Logistic regression is very similar to linear regression; the main difference is that the dependent <a id="id496" class="calibre1"/>variable in linear regression<a id="id497" class="calibre1"/> is continuous, but the dependent variable in logistic regression is dichotomous (or nominal). The primary goal of logistic regression is to use logit to yield the probability of a nominal variable is related to the measurement variable. We can formulate logit in following equation: ln(P/(1-P)), where P is the probability that certain event occurs.</p><p class="calibre7">The advantage of logistic regression is that it is easy to interpret, it directs model logistic probability, and provides a confidence interval for the result. Unlike the decision tree, which is hard to update the model, you can quickly update the classification model to incorporate new data in logistic regression. The main drawback of the algorithm is that it suffers from multicollinearity and, therefore, the explanatory variables must be linear independent. <code class="email">glm</code> provides a generalized linear regression model, which enables specifying the model in the option family. If the family is specified to a binomial logistic, you can set the family as a binomial to classify the dependent variable of the category.</p><p class="calibre7">The classification process begins by generating a logistic regression model with the use of the training dataset by specifying <code class="email">Churn</code> as the class label, the other variables as training features, and family set as binomial. We then use the <code class="email">summary</code> function to generate the model's summary information. From the summary information, we may find some insignificant variables (p-values &gt; 0.05), which may lead to misclassification. Therefore, we should consider only significant variables for the model.</p><p class="calibre7">Next, we use the <code class="email">fit</code> function to predict the categorical dependent variable of the testing dataset, <code class="email">testset</code>. The <code class="email">fit</code> function outputs the probability of a class label, with a result equal to 0.5 and below, suggesting that the predicted label does not match the label of the testing <a id="id498" class="calibre1"/>dataset, and a probability above 0.5 indicates that the predicted label matches the label of the testing dataset. Further, we<a id="id499" class="calibre1"/> can use the <code class="email">summary</code> function to obtain the statistics of whether the predicted label matches the label of the testing dataset. Lastly, in order to generate a confusion matrix, we first generate a classification table, and then use <code class="email">confusionMatrix</code> to generate the performance measurement.</p></div></div>

<div class="book" title="Classifying data with logistic regression">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec216" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">For more information of how to use the <code class="email">glm</code> function, please refer to <a class="calibre1" title="Chapter 4. Understanding Regression Analysis" href="part0046_split_000.html#page">Chapter 4</a>, <span class="strong"><em class="calibre8">Understanding Regression Analysis</em></span>, which covers how to interpret the output of the <code class="email">glm</code> function</li></ul></div></div></div>

<div id="page" style="height:0pt"/><div class="book" title="Classifying data with the Na&#xEF;ve Bayes classifier"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec65" class="calibre1"/>Classifying data with the Naïve Bayes classifier</h1></div></div></div><p class="calibre7">The Naïve Bayes classifier is also a probability-based classifier, which is based on applying<a id="id500" class="calibre1"/> the Bayes theorem with a strong<a id="id501" class="calibre1"/> independent assumption. In this recipe, we will introduce how to classify data with the Naïve Bayes classifier.</p></div>

<div class="book" title="Classifying data with the Na&#xEF;ve Bayes classifier">
<div class="book" title="Getting ready"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec217" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre7">You need to have the first recipe completed by generating training and testing datasets.</p></div></div>

<div class="book" title="Classifying data with the Na&#xEF;ve Bayes classifier">
<div class="book" title="How to do it..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec218" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre7">Perform the following steps to classify the churn data with the Naïve Bayes classifier:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1">Load the <code class="email">e1071</code> library and employ the <code class="email">naiveBayes</code> function to build the classifier:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; library(e1071) </strong></span>
<span class="strong"><strong class="calibre2">&gt; classifier=naiveBayes(trainset[, !names(trainset) %in% c("churn")], trainset$churn)</strong></span>
</pre></div></li><li class="listitem" value="2">Type <code class="email">classifier</code> to examine the function call, a-priori probability, and conditional probability:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; classifier</strong></span>

<span class="strong"><strong class="calibre2">Naive Bayes Classifier for Discrete Predictors</strong></span>

<span class="strong"><strong class="calibre2">Call:</strong></span>
<span class="strong"><strong class="calibre2">naiveBayes.default(x = trainset[, !names(trainset) %in% c("churn")], </strong></span>
<span class="strong"><strong class="calibre2">    y = trainset$churn)</strong></span>

<span class="strong"><strong class="calibre2">A-priori probabilities:</strong></span>
<span class="strong"><strong class="calibre2">trainset$churn</strong></span>
<span class="strong"><strong class="calibre2">      yes        no </strong></span>
<span class="strong"><strong class="calibre2">0.1477322 0.8522678 </strong></span>

<span class="strong"><strong class="calibre2">Conditional probabilities:</strong></span>
<span class="strong"><strong class="calibre2">              international_plan</strong></span>
<span class="strong"><strong class="calibre2">trainset$churn         no        yes</strong></span>
<span class="strong"><strong class="calibre2">           yes 0.70467836 0.29532164</strong></span>
<span class="strong"><strong class="calibre2">           no  0.93512418 0.06487582</strong></span>
</pre></div></li><li class="listitem" value="3">Next, you<a id="id502" class="calibre1"/> can generate a <a id="id503" class="calibre1"/>classification table for the testing dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; bayes.table = table(predict(classifier, testset[, !names(testset) %in% c("churn")]), testset$churn)</strong></span>
<span class="strong"><strong class="calibre2">&gt; bayes.table</strong></span>
<span class="strong"><strong class="calibre2">     </strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes  68  45</strong></span>
<span class="strong"><strong class="calibre2">  no   73 832</strong></span>
</pre></div></li><li class="listitem" value="4">Lastly, you can generate a confusion matrix from the classification table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre2">&gt; confusionMatrix(bayes.table)</strong></span>
<span class="strong"><strong class="calibre2">Confusion Matrix and Statistics</strong></span>

<span class="strong"><strong class="calibre2">     </strong></span>
<span class="strong"><strong class="calibre2">      yes  no</strong></span>
<span class="strong"><strong class="calibre2">  yes  68  45</strong></span>
<span class="strong"><strong class="calibre2">  no   73 832</strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">               Accuracy : 0.8841          </strong></span>
<span class="strong"><strong class="calibre2">                 95% CI : (0.8628, 0.9031)</strong></span>
<span class="strong"><strong class="calibre2">    No Information Rate : 0.8615          </strong></span>
<span class="strong"><strong class="calibre2">    P-Value [Acc &gt; NIR] : 0.01880         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">                  Kappa : 0.4701          </strong></span>
<span class="strong"><strong class="calibre2"> Mcnemar's Test P-Value : 0.01294         </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">            Sensitivity : 0.4823          </strong></span>
<span class="strong"><strong class="calibre2">            Specificity : 0.9487          </strong></span>
<span class="strong"><strong class="calibre2">         Pos Pred Value : 0.6018          </strong></span>
<span class="strong"><strong class="calibre2">         Neg Pred Value : 0.9193          </strong></span>
<span class="strong"><strong class="calibre2">             Prevalence : 0.1385          </strong></span>
<span class="strong"><strong class="calibre2">         Detection Rate : 0.0668          </strong></span>
<span class="strong"><strong class="calibre2">   Detection Prevalence : 0.1110          </strong></span>
<span class="strong"><strong class="calibre2">      Balanced Accuracy : 0.7155          </strong></span>
<span class="strong"><strong class="calibre2">                                          </strong></span>
<span class="strong"><strong class="calibre2">       'Positive' Class : yes    </strong></span>
</pre></div></li></ol><div class="calibre14"/></div></div></div>

<div class="book" title="Classifying data with the Na&#xEF;ve Bayes classifier">
<div class="book" title="How it works..."><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec219" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre7">Naive Bayes <a id="id504" class="calibre1"/>assumes that features are <a id="id505" class="calibre1"/>conditionally independent, which the effect of a predictor(x) to class (c) is independent of the effect of other predictors to class(c). It computes the posterior probability, <span class="strong"><em class="calibre8">P(c|x)</em></span>, as the following formula:</p><div class="mediaobject"><img src="../images/00110.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">Where <span class="strong"><em class="calibre8">P(x|c)</em></span> is called likelihood, <span class="strong"><em class="calibre8">p(x)</em></span> is called the marginal likelihood, and <span class="strong"><em class="calibre8">p(c)</em></span> is called the prior probability. If there are many predictors, we can formulate the posterior probability as follows:</p><div class="mediaobject"><img src="../images/00111.jpeg" alt="How it works..." class="calibre9"/></div><p class="calibre10"> </p><p class="calibre7">The advantage of Naïve Bayes is that it is relatively simple and straightforward to use. It is suitable when the training set is relative small, and may contain some noisy and missing data. Moreover, you can easily obtain the probability for a prediction. The drawbacks of Naïve Bayes are that it assumes that all features are independent and equally important, which is very unlikely in real-world cases.</p><p class="calibre7">In this<a id="id506" class="calibre1"/> recipe, we use the Naïve Bayes <a id="id507" class="calibre1"/>classifier from the <code class="email">e1071</code> package to build a classification model. First, we specify all the variables (excluding the <code class="email">churn</code> class label) as the first input parameters, and specify the <code class="email">churn</code> class label as the second parameter in the <code class="email">naiveBayes</code> function call. Next, we assign the classification model into the variable classifier. Then, we print the variable classifier to obtain information, such as function call, A-priori probabilities, and conditional probabilities. We can also use the <code class="email">predict</code> function to obtain the predicted outcome and the <code class="email">table</code> function to retrieve the classification table of the testing dataset. Finally, we use a confusion matrix to calculate the performance measurement of the classification model.</p><p class="calibre7">At last, we list a comparison table of all the mentioned algorithms in this chapter:</p><div class="informalexample"><table border="1" class="calibre15"><colgroup class="calibre16"><col class="calibre17"/><col class="calibre17"/><col class="calibre17"/></colgroup><thead class="calibre18"><tr class="calibre19"><th valign="bottom" class="calibre20">
<p class="calibre21">Algorithm</p>
</th><th valign="bottom" class="calibre20">
<p class="calibre21">Advantage</p>
</th><th valign="bottom" class="calibre20">
<p class="calibre21">Disadvantage</p>
</th></tr></thead><tbody class="calibre22"><tr class="calibre19"><td valign="top" class="calibre23">
<p class="calibre21">Recursive partitioning tree</p>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Very<a id="id508" class="indexterm"/> flexible and easy to interpret</li><li class="listitem1">Works on both classification and regression problems </li><li class="listitem1">Nonparametric</li></ul></div>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Prone <a id="id509" class="indexterm"/>to bias and over-fitting</li></ul></div>
</td></tr><tr class="calibre19"><td valign="top" class="calibre23">
<p class="calibre21">Conditional inference tree</p>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Very flexible and easy to interpret</li><li class="listitem1">Works <a id="id510" class="indexterm"/>on both classification and regression problems</li><li class="listitem1">Nonparametric</li><li class="listitem1">Less prone to bias than a recursive partitioning tree</li></ul></div>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Prone <a id="id511" class="indexterm"/>to over-fitting</li></ul></div>
</td></tr><tr class="calibre19"><td valign="top" class="calibre23">
<p class="calibre21">K-nearest neighbor classifier</p>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">The <a id="id512" class="indexterm"/>cost of the learning process is zero </li><li class="listitem1">Nonparametric</li><li class="listitem1">You can classify any data whenever you can find similarity measures of any given instances</li></ul></div>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Hard to<a id="id513" class="indexterm"/> interpret the classified result</li><li class="listitem1">Computation is expensive for a large dataset</li><li class="listitem1">The performance relies on the number of dimensions</li></ul></div>
</td></tr><tr class="calibre19"><td valign="top" class="calibre23">
<p class="calibre21">Logistic regression</p>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Easy to interpret</li><li class="listitem1">Provides <a id="id514" class="indexterm"/>model logistic probability</li><li class="listitem1">Provides confidence interval</li><li class="listitem1">You can quickly update the classification model to incorporate new data</li></ul></div>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Suffers multicollinearity</li><li class="listitem1">Does not handle the missing value <a id="id515" class="indexterm"/>of continuous variables</li><li class="listitem1">Sensitive to extreme values of continuous variables</li></ul></div>
</td></tr><tr class="calibre19"><td valign="top" class="calibre23">
<p class="calibre21">Naïve Bayes</p>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Relatively simple and straightforward to use</li><li class="listitem1">Suitable<a id="id516" class="indexterm"/> when the training set is relative small</li><li class="listitem1">Can deal with some noisy and missing data</li><li class="listitem1">Can easily obtain the probability for a prediction</li></ul></div>
</td><td valign="top" class="calibre23">
<div class="itemizedlist1"><ul class="itemizedlist2"><li class="listitem1">Assumes <a id="id517" class="indexterm"/>all features are independent and equally important, which is very unlikely in real-world cases</li><li class="listitem1">Prone to bias when the number of training sets increase</li></ul></div>
</td></tr></tbody></table></div></div></div>

<div class="book" title="Classifying data with the Na&#xEF;ve Bayes classifier">
<div class="book" title="See also"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec220" class="calibre1"/>See also</h2></div></div></div><div class="book"><ul class="itemizedlist"><li class="listitem">To <a id="id518" class="calibre1"/>learn more about the Bayes<a id="id519" class="calibre1"/> theorem, you can refer to the<a id="id520" class="calibre1"/> following Wikipedia article: <a class="calibre1" href="http://en.wikipedia.org/wiki/Bayes'_theorem">http://en.wikipedia.org/wiki/Bayes'_theorem</a></li></ul></div></div></div></body></html>