- en: Chapter 7. Tree-Based Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to present one of the most intuitive ways to create
    a predictive model—using the concept of a tree. Tree-based models, often also
    known as decision tree models, are successfully used to handle both regression
    and classification type problems. We'll explore both scenarios in this chapter,
    and we'll be looking at a range of different algorithms that are effective in
    training these models. We will also learn about a number of useful properties
    that these models possess, such as their ability to handle missing data and the
    fact that they are highly interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition for tree models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **decision tree** is a model with a very straightforward structure that allows
    us to make a prediction on an output variable, based on a series of rules arranged
    in a tree-like structure. The output variable that we can model can be categorical,
    allowing us to use a decision tree to handle classification problems. Equally,
    we can use decision trees to predict a numerical output, and in this way we'll
    also be able to tackle problems where the predictive task is a regression task.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees consist of a series of split points, often referred to as **nodes**.
    In order to make a prediction using a decision tree, we start at the top of the
    tree at a single node known as the **root node**. The root node is a decision
    or split point, because it imposes a condition in terms of the value of one of
    the input features, and based on this decision we know whether to continue on
    with the left part of the tree or with the right part of the tree. We repeat this
    process of choosing to go left or right at each **inner node** that we encounter
    until we reach one of the **leaf nodes**. These are the nodes at the base of the
    tree, which give us a specific value of the output to use as our prediction.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, let's look at a very simple decision tree in terms of two
    features, *x1* and *x2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![The intuition for tree models](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the tree is a recursive structure, in that the left and right parts
    of the tree that lie beneath a particular node are trees themselves. They are
    referred to as the **left subtree** and the **right subtree** respectively and
    the nodes that they lead to are the **left child** and **right child**. To understand
    how we go about using a decision tree in practice, we can try a simple example.
    Suppose we want to use our tree to predict the output for an observation where
    the value of *x1* is 96.0 and the value of *x2* is 79.9\. We start at the root
    and make a decision as to which subtree to follow. Our value of *x2* is larger
    than 23, so we follow the right branch and come to a new node with a new condition
    to check. Our value of *x1* is larger than 46, so we once again take the right
    branch and arrive at a leaf node. Thus, we output the value indicated by the leaf
    node, which is -3.7\. This is the value that our model predicts given the pair
    of inputs that we specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way of thinking about decision trees is that they are in fact encoding
    a series of if-then rules leading to distinct outputs. For every leaf node, we
    can write a single rule (using the boolean `AND` operator if necessary to join
    together multiple conditions) that must hold true for the tree to output that
    node''s value. We can extract all of these if-then rules by starting at the root
    node and following every path down the tree that leads to a leaf node. For example,
    our small regression tree leads to the following three rules, one for each of
    its leaf nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`If (x2 < 23) Then Output 2.1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`If (x2 > 23) AND (x1 < 46) Then Output 1.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`If (x2 > 23) AND (x1 > 46) Then Output -3.7`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we had to join together two conditions for each one of the last two
    rules using the `AND` operator, as the corresponding paths leading down to a leaf
    node included more than one decision node (counting the root node).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to think about decision trees is that they partition the feature
    space into a series of rectangular regions in two dimensions, cubes in three dimensions,
    and hypercubes in higher dimensions. Remember that the number of dimensions in
    the feature space is just the number of features. The feature space for our example
    regression tree has two dimensions and we can visualize how this space is split
    up into rectangular regions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The intuition for tree models](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The rule-based interpretation and the space partitioning interpretation are
    equivalent views of the same model. The space partitioning interpretation in particular
    is very useful in helping us appreciate one particular characteristic of decision
    trees: they must completely cover all possible combinations of input features.
    Put differently, there should be no particular input for which there is no path
    to a leaf node in the decision tree. Every time we are given a value for our input
    features, we should always be able to return an answer. Our feature space partitioning
    interpretation of a decision tree essentially tells us that there is no point
    or space of points that doesn''t belong to a particular partition with an assigned
    value. Similarly, with our if-then ruleset view of a decision tree, we are saying
    that there is always one rule that can be used for any input feature combination,
    and therefore we can reorganize our rules into an equivalent `if-then-else` structure
    where the last rule is an `else` statement.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms for training decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have understood how a decision tree works, we''ll want to address
    the issue of how we can train one using some data. There are several algorithms
    that have been proposed to build decision trees, and in this section we will present
    a few of the most well-known. One thing we should bear in mind is that, whatever
    tree-building algorithm we choose, we will have to answer four fundamental questions:'
  prefs: []
  type: TYPE_NORMAL
- en: For every node (including the root node), how should we choose the input feature
    to split on and, given this feature, what is the value of the split point?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we decide whether a node should become a leaf node or if we should make
    another split point?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How deep should our tree be allowed to become?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we arrive at a leaf node, what value should we predict?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A great introduction to decision trees is *Chapter 3* of *Machine Learning*,
    *Tom Mitchell*. This book was probably the first comprehensive introduction to
    machine learning and is well worth reading. Although published in 1997, much of
    the material in the book remains relevant today. Furthermore, according to the
    book's website at [http://www.cs.cmu.edu/~tom/mlbook.html](http://www.cs.cmu.edu/~tom/mlbook.html),
    there is a planned second edition in the works.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Classification and regression tree** (**CART**) methodology, which we
    will henceforth refer to simply as CART, is one of the earliest proposed approaches
    to building tree-based models. As the name implies, the methodology encompasses
    both an approach to building regression trees and an approach to building classification
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: CART regression trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For regression trees, the key intuition with the CART approach is that, at any
    given point in the tree, we choose both the input feature to split on and the
    value of the split point within that feature, by finding which combination of
    these maximizes the reduction in the **sum of squared error** (**SSE**). For every
    leaf node in a regression tree built using CART, the predicted value is simply
    the average value of the output predicted by all the data points that are assigned
    to that particular leaf node. To determine whether a new split point should be
    made or whether the tree should grow a leaf node, we simply count the number of
    data points that are currently assigned to a node; if this value is less than
    a predetermined threshold, we create a new leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: 'For any given node in the tree, including the root node, we begin by having
    some data points assigned to that node. At the root node, all the data points
    are assigned, but once we make a split, some of the data points are assigned to
    the left child and the remaining points are assigned to the right child. The starting
    value of the SSE is just the sum of squared error computed using the average value
    ![CART regression trees](img/00127.jpeg) of the output variable ![CART regression
    trees](img/00128.jpeg) for the *n* data points assigned to the current node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART regression trees](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we split these data points into two groups of size *n[1]* and *n[2]* so
    that *n[1] + n[2] = n*, and we compute the new SSE for all the data points as
    the sum of the SSE values for each of the two new groups, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART regression trees](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the first sum iterates over *j*, which is the new indices of the data
    points in the first group corresponding to the left child, and the second sum
    iterates over *k*, which is the new indices of the data points inside the second
    group belonging to the right child. The idea behind CART is that we find a way
    to form these two groups of data points by considering every possible feature
    and every possible split point within that feature so that this new quantity is
    minimized. Thus, we can think of our error function in CART as the SSE.
  prefs: []
  type: TYPE_NORMAL
- en: One of the natural advantages of CART, and tree-based models in general, is
    that they are capable of handling various input types, from numerical inputs (both
    discrete and continuous) to binary inputs as well as categorical inputs. Numerical
    inputs can be ordered in a natural way by sorting them in ascending order, for
    example. When we do this, we can see that, if we have *k* distinct numbers, there
    are *k-1* distinct ways to split these into two groups so that all the numbers
    in one group are smaller than all the numbers in the second group, and both groups
    have at least one element. This is simply done by picking the numbers themselves
    as split points and not counting the smallest number as a split point (which would
    produce an empty group). So if we have a feature vector *x* with the numbers `{5.6,
    2.8, 9.0}`, we first sort these into `{2.8, 5.6, 9.0}`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we take each number except the smallest `{2.8}`to form a split point
    and a corresponding rule that checks whether the input value is smaller than the
    split point. In this way, we produce the only two possible groupings for our feature
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Group1 = {2.8}, Group2 = {5.6, 9.0} IF x < 5.6 THEN Group1 ELSE Group2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Group1 = {2.8, 5.6}, Group2 = {9.0} IF x < 9.0 THEN Group1 ELSE Group2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that it is important to have at least one element in each group, otherwise
    we haven't actually split our data. Binary input features can also be handled
    by simply using the split point that corresponds to putting all data points that
    have this feature take the first value in the first group, and the remaining data
    points that have the second value of this feature in the second group.
  prefs: []
  type: TYPE_NORMAL
- en: Handling unordered categorical input features (factors) is substantially harder
    because there is no natural order. As a result, any combination of levels can
    be assigned to the first group and the remainder to the second group. If we are
    dealing with a factor that has *k* distinct levels, then there are *2^(k-1)-1*
    possible ways to form two groups with at least one level assigned to each group.
  prefs: []
  type: TYPE_NORMAL
- en: So, a binary-valued feature has one possible split, as we know, and a three-valued
    feature has three possible splits. With the numerical feature vector containing
    the numbers `{5.6, 2.8, 9.0}`, we've already seen two possible splits. The third
    possible split that could arise if these numbers were labels is the one in which
    one group has data points with this feature taking the value `5.6`, and another
    group with the two values `2.8` and `9.0`. Clearly, this is not a valid split
    when we treat the feature as numerical.
  prefs: []
  type: TYPE_NORMAL
- en: As a final note, we always have the option of a one-versus-all approach for
    categorical input features, which is essentially the same as considering splits
    in which one group always consists of a single element. This is not always a good
    idea as it may turn out that a particular subset of the levels when taken together
    is more predictive of an output compared to a single level. If this is the case,
    the resulting tree will probably be more complex, having a greater number of node
    splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways to deal with the large increase in complexity associated
    with finding and evaluating all the different split points for categorical input
    features, but we won''t go into further detail right now. Instead, let''s write
    some R code to see how we might find the split point of a numerical input feature
    using the SSE criterion that CART uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In practice, 20 data points might be a suitable number to use as a threshold
    for building a leaf node, but for this example we will simply suppose that we
    wanted to make a new split using this data. We have two input features, `x1` and
    `x2`. The former is a binary input feature that we have coded using the numerical
    labels 0 and 1\. This allows us to reuse the functions we just wrote to compute
    the possible splits. The latter is a numerical input feature. By applying our
    `compute_all_SSE_splits()` function on each feature separately, we can compute
    all the possible split points for each feature and their SSE. The following two
    plots show these SSE values for each feature in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART regression trees](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at both plots, we can see that the best possible split produces an
    SSE value of `124.05` and this can be achieved by splitting on the feature `x2`
    at the value `18.7`. Consequently, our regression tree would contain a node with
    the following splitting rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The CART methodology always applies the same logic to determine whether to make
    a new split at each node as well as how to pick which feature and value to split
    on. This recursive approach of splitting up the data points at each node to build
    the regression tree is why this process is also known as **recursive partitioning**.
  prefs: []
  type: TYPE_NORMAL
- en: Tree pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we were to allow the recursive partitioning process to repeat indefinitely,
    we would eventually terminate by having leaf nodes with a single data point each,
    because that is when we cannot split the data any further. This model would fit
    the training data perfectly, but it is highly unlikely that its performance would
    generalize on unseen data. Thus, tree-based models are susceptible to overfitting.
    To combat this, we need to control the depth of our final decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: The process of removing nodes from the tree to limit its size and complexity
    is known as **pruning**. One possible pruning method is to impose a threshold
    for the smallest number of data points that can be used in order to create a new
    split in the tree instead of creating a leaf node. This will create leaf nodes
    earlier on in the procedure and the data points that are assigned to them may
    not all have the same output. In this case, we can simply predict the average
    value for regression (and the most popular class for classification). This is
    an example of **pre-pruning**, as we are pruning the tree while building it and
    before it is fully constructed.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we should be able to see that, the larger the depth of the tree
    and the smaller the average number of data points assigned to leaf nodes, the
    greater the degree of overfitting. Of course, if we have fewer nodes in the tree,
    we probably aren't being granular enough in our modeling of the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: The question of how large a tree should be allowed to grow is thus effectively
    a question of how to model our data as closely as possible while controlling the
    degree of overfitting. In practice, using pre-pruning is tricky as it is difficult
    to find an appropriate threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another regularization process that is used by the CART methodology to prune
    trees is known as **cost-complexity tuning**. In effect, trees are often allowed
    to grow fully using the recursive partitioning approach described in the previous
    section. Once this completes, we prune the resulting tree, that is to say, we
    start removing split points and merging leaf nodes to shrink the tree according
    to a certain criterion. This is known as **post-pruning**, as we prune the tree
    after it has been built. When we construct the original tree, the error function
    that we use is the SSE. To prune the tree, we use a penalized version of the SSE
    for minimizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tree pruning](img/00132.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, *α* is a complexity parameter controlling the degree of regularization
    and *Tp* is the number of nodes in the tree, which is a way to model the size
    of the tree. Similar to the way in which lasso limits the size of regression coefficients
    in generalized linear models, this regularization procedure limits the size of
    the resulting tree. A very small value of *α* results in a small degree of pruning,
    which in the limit of *α* taking the value of 0 corresponds to no pruning at all.
    On the other hand, using a high value for this parameter results in trees that
    are much shorter, which, at its limit, can result in a zero-sized tree with no
    splits at all that predicts a single average value of the output for all possible
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that every particular value of *α* corresponds to a unique tree
    structure that minimizes this penalized form of the SSE for that particular value.
    Put differently, given a particular value for *α*, there is a unique and predictable
    way to prune a tree in order to minimize the penalized SSE, but the details of
    this procedure are beyond the scope of this book. For now, we can simply assume
    that every value of *α* is associated with a single tree.
  prefs: []
  type: TYPE_NORMAL
- en: This particular feature is very useful, as we don't have any ambiguity in picking
    a tree once we settle on a value for the complexity parameter *α*. It does not,
    however, give us a way to determine what actual value we should use. Cross-validation,
    which we saw in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Support Vector Machines*, is a commonly used approach
    designed to estimate an appropriate value of this parameter. Cross-validation
    applied to this problem would involve partitioning the data into *k* folds. We
    then train and prune *k* trees by using all the data excluding a single fold and
    repeating this for each of the *k* folds. Finally, we measure the SSE on the folds
    held out for testing and average the results. We can repeat our cross-validation
    procedure for different values of *a*. Another approach when more data is available
    is to use a validation dataset for evaluating models that have been trained on
    the same training dataset, but with different values of *α*.
  prefs: []
  type: TYPE_NORMAL
- en: Missing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One characteristic of decision trees is that they have a natural way of handling
    missing data during training. For example, when we consider which feature to split
    on at a particular node, we can ignore data points that have a missing value for
    a particular feature and compute the potential reduction in our error function
    (deviance, SSE, and so on) using the remaining data points. Note that while this
    approach is handy, it could potentially increase the bias of the model substantially,
    especially if we are ignoring a large portion of our available training data because
    of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: One might wonder whether we are able to handle missing values during prediction
    for unseen data points. If we are at a particular node in the tree that is splitting
    on a feature, and for which our test data point has a missing value, we are seemingly
    stuck. In practice, this situation can be dealt with via the use of **surrogate
    splits**. The key notion behind these is that, for every node in the tree, apart
    from the feature that was optimally chosen to split on, we keep track of a list
    of other features that produce splits in the data similar to the feature we actually
    chose. In this way, when our testing data point has a missing value for a feature
    that we need in order to make a prediction, we can refer to a node's surrogate
    splits instead, and use a different feature for this node.
  prefs: []
  type: TYPE_NORMAL
- en: Regression model trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the potential drawbacks of regression trees built with CART is that,
    even though we limit the number of data points that are assigned to a particular
    leaf node, these may still have significant variations in the output variable
    among themselves. When this happens, taking the average value and using this as
    a single prediction for that leaf node may not be the best idea.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression model trees** attempt to overcome this limitation by using the
    data points at the leaf nodes to construct a linear model to predict the output.
    The original regression model tree algorithm was developed by *J. Ross Quinlan*
    and is known as **M5**. The M5 algorithm computes a linear model at each node
    in the tree. For a test data point, we first compute the decision path traversed
    from the root node to the leaf node. The prediction that is then made is the output
    of the linear model associated with that leaf node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'M5 also differs from the algorithm used in CART in that it employs a different
    criterion to determine which feature to split on. This criterion is the weighted
    reduction in standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Regression model trees](img/00133.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This general equation assumes that we split the data into *p* partitions (as
    we have seen for trees, *p* is typically 2). For each partition *i*, we compute
    the standard deviation *σ[i]*. Then we compute the weighted average of these standard
    deviations using the relative size of each partition (*n[i]/n*) as the weights.
    This is subtracted from the initial standard deviation of the unpartitioned data.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind this criterion is that splitting a node should produce groups
    of data points that within each group display less variability with respect to
    the output variable than all the data points when grouped together. We'll have
    a chance to see *M5 trees* as well as CART trees in action later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: CART classification trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building classification trees using the CART methodology continues the notion
    of recursively splitting up groups of data points in order to minimize some error
    function. Our first guess for an appropriate error function is the classification
    accuracy. It turns out that this is not a particularly good measure to use to
    build a classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: What we would actually like to use is a measure for node purity that would score
    nodes based on whether they contain data points primarily belonging to one of
    the output classes. This is a very intuitive idea because what we are effectively
    aiming for in a classification tree is to eventually be able to group our training
    data points into sets of data points at the leaf nodes, so that each leaf node
    contains data points belonging to only one of the classes. This will mean that
    we can confidently predict this class if we arrive at that leaf node during prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible measure of node purity, frequently used with CART for classification
    trees, is the **Gini index**. For an output variable with *K* different classes,
    the Gini index *G* is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART classification trees](img/00134.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To calculate the Gini index, we compute an estimate of the probability of every
    class and multiply this with the probability of not being that class. We then
    add up all these products. For a binary classification problem, it should be easy
    to see that the Gini index evaluates to *2* ![CART classification trees](img/00135.jpeg)
    (*1- * ![CART classification trees](img/00135.jpeg)), where ![CART classification
    trees](img/00135.jpeg) is the estimated probability of one of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the Gini index at a particular node in a tree, we can simply use
    the ratio of the number of data points labeled as class *k* over the total number
    of data points as an estimate for the probability of a data point belonging to
    class *k* at the node in question. Here is a simple R function to compute the
    Gini index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'To compute the Gini index, our `gini_index()` function first tabulates all
    the entries in a vector. It divides each of these frequency counts with the total
    number of counts to transform them into probability estimates. Finally, it computes
    the product (*1-*) for each of these and sums up all the terms. Let''s try a few
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how the Gini index for a completely pure node (a node with only one class)
    is 0\. For a binary output with equal proportions of the two classes, the Gini
    index is 0.5\. Similar to the standard deviation in regression trees, we use the
    weighted reduction in the Gini index, where we weigh each partition by its relative
    size, to determine appropriate split points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART classification trees](img/00136.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another commonly used criterion is deviance. When we studied logistic regression,
    we saw that this is just the constant *-2* multiplied by the log-likelihood of
    the data. In a classification tree setting, we compute the deviance of a node
    in a classification tree as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![CART classification trees](img/00137.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the Gini index, the total number of observations *n[k]* at a node affects
    the value of deviance. All nodes that have the same proportion of data points
    across different classes will have the same value of the Gini index, but if they
    have different numbers of observations, they will have different values of deviance.
    In both splitting criteria, however, a completely pure node will have a value
    of 0 and a positive value otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from using a different splitting criterion, the logic to build a classification
    tree using the CART methodology is exactly parallel to that of building a regression
    tree. Missing values are handled in the same way and the tree is pre-pruned in
    the same way using a threshold on the number of data points left to build leaf
    nodes. The tree is also post-pruned using the same cost-complexity approach outlined
    for regression trees, but after replacing the SSE as the error function with either
    the Gini index or the deviance.
  prefs: []
  type: TYPE_NORMAL
- en: C5.0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **C5.0** algorithm developed by *Ross Quinlan* is an algorithm to build
    a decision tree for classification. This algorithm is the latest in a chain of
    successively improved versions starting from an algorithm known as **ID3**, which
    developed into **C4.5** (and an open source implementation in the Java programming
    language known as **J48**) before culminating in C5.0\. There are many good acronyms
    used for decision trees, but thankfully many of them are related to each other.
    The C5.0 chain of algorithms has several differences from the CART methodology,
    most notably in the choice of the splitting criterion as well as in the pruning
    procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The splitting criterion used with C5.0 is known as **entropy** or the **information
    statistic**, and has its roots in information theory. Entropy is defined as the
    average number of binary digits (bits) needed to communicate information via a
    message as a function of the probabilities of the different symbols used. Entropy
    also has roots in statistical physics, where it is used to represent the degree
    of chaos and uncertainty in a system. When the symbols or components of a system
    have equal probabilities, there is a high degree of uncertainty, but entropy is
    lower when one symbol is far likelier than the others. This observation renders
    the definition of entropy very useful in measuring node purity. The formal definition
    of entropy in bits for the multiclass scenario with *K* classes is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C5.0](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the binary case, the equation simplifies to (where *p* arbitrarily refers
    to the probability of one of the two classes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![C5.0](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compare entropy to the Gini index for binary classification in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C5.0](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the plot, we can see that both functions have the same general shape for
    the binary class problem. Recall that the lower the entropy, the lower the uncertainty
    we have about the distribution of our classes and hence we have higher node purity.
    Consequently, we want to minimize the entropy as we build our tree. In ID3, the
    splitting criterion that is used is the weighted entropy reduction, which is also
    known as **information gain**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C5.0](img/00141.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that this criterion suffers from **selection bias**, in that it
    tends to favor categorical variables because of the large number of possible groupings
    compared to the linear range of splits we find with continuous features. To combat
    this, from C4.5 onwards the criterion was refined into the **information gain
    ratio**. This is a normalized version of information gain, where we normalize
    with respect to a quantity known as the **split information value**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This in turn represents the potential increase in information that we can get
    just by the size of the partitions themselves. A high split information value
    occurs when we have evenly sized partitions and a low value occurs when most of
    the data points are concentrated in a small number of the partitions. In summary,
    we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C5.0](img/00142.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The C5.0 chain of algorithms also incorporate alternative methods to prune a
    tree that go beyond the simple elimination of nodes and sub-trees. For example,
    inner nodes may be removed before leaf nodes so that the nodes beneath the removed
    node (the sub-tree) are pushed up (raised) to replace the removed node. C5.0,
    in particular, is a very powerful algorithm that also contains improvements to
    speed, memory usage, native boosting (covered in the next chapter) capabilities,
    as well as the ability to specify a cost matrix so that the algorithm can avoid
    making certain types of misclassifications over others, just as we saw with support
    vector machines in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We'll demonstrate how to build trees with C5.0 in R in a subsequent section.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting class membership on synthetic 2D data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our first example showcasing tree-based methods in R will operate on a synthetic
    dataset that we have created. The dataset can be generated using commands in the
    companion R file for this chapter, available from the publisher. The data consists
    of 287 observations of two input features, `x1` and `x2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output variable is a categorical variable with three possible classes:
    `a`, `b`, and `c`. If we follow the commands in the code file, we will end up
    with a data frame in R, `mcdf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This problem is actually very simple because, on the one hand, we have a very
    small dataset with only two features, and on the other the classes happen to be
    quite well separated in the feature space, something that is very rare. Nonetheless,
    our objective in this section is to demonstrate the construction of a classification
    tree on *well-behaved* data before we get our hands (or keyboards) dirty on a
    real-world dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: To build a classification tree for this dataset, we will use the `tree` package,
    which provides us with the `tree()` function that trains a model using the CART
    methodology. As is the norm, the first parameter to be provided is a formula and
    the second parameter is the data frame. The function also has a `split` parameter
    that identifies the criterion to be used for splitting. By default, this is set
    to `deviance` for the deviance criterion, for which we observed better performance
    on this dataset. We encourage readers to repeat these experiments by setting the
    `split` parameter to `gini` for splitting on the Gini index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without further ado, let us train our first decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We invoke the `summary()` function on our trained model to get some useful
    information about the tree we built. Note that for this example, we won''t be
    splitting our data into a training and test set, as our goal is to discuss the
    quality of the model fit first. From the provided summary, we seem to have only
    misclassified a single example in our entire dataset. Ordinarily, this would raise
    suspicion that we are overfitting; however, we already know that our classes are
    well separated in the feature space. We can use the `plot()` function to plot
    the shape of our tree as well as the `text()` function to display all the relevant
    labels so we can fully visualize the classifier we have built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the plot that is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting class membership on synthetic 2D data](img/00143.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that our plot shows a predicted class for every node, including non-leaf
    nodes. This simply allows us to see which class is predominant at every step of
    the tree. For example, at the root node, we see that the predominant class is
    class `b`, simply because this is the most commonly represented class in our dataset.
    It is instructive to be able to see the partitioning of our 2D space that our
    decision tree represents.
  prefs: []
  type: TYPE_NORMAL
- en: 'For one and two features, the `tree` package allows us to use the `partition.tree()`
    function to visualize our decision tree. We have done this and superimposed our
    original data over it in order to see how the classifier has partitioned the space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting class membership on synthetic 2D data](img/00144.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Most of us would probably identify six clusters in our data; however, the clusters
    on the top-right of the plot are both assigned to class `b` and so the tree classifier
    has identified this entire region of space as a single leaf node. Finally, we
    can spot the misclassified point belonging to class `b` that has been assigned
    to class `c` (it is the triangle in the middle of the top part of the graph).
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting observation to make is how efficiently the space has been
    partitioned into rectangles in this particular case (only five rectangles for
    a dataset with six clusters). On the other hand, we can expect this model to have
    some instabilities because several of the boundaries of the rectangles are very
    close to data points in the dataset (and thus close to the edges of a cluster).
    Consequently, we should also expect to obtain a lower accuracy with unseen data
    that is generated from the same process that generated our training data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will build a tree model for a real-world classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the authenticity of banknotes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will study the problem of predicting whether a particular
    banknote is genuine or whether it has been forged. The *banknote authentication
    dataset* is hosted at [https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).
    The creators of the dataset have taken specimens of both genuine and forged banknotes
    and photographed them with an industrial camera. The resulting grayscale image
    was processed using a type of time-frequency transformation known as a **wavelet
    transform**. Three features of this transform are constructed, and, along with
    the image entropy, they make up the four features in total for this binary classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `waveletVar` | Numerical | Variance of the wavelet-transformed image |'
  prefs: []
  type: TYPE_TB
- en: '| `waveletSkew` | Numerical | Skewness of the wavelet-transformed image |'
  prefs: []
  type: TYPE_TB
- en: '| `waveletCurt` | Numerical | Kurtosis of the wavelet-transformed image |'
  prefs: []
  type: TYPE_TB
- en: '| `entropy` | Numerical | Entropy of the image |'
  prefs: []
  type: TYPE_TB
- en: '| `class` | Binary | Authenticity (a 0 output means genuine and a 1 output
    means forged) |'
  prefs: []
  type: TYPE_TB
- en: 'First, we will split our 1,372 observations into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will introduce the `C50` R package that contains an implementation
    of the C5.0 algorithm for classification. The `C5.0()` function that belongs to
    this package also takes in a formula and a data frame as its minimum required
    input. Just as before, we can use the `summary()` function to examine the resulting
    model. Instead of reproducing the entire output of the latter, we''ll focus on
    just the tree that is built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, it is perfectly acceptable to use a feature more than once in
    the tree in order to make a new split. The numbers in brackets to the right of
    the leaf nodes in the tree indicate the number of observations from each class
    that are assigned to that node. As we can see, the vast majority of the leaf nodes
    in the tree are pure nodes, so that only observations from one class are assigned
    to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Only two leaf nodes have a single observation each from the minority class
    for that node, and with this we can infer that we only made two mistakes in our
    training data using this model. To see if our model has overfitted the data or
    whether it really can generalize well, we''ll test it on our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The test accuracy is near perfect, a rare sight and the last time in this chapter
    that we'll be finished so easily! As a final note, `C50()` also has a `costs`
    parameter, which is useful for dealing with asymmetric error costs.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting complex skill learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll have a chance to explore data from an innovative and
    recent project known as *SkillCraft*. The interested reader can find out more
    about this project on the web by going to [http://skillcraft.ca/](http://skillcraft.ca/).
    The key premise behind the project is that, by studying the performance of players
    in a **real-time strategy** (**RTS**) game that involves complex resource management
    and strategic decisions, we can study how humans learn complex skills and develop
    speed and competence in dynamic resource allocation scenarios. To achieve this,
    data has been collected from players playing the popular real-time strategy game,
    *Starcraft 2*, developed by *Blizzard*.
  prefs: []
  type: TYPE_NORMAL
- en: In this game, players compete against each other on one of many fixed maps and
    starting locations. Each player must choose a fictional race from three available
    choices and start with six worker units, which are used to collect one of two
    game resources. These resources are needed in order to build military and production
    buildings, military units unique to each race, research technologies, and to build
    more worker units. The game involves a mix of economic advancement, military growth,
    and military strategy in real-time engagements.
  prefs: []
  type: TYPE_NORMAL
- en: Players are pitted against each other via an online matching algorithm that
    groups players into leagues according to their perceived level of skill. The algorithm's
    perception of a player's skill changes over time on the basis of that player's
    performance across the games in which the player participates. There are eight
    leagues in total, which are uneven in population in that the lower leagues tend
    to have more players and the upper leagues have fewer players.
  prefs: []
  type: TYPE_NORMAL
- en: Having a basic understanding of the game, we can download the SkillCraft1 Master
    Table dataset from the UCI Machine Learning repository by going to [https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset](https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset).
    The rows of this dataset are individual games that are played and the features
    of the games are metrics of a player's playing speed, competence, and decision-making.
    The authors of the dataset have used both standard performance metrics familiar
    to players of the game, as well as other metrics such as **Perception Action Cycles**
    (**PACs**), which attempt to quantify a player's actions at the fixed location
    on the map at which a player is looking during a particular time window.
  prefs: []
  type: TYPE_NORMAL
- en: The task at hand is to predict which of the eight leagues a player is currently
    assigned to on the basis of these performance metrics. Our output variable is
    an ordered categorical variable because we have eight distinct leagues ordered
    from 1 to 8, where the latter corresponds to the league with players of the highest
    skill.
  prefs: []
  type: TYPE_NORMAL
- en: 'One possible way to deal with ordinal outputs is to treat them as a numeric
    variable, modeling this as a regression task, and build a regression tree. The
    following table describes the features and output variables that we have in our
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature name | Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Age` | Numeric | Player''s age |'
  prefs: []
  type: TYPE_TB
- en: '| `HoursPerWeek` | Numeric | Reported hours spent playing per week |'
  prefs: []
  type: TYPE_TB
- en: '| `TotalHours` | Numeric | Reported total hours ever spent playing |'
  prefs: []
  type: TYPE_TB
- en: '| `APM` | Numeric | Game actions per minute |'
  prefs: []
  type: TYPE_TB
- en: '| `SelectByHotkeys` | Numeric | Number of unit or building selections made
    using hotkeys per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `AssignToHotkeys` | Numeric | Number of units or buildings assigned to hotkeys
    per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `UniqueHotkeys` | Numeric | Number of unique hotkeys used per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `MinimapAttacks` | Numeric | Number of attack actions on minimap per timestamp
    |'
  prefs: []
  type: TYPE_TB
- en: '| `MinimapRightClicks` | Numeric | Number of right-clicks on minimap per timestamp
    |'
  prefs: []
  type: TYPE_TB
- en: '| `NumberOfPACs` | Numeric | Number of PACs per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `GapBetweenPACs` | Numeric | Mean duration in milliseconds between PACs |'
  prefs: []
  type: TYPE_TB
- en: '| `ActionLatency` | Numeric | Mean latency from the onset of a PAC to their
    first action in milliseconds |'
  prefs: []
  type: TYPE_TB
- en: '| `ActionsInPAC` | Numeric | Mean number of actions within each PAC |'
  prefs: []
  type: TYPE_TB
- en: '| `TotalMapExplored` | Numeric | The number of 24x24 game coordinate grids
    viewed by the player per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `WorkersMade` | Numeric | Number of worker units trained per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `UniqueUnitsMade` | Numeric | Unique units made per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `ComplexUnitsMade` | Numeric | Number of complex units trained per timestamp
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ComplexAbilitiesUsed` | Numeric | Abilities requiring specific targeting
    instructions used per timestamp |'
  prefs: []
  type: TYPE_TB
- en: '| `LeagueIndex` | Numeric | Bronze, Silver, Gold, Platinum, Diamond, Master,
    GrandMaster, and Professional leagues coded 1-8 (output) |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the reader has never played a real-time strategy game like *Starcraft 2*
    on a computer before, it is likely that many of the features used by the dataset
    will sound arcane. If one simply takes on board that these features represent
    various aspects of a player's level of performance in the game, it will still
    be possible to follow all the discussion surrounding the training and testing
    of our regression tree without any difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: To start with, we load this dataset onto the data frame `skillcraft`. Before
    beginning to work with the data, we will have to do some preprocessing. Firstly,
    we'll drop the first column. This simply has a unique game identifier that we
    don't need and won't use. Secondly, a quick inspection of the imported data frame
    will show that three columns have been interpreted as factors because the input
    dataset contains a question mark to denote a missing value. To deal with this,
    we first need to convert these columns to numeric columns, a process that will
    introduce missing values in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, although we've seen that trees are quite capable of handling these missing
    values, we are going to remove the few rows that have them. We will do this because
    we want to be able to compare the performance of several different models in this
    chapter and in the next, not all of which support missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code for the preprocessing steps just described:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, the next step will be to split our data into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we will use the `rpart` package in order to build our decision tree
    (along with the `tree` package, these two are the most commonly used packages
    for building tree-based models in R). This package provides us with an `rpart()`
    function to build our tree. Just as with the `tree()` function, we can build a
    regression tree using the default behavior by simply providing a formula and our
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot our regression tree to see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the plot that is produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting complex skill learning](img/00145.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'To get a sense of the accuracy of our regression tree, we will compute predictions
    on the test data and then measure the SSE. This can be done with the help of a
    simple function that we will define, `compute_SSE()`, which calculates the sum
    of squared error when given a vector of target values and a vector of predicted
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Tuning model parameters in CART trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, all we have done is use default values for all the parameters of the
    recursive partitioning algorithm for building the tree. The `rpart()` function
    has a special `control` parameter to which we can provide an object containing
    the values of any parameters we wish to override. To build this object, we must
    use the special `rpart.control()` function. There are a number of different parameters
    that we could tweak, and it is worth studying the help file for this function
    to learn more about them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we will focus on three important parameters that affect the size and complexity
    of our tree. The `minsplit` parameter holds the minimum number of data points
    that are needed in order for the algorithm to attempt a split before it is forced
    to create a leaf node. The default value is 30\. The `cp` parameter is the complexity
    parameter we have seen before and the default value of this is 0.01\. Finally,
    the `maxdepth` parameter limits the maximum number of nodes between a leaf node
    and the root node. The default value of 30 is quite liberal here, allowing for
    fairly large trees to be built. We can try out a different regression tree by
    specifying some values for these that are different from their default. We''ll
    do this, and see if this affects the SSE performance on our test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using these values we are trying to limit the tree to a depth of 10, while making
    it easier to force a split by needing 20 or more data points at a node. We are
    also lowering the effect of regularization by setting the complexity parameter
    to 0.001\. This is a completely random choice that happens to give us a worse
    SSE value on our test set. In practice, what is needed is a systematic way to
    find appropriate values of these parameters for our tree by trying out a number
    of different combinations and using cross-validation as a way to estimate their
    performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, we would like to tune our regression tree training and in [Chapter
    5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7 "Chapter 5. Neural
    Networks"), *Support Vector Machines*, we met the `tune()` function inside the
    `e1071` package, which can help us do just that. We will use this function with
    `rpart()` and provide it with ranges for the three parameters we just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding tasks will likely take several minutes to complete, as
    there are many combinations of parameters. Once the procedure completes, we can
    train a tree with the suggested values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, we have a lower SSE value with these settings on our test set. If we
    type in the name of our new regression tree model, `regree.tuned`, we'll see that
    we have many more nodes in our tree, which is now substantially more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Variable importance in tree models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For large trees such as this, plotting is less useful as it is very hard to
    make the plot readable. One interesting plot that we can obtain is a plot of **variable
    importance**. For every input feature, we keep track of the reduction in the optimization
    criterion (for example, deviance or SSE) that occurs every time it is used anywhere
    in the tree. We can then tally up this quantity for all the splits in the tree
    and thus obtain relative amounts of variable importance.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, features that are highly important will tend to have been used
    early to split the data (and hence appear higher up in the tree, closer to the
    root node) as well as more often. If a feature is never used, then it is not important
    and in this way we can see that we have a built-in feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this approach is sensitive to correlation in the features. When trying
    to determine what feature to split on, we may randomly end up picking between
    two highly correlated features resulting in the model using more features than
    necessary and as a result, the importance of these features is lower than if either
    had been chosen on its own. It turns out that variable importance is automatically
    computed by `rpart()` and stored in the `variable.importance` attribute on the
    tree model that is returned. Plotting this using `barplot()` produces the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable importance in tree models](img/00146.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To an experienced player of the RTS genre, this graph looks quite reasonable
    and intuitive. The biggest separator of skill according to this graph is the average
    number of game actions that a player makes in a minute (`APM`). Experienced and
    effective players are capable of making many actions, whereas less experienced
    players will make fewer.
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, this may seem to be simply a matter of acquiring so-called
    muscle memory and developing faster reflexes, but in actuality it is knowing which
    actions to carry out, and playing with strategy and planning during the game (a
    characteristic of better players), which also significantly increases this metric.
  prefs: []
  type: TYPE_NORMAL
- en: Another speed-related attribute is the `ActionLatency` feature, which essentially
    measures the time between choosing to focus the map on a particular location on
    the battlefield and executing the first action at that location. Better players
    will spend less time looking at a map location and will be faster at selecting
    units, giving orders, and deciding what to do given an image of a situation in
    the game.
  prefs: []
  type: TYPE_NORMAL
- en: Regression model trees in action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll wrap up the experiments in this chapter with a very short demonstration
    of how to run a regression model tree in R, followed by some information around
    the notion of improving the M5 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this very easily using the `RWeka` package, which contains the `M5P()`
    function. This follows the typical convention of requiring a formula and a data
    frame with the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that we get almost comparable performance to our tuned CART tree using
    the default settings. We'll leave the readers to explore this function further,
    but we will be revisiting this dataset once again in [Chapter 9](part0071_split_000.html#23MNU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 9. Ensemble Methods"), *Ensemble Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A good reference on regression model trees containing several case studies is
    the original paper by *Quinlan*, entitled *Learning with continuous cases*, from
    the proceedings of the *Australian Joint Conference on Artificial Intelligence*
    (1992).
  prefs: []
  type: TYPE_NORMAL
- en: Improvements to the M5 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The standard M5 algorithm tree currently has been received as the most state-of-the-art
    model among decision trees for completing complex regression tasks. This is mainly
    because of the accurate results it yields as well as its ability to handle tasks
    with a very large number of dimensions with upwards of hundreds of attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In an attempt to improve on or otherwise optimize the standard M5 algorithm,
    *M5Flex* has recently been introduced as perhaps the most viable option. The M5Flex
    algorithm approach will attempt to *augment* a standard M5 tree model with *domain
    knowledge*. In other words, M5Flex empowers someone who has familiarity with the
    data population to review and choose the *split attributes* and *split values*
    for those important nodes (within the model tree) with the assumption that, since
    they may "know best," the resulting model will be even more accurate, consistent,
    and appropriate for practical applications than it would be by relying exclusively
    on the standard M5\. One drawback or criticism to using M5Flex is that, in most
    cases, a domain expert may not always be available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Still another attempt at improving M5 is **M5opt**. M5opt is a semi-non-greedy
    algorithm utilizing the unassuming approach of not trying to solve your globally
    complex optimization problem holistically, or as "one entity", but rather by splitting
    the procedure of generating the tree layers into two distinct steps, each using
    a different type or nature of algorithm, based upon the layer of the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global optimization**: Generate upper layers of the tree (from the first
    layer) by using a global (multi-extremum) optimization algorithm (or a better-than-greedy
    approach algorithm).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Greedy searching**: Generate the rest of the tree (the tree''s lower layers)
    by using a faster "greedy algorithm" like the standard M5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, the layer up to which global optimization is applied could be
    different in different branches. However, it would be reasonable to fix it at
    some value for all branches; this allows for a flexible trade-off between speed
    and optimization. Although using the M5opt algorithm to optimize the process of
    constructing tree models has been shown to be successful in yielding models more
    accurate than those created using standard M5, the computational costs will be
    increased due to the nature of how "non-greedy" algorithms work.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, one can control the cost by reviewing what tree level is "most
    appropriate," or which level would yield the most accuracy with the least possible
    cost required, and then performing the more exhaustive, non-greedy search at that
    level of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Further attempts to optimize standard M5 have been to try to combine both the
    M5opt and the M5flex approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, **artificial neural networks** (**ANN's**) discussed in [Chapter 5](part0045_split_000.html#1AT9A1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 5. Neural Networks"), *Neural Networks* have been offered as an alternative
    to standard M5, but only in such scenarios where the tree model is presumed to
    be less complex. In complex models, M5 almost always outperforms ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to build decision trees for regression and classification
    tasks. We saw that, although the idea is simple, there are several decisions that
    we have to make in order to construct our tree model, such as what splitting criterion
    to use, as well as when and how to prune our final tree.
  prefs: []
  type: TYPE_NORMAL
- en: In each case, we considered a number of viable options and it turns out that
    there are several algorithms that are used to build decision tree models. Some
    of the best qualities of decision trees are the fact that they are typically easy
    to implement and very easy to interpret, while making no assumptions about the
    underlying model of the data. Decision trees have native options for performing
    feature selection and handling missing data, and are very capable of handling
    a wide range of feature types.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, we saw that, from a computational perspective, finding a split
    for categorical variables is quite expensive due to the exponential growth of
    the number of possible splits. In addition, we saw that categorical features can
    often tend to impose selection bias in splitting criteria such as information
    gain, because of this large number of potential splits.
  prefs: []
  type: TYPE_NORMAL
- en: Another drawback to using decision tree models is the fact that they can be
    unstable in the sense that small changes in the data can potentially alter a splitting
    decision high up in the tree, and consequently we can end up with a very different
    tree after that. Additionally, and this is particularly relevant to regression
    problems, because of the finite number of leaf nodes, our model may not be sufficiently
    granular in its output. Finally, although there are several different approaches
    to pruning, we should note that decision trees can be vulnerable to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are not going to focus on a new type of model. Instead,
    we are going to look at different techniques to combine multiple models together,
    such as bagging and boosting. Collectively, these are known as ensemble methods.
    These methods have been demonstrated to be quite effective in improving the performance
    of simpler models, and overcoming some of the limitations just discussed for tree-based
    models, such as model instability and susceptibility to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We'll present a well-known algorithm, AdaBoost, which can be used with a number
    of models that we've seen so far. In addition, we will also introduce random forests,
    as a special type of ensemble model specifically designed for decision trees.
    Ensemble methods in general are typically not easy to interpret, but for random
    forests we can still use the notion of variable importance that we saw in this
    chapter in order to get an overall idea of which features our model relies upon
    the most.
  prefs: []
  type: TYPE_NORMAL
