<html><head></head><body>
<div id="_idContainer126">
<h1 class="chapter-number" id="_idParaDest-133"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-134"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.2.1">Dealing with Edge Cases and Rare Events in Machine Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the field of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">machine learning</span></strong><span class="koboSpan" id="kobo.5.1">, it is crucial to identify and handle edge cases properly. </span><span class="koboSpan" id="kobo.5.2">Edge cases refer to instances in your dataset that are significantly different from the majority of the data, and they can have a substantial impact on the performance and reliability of your machine learning models. </span><span class="koboSpan" id="kobo.5.3">Rare events can be challenging for machine learning models due to class imbalance problems as they might not have enough data to learn patterns effectively. </span><span class="koboSpan" id="kobo.5.4">Class imbalance occurs when one class (the rare event) is significantly underrepresented compared to the other class(es). </span><span class="koboSpan" id="kobo.5.5">Traditional machine learning algorithms tend to perform poorly in such scenarios because they may be biased toward the majority class, leading to lower accuracy in identifying </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">rare events.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">In this chapter, we will explore various techniques and approaches to detect edge cases in machine learning and data, using Python code examples. </span><span class="koboSpan" id="kobo.7.2">We’ll explore statistical techniques, including using visualizations and other measures such as Z-scores, to analyze data distributions and identify potential outliers. </span><span class="koboSpan" id="kobo.7.3">We will also focus on methodologies such as isolation forests and semi-supervised methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">autoencoders</span></strong><span class="koboSpan" id="kobo.9.1"> to </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.10.1">uncover anomalies and irregular patterns in your datasets. </span><span class="koboSpan" id="kobo.10.2">We will learn how to address class imbalance and enhance model performance through techniques such as oversampling, undersampling, and generating </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">synthetic data.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">In the latter half of this chapter, we’ll understand the importance of adjusting the learning process to account for imbalanced classes, especially in scenarios where the rare event carries significant consequences. </span><span class="koboSpan" id="kobo.12.2">We’ll explore the significance of selecting appropriate evaluation metrics, emphasizing those that account for class imbalance, to ensure a fair and accurate assessment of model performance. </span><span class="koboSpan" id="kobo.12.3">Finally, we’ll understand how ensemble methods such as bagging, boosting, and stacking help us enhance the robustness of models, particularly in scenarios where rare events play a </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">crucial role.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">The following key topics will </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">be covered:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.16.1">Importance of detecting rare events and edge cases in </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">machine learning</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.18.1">Statistical methods</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.19.1">Anomaly detection</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Data augmentation and </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">resampling techniques</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.22.1">Cost-sensitive learning</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">Choosing </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">evaluation metrics</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.25.1">Ensemble techniques</span></span></li>
</ul>
<h1 id="_idParaDest-135"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.26.1">Importance of detecting rare events and edge cases in machine learning</span></h1>
<p><span class="koboSpan" id="kobo.27.1">Detecting </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.28.1">rare events and edge cases is crucial in machine learning for </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">several reasons:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.30.1">Decision-making in critical scenarios</span></strong><span class="koboSpan" id="kobo.31.1">: Rare events often represent critical scenarios or anomalies that require immediate attention or special treatment. </span><span class="koboSpan" id="kobo.31.2">For instance, in medical diagnosis, rare diseases or extreme cases might need urgent intervention. </span><span class="koboSpan" id="kobo.31.3">Accurate detection of these events can lead to better decision-making and prevent </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">adverse consequences.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.33.1">Unbalanced datasets</span></strong><span class="koboSpan" id="kobo.34.1">: Many real-world datasets suffer from class imbalance, where one class (often the rare event) is significantly underrepresented compared to the other classes. </span><span class="koboSpan" id="kobo.34.2">This can lead to biased models that perform poorly on the minority class. </span><span class="koboSpan" id="kobo.34.3">Detecting rare events helps identify the need for special handling, such as using resampling techniques or employing appropriate evaluation metrics to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">fair evaluation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.36.1">Fraud detection</span></strong><span class="koboSpan" id="kobo.37.1">: In fraud detection applications, rare events often correspond to fraudulent transactions or activities. </span><span class="koboSpan" id="kobo.37.2">Detecting these rare cases is crucial for preventing financial losses and ensuring the security of </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">financial systems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.39.1">Quality control and anomaly detection</span></strong><span class="koboSpan" id="kobo.40.1">: In manufacturing and industrial processes, detecting rare events can help identify faulty or anomalous products or processes. </span><span class="koboSpan" id="kobo.40.2">This enables timely intervention to improve product quality and maintain </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">operational efficiency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.42.1">Predictive maintenance</span></strong><span class="koboSpan" id="kobo.43.1">: In</span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.44.1"> predictive maintenance, detecting edge cases can indicate potential equipment failures or abnormal behaviors, allowing proactive maintenance to reduce downtime and </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">increase productivity.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.46.1">Model generalization</span></strong><span class="koboSpan" id="kobo.47.1">: By accurately identifying and handling rare events, machine learning models can better generalize to unseen data and handle real-world </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">scenarios effectively.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Customer behavior analysis</span></strong><span class="koboSpan" id="kobo.50.1">: In marketing and customer analysis, detecting rare events can reveal unusual patterns or behaviors of interest, such as identifying high-value customers or detecting </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">potential churners.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.52.1">Security and intrusion detection</span></strong><span class="koboSpan" id="kobo.53.1">: In cybersecurity, rare events may indicate security breaches or cyber-attacks. </span><span class="koboSpan" id="kobo.53.2">Detecting and responding to these events in real time is essential for ensuring the safety and integrity of </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">digital systems.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Environmental monitoring</span></strong><span class="koboSpan" id="kobo.56.1">: In environmental applications, rare events might signify unusual ecological conditions or natural disasters. </span><span class="koboSpan" id="kobo.56.2">Detecting such events aids in disaster preparedness and environmental </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">monitoring efforts.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.58.1">Let’s discuss different statistical methods to analyze data distributions and identify </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">potential outliers.</span></span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.60.1">Statistical methods</span></h1>
<p><span class="koboSpan" id="kobo.61.1">Statistical methods</span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.62.1"> provide valuable tools for identifying outliers and anomalies in our data, aiding in data preprocessing and decision-making. </span><span class="koboSpan" id="kobo.62.2">In this section, we’ll talk about how to use methods such</span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.63.1"> as Z-scores, </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">Interquartile Range</span></strong><span class="koboSpan" id="kobo.65.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.66.1">IQR</span></strong><span class="koboSpan" id="kobo.67.1">), box plots, and scatter plots to uncover anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">our data.</span></span></p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.69.1">Z-scores</span></h2>
<p><span class="koboSpan" id="kobo.70.1">Z-scores, also</span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.71.1"> known as standard scores, are a statistical measure that indicates how many standard deviations a data point is away from the </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.72.1">mean of the data. </span><span class="koboSpan" id="kobo.72.2">Z-scores are used to standardize data and allow for comparisons between different datasets, even if they have different units or scales. </span><span class="koboSpan" id="kobo.72.3">They are particularly useful in detecting outliers and identifying extreme values in a dataset. </span><span class="koboSpan" id="kobo.72.4">The formula to calculate the Z-score for a data point </span><em class="italic"><span class="koboSpan" id="kobo.73.1">x</span></em><span class="koboSpan" id="kobo.74.1"> in a dataset with mean </span><em class="italic"><span class="koboSpan" id="kobo.75.1">μ</span></em><span class="koboSpan" id="kobo.76.1"> and standard deviation </span><em class="italic"><span class="koboSpan" id="kobo.77.1">σ</span></em><span class="koboSpan" id="kobo.78.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">presented here:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.80.1">Z</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.81.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.82.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.83.1">x</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.84.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.85.1">μ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.86.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.87.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.88.1">σ</span></span></p>
<p><span class="koboSpan" id="kobo.89.1">Here, the </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">following applies:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.91.1">Z</span></em><span class="koboSpan" id="kobo.92.1"> is the Z-score of the data </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">point </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.94.1">x</span></em></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.95.1">x</span></em><span class="koboSpan" id="kobo.96.1"> is the value of the </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">data point</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.98.1">μ</span></em><span class="koboSpan" id="kobo.99.1"> is the mean of </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">the dataset</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.101.1">σ</span></em><span class="koboSpan" id="kobo.102.1"> is the standard deviation of </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">the dataset</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.104.1">Z-scores are widely used to detect outliers in a dataset. </span><span class="koboSpan" id="kobo.104.2">Data points with Z-scores that fall outside a certain threshold (e.g., </span><em class="italic"><span class="koboSpan" id="kobo.105.1">Z</span></em><span class="koboSpan" id="kobo.106.1"> &gt; 3 or </span><em class="italic"><span class="koboSpan" id="kobo.107.1">Z</span></em><span class="koboSpan" id="kobo.108.1"> &lt; -3) are considered outliers. </span><span class="koboSpan" id="kobo.108.2">These outliers can represent extreme values or measurement errors in the data. </span><span class="koboSpan" id="kobo.108.3">By transforming the data into Z-scores, the mean becomes 0, and the standard deviation becomes 1, resulting in a </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">standardized distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">Z-scores are employed in normality testing to assess whether a dataset follows a normal (Gaussian) distribution. </span><span class="koboSpan" id="kobo.110.2">If the dataset follows a normal distribution, approximately 68% of the data points should have Z-scores between -1 and 1, about 95% between -2 and 2, and nearly all between -3 and 3. </span><span class="koboSpan" id="kobo.110.3">In hypothesis testing, Z-scores are used to compute </span><em class="italic"><span class="koboSpan" id="kobo.111.1">p</span></em><span class="koboSpan" id="kobo.112.1">-values and make inferences about </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">population parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">For example, Z-tests are commonly used for sample mean comparisons when the population standard deviation is known. </span><span class="koboSpan" id="kobo.114.2">Z-scores can be useful in anomaly detection where we want to identify data points that deviate significantly from the norm. </span><span class="koboSpan" id="kobo.114.3">High Z-scores may indicate anomalous behavior or rare events in </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">Let’s explore this concept in Python using the </span><em class="italic"><span class="koboSpan" id="kobo.117.1">Loan Prediction</span></em><span class="koboSpan" id="kobo.118.1"> dataset. </span><span class="koboSpan" id="kobo.118.2">Let’s start by loading </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.120.1">
import pandas as pd
import numpy as np
df = pd.read_csv('train_loan_prediction.csv')
df.head().T</span></pre> <p><span class="koboSpan" id="kobo.121.1">Here, we see</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.122.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">sample dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<span class="koboSpan" id="kobo.124.1"><img alt="Figure 9.1 – The df DataFrame" src="image/B19297_09_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.125.1">Figure 9.1 – The df DataFrame</span></p>
<p><span class="koboSpan" id="kobo.126.1">Now that we </span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.127.1">have loaded the dataset, we can calculate Z-scores on some of the </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">numerical features:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.129.1">
numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
z_scores = df[numerical_features].apply(lambda x: (x - np.mean(x)) / np.std(x))</span></pre> <p><span class="koboSpan" id="kobo.130.1">We leverage Z-scores to detect outliers in our </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">dataset effectively:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.132.1">
threshold = 3
outliers = (z_scores &gt; threshold) | (z_scores &lt; -threshold)
outliers['is_outlier'] = outliers.any(axis=1)
outlier_rows = df[outliers['is_outlier']]
outlier_rows</span></pre> <p><span class="koboSpan" id="kobo.133.1">By</span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.134.1"> setting a </span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.135.1">Z-score threshold of 3, we can identify outliers by examining if any rows have values that lie beyond the Z-score boundaries of greater than 3 or less than -3. </span><span class="koboSpan" id="kobo.135.2">This approach allows us to pinpoint data points that deviate significantly from the mean and enables us to take appropriate actions to handle these outliers, ensuring the integrity of our data and the accuracy of subsequent analyses </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">and models.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">Here is the output DataFrame of outliers using </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">this approach:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<span class="koboSpan" id="kobo.139.1"><img alt="Figure 9.2 – The resulting outlier_rows DataFrame" src="image/B19297_09_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.140.1">Figure 9.2 – The resulting outlier_rows DataFrame</span></p>
<p><span class="koboSpan" id="kobo.141.1">In conclusion, Z-scores make it easier to spot outliers accurately. </span><span class="koboSpan" id="kobo.141.2">They act as a helpful tool, guiding us to understand data patterns better. </span><span class="koboSpan" id="kobo.141.3">In the next section on IQR, we will further explore alternative methods for detecting and </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">addressing outliers.</span></span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.143.1">Interquartile Range (IQR)</span></h2>
<p><span class="koboSpan" id="kobo.144.1">IQR is a</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.145.1"> statistical measure used to describe the spread or dispersion of a dataset. </span><span class="koboSpan" id="kobo.145.2">It is particularly useful in identifying and handling outliers and understanding the central tendency of the data. </span><span class="koboSpan" id="kobo.145.3">The IQR is defined as the range </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.146.1">between the first quartile (Q1) and the third quartile (Q3) of a dataset. </span><span class="koboSpan" id="kobo.146.2">Quartiles are points that divide a dataset into four equal parts, each containing 25% of </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">The IQR can be calculated using the </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.150.1">I</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.151.1">Q</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.152.1">R</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.153.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.154.1">Q</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.155.1">3</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.156.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.157.1">Q</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.158.1">1</span></span></span></p>
<p><span class="koboSpan" id="kobo.159.1">Here, the </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">following applies:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.161.1">Q1 is the first quartile (25th percentile), representing the value below which 25% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">data lies</span></span></li>
<li><span class="koboSpan" id="kobo.163.1">Q3 is the third quartile (75th percentile), representing the value below which 75% of the </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">data lies</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.165.1">IQR is commonly used to identify outliers in a dataset. </span><span class="koboSpan" id="kobo.165.2">Data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers and may warrant further investigation. </span><span class="koboSpan" id="kobo.165.3">IQR provides valuable information about the distribution of the data. </span><span class="koboSpan" id="kobo.165.4">It helps to understand the spread of the middle 50% of the dataset and can be used to assess the symmetry of the distribution. </span><span class="koboSpan" id="kobo.165.5">When comparing datasets, IQR can be used to assess differences in the spread of data between two or </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">more datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">Here’s a simple example of how to calculate the IQR for a dataset using Python and NumPy on the Loan Prediction dataset. </span><span class="koboSpan" id="kobo.167.2">To begin with outlier detection, we calculate the IQR for numerical features. </span><span class="koboSpan" id="kobo.167.3">By defining a threshold, typically 1.5 times the IQR based on Tukey’s method, which involves calculating the IQR as the difference between the third quartile (Q3) and the first quartile (Q1), we can identify outliers. </span><span class="koboSpan" id="kobo.167.4">Tukey’s method is a robust statistical technique that aids in detecting data points that deviate significantly from the overall distribution, providing a reliable measure for identifying potential anomalies in the dataset. </span><span class="koboSpan" id="kobo.167.5">Once outliers are flagged for all numerical features, we can display the rows with outlier values, which aids in further analysis and potential data treatment. </span><span class="koboSpan" id="kobo.167.6">Using the IQR and Tukey’s method together facilitates effective outlier detection and </span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.168.1">helps ensure the integrity of </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">the dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.170.1">
Q1 = df[numerical_features].quantile(0.25)
Q3 = df[numerical_features].quantile(0.75)
IQR = Q3 - Q1
threshold = 1.5
outliers = (df[numerical_features] &lt; (Q1 - threshold * IQR)) | (df[numerical_features] &gt; (Q3 + threshold * IQR))
outliers['is_outlier'] = outliers.any(axis=1)
outlier_rows = df[outliers['is_outlier']]
outlier_rows</span></pre> <p><span class="koboSpan" id="kobo.171.1">Here is the </span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.172.1">output DataFrame of outliers using </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">this approach:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<span class="koboSpan" id="kobo.174.1"><img alt="Figure 9.3 – The resulting outlier_rows DataFrame" src="image/B19297_09_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.175.1">Figure 9.3 – The resulting outlier_rows DataFrame</span></p>
<p><span class="koboSpan" id="kobo.176.1">In conclusion, the exploration of IQR provides a valuable technique for identifying outliers, particularly effective in capturing the central tendencies of a dataset. </span><span class="koboSpan" id="kobo.176.2">While IQR proves advantageous in scenarios where a focus on the middle range is paramount, it’s essential to recognize its limitations in capturing the entire data distribution. </span><span class="koboSpan" id="kobo.176.3">In the upcoming section on box plots, we will delve into a graphical representation that complements IQR, offering a visual tool to better understand data dispersion and outliers in </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">diverse contexts.</span></span></p>
<h2 id="_idParaDest-139"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.178.1">Box plots</span></h2>
<p><span class="koboSpan" id="kobo.179.1">Box plots, also</span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.180.1"> known as box-and-whisker plots, are a graphical </span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.181.1">representation of the distribution of a dataset. </span><span class="koboSpan" id="kobo.181.2">They provide a quick and informative way to visualize the spread and skewness of the data, identify potential outliers, and compare multiple datasets. </span><span class="koboSpan" id="kobo.181.3">Box plots are particularly useful when dealing with continuous numerical data and can be used to gain insights into the central tendency and variability of </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">Here are the components of a </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">box plot:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.185.1">Box (IQR)</span></strong><span class="koboSpan" id="kobo.186.1">: The</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.187.1"> box represents the IQR, which is the range between the first quartile (Q1) and the third quartile (Q3) of the data. </span><span class="koboSpan" id="kobo.187.2">It spans the middle 50% of the dataset and provides a visual representation of the </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">data’s spread.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.189.1">Median (Q2)</span></strong><span class="koboSpan" id="kobo.190.1">: The </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.191.1">median, represented by a horizontal line inside the box, indicates the central value of the dataset. </span><span class="koboSpan" id="kobo.191.2">It divides the data into two equal halves, with 50% of the data points below and 50% above </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">the median.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.193.1">Whiskers</span></strong><span class="koboSpan" id="kobo.194.1">: Whiskers </span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.195.1">extend from the edges of the box to the furthest data points that lie within the “whisker length.” </span><span class="koboSpan" id="kobo.195.2">The length of the whiskers is typically determined by a factor (for example, 1.5 times the IQR) and is used to identify </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">potential outliers.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.197.1">Outliers</span></strong><span class="koboSpan" id="kobo.198.1">: Data</span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.199.1"> points lying beyond whiskers are considered outliers and are usually plotted individually as individual points or circles. </span><span class="koboSpan" id="kobo.199.2">They are data points that deviate significantly from the central distribution and may warrant </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">further investigation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">The following are the</span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.202.1"> benefits of </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">box plots:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.204.1">Visualizing data distribution</span></strong><span class="koboSpan" id="kobo.205.1">: Box plots offer an intuitive way to see the spread and skewness of the data, as well as identify any potential data clusters </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">or gaps</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.207.1">Comparing datasets</span></strong><span class="koboSpan" id="kobo.208.1">: Box plots </span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.209.1">are useful for comparing multiple datasets side by side, allowing for easy comparisons of central tendencies </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">and variabilities</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.211.1">Outlier detection</span></strong><span class="koboSpan" id="kobo.212.1">: Box plots facilitate outlier detection by highlighting data points that lie beyond whiskers, helping identify unusual or </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">extreme values</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.214.1">Handling skewed data</span></strong><span class="koboSpan" id="kobo.215.1">: Box plots are robust to the influence of extreme values and can handle skewed data distributions more effectively than traditional mean and </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">standard deviation</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.217.1">Let’s implement </span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.218.1">this approach using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.219.1">matplotlib</span></strong><span class="koboSpan" id="kobo.220.1"> library in </span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.221.1">Python. </span><span class="koboSpan" id="kobo.221.2">Here, we will produce two box plots for </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">ApplicantIncome</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.223.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">LoanAmount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.226.1">
import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=(10, 6))
plt.subplot(2, 1, 1)
df.boxplot(column='ApplicantIncome')
plt.title('Box Plot - ApplicantIncome')
plt.subplot(2, 1, 2)
df.boxplot(column='LoanAmount')
plt.title('Box Plot - LoanAmount')
plt.tight_layout()
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.227.1">Here are the plot outputs. </span><span class="koboSpan" id="kobo.227.2">We see that there are some possible outliers in </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">these columns:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.229.1"><img alt="Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)" src="image/B19297_09_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.230.1">Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)</span></p>
<p><span class="koboSpan" id="kobo.231.1">In summarizing</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.232.1"> the use of box plots, we find them to be a powerful </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.233.1">visual aid, complementing IQR in portraying both central tendencies and data dispersion. </span><span class="koboSpan" id="kobo.233.2">While box plots excel in providing a holistic view, it’s crucial to acknowledge that they may not capture all nuances of complex datasets. </span><span class="koboSpan" id="kobo.233.3">In the next section on scatter plots, we will explore a versatile graphical tool that offers a broader perspective, facilitating the identification of relationships and patterns between variables in </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">our data.</span></span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.235.1">Scatter plots</span></h2>
<p><span class="koboSpan" id="kobo.236.1">Scatter plots</span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.237.1"> are a popular and versatile data visualization</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.238.1"> technique used to explore the relationship between two continuous numerical variables. </span><span class="koboSpan" id="kobo.238.2">They provide a clear visual representation of how one variable (the independent variable) affects or influences another (the dependent variable). </span><span class="koboSpan" id="kobo.238.3">Scatter plots are especially useful in identifying patterns, correlations, clusters, and outliers in data, making them</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.239.1"> an essential tool in data analysis and </span><strong class="bold"><span class="koboSpan" id="kobo.240.1">exploratory data </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.241.1">analysis</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.242.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.243.1">EDA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">Let’s now look at how scatter plots are constructed and their </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">key characteristics.</span></span></p>
<h3><span class="koboSpan" id="kobo.247.1">Scatter plot construction</span></h3>
<p><span class="koboSpan" id="kobo.248.1">To create a</span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.249.1"> scatter plot, the values of two numerical variables are plotted as points on a Cartesian coordinate system. </span><span class="koboSpan" id="kobo.249.2">Each point represents a data observation, where the </span><em class="italic"><span class="koboSpan" id="kobo.250.1">x</span></em><span class="koboSpan" id="kobo.251.1"> coordinate corresponds to the value of the independent variable and the </span><em class="italic"><span class="koboSpan" id="kobo.252.1">y</span></em><span class="koboSpan" id="kobo.253.1"> coordinate corresponds to the value of the dependent variable. </span><span class="koboSpan" id="kobo.253.2">Multiple data points collectively form a scatter plot that provides insights into the relationship between the </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">two variables.</span></span></p>
<h3><span class="koboSpan" id="kobo.255.1">Key characteristics of scatter plots</span></h3>
<p><span class="koboSpan" id="kobo.256.1">The</span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.257.1"> following are some key characteristics of </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">scatter plots:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.259.1">Correlation</span></strong><span class="koboSpan" id="kobo.260.1">: Scatter plots help us assess the correlation or relationship between two variables. </span><span class="koboSpan" id="kobo.260.2">If points on the plot appear to form a clear trend or pattern (e.g., a linear or non-linear trend), it suggests a significant correlation between the variables. </span><span class="koboSpan" id="kobo.260.3">If points are scattered randomly, there might be no or </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">weak correlation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.262.1">Cluster analysis</span></strong><span class="koboSpan" id="kobo.263.1">: Scatter plots can reveal clusters of data points, indicating potential subgroups or patterns within </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.265.1">Outlier detection</span></strong><span class="koboSpan" id="kobo.266.1">: Scatter plots facilitate outlier detection by identifying data points that lie far away from the main cluster </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">of points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.268.1">Data spread</span></strong><span class="koboSpan" id="kobo.269.1">: The spread or distribution of data points along the </span><em class="italic"><span class="koboSpan" id="kobo.270.1">x</span></em><span class="koboSpan" id="kobo.271.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.272.1">y</span></em><span class="koboSpan" id="kobo.273.1"> axes provides insights into the variability of </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">the variables.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.275.1">Visualizing regression lines</span></strong><span class="koboSpan" id="kobo.276.1">: In some cases, a regression line can be fitted to the scatter plot to model the relationship between the variables and </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">make predictions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.278.1">Let’s implement</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.279.1"> this in Python on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">ApplicantIncome</span></strong><span class="koboSpan" id="kobo.281.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.282.1">LoanAmount</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.283.1"> columns:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.284.1">
plt.figure(figsize=(8, 6))
plt.scatter(df['ApplicantIncome'], df['LoanAmount'])
plt.xlabel('ApplicantIncome')
plt.ylabel('LoanAmount')
plt.title('Scatter Plot - ApplicantIncome vs. </span><span class="koboSpan" id="kobo.284.2">LoanAmount')
plt.show()</span></pre> <p><span class="koboSpan" id="kobo.285.1">Here is the output showcasing the scatter </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">plot results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.287.1"><img alt="Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount" src="image/B19297_09_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.288.1">Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount</span></p>
<p><span class="koboSpan" id="kobo.289.1">As you can see, some points are notably distant from the majority of the population, suggesting the presence of potential outliers. </span><span class="koboSpan" id="kobo.289.2">These outlying data points stand apart from the overall pattern, warranting further investigation to understand their significance and</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.290.1"> impact on the relationship between the two variables. </span><span class="koboSpan" id="kobo.290.2">Identifying and handling outliers is crucial for ensuring accurate data analysis and model performance. </span><span class="koboSpan" id="kobo.290.3">By visualizing the scatter plot, we can gain valuable insights into data distribution and correlations, paving the way for effective decision-making and </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">data exploration.</span></span></p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.292.1">Anomaly detection</span></h1>
<p><span class="koboSpan" id="kobo.293.1">Anomaly detection</span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.294.1"> is a specific approach to detecting rare events, where the focus is on identifying instances that significantly deviate from the norm or normal behavior. </span><span class="koboSpan" id="kobo.294.2">Anomalies can be caused by rare events, errors, or unusual patterns that are not typical in the dataset. </span><span class="koboSpan" id="kobo.294.3">This technique is particularly useful when there is limited or no labeled data for rare events. </span><span class="koboSpan" id="kobo.294.4">Common anomaly detection algorithms include </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.296.1">Unsupervised methods</span></strong><span class="koboSpan" id="kobo.297.1">: Techniques </span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.298.1">such as Isolation Forest and </span><strong class="bold"><span class="koboSpan" id="kobo.299.1">One-Class SVM</span></strong><span class="koboSpan" id="kobo.300.1">) can be used to identify anomalies in data without requiring labeled examples of the </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">rare event.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.302.1">Semi-supervised methods</span></strong><span class="koboSpan" id="kobo.303.1">: These approaches combine normal and abnormal </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.304.1">data during </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.305.1">training but have only a limited number of labeled anomalies. </span><span class="koboSpan" id="kobo.305.2">Autoencoders and variational autoencoders are examples of semi-supervised anomaly </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">detection algorithms.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.307.1">Supervised methods</span></strong><span class="koboSpan" id="kobo.308.1">: If a</span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.309.1"> small </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.310.1">number of labeled anomalies are available, </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">supervised learning</span></strong><span class="koboSpan" id="kobo.312.1"> algorithms</span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.313.1"> such as </span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.314.1">Random Forest, </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.316.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.317.1">SVM</span></strong><span class="koboSpan" id="kobo.318.1">), and neural networks can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">anomaly detection.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.320.1">Let’s understand these methods in detail with Python </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">code examples.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.322.1">Unsupervised method using Isolation Forest</span></h2>
<p><span class="koboSpan" id="kobo.323.1">Isolation Forest </span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.324.1">is an efficient and effective algorithm used for anomaly detection in </span><strong class="bold"><span class="koboSpan" id="kobo.325.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.326.1"> scenarios. </span><span class="koboSpan" id="kobo.326.2">It works by isolating anomalies or rare events in the data by constructing</span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.327.1"> isolation trees (random decision trees) that separate the anomalies from the majority of the normal data points. </span><span class="koboSpan" id="kobo.327.2">It was introduced by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in their 2008 paper titled </span><em class="italic"><span class="koboSpan" id="kobo.328.1">Isolation Forest</span></em><span class="koboSpan" id="kobo.329.1">. </span><span class="koboSpan" id="kobo.329.2">Here are some key concepts and features </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.330.1">of the Isolation </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">Forest algorithm:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.332.1">Random partitioning</span></strong><span class="koboSpan" id="kobo.333.1">: Isolation Forest uses a random partitioning strategy to create isolation trees. </span><span class="koboSpan" id="kobo.333.2">At each step of constructing a tree, a random feature is selected, and a random split value within the range of the selected feature’s values is chosen to create a node. </span><span class="koboSpan" id="kobo.333.3">This random partitioning leads to shorter paths for anomalies, making them easier to isolate from normal </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">data points.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.335.1">Path length</span></strong><span class="koboSpan" id="kobo.336.1">: The key idea behind Isolation Forest is that anomalies are isolated into smaller partitions with fewer data points, while normal data points are distributed more uniformly across larger partitions. </span><span class="koboSpan" id="kobo.336.2">The average path length of a data point to reach an anomaly in a tree is used as a measure of </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">its “isolation.”</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.338.1">Anomaly score</span></strong><span class="koboSpan" id="kobo.339.1">: Based on the average path length, each data point is assigned an anomaly score. </span><span class="koboSpan" id="kobo.339.2">The anomaly score represents how easily the data point can be isolated or separated from the rest of the data. </span><span class="koboSpan" id="kobo.339.3">Shorter average path lengths correspond to higher anomaly scores, indicating that the data point is more likely to be </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">an anomaly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.341.1">Contamination parameter</span></strong><span class="koboSpan" id="kobo.342.1">: The Isolation Forest algorithm has a hyperparameter called “contamination” that represents the expected proportion of anomalies in the dataset. </span><span class="koboSpan" id="kobo.342.2">This parameter helps in setting a threshold for identifying anomalies. </span><span class="koboSpan" id="kobo.342.3">The contamination parameter can be set explicitly or as “</span><strong class="source-inline"><span class="koboSpan" id="kobo.343.1">auto</span></strong><span class="koboSpan" id="kobo.344.1">,” which estimates the contamination based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">dataset’s size.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.346.1">A few advantages </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.347.1">of Isolation Forest are </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">listed here:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.349.1">Isolation Forest is computationally efficient and scalable, making it suitable for large datasets. </span><span class="koboSpan" id="kobo.349.2">It does not require a large number of trees to achieve good performance, reducing the </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">computational overhead.</span></span></li>
<li><span class="koboSpan" id="kobo.351.1">The </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.352.1">algorithm is relatively insensitive to the number of dimensions/features, which is particularly advantageous when dealing with high-dimensional datasets. </span><span class="koboSpan" id="kobo.352.2">Isolation Forest is an unsupervised learning algorithm, making it suitable for scenarios where labeled anomaly data is scarce </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">or unavailable.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.354.1">There are, however, some</span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.355.1"> limitations of </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">Isolation Forest:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.357.1">Isolation Forest may not perform well on datasets with multiple clusters of anomalies or when anomalies are close to the majority of normal </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">data points.</span></span></li>
<li><span class="koboSpan" id="kobo.359.1">As with most unsupervised algorithms, Isolation Forest may produce false positives (normal data points misclassified as anomalies) and false negatives (anomalies misclassified as normal </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">data points).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.361.1">Let’s implement this </span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.362.1">approach in Python with the </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">following steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.364.1">Import libraries</span></strong><span class="koboSpan" id="kobo.365.1">: The code begins by importing the necessary libraries. </span><span class="koboSpan" id="kobo.365.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">pandas</span></strong><span class="koboSpan" id="kobo.367.1"> library is imported as </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">pd</span></strong><span class="koboSpan" id="kobo.369.1"> to handle data in tabular format, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">IsolationForest</span></strong><span class="koboSpan" id="kobo.371.1"> is imported from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">sklearn.ensemble</span></strong><span class="koboSpan" id="kobo.373.1"> module for performing anomaly detection using the Isolation </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">Forest algorithm:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.375.1">
Import pandas as pd
from sklearn.ensemble import IsolationForest</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.376.1">Extract numerical features</span></strong><span class="koboSpan" id="kobo.377.1">: A </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">numerical_features</span></strong><span class="koboSpan" id="kobo.379.1"> list is defined, containing the names of the numerical columns to be used for anomaly detection. </span><span class="koboSpan" id="kobo.379.2">These columns are </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">'ApplicantIncome'</span></strong><span class="koboSpan" id="kobo.381.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.382.1">'CoapplicantIncome'</span></strong><span class="koboSpan" id="kobo.383.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">'LoanAmount'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.387.1">
numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.388.1">Create a DataFrame for anomaly detection</span></strong><span class="koboSpan" id="kobo.389.1">: A new </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.391.1"> DataFrame is created by extracting the columns specified in </span><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">numerical_features</span></strong><span class="koboSpan" id="kobo.393.1"> from the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.394.1">df</span></strong><span class="koboSpan" id="kobo.395.1"> DataFrame. </span><span class="koboSpan" id="kobo.395.2">This new DataFrame will be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">anomaly detection:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.397.1">
X_anomaly = df[numerical_features]</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.398.1">Handle missing values</span></strong><span class="koboSpan" id="kobo.399.1">: To handle any missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.400.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.401.1"> DataFrame, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.402.1">fillna()</span></strong><span class="koboSpan" id="kobo.403.1"> method is used with the mean of each column. </span><span class="koboSpan" id="kobo.403.2">This ensures that any missing values are replaced with the mean value of their </span><span class="No-Break"><span class="koboSpan" id="kobo.404.1">respective columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.405.1">
X_anomaly.fillna(X_anomaly.mean(), inplace=True)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.406.1">Initialize the Isolation Forest model</span></strong><span class="koboSpan" id="kobo.407.1">: The Isolation Forest model is initialized with </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">IsolationForest(contamination='auto', random_state=42)</span></strong><span class="koboSpan" id="kobo.409.1">. </span><span class="koboSpan" id="kobo.409.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.410.1">contamination</span></strong><span class="koboSpan" id="kobo.411.1"> parameter is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.412.1">'auto'</span></strong><span class="koboSpan" id="kobo.413.1">, which means</span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.414.1"> it will automatically detect the percentage of outliers in the dataset. </span><span class="koboSpan" id="kobo.414.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">random_state</span></strong><span class="koboSpan" id="kobo.416.1"> parameter is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">42</span></strong><span class="koboSpan" id="kobo.418.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">ensure reproducibility:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.420.1">
Isolation_forest = IsolationForest(contamination='auto', random_state=42)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.421.1">Fit the model and predict anomalies</span></strong><span class="koboSpan" id="kobo.422.1">: The Isolation Forest model is fitted to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.424.1"> data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">fit_predict()</span></strong><span class="koboSpan" id="kobo.426.1"> method. </span><span class="koboSpan" id="kobo.426.2">This method simultaneously fits the model to the data and predicts whether each data point is an outlier or not. </span><span class="koboSpan" id="kobo.426.3">The predictions are stored in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">anomaly_predictions</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.428.1"> array.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.429.1">Add anomaly predictions to the original dataset</span></strong><span class="koboSpan" id="kobo.430.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.431.1">anomaly_predictions</span></strong><span class="koboSpan" id="kobo.432.1"> array contains predicted labels for each data point: </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">-1</span></strong><span class="koboSpan" id="kobo.434.1"> for anomalies (outliers) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">1</span></strong><span class="koboSpan" id="kobo.436.1"> for inliers (non-outliers). </span><span class="koboSpan" id="kobo.436.2">These predictions are added as a new </span><strong class="source-inline"><span class="koboSpan" id="kobo.437.1">'IsAnomaly'</span></strong><span class="koboSpan" id="kobo.438.1"> column to the original </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.439.1">df</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.440.1"> DataFrame.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.441.1">Display rows with anomalies</span></strong><span class="koboSpan" id="kobo.442.1">: Finally, the code filters rows in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.443.1">df</span></strong><span class="koboSpan" id="kobo.444.1"> DataFrame where </span><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">IsAnomaly</span></strong><span class="koboSpan" id="kobo.446.1"> is equal to -1, indicating the presence of outliers. </span><span class="koboSpan" id="kobo.446.2">The resulting DataFrame contains all rows with anomalies, which can then be further analyzed or processed </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">as needed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.448.1">
anomaly_predictions = Isolation_forest.fit_predict(X_anomaly)
df['IsAnomaly'] = anomaly_predictions
anomalies = df[df['IsAnomaly'] == -1]
anomalies.head()</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.449.1">We can now view the DataFrame</span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.450.1"> with rows that the model has predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">as anomalies:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<span class="koboSpan" id="kobo.452.1"><img alt="Figure 9.6 – The resulting anomalies DataFrame" src="image/B19297_09_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.453.1">Figure 9.6 – The resulting anomalies DataFrame</span></p>
<p><span class="koboSpan" id="kobo.454.1">In conclusion, Isolation Forest stands out as a powerful and efficient tool for anomaly detection, particularly in scenarios where anomalies are rare and distinctly different from normal instances. </span><span class="koboSpan" id="kobo.454.2">Its ability to isolate anomalies through the creation of random trees makes it a valuable asset in various applications, from fraud detection to network security. </span><span class="koboSpan" id="kobo.454.3">However, it’s essential to acknowledge the algorithm’s limits. </span><span class="koboSpan" id="kobo.454.4">Isolation Forest might face challenges when anomalies are not well separated or when datasets are highly dimensional. </span><span class="koboSpan" id="kobo.454.5">In the next section, we will explore autoencoders – a semi-supervised method for </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">anomaly detection.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.456.1">Semi-supervised methods using autoencoders</span></h2>
<p><span class="koboSpan" id="kobo.457.1">Anomaly detection </span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.458.1">using autoencoders is an unsupervised learning approach that leverages neural networks (NNs) to detect anomalies in data. </span><span class="koboSpan" id="kobo.458.2">Autoencoders are a type of NN architecture designed to reconstruct the input data from a compressed representation. </span><span class="koboSpan" id="kobo.458.3">In anomaly detection, we exploit the fact that autoencoders struggle to reconstruct anomalous instances, making them useful for identifying unusual patterns </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">or outliers.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">Autoencoders </span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.461.1">consist of two main components: an encoder and a decoder. </span><span class="koboSpan" id="kobo.461.2">The </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.462.1">encoder compresses the input data into a lower-dimensional</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.463.1"> representation called the “latent space,” while the </span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.464.1">decoder tries to reconstruct the original input from this representation. </span><span class="koboSpan" id="kobo.464.2">The encoder and decoder are typically symmetric, and the network is trained to minimize </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">reconstruction errors.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">In anomaly detection, we train the autoencoder on normal data without anomalies. </span><span class="koboSpan" id="kobo.466.2">Since the autoencoder learns to reconstruct normal data, it will be less capable of reconstructing anomalies, leading to higher reconstruction errors for anomalous instances. </span><span class="koboSpan" id="kobo.466.3">This property allows us to use the reconstruction error as an </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">anomaly score.</span></span></p>
<p><span class="koboSpan" id="kobo.468.1">During training, we compare the original input (for example, numerical features) to the reconstructed output. </span><span class="koboSpan" id="kobo.468.2">The difference between the two is the reconstruction error. </span><span class="koboSpan" id="kobo.468.3">A low reconstruction error indicates that the input is close to the normal data distribution, while a high reconstruction error suggests that the input is likely </span><span class="No-Break"><span class="koboSpan" id="kobo.469.1">an anomaly.</span></span></p>
<p><span class="koboSpan" id="kobo.470.1">After training the autoencoder, we need to set a threshold to distinguish between normal and anomalous instances based on the reconstruction error. </span><span class="koboSpan" id="kobo.470.2">There are several methods to set the threshold, such as percentile-based or using validation data. </span><span class="koboSpan" id="kobo.470.3">The threshold will depend on the desired trade-off between false positives and false negatives, which can be adjusted based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">application’s requirements.</span></span></p>
<p><span class="koboSpan" id="kobo.472.1">Autoencoders are flexible and can capture complex patterns in the data, making them suitable for high-dimensional data with non-linear relationships. </span><span class="koboSpan" id="kobo.472.2">They can handle both global and local anomalies, meaning they can detect anomalies that differ from the majority of data points and anomalies within specific regions of the data. </span><span class="koboSpan" id="kobo.472.3">Autoencoders are capable of unsupervised learning, which is advantageous when labeled anomaly data is limited or unavailable. </span><span class="koboSpan" id="kobo.472.4">As with other unsupervised methods, autoencoders may produce false positives (normal data misclassified as anomalies) and false negatives (anomalies misclassified as normal data). </span><span class="koboSpan" id="kobo.472.5">They may struggle to detect anomalies that are very similar to the normal data, as the reconstruction error might not be </span><span class="No-Break"><span class="koboSpan" id="kobo.473.1">significantly different.</span></span></p>
<p><span class="koboSpan" id="kobo.474.1">Let’s see an</span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.475.1"> example implementation of this approach using the TensorFlow library and the Loan Prediction dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">in Python:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.477.1">Load data</span></strong><span class="koboSpan" id="kobo.478.1">: The code begins by importing necessary libraries and loading the dataset from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.479.1">train_loan_prediction.csv</span></strong><span class="koboSpan" id="kobo.480.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.482.1">pd.read_csv()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.484.1">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
df = pd.read_csv('train_loan_prediction.csv')</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.485.1">Extract numerical features</span></strong><span class="koboSpan" id="kobo.486.1">: The code defines a </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">numerical_features</span></strong><span class="koboSpan" id="kobo.488.1"> list containing the names of the numerical columns to be used for anomaly detection. </span><span class="koboSpan" id="kobo.488.2">These columns are </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">'ApplicantIncome'</span></strong><span class="koboSpan" id="kobo.490.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">'CoapplicantIncome'</span></strong><span class="koboSpan" id="kobo.492.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">'LoanAmount'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.496.1">
numerical_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.497.1">Create a DataFrame for anomaly detection</span></strong><span class="koboSpan" id="kobo.498.1">: A new </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.500.1"> DataFrame is created by extracting the columns specified in </span><strong class="source-inline"><span class="koboSpan" id="kobo.501.1">numerical_features</span></strong><span class="koboSpan" id="kobo.502.1"> from the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.503.1">df</span></strong><span class="koboSpan" id="kobo.504.1"> DataFrame. </span><span class="koboSpan" id="kobo.504.2">This new DataFrame will be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">anomaly detection.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.506.1">Handle missing values</span></strong><span class="koboSpan" id="kobo.507.1">: Any missing values in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.509.1"> DataFrame are replaced with the mean value of their respective columns using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.510.1">fillna()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.511.1"> method:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.512.1">
X_anomaly = df[numerical_features]
X_anomaly.fillna(X_anomaly.mean(), inplace=True)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.513.1">Standardize numerical features</span></strong><span class="koboSpan" id="kobo.514.1">: Numerical features in </span><strong class="source-inline"><span class="koboSpan" id="kobo.515.1">X_anomaly</span></strong><span class="koboSpan" id="kobo.516.1"> are standardized using </span><strong class="source-inline"><span class="koboSpan" id="kobo.517.1">StandardScaler()</span></strong><span class="koboSpan" id="kobo.518.1">. </span><span class="koboSpan" id="kobo.518.2">Standardization scales the features to have </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.519.1">zero mean and unit variance, which is important for training machine </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">learning models:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.521.1">
original_indices = X_anomaly.index
scaler = StandardScaler()
X_anomaly_scaled = scaler.fit_transform(X_anomaly)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.522.1">Split data into training and testing sets</span></strong><span class="koboSpan" id="kobo.523.1">: The standardized data is split into training (</span><strong class="source-inline"><span class="koboSpan" id="kobo.524.1">X_train</span></strong><span class="koboSpan" id="kobo.525.1">) and testing (</span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">X_test</span></strong><span class="koboSpan" id="kobo.527.1">) sets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.529.1"> function. </span><span class="koboSpan" id="kobo.529.2">The original indices of the data are also stored </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">original_indices</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.533.1">
X_train, X_test, _, _ = train_test_split(X_anomaly_scaled, original_indices, test_size=0.2, random_state=42)
X_test_df = pd.DataFrame(X_test, columns=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'])</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.534.1">Build and train an autoencoder model</span></strong><span class="koboSpan" id="kobo.535.1">: An autoencoder neural network model is constructed using TensorFlow’s Keras API. </span><span class="koboSpan" id="kobo.535.2">The autoencoder is an unsupervised learning model designed to reconstruct the input data. </span><span class="koboSpan" id="kobo.535.3">It consists of an encoder and decoder, both composed of </span><strong class="source-inline"><span class="koboSpan" id="kobo.536.1">Dense</span></strong><span class="koboSpan" id="kobo.537.1"> layers. </span><span class="koboSpan" id="kobo.537.2">The model is trained using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">fit()</span></strong><span class="koboSpan" id="kobo.539.1"> method, with the </span><strong class="bold"><span class="koboSpan" id="kobo.540.1">mean squared error</span></strong><span class="koboSpan" id="kobo.541.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.542.1">MSE</span></strong><span class="koboSpan" id="kobo.543.1">) as the </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">loss function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.545.1">
input_dim = X_anomaly.shape[1]
encoding_dim = 2
input_layer = Input(shape=(input_dim,))
encoder_layer = Dense(encoding_dim, activation='relu')(input_layer)
decoder_layer = Dense(input_dim, activation='sigmoid')(encoder_layer)
autoencoder = Model(inputs=input_layer, outputs=decoder_layer)
autoencoder.compile(optimizer='adam', loss='mean_squared_error')
autoencoder.fit(X_anomaly_scaled, X_anomaly_scaled, epochs=50, batch_size=16)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.546.1">Reconstruct data and calculate reconstruction error</span></strong><span class="koboSpan" id="kobo.547.1">: The trained autoencoder </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.548.1">is used to reconstruct data points in </span><strong class="source-inline"><span class="koboSpan" id="kobo.549.1">X_test</span></strong><span class="koboSpan" id="kobo.550.1">, and reconstruction errors are calculated as the mean squared difference between the original and </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">reconstructed data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.552.1">
X_test_reconstructed = autoencoder.predict(X_test)
reconstruction_error_test = np.mean(np.square(X_test - X_test_reconstructed), axis=1)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.553.1">Define threshold for anomaly detection</span></strong><span class="koboSpan" id="kobo.554.1">: A threshold for anomaly detection is defined by calculating the 95th percentile of reconstruction errors </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">X_test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.558.1">
threshold = np.percentile(reconstruction_error_test, 95)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.559.1">Predict anomalies</span></strong><span class="koboSpan" id="kobo.560.1">: Anomalies are predicted by comparing reconstruction errors against the threshold. </span><span class="koboSpan" id="kobo.560.2">If the reconstruction error for a data point is greater than the threshold, it is classified as an anomaly and assigned a value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.561.1">1</span></strong><span class="koboSpan" id="kobo.562.1"> in </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">anomaly_predictions</span></strong><span class="koboSpan" id="kobo.564.1">; otherwise, it is </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">assigned </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.568.1">
anomaly_predictions = (reconstruction_error_test &gt; threshold).astype(int)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.569.1">Create a new DataFrame with anomaly predictions</span></strong><span class="koboSpan" id="kobo.570.1">: A new </span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">anomaly_df</span></strong><span class="koboSpan" id="kobo.572.1"> DataFrame is created with the anomaly predictions and the corresponding index </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.574.1">X_test_df</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.576.1">
anomaly_df = pd.DataFrame({'IsAnomaly': anomaly_predictions}, index=X_test_df.index)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.577.1">Merge anomaly predictions with the original DataFrame</span></strong><span class="koboSpan" id="kobo.578.1">: The anomaly predictions</span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.579.1"> are merged with the original </span><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">df</span></strong><span class="koboSpan" id="kobo.581.1"> DataFrame using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.582.1">merge()</span></strong><span class="koboSpan" id="kobo.583.1"> method, adding the </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">'IsAnomaly'</span></strong><span class="koboSpan" id="kobo.585.1"> column </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.587.1">df</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.589.1">Display rows with anomalies</span></strong><span class="koboSpan" id="kobo.590.1">: The code checks if the </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">'IsAnomaly'</span></strong><span class="koboSpan" id="kobo.592.1"> column is present in </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">df</span></strong><span class="koboSpan" id="kobo.594.1">. </span><span class="koboSpan" id="kobo.594.2">If present, it displays rows where </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">'IsAnomaly'</span></strong><span class="koboSpan" id="kobo.596.1"> is equal to </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">1</span></strong><span class="koboSpan" id="kobo.598.1">, indicating the presence of anomalies. </span><span class="koboSpan" id="kobo.598.2">If not present, it prints </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">"No </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">anomalies detected."</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.602.1">
df = df.merge(anomaly_df, how='left', left_index=True, right_index=True)
if 'IsAnomaly' in df.columns:
    # Display the rows with anomalies
    anomalies = df[df['IsAnomaly'] == 1]
    anomalies
else:
    print("No anomalies detected.")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.603.1">The resulting DataFrame is </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">as follows:</span></span></p></li> </ul>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.605.1"><img alt="Figure 9.7 – The resulting IsAnomaly DataFrame" src="image/B19297_09_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.606.1">Figure 9.7 – The resulting IsAnomaly DataFrame</span></p>
<p><span class="koboSpan" id="kobo.607.1">In summary, autoencoders </span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.608.1">prove to be a versatile and powerful tool for anomaly detection, capturing nuanced patterns that may elude traditional methods. </span><span class="koboSpan" id="kobo.608.2">Their ability to discover subtle anomalies within complex data structures makes them invaluable in diverse domains, including image analysis, cybersecurity, and industrial </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">quality control.</span></span></p>
<p><span class="koboSpan" id="kobo.610.1">However, the effectiveness of autoencoders is contingent on various factors. </span><span class="koboSpan" id="kobo.610.2">The architecture’s complexity and the selection of hyperparameters can influence performance, requiring careful tuning for optimal results. </span><span class="koboSpan" id="kobo.610.3">In the next section, we will understand how SVMs can be used in </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">anomaly detection.</span></span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.612.1">Supervised methods using SVMs</span></h2>
<p><span class="koboSpan" id="kobo.613.1">SVMs are a </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.614.1">powerful class of supervised</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.615.1"> learning algorithms commonly used for classification tasks. </span><span class="koboSpan" id="kobo.615.2">When applied to anomaly detection, SVMs prove to be effective in separating normal instances from anomalies by finding a hyperplane with a maximum margin. </span><span class="koboSpan" id="kobo.615.3">Here is how</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.616.1"> SVMs work under </span><span class="No-Break"><span class="koboSpan" id="kobo.617.1">the hood:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.618.1">Hyperplane definition</span></strong><span class="koboSpan" id="kobo.619.1">: In a two-dimensional space, a hyperplane is a flat, two-dimensional subspace. </span><span class="koboSpan" id="kobo.619.2">SVM aims to find a hyperplane that best separates the dataset into two classes — normal and anomalous. </span><span class="koboSpan" id="kobo.619.3">This hyperplane is positioned to maximize the margin, which is the distance between the hyperplane and the nearest data points of </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">each class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.621.1">Decision boundary</span></strong><span class="koboSpan" id="kobo.622.1">: The hyperplane serves as a decision boundary that separates instances of one class from another. </span><span class="koboSpan" id="kobo.622.2">In a binary classification scenario, instances on one side of the hyperplane are classified as belonging to one class, and those on the other side are classified as belonging to the </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">other class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.624.1">Kernel trick</span></strong><span class="koboSpan" id="kobo.625.1">: SVM </span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.626.1">can handle complex relationships in the data through the use of a kernel function. </span><span class="koboSpan" id="kobo.626.2">In many real-world scenarios, the relationship between features may not be linear. </span><span class="koboSpan" id="kobo.626.3">SVM addresses this by using a kernel function. </span><span class="koboSpan" id="kobo.626.4">This function transforms the input data into a higher-dimensional space, making it easier to find a hyperplane that effectively separates the classes. </span><span class="koboSpan" id="kobo.626.5">Commonly used kernel functions include the linear kernel (for linearly separable data), polynomial kernel, </span><strong class="bold"><span class="koboSpan" id="kobo.627.1">radial basis function</span></strong><span class="koboSpan" id="kobo.628.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.629.1">RBF</span></strong><span class="koboSpan" id="kobo.630.1">) or </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.631.1">Gaussian kernel, and sigmoid kernel. </span><span class="koboSpan" id="kobo.631.2">The choice of kernel depends on the nature of </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Optimal hyperplane</span></strong><span class="koboSpan" id="kobo.634.1">: SVM aims to find the hyperplane that maximizes the margin, which is the distance between the hyperplane and the nearest data points of each class. </span><span class="koboSpan" id="kobo.634.2">The larger the margin, the more robust and generalizable the model is likely to be. </span><span class="koboSpan" id="kobo.634.3">Support vectors are data points that lie closest to the decision boundary. </span><span class="koboSpan" id="kobo.634.4">They play a crucial role in defining the optimal hyperplane and the margin. </span><span class="koboSpan" id="kobo.634.5">SVM focuses on these support vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">during training.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.636.1">Let’s implement a </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.637.1">Python example of SVM in anomaly detection using our Loan </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">Prediction dataset:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.639.1">Load data</span></strong><span class="koboSpan" id="kobo.640.1">: Let’s begin by importing the necessary libraries and loading the dataset from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">train_loan_prediction.csv</span></strong><span class="koboSpan" id="kobo.642.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">pd.read_csv()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.646.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import OneClassSVM
from sklearn.metrics import classification_report, accuracy_score
df = pd.read_csv('train_loan_prediction.csv')</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.647.1">Data preprocessing</span></strong><span class="koboSpan" id="kobo.648.1">: We will do some basic data preprocessing tasks, including handling missing values. </span><span class="koboSpan" id="kobo.648.2">For this anomaly detection example, we simplify the analysis by excluding categorical variables. </span><span class="koboSpan" id="kobo.648.3">In a more complex analysis, you </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.649.1">might choose to encode and include these variables if they are deemed relevant to your specific anomaly </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">detection task:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.651.1">
df = df.drop(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'], axis=1)
df['Loan_Status'] = df['Loan_Status'].map({'Y': 0, 'N': 1})
df.fillna(df.mean(), inplace=True)</span></pre></li> <li><span class="koboSpan" id="kobo.652.1">Create a train-test split for the </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">SVM model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.654.1">
X = df.drop('Loan_Status', axis=1)
y = df['Loan_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.655.1">Standardize the features using </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">StandardScaler</span></strong><span class="koboSpan" id="kobo.657.1"> from scikit-learn to ensure that all features have the </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">same scale:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.659.1">
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)</span></pre></li> <li><span class="koboSpan" id="kobo.660.1">Train the One-Class SVM model for anomaly detection. </span><span class="koboSpan" id="kobo.660.2">Adjust the </span><em class="italic"><span class="koboSpan" id="kobo.661.1">nu</span></em><span class="koboSpan" id="kobo.662.1"> parameter based on the expected proportion of outliers in your dataset. </span><span class="koboSpan" id="kobo.662.2">The </span><em class="italic"><span class="koboSpan" id="kobo.663.1">nu</span></em><span class="koboSpan" id="kobo.664.1"> parameter represents an upper bound on the fraction of margin errors and a lower bound on the fraction of support vectors. </span><span class="koboSpan" id="kobo.664.2">It essentially controls the proportion of outliers or anomalies the algorithm should consider. </span><span class="koboSpan" id="kobo.664.3">Choosing an appropriate value for </span><em class="italic"><span class="koboSpan" id="kobo.665.1">nu</span></em><span class="koboSpan" id="kobo.666.1"> is crucial, and it depends on the characteristics of your dataset and the expected proportion of anomalies. </span><span class="koboSpan" id="kobo.666.2">Here are some guidelines to help you select the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.667.1">nu</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.668.1"> parameter:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.669.1">Understand the nature of anomalies</span></strong><span class="koboSpan" id="kobo.670.1">: Assess the domain knowledge and characteristics of your dataset. </span><span class="koboSpan" id="kobo.670.2">Understand the expected proportion of anomalies. </span><span class="koboSpan" id="kobo.670.3">If anomalies are rare, a smaller value of </span><em class="italic"><span class="koboSpan" id="kobo.671.1">nu</span></em><span class="koboSpan" id="kobo.672.1"> might </span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">be appropriate.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.674.1">Experiment with a range of values</span></strong><span class="koboSpan" id="kobo.675.1">: Start by experimenting with a range of </span><em class="italic"><span class="koboSpan" id="kobo.676.1">nu</span></em><span class="koboSpan" id="kobo.677.1"> values, such as 0.01, 0.05, 0.1, 0.2, and so on. </span><span class="koboSpan" id="kobo.677.2">You can adjust this range based on your understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">the data.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.679.1">Consider the dataset size</span></strong><span class="koboSpan" id="kobo.680.1">: The size of your dataset can also influence the choice of </span><em class="italic"><span class="koboSpan" id="kobo.681.1">nu</span></em><span class="koboSpan" id="kobo.682.1">. </span><span class="koboSpan" id="kobo.682.2">For larger datasets, a smaller value might be suitable, while for smaller datasets, a relatively larger value may </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">be appropriate.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.684.1">Balance false positives and false negatives</span></strong><span class="koboSpan" id="kobo.685.1">: Depending on the application, you might prioritize minimizing false positives or false negatives. </span><span class="koboSpan" id="kobo.685.2">Adjust </span><em class="italic"><span class="koboSpan" id="kobo.686.1">nu</span></em><span class="koboSpan" id="kobo.687.1"> accordingly to achieve the </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">desired balance.</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.689.1">We will </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.690.1">implement an experiment with a range of values for </span><em class="italic"><span class="koboSpan" id="kobo.691.1">nu</span></em><span class="koboSpan" id="kobo.692.1">. </span><span class="koboSpan" id="kobo.692.2">We will specify a list of </span><em class="italic"><span class="koboSpan" id="kobo.693.1">nu</span></em><span class="koboSpan" id="kobo.694.1"> values that we want to experiment with. </span><span class="koboSpan" id="kobo.694.2">These values represent the upper bound of the fraction of margin errors and the lower bound of the fraction of support vectors in the One-Class SVM model. </span><span class="koboSpan" id="kobo.694.3">We then create an empty list to store the mean decision function values for each </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.695.1">nu</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.696.1"> value:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.697.1">
nu_values = [0.01, 0.05, 0.1, 0.2, 0.3]
mean_decision_function_values = []</span></pre></li> <li><span class="koboSpan" id="kobo.698.1">For each </span><em class="italic"><span class="koboSpan" id="kobo.699.1">nu</span></em><span class="koboSpan" id="kobo.700.1"> value in the list, train a One-Class SVM model with that </span><em class="italic"><span class="koboSpan" id="kobo.701.1">nu</span></em><span class="koboSpan" id="kobo.702.1"> value. </span><span class="koboSpan" id="kobo.702.2">Retrieve the decision function values for the test set and calculate the mean decision function value. </span><span class="koboSpan" id="kobo.702.3">Append the mean decision function value to </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">the list:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.704.1">
for nu in nu_values:
    svm_model = OneClassSVM(nu=nu, kernel='rbf', gamma=0.1)
    svm_model.fit(X_train_scaled)
    decision_function_values=
    svm_model.decision_function(X_test_scaled)
    mean_decision_function = np.mean(decision_function_values)
    mean_decision_function_values.append(mean_decision_function)</span></pre></li> <li><span class="koboSpan" id="kobo.705.1">Identify </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.706.1">the index of the </span><em class="italic"><span class="koboSpan" id="kobo.707.1">nu</span></em><span class="koboSpan" id="kobo.708.1"> value that corresponds to the highest mean decision function value. </span><span class="koboSpan" id="kobo.708.2">Then, retrieve the best </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.709.1">nu</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.710.1"> value:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.711.1">
best_nu_index = np.argmax(mean_decision_function_values)
best_nu = nu_values[best_nu_index]</span></pre></li> <li><span class="koboSpan" id="kobo.712.1">Create a final One-Class SVM model using the best </span><em class="italic"><span class="koboSpan" id="kobo.713.1">nu</span></em><span class="koboSpan" id="kobo.714.1"> value and train it on the scaled </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">training data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.716.1">
final_model = OneClassSVM(nu=best_nu, kernel='rbf', gamma=0.1)
final_model.fit(X_train_scaled)</span></pre></li> <li><span class="koboSpan" id="kobo.717.1">We now use this model to predict anomalies on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.718.1">X_test_scaled</span></strong><span class="koboSpan" id="kobo.719.1"> test dataset. </span><span class="koboSpan" id="kobo.719.2">This line creates a binary representation of the predictions (</span><strong class="source-inline"><span class="koboSpan" id="kobo.720.1">y_pred</span></strong><span class="koboSpan" id="kobo.721.1">) by mapping -1 to 1 (indicating anomalies) and any other value (typically 1) to 0 (indicating normal instances). </span><span class="koboSpan" id="kobo.721.2">This is done because the One-Class SVM model often assigns -1 to anomalies and 1 to normal instances. </span><span class="koboSpan" id="kobo.721.3">We will store this in a new DataFrame </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">as </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.723.1">df_with_anomalies</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.725.1">
y_pred = final_model.predict(X_test_scaled)
y_pred_binary = [1 if pred == -1 else 0 for pred in y_pred]
test_set_df = pd.DataFrame(data=X_test_scaled, columns=X.columns, index=X_test.index)
test_set_df['Anomaly_Label'] = y_pred_binary
df_with_anomalies = pd.concat([df, test_set_df['Anomaly_Label']], axis=1, join='outer')
df_with_anomalies['Anomaly_Label'].fillna(0, inplace=True)</span></pre></li> <li><span class="koboSpan" id="kobo.726.1">Print the </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.727.1">confusion matrix and </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">accuracy score:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.729.1">
print("Classification Report:\n", classification_report(y_test, y_pred_binary))print("Accuracy Score:", accuracy_score(y_test, y_pred_binary))</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.730.1">This will print the </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">following report:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.732.1"><img alt="Figure 9.8 – Output classification report" src="image/B19297_09_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.733.1">Figure 9.8 – Output classification report</span></p>
<ol>
<li value="12"><span class="koboSpan" id="kobo.734.1">Print DataFrame rows predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.735.1">as anomalies:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.736.1">
df_with_anomalies[df_with_anomalies['Anomaly_Label'] == 1]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.737.1">This will output the </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">following DataFrame:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.739.1"><img alt="Figure 9.9 – The df_with_anomalies DataFrame" src="image/B19297_09_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.740.1">Figure 9.9 – The df_with_anomalies DataFrame</span></p>
<p><span class="koboSpan" id="kobo.741.1">In this section, we’ve </span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.742.1">walked through the process of implementing anomaly detection using SVM in Python. </span><span class="koboSpan" id="kobo.742.2">Anomaly detection using SVM can be adapted for various datasets with clear anomalies, making it a valuable tool for outlier identification. </span><span class="koboSpan" id="kobo.742.3">In the next section, we will explore data augmentation and resampling techniques for identifying edge cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">rare events.</span></span></p>
<h1 id="_idParaDest-145"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.744.1">Data augmentation and resampling techniques</span></h1>
<p><span class="koboSpan" id="kobo.745.1">Class imbalance is a</span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.746.1"> common issue in datasets with rare events. </span><span class="koboSpan" id="kobo.746.2">Class imbalance can adversely affect the model’s performance, as the model tends to be biased toward the majority class. </span><span class="koboSpan" id="kobo.746.3">To address this, we will explore two </span><span class="No-Break"><span class="koboSpan" id="kobo.747.1">resampling techniques:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.748.1">Oversampling</span></strong><span class="koboSpan" id="kobo.749.1">: Increasing the number of instances in the minority class by generating </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">synthetic samples</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.751.1">Undersampling</span></strong><span class="koboSpan" id="kobo.752.1">: Reducing the number of instances in the majority class to balance </span><span class="No-Break"><span class="koboSpan" id="kobo.753.1">class distribution</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.754.1">Let’s discuss </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.755.1">these resampling techniques in </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">more detail.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.757.1">Oversampling using SMOTE</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.758.1">Synthetic Minority Over-sampling TEchnique</span></strong><span class="koboSpan" id="kobo.759.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.760.1">SMOTE</span></strong><span class="koboSpan" id="kobo.761.1">) is a widely used resampling</span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.762.1"> method for addressing </span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.763.1">class imbalance in machine learning datasets, especially when dealing with rare events or minority classes. </span><span class="koboSpan" id="kobo.763.2">SMOTE</span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.764.1"> helps to generate synthetic samples for the minority class by interpolating between existing minority class samples. </span><span class="koboSpan" id="kobo.764.2">This technique aims to balance class distribution by creating additional synthetic instances, thereby mitigating the effects of class imbalance. </span><span class="koboSpan" id="kobo.764.3">In a dataset with class imbalance, the minority class contains significantly fewer instances than the majority class. </span><span class="koboSpan" id="kobo.764.4">This can</span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.765.1"> lead to biased model training, where the model tends to favor the majority class and performs poorly on the </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">minority class.</span></span></p>
<p><span class="koboSpan" id="kobo.767.1">Here are the key</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.768.1"> steps of a </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">SMOTE</span></span><span class="No-Break"><a id="_idIndexMarker762"/></span><span class="No-Break"><span class="koboSpan" id="kobo.770.1"> algorithm:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.771.1">Identifying minority class instances</span></strong><span class="koboSpan" id="kobo.772.1">: The first step of SMOTE is to identify instances belonging to the </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">minority class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.774.1">Selecting nearest neighbors</span></strong><span class="koboSpan" id="kobo.775.1">: For each minority class instance, SMOTE selects its </span><em class="italic"><span class="koboSpan" id="kobo.776.1">k</span></em><span class="koboSpan" id="kobo.777.1"> nearest neighbors (commonly chosen through the </span><strong class="bold"><span class="koboSpan" id="kobo.778.1">k-nearest neighbors</span></strong><span class="koboSpan" id="kobo.779.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.780.1">KNN</span></strong><span class="koboSpan" id="kobo.781.1">) algorithm). </span><span class="koboSpan" id="kobo.781.2">These </span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.782.1">neighbors are used to create </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">synthetic samples.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.784.1">Creating synthetic samples</span></strong><span class="koboSpan" id="kobo.785.1">: For each minority class instance, SMOTE generates synthetic samples along the line connecting the instance to its </span><em class="italic"><span class="koboSpan" id="kobo.786.1">k</span></em><span class="koboSpan" id="kobo.787.1"> nearest neighbors in the feature space. </span><span class="koboSpan" id="kobo.787.2">Synthetic samples are created by adding a random fraction (usually between 0 and 1) of the feature differences between the instance and its neighbors. </span><span class="koboSpan" id="kobo.787.3">This process effectively introduces variability to </span><span class="No-Break"><span class="koboSpan" id="kobo.788.1">synthetic samples.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.789.1">Combining with the original data</span></strong><span class="koboSpan" id="kobo.790.1">: The synthetic samples are combined with the original minority class instances, resulting in a resampled dataset with a more balanced </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">class distribution.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.792.1">SMOTE helps to address class imbalance without discarding any data, as it generates synthetic samples rather than removing instances from the majority class. </span><span class="koboSpan" id="kobo.792.2">It increases the information available to the model, potentially improving the model’s ability to generalize to the minority class. </span><span class="koboSpan" id="kobo.792.3">SMOTE is straightforward to implement and is available in popular libraries such as imbalanced-learn </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.794.1">While SMOTE is effective in many cases, it might not always perform optimally for highly imbalanced datasets or datasets with complex decision boundaries. </span><span class="koboSpan" id="kobo.794.2">Generating too many synthetic samples can lead to overfitting on the training data, so it is crucial to choose an appropriate value for the number of nearest neighbors (</span><em class="italic"><span class="koboSpan" id="kobo.795.1">k</span></em><span class="koboSpan" id="kobo.796.1">). </span><span class="koboSpan" id="kobo.796.2">SMOTE may introduce some noise and may not be as effective if the minority class is too sparse or scattered in the feature space. </span><span class="koboSpan" id="kobo.796.3">SMOTE can be combined with other techniques, such as undersampling the majority class or using different resampling ratios, to achieve better performance. </span><span class="koboSpan" id="kobo.796.4">It is</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.797.1"> essential to evaluate the model’s performance on appropriate metrics (for example, precision, recall, or F1-score) to assess the impact of SMOTE and other techniques on the model’s ability to detect </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">rare events.</span></span></p>
<p><span class="koboSpan" id="kobo.799.1">Let’s implement </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.800.1">this approach </span><span class="No-Break"><span class="koboSpan" id="kobo.801.1">in Python:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.802.1">Load data</span></strong><span class="koboSpan" id="kobo.803.1">: The code starts by importing necessary libraries and loading the Loan Prediction dataset from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.804.1">train_loan_prediction.csv</span></strong><span class="koboSpan" id="kobo.805.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.807.1">pd.read_csv()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.809.1">
import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
df = pd.read_csv('train_loan_prediction.csv')</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.810.1">Map target variable</span></strong><span class="koboSpan" id="kobo.811.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">'Loan_Status'</span></strong><span class="koboSpan" id="kobo.813.1"> column in the dataset contains </span><strong class="source-inline"><span class="koboSpan" id="kobo.814.1">'Y'</span></strong><span class="koboSpan" id="kobo.815.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">'N'</span></strong><span class="koboSpan" id="kobo.817.1"> categorical values, which represent loan approval (</span><strong class="source-inline"><span class="koboSpan" id="kobo.818.1">'Y'</span></strong><span class="koboSpan" id="kobo.819.1">) and rejection (</span><strong class="source-inline"><span class="koboSpan" id="kobo.820.1">'N'</span></strong><span class="koboSpan" id="kobo.821.1">). </span><span class="koboSpan" id="kobo.821.2">To convert this categorical target variable into a numerical format, </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">'Y'</span></strong><span class="koboSpan" id="kobo.823.1"> is mapped to </span><strong class="source-inline"><span class="koboSpan" id="kobo.824.1">1</span></strong><span class="koboSpan" id="kobo.825.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.826.1">'N'</span></strong><span class="koboSpan" id="kobo.827.1"> is mapped to </span><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">0</span></strong><span class="koboSpan" id="kobo.829.1"> using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.830.1">map()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.831.1"> function:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.832.1">
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.833.1">Handle missing values</span></strong><span class="koboSpan" id="kobo.834.1">: The code applies mean imputation to all columns with missing values in the dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">fillna()</span></strong><span class="koboSpan" id="kobo.836.1"> method. </span><span class="koboSpan" id="kobo.836.2">This ensures that any missing values in the dataset are replaced with the mean value of their </span><span class="No-Break"><span class="koboSpan" id="kobo.837.1">respective columns:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.838.1">
df.fillna(df.mean(), inplace=True)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.839.1">Exclude non-numerical columns</span></strong><span class="koboSpan" id="kobo.840.1">: The code selects only numerical columns from the dataset to build an </span><strong class="source-inline"><span class="koboSpan" id="kobo.841.1">X</span></strong><span class="koboSpan" id="kobo.842.1"> feature set. </span><span class="koboSpan" id="kobo.842.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.843.1">select_dtypes()</span></strong><span class="koboSpan" id="kobo.844.1"> method is used to include only columns with </span><strong class="source-inline"><span class="koboSpan" id="kobo.845.1">float</span></strong><span class="koboSpan" id="kobo.846.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">int</span></strong><span class="koboSpan" id="kobo.848.1"> data types while excluding non-numerical columns such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.849.1">'Loan_Status'</span></strong><span class="koboSpan" id="kobo.850.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.851.1">'Loan_ID'</span></strong><span class="koboSpan" id="kobo.852.1">. </span><span class="koboSpan" id="kobo.852.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.853.1">y</span></strong><span class="koboSpan" id="kobo.854.1"> target </span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.855.1">variable is set </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.857.1">'Loan_Status'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.859.1">
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.860.1">Split data into training and testing sets</span></strong><span class="koboSpan" id="kobo.861.1">: The data is split into training (</span><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">X_train</span></strong><span class="koboSpan" id="kobo.863.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.864.1">y_train</span></strong><span class="koboSpan" id="kobo.865.1">) and testing (</span><strong class="source-inline"><span class="koboSpan" id="kobo.866.1">X_test</span></strong><span class="koboSpan" id="kobo.867.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.868.1">y_test</span></strong><span class="koboSpan" id="kobo.869.1">) sets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.870.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.871.1"> function from scikit-learn. </span><span class="koboSpan" id="kobo.871.2">The training set consists of 80% of the data, while the testing set contains 20% of the data. </span><span class="koboSpan" id="kobo.871.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.872.1">random_state</span></strong><span class="koboSpan" id="kobo.873.1"> parameter is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.874.1">42</span></strong><span class="koboSpan" id="kobo.875.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">ensure reproducibility:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.877.1">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.878.1">Instantiate SMOTE</span></strong><span class="koboSpan" id="kobo.879.1">: Here, SMOTE is instantiated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.880.1">SMOTE(random_state=42</span></strong><span class="koboSpan" id="kobo.881.1">). </span><span class="koboSpan" id="kobo.881.2">SMOTE is then applied to the training data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.882.1">fit_resample()</span></strong><span class="koboSpan" id="kobo.883.1"> method. </span><span class="koboSpan" id="kobo.883.2">This method oversamples the minority class (loan rejection) by generating synthetic samples, creating a balanced dataset. </span><span class="koboSpan" id="kobo.883.3">The resulting resampled data is stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.884.1">X_train_resampled</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.885.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.886.1">y_train_resampled</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.888.1">
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.889.1">Train Random Forest classifier</span></strong><span class="koboSpan" id="kobo.890.1">: A Random Forest classifier is instantiated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.891.1">RandomForestClassifier(random_state=42)</span></strong><span class="koboSpan" id="kobo.892.1">. </span><span class="koboSpan" id="kobo.892.2">The classifier is trained on the resampled data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.893.1">X_train_resampled</span></strong><span class="koboSpan" id="kobo.894.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.895.1">y_train_resampled</span></strong><span class="koboSpan" id="kobo.896.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">fit()</span></strong><span class="koboSpan" id="kobo.898.1"> method. </span><span class="koboSpan" id="kobo.898.2">The trained classifier is used to make predictions on the test data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.899.1">X_test</span></strong><span class="koboSpan" id="kobo.900.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.901.1">predict()</span></strong><span class="koboSpan" id="kobo.902.1"> method. </span><span class="koboSpan" id="kobo.902.2">Predictions are stored </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.904.1">y_pred</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.906.1">
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_resampled, y_train_resampled)
y_pred = clf.predict(X_test)</span></pre></li> <li><span class="koboSpan" id="kobo.907.1">A classification report is generated using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.908.1">classification_report()</span></strong><span class="koboSpan" id="kobo.909.1"> function </span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.910.1">from scikit-learn. </span><span class="koboSpan" id="kobo.910.2">The</span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.911.1"> classification report provides precision, recall, F1-score, and support for each class (loan approval and rejection) based on predictions (</span><strong class="source-inline"><span class="koboSpan" id="kobo.912.1">y_pred</span></strong><span class="koboSpan" id="kobo.913.1">) and true labels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.914.1">y_test</span></strong><span class="koboSpan" id="kobo.915.1">) from the test set. </span><span class="koboSpan" id="kobo.915.2">The classification report is initially returned in a dictionary format. </span><span class="koboSpan" id="kobo.915.3">The code converts this dictionary to a </span><strong class="source-inline"><span class="koboSpan" id="kobo.916.1">clf_report</span></strong><span class="koboSpan" id="kobo.917.1"> DataFrame using </span><strong class="source-inline"><span class="koboSpan" id="kobo.918.1">pd.DataFrame()</span></strong><span class="koboSpan" id="kobo.919.1">, making it easier to work with the data. </span><span class="koboSpan" id="kobo.919.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.920.1">clf_report</span></strong><span class="koboSpan" id="kobo.921.1"> DataFrame is transposed using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.922.1">.T</span></strong><span class="koboSpan" id="kobo.923.1"> attribute to have classes (</span><strong class="source-inline"><span class="koboSpan" id="kobo.924.1">0</span></strong><span class="koboSpan" id="kobo.925.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.926.1">1</span></strong><span class="koboSpan" id="kobo.927.1">) as rows and evaluation metrics (precision, recall, F1-score, and support) as columns. </span><span class="koboSpan" id="kobo.927.2">This transposition provides a more convenient and readable format for further analysis </span><span class="No-Break"><span class="koboSpan" id="kobo.928.1">or presentation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.929.1">
clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))
clf_report = clf_report.T
clf_report</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.930.1">Let’s print the classification report to understand how good the </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">model is:</span></span></p></li> </ul>
<div>
<div class="IMG---Figure" id="_idContainer123">
<span class="koboSpan" id="kobo.932.1"><img alt="Figure 9.10 – Classification report generated by the preceding code" src="image/B19297_09_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.933.1">Figure 9.10 – Classification report generated by the preceding code</span></p>
<p><span class="koboSpan" id="kobo.934.1">The report</span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.935.1"> indicates that the model performs moderately well in identifying instances of class 0, with a precision of 73.33%. </span><span class="koboSpan" id="kobo.935.2">However, recall is relatively lower at 51.16%, indicating that the model might miss some actual instances of class 0. </span><span class="koboSpan" id="kobo.935.3">The model excels in identifying </span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.936.1">instances of class 1, with a high precision of 77.42% and a very high recall of 90.00%. </span><span class="koboSpan" id="kobo.936.2">The weighted average metrics consider the class imbalance, providing a balanced evaluation across both classes. </span><span class="koboSpan" id="kobo.936.3">The overall accuracy of the model is 76.42%, indicating the percentage of correctly </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">predicted instances.</span></span></p>
<p><span class="koboSpan" id="kobo.938.1">In the next section, let’s explore undersampling – another method to address class imbalance problems in </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">machine learning.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.940.1">Undersampling using RandomUnderSampler</span></h2>
<p><span class="koboSpan" id="kobo.941.1">Handling</span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.942.1"> class imbalance with </span><strong class="source-inline"><span class="koboSpan" id="kobo.943.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.944.1"> is an effective approach to address the challenge of imbalanced datasets, where one class significantly outweighs the other class(es). </span><span class="koboSpan" id="kobo.944.2">In such</span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.945.1"> cases, traditional machine learning algorithms may struggle to learn from the data and tend to be biased toward the majority class, leading to poor performance on the minority class or </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">rare events.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.948.1"> is a </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.949.1">resampling technique that aims to balance class distribution by randomly removing instances from the majority class until class proportions become more balanced. </span><span class="koboSpan" id="kobo.949.2">By reducing the number of instances in the majority class, </span><strong class="source-inline"><span class="koboSpan" id="kobo.950.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.951.1"> ensures that the minority class is represented more proportionately, making it easier for the model to detect and learn patterns related to </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">rare events.</span></span></p>
<p><span class="koboSpan" id="kobo.953.1">Here are some key</span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.954.1"> points about handling class imbalance </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.956.1">RandomUnderSampler</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.958.1">Resampling for class balance</span></strong><span class="koboSpan" id="kobo.959.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.960.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.961.1"> is a type of data-level resampling method. </span><span class="koboSpan" id="kobo.961.2">Data-level resampling techniques involve manipulating the training data to balance class distribution. </span><span class="koboSpan" id="kobo.961.3">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.962.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.963.1">, instances from the majority class are randomly selected and removed, resulting in a smaller dataset with a balanced </span><span class="No-Break"><span class="koboSpan" id="kobo.964.1">class distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.965.1">Preserving minority class information</span></strong><span class="koboSpan" id="kobo.966.1">: Unlike some other undersampling techniques that merge instances or create synthetic samples, </span><strong class="source-inline"><span class="koboSpan" id="kobo.967.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.968.1"> directly removes instances from the majority class without altering minority class instances. </span><span class="koboSpan" id="kobo.968.2">This approach helps preserve information from the minority class, making it easier for the model to focus on learning patterns associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">rare events.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.970.1">Potential information loss</span></strong><span class="koboSpan" id="kobo.971.1">: One potential drawback of </span><strong class="source-inline"><span class="koboSpan" id="kobo.972.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.973.1"> is the loss of information from the majority class. </span><span class="koboSpan" id="kobo.973.2">By removing </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.974.1">instances randomly, some informative instances may be discarded, potentially leading to a reduction in the model’s ability to generalize on the </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">majority class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.976.1">Computationally efficient</span></strong><span class="koboSpan" id="kobo.977.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.978.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.979.1"> is computationally efficient since it simply involves randomly removing instances from the majority class. </span><span class="koboSpan" id="kobo.979.2">This makes it faster compared to some other </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">resampling methods.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.981.1">Choosing the right resampling technique</span></strong><span class="koboSpan" id="kobo.982.1">: While </span><strong class="source-inline"><span class="koboSpan" id="kobo.983.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.984.1"> can be effective in certain scenarios, it might not always be the best choice, especially if the majority class contains important patterns and information. </span><span class="koboSpan" id="kobo.984.2">Careful consideration of the problem and dataset characteristics is crucial when selecting the appropriate </span><span class="No-Break"><span class="koboSpan" id="kobo.985.1">resampling technique.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.986.1">Combining with other techniques</span></strong><span class="koboSpan" id="kobo.987.1">: In practice, </span><strong class="source-inline"><span class="koboSpan" id="kobo.988.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.989.1"> can be used in combination with other techniques. </span><span class="koboSpan" id="kobo.989.2">For example, one can apply </span><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.991.1"> first</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.992.1"> and then use </span><strong class="source-inline"><span class="koboSpan" id="kobo.993.1">RandomOverSampler</span></strong><span class="koboSpan" id="kobo.994.1"> (oversampling) to further balance class distribution. </span><span class="koboSpan" id="kobo.994.2">This approach helps in achieving a more balanced representation of </span><span class="No-Break"><span class="koboSpan" id="kobo.995.1">both classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.996.1">Evaluation and model selection</span></strong><span class="koboSpan" id="kobo.997.1">: When handling class imbalance, it is essential to evaluate the model’s performance on relevant metrics such as precision, recall, F1-score, and </span><strong class="bold"><span class="koboSpan" id="kobo.998.1">Area Under the ROC Curve</span></strong><span class="koboSpan" id="kobo.999.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1000.1">AUC-ROC</span></strong><span class="koboSpan" id="kobo.1001.1">). </span><span class="koboSpan" id="kobo.1001.2">These </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.1002.1">metrics provide a comprehensive assessment of the model’s ability to handle rare events and </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">edge cases.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1004.1">Let’s implement</span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.1005.1"> this approach </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">using Python:</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.1007.1">Load data</span></strong><span class="koboSpan" id="kobo.1008.1">: The code begins by importing necessary libraries and loading the Loan Prediction dataset from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1009.1">train_loan_prediction.csv</span></strong><span class="koboSpan" id="kobo.1010.1"> file </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1012.1">pd.read_csv()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1014.1">
import pandas as pd
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
df = pd.read_csv('train_loan_prediction.csv')</span></pre> <p><span class="koboSpan" id="kobo.1015.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1016.1">'Loan_Status'</span></strong><span class="koboSpan" id="kobo.1017.1"> column in the dataset contains </span><strong class="source-inline"><span class="koboSpan" id="kobo.1018.1">'Y'</span></strong><span class="koboSpan" id="kobo.1019.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">'N'</span></strong><span class="koboSpan" id="kobo.1021.1"> categorical values, which </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.1022.1">represent loan approval (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">'Y'</span></strong><span class="koboSpan" id="kobo.1024.1">) and rejection (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">'N'</span></strong><span class="koboSpan" id="kobo.1026.1">). </span><span class="koboSpan" id="kobo.1026.2">To convert this categorical target variable into numerical format, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">'Y'</span></strong><span class="koboSpan" id="kobo.1028.1"> is mapped to 1 and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1029.1">'N'</span></strong><span class="koboSpan" id="kobo.1030.1"> is mapped to 0 using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1031.1">map()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1"> function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1033.1">
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})</span></pre> <p><span class="koboSpan" id="kobo.1034.1">The following code applies mean imputation to all columns with missing values in the dataset using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">fillna()</span></strong><span class="koboSpan" id="kobo.1036.1"> method. </span><span class="koboSpan" id="kobo.1036.2">This ensures that any missing values in the dataset are replaced with the mean value of their </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">respective columns:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1038.1">
df.fillna(df.mean(), inplace=True)</span></pre> <p><span class="koboSpan" id="kobo.1039.1">The </span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.1040.1">code selects only numerical columns from the dataset to build the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1041.1">X</span></strong><span class="koboSpan" id="kobo.1042.1"> feature set. </span><span class="koboSpan" id="kobo.1042.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1043.1">select_dtypes()</span></strong><span class="koboSpan" id="kobo.1044.1"> method is used to include only columns with data types </span><strong class="source-inline"><span class="koboSpan" id="kobo.1045.1">float</span></strong><span class="koboSpan" id="kobo.1046.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">int</span></strong><span class="koboSpan" id="kobo.1048.1"> data types while excluding non-numerical columns such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1049.1">'Loan_Status'</span></strong><span class="koboSpan" id="kobo.1050.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1051.1">'Loan_ID'</span></strong><span class="koboSpan" id="kobo.1052.1">. </span><span class="koboSpan" id="kobo.1052.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">y</span></strong><span class="koboSpan" id="kobo.1054.1"> target variable is set </span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1056.1">'Loan_Status'</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1058.1">
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']</span></pre> <p><span class="koboSpan" id="kobo.1059.1">The data is split into training (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">X_train</span></strong><span class="koboSpan" id="kobo.1061.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1062.1">y_train</span></strong><span class="koboSpan" id="kobo.1063.1">) and testing (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1064.1">X_test</span></strong><span class="koboSpan" id="kobo.1065.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1066.1">y_test</span></strong><span class="koboSpan" id="kobo.1067.1">) sets using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1068.1">train_test_split()</span></strong><span class="koboSpan" id="kobo.1069.1"> function from scikit-learn. </span><span class="koboSpan" id="kobo.1069.2">The training set consists of 80% of the data, while the testing set contains 20% of the data. </span><span class="koboSpan" id="kobo.1069.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">random_state</span></strong><span class="koboSpan" id="kobo.1071.1"> parameter is set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1072.1">42</span></strong><span class="koboSpan" id="kobo.1073.1"> to </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">ensure reproducibility:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1075.1">
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.1076.1">We</span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.1077.1"> then instantiate </span><strong class="source-inline"><span class="koboSpan" id="kobo.1078.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.1079.1"> and apply it to the training data using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">fit_resample()</span></strong><span class="koboSpan" id="kobo.1081.1"> method. </span><span class="koboSpan" id="kobo.1081.2">This method undersamples the majority class (loan approval) to create a balanced dataset. </span><span class="koboSpan" id="kobo.1081.3">The resulting resampled data is stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1082.1">X_train_resampled</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1083.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1084.1">y_train_resampled</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1086.1">
rus = RandomUnderSampler(random_state=42)
X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)</span></pre> <p><span class="koboSpan" id="kobo.1087.1">A Random Forest classifier is then trained on the resampled data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1088.1">X_train_resampled</span></strong><span class="koboSpan" id="kobo.1089.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1090.1">y_train_resampled</span></strong><span class="koboSpan" id="kobo.1091.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1092.1">fit()</span></strong><span class="koboSpan" id="kobo.1093.1"> method. </span><span class="koboSpan" id="kobo.1093.2">The trained classifier is used to make predictions on the test data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1094.1">X_test</span></strong><span class="koboSpan" id="kobo.1095.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1096.1">predict()</span></strong><span class="koboSpan" id="kobo.1097.1"> method. </span><span class="koboSpan" id="kobo.1097.2">Predictions are stored </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">in </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1099.1">y_pred</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1101.1">
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train_resampled, y_train_resampled)
y_pred = clf.predict(X_test)
clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))
clf_report = clf_report.T
clf_report</span></pre> <p><span class="koboSpan" id="kobo.1102.1">Let’s inspect </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.1103.1">the classification report to assess the </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">model performance:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<span class="koboSpan" id="kobo.1105.1"><img alt="Figure 9.11 – Classification report of the model’s performance" src="image/B19297_09_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1106.1">Figure 9.11 – Classification report of the model’s performance</span></p>
<p><span class="koboSpan" id="kobo.1107.1">The </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.1108.1">model shows decent performance for both classes, with higher precision, recall, and F1-score for class </span><strong class="source-inline"><span class="koboSpan" id="kobo.1109.1">1</span></strong><span class="koboSpan" id="kobo.1110.1"> compared to class </span><strong class="source-inline"><span class="koboSpan" id="kobo.1111.1">0</span></strong><span class="koboSpan" id="kobo.1112.1">.The weighted average considers the imbalance in class distribution, providing a more representative measure of overall performance. </span><span class="koboSpan" id="kobo.1112.2">The accuracy score of 0.7805% suggests that the model correctly predicted the class for approximately 78% of instances in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1113.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.1114.1">In the next section, let’s understand cost-sensitive learning and explore its crucial role in scenarios where rare events bear </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">significant consequences.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.1116.1">Cost-sensitive learning</span></h1>
<p><span class="koboSpan" id="kobo.1117.1">Cost-sensitive learning</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.1118.1"> is a machine learning approach that takes into account costs associated with misclassifications of different classes during the model training process. </span><span class="koboSpan" id="kobo.1118.2">In traditional machine learning, the focus is on maximizing overall accuracy, but in many real-world scenarios, misclassifying certain classes can have more severe consequences than </span><span class="No-Break"><span class="koboSpan" id="kobo.1119.1">misclassifying others.</span></span></p>
<p><span class="koboSpan" id="kobo.1120.1">For example, in a medical diagnosis application, misdiagnosing a severe disease as not present (false negative) could have more significant consequences than misdiagnosing a mild condition as present (false positive). </span><span class="koboSpan" id="kobo.1120.2">In fraud detection, incorrectly flagging a legitimate transaction as fraudulent (false positive) might inconvenience the customer, while failing to detect actual fraudulent transactions (false negative) could lead to significant </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">financial losses.</span></span></p>
<p><span class="koboSpan" id="kobo.1122.1">Cost-sensitive learning addresses these imbalances in costs by assigning different misclassification costs to different classes. </span><span class="koboSpan" id="kobo.1122.2">By incorporating these costs into the training process, the model is encouraged to prioritize minimizing the overall misclassification cost rather than simply </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">optimizing accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.1124.1">There are </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.1125.1">several approaches to implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">cost-sensitive learning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1127.1">Modifying loss functions</span></strong><span class="koboSpan" id="kobo.1128.1">: The loss function used during model training can be modified to incorporate class-specific misclassification costs. </span><span class="koboSpan" id="kobo.1128.2">The goal is to minimize the expected cost, which is a combination of misclassification costs and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">model’s predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1130.1">Class weights</span></strong><span class="koboSpan" id="kobo.1131.1">: Another approach is to assign higher weights to the minority class or the class with higher misclassification costs. </span><span class="koboSpan" id="kobo.1131.2">This technique can be applied to various classifiers, such as decision trees, random forests, and SVMs, to emphasize learning from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">minority class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1133.1">Sampling techniques</span></strong><span class="koboSpan" id="kobo.1134.1">: In addition to assigning weights, resampling techniques such as oversampling the minority class or undersampling the majority class can also be used to balance class distribution and improve the model’s ability to learn from </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">rare events.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1136.1">Threshold adjustment</span></strong><span class="koboSpan" id="kobo.1137.1">: By adjusting the classification threshold, we can control the trade-off between precision and recall, allowing us to make predictions that are more sensitive to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">minority class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1139.1">Ensemble methods</span></strong><span class="koboSpan" id="kobo.1140.1">: Ensemble methods such as cost-sensitive boosting combine multiple models to focus on hard-to-classify instances and assign higher weights to</span><a id="_idIndexMarker786"/> <span class="No-Break"><span class="koboSpan" id="kobo.1141.1">misclassified samples.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1142.1">Cost-sensitive learning is especially important in scenarios where the class imbalance is severe and the consequences of misclassification are critical. </span><span class="koboSpan" id="kobo.1142.2">By taking into account costs associated with different classes, the model can make more informed decisions and improve overall performance in detecting rare events and handling </span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">edge cases.</span></span></p>
<p><span class="koboSpan" id="kobo.1144.1">It is important to note that cost-sensitive learning requires careful consideration of the cost matrix, as incorrectly specified costs can lead to unintended results. </span><span class="koboSpan" id="kobo.1144.2">Proper validation and evaluation of the model on relevant metrics, considering real-world costs, are crucial to ensure the effectiveness and reliability of cost-sensitive </span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">learning algorithms.</span></span></p>
<p><span class="koboSpan" id="kobo.1146.1">Let’s now</span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.1147.1"> demonstrate cost-sensitive learning using the Loan Prediction dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.1148.1">in Python:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1149.1">Load the required libraries and datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1151.1">pandas</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1153.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
df = pd.read_csv('train_loan_prediction.csv')</span></pre></li> <li><span class="koboSpan" id="kobo.1154.1">We now need to perform data preprocessing to handle missing values and convert target variables to numeric </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">data types:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1156.1">
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})
df.fillna(df.mean(), inplace=True)</span></pre></li> <li><span class="koboSpan" id="kobo.1157.1">For this example, we will use only numeric columns. </span><span class="koboSpan" id="kobo.1157.2">We will then split the dataset into </span><strong class="source-inline"><span class="koboSpan" id="kobo.1158.1">train</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1159.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1160.1">test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1161.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1162.1">
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.1163.1">We</span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.1164.1"> first calculate the class weights based on the inverse of class frequencies in the training data. </span><span class="koboSpan" id="kobo.1164.2">The higher the frequency of a class, the lower its weight, and vice versa. </span><span class="koboSpan" id="kobo.1164.3">This way, the model assigns higher importance to the minority class (rare events) and is more sensitive to its </span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1">correct prediction:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1166.1">
class_weights = dict(1 / y_train.value_counts(normalize=True))</span></pre></li> <li><span class="koboSpan" id="kobo.1167.1">Next, we train the Random Forest classifier with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1168.1">class_weight</span></strong><span class="koboSpan" id="kobo.1169.1"> parameter set to the calculated class weights. </span><span class="koboSpan" id="kobo.1169.2">This modification allows the classifier to consider the class weights during the training process, effectively implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.1170.1">cost-sensitive learning:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1171.1">
clf = RandomForestClassifier(random_state=42, class_weight=class_weights)
clf.fit(X_train, y_train)</span></pre></li> <li><span class="koboSpan" id="kobo.1172.1">After training the model, we make predictions on the test data and evaluate the classifier’s performance using the classification report, which provides precision, recall, F1-score, and support for </span><span class="No-Break"><span class="koboSpan" id="kobo.1173.1">each class:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1174.1">
y_pred = clf.predict(X_test)
clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))
clf_report = clf_report.T
clf_report</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.1175.1">Let’s view the classification report and assess the Random Forest </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">classifier’s performance:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer125">
<span class="koboSpan" id="kobo.1177.1"><img alt="Figure 9.12 – Classification report of the Random Forest classifier’s performance" src="image/B19297_09_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1178.1">Figure 9.12 – Classification report of the Random Forest classifier’s performance</span></p>
<p><span class="koboSpan" id="kobo.1179.1">In </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.1180.1">cost-sensitive learning, you would typically define a cost matrix that quantifies misclassification costs for each class and use it to guide the model’s training. </span><span class="koboSpan" id="kobo.1180.2">The results from the classification report can help you identify areas where adjustments may be needed to align the model with specific cost considerations in your application. </span><span class="koboSpan" id="kobo.1180.3">A cost matrix is especially useful in situations where the costs of false positives and false negatives are not equal. </span><span class="koboSpan" id="kobo.1180.4">If the cost of false positives is higher, consider raising the decision threshold. </span><span class="koboSpan" id="kobo.1180.5">If the cost of false negatives is higher, consider lowering </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">the threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.1182.1">In the next section, let’s understand which evaluation metrics are used for detecting edge cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.1183.1">rare events.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.1184.1">Choosing evaluation metrics</span></h1>
<p><span class="koboSpan" id="kobo.1185.1">When </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.1186.1">dealing with edge cases and rare events in machine learning, selecting the right evaluation metrics is crucial to accurately assess the performance of the model. </span><span class="koboSpan" id="kobo.1186.2">Traditional evaluation metrics, such as accuracy, may not be sufficient in imbalanced datasets where the class of interest (the rare event) is vastly outnumbered by the majority class. </span><span class="koboSpan" id="kobo.1186.3">In imbalanced datasets, where the rare event is a minority class, traditional evaluation metrics such as accuracy can be misleading. </span><span class="koboSpan" id="kobo.1186.4">For instance, if a dataset has 99% of the majority class and only 1% of the rare event, a model that predicts all instances as the majority class will still achieve an accuracy of 99%, which is deceptively high. </span><span class="koboSpan" id="kobo.1186.5">However, such a model would be ineffective in detecting the rare event. </span><span class="koboSpan" id="kobo.1186.6">To </span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.1187.1">address this issue, we need evaluation metrics that focus on the model’s performance in correctly identifying the rare event, even at the expense of a decrease </span><span class="No-Break"><span class="koboSpan" id="kobo.1188.1">in accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.1189.1">Here are some evaluation metrics that are more suitable for detecting edge cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">rare events:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1191.1">Precision</span></strong><span class="koboSpan" id="kobo.1192.1">: Precision</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.1193.1"> measures the accuracy of positive predictions made by the model. </span><span class="koboSpan" id="kobo.1193.2">It is the ratio of true positive (correctly predicted rare event) to the sum of true positive and false positive (incorrectly predicted rare event as the majority class). </span><span class="koboSpan" id="kobo.1193.3">High precision indicates that the model is cautious in making positive predictions and has a low false </span><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">positive rate.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1195.1">Recall (sensitivity)</span></strong><span class="koboSpan" id="kobo.1196.1">: Recall </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.1197.1">measures the proportion of true positives predicted by the model out of all actual positive instances. </span><span class="koboSpan" id="kobo.1197.2">It is the ratio of true positive to the sum of true positive and false negative (incorrectly predicted majority class as the rare event). </span><span class="koboSpan" id="kobo.1197.3">High recall indicates that the model is capable of capturing a significant portion of rare </span><span class="No-Break"><span class="koboSpan" id="kobo.1198.1">event instances.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1199.1">F1-score</span></strong><span class="koboSpan" id="kobo.1200.1">: The</span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.1201.1"> F1-score is the harmonic mean of precision and recall. </span><span class="koboSpan" id="kobo.1201.2">It provides a balance between the two metrics and is especially useful when there is an imbalance between precision and recall. </span><span class="koboSpan" id="kobo.1201.3">F1-score penalizes models that prioritize either precision or recall at the expense of </span><span class="No-Break"><span class="koboSpan" id="kobo.1202.1">the other.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1203.1">Area Under the Receiver Operating Characteristic (ROC-AUC)</span></strong><span class="koboSpan" id="kobo.1204.1">: ROC-AUC is a</span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.1205.1"> performance metric used to evaluate binary classification models. </span><span class="koboSpan" id="kobo.1205.2">It measures the area under the ROC curve, which plots the true positive rate (recall) against the false positive rate as the classification threshold changes. </span><span class="koboSpan" id="kobo.1205.3">A higher ROC-AUC indicates better model performance, especially in detecting </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">rare events.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1207.1">In the next section, let’s delve into ensemble techniques and understand their crucial role in machine learning models, especially when dealing with data containing edge cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.1208.1">rare events.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.1209.1">Ensemble techniques</span></h1>
<p><span class="koboSpan" id="kobo.1210.1">Ensemble techniques</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.1211.1"> are powerful methods used to improve the performance of machine learning models, particularly in scenarios with imbalanced datasets, rare events, and edge cases. </span><span class="koboSpan" id="kobo.1211.2">These techniques combine multiple base models to create a more robust and accurate final prediction. </span><span class="koboSpan" id="kobo.1211.3">Let’s discuss some popular </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">ensemble techniques.</span></span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.1213.1">Bagging</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1214.1">Bootstrap aggregating</span></strong><span class="koboSpan" id="kobo.1215.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1216.1">bagging</span></strong><span class="koboSpan" id="kobo.1217.1">) is an ensemble</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.1218.1"> technique that creates multiple bootstrap </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.1219.1">samples (random subsets with replacement) from the training data and trains a separate base model on each sample. </span><span class="koboSpan" id="kobo.1219.2">The final prediction is obtained by averaging or voting the predictions of all base models. </span><span class="koboSpan" id="kobo.1219.3">Bagging is particularly useful when dealing with high variance and complex models, as it reduces overfitting and enhances the model’s generalization ability. </span><span class="koboSpan" id="kobo.1219.4">Here are the key concepts associated </span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">with bagging:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1221.1">Bootstrap sampling</span></strong><span class="koboSpan" id="kobo.1222.1">: The </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.1223.1">bagging process begins by creating multiple random subsets of the training data through a process called bootstrap sampling. </span><span class="koboSpan" id="kobo.1223.2">Bootstrap sampling involves randomly selecting data points from the original dataset with replacements. </span><span class="koboSpan" id="kobo.1223.3">As a result, some data points may appear more than once in a subset, while others may be </span><span class="No-Break"><span class="koboSpan" id="kobo.1224.1">left out.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1225.1">Base model training</span></strong><span class="koboSpan" id="kobo.1226.1">: For</span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.1227.1"> each bootstrap sample, a base model (learner) is trained independently on that particular subset of the training data. </span><span class="koboSpan" id="kobo.1227.2">The base models can be any machine learning algorithm, such as decision trees, random forests, </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">or SVMs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1229.1">Aggregating predictions</span></strong><span class="koboSpan" id="kobo.1230.1">: Once</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.1231.1"> all base models are trained, they are used to make predictions on new, unseen data. </span><span class="koboSpan" id="kobo.1231.2">For classification tasks, the final prediction is typically determined by majority voting, where the class that receives the most votes across the base models is chosen. </span><span class="koboSpan" id="kobo.1231.3">In regression tasks, the final prediction is obtained by averaging the predictions from all </span><span class="No-Break"><span class="koboSpan" id="kobo.1232.1">base models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1233.1">Here are a few benefits </span><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">of bagging:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1235.1">Variance reduction</span></strong><span class="koboSpan" id="kobo.1236.1">: Bagging</span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.1237.1"> helps reduce variance in the model by combining predictions from multiple models trained on different subsets of the data. </span><span class="koboSpan" id="kobo.1237.2">This results in a more stable and </span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1">robust model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1239.1">Overfitting prevention</span></strong><span class="koboSpan" id="kobo.1240.1">: By training each base model on different subsets of the data, bagging prevents individual models from overfitting to noise in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">training set.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1242.1">Model generalization</span></strong><span class="koboSpan" id="kobo.1243.1">: Bagging improves the model’s generalization ability by reducing bias and variance, leading to better performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.1244.1">unseen data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1245.1">Parallelism</span></strong><span class="koboSpan" id="kobo.1246.1">: Since the base models are trained independently, bagging is amenable to parallel processing, making it </span><span class="No-Break"><span class="koboSpan" id="kobo.1247.1">computationally efficient.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1248.1">Random Forest is a popular example of the bagging technique. </span><span class="koboSpan" id="kobo.1248.2">In Random Forest, the base models are decision trees, and the predictions from multiple decision trees are combined to make the </span><span class="No-Break"><span class="koboSpan" id="kobo.1249.1">final prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.1250.1">Here’s an example of</span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.1251.1"> implementing bagging using Random Forest in Python on the Loan </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">Prediction dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1253.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
df = pd.read_csv('train_loan_prediction.csv')
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})
df.fillna(df.mean(), inplace=True)
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))
clf_report = clf_report.T
clf_report</span></pre> <p><span class="koboSpan" id="kobo.1254.1">In conclusion, bagging </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.1255.1">techniques offer a robust and effective strategy for handling edge cases. </span><span class="koboSpan" id="kobo.1255.2">By aggregating predictions from multiple base models, bagging not only enhances the overall model stability but also fortifies its ability to accurately identify and address edge cases, contributing to a more resilient and reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.1256.1">predictive framework.</span></span></p>
<p><span class="koboSpan" id="kobo.1257.1">In the next section, we will explore boosting, another method to handle edge cases and </span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">rare events.</span></span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.1259.1">Boosting</span></h2>
<p><span class="koboSpan" id="kobo.1260.1">Boosting is </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.1261.1">an ensemble technique that builds base models </span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.1262.1">sequentially, with each subsequent model focusing on misclassified instances of the previous model. </span><span class="koboSpan" id="kobo.1262.2">It assigns higher weights to misclassified instances, thus giving more attention to rare events. </span><span class="koboSpan" id="kobo.1262.3">Popular boosting algorithms include </span><strong class="bold"><span class="koboSpan" id="kobo.1263.1">Adaptive Boosting</span></strong><span class="koboSpan" id="kobo.1264.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1265.1">AdaBoost</span></strong><span class="koboSpan" id="kobo.1266.1">), Gradient Boosting, and XGBoost. </span><span class="koboSpan" id="kobo.1266.2">Boosting aims to </span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.1267.1">create a strong learner by combining weak </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">learners iteratively.</span></span></p>
<p><span class="koboSpan" id="kobo.1269.1">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">boosting</span></span><span class="No-Break"><a id="_idIndexMarker808"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1"> works:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1272.1">Base model training</span></strong><span class="koboSpan" id="kobo.1273.1">: Boosting</span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.1274.1"> starts by training a base model (also known as a weak learner) on the entire training dataset. </span><span class="koboSpan" id="kobo.1274.2">Weak learners are usually simple models with limited predictive power, such as decision stumps (a decision tree with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">single split).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1276.1">Weighted training</span></strong><span class="koboSpan" id="kobo.1277.1">: After</span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.1278.1"> the first model is trained, data points that were misclassified by the model are assigned higher weights. </span><span class="koboSpan" id="kobo.1278.2">This means that the subsequent model will pay more attention to those misclassified data points, attempting to correct </span><span class="No-Break"><span class="koboSpan" id="kobo.1279.1">their predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1280.1">Iterative training</span></strong><span class="koboSpan" id="kobo.1281.1">: Boosting</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.1282.1"> follows an iterative approach. </span><span class="koboSpan" id="kobo.1282.2">For each iteration (or boosting round), a new weak learner is trained on the updated training data with adjusted weights. </span><span class="koboSpan" id="kobo.1282.3">The weak learners are then combined to create a strong learner, which improves its predictive performance compared to the individual </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">weak learners.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1284.1">Weighted voting</span></strong><span class="koboSpan" id="kobo.1285.1">: During </span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.1286.1">the final prediction, the weak learners’ predictions are combined with weighted voting, where models with higher accuracy have more influence on the final prediction. </span><span class="koboSpan" id="kobo.1286.2">This allows the boosting algorithm to focus on difficult-to-classify instances and improve the model’s sensitivity to </span><span class="No-Break"><span class="koboSpan" id="kobo.1287.1">rare events.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1288.1">The benefits</span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.1289.1"> of boosting are </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1291.1">Increased accuracy</span></strong><span class="koboSpan" id="kobo.1292.1">: Boosting improves the model’s accuracy by focusing on the most challenging instances in the dataset and refining predictions over </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">multiple iterations</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1294.1">Robustness</span></strong><span class="koboSpan" id="kobo.1295.1">: Boosting reduces the model’s sensitivity to noise and outliers in the data by iteratively adjusting weights and learning from </span><span class="No-Break"><span class="koboSpan" id="kobo.1296.1">previous mistakes</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1297.1">Model adaptation</span></strong><span class="koboSpan" id="kobo.1298.1">: Boosting adapts well to different types of data and can handle complex relationships between features and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1299.1">target variable</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1300.1">Ensemble diversity</span></strong><span class="koboSpan" id="kobo.1301.1">: Boosting creates a diverse ensemble of weak learners, which results in better </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.1302.1">generalization and </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">reduced overfitting</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1304.1">Let’s look at an example of boosting </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">using AdaBoost.</span></span></p>
<p><span class="koboSpan" id="kobo.1306.1">AdaBoost is a</span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.1307.1"> popular boosting algorithm that is commonly used in practice. </span><span class="koboSpan" id="kobo.1307.2">In AdaBoost, the base models are typically decision stumps, and the model’s weights are adjusted after each iteration to emphasize </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">misclassified instances.</span></span></p>
<p><span class="koboSpan" id="kobo.1309.1">Here’s an example of implementing boosting using AdaBoost </span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">in Python:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1311.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import classification_report
df = pd.read_csv('train_loan_prediction.csv')
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})
df.fillna(df.mean(), inplace=True)
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
clf = AdaBoostClassifier(random_state=42)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
clf_report = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True))
clf_report = clf_report.T
clf_report</span></pre> <p><span class="koboSpan" id="kobo.1312.1">In summary, the </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.1313.1">application of boosting techniques emerges as a robust strategy for handling edge cases. </span><span class="koboSpan" id="kobo.1313.2">Through its iterative approach, boosting empowers models to focus on instances that pose challenges, ultimately enhancing their ability to discern and accurately predict </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">rare events.</span></span></p>
<p><span class="koboSpan" id="kobo.1315.1">We will now explore how to use the stacking method to detect and handle edge cases and rare events in </span><span class="No-Break"><span class="koboSpan" id="kobo.1316.1">machine learning.</span></span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.1317.1">Stacking</span></h2>
<p><span class="koboSpan" id="kobo.1318.1">Stacking is</span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.1319.1"> an advanced ensemble learning technique</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.1320.1"> that combines the predictions of multiple base models by training a meta-model on their outputs. </span><span class="koboSpan" id="kobo.1320.2">Stacking aims to leverage the strengths of different base models to create a more accurate and robust final prediction. </span><span class="koboSpan" id="kobo.1320.3">It is a form of “learning to learn” where the meta-model learns how to best combine the predictions of the base models. </span><span class="koboSpan" id="kobo.1320.4">The base models act as “learners,” and their predictions become the input features for the meta-model, which makes the final prediction. </span><span class="koboSpan" id="kobo.1320.5">Stacking can often improve performance by capturing complementary patterns from different </span><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">base models.</span></span></p>
<p><span class="koboSpan" id="kobo.1322.1">Here is the </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">model </span></span><span class="No-Break"><a id="_idIndexMarker819"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">methodology:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1325.1">Base model training</span></strong><span class="koboSpan" id="kobo.1326.1">: The </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.1327.1">stacking process starts by training multiple diverse base models on the training dataset. </span><span class="koboSpan" id="kobo.1327.2">These base models can be different types of machine learning algorithms or even the same algorithm with </span><span class="No-Break"><span class="koboSpan" id="kobo.1328.1">different hyperparameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1329.1">Base model predictions</span></strong><span class="koboSpan" id="kobo.1330.1">: Once the base models are trained, they are used to make </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.1331.1">predictions on the same training data (in-sample predictions) or a separate validation dataset (</span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">out-of-sample predictions).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1333.1">Meta-model training</span></strong><span class="koboSpan" id="kobo.1334.1">: The </span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.1335.1">predictions from the base models </span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.1336.1">are then combined to create a new dataset that serves as the input for the meta-model. </span><span class="koboSpan" id="kobo.1336.2">Each base model’s predictions become a new feature in this dataset. </span><span class="koboSpan" id="kobo.1336.3">The meta-model is trained on this new dataset along with the true </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">target labels.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1338.1">Final prediction</span></strong><span class="koboSpan" id="kobo.1339.1">: During the</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.1340.1"> final prediction phase, the base models make predictions on the new, unseen data. </span><span class="koboSpan" id="kobo.1340.2">These predictions are then used as input features for the meta-model, which makes the </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">final prediction.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1342.1">Stacking has the</span><a id="_idIndexMarker825"/> <span class="No-Break"><span class="koboSpan" id="kobo.1343.1">following benefits:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1344.1">Improved predictive performance</span></strong><span class="koboSpan" id="kobo.1345.1">: Stacking leverages the complementary strengths of different base models, potentially leading to better overall predictive performance compared to using </span><span class="No-Break"><span class="koboSpan" id="kobo.1346.1">individual models</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1347.1">Reduction of bias and variance</span></strong><span class="koboSpan" id="kobo.1348.1">: Stacking can reduce the model’s bias and variance by combining multiple models, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.1349.1">improved generalization</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1350.1">Flexibility</span></strong><span class="koboSpan" id="kobo.1351.1">: Stacking allows the use of diverse base models, making it suitable for various types of data </span><span class="No-Break"><span class="koboSpan" id="kobo.1352.1">and problems</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1353.1">Ensemble diversity</span></strong><span class="koboSpan" id="kobo.1354.1">: Stacking creates a diverse ensemble by using various base models, which can help </span><span class="No-Break"><span class="koboSpan" id="kobo.1355.1">prevent overfitting</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1356.1">Here’s an</span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.1357.1"> example of implementing stacking using scikit-learn in Python using the Loan </span><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">Prediction dataset:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1359.1">We first import the required libraries and load </span><span class="No-Break"><span class="koboSpan" id="kobo.1360.1">the dataset:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1361.1">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
df = pd.read_csv('train_loan_prediction.csv')</span></pre></li> <li><span class="koboSpan" id="kobo.1362.1">We now</span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.1363.1"> need to perform data preprocessing to handle missing values and convert target variables to numeric </span><span class="No-Break"><span class="koboSpan" id="kobo.1364.1">data types:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1365.1">
df['Loan_Status'] = df['Loan_Status'].map({'Y': 1, 'N': 0})
df.fillna(df.mean(), inplace=True)</span></pre></li> <li><span class="koboSpan" id="kobo.1366.1">For simplicity, we will use only numeric columns for this example. </span><span class="koboSpan" id="kobo.1366.2">We then split the dataset into </span><strong class="source-inline"><span class="koboSpan" id="kobo.1367.1">train</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1368.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1369.1">test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1370.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1371.1">
numerical_columns = df.select_dtypes(include=[float, int]).columns
X = df[numerical_columns].drop('Loan_Status', axis=1)
y = df['Loan_Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1372.1">Instantiate and train base models</span></strong><span class="koboSpan" id="kobo.1373.1">: Two base models, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1374.1">RandomForestClassifier</span></strong><span class="koboSpan" id="kobo.1375.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1376.1">GradientBoostingClassifier</span></strong><span class="koboSpan" id="kobo.1377.1">, are instantiated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1378.1">RandomForestClassifier(random_state=42)</span></strong><span class="koboSpan" id="kobo.1379.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1380.1">GradientBoostingClassifier(random_state=42)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1381.1"> respectively:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1382.1">
base_model_1 = RandomForestClassifier(random_state=42)
base_model_2 = GradientBoostingClassifier(random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.1383.1">These base models are trained on the training data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1384.1">X_train</span></strong><span class="koboSpan" id="kobo.1385.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1386.1">y_train</span></strong><span class="koboSpan" id="kobo.1387.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1388.1">fit()</span></strong><span class="koboSpan" id="kobo.1389.1"> method, as seen next. </span><span class="koboSpan" id="kobo.1389.2">The trained base models are used to make predictions on the test data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1390.1">X_test</span></strong><span class="koboSpan" id="kobo.1391.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1392.1">predict()</span></strong><span class="koboSpan" id="kobo.1393.1"> method. </span><span class="koboSpan" id="kobo.1393.2">Predictions from both base models are stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1394.1">pred_base_model_1</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1395.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1396.1">pred_base_model_2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1398.1">
base_model_1.fit(X_train, y_train)
base_model_2.fit(X_train, y_train)
pred_base_model_1 = base_model_1.predict(X_test)
pred_base_model_2 = base_model_2.predict(X_test)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1399.1">Create a new dataset for stacking</span></strong><span class="koboSpan" id="kobo.1400.1">: A new </span><strong class="source-inline"><span class="koboSpan" id="kobo.1401.1">stacking_X_train</span></strong><span class="koboSpan" id="kobo.1402.1"> dataset is created by combining the predictions from the base models (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1403.1">pred_base_model_1</span></strong><span class="koboSpan" id="kobo.1404.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1405.1">pred_base_model_2</span></strong><span class="koboSpan" id="kobo.1406.1">). </span><span class="koboSpan" id="kobo.1406.2">This new dataset will be used as input features for </span><span class="No-Break"><span class="koboSpan" id="kobo.1407.1">the meta-model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1408.1">
stacking_X_train = pd.DataFrame({
    'BaseModel1': pred_base_model_1,
    'BaseModel2': pred_base_model_2
})</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1409.1">Instantiate and train meta-model</span></strong><span class="koboSpan" id="kobo.1410.1">: A meta-model (Logistic Regression, in this case) is instantiated with </span><strong class="source-inline"><span class="koboSpan" id="kobo.1411.1">LogisticRegression()</span></strong><span class="koboSpan" id="kobo.1412.1">. </span><span class="koboSpan" id="kobo.1412.2">The meta-model is trained on</span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.1413.1"> the new dataset (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1414.1">stacking_X_train</span></strong><span class="koboSpan" id="kobo.1415.1">) and the true labels from the test set (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1416.1">y_test</span></strong><span class="koboSpan" id="kobo.1417.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1418.1">fit()</span></strong><span class="koboSpan" id="kobo.1419.1"> method. </span><span class="koboSpan" id="kobo.1419.2">The meta-model learns to combine predictions of the base models and make the </span><span class="No-Break"><span class="koboSpan" id="kobo.1420.1">final prediction:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1421.1">
meta_model = LogisticRegression()
meta_model.fit(stacking_X_train, y_test)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1422.1">Create unseen data for demonstration and make predictions</span></strong><span class="koboSpan" id="kobo.1423.1">: For demonstration purposes, a new sample of unseen data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1424.1">new_unseen_data</span></strong><span class="koboSpan" id="kobo.1425.1">) is created by randomly selecting 20% of the test data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1426.1">X_test</span></strong><span class="koboSpan" id="kobo.1427.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1428.1">sample()</span></strong><span class="koboSpan" id="kobo.1429.1"> method. </span><span class="koboSpan" id="kobo.1429.2">The base models are used to make predictions on the new, unseen data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1430.1">new_unseen_data</span></strong><span class="koboSpan" id="kobo.1431.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1432.1">predict()</span></strong><span class="koboSpan" id="kobo.1433.1"> method. </span><span class="koboSpan" id="kobo.1433.2">Predictions from both base models for the new data are stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1434.1">new_pred_base_model_1</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1435.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1436.1">new_pred_base_model_2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1437.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1438.1">
new_unseen_data = X_test.sample(frac=0.2, random_state=42)
new_pred_base_model_1 = base_model_1.predict(new_unseen_data)
new_pred_base_model_2 = base_model_2.predict(new_unseen_data)</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1439.1">Create a new dataset for stacking with unseen data</span></strong><span class="koboSpan" id="kobo.1440.1">: A new </span><strong class="source-inline"><span class="koboSpan" id="kobo.1441.1">stacking_new_unseen_data</span></strong><span class="koboSpan" id="kobo.1442.1"> dataset is created by combining predictions from the base models (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1443.1">new_pred_base_model_1</span></strong><span class="koboSpan" id="kobo.1444.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1445.1">new_pred_base_model_2</span></strong><span class="koboSpan" id="kobo.1446.1">) for the new, unseen data. </span><span class="koboSpan" id="kobo.1446.2">This new dataset will be used </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.1447.1">as input features for the meta-model to make the </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">final prediction:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1449.1">
stacking_new_unseen_data = pd.DataFrame({
    'BaseModel1': new_pred_base_model_1,
    'BaseModel2': new_pred_base_model_2
})</span></pre></li> <li><strong class="bold"><span class="koboSpan" id="kobo.1450.1">Make final prediction using meta-model</span></strong><span class="koboSpan" id="kobo.1451.1">: The meta-model (Logistic Regression) is used to make the final prediction on the new, unseen data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1452.1">stacking_new_unseen_data</span></strong><span class="koboSpan" id="kobo.1453.1">) using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1454.1">predict()</span></strong><span class="koboSpan" id="kobo.1455.1"> method. </span><span class="koboSpan" id="kobo.1455.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1456.1">final_prediction</span></strong><span class="koboSpan" id="kobo.1457.1"> variable holds the predicted classes (0 or 1) based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.1458.1">meta-model’s decision:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1459.1">
final_prediction = meta_model.predict(stacking_new_unseen_data)
final_prediction</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.1460.1">In summary, this code demonstrates the concept of stacking, where base models (Random Forest and Gradient Boosting) are trained on the original data, their predictions are used as input features for a meta-model (Logistic Regression), and the final prediction is made using the meta-model on new, unseen data. </span><span class="koboSpan" id="kobo.1460.2">Stacking allows the models to work together and can potentially improve the prediction performance compared to using the base </span><span class="No-Break"><span class="koboSpan" id="kobo.1461.1">models alone.</span></span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.1462.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1463.1">In this chapter, we explored the critical aspect of detecting rare events and edge cases in machine learning. </span><span class="koboSpan" id="kobo.1463.2">Rare events, by their infrequency, hold significant implications across various domains and necessitate special attention. </span><span class="koboSpan" id="kobo.1463.3">We delved into several techniques and methodologies that equip us to effectively identify and handle these </span><span class="No-Break"><span class="koboSpan" id="kobo.1464.1">uncommon occurrences.</span></span></p>
<p><span class="koboSpan" id="kobo.1465.1">Statistical methods, such as Z-scores and IQR, provide powerful tools to pinpoint outliers and anomalies in our data. </span><span class="koboSpan" id="kobo.1465.2">These methods aid in establishing meaningful thresholds for identifying rare events, enabling us to distinguish significant data points </span><span class="No-Break"><span class="koboSpan" id="kobo.1466.1">from noise.</span></span></p>
<p><span class="koboSpan" id="kobo.1467.1">We also explored machine learning-based anomaly detection techniques, such as isolation forest and autoencoders. </span><span class="koboSpan" id="kobo.1467.2">These methods leverage unsupervised learning to identify patterns and deviations that diverge from the majority of the data, making them well suited for detecting rare events in </span><span class="No-Break"><span class="koboSpan" id="kobo.1468.1">complex datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1469.1">Additionally, we discussed the significance of resampling methods such as SMOTE and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1470.1">RandomUnderSampler</span></strong><span class="koboSpan" id="kobo.1471.1"> to tackle class imbalances. </span><span class="koboSpan" id="kobo.1471.2">These techniques enable us to create balanced datasets that enhance the performance of models in identifying rare events while preserving </span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">data integrity.</span></span></p>
<p><span class="koboSpan" id="kobo.1473.1">Furthermore, we uncovered the potential of ensemble techniques, including stacking, bagging, and boosting, in augmenting the capabilities of our models for detecting edge cases. </span><span class="koboSpan" id="kobo.1473.2">The combined power of multiple models through ensembles enhances generalization and </span><span class="No-Break"><span class="koboSpan" id="kobo.1474.1">model robustness.</span></span></p>
<p><span class="koboSpan" id="kobo.1475.1">It is crucial to select appropriate evaluation metrics, especially in the presence of rare events, to ensure fair assessment and accurate model performance evaluation. </span><span class="koboSpan" id="kobo.1475.2">Metrics such as precision, recall, F1-score, and AUC-ROC provide comprehensive insights into model performance and </span><span class="No-Break"><span class="koboSpan" id="kobo.1476.1">guide decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.1477.1">Detecting rare events and edge cases has far-reaching implications across diverse domains, including medical diagnosis, fraud detection, predictive maintenance, and environmental monitoring. </span><span class="koboSpan" id="kobo.1477.2">By employing effective techniques to identify and handle these infrequent occurrences, we enhance the reliability and efficiency of machine </span><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">learning applications.</span></span></p>
<p><span class="koboSpan" id="kobo.1479.1">As we conclude this chapter, let us recognize the significance of this skill in real-world scenarios. </span><span class="koboSpan" id="kobo.1479.2">Detecting rare events empowers us to make informed decisions, protect against potential risks, and leverage the full potential of machine learning to drive positive impact in a multitude </span><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">of fields.</span></span></p>
<p><span class="koboSpan" id="kobo.1481.1">In the next chapter, we will explore some of the challenges faced by the data-centric approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">machine learning.</span></span></p>
</div>


<div class="Content" id="_idContainer127">
<h1 id="_idParaDest-155" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.1.1">Part 4: Getting Started with Data-Centric ML</span></h1>
<p><span class="koboSpan" id="kobo.2.1">By now you may have realized that shifting to a data-centric approach to ML involves not just adapting your own ways of working, but also influencing those around you – a task that’s far from simple. </span><span class="koboSpan" id="kobo.2.2">In this part, we explore both the technical and non-technical hurdles you might encounter during the development and deployment of models, and reveal how adopting a data-centric approach can aid in overcoming </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">these obstacles.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapter:</span></span></p>
<ul>
<li><a href="B19297_10.xhtml#_idTextAnchor164"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 10</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Kick-Starting Your Journey in Data-Centric Machine Learning</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer128">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer129">
</div>
</div>
</body></html>