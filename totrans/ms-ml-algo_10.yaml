- en: Advanced Neural Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级神经网络模型
- en: 'In this chapter, we continue our pragmatic exploration of the world of deep
    learning, analyzing two very important elements: deep convolutional networks and
    **recurrent neural networks** (**RNN**). The former represents the most accurate
    and best performing visual processing technique for almost any purpose. Results
    like the ones obtained in fields such as real-time image recognition, self-driving
    cars, and Deep Reinforcement Learning have been possible thanks to the expressivity
    of this kind of network. On the other hand, in order to fully manage the temporal
    dimension, it is necessary to introduce advanced recurrent layers, whose performance
    must be greater than any other regression method. Employing these two techniques
    together with all the elements already discussed in the previous chapter makes
    it possible to achieve extraordinary results in the field of video processing,
    decoding, segmentation, and generation.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续对深度学习世界的实用探索，分析两个非常重要的元素：深度卷积网络和**循环神经网络**（**RNN**）。前者代表了几乎所有目的的最准确和性能最好的视觉处理技术。在实时图像识别、自动驾驶汽车和深度强化学习等领域取得的成果都得益于这种网络的表达能力。另一方面，为了完全管理时间维度，有必要引入高级循环层，其性能必须优于任何其他回归方法。将这两种技术与上一章中讨论的所有元素结合起来，使得在视频处理、解码、分割和生成领域取得非凡的结果成为可能。
- en: 'In particular, in this chapter, we are going to discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在本章中，我们将讨论以下主题：
- en: Deep convolutional networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积网络
- en: Convolutions, atrous convolutions, separable convolutions, and transpose convolutions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积、扩张卷积、可分离卷积和转置卷积
- en: Pooling and other support layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化和其他支持层
- en: Recurrent neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: LSTM and GRU cells
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM和GRU细胞
- en: Transfer learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Deep convolutional networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积网络
- en: In the previous chapter, [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural
    Networks for Machine Learning* we have seen how a multi-layer perceptron can achieve
    a very high accuracy when working with an complex image dataset that is not very
    complex, such as the MNIST handwritten digits one. However, as the fully-connected
    layers are *horizontal*, the images, which in general are three-dimensional structures
    (*width × height × channels*), must be flattened and transformed into one-dimensional
    arrays where the geometric properties are definitively lost. With more complex
    datasets, where the distinction between classes depends on more details and on
    their relationships, this approach can yield moderate accuracies, but it can never
    reach the precision required by production-ready applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，[第9章](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml)，《机器学习的神经网络》中，我们看到了多层感知器如何在与复杂但不是非常复杂的图像数据集（如MNIST手写数字数据集）一起工作时达到非常高的准确率。然而，由于全连接层是**水平**的，图像，通常是一维结构（**宽度×高度×通道**），必须被展平并转换成一维数组，其中几何属性最终丢失。对于更复杂的数据集，其中类别的区分依赖于更多的细节和它们之间的关系，这种方法可以产生适度的准确率，但它永远无法达到生产就绪应用所需的精度。
- en: 'The conjunction of neuroscientific studies and image processing techniques
    suggested experimenting with neural networks where the first layers work with
    bidimensional structures (without the channels), trying to extract a hierarchy
    of features that are strictly dependent on the geometric properties of the image.
    In fact, as confirmed by neuroscientific research about the visual cortex, a human
    being doesn''t decode an image directly. The process is sequential and starts by
    detecting low-level elements such as lines are orientations; progressively, it
    proceeds by focusing on sub-properties that define more and more complex shapes,
    different colors, structural features, and so on, until the amount of information
    is enough to resolve any possible ambiguity (for further scientific details, I
    recommend the book *Vision and Brain: How We Perceive the World, Stone J. V.,
    MIT Press*).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经科学研究和图像处理技术的结合建议在神经网络中尝试，其中第一层处理二维结构（没有通道），试图提取严格依赖于图像几何属性的特征层次。事实上，正如关于视觉皮层的神经科学研究所证实的那样，人类不会直接解码图像。这个过程是连续的，首先通过检测低级元素，如线条和方向；然后，它通过关注定义越来越复杂形状、不同颜色、结构特征等的子属性来逐步进行，直到信息量足够解决任何可能的歧义（关于更详细的科学信息，我推荐阅读书籍《视觉与大脑：我们如何感知世界》，作者J.
    V. Stone，MIT出版社）。
- en: 'For example, we can image the decoding process of an eye as a sequence made
    up of these filters (of course, this is only a didactic example): directions (dominant
    horizontal dimension), a central circle inside an ellipsoidal shape, a darker
    center (pupil) and a clear background (bulb), a smaller darker circle in the middle
    of the pupil, the presence of eyebrows, and so on. Even if the process is not
    biologically correct, it can be considered as a reasonable hierarchical process
    where a higher level sub-feature is obtained after a lower-level filtering.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将眼睛的解码过程想象成一个由这些滤波器组成的序列（当然，这只是一个教学示例）：方向（主导水平维度）、一个椭圆形状内部的中心圆圈、一个较暗的中心（瞳孔）和一个清晰的背景（灯泡），瞳孔中间的一个较小的较暗圆圈，眉毛的存在，等等。即使这个过程在生物学上不正确，它也可以被认为是一个合理的分层过程，其中在较低级别的滤波之后获得了一个较高级别的子特征。
- en: 'This approach has been synthesized using the bidimensional convolutional operator,
    which was already known as a powerful image processing tool. However, in this
    case, there''s a very important difference: the structure of the filters is not
    pre-imposed but learned by the network using the same back-propagation algorithm
    employed for MLPs. In this way, the model can adapt the weights considering a
    final goal (which is the classification output), without taking into account any
    pre-processing steps. In fact, a deep convolutional network, more than an MLP,
    is based on the concept of end-to-end learning, which is a different way to express
    what we have described before. The input is the source; in the middle, there''s
    a flexible structure; and, at the end, we define a global cost function, measuring
    the accuracy of the classification. The learning process has to back-propagate
    the errors and correct the weights to reach a specific goal, but we don''t know
    exactly how this process works. What we can easily do is analyze the structure
    of the filters at the end of the learning phase, discovering that the network
    has specialized the first layers on low-level details (such as orientations) and
    the last ones on high-level, sometimes recognizable, ones (such as the components
    of a face). It''s not surprising that such models achieved state-of-the-art performance
    in tasks such as image recognition, segmentation (detecting the boundaries of
    different parts composing an image), and tracking (detecting the position of moving
    objects). Nevertheless, deep convolutional networks have become the first block
    of many different architectures (such as deep reinforcement learning or neural
    style transfer) and, even with a few known limitations, continue to be the first
    choice for solving several complex real-life problems. The main drawback of such
    models (which is also a common objection) is that they require very large datasets
    to reach high accuracies. All the most important models are trained with millions
    of images and their generalization ability (that is, the main goal) is proportional
    to the number of different samples. There were researchers who noticed that a
    human being learns to generalize without this huge amount of experience and, in
    the coming decades, we are likely to observe improvements under this viewpoint.
    However, deep convolutional networks have revolutionized many Artificial Intelligence
    fields, allowing results that were considered almost impossible just a few years
    ago.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是通过使用二维卷积算子综合而成的，它已经是一个众所周知的有力图像处理工具。然而，在这种情况下，有一个非常重要的区别：滤波器的结构不是预先设定的，而是由网络通过用于
    MLP 的相同反向传播算法学习。这样，模型可以适应权重，考虑一个最终目标（即分类输出），而不考虑任何预处理步骤。实际上，深度卷积网络比 MLP 更基于端到端学习的概念，这是表达我们之前描述内容的一种不同方式。输入是源数据；在中间，有一个灵活的结构；在最后，我们定义一个全局成本函数，衡量分类的准确性。学习过程必须反向传播错误并修正权重以达到特定目标，但我们并不确切知道这个过程是如何工作的。我们能够轻松做到的是分析学习阶段结束时的滤波器结构，发现网络已经将第一层专门化于低级细节（如方向），而将最后几层专门化于高级、有时可识别的细节（如面部组件）。这样的模型在图像识别、分割（检测图像不同部分的边界）和跟踪（检测移动物体的位置）等任务上取得了最先进的性能，这并不令人惊讶。然而，深度卷积网络已成为许多不同架构（如深度强化学习或神经风格迁移）的第一块，即使存在一些已知的局限性，但仍然是解决许多复杂现实问题的首选。这种模型的主要缺点（也是一个常见的反对意见）是它们需要非常大的数据集才能达到高精度。所有最重要的模型都是用数百万张图片训练的，它们的泛化能力（即主要目标）与不同样本的数量成正比。有研究人员注意到，人类在没有这种大量经验的情况下学会泛化，在未来的几十年里，我们可能会在这个观点下观察到改进。然而，深度卷积网络已经彻底改变了许多人工智能领域，使得几年前被认为几乎不可能的结果成为可能。
- en: In this section, we are going to discuss different kinds of convolutions and
    how they can be implemented using Keras; therefore, for specific technical details
    I continue suggesting to check the official documentation and the book *Deep Learning
    with Keras, Gulli A, Pal S., Packt*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论不同类型的卷积以及如何使用 Keras 来实现它们；因此，对于具体的技术细节，我继续建议查看官方文档和书籍《深度学习与 Keras，作者：Gulli
    A，Pal S.，Packt 出版》。
- en: Convolutions
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: 'Even if we work only with finite and discrete convolutions, it''s useful to
    start providing the standard definition based on integrable functions. For simplicity,
    let''s suppose that *f(τ)* and *k(τ)* are two real functions of a single variable
    defined in *ℜ*. The convolution of *f(τ)* and *k(τ)* (conventionally denoted as
    *f ∗ k*), which we are going to call kernel, is defined as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们在有限和离散卷积中工作，也很有必要从基于可积函数的标准定义开始。为了简单起见，让我们假设 *f(τ)* 和 *k(τ)* 是在 *ℜ* 中定义的单变量两个实函数。*f(τ)*
    和 *k(τ)* 的卷积（通常表示为 *f ∗ k*），我们将其称为核，定义为以下：
- en: '![](img/afdf56cb-4be5-4a1b-ae5c-b189cb096e04.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/afdf56cb-4be5-4a1b-ae5c-b189cb096e04.png)'
- en: 'The expression may not be very easy to understand without a mathematical background,
    but it can become exceptionally simple with a few considerations. First of all,
    the integral sums over all values of *τ*; therefore, the convolution is a function
    of the remaining variable, *t*. The second fundamental element is a sort of dynamic
    property: the kernel is reversed (*-**τ*) and transformed into a function of a
    new variable *z = t - τ*. Without deep mathematical knowledge, it''s possible
    to understand that this operation shifts the function along the *τ* (independent
    variable) axis. In the following graphs, there''s an example based on a parabola:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数学背景，这个表达式可能不太容易理解，但通过一些考虑，它可以变得特别简单。首先，积分覆盖了所有 *τ* 的值；因此，卷积是剩余变量 *t* 的函数。第二个基本元素是一种动态属性：核被反转（*-τ*）并转换为新变量
    *z = t - τ* 的函数。没有深厚的数学知识，也可以理解这个操作沿着 *τ*（独立变量）轴移动函数。在下面的图中，有一个基于抛物线的例子：
- en: '![](img/17860d83-4962-4dfb-8c8c-e33af0dbdc24.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17860d83-4962-4dfb-8c8c-e33af0dbdc24.png)'
- en: The first diagram is the original kernel (which is also symmetric). The other
    two plots show, respectively, a forward and a backward shift. It should be clearer
    now that a convolution multiplies the function *f(τ**)* times the shifted kernel
    and computes the area under the resulting curve. As the variable *t* is not integrated,
    the area is a function of *t* and defines a new function, which is the convolution
    itself. In other words, the value of convolution of *f(τ)* and *k(τ)* computed
    for *t = 5* is the area under the curve obtained by the multiplication *f(τ)k(5
    - τ)*. By definition, a convolution is commutative *(f ∗ k = k ∗ f)* and distributive
    *(f ∗ (k + g) = (f ∗ k) + (f ∗ g))*. Moreover, it's also possible to prove that
    it's associative *(f ∗ (k ∗ g) = (f ∗ k) ∗ g)*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图是原始核（它也是对称的）。其他两个图分别显示了前向和后向位移。现在应该更清楚，卷积是函数 *f(τ)* 乘以位移核，并计算结果曲线下的面积。由于变量
    *t* 没有积分，面积是 *t* 的函数，并定义了一个新函数，即卷积本身。换句话说，当 *t = 5* 时，*f(τ)* 和 *k(τ)* 的卷积值是乘积
    *f(τ)k(5 - τ)* 获得的曲线下的面积。根据定义，卷积是可交换的 *(f ∗ k = k ∗ f)* 和分配的 *(f ∗ (k + g) = (f ∗
    k) + (f ∗ g))*. 此外，还可以证明它是结合的 *(f ∗ (k ∗ g) = (f ∗ k) ∗ g)*。
- en: However, in deep learning, we never work with continuous convolutions; therefore,
    I omit all the properties and mathematical details, focusing the attention on
    the discrete case. The reader who is interested in the theory can find further
    details in *Circuits, Signals, and Systems, Siebert W. M., MIT Press*. A common
    practice is, instead, to stack multiple convolutions with different kernels (often
    called filters), to transform an input containing *n* channels into an output
    with *m* channels, where *m* corresponds to the number of kernels. This approach
    allows the unleashing of the full power of convolutions, thanks to the synergic
    actions of different outputs. Conventionally, the output of a convolution layer
    with *n* filters is called a **feature map** (*w^((t)) × h^((t)) × n*), because
    its structure is no longer related to a specific image but resembles the overlap
    of different feature detectors. In this chapter, we often talk about images (considering
    a hypothetical first layer), but all the considerations are implicitly extended
    to any feature map.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深度学习中，我们从不处理连续卷积；因此，我省略了所有属性和数学细节，专注于离散情况。对理论感兴趣的读者可以在 *Circuits, Signals,
    and Systems, Siebert W. M., MIT Press* 中找到更多细节。相反，一种常见的做法是将多个具有不同核（通常称为滤波器）的卷积堆叠起来，将包含
    *n* 通道的输入转换为具有 *m* 通道的输出，其中 *m* 对应于核的数量。这种方法可以通过不同输出的协同作用释放卷积的全部力量。通常，具有 *n* 个滤波器的卷积层的输出被称为**特征图**（*w^((t))
    × h^((t)) × n*），因为其结构不再与特定图像相关，而更像不同特征检测器的重叠。在本章中，我们经常谈论图像（考虑一个假设的第一层），但所有考虑都是隐含地扩展到任何特征图。
- en: Bidimensional discrete convolutions
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二维离散卷积
- en: 'The most common type of convolution employed in deep learning is based on bidimensional
    arrays with any number of channels (such as grayscale or RGB images). For simplicity,
    let''s analyze a single layer (channel) convolution because the extension to *n*
    layers is straightforward. If *X ∈ ℜ^(w × h)* and *k ∈ ℜ^(n × m)*, the convolution
    *X ∗ k* is defined as (the indexes start from 0):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中应用最广泛的卷积类型是基于具有任意数量通道的二维数组（如灰度图或RGB图像）。为了简化，让我们分析单层（通道）卷积，因为扩展到*n*层是直接的。如果*X∈ℜ^(w×h)*
    和 *k∈ℜ^(n×m)*，卷积 *X∗k* 定义为（索引从0开始）：
- en: '![](img/122c8b29-9cee-4e07-97f6-15a9949ddc58.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/122c8b29-9cee-4e07-97f6-15a9949ddc58.png)'
- en: 'It''s clear that the previous expression is a natural derivation of the continuous
    definition. In the following graph, there''s an example with a 3 × 3 kernel:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，前面的表达式是连续定义的自然推导。在下面的图中，有一个使用3×3核的示例：
- en: '![](img/745e26a6-355c-4895-96be-d34f88cc1ad9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/745e26a6-355c-4895-96be-d34f88cc1ad9.png)'
- en: Example of bidimensional convolution with a 3x3 kernel
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3核的二维卷积示例
- en: 'The kernel is shifted horizontally and vertically, yielding the sum of the
    element-wise multiplication of corresponding elements. Therefore, every operation
    leads to the output of a single pixel. The kernel employed in the example is called
    the **discrete Laplacian** **operator** (because it''s obtained by discretizing
    the real Laplacian); let''s observe the effect of this kernel on a complete greyscale
    diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 核在水平和垂直方向上移动，产生对应元素逐元素乘积的总和。因此，每个操作都导致单个像素的输出。示例中使用的核被称为**离散拉普拉斯** **算子**（因为它是由离散化实拉普拉斯得到的）；让我们观察这个核对完整灰度图的影响：
- en: '![](img/84f68b67-d2b8-47c5-993a-8f58baca3ef5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/84f68b67-d2b8-47c5-993a-8f58baca3ef5.png)'
- en: Example of convolution with a Discrete Laplacian Kernel
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用离散拉普拉斯核的卷积示例
- en: 'As it''s possible to notice, the effect of the convolution is to emphasize
    the borders of the various shapes. The reader can now understand how variable
    kernels can be tuned up in order to fulfill precise requirements. However, instead
    of trying to do it manually, a deep convolutional network leaves this tasks to
    the learning process, which is subject to a precise goal expressed as the minimization
    of a cost function. A parallel application of different filters yields complex
    overlaps that can simplify the extraction of those features that are really important
    for a classification. The main difference between a fully-connected layer and
    a convolutional one is the ability of the latter to work with an existing geometry,
    which encodes all the elements needed to distinguish an object from another one.
    These elements cannot be immediately generalizable (think about the branches of
    a decision tree, where a split defines a precise path towards a final class),
    but require subsequent processing steps to perform a necessary disambiguation.
    Considering the previous photo, for example, eyes and nose are rather similar.
    How is it possible to segment the picture correctly? The answer is provided by
    a double analysis: there are subtle differences that can be discovered by fine-grained
    filters and, above all, the global geometry of real objects is based on internal
    relationships that are almost invariant. For example (only for didactic purposes),
    eyes and nose should make up an isosceles triangle, because the symmetry of a
    face implies the same distance between each eye and the nose. This consideration
    can be made *apriori*, like in many visual processing techniques, or, thanks to
    the power of deep learning, it can be left to the training process. As the cost
    function and the output classes implicitly control the differences, a deep convolutional
    network can learn what is important to reach a specific goal, discarding at the
    same time all those details that are useless.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所注意到的，卷积的效果是强调各种形状的边缘。读者现在可以理解如何调整可变核以满足精确的要求。然而，与其手动尝试，深度卷积网络将这项任务留给学习过程，该过程受一个精确目标所控制，该目标以最小化成本函数的形式表达。不同滤波器的并行应用会产生复杂的重叠，这可以简化提取那些对分类真正重要的特征。全连接层与卷积层的主要区别在于后者能够处理现有的几何形状，这些几何形状编码了区分一个对象与另一个对象所需的所有元素。这些元素不能立即推广（想想决策树的分支，其中一次分割定义了通向最终类别的精确路径），但需要后续的处理步骤来执行必要的区分。以之前的照片为例，例如，眼睛和鼻子相当相似。如何正确分割图片呢？答案是双重分析：可以通过细粒度滤波器发现细微的差异，更重要的是，真实物体的全局几何形状基于几乎不变的内部关系。例如（仅用于教学目的），眼睛和鼻子应该组成一个等腰三角形，因为面部的对称性意味着每只眼睛与鼻子的距离相同。这种考虑可以像许多视觉处理技术一样*事先*进行，或者，多亏了深度学习的力量，它可以留给训练过程。由于成本函数和输出类别隐式控制差异，深度卷积网络可以学习达到特定目标所需的重要信息，同时丢弃所有无用的细节。
- en: In the previous section, we have said that the feature extraction process is
    mainly hierarchical. Now, it should be clear that different kernel sizes and subsequent
    convolutions achieve exactly this objective. Let's suppose that we have a *100 ×
    100* image and a (*3 × 3*) kernel. The resulting image will be *98 × 98* pixels
    (we will explain this concept later). However, each pixel encodes the information
    of a *3 × 3* block and, as these blocks are overlapping, two consecutive pixels
    will share some knowledge but, at the same time, they emphasize the difference
    between the corresponding blocks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们说过特征提取过程主要是层次化的。现在，应该清楚不同核大小和后续卷积正好达到这个目标。假设我们有一个 *100 × 100* 的图像和一个
    (*3 × 3*) 的核。结果图像将是 *98 × 98* 像素（我们稍后会解释这个概念）。然而，每个像素编码了一个 *3 × 3* 块的信息，由于这些块是重叠的，连续的两个像素将共享一些知识，但同时也强调了对应块之间的差异。
- en: 'In the following diagram, the same Laplacian Kernel is applied to a simple
    white square on a black background:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，相同的拉普拉斯核应用于黑色背景上的简单白色方块：
- en: '![](img/0bdaea9e-da79-41c1-9e16-aeac602803cd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0bdaea9e-da79-41c1-9e16-aeac602803cd.png)'
- en: Orginal image (left); convolution with Laplacian kernel result (right)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像（左）；使用拉普拉斯核的卷积结果（右）
- en: 'Even if the image is very simple, it''s possible to notice that the result
    of a convolution enriched the output image with some very important pieces of
    information: the borders of the square are now clearly visible (they are black
    and white) and they can be immediately detected by thresholding the image. The
    reason is straightforward: the effect of the kernel on the compact surfaces is
    compact too but, when the kernel is shifted upon the border, the effect of the
    difference becomes visible. Three adjacent pixels in the original image can be
    represented as (*0, 1, 1*), indicating the horizontal transition between black
    and white. After the convolution, the result is approximately (*0.75, 0.0, 0.25*).
    All the original black pixels have been transformed into a light gray, the white
    square became darker, and the border (which is not marked in the original picture)
    is now black (or white, depending on the shift direction). Reapplying the same
    filter to the output of the previous convolution, we obtain the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即使图像非常简单，也可能注意到卷积的结果丰富了输出图像，增加了一些非常重要的信息片段：正方形的边缘现在清晰可见（它们是黑白相间的），并且可以通过对图像进行阈值处理立即检测到。原因很简单：核在紧凑表面上的效果也是紧凑的，但当核在边缘上移动时，差异的效果变得可见。原始图像中的三个相邻像素可以表示为（*0,
    1, 1*），表示黑白之间的水平过渡。经过卷积后，结果大约为（*0.75, 0.0, 0.25*）。所有原始的黑色像素都变成了浅灰色，白色正方形变暗了，而边缘（在原始图片中没有标记）现在变成了黑色（或白色，取决于移动方向）。将相同的过滤器重新应用于前一次卷积的输出，我们得到以下结果：
- en: '![](img/40f86477-69d4-4e53-966c-a26b30dba51a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/40f86477-69d4-4e53-966c-a26b30dba51a.png)'
- en: Second application of the Laplacian kernel
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯核的第二次应用
- en: 'A sharp eye can immediately notice three results: the compact surfaces (black
    and white) are becoming more and more similar, the borders are still visible,
    and, above all, the top and lower left corners are now more clearly marked with
    white pixels. Therefore, the result of the second convolution added a finer-grained
    piece of information, which was much more difficult to detect in the original
    image. Indeed, the effect of the Laplacian operator is very straightforward and
    it''s useful only for didactic purposes. In real deep convolutional networks,
    the filters are trained to perform more complex processing operations that can
    reveal details (together with their internal and external relationships) that
    are not immediately exploited to classify the image. Their isolation (obtained
    thanks to the effect of many parallel filters) allows the network to mark similar
    elements (like the corners of the square) in a different way and make more accurate
    decisions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 留意观察可以立即发现三个结果：紧凑的表面（黑白）越来越相似，边缘仍然可见，最重要的是，顶部和左下角的白色像素现在更加清晰。因此，第二次卷积的结果增加了一个更细粒度的信息片段，这在原始图像中很难检测到。实际上，拉普拉斯算子的效果非常直接，它只对教学目的有用。在真实的深度卷积网络中，过滤器被训练执行更复杂的处理操作，这些操作可以揭示细节（包括它们的内部和外部关系），这些细节没有被立即用于图像分类。它们的隔离（得益于许多并行过滤器的效果）允许网络以不同的方式标记相似元素（如正方形的角），并做出更准确的决策。
- en: The purpose of this example is to show how a sequence of convolutions allows
    the generation of a hierarchical process that will extract coarse-grained features
    at the beginning and very high-level ones at the end, without losing the information
    already collected. Metaphorically, we could say that a deep convolutional network
    starts placing labels indicating lines, orientations, and borders and proceeds
    by enriching the existing ontology with further details (such as corners, particular
    shapes, and so on). Thanks to this ability, such models can easily outperform
    any MLP and reach almost to the Bayes level if the number of training samples
    is large enough. The main drawback of this models is their inability to easily
    recognize objects after the application of affine transformations (such as rotations
    or translations). In other words, if a network is trained with a dataset containing
    only faces in their natural position, it will achieve poor performance when a
    rotated (or upside-down) sample is presented. In the next sections, we are going
    to discuss a couple of methods that are helpful for mitigating this problem (in
    the case of translations); however, a new experimental architecture called a **capsule
    network** (which is beyond the scope of this book) has been proposed in order
    to solve this problem with a slightly different and much more robust approach
    (the reader can find further details in *Dynamic Routing Between Capsules, Sabour
    S., Frosst N., Hinton G. E., arXiv:1710.09829 [cs.CV]*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的目的是展示一系列卷积如何生成一个层次化的过程，该过程在开始时提取粗粒度特征，在结束时提取非常高级的特征，同时不丢失已收集的信息。比喻地说，我们可以认为深度卷积网络开始放置表示线条、方向和边界的标签，并通过添加更多细节（如角、特定形状等）来丰富现有的本体。多亏了这种能力，这样的模型可以轻易地超越任何MLP，并在训练样本数量足够大时几乎达到贝叶斯水平。这种模型的主要缺点是它们在应用仿射变换（如旋转或平移）后难以识别对象。换句话说，如果一个网络是在只包含自然位置的面部数据集上训练的，那么当呈现旋转（或颠倒）的样本时，它将表现不佳。在接下来的章节中，我们将讨论几种有助于减轻这种问题的方法（在平移的情况下）；然而，一种名为**胶囊网络**的新实验架构（超出了本书的范围）已被提出，以略微不同的方式解决此问题（读者可以在*Sabour
    S.，Frosst N.，Hinton G. E.，arXiv:1710.09829 [cs.CV]*中找到更多细节）。
- en: Strides and padding
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步长和填充
- en: Two important parameters common to all convolutions are **padding** and **strides**.
    Let's consider the bidimensional case, but keep in mind that the concepts are
    always the same. When a kernel (*n × m* with *n, m > 1*) is shifted upon an image
    and it arrives at the end of a dimension, there are two possibilities. The first
    one, called **valid padding**, consists of not continuing even if the resulting
    image is smaller than the original. In particular, if *X* is a *w × h* matrix,
    the resulting convolution output will have dimensions equal to *(w - n + 1) ×
    (h - m + 1)*. However, there are many cases when it's useful to keep the original
    dimensions, for example, to be able to sum different outputs. This approach is
    called **same padding** and it's based on the simple idea to add *n - 1* blank
    columns and *m - 1* blank rows to allow the kernel to shift over the original
    image, yielding a number of pixels equal to the initial dimensions. In many implementations,
    the default value is set to valid padding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有卷积都共有的两个重要参数是**填充**和**步长**。让我们考虑二维情况，但请记住，概念始终相同。当一个核(*n × m*，其中*n, m > 1*)在图像上移动并到达一个维度的末端时，有两种可能性。第一种，称为**有效填充**，是指即使结果图像小于原始图像，也不继续进行。特别是，如果*X*是一个*w
    × h*矩阵，那么最终的卷积输出将具有*(w - n + 1) × (h - m + 1)*的尺寸。然而，有许多情况下保持原始尺寸是有用的，例如，能够对不同的输出进行求和。这种方法称为**相同填充**，它基于简单地向原始图像添加*n
    - 1*空白列和*m - 1*空白行的想法，以便允许核在原始图像上移动，从而产生与初始尺寸相等的像素数。在许多实现中，默认值设置为有效填充。
- en: 'The other parameter, called **strides**, defines the number of pixels to skip
    during each shift. For example, a value set to (*1, 1*) corresponds to a standard
    convolution, while strides set to (*2, 1*) are shown in the following diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个参数，称为**步长**，定义了每次平移时跳过的像素数。例如，设置为(*1, 1*)的值对应于标准的卷积，而步长设置为(*2, 1*)的情况在以下图中展示：
- en: '![](img/bc335fd3-8153-4689-be9f-ecec0a4cfcd2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bc335fd3-8153-4689-be9f-ecec0a4cfcd2.png)'
- en: Example of bidimensional convolution with strides=2 on the x-axis
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: x轴步长为2的二维卷积示例
- en: In this case, every horizontal shift skips a pixel. Larger strides force a dimensionality
    reduction when a high granularity is not necessary (for example, in the first
    layers), while strides set to (*1, 1*) are normally employed in the last layers
    to capture smaller details. There are no standard rules to find out the optimal
    value and testing different configurations is always the best approach. Like any
    other hyperparameter, too many elements should be taken into account when determining
    whether a choice is acceptable or not; however, some general pieces of information
    about the dataset (and therefore about the underlying data generating process)
    can help in making a reasonable initial decision. For example, if we are working
    with pictures of buildings whose dimension is vertical, it's possible to start
    picking a value of (*1, 2*), because we can assume that there's more informative
    redundancy in the *y*-axis than in the *x*-axis. This choice can dramatically
    speed up the training process, as the output has one dimension, which is half
    (with the same padding) of the original one. In this way, larger strides produce
    a partial denoising and can improve the training speed. At the same time, the
    information loss could have a negative impact on the accuracy. If that happens,
    it probably means that the scale isn't high enough to allow skipping some elements
    without compromising the *semantics*. For example, an image with very small faces
    could be irreversibly *damaged* with large strides, yielding an inability to detect
    the right feature and a consequent worsening of the classification accuracy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每次水平移动都会跳过一个像素。较大的步长在不需要高粒度时（例如，在第一层）会强制进行维度降低，而将步长设置为（*1, 1*）通常用于最后一层以捕捉更小的细节。没有标准规则来确定最佳值，测试不同的配置始终是最好的方法。像任何其他超参数一样，在确定一个选择是否可接受时，应该考虑太多因素；然而，关于数据集（以及因此关于底层数据生成过程）的一些一般信息可以帮助做出合理的初始决策。例如，如果我们正在处理建筑物图片，其维度是垂直的，我们可以开始选择一个值为（*1,
    2*）的值，因为我们可以假设在*y*-轴上的信息冗余比在*x*-轴上更多。这个选择可以显著加快训练过程，因为输出有一个维度，是原始维度的一半（具有相同的填充）。这样，较大的步长会产生部分去噪并可以提高训练速度。同时，信息损失可能会对准确性产生负面影响。如果发生这种情况，可能意味着尺度不够高，无法跳过一些元素而不损害*语义*。例如，具有非常小脸部的图像可能会因为大步长而被不可逆地*损坏*，导致无法检测到正确的特征，从而降低分类准确性。
- en: Atrous convolution
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空洞卷积
- en: 'In some cases, a stride larger than one could be a good solution because it
    reduces the dimensionality and speeds up the training process, but it can lead
    to distorted images where the main features are not detectable anymore. An alternative
    approach is provided by the **atrous convolution** (also known as **dilated convolution**).
    In this case, the kernel is applied to a larger image patch, but skips some pixels
    inside the area itself (that''s why someone called it convolution with holes).
    In the following graph, there''s an example with (*3 × 3*) and dilation rate set
    to *2*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，大于一的步长可能是一个好的解决方案，因为它可以减少维度并加快训练过程，但它可能导致图像变形，其中主要特征不再可检测。**空洞卷积**（也称为**膨胀卷积**）提供了一种替代方法。在这种情况下，核应用于更大的图像块，但跳过该区域内部的一些像素（这就是为什么有人称之为带孔卷积）。在下面的图中，有一个（*3 ×
    3*）和膨胀率设置为*2*的示例：
- en: '![](img/c326924a-c20e-400f-b3ab-0e210a2a3940.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c326924a-c20e-400f-b3ab-0e210a2a3940.png)'
- en: Example of atrous convolution with a Laplacian kernel
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 带有拉普拉斯核的空洞卷积示例
- en: Every patch is now *9 **× 9*, but the kernel remains a *3 × 3* Laplacian operator.
    The effect of this approach is more robust than increasing the strides because
    the kernel *perimeter* will always contain a group of pixels with the same geometrical
    relationships. Of course, fine-grained features could be distorted, but as the
    strides are normally set to (*1, 1*), the final result is normally more coherent.
    The main difference with a standard convolution is that in this case, we are assuming
    that farther elements can be taken into account to determine the nature of an
    output pixel. For example, if the main features don't contain very small details,
    an atrous convolution can consider larger areas, focusing directly on elements
    that a standard convolution can detect only after several operations. The choice
    of this technique must be made considering the final accuracy, but just like for
    the strides, it can be considered from the beginning whenever the geometric properties
    can be detected more efficiently, considering larger patches with a few representative
    elements. Even if this method can be very effective in particular contexts, it
    isn't normally the first choice for very deep models. In the most important image
    classification models, standard convolutions (with or without larger strides)
    are employed because they have been proven to yield the best performance with
    very generic datasets (such as ImageNet or Microsoft Coco). However, I suggest
    the reader experiment with this method and compare the results. In particular,
    it would be a good idea to analyze which classes are better classified and try
    to find a rational explanation for the observed behavior.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每个补丁现在是 *9 **× 9*，但核仍然是一个 *3 × 3* 拉普拉斯算子。这种方法的效果比增加步长更稳健，因为核的 *周界* 将始终包含具有相同几何关系的像素组。当然，细粒度特征可能会扭曲，但通常步长设置为
    (*1, 1*)，最终结果通常更连贯。与标准卷积的主要区别在于，在这种情况下，我们假设可以考虑到更远的元素来确定输出像素的性质。例如，如果主要特征不包含非常小的细节，扩张卷积可以考虑到更大的区域，直接关注标准卷积需要经过多次操作才能检测到的元素。这种技术的选择必须考虑到最终的准确性，但就像步长一样，只要几何属性可以更有效地检测，就可以从一开始就考虑使用较大的补丁和一些代表性元素。即使这种方法在特定情况下可能非常有效，但它通常不是非常深层模型的首选。在最重要的图像分类模型中，使用标准卷积（带或不带更大的步长）是因为它们已被证明在非常通用的数据集（如ImageNet或Microsoft
    Coco）上产生最佳性能。然而，我建议读者尝试这种方法并比较结果。特别是，分析哪些类别被更好地分类，并尝试为观察到的行为找到合理的解释是个好主意。
- en: In some frameworks, such as Keras, there are no explicit layers to define an
    atrous convolution. Instead, a standard convolutional layer normally has a parameter
    to define the dilation rate (in Keras, it's called `dilation_rate`). Of course,
    the default value is 1, meaning that the kernel will be applied to patches matching
    its size.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些框架中，例如Keras，没有显式的层来定义一个扩张卷积。相反，标准的卷积层通常有一个参数来定义扩张率（在Keras中，它被称为 `dilation_rate`）。当然，默认值是1，这意味着核将应用于与其大小匹配的补丁。
- en: Separable convolution
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可分离卷积
- en: 'If we consider an image *X ∈ ℜ^(w × h)* (single channel) and a kernel *k ∈ ℜ**^(n
    × m)*, the number of operations is *nmwh*. When the kernel is not very small and
    the image is large, the cost of this computation can be quite high, even with
    GPU support. An improvement can be achieved by taking into account the associated
    property of convolutions. In particular, if the original kernel can be split into
    the dot product of two vectorial kernels, *k^((1))* with dimensions (*n × 1*)
    and *k^((2))* with dimensions (*1 × m*), the convolution is said to be **separable**.
    This means that we can perform a (*n **× m*) convolution with two subsequent operations:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑一个图像 *X ∈ ℜ^(w × h)*（单通道）和一个核 *k ∈ ℜ**^(n × m)*，所需的操作数是 *nmwh*。当核不是非常小且图像很大时，即使有GPU支持，这种计算的代价也可能相当高。通过考虑卷积的相关属性，我们可以实现改进。特别是，如果原始核可以分解为两个向量核的点积，即
    *k^((1))*（维度为 *n × 1*）和 *k^((2))*（维度为 *1 × m*），则称卷积为 **可分离的**。这意味着我们可以通过两个后续操作执行
    (*n **× m*) 卷积：
- en: '![](img/f804d5ab-40c8-4045-bcaf-c28d1f76566b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f804d5ab-40c8-4045-bcaf-c28d1f76566b.png)'
- en: The advantage is clear, because now the number of operations is *(n + m)wh*.
    In particular, when *nm >> n + m*, it's possible to avoid a large number of multiplications
    and speed up both the training and the prediction process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 优势是明显的，因为现在操作数是 *(n + m)wh*。特别是当 *nm >> n + m* 时，可以避免大量乘法，并加快训练和预测过程。
- en: 'A slightly different approach has been proposed in *Xception: Deep Learning
    with Depthwise Separable Convolutions, Chollet F., arXiv:1610.02357 [cs.CV]*.
    In this case, which is properly called **depthwise separable convolution**, the
    process is split into two steps. The first one operates along the channel axis,
    transforming it into a single dimensional map with a variable number of channels
    (for example, if the original diagram is *768 × 1024 × 3*, the output of the first
    stage will be *n × 768 × 1024 **× 1*). Then, a standard convolution is applied
    to the single layer (which can have indeed more than one channel). In the majority
    of implementations, the default number of output channels for the depthwise convolution
    is 1 (this is conventionally expressed by saying that the **depth multiplier**
    is 1). This approach allows a dramatic parameter reduction with respect to a standard
    convolution. In fact, if the input generic feature map is *X ∈ ℜ^(w × h × p)* and
    we want to perform a standard convolution with *q* kernels *k^((i)) ∈ ℜ^(n × m)*,
    we need to learn *nmqp* parameters (each kernel *k^((i))* is applied to all input
    channels). Employing the Depthwise Separable Convolution, the first step (working
    with only the channels) requires *nmp* parameters. As the output has still *p*
    feature maps and we need to output *q* channels, the process employs a *trick*: processing
    each feature map with *q 1 × 1* kernels (in this way, the output will have *q*
    layers and the same dimensions). The number of parameters required for the second
    step is *pq*, so the total number of parameters becomes *nmp + pq*. Comparing
    this value with the one required for a standard convolution, we obtain an interesting
    result:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在*《Xception：深度学习与深度可分离卷积》*（Chollet F.，arXiv:1610.02357 [cs.CV]）中提出了一个略有不同的方法。在这种情况下，这被称为**深度可分离卷积**，过程分为两个步骤。第一个步骤沿着通道轴操作，将其转换为一个具有可变数量通道的单维映射（例如，如果原始图是*768
    × 1024 × 3*，则第一阶段的输出将是*n × 768 × 1024 × 1*）。然后，对单层应用标准卷积（实际上可以有多于一个通道）。在大多数实现中，深度卷积的默认输出通道数是1（这通常通过说**深度乘数**为1来表示）。这种方法与标准卷积相比，可以实现参数的显著减少。实际上，如果输入通用特征图是*X
    ∈ ℜ^(w × h × p)*，我们想要执行带有*q*核*k^((i)) ∈ ℜ^(n × m)*的标准卷积，我们需要学习*nmqp*个参数（每个核*k^((i))*应用于所有输入通道）。采用深度可分离卷积，第一步（仅处理通道）需要*nmp*个参数。由于输出仍然有*p*个特征图，我们需要输出*q*个通道，这个过程使用了一个*技巧*：使用*q
    1 × 1*核处理每个特征图（这样，输出将有*q*层和相同的维度）。第二步所需的参数数是*pq*，因此总参数数变为*nmp + pq*。将这个值与标准卷积所需的参数数进行比较，我们得到一个有趣的结果：
- en: '![](img/82c2ffc6-4b01-480f-b748-b47768c4643d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![转置卷积图片](img/82c2ffc6-4b01-480f-b748-b47768c4643d.png)'
- en: As this condition is easily true, this approach is extremely effective in optimizing
    the training and prediction processes, as well as the memory consumption in any
    scenario. It's not surprising that the Xception model has been immediately implemented
    in mobile devices, allowing real-time image classification with very limited resources.
    Of course, depthwise separable convolutions don't always have the same accuracy
    as standard ones, because they are based on the assumption that the geometrical
    features observable inside a channel of a composite feature map are independent
    of each other. This is not always true, because we know that the effect of multiple
    layers is based also on their combinations (which increases the expressivity of
    a network). However, in many cases the final result has an accuracy comparable
    to some state-of-the-art models; therefore, this technique can very often be considered
    as a valid alternative to a standard convolution.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个条件很容易成立，这种方法在优化训练和预测过程以及在任何场景下的内存消耗方面都极为有效。Xception模型能够立即在移动设备上实现，允许使用非常有限的资源进行实时图像分类，这并不令人惊讶。当然，深度可分离卷积并不总是与标准卷积具有相同的精度，因为它们基于这样的假设：复合特征图通道内可观察到的几何特征是相互独立的。这并不总是正确的，因为我们知道多层的效果也基于它们的组合（这增加了网络的表达能力）。然而，在许多情况下，最终结果与一些最先进的模型具有可比的精度；因此，这种技术通常可以被视为标准卷积的有效替代方案。
- en: Since version 2.1.5, Keras has introduced a layer called `DepthwiseConv2D` that
    implements a depthwise separable convolution. This layer extends the existing
    `SeparableConv2D`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 自从2.1.5版本以来，Keras引入了一个名为`DepthwiseConv2D`的层，该层实现了深度可分离卷积。这个层扩展了现有的`SeparableConv2D`。
- en: Transpose convolution
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置卷积
- en: A **transpose convolution** (sometimes wrongly called deconvolution, even if
    the mathematical definition is different) is not very different from a standard
    convolution, but its goal is to rebuild a structure with the same features as
    the input sample. Let's suppose that the output of a convolutional network is
    the feature map *X ∈ ℜ^(w' × h' × p)* and we need to build an output element *Y ∈ ℜ^(w ×
    h × 3)* (assuming the w and h are the original dimensions). We can achieve this
    result by applying a transpose convolution with appropriate strides and padding
    to *X*. For example, let's suppose that *X ∈ ℜ^(128 × 128 × 256)* and our output
    must be *512 × 512 × 3*. The last transpose convolution must learn three filters
    with strides set to four and same padding. We are going to see some practical
    examples of this method in the next chapter [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml),
    *Autoencoders* when discussing autoencoders; however, there are no very important
    differences between transpose and standard convolution in terms of internal dynamics.
    The main difference is the cost function, because when a transpose convolution
    is used as the last layer, the comparison must be done between a target image
    and a reconstructed one. In the next chapter,  [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml), *Autoencoders* 
    we are also going to analyze some techniques to improve the quality of the output
    even when the cost function doesn't focus on specific areas of the image.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**转置卷积**（有时错误地称为反卷积，即使数学定义不同）与标准卷积并没有很大区别，但其目标是重建一个与输入样本具有相同特征的结构。假设卷积神经网络的输出是特征图
    *X ∈ ℜ^(w'' × h'' × p)*，我们需要构建一个输出元素 *Y ∈ ℜ^(w × h × 3)*（假设 w 和 h 是原始维度）。我们可以通过在
    *X* 上应用适当的步长和填充的转置卷积来实现这个结果。例如，假设 *X ∈ ℜ^(128 × 128 × 256)*，我们的输出必须是 *512 × 512
    × 3*。最后一个转置卷积必须学习三个滤波器，步长设置为四，且使用相同的填充。我们将在下一章（[第11章](8022fb07-a741-4a0e-a965-6678577d3563.xhtml)，*自编码器*）中看到这个方法的一些实际例子，当讨论自编码器时；然而，在内部动态方面，转置卷积和标准卷积之间并没有非常重要的区别。主要区别在于损失函数，因为当转置卷积作为最后一层使用时，比较必须在目标图像和重建图像之间进行。在下一章（[第11章](8022fb07-a741-4a0e-a965-6678577d3563.xhtml)，*自编码器*）中，我们还将分析一些技术，以提高输出质量，即使损失函数没有专注于图像的特定区域。'
- en: Pooling layers
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'In a deep convolutional network, **pooling layers** are extremely useful elements.
    There are mainly two kinds of these structures: **max pooling** and **average
    pooling**. They both work on patches *p ∈ ℜ^(n × m)*, shifting horizontally and
    vertically according to the predefined stride value and transforming the patches
    into single pixels according to the following rules:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度卷积网络中，**池化层**是极其有用的元素。主要有两种这样的结构：**最大池化**和**平均池化**。它们都在 *p ∈ ℜ^(n × m)* 的块上工作，根据预定义的步长值水平垂直移动，并根据以下规则将块转换为单个像素：
- en: '![](img/df1b03d7-3268-41ad-bf78-7174796f2eda.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df1b03d7-3268-41ad-bf78-7174796f2eda.png)'
- en: There are two main reasons that justify the use of these layers. The first one
    is a dimensionality reduction with limited information loss (for example, setting
    the strides to (*2, 2*), it's possible to halve the dimensions of an image/feature
    map). Clearly, all pooling techniques can be more or less lossy (in particular
    max pooling) and the specific result depends on the single image. In general,
    pooling layers try to summarize the information contained in a small chunk into
    a single pixel. This idea is supported by a perceptual-oriented approach; in fact,
    when the pools are not too large, it's rather unlikely to find high variances
    in subsequent shifts (natural images have very few isolated pixels). Therefore,
    all the pooling operations allow us to set up strides greater than one with a
    mitigated risk of compromising the information content. However, considering several
    experiments and architectures, I suggest that you set up larger strides in the
    convolutional layers (in particular, in the first layer of a convolutional sequence)
    instead of in pooling ones. In this way, it's possible to apply the transformation
    with a minimum loss and to fully exploit the next fundamental property.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个主要原因可以证明使用这些层是合理的。第一个原因是通过有限的损失进行降维（例如，将步长设置为(*2, 2*)，可以将图像/特征图的维度减半）。显然，所有池化技术或多或少都有损失（特别是最大池化），具体结果取决于单个图像。一般来说，池化层试图将一小块信息中的信息总结到一个像素中。这一想法得到了以感知为导向的方法的支持；事实上，当池子不太大时，在后续的偏移中找到高方差的可能性相当低（自然图像中很少有孤立像素）。因此，所有池化操作都允许我们设置大于一的步长，同时降低损害信息内容的风险。然而，考虑到几个实验和架构，我建议你在卷积层中设置更大的步长（特别是在卷积序列的第一层），而不是在池化层中。这样，就可以以最小的损失应用变换并充分利用下一个基本属性。
- en: 'The second (and probably the most important) reason is that they slightly increase
    the robustness to translations and limited distortions with an effect that is
    proportional to the pool size. Let''s consider the following diagram, representing
    an original image of a cross and the version after a 10-pixel diagonal translation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个（也许是最重要的）原因是，它们略微增加了对平移和有限扭曲的鲁棒性，其效果与池大小成比例。让我们考虑以下图表，表示一个十字的原始图像和经过10像素对角线平移后的版本：
- en: '![](img/58f9f3ec-b9c2-4faa-8c2a-cf383b8ddb2b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58f9f3ec-b9c2-4faa-8c2a-cf383b8ddb2b.png)'
- en: Original image (left); diagonally translated image (right)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像（左）；对角线平移的图像（右）
- en: 'This is a very simple example and the translated image is not very different
    from the original one. However, in a more complex scenario, a classifier could
    also fail to correctly classify an object in similar conditions. Applying a max
    pooling (with a (*2 × 2*) pool size and 2-pixel strides) on the translated image,
    we get the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的例子，翻译图像与原始图像没有太大差异。然而，在更复杂的场景中，分类器也可能无法在类似条件下正确分类对象。对翻译图像应用最大池化（池大小为(*2 ×
    2*)，步长为2像素），我们得到以下结果：
- en: '![](img/4791bc89-b259-4977-9435-335bd3afcb4e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4791bc89-b259-4977-9435-335bd3afcb4e.png)'
- en: Original image (left); result of a max pooling on the translated image (right)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像（左）；对翻译图像进行最大池化后的结果（右）
- en: 'The result is a larger cross, whose arms are slightly more aligned to the axis.
    When compared with the original image, it''s easier for a classifier with a good
    generalization ability to filter out the spurious elements and recognize the original
    shape (which can be considered a cross surrounded by a noisy frame). Repeating
    the same experiment with average pooling (same parameters), we obtain the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个更大的十字，其臂部与轴稍微对齐。与原始图像相比，具有良好泛化能力的分类器更容易过滤掉虚假元素并识别原始形状（可以被认为是一个被噪声框架包围的十字）。使用相同的平均池化（相同的参数）重复相同的实验，我们得到以下结果：
- en: '![](img/ce0227d0-0e00-477e-b0b4-9363e8e003f3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce0227d0-0e00-477e-b0b4-9363e8e003f3.png)'
- en: Original image (left); result of an average pooling on the translated image
    (right)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像（左）；对翻译图像进行平均池化后的结果（右）
- en: In this case, the picture is partially smoothed, but it's still possible to
    see a better alignment (thanks mainly to the fading effect). Also, if these methods
    are simple and somewhat effective, the robustness to invariant transformations
    is never dramatically improved and higher levels of invariance are possible only
    by increasing the pool size. This choice leads to coarser-grained feature maps
    whose amount of information is drastically reduced; therefore, whenever it's necessary
    to extend the classification to samples that can be distorted or rotated, it can
    be a good idea (which allows working with a dataset that better represents the
    real data generating process) to use a data augmentation technique to produce
    artificial images and to also train the classifier on them. However, as pointed
    out in *Deep Learning, Goodfellow I., Bengio Y.,‎ Courville A., MIT Press*, pooling
    layers can also provide a robust invariance to rotations when they are used together
    with the output of a multiple convolution layer or a rotated image stack. In fact,
    in these cases, a single pattern response is elicited and the effect of the pooling
    layer becomes similar to a collector that standardizes the output. In other words,
    it will produce the same result without an explicit selection of the best matching
    pattern. For this reason, if the dataset contains enough samples, pooling layers
    in intermediate positions of the network can provide a moderate robustness to
    small rotations, increasing the generalization ability of the whole deep architecture.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，图片部分被平滑处理，但仍能看出更好的对齐（主要归功于渐隐效果）。此外，如果这些方法简单且有一定效果，对不变变换的鲁棒性从未有显著提高，而要实现更高层次的不变性，只能通过增加池化大小。这种选择导致特征图粒度更粗，信息量大幅减少；因此，每当需要将分类扩展到可能被扭曲或旋转的样本时，使用数据增强技术生成人工图像，并在此之上训练分类器，可能是一个好主意（这允许使用更好地代表真实数据生成过程的数据库）。然而，正如*深度学习，Goodfellow
    I.，Bengio Y.，Courville A.，MIT Press*所指出的，当与多个卷积层的输出或旋转图像堆叠一起使用时，池化层也可以提供对旋转的鲁棒不变性。实际上，在这些情况下，会引发单个模式响应，池化层的效果类似于一个标准化输出的收集器。换句话说，它将产生相同的结果，而无需显式选择最佳匹配模式。因此，如果数据集包含足够的样本，网络中间位置的池化层可以提供对微小旋转的适度鲁棒性，从而提高整个深度架构的泛化能力。
- en: As it's easy to see in the previous example, the main difference between the
    two variants is the final result. Average pooling performs a sort of very simple
    interpolation, smoothing the borders and avoiding abrupt changes. On the other
    hand, max pooling is less noisy and can yield better results when the features
    need to be detected without any kind of smoothing (which could alter their geometry).
    I always suggest testing both techniques, because it's almost impossible to pick
    the best method with the right pool size according only to heuristic considerations
    (above all, when the datasets are not made up of very simple images).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，两种变体之间的主要区别在于最终结果。平均池化执行一种非常简单的插值，平滑边缘并避免突变。另一方面，最大池化噪声较少，当需要检测特征而无需任何平滑（这可能会改变它们的几何形状）时，可以产生更好的结果。我总是建议测试这两种技术，因为仅根据经验考虑（尤其是当数据集不是由非常简单的图像组成时），几乎不可能选择最佳方法并确定合适的池化大小。
- en: Clearly, it's always preferable to use these layers after a group of convolutions,
    avoiding very large pool sizes that can irreversibly destroy the information content.
    In many important deep architectures, the pooling layers are always based on (*2,
    2*) or (*3, 3*) pools, independently of their position, and the strides are always
    set to 1 or 2. In both cases, the information loss is proportional to the pool
    size/strides; therefore, large pools are normally avoided when small features
    must be detected together with larger ones (for example, foreground and background
    faces).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，始终最好在一系列卷积之后使用这些层，避免使用非常大的池化大小，因为这可能会永久性地破坏信息内容。在许多重要的深度架构中，池化层始终基于(*2, 2*)或(*3,
    3*)池化，无论它们的位置如何，步长始终设置为1或2。在两种情况下，信息损失与池化大小/步长成比例；因此，当需要检测小特征和大特征（例如，前景和背景人脸）时，通常避免使用大池化。
- en: Other useful layers
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他有用的层
- en: 'Even if convolution and pooling layers are the backbone of almost all deep
    convolutional networks, other layers can be helpful to manage specific situations.
    They are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 即使卷积和池化层几乎是所有深度卷积网络的核心，其他层也可以帮助处理特定情况。具体如下：
- en: '**Padding layers**: These can be employed to increase the size of a feature
    map (for example, to align it with another one) by surrounding it with a blank
    frame (*n* black pixels are added before and after each side).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充层**：这些层可以通过在特征图周围添加空白框架（例如，在每一边添加 *n* 个黑色像素）来增加特征图的大小（例如，将其与另一个特征图对齐）。。'
- en: '**Upsampling layers**: These increase the size of a feature map by creating
    larger blocks out of a single pixel. To a certain extent, they can be considered
    as a transformation opposite to a pooling layer, even if, in this case, the upsampling
    is not based on any kind of interpolation. These kinds of layers can be used to
    prepare the feature maps for transformations similar to the ones obtained with
    a transpose convolution, even if many experiments confirmed that using larger
    strides can yield very accurate results without the need of an extra computational
    step.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上采样层**：这些层通过将单个像素创建成更大的块来增加特征图的大小。在某种程度上，它们可以被视为与池化层相反的转换，尽管在这种情况下，上采样不是基于任何类型的插值。这些类型的层可以用来准备特征图，以便进行与转置卷积类似的转换，尽管许多实验证实使用更大的步长可以在不需要额外计算步骤的情况下产生非常准确的结果。'
- en: '**Cropping layers**: These are helpful for selecting specific rectangular areas
    of an image/feature map. They are particularly useful in modular architectures,
    where the first part determines the cropping boundaries (for example, of a face),
    while the second part, after having removed the background, can perform high-level
    operations such as detail segmentation (marking the areas of eyes, nose, mouth,
    and so on). The possibility of inserting these layers directly into a deep neural
    model avoids multiple data transfers. Unfortunately, many frameworks (such as
    Keras) don''t allow us to use variable boundaries, limiting *de facto* the number
    of possible use cases.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪层**：这些层有助于选择图像/特征图中的特定矩形区域。它们在模块化架构中特别有用，其中第一部分确定裁剪边界（例如，面部），而第二部分在移除背景后可以执行高级操作，如细节分割（标记眼睛、鼻子、嘴巴等区域）。将这些层直接插入到深度神经网络模型中的可能性避免了多次数据传输。不幸的是，许多框架（如
    Keras）不允许我们使用可变边界，实际上限制了可能的用例数量。'
- en: '**Flattening layers:** These are the conjunction link between feature maps
    and fully-connected layers. Normally, a single flattening layer is used before
    processing the output of the convolutional blocks, with a few dense layers terminating
    in a final Softmax layer (for classifications). The operation is computationally
    very cheap as it works only with the metadata and doesn''t perform any calculations.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**展平层**：这些层是特征图和全连接层之间的连接。通常，在处理卷积块的输出之前，使用单个展平层，然后是几个密集层，最终以 Softmax 层结束（用于分类）。这个操作在计算上非常便宜，因为它只与元数据一起工作，不执行任何计算。'
- en: Examples of deep convolutional networks with Keras
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 的深度卷积网络示例
- en: 'In the first example, we want to consider again the complete MNIST handwritten
    digit dataset, but instead of using an MLP, we are going to employ a small deep
    convolutional network. The first step consists of loading and normalizing the
    dataset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们再次考虑完整的 MNIST 手写数字数据集，但不是使用 MLP，而是将使用一个小型深度卷积网络。第一步包括加载数据集并进行归一化：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now define the model architecture. The samples are rather small (*28 ×
    28*); therefore it can be helpful to use small kernels. This is not a general
    rule and it''s useful to also evaluate larger kernels (in particular in the first
    layers); however, many state-of-the-art architectures confirmed large kernel sizes
    with small images can lead to a performance loss. In my personal experiments,
    I''ve always obtained the best results when the largest kernels were *8 ÷ 10*
    smaller than the image dimensions. Our model is made up of the following layers:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义模型架构。样本相对较小（*28 × 28*）；因此，使用小型核可能是有帮助的。这并不是一个普遍的规则，评估较大的核（特别是在第一层）也是有用的；然而，许多最先进的架构证实，在小型图像中使用较大的核大小会导致性能下降。在我的个人实验中，我总是当最大的核比图像尺寸小
    *8 ÷ 10* 时获得最佳结果。我们的模型由以下层组成：
- en: Input dropout 25%.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入丢弃率 25%。
- en: Convolution with 16 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding (the default weight initializer is Xavier). Keras implements
    the `Conv2D` class, whose main parameters are immediately understandable.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 16 个滤波器、（3 × 3）核、步长为 1、ReLU 激活和相同的填充（默认权重初始化器是 Xavier）。Keras 实现了 `Conv2D`
    类，其主要参数是立即可理解的。
- en: Dropout 50%.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃率 50%。
- en: Convolution with 32 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用32个过滤器，(3 × 3)核，步长为1，ReLU激活函数，以及相同的填充。
- en: Dropout 50%.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout 50%。
- en: Average pooling with (2 × 2) pool size and strides equal to 1 (using the Keras
    class `AveragePooling2D`).
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用(2 × 2)池化大小和步长为1的平均池化（使用Keras类`AveragePooling2D`）。
- en: Convolution with 64 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用64个过滤器，(3 × 3)核，步长为1，ReLU激活函数，以及相同的填充。
- en: Average pooling with (2 × 2) pool size and strides equal to 1.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用(2 × 2)池化大小和步长为1的平均池化。
- en: Convolution with 64 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用64个过滤器，(3 × 3)核，步长为1，ReLU激活函数，以及相同的填充。
- en: Dropout 50%.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout 50%。
- en: Average pooling with (2 × 2) pool size and strides equal to 1.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用(2 × 2)池化大小和步长为1的平均池化。
- en: Fully-connected layer with 1024 ReLU units.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有1024个ReLU单元的全连接层。
- en: Dropout 50%.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout 50%。
- en: Fully-connected layer with 10 Softmax units.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有十个Softmax单元的全连接层。
- en: 'The goal is to capture the low-level features (horizontal and vertical lines,
    intersections, and so on) in the first layers and use the pooling layers and all
    the subsequent convolutions to increase the accuracy when distorted samples are
    presented. At this point, we can create and compile the model (using the Adam
    optimizer with *η = 0.001* and a decay rate equal to *10^(-5)*):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在第一层捕获低级特征（水平线和垂直线、交叉点等），并使用池化层和所有后续卷积来提高在扭曲样本出现时的准确性。在此阶段，我们可以创建和编译模型（使用η
    = 0.001的Adam优化器和衰减率等于10^(-5)）：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now proceed to train the model with 200 epochs and a batch size of 256
    samples:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用200个周期和256个样本的批量大小来训练模型：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The final validation accuracy is now `0.9950`, which means that only 50 samples
    (out of 10,000) have been misclassified. To better understand the behavior, we
    can plot the accuracy and loss diagrams:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最终验证准确率现在是`0.9950`，这意味着只有50个样本（在10000个样本中）被错误分类。为了更好地理解行为，我们可以绘制准确率和损失图：
- en: '![](img/d6947af3-e86a-4e5b-a396-6164e4045a70.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6947af3-e86a-4e5b-a396-6164e4045a70.png)'
- en: As it's possible to see, both validation accuracy and loss easily reach the
    optimal values. In particular, the initial validation accuracy is about 0.97 and
    the remaining epochs are necessary to improve the performance with all those samples,
    whose shapes can lead to confusion (for example, malformed 8s that resemble 0s,
    or 7s that are very similar to 1s). It's evident that the *geometric* approach
    employed by convolutions guarantees a much higher robustness than a standard fully-connected
    network, thanks also to the contribution of pooling layers, which reduce the variance
    due to noisy samples.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，验证准确率和损失很容易达到最优值。特别是，初始验证准确率约为0.97，剩余的周期是必要的，以提高所有这些样本的性能，其形状可能导致混淆（例如，形状不规则的8看起来像0，或者非常相似的7看起来像1）。很明显，卷积使用的*几何*方法比标准全连接网络提供了更高的鲁棒性，这也要归功于池化层的贡献，它们减少了由于噪声样本引起的方差。
- en: Example of a deep convolutional network with Keras and data augmentation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras和数据增强的深度卷积网络的示例
- en: 'In this example, we are going to use the Fashion MNIST dataset, which was freely
    provided by Zalando as a more difficult replacement for the standard MNIST dataset.
    In this case, instead of handwritten digits, there are greyscale photos of different
    articles of clothing. An example of a few samples is shown in the following screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用由Zalando免费提供的Fashion MNIST数据集，作为标准MNIST数据集的更困难替代品。在这种情况下，而不是手写数字，这里有不同服装的灰度照片。以下截图显示了几个样本的示例：
- en: '![](img/5f938c3a-e85d-4b71-82c0-b28a23358c2a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5f938c3a-e85d-4b71-82c0-b28a23358c2a.png)'
- en: 'However, in this case, we want to employ a utility class provided by Keras
    (`ImageDataGenerator`) in order to create a data-augmented sample set to improve
    the generalization ability of the deep convolutional network. This class allows
    us to add random transformations (such as standardization, rotations, shifting,
    flipping, zooming, shearing, and so on) and output the samples using a Python
    generator (with an infinite loop). Let''s start loading the dataset (we don''t
    need to standardize it, as this transformation is performed by the generator):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们想要使用Keras提供的实用类（`ImageDataGenerator`）来创建一个数据增强样本集，以提高深度卷积网络的一般化能力。这个类允许我们添加随机转换（如标准化、旋转、平移、翻转、缩放、剪切等），并使用Python生成器（具有无限循环）输出样本。让我们开始加载数据集（我们不需要标准化它，因为这个转换由生成器执行）：
- en: '[PRE3]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At this point, we can create the generators, selecting the transformation that
    best suits our case. As the dataset is rather *standard* (all the samples are
    represented only in a few positions), we''ve decided to augment the dataset by
    applying a sample-wise standardization (which doesn''t rely on the entire dataset),
    horizontal flip, zooming, small rotations, and small shears. This choice has been
    made according to an objective analysis, but I suggest the reader repeat the experiment
    with different parameters (for example, adding whitening, vertical flip, horizontal/vertical
    shifting, and extended rotations). Of course, increasing the augmentation variability
    needs larger processed sets. In our case, we are going to use 384,000 training
    samples (the original size is 60,000), but larger values can be employed to train
    deeper networks:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以创建生成器，选择最适合我们情况的转换。由于数据集相当*标准*（所有样本只表示在几个位置），我们决定通过应用样本级标准化（不依赖于整个数据集）、水平翻转、缩放、小旋转和小剪切来增加数据集。这个选择是根据客观分析做出的，但我建议读者用不同的参数重复实验（例如，添加白化、垂直翻转、水平/垂直平移和扩展旋转）。当然，增加增强的变异性需要更大的处理集。在我们的情况下，我们将使用384,000个训练样本（原始大小为60,000），但可以使用更大的值来训练更深的网络：
- en: '[PRE4]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once an image data generator has been initialized, it must be fitted, specifying
    the input dataset and the desired batch size (the output of this operation is
    the actual Python generator). The test image generator is voluntarily kept without
    transformations except for normalization and standardization, in order to avoid
    a validation on a dataset drawn from a different distribution. At this point,
    we can create and compile our network, using 2D convolutions based on Leaky ReLU
    activations (using the `LeakyReLU` class, which replaces the standard layer `Activation`),
    batch normalizations, and max poolings:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了图像数据生成器，就必须对其进行拟合，指定输入数据集和期望的批量大小（此操作的输出是实际的Python生成器）。测试图像生成器自愿不进行任何转换，除了归一化和标准化，以避免在来自不同分布的数据集上进行验证。在这个阶段，我们可以创建和编译我们的网络，使用基于Leaky
    ReLU激活的2D卷积（使用`LeakyReLU`类，该类取代了标准的`Activation`层），批量归一化，以及最大池化：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'All the batch normalizations are always applied to the linear transformation
    before the activation function. Considering the additional complexity, we are
    also going to use a callback, which is a class that Keras uses in order to perform
    in-training operations. In our case, we want to reduce the learning rate when
    the validation loss stops improving. The specific callback is called `ReduceLROnPlateau`
    and it''s tuned in order to reduce *η* multiplying it by `0.1` (after a number
    of epochs equal to the value of the `patience` parameter) with a cooldown period
    (the number of epochs to wait before restoring the original learning rate) of
    1 epoch and a minimum *η = 10^(-6)*. The training method is now `fit_generator()`,
    which accepts Python generators instead of finite datasets and the number of iterations
    per epoch (all the other parameters are the same as implemented by `fit()`):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有批量归一化都是应用于激活函数之前的线性变换。考虑到额外的复杂性，我们还将使用一个回调，这是Keras用来执行训练中操作的类。在我们的情况下，我们希望在验证损失停止改进时降低学习率。特定的回调称为`ReduceLROnPlateau`，并调整以将*η*乘以`0.1`（在等于`patience`参数值的epoch数之后），有一个冷却期（在恢复原始学习率之前等待的epoch数）为1个epoch，以及最小*η
    = 10^(-6)*。训练方法现在是`fit_generator()`，它接受Python生成器而不是有限的数据集，以及每个epoch的迭代次数（所有其他参数与`fit()`实现相同）：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In this case, the complexity is higher and the result is not as accurate as
    the one obtained with the standard MNIST dataset. The validation and loss plots
    are shown in the following graph:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，复杂性更高，结果并不像使用标准的MNIST数据集获得的结果那样准确。验证和损失曲线在以下图表中展示：
- en: '![](img/3fba92d8-51bd-407b-9c2d-dff943933638.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3fba92d8-51bd-407b-9c2d-dff943933638.png)'
- en: The loss plot doesn't show a U-curve, but it seems that there are no real improvements
    starting from the 20^(th) epoch. This is also confirmed by the validation plot,
    which continues oscillating between 0.935 and about 0.94\. On the other side,
    the training loss hasn't reached its minimum (nor has the training accuracy),
    mainly because of the batch normalizations. However, considering several benchmarks,
    the result is not bad (even if state-of-the-art models can reach a validation
    accuracy of about 0.96). I suggest that the reader try different configurations
    (with and without dropout and other activations) based on deeper architectures
    with larger training sets. This example offers many chances to practice with this
    kind of models, as the complexity is not as high as to require dedicated hardware,
    but at the same time, there are many ambiguities (for example, between shirts
    and t-shirts) that can reduce the generalization ability.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 损失曲线并没有显示出U形曲线，但似乎从第20个epoch开始并没有真正的改进。这一点也由验证曲线得到证实，它继续在0.935和大约0.94之间波动。另一方面，训练损失并没有达到其最小值（训练准确率也是如此），这主要是因为批量归一化。然而，考虑到几个基准，结果并不差（即使最先进的模型可以达到大约0.96的验证准确率）。我建议读者尝试不同的配置（带有和不带有dropout以及其他激活函数），基于更深层次的架构和更大的训练集。这个例子提供了许多练习这类模型的机会，因为其复杂性并不高，不需要专用硬件，但与此同时，存在许多模糊性（例如，衬衫和T恤之间的区别），这可能会降低泛化能力。
- en: Recurrent networks
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'All the models that we have analyzed until now have a common feature. Once
    the training process is completed, the weights are frozen and the output depends
    only on the input sample. Clearly, this is the expected behavior of a classifier,
    but there are many scenarios where a prediction must take into account the history
    of the input values. A time series is a classic example. Let''s suppose that we
    need to predict the temperature for the next week. If we try to use only the last
    known *x^((t))* value and an MLP trained to predict *x^((t+1))*, it''s impossible
    to take into account temporal conditions like the season, the history of the season
    over the years, the position in the season, and so on. The regressor will be able
    to associate the output that yields the minimum average error, but in real-life
    situations, this isn''t enough. The only reasonable way to solve this problem
    is to define a new architecture for the artificial neuron, to provide it with
    a memory. This concept is shown in the following diagram:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所分析的所有模型都有一个共同特征。一旦训练过程完成，权重就会被冻结，输出只取决于输入样本。显然，这是分类器预期的行为，但在许多场景中，预测必须考虑到输入值的历史。时间序列是一个经典的例子。假设我们需要预测下周的温度。如果我们试图只使用最后一个已知的*x^(t)*值和一个训练用来预测*x^(t+1)*的多层感知器（MLP），我们就无法考虑到像季节、季节多年的历史、季节中的位置等时间条件。回归器将能够关联产生最小平均误差的输出，但在现实生活中的情况下，这并不足够。解决这个问题的唯一合理方式是为人工神经元定义一个新的架构，给它提供记忆。这个概念在以下图中展示：
- en: '![](img/d906787b-104d-4d25-9e6f-d05472c1fb8d.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d906787b-104d-4d25-9e6f-d05472c1fb8d.png)'
- en: 'Now the neuron is no longer a pure feed-forward computational unit because
    the feedback connection forces it to remember its past and use it in order to
    predict new values. The new dynamic rule is now as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这个神经元不再是一个纯粹的纯前向计算单元，因为反馈连接迫使它记住其过去并使用它来预测新的值。新的动态规则现在如下：
- en: '![](img/bf8e2970-8cbc-4784-b92d-a13ed9ffde8a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bf8e2970-8cbc-4784-b92d-a13ed9ffde8a.png)'
- en: The previous prediction is fed back and summed to new linear output. The resulting
    value is transformed by the activation function in order to produce the actual
    new output (conventionally the first output is null, but this is not a constraint).
    An immediate consideration concerns the activation function—this is a dynamic
    system that could easily become unstable. The only way to prevent this phenomenon
    is to employ saturating functions (such as the sigmoid or hyperbolic tangent).
    In fact, whatever the input is, the output can never *explode* by moving towards
    *+∞* or *-∞*.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的预测被反馈并累加到新的线性输出中。这个结果通过激活函数转换，以产生实际的新输出（通常第一个输出是空的，但这不是一个约束）。一个立即需要考虑的问题是激活函数——这是一个可能很容易变得不稳定的动态系统。防止这种现象的唯一方法就是使用饱和函数（如Sigmoid或双曲正切）。实际上，无论输入是什么，输出永远不会通过向*+∞*或*-∞*移动而*爆炸*。
- en: Suppose that, instead, we were to use a ReLU activation—under some conditions,
    the output will grow indefinitely, leading to an overflow. Clearly, the situation
    is even worse with a linear activation and could be very similar even when using
    a Leaky ReLU or ELU. Hence, it's obvious that we need to select saturating functions,
    but is this enough to ensure stability? Even if a hyperbolic tangent (as well
    as a sigmoid) has two stable points (*-1* and *+1*), this isn't enough to ensure
    stability. Let's imagine that the output is affected by noise and oscillates around
    0.0\. The unit cannot converge towards a value and remains trapped in a limit
    cycle.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用的是ReLU激活函数——在某些条件下，输出将无限增长，导致溢出。显然，使用线性激活函数的情况更糟，即使使用Leaky ReLU或ELU，情况也可能非常相似。因此，很明显我们需要选择饱和函数，但这足够确保稳定性吗？即使双曲正切（以及Sigmoid）有两个稳定点（*-1*和*+1*），这也不足以确保稳定性。让我们想象输出受到噪声的影响，并在0.0附近振荡。该单元无法收敛到某个值，并保持在极限循环中。
- en: Luckily, the possibility to learn the weights allows us to increase the robustness
    to noise, avoiding that limited changes in the input could invert the dynamic
    of the neuron. This is a very important (and easy to prove) result that guarantees
    stability under very simple conditions, but again, what is the price that we need
    to pay? Is it anything simple and straightforward? Unfortunately, the answer is
    negative and the price for stability is extremely high. However, before discussing
    this problem, let's show how a simple recurrent network can be trained.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，学习权重的可能性使我们能够提高对噪声的鲁棒性，避免输入的有限变化会逆转神经元的动态。这是一个非常重要（且容易证明）的结果，它保证了在非常简单的条件下稳定性，但再次，我们需要付出什么代价？这是否简单直接？不幸的是，答案是消极的，稳定性的代价非常高。然而，在讨论这个问题之前，让我们看看一个简单的循环网络是如何被训练的。
- en: Backpropagation through time (BPTT)
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: 'The simplest way to train an RNN is based on a representational trick. As the
    input sequences are limited and their length can be fixed, it''s possible to restructure
    the simple neuron with a feedback connection as an unrolled feed-forward network.
    In the following diagram, there''s an example with *k* timesteps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 训练RNN最简单的方法是基于一种表示技巧。由于输入序列有限且长度可以固定，因此可以将具有反馈连接的简单神经元重新结构化为展开的前馈网络。在下面的图中，有一个带有*k*个时间步长的示例：
- en: '![](img/6be24702-989d-4d8c-82af-2018dc2e9c5c.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6be24702-989d-4d8c-82af-2018dc2e9c5c.png)'
- en: Example of unrolled recurrent network
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 展开的循环神经网络示例
- en: This network (which can be easily extended to more complex architecture with
    several layers) is exactly like an MLP, but in this case, the weights of each
    *clone* are the same. The algorithm called **BPTT** is the natural extension of
    the standard learning technique to unrolled recurrent networks. The procedure
    is straightforward. Once all the outputs have been computed, it's possible to
    determine the value of the cost function for every single network. At this point,
    starting from the last step, the corrections (the gradients) are computed and
    stored, and the process is repeated until the initial step. Then, all of the gradients
    are summed and applied to the network. As every single contribution is based on
    a precise *temporal experience* (made up of a local sample and a previous memory
    element), the standard backpropagation will learn how to manage a dynamic condition
    as if it were a point-wise prediction. However, we know that the actual network
    is not unrolled and the past dependencies are theoretically propagated and remembered.
    I voluntarily used the word *theoretically,* because all practical experiments
    show a completely different behavior that we are going to discuss. This technique
    is very easy to implement, but it can be very expensive for deep networks that
    must be unrolled for a large number of timesteps. For this reason, a variant called
    **truncated backpropagation through time** (**TBPTT**) has been proposed (in *Subgrouping
    reduces complexity and speeds up learning in recurrent networks, Zipser D., Advances
    in Neural Information Processing Systems, II 1990*).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络（可以轻松扩展到具有多个层的更复杂架构）与MLP（多层感知器）完全相同，但在这个情况下，每个*克隆*的权重是相同的。称为**BPTT（反向传播时间）**的算法是标准学习技术向展开循环网络的自然扩展。这个过程很简单。一旦所有输出都被计算出来，就可以确定每个单独网络的成本函数值。在这个时候，从最后一步开始，计算并存储校正（梯度），然后重复这个过程直到初始步骤。然后，将所有梯度求和并应用于网络。由于每个贡献都是基于精确的*时间经验*（由局部样本和先前记忆元素组成），标准反向传播将学会如何管理动态条件，就像它是点预测一样。然而，我们知道实际的网络并没有展开，过去的依赖关系在理论上被传播和记住。我故意使用了“理论上”这个词，因为所有实际实验都显示出完全不同的行为，我们将在后面讨论。这种技术非常容易实现，但对于必须展开以处理大量时间步长的深度网络来说，可能会非常昂贵。因此，提出了一个名为**截断反向传播时间（TBPTT**）的变体（在*Subgrouping
    reduces complexity and speeds up learning in recurrent networks, Zipser D., Advances
    in Neural Information Processing Systems, II 1990*）。
- en: The idea is to use two sequence lengths *t[1]* and *t*[*2* ](with *t[1] >> t[2]*)—the
    longer one (*t[1]*) is employed for the feed-forward phase, while the shorter
    length (*t[2]*) is used to train the network. At first sight, this version seems
    like a normal BPTT with a short sequence; however, the key idea is to force the
    network to update the hidden states with more pieces of information and then compute
    the corrections according to the result of the longer sequence (even if the updates
    are propagated to a limited number of previous timesteps). Clearly, this is an
    approximation that can speed up the training process, but the final result is
    normally comparable with the one obtained by processing long sequences, in particular
    when the dependencies can be split into shorter temporal chunks (and therefore
    the assumption is that there are no very long dependencies).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是使用两个序列长度 *t[1]* 和 *t[2]*（其中 *t[1]* >> *t[2]*）——较长的序列（*t[1]*）用于正向传播阶段，而较短的长度（*t[2]*）用于训练网络。乍一看，这个版本看起来就像一个带有短序列的正常BPTT；然而，关键思想是迫使网络使用更多信息更新隐藏状态，然后根据较长序列的结果（即使更新只传播到有限的前一时间步）计算校正。显然，这是一个可以加快训练过程的近似，但最终结果通常与处理长序列得到的结果相当，特别是在依赖关系可以被分成更短的时序块时（因此假设没有非常长的依赖关系）。
- en: 'Even if the BPTT algorithm is mathematically correct and it''s not difficult
    to learn short-term dependencies (corresponding to short unrolled networks), several
    experiments confirmed that it''s extremely difficult (or almost impossible) learning
    long-term dependencies. In other words, it''s easy to exploit past experiences
    whose contribution is limited to a short window (and therefore whose importance
    is limited because they cannot manage the most complex trends) but the network
    cannot easily learn all behaviors that, for example, have a periodicity of hundreds
    of timesteps. In 1994, Bengio, Simard, and Frasconi provided a theoretical explanation
    of the problem (in *Learning Long-Term Dependencies with Gradient Descent is Difficult,
    Bengio Y., Simard P., Frasconi P., IEEE Transactions on Neural Networks, 5/1994*).
    The mathematical details are rather complex, because they involve dynamic system
    theory; however, the final result is that a network whose neurons are forced to
    become robust to noise (the normal expected behavior) is affected by the vanishing
    gradients problem when *t → ∞*. More generally, we can represent a vectorial recurrent
    neuron dynamic as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 即使BPTT算法在数学上是正确的，并且学习短期依赖（对应于短展开网络）并不困难，但多项实验证实，学习长期依赖性极其困难（或几乎不可能）。换句话说，利用过去有限窗口内的经验（因此其重要性有限，因为它们无法管理最复杂的变化）是容易的，但网络难以学习所有行为，例如，具有数百个时间步长周期性的行为。1994年，Bengio、Simard和Frasconi提供了对这个问题的理论解释（在*Using
    Gradient Descent to Learn Long-Term Dependencies is Difficult, Bengio Y., Simard
    P., Frasconi P., IEEE Transactions on Neural Networks, 5/1994*）。数学细节相当复杂，因为它们涉及到动态系统理论；然而，最终结果是，当神经元被迫对噪声（正常预期行为）变得鲁棒时，网络会受到梯度消失问题的困扰，当
    *t → ∞*。更普遍地说，我们可以将向量循环神经元动态表示如下：
- en: '![](img/e64e0be6-5cb2-4b1b-b5f8-188eb4c89d5b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e64e0be6-5cb2-4b1b-b5f8-188eb4c89d5b.png)'
- en: 'The multiplicative effect of BPTT forces the gradients to be proportional to
    *W^t*. If the largest absolute eigenvalue (also known as spectral radius) of *W*
    is smaller than 1, then the following applies:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT的乘法效应迫使梯度与 *W^t* 成正比。如果 *W* 的最大绝对特征值（也称为谱半径）小于1，那么以下适用：
- en: '![](img/4712b54a-8f73-4bdf-9236-cf2c910f8a5b.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4712b54a-8f73-4bdf-9236-cf2c910f8a5b.png)'
- en: More simply, we can re-express the result saying that the magnitude of the gradients
    is proportional to the length of the sequences and even if the condition is asymptotically
    valid, many experiments confirmed that the limited precision of numeric computations
    and the exponential decay due to subsequent multiplications can force the gradients
    to vanish even when the sequences are not extremely long. This seems to be the
    end of any RNN architecture, but luckily more recent approaches have been designed
    and proposed to resolve this problem, allowing RNNs to learn both short and long-term
    dependencies without particular complications. A new era of RNNs started and the
    results were immediately outstanding.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，我们可以重新表述这个结果，即梯度的幅度与序列长度成正比，即使条件在渐近上是有效的，许多实验证实，数值计算的有限精度和由于后续乘法引起的指数衰减可以迫使梯度消失，即使序列不是特别长。这似乎是任何RNN架构的终结，但幸运的是，更近期的方案已经被设计和提出，以解决这个问题，允许RNNs在没有特别复杂的情况下学习短期和长期依赖。RNN的新时代开始了，结果立即显著。
- en: LSTM
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM
- en: This model (which represents the state-of-the-art recurrent cell in many fields)
    was proposed in 1997 by Hochreiter and Schmidhuber (in *Long Short-Term Memory,
    Hochreiter S., Schmidhuber J., Neural Computation, Vol. 9, 11/1997*) with the
    emblematic name **long-short-term memory**  (**LSTM**). As the name suggests,
    the idea is to create a more complex artificial recurrent neuron that can be plugged
    into larger networks and trained without the risk of vanishing and, of course,
    exploding gradients. One of the key elements of classic recurrent networks is
    that they are focused on learning, but not on selectively forgetting. This ability
    is indeed necessary for optimizing the memory in order to remember what is really
    important and removing all those pieces of information that are not necessary
    to predict new values.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型（在许多领域代表了最先进的循环单元）是由Hochreiter和Schmidhuber在1997年提出的，具有标志性的名字**长短期记忆**（**LSTM**）。正如其名所示，想法是创建一个更复杂的艺术性循环神经元，可以插入到更大的网络中，并且可以在没有消失和爆炸梯度风险的情况下进行训练。经典循环网络的一个关键元素是它们专注于学习，而不是选择性遗忘。这种能力确实是优化记忆以记住真正重要的东西并移除所有那些对预测新值不必要的信
- en: To achieve this goal, LSTM exploits two important features (it's helpful to
    expose them before discussing the model). The first one is an explicit state,
    which is a separate set of variables that store the elements necessary to build
    long and short-term dependencies, including the current state. These variables
    are the building blocks of a mechanism called **constant error carousel** (**CEC**),
    named in this way because it's responsible for the cyclical and internal management
    of the error provided by the backpropagation algorithm. This approach allows the
    correction of the weights without suffering the multiplicative effect anymore.
    The internal LSTM dynamics allow better understanding of how the error is safely
    fed back; however, the exact explanation of the training procedure (which is always
    based on the gradient descent) is beyond the scope of this book and can be found
    in the aforementioned paper.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个目标，LSTM利用了两个重要的特征（在讨论模型之前先揭露它们是有帮助的）。第一个是一个显式状态，它是一组独立的变量，用于存储构建长期和短期依赖关系所需的元素，包括当前状态。这些变量是称为**恒定误差轮**（**CEC**）的机制的构建块，之所以这样命名是因为它负责由反向传播算法提供的错误循环和内部管理。这种方法允许纠正权重而不再受到乘法效应的影响。内部LSTM动态有助于更好地理解错误是如何安全地反馈的；然而，关于训练过程（始终基于梯度下降）的详细解释超出了本书的范围，可以在上述论文中找到。
- en: 'The second feature is the presence of gates. We can simply define a gate as
    an element that can modulate the amount of information flowing through it. For
    example, if *y = ax* and *a* is a variable bounded between 0 and 1, it can be
    considered as a gate, because when it''s equal to 0, it blocks the input *x;* when
    it''s equal to 1, it allows the input to flow in without restrictions; and when
    it has an intermediate value, it reduces the amount of information proportionally.
    In LSTMs, gates are managed by sigmoid functions, while the activations are based
    on hyperbolic tangents (whose symmetry guarantees better performances). At this
    point, we can show the structural diagram of an LSTM cell and discuss its internal
    dynamics:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个特征是存在门。我们可以简单地将门定义为可以调节通过它的信息量的元素。例如，如果 *y = ax* 且 *a* 是介于0和1之间的变量，它可以被认为是一个门，因为当它等于0时，它会阻止输入
    *x*；当它等于1时，它允许输入无限制地流入；当它具有中间值时，它会按比例减少信息量。在LSTMs中，门由sigmoid函数管理，而激活基于双曲正切（其对称性保证了更好的性能）。在这个阶段，我们可以展示LSTM单元的结构图并讨论其内部动态：
- en: '![](img/fec7669a-ede2-4853-b198-b164c777829a.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fec7669a-ede2-4853-b198-b164c777829a.png)'
- en: 'The first (and most important) element is the memory state, which is responsible
    for the dependencies and for the actual output. In the diagram, it is represented
    by the upper line and its dynamics are represented by the following general equation:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个（也是最重要的）元素是记忆状态，它负责依赖关系和实际输出。在图中，它由上面的线表示，其动态由以下一般方程表示：
- en: '![](img/66896d56-f095-4293-a24c-12fd521ab81e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66896d56-f095-4293-a24c-12fd521ab81e.png)'
- en: 'So, the state depends on the previous value, on the current input, and on the
    previous output. Let''s start with the first term, introducing the forget gate.
    As the name says, it''s responsible for the persistence of the existing memory
    elements or for their deletion. In the diagram, it''s represented by the first
    vertical block and its value is obtained by considering the concatenation of previous
    output and current input:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，状态取决于先前值、当前输入和先前输出。让我们从第一个项开始，引入遗忘门。正如其名所示，它负责现有记忆元素的持久性或删除。在图中，它由第一个垂直块表示，其值是通过考虑先前输出和当前输入的连接来获得的：
- en: '![](img/c6a0590d-869d-45ff-aaa3-fc2ab8033d95.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6a0590d-869d-45ff-aaa3-fc2ab8033d95.png)'
- en: 'The operation is a classical neuron activation with a vectorial output. An
    alternative version can use two weight matrices and keep the input elements separated:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作是一个经典的神经元激活，具有向量输出。另一种版本可以使用两个权重矩阵并保持输入元素分离：
- en: '![](img/ca16f462-02b6-4af1-8bbf-be1ee4a1777b.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca16f462-02b6-4af1-8bbf-be1ee4a1777b.png)'
- en: 'However, I prefer the previous version, because it can better express the homogeneity
    of input and output, and also their consequentiality. Using the forget gate, it''s
    possible to determine the value of *g[1](C^((t)))* using the Hadamard (or element-wise)
    product:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我更喜欢先前的版本，因为它能更好地表达输入和输出的同质性及其后果性。使用遗忘门，可以通过哈达玛（或逐元素）积确定 *g[1](C^((t)))*
    的值：
- en: '![](img/07fa0421-c3e2-46de-b9a7-2a5fde4eb16b.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/07fa0421-c3e2-46de-b9a7-2a5fde4eb16b.png)'
- en: 'The effect of this computation is filtering the content of *C^((t))* that must
    be preserved and the validity degree (which is proportional to the value of *f^((t+1))*).
    If the forget gate outputs a value close to 1, the corresponding element is still
    considered valid, while lower values determine a sort of obsolescence that can
    even lead the cell to completely remove an element when the forget gate value
    is 0 or close to it. The next step is to consider the amount of the input sample
    that must be considered to update the state. This task is achieved by the input
    gate (second vertical block). The equation is perfectly analogous to the previous
    one:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种计算的效应是过滤掉必须保留的 *C^((t))* 的内容及其有效性程度（与 *f^((t+1))* 的值成正比）。如果遗忘门输出接近1的值，则相应的元素仍然被认为是有效的，而较低的值则确定一种过时性，甚至可能导致当遗忘门值为0或接近0时，细胞完全删除一个元素。下一步是考虑必须考虑的输入样本量以更新状态。这项任务是通过输入门（第二个垂直块）完成的。方程与先前的方程完全类似：
- en: '![](img/6cf6e0b8-698c-4fdd-ae91-e05b9f41d2d7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6cf6e0b8-698c-4fdd-ae91-e05b9f41d2d7.png)'
- en: 'However, in this case, we also need to compute the term that must be added
    to the current state. As already mentioned, LSTM cells employ hyperbolic tangents
    for the activations; therefore, the new contribution to the state is obtained
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这种情况下，我们还需要计算必须添加到当前状态中的项。如前所述，LSTM单元使用双曲正切作为激活函数；因此，新状态贡献的获得如下：
- en: '![](img/33a99c4c-f9a4-4d66-9354-3b77cf1a4570.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33a99c4c-f9a4-4d66-9354-3b77cf1a4570.png)'
- en: 'Using the input gate and the state contribution, it''s possible to determine
    the function *g[2](x^((t+1)), y^((t)))*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用输入门和状态贡献，可以确定函数 *g[2](x^((t+1)), y^((t)))*：
- en: '![](img/77269aa9-dd72-4d95-a1fd-3c4719746ddd.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77269aa9-dd72-4d95-a1fd-3c4719746ddd.png)'
- en: 'Hence, the complete state equation becomes as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，完整的状态方程如下：
- en: '![](img/6e4227fe-745b-4a0d-8371-8c6f65255ac4.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6e4227fe-745b-4a0d-8371-8c6f65255ac4.png)'
- en: 'Now, the inner logic of an LSTM cell is more evident. The state is based on
    the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，LSTM单元的内部逻辑更加明显。状态基于以下内容：
- en: A dynamic balance between previous experience and its re-evaluation according
    to new experience (modulated by the forget gate)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在先前经验和根据新经验重新评估之间保持动态平衡（由遗忘门调节）
- en: The *semantic* effect of the current input (modulated by the input gate) and
    the potential additive activation
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前输入的*语义*效应（由输入门调节）和潜在的加性激活
- en: Realistic scenarios are many. It's possible that a new input forces the LSTM
    to reset the state and store the new incoming value. On the other hand, the input
    gate can also remain closed, giving a very low priority to the new input (together
    with the previous output). In this case, the LSTM, considering the long-term dependencies,
    can decide to discard a sample that is considered noisy and not necessarily able
    to contribute to an accurate prediction. In other situations, both the forget
    and input gates can be partially open, letting only some values influence the
    state. All these possibilities are managed by the learning process through the
    correction of the weight matrices and the biases. The difference with BPTT is
    that the long-term dependencies are no longer impeded by the vanishing gradients
    problem.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 实际场景很多。可能新的输入会迫使LSTM重置状态并存储新的传入值。另一方面，输入门也可以保持关闭，给予新输入（连同前一个输出）非常低的优先级。在这种情况下，LSTM考虑到长期依赖关系，可以决定丢弃被认为是有噪声且不一定能有助于准确预测的样本。在其他情况下，遗忘和输入门都可以部分打开，只让一些值影响状态。所有这些可能性都通过学习过程通过修正权重矩阵和偏差来管理。与BPTT的不同之处在于，长期依赖关系不再受到梯度消失问题的阻碍。
- en: 'The last step is determining the output. The third vertical block is called
    the output gate and controls the information that must transit from the state
    to the output unit. Its equation is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是确定输出。第三个垂直块被称为输出门，它控制从状态到输出单元必须传递的信息。其方程如下：
- en: '![](img/5d0fb809-2543-49ec-8e5f-e321db0b8ab5.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d0fb809-2543-49ec-8e5f-e321db0b8ab5.png)'
- en: 'The actual output is hence determined as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，实际输出如下确定：
- en: '![](img/82a504a1-3d4d-47ab-964e-53358805a3da.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/82a504a1-3d4d-47ab-964e-53358805a3da.png)'
- en: An important consideration concerns the gates. They are all fed with the same
    vector, containing the previous output and the current input. As they are homogenous
    values, the concatenation yields a coherent entity that encodes a sort of *inverse*
    cause-effect relationship (this is an improper definition, as we work with previous
    effect and current cause). The gates work like logistic regressions without thresholding;
    therefore, they can be considered as pseudo-probability vectors (not distributions,
    as each element is independent). The forget gate expresses the probability that
    last sequence (effect, cause) is more important than the current state; however,
    only the input gate has the responsibility to grant it the right to influence
    the new state. Moreover, the output gate expresses the probability that the current
    sequence is able to let the current state flow out. The dynamic is indeed very
    complex and has some drawbacks. For example, when the output gate remains closed,
    the output is close to zero and this influences both forget and input gates. As
    they control the new state and the CEC, they could limit the amount of incoming
    information and consequent corrections, leading to poor performance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的考虑因素是门。它们都使用相同的向量，包含前一个输出和当前输入。由于它们是同质值，连接产生了一个有意义的实体，它编码了一种某种**逆**因果关系（这是一个不恰当的定义，因为我们处理的是前因后果）。门的工作方式类似于没有阈值逻辑回归；因此，它们可以被视为伪概率向量（不是分布，因为每个元素都是独立的）。遗忘门表示最后一个序列（效果，原因）比当前状态更重要；然而，只有输入门有责任赋予它影响新状态的权利。此外，输出门表示当前序列能否让当前状态流出。这种动态确实非常复杂，存在一些缺点。例如，当输出门保持关闭时，输出接近零，这会影响遗忘和输入门。由于它们控制新状态和CEC，它们可能会限制传入信息量和后续修正，导致性能不佳。
- en: 'A simple solution that can mitigate this problem is provided by a variant called
    **peephole** **LSTM**. The idea is to feed the previous state to every gate so
    that they can take decisions more independently. The generic gate equation becomes
    as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以缓解此问题的简单解决方案是由一种称为**窥视孔** **LSTM**的变体提供的。其思路是将前一个状态输入到每个门中，以便它们可以更独立地做出决策。通用的门方程如下：
- en: '![](img/d7cde201-8502-45e4-bd6d-827467a76324.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7cde201-8502-45e4-bd6d-827467a76324.png)'
- en: 'The new set of weights *U[g]* (for all three gates) must be learned in the
    same way as the standard *W[g]* and *b[g]*. The main difference with a classic
    LSTM is that the sequential dynamic: forget gate | input gate | new state | output
    gate | actual output is now partially shortcutted. The presence of the state in
    every gate activation allows them to exploit multiple recurrent connections, yielding
    a better accuracy in many complex situations. Another important consideration
    is about the learning process: in this case, the peepholes are closed and the
    only feedback channel is the output gate. Unfortunately, not every LSTM implementation
    support peepholes; however, several studies confirmed that in most cases all the
    models yield similar performances.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 新的权重集 *U[g]*（对于所有三个门）必须以与标准 *W[g]* 和 *b[g]* 相同的方式学习。与经典LSTM的主要区别在于，序列动态：忘记门
    | 输入门 | 新状态 | 输出门 | 实际输出现在部分被简略处理。状态在每个门激活中的存在允许它们利用多个循环连接，在许多复杂情况下提供更高的准确性。另一个重要的考虑因素是学习过程：在这种情况下，窥视孔被关闭，唯一的反馈通道是输出门。不幸的是，并非每个LSTM实现都支持窥视孔；然而，几项研究表明，在大多数情况下，所有模型都产生相似的性能。
- en: 'Xingjian et al. (in *Convolutional LSTM Network: A Machine Learning Approach
    for Precipitation Nowcasting, Xingjian S., Zhourong C., Hao W., Dit-Yan Y., Wai-kin
    W., Wang-Chun W., arXiv:1506.04214 [cs.CV]*) proposed a variant called **convolutional
    LSTM**, which clearly mixes Convolutions and LSTM cells. The main internal difference
    concerns the gate computations, which now become (without peepholes, which however,
    can always be added):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xingjian等人（在*卷积LSTM网络：一种用于降水预报的机器学习方法，Xingjian S.，Zhourong C.，Hao W.，Dit-Yan
    Y.，Wai-kin W.，Wang-Chun W.，arXiv:1506.04214 [cs.CV]*）提出了一种称为**卷积LSTM**的变体，它明显地将卷积和LSTM单元混合在一起。主要内部差异在于门计算，现在变为（没有窥视孔，尽管如此，总是可以添加）:'
- en: '![](img/ed3d78d2-a8a7-48af-9fe0-cf5931c54357.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed3d78d2-a8a7-48af-9fe0-cf5931c54357.png)'
- en: '*W[g]* is now a kernel that is convoluted with the input-output vector (which
    is usually the concatenation of two images). Of course, it''s possible to train
    any number of kernels to increase the decoding power of the cell and the output
    will have a shape equal to (*batch size × width × height × kernels*). This kind
    of cell is particularly useful for joining spatial processing with a robust temporal
    approach. Given a sequence of images (for example, satellite images, game screenshots,
    and so on), a convolutional LSTM network can learn long-term relationships that
    are manifested through geometric feature evolutions (for example, cloud movements
    or specific sprite strategies that it''s possible to anticipate considering a
    long history of events). This approach (even with a few modifications) is widely
    employed in Deep Reinforcement Learning in order to solve complex problems where
    the only input is provided by a sequence of images. Of course, the computational
    complexity is very high, in particular when many subsequent layers are used; however,
    the results outperformed any existing method and this approach became one of the
    first choices to manage this kind of problem.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*W[g]* 现在是一个与输入输出向量（通常是两个图像的拼接）卷积的核。当然，可以训练任意数量的核来增加单元的解码能力，输出将具有 (*批大小 × 宽度
    × 高度 × 核数*) 的形状。这种单元特别适用于将空间处理与鲁棒的时间方法相结合。给定一系列图像（例如，卫星图像、游戏截图等），卷积LSTM网络可以学习通过几何特征演变（例如，云的移动或可以基于长期事件历史预测的具体精灵策略）表现出的长期关系。这种方法（即使经过一些修改）在深度强化学习中得到了广泛应用，以解决仅由一系列图像提供输入的复杂问题。当然，计算复杂度非常高，尤其是在使用许多后续层时；然而，结果优于任何现有方法，这种方法成为管理这类问题的首选之一。'
- en: Another important variant, which is common to many Recurrent Neural Networks,
    is provided by a bidirectional interface. This isn't an actual layer, but a strategy
    that is employed in order to join the forward analysis of a sequence with the
    backward one. Two cellblocks are fed with a sequence and its inverse and the output,
    for example, is concatenated and used for further processing steps. In fields
    such as NLP, this method allows us to dramatically improve the accuracy of classifications
    and real-time translations. The reason is strictly related to the rules underlying
    the structure of a sequence. In natural language, a sentence *w[1] w[2] ... w[n]*
    has forward relationships (for example, a singular noun can be followed by *is*),
    but the knowledge of backward relationships (for example, the sentence *this place
    is pretty awful*) permits avoiding common mistakes that, in the past, had to be
    corrected using post-processing steps (the initial translation of *pretty* could
    be similar to the translation of *nice*, but a subsequent analysis can reveal
    that the adjective mismatches and a special rule can be applied). Deep learning,
    on the other side, is not based on *special* *rules*, but on the ability to learn
    an internal representation that should be autonomous in making final decisions
    (without further external aids) and bidirectional LSTM networks help in reaching
    this goal in many important contexts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的变体，这在许多循环神经网络中都很常见，是由双向接口提供的。这不是一个实际的层，而是一种策略，用于将序列的前向分析与其反向分析结合起来。两个cellblocks使用一个序列及其逆序作为输入，输出，例如，可以连接起来并用于后续处理步骤。在NLP等领域，这种方法可以显著提高分类和实时翻译的准确性。原因是严格与序列结构的规则有关。在自然语言中，一个句子*w[1]
    w[2] ... w[n]*有前向关系（例如，单数名词后面可以跟*is*），但了解反向关系（例如，句子*这个地方很糟糕*）可以避免过去需要通过后处理步骤纠正的常见错误（*pretty*的初始翻译可能与*nice*的翻译相似，但后续分析可以揭示形容词不匹配，并可以应用特殊规则）。另一方面，深度学习不是基于*特殊*的*规则*，而是基于学习内部表示的能力，这种表示应该能够自主做出最终决策（无需进一步的外部帮助），双向LSTM网络有助于在许多重要场景中实现这一目标。
- en: Keras implements the classes `LSTM` since its origins. It also provides a `Bidirectional`
    class wrapper that can be used with every RNN layer in order to obtain a double
    output (computed with the forward and backward sequences). Moreover, in Keras
    2 there are optimized versions of LSTM based on NVIDIA CUDA (`CuDNNLSTM`), which
    provide very high performance when a compatible GPU is available. In the same
    package, it's possible to also find the `ConvLSTM2D` class, which implements a
    convolutional LSTM layer. In this case, the reader can immediately identify many
    of the parameters, as they are the same as a standard convolutional layer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Keras自其起源就实现了`LSTM`类。它还提供了一个`Bidirectional`类包装器，可以与每个RNN层一起使用，以获得双重输出（使用正向和反向序列计算）。此外，在Keras
    2中，基于NVIDIA CUDA（`CuDNNLSTM`）的LSTM优化版本提供了非常高的性能，当有兼容的GPU时。在同一个包中，还可以找到`ConvLSTM2D`类，它实现了卷积LSTM层。在这种情况下，读者可以立即识别出许多参数，因为它们与标准卷积层相同。
- en: GRU
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GRU
- en: 'This model, named **Gated recurrent unit** (**GRU**), proposed by Cho et al.
    (in *Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, *Cho K.*, *Van Merrienboer B.*, *Gulcehre C.*, *Bahdanau
    D.*, *Bougares F.*, *Schwenk H.*, *Bengio Y.*, *arXiv:1406.1078 [cs.CL]*) can
    be considered as a simplified LSTM with a few variations. The structure of a generic
    full-gated unit is represented in the following diagram:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型被称为**门控循环单元**（**GRU**），由Cho等人提出（在《使用RNN编码器-解码器进行统计机器翻译学习短语表示》*Cho K.*，*Van
    Merrienboer B.*，*Gulcehre C.*，*Bahdanau D.*，*Bougares F.*，*Schwenk H.*，*Bengio
    Y.*，*arXiv:1406.1078 [cs.CL]*），可以看作是经过一些变体的简化LSTM。一个通用全门控单元的结构在以下图中表示：
- en: '![](img/bd472a98-66f5-4ac4-a1d2-506708f1f911.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd472a98-66f5-4ac4-a1d2-506708f1f911.png)'
- en: The main differences from LSTM are the presence of only two gates and the absence
    of an explicit state. These simplifications can speed both the training and the
    prediction phases while avoiding the vanishing gradient problem.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与LSTM相比，主要区别在于只有两个门和没有显式的状态。这些简化可以加快训练和预测阶段的速度，同时避免梯度消失问题。
- en: 'The first gate is called the **reset gate** (conventionally denoted with the
    letter *r*) and its function is analogous to the forget gate:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个门被称为**重置门**（通常用字母*r*表示），其功能与遗忘门类似：
- en: '![](img/b14262aa-5a81-4306-8c2e-53b870d730a5.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b14262aa-5a81-4306-8c2e-53b870d730a5.png)'
- en: 'Similar to the forget gate, its role is to decide what content of the previous
    output must be preserved and the relative degree. In fact, the additive contribution
    to new output is obtained as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与遗忘门类似，它的作用是决定前一个输出中必须保留的内容及其相对程度。事实上，新输出的附加贡献是通过以下方式获得的：
- en: '![](img/e32e648a-d38d-4a9d-acf2-8e70935ef64a.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e32e648a-d38d-4a9d-acf2-8e70935ef64a.png)'
- en: 'In the previous expression, I''ve preferred to separate the weight matrices
    to better exposes the behavior. The argument of *tanh(•)* is the sum of a linear
    function of the new input and a weighted term that is a function of the previous
    state. Now, it''s clear how the reset gate works: it modulates the amount of history
    (accumulated in the previous output value) that must be preserved and what instead
    can be discarded. However, the reset gate is not enough to determine the right
    output with enough accuracy, considering both short and long-term dependencies.
    In order to increase the expressivity of the unit, an update gate (with a role
    similar to the LSTM input gate) has been added:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，我更喜欢将权重矩阵分开，以便更好地展示其行为。*tanh(•)*的参数是新的输入的线性函数之和以及一个加权项，它是前一个状态的函数。现在，很明显重置门是如何工作的：它调节必须保留的历史量（累积在前一个输出值中）以及可以丢弃的内容。然而，仅重置门不足以以足够的准确性确定正确的输出，考虑到短期和长期依赖。为了提高单元的表达能力，添加了一个更新门（其作用类似于LSTM输入门）：
- en: '![](img/7af9d899-2abc-46e6-b917-70ceea3d9d7e.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7af9d899-2abc-46e6-b917-70ceea3d9d7e.png)'
- en: 'The update gate controls the amount of information that must contribute to
    the new output (and hence to the state). As it''s a value bounded between *0*
    and *1*, GRUs are trained to mix old output and new additive contribution with
    an operation similar to a weighted average:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门控制必须贡献给新输出的信息量（因此也影响到状态）。由于它是一个介于*0*和*1*之间的值，GRUs被训练通过类似于加权平均的操作来混合旧输出和新加的附加贡献：
- en: '![](img/efbd854c-0abc-4659-95c2-c75bf59aee89.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/efbd854c-0abc-4659-95c2-c75bf59aee89.png)'
- en: 'Therefore, the update gate becomes a modulator that can select which components
    of each flow must be output and stored for the next operation. This unit is structurally
    simpler than an LSTM, but several studies confirmed that its performance is on
    average, equivalent to LSTM, with some particular cases when GRU has even outperformed
    the more complex cell. My suggestion is that you test both models, starting with
    LSTM. The computational cost has been dramatically reduced by modern hardware
    and in many contexts the advantage of GRUs is negligible. In both cases, the philosophy
    is the same: the error is kept inside the cell and the weights of the gates are
    corrected in order to maximize the accuracy. This behavior prevents the multiplicative
    cascade of small gradients and increases the ability to learn very complex temporal
    behaviors.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更新门变成了一个调节器，可以选择每个流必须输出和存储以供下一次操作的部分。这个单元在结构上比LSTM简单，但几项研究表明，其性能平均来说与LSTM相当，在某些特定情况下，GRU甚至优于更复杂的细胞。我的建议是您测试这两种模型，从LSTM开始。现代硬件大大降低了计算成本，在许多情况下，GRUs的优势可以忽略不计。在这两种情况下，哲学是相同的：错误被保留在细胞内，门控的权重被纠正以最大化准确性。这种行为阻止了小梯度的乘法级联，并增加了学习非常复杂的时间行为的能力。
- en: However, a single cell/layer would not be able to successfully achieve the desired
    accuracy. In all these cases, it's possible to stack multiple layers made up of
    a variable number of cells. Every layer can normally output the last value or
    the entire sequence. The former is used when connecting the LSTM/GRU layer to
    a fully-connected one, while the whole sequence is necessary to feed another recurrent
    layer. We are going to see how to implement these techniques with Keras in the
    following example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，单个细胞/层无法成功实现所需的准确性。在这些所有情况下，可以通过堆叠由可变数量的细胞组成的多个层来实现。每一层通常可以输出最后一个值或整个序列。前者用于将LSTM/GRU层连接到全连接层时，而整个序列对于喂养另一个循环层是必要的。我们将在下面的例子中看到如何使用Keras实现这些技术。
- en: Just like for LSTMs, Keras implements the`GRU` class and its NVIDIA CUDA optimized
    version `CuDNNGRU`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 就像对于LSTMs一样，Keras实现了`GRU`类及其NVIDIA CUDA优化的版本`CuDNNGRU`。
- en: Example of an LSTM network with Keras
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的LSTM网络示例
- en: 'In this example, we want to test the ability of an LSTM network to learn long-term
    dependencies. For this reason, we employ a dataset called Zuerich Monthly Sunspots
    (freely provided by Andrews and Herzberg in 1985) containing the numbers observed
    in all the months starting from 1749 to 1983 (please read the information box
    for how to download the dataset). As we are not interested in the dates, we need
    to parse the file in order to extract only the values needed for the time series
    (which contains 2,820 steps):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们想测试LSTM网络学习长期依赖关系的能力。因此，我们使用了一个名为Zuerich月太阳黑子（由Andrews和Herzberg在1985年免费提供）的数据集，其中包含从1749年到1983年所有月份观察到的数值（请阅读信息框了解如何下载数据集）。由于我们不对日期感兴趣，我们需要解析文件以提取仅用于时间序列（包含2,820个步骤）所需的数据：
- en: '[PRE7]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Alternatively, it''s possible to load the CSV dataset using pandas ([https://pandas.pydata.org](https://pandas.pydata.org)),
    which is a powerful data manipulation/analysis library (for further information,
    please refer to *Learning pandas Second Edition, Heydt M., Packt*):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用pandas ([https://pandas.pydata.org](https://pandas.pydata.org)) 加载CSV数据集，这是一个强大的数据处理/分析库（有关更多信息，请参阅*《学习pandas第二版》，Heydt
    M.，Packt*）：
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The values are unnormalized and as LSTMs work with hyperbolic tangents, it''s
    helpful to normalize them in the interval `-1` and `1`. We can easily perform
    this step using the Scikit-Learn class `MinMaxScaler`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值未经归一化，由于LSTMs使用双曲正切函数，因此将它们归一化到区间`-1`和`1`是有帮助的。我们可以轻松地使用Scikit-Learn类`MinMaxScaler`执行此步骤：
- en: '[PRE9]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The complete dataset is shown in the following diagram:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的数据集如下所示：
- en: '![](img/98e25ffc-1ca5-43ea-85c1-b53cf62523ac.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98e25ffc-1ca5-43ea-85c1-b53cf62523ac.png)'
- en: 'In order to train the model, we have decided to use 2,300 samples for training
    and the remaining 500 for validation (corresponding to about 42 years). The input
    of the model is a batch of sequences of 15 samples (shifted along the time axis)
    and the output is the subsequent month; therefore, before training, we need to
    prepare the dataset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们决定使用2,300个样本进行训练，剩余的500个样本用于验证（对应约42年）。模型的输入是一个包含15个样本的序列批量（沿时间轴移动），输出是随后的月份；因此，在训练之前，我们需要准备数据集：
- en: '[PRE10]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we can create and compile a simple model with a single stateful LSTM layer
    containing four cells, followed by a hyperbolic tangent output neuron (I always
    suggest that the reader experiment with more complex architectures and different
    parameters):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建并编译一个简单的模型，该模型包含一个包含四个单元的单个状态化LSTM层，后面跟着一个双曲正切输出神经元（我总是建议读者尝试更复杂的架构和不同的参数）：
- en: '[PRE11]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Setting the `stateful=True` parameter in the `LSTM` class forces Keras not
    to reset the state after each batch. In fact, our goal is learning long-term dependencies
    and the internal LSTM state must reflect the overall trend. When an LSTM network
    is stateful, it''s also necessary to specify the batch size in the input shape
    (through the `batch_input_shape` parameter ). In our case, we have selected a
    batch size equal to 20 samples. The optimizer is `Adam` with a higher decay (to
    avoid instabilities) and a loss based on the mean squared error (which is the
    most common choice in this kind of scenario). At this point, we can train the
    model (for 100 epochs):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在`LSTM`类中将`stateful=True`参数设置为True强制Keras在每次批量后不重置状态。实际上，我们的目标是学习长期依赖关系，内部LSTM状态必须反映整体趋势。当LSTM网络是状态化的时，还必须在输入形状中指定批量大小（通过`batch_input_shape`参数）。在我们的例子中，我们选择了20个样本的批量大小。优化器是具有更高衰减（以避免不稳定性）的`Adam`，以及基于均方误差的损失（这是此类场景中最常见的选择）。到此为止，我们可以训练模型（100个周期）：
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is an example whose purpose is only didactic; therefore, the final validation
    mean squared error is not extremely low. However, as it''s possible to see in
    the following diagram (representing the predictions on the validation set), the
    model has successfully learned the global trend:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个仅用于教学目的的示例；因此，最终的验证均方误差并不特别低。然而，正如以下图表（表示验证集上的预测）所示，该模型已成功学习到全局趋势：
- en: '![](img/0e8c3134-ec30-489d-9909-926ead5df109.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0e8c3134-ec30-489d-9909-926ead5df109.png)'
- en: LSTM predictions on the Zuerich dataset
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在Zuerich数据集上的LSTM预测
- en: The model is still unable to achieve a very high accuracy in correspondence
    of all the very rapid spikes, but it's able to correctly model the amplitude of
    the oscillations and the length of the tails. For the sake of intellectual honesty,
    we must consider that this validation is performed on true data; however, when
    working with time series, it's normal to predict a new value using the ground
    truth. In this case, it's like a moving prediction where each value is obtained
    using the training history and a set of real observations. It's clear that the
    model is able to predict the long-term oscillations and also some local ones (for
    example, the sequence starting from step 300), but it can be improved in order
    to have better performance on the whole validation set. To achieve this goal,
    it is necessary to increase the network complexity and tune up the learning rate
    (it's a very interesting exercise on a real dataset).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 模型仍然无法在所有非常快速的峰值上达到非常高的精度，但它能够正确地模拟振荡的幅度和尾部的长度。为了保持学术诚信，我们必须考虑这种验证是在真实数据上进行的；然而，当处理时间序列时，使用真实值来预测新值是正常的。在这种情况下，它就像是一个移动预测，其中每个值都是使用训练历史和一组真实观察值获得的。很明显，模型能够预测长期振荡和一些局部振荡（例如，从步骤300开始的序列），但它可以通过提高整个验证集的性能来改进。为了实现这一目标，有必要增加网络复杂度并调整学习率（这是一个在真实数据集上非常有趣的练习）。
- en: Observing the previous diagram, it's possible to see that the model is relatively
    more accurate at some high frequencies (rapid changes), while it's more imprecise
    on others. This is not a strange behavior, because very oscillating functions
    *need more non-linearity* (think about the Taylor expansion and the relative error
    when it's truncated to a specific degree) to achieve high accuracies (this means
    employing more layers). My suggestion is that you repeat the experiment using
    more LSTM layers, considering that we need to pass the whole output sequence to
    the following recurrent layer (this can be achieved by setting the `return_sequences=True` parameter).
    The last layer, instead, must return only the final value (which is the default
    behavior). I also suggest testing the GRU layers, comparing the performance with
    the LSTM version and picking the simplest (benchmarking the training time) and
    most accurate solution.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 观察前面的图表，可以看到模型在某些高频（快速变化）的情况下相对更准确，而在其他情况下则更不精确。这不是一个奇怪的行为，因为非常振荡的函数需要更多的非线性（想想泰勒展开和截断到特定程度时的相对误差）来实现高精度（这意味着使用更多的层）。我的建议是，您使用更多的LSTM层重复实验，考虑到我们需要将整个输出序列传递到下一个循环层（这可以通过设置`return_sequences=True`参数来实现）。相反，最后一层必须只返回最终值（这是默认行为）。我还建议测试GRU层，将性能与LSTM版本进行比较，并选择最简单（基准测试训练时间）且最准确的解决方案。
- en: The dataset can be freely downloaded in CSV format from [https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line](https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以免费从[https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line](https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line)以CSV格式下载。
- en: Transfer learning
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: We have discussed how deep learning is fundamentally based on gray-box models
    that learn how to associate input patterns to specific classification/regression
    outcomes. All the processing pipeline that is often employed to prepare the data
    for specific detections is absorbed by the complexity of the neural architecture.
    However, the price to pay for high accuracies is a proportionally large number
    of training samples. State-of-the-art visual networks are trained with millions
    of images and, obviously, each of them must be properly labeled. Even if there
    are many free datasets that can be employed to train several models, many specific
    scenarios need hard preparatory work that sometimes is very difficult to achieve.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了深度学习在本质上是基于灰盒模型，这些模型学习如何将输入模式与特定的分类/回归结果关联起来。通常用于为特定检测准备数据的所有处理流程都被神经网络架构的复杂性所吸收。然而，为了获得高精度，需要付出代价，那就是成比例的大量训练样本。最先进的视觉网络使用数百万张图片进行训练，显然，每张图片都必须被正确标注。即使有许多可以用于训练多个模型的免费数据集，许多特定场景仍需要进行艰苦的准备工作，有时这些工作非常难以实现。
- en: Luckily, deep neural architectures are hierarchical models that learn in a structured
    way. As we have seen in the examples of deep convolutional networks, the first
    layers become more and more sensitive to detect low-level features, while the
    higher ones concentrate their work on extracting more detailed high-level features.
    In several tasks, it's reasonable to think that a network trained, for example,
    with a large visual dataset (such as ImageNet or Microsoft Coco) could be reused
    to achieve a specialization in a slightly different task. This concept is known
    as **transfer learning** and it's one of the most useful techniques when it's
    necessary to create state-of-the-art models with brand new datasets and specific
    objectives. For example, a customer can ask for a system to monitor a few cameras
    with the goal to segment the images and highlight the boundaries of specific targets.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，深度神经网络架构是层次化的模型，它们以结构化的方式进行学习。正如我们在深度卷积网络的例子中所看到的，第一层变得越来越敏感，能够检测到低级特征，而高层则专注于提取更详细的高级特征。在多个任务中，我们可以合理地认为，使用大型视觉数据集（例如ImageNet或Microsoft
    Coco）训练的神经网络可以被重新用于在略微不同的任务中实现专业化。这个概念被称为**迁移学习**，当需要使用全新的数据集和特定目标创建最先进的模型时，它是最有用的技术之一。例如，客户可以要求一个系统监控几个摄像头，目的是对图像进行分割并突出特定目标的边界。
- en: The input is made up of video frames with the same geometric properties as thousands
    of images employed in training very powerful models (for example, Inception, ResNet,
    or VGG); therefore, we can take a pre-trained model, remove the highest layers
    (normally dense ones ending in a softmax classification layer) and connect the
    flattening layer to an MLP that outputs the coordinates of the bounding boxes.
    The first part of the network can be *frozen* (the weights are not modified anymore),
    while the SGD is applied to tune up the weights of the newly specialized sub-network.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输入由具有与在训练非常强大的模型（例如Inception、ResNet或VGG）中使用的数千张图像相同的几何属性的视频帧组成；因此，我们可以取一个预训练的模型，移除最高层（通常是结束于softmax分类层的密集层），并将展平层连接到一个输出边界框坐标的多层感知器（MLP）。网络的第一个部分可以被*冻结*（权重不再修改），而随机梯度下降（SGD）被应用于调整新专业化的子网络的权重。
- en: Clearly, such an approach can dramatically speed up the training process, because
    the most complex part of the model is already trained and can also guarantee an
    extremely high accuracy (with respect to a naive solution), thanks to the optimization
    already performed on the original model. Obviously, the most natural question
    is how does this method work? Is there any formal proof? Unfortunately, there
    are no mathematical proofs, but there's enough evidence to assure about us of
    this approach. Generally speaking, the goal of a neural training process is to
    specialize each layer in order to provide a more particular (detailed, filtered,
    and so on) representation to the following one. Convolutional networks are a clear
    example of this behavior, but the same is observable in MLPs as well. The analysis
    of very deep convolutional networks showed how the content is still *visual* until
    reaching the flattening layer, where it's sent to a series of dense layers that
    are responsible for feeding the final softmax layer. In other words, the output
    of the convolutional block is a higher-level, segmented representation of the
    input, which is seldom affected by the specific classification problem. For this
    reason, transfer learning is generally sound and doesn't normally require a retraining
    of the lower layers. However, it's difficult to understand which model can yield
    the best performances and it's very useful to know which dataset has been used
    to train the original network. General purpose datasets (for example, ImageNet)
    are very useful in many contexts, while specific ones (such as Cifar-10 or Fashion;
    MNIST can be too restrictive). Luckily, Keras offers (in the package `keras.applications`)
    many models (even quite complex ones) that are always trained with ImageNet datasets
    and that can be immediately employed in a production-ready application. Even if
    using them is extremely simple, it requires a deeper knowledge of this framework,
    which is beyond the scope of this book. I invite the reader interested in this
    topic to check the book *Deep Learning with Keras, Gulli A., Pal S., Packt*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种方法可以显著加快训练过程，因为模型中最复杂的部分已经经过训练，并且可以保证极高的准确率（相对于原始的简单解决方案），这得益于对原始模型已经进行的优化。显然，最自然的问题就是这种方法是如何工作的？有没有任何正式的证明？不幸的是，没有数学证明，但已有足够的证据来保证我们采用这种方法。一般来说，神经训练过程的目的是专门化每一层，以便为下一层提供更特定的（详细、过滤等）表示。卷积网络是这种行为的明显例子，但在MLPs中也可以观察到同样的情况。对非常深的卷积网络的分析显示了内容在达到展平层之前仍然是*视觉的*，在展平层，内容被发送到一系列密集层，这些层负责向最终的softmax层提供数据。换句话说，卷积块输出的是输入的高级、分段表示，这很少受到特定分类问题的影响。因此，迁移学习通常是合理的，并且通常不需要重新训练底层。然而，很难理解哪种模型可以产生最佳性能，了解用于训练原始网络的哪个数据集非常有用。通用数据集（例如，ImageNet）在许多情况下非常有用，而特定数据集（如Cifar-10或Fashion；MNIST可能过于限制）。幸运的是，Keras在`keras.applications`包中提供了许多模型（甚至相当复杂），这些模型总是使用ImageNet数据集进行训练，并且可以立即用于生产就绪的应用。尽管使用它们非常简单，但需要更深入地了解这个框架，而这超出了本书的范围。我邀请对这一主题感兴趣的读者查阅由Gulli
    A.、Pal S.和Packt出版的《深度学习与Keras》一书。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have presented the concept of a deep convolutional network,
    which is a generic architecture that can be employed in any visual processing
    task. The idea is based on hierarchical information management, aimed at extracting
    the features starting from low-level elements and moving forward until the high-level
    details that can be helpful to achieve specific goals.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了深度卷积网络的概念，这是一种通用的架构，可以用于任何视觉处理任务。这个想法基于层次信息管理，旨在从低级元素开始提取特征，并逐步前进，直到达到有助于实现特定目标的高级细节。
- en: The first topic was the concept of convolution and how it's applied in discrete
    and finite samples. We discussed the properties of standard convolution, before
    analyzing some important variants such as atrous (or dilated convolution), separable
    (and depthwise separable) convolution and, eventually, transpose convolution.
    All these methods can work with 1D, 2D, and 3D samples, even if the most diffused
    applications are based on bidimensional (not considering the channels) matrices
    representing static images. In the same section, we also discussed how pooling
    layers can be employed to reduce the dimensionality and improve the robustness
    to small translations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主题是卷积的概念及其在离散和有限样本中的应用。我们讨论了标准卷积的性质，然后分析了某些重要的变体，例如空洞（或膨胀）卷积、可分离（和深度可分离）卷积，最终是转置卷积。所有这些方法都可以与1D、2D和3D样本一起工作，尽管最广泛的应用是基于二维（不考虑通道）矩阵，这些矩阵代表静态图像。在同一部分中，我们还讨论了如何使用池化层来降低维度并提高对微小平移的鲁棒性。
- en: In the next section, we introduced the concept of RNN, emphasizing the issues
    that normally arise when classic models are trained using the backpropagation
    through time algorithm. In particular, we explained why these networks cannot
    easily learn long-term dependencies. For this reason, new models have been proposed,
    whose performance was immediately outstanding. We discussed the most famous recurrent
    cell, called **L****ong-short-term memory** (**LSTM**), which can be used in layers
    that can easily learn all the most important dependencies of a sequence, allowing
    us to minimize the prediction error even in contexts with a very high variance
    (such as stock market quotations). The last topic was a simplified version of
    the idea implemented in LSTMs, which led to a model called a **Gated recurrent
    unit** (**GRU**). This cell is simpler and more computationally efficient, and
    many benchmarks confirmed that its performance is approximately the same as LSTM.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们介绍了循环神经网络（RNN）的概念，强调了当使用时间反向传播算法训练经典模型时通常会出现的问题。特别是，我们解释了为什么这些网络难以学习长期依赖。因此，提出了新的模型，其性能立即表现出色。我们讨论了最著名的循环单元，称为**长短期记忆**（**LSTM**），它可用于易于学习序列中所有最重要的依赖关系的层，即使在具有非常高的方差（如股票市场报价）的上下文中，也能最小化预测误差。最后一个主题是LSTMs中实现的想法的简化版本，这导致了名为**门控循环单元**（**GRU**）的模型。这个单元更简单，计算效率更高，许多基准测试证实其性能与LSTM大致相同。
- en: In the next chapter, [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml), *Autoencoders* we
    are going to discuss some particular models called autoencoders, whose main property
    is to create internal representations of an arbitrarily complex input distribution.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第11章](8022fb07-a741-4a0e-a965-6678577d3563.xhtml)，*自编码器*中，我们将讨论一些称为自编码器的特定模型，其主要特性是创建任意复杂输入分布的内部表示。
