- en: Advanced Neural Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we continue our pragmatic exploration of the world of deep
    learning, analyzing two very important elements: deep convolutional networks and
    **recurrent neural networks** (**RNN**). The former represents the most accurate
    and best performing visual processing technique for almost any purpose. Results
    like the ones obtained in fields such as real-time image recognition, self-driving
    cars, and Deep Reinforcement Learning have been possible thanks to the expressivity
    of this kind of network. On the other hand, in order to fully manage the temporal
    dimension, it is necessary to introduce advanced recurrent layers, whose performance
    must be greater than any other regression method. Employing these two techniques
    together with all the elements already discussed in the previous chapter makes
    it possible to achieve extraordinary results in the field of video processing,
    decoding, segmentation, and generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, in this chapter, we are going to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep convolutional networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutions, atrous convolutions, separable convolutions, and transpose convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling and other support layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM and GRU cells
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep convolutional networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, [Chapter 9](cb8a2a42-13fc-40ed-b6f0-56e4843284d7.xhtml), *Neural
    Networks for Machine Learning* we have seen how a multi-layer perceptron can achieve
    a very high accuracy when working with an complex image dataset that is not very
    complex, such as the MNIST handwritten digits one. However, as the fully-connected
    layers are *horizontal*, the images, which in general are three-dimensional structures
    (*width × height × channels*), must be flattened and transformed into one-dimensional
    arrays where the geometric properties are definitively lost. With more complex
    datasets, where the distinction between classes depends on more details and on
    their relationships, this approach can yield moderate accuracies, but it can never
    reach the precision required by production-ready applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The conjunction of neuroscientific studies and image processing techniques
    suggested experimenting with neural networks where the first layers work with
    bidimensional structures (without the channels), trying to extract a hierarchy
    of features that are strictly dependent on the geometric properties of the image.
    In fact, as confirmed by neuroscientific research about the visual cortex, a human
    being doesn''t decode an image directly. The process is sequential and starts by
    detecting low-level elements such as lines are orientations; progressively, it
    proceeds by focusing on sub-properties that define more and more complex shapes,
    different colors, structural features, and so on, until the amount of information
    is enough to resolve any possible ambiguity (for further scientific details, I
    recommend the book *Vision and Brain: How We Perceive the World, Stone J. V.,
    MIT Press*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can image the decoding process of an eye as a sequence made
    up of these filters (of course, this is only a didactic example): directions (dominant
    horizontal dimension), a central circle inside an ellipsoidal shape, a darker
    center (pupil) and a clear background (bulb), a smaller darker circle in the middle
    of the pupil, the presence of eyebrows, and so on. Even if the process is not
    biologically correct, it can be considered as a reasonable hierarchical process
    where a higher level sub-feature is obtained after a lower-level filtering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has been synthesized using the bidimensional convolutional operator,
    which was already known as a powerful image processing tool. However, in this
    case, there''s a very important difference: the structure of the filters is not
    pre-imposed but learned by the network using the same back-propagation algorithm
    employed for MLPs. In this way, the model can adapt the weights considering a
    final goal (which is the classification output), without taking into account any
    pre-processing steps. In fact, a deep convolutional network, more than an MLP,
    is based on the concept of end-to-end learning, which is a different way to express
    what we have described before. The input is the source; in the middle, there''s
    a flexible structure; and, at the end, we define a global cost function, measuring
    the accuracy of the classification. The learning process has to back-propagate
    the errors and correct the weights to reach a specific goal, but we don''t know
    exactly how this process works. What we can easily do is analyze the structure
    of the filters at the end of the learning phase, discovering that the network
    has specialized the first layers on low-level details (such as orientations) and
    the last ones on high-level, sometimes recognizable, ones (such as the components
    of a face). It''s not surprising that such models achieved state-of-the-art performance
    in tasks such as image recognition, segmentation (detecting the boundaries of
    different parts composing an image), and tracking (detecting the position of moving
    objects). Nevertheless, deep convolutional networks have become the first block
    of many different architectures (such as deep reinforcement learning or neural
    style transfer) and, even with a few known limitations, continue to be the first
    choice for solving several complex real-life problems. The main drawback of such
    models (which is also a common objection) is that they require very large datasets
    to reach high accuracies. All the most important models are trained with millions
    of images and their generalization ability (that is, the main goal) is proportional
    to the number of different samples. There were researchers who noticed that a
    human being learns to generalize without this huge amount of experience and, in
    the coming decades, we are likely to observe improvements under this viewpoint.
    However, deep convolutional networks have revolutionized many Artificial Intelligence
    fields, allowing results that were considered almost impossible just a few years
    ago.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to discuss different kinds of convolutions and
    how they can be implemented using Keras; therefore, for specific technical details
    I continue suggesting to check the official documentation and the book *Deep Learning
    with Keras, Gulli A, Pal S., Packt*.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even if we work only with finite and discrete convolutions, it''s useful to
    start providing the standard definition based on integrable functions. For simplicity,
    let''s suppose that *f(τ)* and *k(τ)* are two real functions of a single variable
    defined in *ℜ*. The convolution of *f(τ)* and *k(τ)* (conventionally denoted as
    *f ∗ k*), which we are going to call kernel, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afdf56cb-4be5-4a1b-ae5c-b189cb096e04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression may not be very easy to understand without a mathematical background,
    but it can become exceptionally simple with a few considerations. First of all,
    the integral sums over all values of *τ*; therefore, the convolution is a function
    of the remaining variable, *t*. The second fundamental element is a sort of dynamic
    property: the kernel is reversed (*-**τ*) and transformed into a function of a
    new variable *z = t - τ*. Without deep mathematical knowledge, it''s possible
    to understand that this operation shifts the function along the *τ* (independent
    variable) axis. In the following graphs, there''s an example based on a parabola:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17860d83-4962-4dfb-8c8c-e33af0dbdc24.png)'
  prefs: []
  type: TYPE_IMG
- en: The first diagram is the original kernel (which is also symmetric). The other
    two plots show, respectively, a forward and a backward shift. It should be clearer
    now that a convolution multiplies the function *f(τ**)* times the shifted kernel
    and computes the area under the resulting curve. As the variable *t* is not integrated,
    the area is a function of *t* and defines a new function, which is the convolution
    itself. In other words, the value of convolution of *f(τ)* and *k(τ)* computed
    for *t = 5* is the area under the curve obtained by the multiplication *f(τ)k(5
    - τ)*. By definition, a convolution is commutative *(f ∗ k = k ∗ f)* and distributive
    *(f ∗ (k + g) = (f ∗ k) + (f ∗ g))*. Moreover, it's also possible to prove that
    it's associative *(f ∗ (k ∗ g) = (f ∗ k) ∗ g)*.
  prefs: []
  type: TYPE_NORMAL
- en: However, in deep learning, we never work with continuous convolutions; therefore,
    I omit all the properties and mathematical details, focusing the attention on
    the discrete case. The reader who is interested in the theory can find further
    details in *Circuits, Signals, and Systems, Siebert W. M., MIT Press*. A common
    practice is, instead, to stack multiple convolutions with different kernels (often
    called filters), to transform an input containing *n* channels into an output
    with *m* channels, where *m* corresponds to the number of kernels. This approach
    allows the unleashing of the full power of convolutions, thanks to the synergic
    actions of different outputs. Conventionally, the output of a convolution layer
    with *n* filters is called a **feature map** (*w^((t)) × h^((t)) × n*), because
    its structure is no longer related to a specific image but resembles the overlap
    of different feature detectors. In this chapter, we often talk about images (considering
    a hypothetical first layer), but all the considerations are implicitly extended
    to any feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Bidimensional discrete convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most common type of convolution employed in deep learning is based on bidimensional
    arrays with any number of channels (such as grayscale or RGB images). For simplicity,
    let''s analyze a single layer (channel) convolution because the extension to *n*
    layers is straightforward. If *X ∈ ℜ^(w × h)* and *k ∈ ℜ^(n × m)*, the convolution
    *X ∗ k* is defined as (the indexes start from 0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/122c8b29-9cee-4e07-97f6-15a9949ddc58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s clear that the previous expression is a natural derivation of the continuous
    definition. In the following graph, there''s an example with a 3 × 3 kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/745e26a6-355c-4895-96be-d34f88cc1ad9.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of bidimensional convolution with a 3x3 kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel is shifted horizontally and vertically, yielding the sum of the
    element-wise multiplication of corresponding elements. Therefore, every operation
    leads to the output of a single pixel. The kernel employed in the example is called
    the **discrete Laplacian** **operator** (because it''s obtained by discretizing
    the real Laplacian); let''s observe the effect of this kernel on a complete greyscale
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84f68b67-d2b8-47c5-993a-8f58baca3ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of convolution with a Discrete Laplacian Kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to notice, the effect of the convolution is to emphasize
    the borders of the various shapes. The reader can now understand how variable
    kernels can be tuned up in order to fulfill precise requirements. However, instead
    of trying to do it manually, a deep convolutional network leaves this tasks to
    the learning process, which is subject to a precise goal expressed as the minimization
    of a cost function. A parallel application of different filters yields complex
    overlaps that can simplify the extraction of those features that are really important
    for a classification. The main difference between a fully-connected layer and
    a convolutional one is the ability of the latter to work with an existing geometry,
    which encodes all the elements needed to distinguish an object from another one.
    These elements cannot be immediately generalizable (think about the branches of
    a decision tree, where a split defines a precise path towards a final class),
    but require subsequent processing steps to perform a necessary disambiguation.
    Considering the previous photo, for example, eyes and nose are rather similar.
    How is it possible to segment the picture correctly? The answer is provided by
    a double analysis: there are subtle differences that can be discovered by fine-grained
    filters and, above all, the global geometry of real objects is based on internal
    relationships that are almost invariant. For example (only for didactic purposes),
    eyes and nose should make up an isosceles triangle, because the symmetry of a
    face implies the same distance between each eye and the nose. This consideration
    can be made *apriori*, like in many visual processing techniques, or, thanks to
    the power of deep learning, it can be left to the training process. As the cost
    function and the output classes implicitly control the differences, a deep convolutional
    network can learn what is important to reach a specific goal, discarding at the
    same time all those details that are useless.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we have said that the feature extraction process is
    mainly hierarchical. Now, it should be clear that different kernel sizes and subsequent
    convolutions achieve exactly this objective. Let's suppose that we have a *100 ×
    100* image and a (*3 × 3*) kernel. The resulting image will be *98 × 98* pixels
    (we will explain this concept later). However, each pixel encodes the information
    of a *3 × 3* block and, as these blocks are overlapping, two consecutive pixels
    will share some knowledge but, at the same time, they emphasize the difference
    between the corresponding blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, the same Laplacian Kernel is applied to a simple
    white square on a black background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bdaea9e-da79-41c1-9e16-aeac602803cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Orginal image (left); convolution with Laplacian kernel result (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if the image is very simple, it''s possible to notice that the result
    of a convolution enriched the output image with some very important pieces of
    information: the borders of the square are now clearly visible (they are black
    and white) and they can be immediately detected by thresholding the image. The
    reason is straightforward: the effect of the kernel on the compact surfaces is
    compact too but, when the kernel is shifted upon the border, the effect of the
    difference becomes visible. Three adjacent pixels in the original image can be
    represented as (*0, 1, 1*), indicating the horizontal transition between black
    and white. After the convolution, the result is approximately (*0.75, 0.0, 0.25*).
    All the original black pixels have been transformed into a light gray, the white
    square became darker, and the border (which is not marked in the original picture)
    is now black (or white, depending on the shift direction). Reapplying the same
    filter to the output of the previous convolution, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40f86477-69d4-4e53-966c-a26b30dba51a.png)'
  prefs: []
  type: TYPE_IMG
- en: Second application of the Laplacian kernel
  prefs: []
  type: TYPE_NORMAL
- en: 'A sharp eye can immediately notice three results: the compact surfaces (black
    and white) are becoming more and more similar, the borders are still visible,
    and, above all, the top and lower left corners are now more clearly marked with
    white pixels. Therefore, the result of the second convolution added a finer-grained
    piece of information, which was much more difficult to detect in the original
    image. Indeed, the effect of the Laplacian operator is very straightforward and
    it''s useful only for didactic purposes. In real deep convolutional networks,
    the filters are trained to perform more complex processing operations that can
    reveal details (together with their internal and external relationships) that
    are not immediately exploited to classify the image. Their isolation (obtained
    thanks to the effect of many parallel filters) allows the network to mark similar
    elements (like the corners of the square) in a different way and make more accurate
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this example is to show how a sequence of convolutions allows
    the generation of a hierarchical process that will extract coarse-grained features
    at the beginning and very high-level ones at the end, without losing the information
    already collected. Metaphorically, we could say that a deep convolutional network
    starts placing labels indicating lines, orientations, and borders and proceeds
    by enriching the existing ontology with further details (such as corners, particular
    shapes, and so on). Thanks to this ability, such models can easily outperform
    any MLP and reach almost to the Bayes level if the number of training samples
    is large enough. The main drawback of this models is their inability to easily
    recognize objects after the application of affine transformations (such as rotations
    or translations). In other words, if a network is trained with a dataset containing
    only faces in their natural position, it will achieve poor performance when a
    rotated (or upside-down) sample is presented. In the next sections, we are going
    to discuss a couple of methods that are helpful for mitigating this problem (in
    the case of translations); however, a new experimental architecture called a **capsule
    network** (which is beyond the scope of this book) has been proposed in order
    to solve this problem with a slightly different and much more robust approach
    (the reader can find further details in *Dynamic Routing Between Capsules, Sabour
    S., Frosst N., Hinton G. E., arXiv:1710.09829 [cs.CV]*).
  prefs: []
  type: TYPE_NORMAL
- en: Strides and padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Two important parameters common to all convolutions are **padding** and **strides**.
    Let's consider the bidimensional case, but keep in mind that the concepts are
    always the same. When a kernel (*n × m* with *n, m > 1*) is shifted upon an image
    and it arrives at the end of a dimension, there are two possibilities. The first
    one, called **valid padding**, consists of not continuing even if the resulting
    image is smaller than the original. In particular, if *X* is a *w × h* matrix,
    the resulting convolution output will have dimensions equal to *(w - n + 1) ×
    (h - m + 1)*. However, there are many cases when it's useful to keep the original
    dimensions, for example, to be able to sum different outputs. This approach is
    called **same padding** and it's based on the simple idea to add *n - 1* blank
    columns and *m - 1* blank rows to allow the kernel to shift over the original
    image, yielding a number of pixels equal to the initial dimensions. In many implementations,
    the default value is set to valid padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other parameter, called **strides**, defines the number of pixels to skip
    during each shift. For example, a value set to (*1, 1*) corresponds to a standard
    convolution, while strides set to (*2, 1*) are shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc335fd3-8153-4689-be9f-ecec0a4cfcd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of bidimensional convolution with strides=2 on the x-axis
  prefs: []
  type: TYPE_NORMAL
- en: In this case, every horizontal shift skips a pixel. Larger strides force a dimensionality
    reduction when a high granularity is not necessary (for example, in the first
    layers), while strides set to (*1, 1*) are normally employed in the last layers
    to capture smaller details. There are no standard rules to find out the optimal
    value and testing different configurations is always the best approach. Like any
    other hyperparameter, too many elements should be taken into account when determining
    whether a choice is acceptable or not; however, some general pieces of information
    about the dataset (and therefore about the underlying data generating process)
    can help in making a reasonable initial decision. For example, if we are working
    with pictures of buildings whose dimension is vertical, it's possible to start
    picking a value of (*1, 2*), because we can assume that there's more informative
    redundancy in the *y*-axis than in the *x*-axis. This choice can dramatically
    speed up the training process, as the output has one dimension, which is half
    (with the same padding) of the original one. In this way, larger strides produce
    a partial denoising and can improve the training speed. At the same time, the
    information loss could have a negative impact on the accuracy. If that happens,
    it probably means that the scale isn't high enough to allow skipping some elements
    without compromising the *semantics*. For example, an image with very small faces
    could be irreversibly *damaged* with large strides, yielding an inability to detect
    the right feature and a consequent worsening of the classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Atrous convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In some cases, a stride larger than one could be a good solution because it
    reduces the dimensionality and speeds up the training process, but it can lead
    to distorted images where the main features are not detectable anymore. An alternative
    approach is provided by the **atrous convolution** (also known as **dilated convolution**).
    In this case, the kernel is applied to a larger image patch, but skips some pixels
    inside the area itself (that''s why someone called it convolution with holes).
    In the following graph, there''s an example with (*3 × 3*) and dilation rate set
    to *2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c326924a-c20e-400f-b3ab-0e210a2a3940.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of atrous convolution with a Laplacian kernel
  prefs: []
  type: TYPE_NORMAL
- en: Every patch is now *9 **× 9*, but the kernel remains a *3 × 3* Laplacian operator.
    The effect of this approach is more robust than increasing the strides because
    the kernel *perimeter* will always contain a group of pixels with the same geometrical
    relationships. Of course, fine-grained features could be distorted, but as the
    strides are normally set to (*1, 1*), the final result is normally more coherent.
    The main difference with a standard convolution is that in this case, we are assuming
    that farther elements can be taken into account to determine the nature of an
    output pixel. For example, if the main features don't contain very small details,
    an atrous convolution can consider larger areas, focusing directly on elements
    that a standard convolution can detect only after several operations. The choice
    of this technique must be made considering the final accuracy, but just like for
    the strides, it can be considered from the beginning whenever the geometric properties
    can be detected more efficiently, considering larger patches with a few representative
    elements. Even if this method can be very effective in particular contexts, it
    isn't normally the first choice for very deep models. In the most important image
    classification models, standard convolutions (with or without larger strides)
    are employed because they have been proven to yield the best performance with
    very generic datasets (such as ImageNet or Microsoft Coco). However, I suggest
    the reader experiment with this method and compare the results. In particular,
    it would be a good idea to analyze which classes are better classified and try
    to find a rational explanation for the observed behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In some frameworks, such as Keras, there are no explicit layers to define an
    atrous convolution. Instead, a standard convolutional layer normally has a parameter
    to define the dilation rate (in Keras, it's called `dilation_rate`). Of course,
    the default value is 1, meaning that the kernel will be applied to patches matching
    its size.
  prefs: []
  type: TYPE_NORMAL
- en: Separable convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we consider an image *X ∈ ℜ^(w × h)* (single channel) and a kernel *k ∈ ℜ**^(n
    × m)*, the number of operations is *nmwh*. When the kernel is not very small and
    the image is large, the cost of this computation can be quite high, even with
    GPU support. An improvement can be achieved by taking into account the associated
    property of convolutions. In particular, if the original kernel can be split into
    the dot product of two vectorial kernels, *k^((1))* with dimensions (*n × 1*)
    and *k^((2))* with dimensions (*1 × m*), the convolution is said to be **separable**.
    This means that we can perform a (*n **× m*) convolution with two subsequent operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f804d5ab-40c8-4045-bcaf-c28d1f76566b.png)'
  prefs: []
  type: TYPE_IMG
- en: The advantage is clear, because now the number of operations is *(n + m)wh*.
    In particular, when *nm >> n + m*, it's possible to avoid a large number of multiplications
    and speed up both the training and the prediction process.
  prefs: []
  type: TYPE_NORMAL
- en: 'A slightly different approach has been proposed in *Xception: Deep Learning
    with Depthwise Separable Convolutions, Chollet F., arXiv:1610.02357 [cs.CV]*.
    In this case, which is properly called **depthwise separable convolution**, the
    process is split into two steps. The first one operates along the channel axis,
    transforming it into a single dimensional map with a variable number of channels
    (for example, if the original diagram is *768 × 1024 × 3*, the output of the first
    stage will be *n × 768 × 1024 **× 1*). Then, a standard convolution is applied
    to the single layer (which can have indeed more than one channel). In the majority
    of implementations, the default number of output channels for the depthwise convolution
    is 1 (this is conventionally expressed by saying that the **depth multiplier**
    is 1). This approach allows a dramatic parameter reduction with respect to a standard
    convolution. In fact, if the input generic feature map is *X ∈ ℜ^(w × h × p)* and
    we want to perform a standard convolution with *q* kernels *k^((i)) ∈ ℜ^(n × m)*,
    we need to learn *nmqp* parameters (each kernel *k^((i))* is applied to all input
    channels). Employing the Depthwise Separable Convolution, the first step (working
    with only the channels) requires *nmp* parameters. As the output has still *p*
    feature maps and we need to output *q* channels, the process employs a *trick*: processing
    each feature map with *q 1 × 1* kernels (in this way, the output will have *q*
    layers and the same dimensions). The number of parameters required for the second
    step is *pq*, so the total number of parameters becomes *nmp + pq*. Comparing
    this value with the one required for a standard convolution, we obtain an interesting
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82c2ffc6-4b01-480f-b748-b47768c4643d.png)'
  prefs: []
  type: TYPE_IMG
- en: As this condition is easily true, this approach is extremely effective in optimizing
    the training and prediction processes, as well as the memory consumption in any
    scenario. It's not surprising that the Xception model has been immediately implemented
    in mobile devices, allowing real-time image classification with very limited resources.
    Of course, depthwise separable convolutions don't always have the same accuracy
    as standard ones, because they are based on the assumption that the geometrical
    features observable inside a channel of a composite feature map are independent
    of each other. This is not always true, because we know that the effect of multiple
    layers is based also on their combinations (which increases the expressivity of
    a network). However, in many cases the final result has an accuracy comparable
    to some state-of-the-art models; therefore, this technique can very often be considered
    as a valid alternative to a standard convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Since version 2.1.5, Keras has introduced a layer called `DepthwiseConv2D` that
    implements a depthwise separable convolution. This layer extends the existing
    `SeparableConv2D`.
  prefs: []
  type: TYPE_NORMAL
- en: Transpose convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **transpose convolution** (sometimes wrongly called deconvolution, even if
    the mathematical definition is different) is not very different from a standard
    convolution, but its goal is to rebuild a structure with the same features as
    the input sample. Let's suppose that the output of a convolutional network is
    the feature map *X ∈ ℜ^(w' × h' × p)* and we need to build an output element *Y ∈ ℜ^(w ×
    h × 3)* (assuming the w and h are the original dimensions). We can achieve this
    result by applying a transpose convolution with appropriate strides and padding
    to *X*. For example, let's suppose that *X ∈ ℜ^(128 × 128 × 256)* and our output
    must be *512 × 512 × 3*. The last transpose convolution must learn three filters
    with strides set to four and same padding. We are going to see some practical
    examples of this method in the next chapter [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml),
    *Autoencoders* when discussing autoencoders; however, there are no very important
    differences between transpose and standard convolution in terms of internal dynamics.
    The main difference is the cost function, because when a transpose convolution
    is used as the last layer, the comparison must be done between a target image
    and a reconstructed one. In the next chapter,  [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml), *Autoencoders* 
    we are also going to analyze some techniques to improve the quality of the output
    even when the cost function doesn't focus on specific areas of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a deep convolutional network, **pooling layers** are extremely useful elements.
    There are mainly two kinds of these structures: **max pooling** and **average
    pooling**. They both work on patches *p ∈ ℜ^(n × m)*, shifting horizontally and
    vertically according to the predefined stride value and transforming the patches
    into single pixels according to the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df1b03d7-3268-41ad-bf78-7174796f2eda.png)'
  prefs: []
  type: TYPE_IMG
- en: There are two main reasons that justify the use of these layers. The first one
    is a dimensionality reduction with limited information loss (for example, setting
    the strides to (*2, 2*), it's possible to halve the dimensions of an image/feature
    map). Clearly, all pooling techniques can be more or less lossy (in particular
    max pooling) and the specific result depends on the single image. In general,
    pooling layers try to summarize the information contained in a small chunk into
    a single pixel. This idea is supported by a perceptual-oriented approach; in fact,
    when the pools are not too large, it's rather unlikely to find high variances
    in subsequent shifts (natural images have very few isolated pixels). Therefore,
    all the pooling operations allow us to set up strides greater than one with a
    mitigated risk of compromising the information content. However, considering several
    experiments and architectures, I suggest that you set up larger strides in the
    convolutional layers (in particular, in the first layer of a convolutional sequence)
    instead of in pooling ones. In this way, it's possible to apply the transformation
    with a minimum loss and to fully exploit the next fundamental property.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second (and probably the most important) reason is that they slightly increase
    the robustness to translations and limited distortions with an effect that is
    proportional to the pool size. Let''s consider the following diagram, representing
    an original image of a cross and the version after a 10-pixel diagonal translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58f9f3ec-b9c2-4faa-8c2a-cf383b8ddb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Original image (left); diagonally translated image (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a very simple example and the translated image is not very different
    from the original one. However, in a more complex scenario, a classifier could
    also fail to correctly classify an object in similar conditions. Applying a max
    pooling (with a (*2 × 2*) pool size and 2-pixel strides) on the translated image,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4791bc89-b259-4977-9435-335bd3afcb4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Original image (left); result of a max pooling on the translated image (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is a larger cross, whose arms are slightly more aligned to the axis.
    When compared with the original image, it''s easier for a classifier with a good
    generalization ability to filter out the spurious elements and recognize the original
    shape (which can be considered a cross surrounded by a noisy frame). Repeating
    the same experiment with average pooling (same parameters), we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce0227d0-0e00-477e-b0b4-9363e8e003f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Original image (left); result of an average pooling on the translated image
    (right)
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the picture is partially smoothed, but it's still possible to
    see a better alignment (thanks mainly to the fading effect). Also, if these methods
    are simple and somewhat effective, the robustness to invariant transformations
    is never dramatically improved and higher levels of invariance are possible only
    by increasing the pool size. This choice leads to coarser-grained feature maps
    whose amount of information is drastically reduced; therefore, whenever it's necessary
    to extend the classification to samples that can be distorted or rotated, it can
    be a good idea (which allows working with a dataset that better represents the
    real data generating process) to use a data augmentation technique to produce
    artificial images and to also train the classifier on them. However, as pointed
    out in *Deep Learning, Goodfellow I., Bengio Y.,‎ Courville A., MIT Press*, pooling
    layers can also provide a robust invariance to rotations when they are used together
    with the output of a multiple convolution layer or a rotated image stack. In fact,
    in these cases, a single pattern response is elicited and the effect of the pooling
    layer becomes similar to a collector that standardizes the output. In other words,
    it will produce the same result without an explicit selection of the best matching
    pattern. For this reason, if the dataset contains enough samples, pooling layers
    in intermediate positions of the network can provide a moderate robustness to
    small rotations, increasing the generalization ability of the whole deep architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As it's easy to see in the previous example, the main difference between the
    two variants is the final result. Average pooling performs a sort of very simple
    interpolation, smoothing the borders and avoiding abrupt changes. On the other
    hand, max pooling is less noisy and can yield better results when the features
    need to be detected without any kind of smoothing (which could alter their geometry).
    I always suggest testing both techniques, because it's almost impossible to pick
    the best method with the right pool size according only to heuristic considerations
    (above all, when the datasets are not made up of very simple images).
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, it's always preferable to use these layers after a group of convolutions,
    avoiding very large pool sizes that can irreversibly destroy the information content.
    In many important deep architectures, the pooling layers are always based on (*2,
    2*) or (*3, 3*) pools, independently of their position, and the strides are always
    set to 1 or 2. In both cases, the information loss is proportional to the pool
    size/strides; therefore, large pools are normally avoided when small features
    must be detected together with larger ones (for example, foreground and background
    faces).
  prefs: []
  type: TYPE_NORMAL
- en: Other useful layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even if convolution and pooling layers are the backbone of almost all deep
    convolutional networks, other layers can be helpful to manage specific situations.
    They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Padding layers**: These can be employed to increase the size of a feature
    map (for example, to align it with another one) by surrounding it with a blank
    frame (*n* black pixels are added before and after each side).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upsampling layers**: These increase the size of a feature map by creating
    larger blocks out of a single pixel. To a certain extent, they can be considered
    as a transformation opposite to a pooling layer, even if, in this case, the upsampling
    is not based on any kind of interpolation. These kinds of layers can be used to
    prepare the feature maps for transformations similar to the ones obtained with
    a transpose convolution, even if many experiments confirmed that using larger
    strides can yield very accurate results without the need of an extra computational
    step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cropping layers**: These are helpful for selecting specific rectangular areas
    of an image/feature map. They are particularly useful in modular architectures,
    where the first part determines the cropping boundaries (for example, of a face),
    while the second part, after having removed the background, can perform high-level
    operations such as detail segmentation (marking the areas of eyes, nose, mouth,
    and so on). The possibility of inserting these layers directly into a deep neural
    model avoids multiple data transfers. Unfortunately, many frameworks (such as
    Keras) don''t allow us to use variable boundaries, limiting *de facto* the number
    of possible use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flattening layers:** These are the conjunction link between feature maps
    and fully-connected layers. Normally, a single flattening layer is used before
    processing the output of the convolutional blocks, with a few dense layers terminating
    in a final Softmax layer (for classifications). The operation is computationally
    very cheap as it works only with the metadata and doesn''t perform any calculations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of deep convolutional networks with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first example, we want to consider again the complete MNIST handwritten
    digit dataset, but instead of using an MLP, we are going to employ a small deep
    convolutional network. The first step consists of loading and normalizing the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now define the model architecture. The samples are rather small (*28 ×
    28*); therefore it can be helpful to use small kernels. This is not a general
    rule and it''s useful to also evaluate larger kernels (in particular in the first
    layers); however, many state-of-the-art architectures confirmed large kernel sizes
    with small images can lead to a performance loss. In my personal experiments,
    I''ve always obtained the best results when the largest kernels were *8 ÷ 10*
    smaller than the image dimensions. Our model is made up of the following layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Input dropout 25%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolution with 16 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding (the default weight initializer is Xavier). Keras implements
    the `Conv2D` class, whose main parameters are immediately understandable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout 50%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolution with 32 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout 50%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average pooling with (2 × 2) pool size and strides equal to 1 (using the Keras
    class `AveragePooling2D`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolution with 64 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average pooling with (2 × 2) pool size and strides equal to 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolution with 64 filters, (3 × 3) kernel, strides equal to 1, ReLU activation,
    and the same padding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout 50%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Average pooling with (2 × 2) pool size and strides equal to 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully-connected layer with 1024 ReLU units.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout 50%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully-connected layer with 10 Softmax units.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The goal is to capture the low-level features (horizontal and vertical lines,
    intersections, and so on) in the first layers and use the pooling layers and all
    the subsequent convolutions to increase the accuracy when distorted samples are
    presented. At this point, we can create and compile the model (using the Adam
    optimizer with *η = 0.001* and a decay rate equal to *10^(-5)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now proceed to train the model with 200 epochs and a batch size of 256
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The final validation accuracy is now `0.9950`, which means that only 50 samples
    (out of 10,000) have been misclassified. To better understand the behavior, we
    can plot the accuracy and loss diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6947af3-e86a-4e5b-a396-6164e4045a70.png)'
  prefs: []
  type: TYPE_IMG
- en: As it's possible to see, both validation accuracy and loss easily reach the
    optimal values. In particular, the initial validation accuracy is about 0.97 and
    the remaining epochs are necessary to improve the performance with all those samples,
    whose shapes can lead to confusion (for example, malformed 8s that resemble 0s,
    or 7s that are very similar to 1s). It's evident that the *geometric* approach
    employed by convolutions guarantees a much higher robustness than a standard fully-connected
    network, thanks also to the contribution of pooling layers, which reduce the variance
    due to noisy samples.
  prefs: []
  type: TYPE_NORMAL
- en: Example of a deep convolutional network with Keras and data augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to use the Fashion MNIST dataset, which was freely
    provided by Zalando as a more difficult replacement for the standard MNIST dataset.
    In this case, instead of handwritten digits, there are greyscale photos of different
    articles of clothing. An example of a few samples is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f938c3a-e85d-4b71-82c0-b28a23358c2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in this case, we want to employ a utility class provided by Keras
    (`ImageDataGenerator`) in order to create a data-augmented sample set to improve
    the generalization ability of the deep convolutional network. This class allows
    us to add random transformations (such as standardization, rotations, shifting,
    flipping, zooming, shearing, and so on) and output the samples using a Python
    generator (with an infinite loop). Let''s start loading the dataset (we don''t
    need to standardize it, as this transformation is performed by the generator):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can create the generators, selecting the transformation that
    best suits our case. As the dataset is rather *standard* (all the samples are
    represented only in a few positions), we''ve decided to augment the dataset by
    applying a sample-wise standardization (which doesn''t rely on the entire dataset),
    horizontal flip, zooming, small rotations, and small shears. This choice has been
    made according to an objective analysis, but I suggest the reader repeat the experiment
    with different parameters (for example, adding whitening, vertical flip, horizontal/vertical
    shifting, and extended rotations). Of course, increasing the augmentation variability
    needs larger processed sets. In our case, we are going to use 384,000 training
    samples (the original size is 60,000), but larger values can be employed to train
    deeper networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once an image data generator has been initialized, it must be fitted, specifying
    the input dataset and the desired batch size (the output of this operation is
    the actual Python generator). The test image generator is voluntarily kept without
    transformations except for normalization and standardization, in order to avoid
    a validation on a dataset drawn from a different distribution. At this point,
    we can create and compile our network, using 2D convolutions based on Leaky ReLU
    activations (using the `LeakyReLU` class, which replaces the standard layer `Activation`),
    batch normalizations, and max poolings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'All the batch normalizations are always applied to the linear transformation
    before the activation function. Considering the additional complexity, we are
    also going to use a callback, which is a class that Keras uses in order to perform
    in-training operations. In our case, we want to reduce the learning rate when
    the validation loss stops improving. The specific callback is called `ReduceLROnPlateau`
    and it''s tuned in order to reduce *η* multiplying it by `0.1` (after a number
    of epochs equal to the value of the `patience` parameter) with a cooldown period
    (the number of epochs to wait before restoring the original learning rate) of
    1 epoch and a minimum *η = 10^(-6)*. The training method is now `fit_generator()`,
    which accepts Python generators instead of finite datasets and the number of iterations
    per epoch (all the other parameters are the same as implemented by `fit()`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the complexity is higher and the result is not as accurate as
    the one obtained with the standard MNIST dataset. The validation and loss plots
    are shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fba92d8-51bd-407b-9c2d-dff943933638.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss plot doesn't show a U-curve, but it seems that there are no real improvements
    starting from the 20^(th) epoch. This is also confirmed by the validation plot,
    which continues oscillating between 0.935 and about 0.94\. On the other side,
    the training loss hasn't reached its minimum (nor has the training accuracy),
    mainly because of the batch normalizations. However, considering several benchmarks,
    the result is not bad (even if state-of-the-art models can reach a validation
    accuracy of about 0.96). I suggest that the reader try different configurations
    (with and without dropout and other activations) based on deeper architectures
    with larger training sets. This example offers many chances to practice with this
    kind of models, as the complexity is not as high as to require dedicated hardware,
    but at the same time, there are many ambiguities (for example, between shirts
    and t-shirts) that can reduce the generalization ability.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the models that we have analyzed until now have a common feature. Once
    the training process is completed, the weights are frozen and the output depends
    only on the input sample. Clearly, this is the expected behavior of a classifier,
    but there are many scenarios where a prediction must take into account the history
    of the input values. A time series is a classic example. Let''s suppose that we
    need to predict the temperature for the next week. If we try to use only the last
    known *x^((t))* value and an MLP trained to predict *x^((t+1))*, it''s impossible
    to take into account temporal conditions like the season, the history of the season
    over the years, the position in the season, and so on. The regressor will be able
    to associate the output that yields the minimum average error, but in real-life
    situations, this isn''t enough. The only reasonable way to solve this problem
    is to define a new architecture for the artificial neuron, to provide it with
    a memory. This concept is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d906787b-104d-4d25-9e6f-d05472c1fb8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now the neuron is no longer a pure feed-forward computational unit because
    the feedback connection forces it to remember its past and use it in order to
    predict new values. The new dynamic rule is now as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf8e2970-8cbc-4784-b92d-a13ed9ffde8a.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous prediction is fed back and summed to new linear output. The resulting
    value is transformed by the activation function in order to produce the actual
    new output (conventionally the first output is null, but this is not a constraint).
    An immediate consideration concerns the activation function—this is a dynamic
    system that could easily become unstable. The only way to prevent this phenomenon
    is to employ saturating functions (such as the sigmoid or hyperbolic tangent).
    In fact, whatever the input is, the output can never *explode* by moving towards
    *+∞* or *-∞*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that, instead, we were to use a ReLU activation—under some conditions,
    the output will grow indefinitely, leading to an overflow. Clearly, the situation
    is even worse with a linear activation and could be very similar even when using
    a Leaky ReLU or ELU. Hence, it's obvious that we need to select saturating functions,
    but is this enough to ensure stability? Even if a hyperbolic tangent (as well
    as a sigmoid) has two stable points (*-1* and *+1*), this isn't enough to ensure
    stability. Let's imagine that the output is affected by noise and oscillates around
    0.0\. The unit cannot converge towards a value and remains trapped in a limit
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, the possibility to learn the weights allows us to increase the robustness
    to noise, avoiding that limited changes in the input could invert the dynamic
    of the neuron. This is a very important (and easy to prove) result that guarantees
    stability under very simple conditions, but again, what is the price that we need
    to pay? Is it anything simple and straightforward? Unfortunately, the answer is
    negative and the price for stability is extremely high. However, before discussing
    this problem, let's show how a simple recurrent network can be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation through time (BPTT)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The simplest way to train an RNN is based on a representational trick. As the
    input sequences are limited and their length can be fixed, it''s possible to restructure
    the simple neuron with a feedback connection as an unrolled feed-forward network.
    In the following diagram, there''s an example with *k* timesteps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6be24702-989d-4d8c-82af-2018dc2e9c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of unrolled recurrent network
  prefs: []
  type: TYPE_NORMAL
- en: This network (which can be easily extended to more complex architecture with
    several layers) is exactly like an MLP, but in this case, the weights of each
    *clone* are the same. The algorithm called **BPTT** is the natural extension of
    the standard learning technique to unrolled recurrent networks. The procedure
    is straightforward. Once all the outputs have been computed, it's possible to
    determine the value of the cost function for every single network. At this point,
    starting from the last step, the corrections (the gradients) are computed and
    stored, and the process is repeated until the initial step. Then, all of the gradients
    are summed and applied to the network. As every single contribution is based on
    a precise *temporal experience* (made up of a local sample and a previous memory
    element), the standard backpropagation will learn how to manage a dynamic condition
    as if it were a point-wise prediction. However, we know that the actual network
    is not unrolled and the past dependencies are theoretically propagated and remembered.
    I voluntarily used the word *theoretically,* because all practical experiments
    show a completely different behavior that we are going to discuss. This technique
    is very easy to implement, but it can be very expensive for deep networks that
    must be unrolled for a large number of timesteps. For this reason, a variant called
    **truncated backpropagation through time** (**TBPTT**) has been proposed (in *Subgrouping
    reduces complexity and speeds up learning in recurrent networks, Zipser D., Advances
    in Neural Information Processing Systems, II 1990*).
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to use two sequence lengths *t[1]* and *t*[*2* ](with *t[1] >> t[2]*)—the
    longer one (*t[1]*) is employed for the feed-forward phase, while the shorter
    length (*t[2]*) is used to train the network. At first sight, this version seems
    like a normal BPTT with a short sequence; however, the key idea is to force the
    network to update the hidden states with more pieces of information and then compute
    the corrections according to the result of the longer sequence (even if the updates
    are propagated to a limited number of previous timesteps). Clearly, this is an
    approximation that can speed up the training process, but the final result is
    normally comparable with the one obtained by processing long sequences, in particular
    when the dependencies can be split into shorter temporal chunks (and therefore
    the assumption is that there are no very long dependencies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if the BPTT algorithm is mathematically correct and it''s not difficult
    to learn short-term dependencies (corresponding to short unrolled networks), several
    experiments confirmed that it''s extremely difficult (or almost impossible) learning
    long-term dependencies. In other words, it''s easy to exploit past experiences
    whose contribution is limited to a short window (and therefore whose importance
    is limited because they cannot manage the most complex trends) but the network
    cannot easily learn all behaviors that, for example, have a periodicity of hundreds
    of timesteps. In 1994, Bengio, Simard, and Frasconi provided a theoretical explanation
    of the problem (in *Learning Long-Term Dependencies with Gradient Descent is Difficult,
    Bengio Y., Simard P., Frasconi P., IEEE Transactions on Neural Networks, 5/1994*).
    The mathematical details are rather complex, because they involve dynamic system
    theory; however, the final result is that a network whose neurons are forced to
    become robust to noise (the normal expected behavior) is affected by the vanishing
    gradients problem when *t → ∞*. More generally, we can represent a vectorial recurrent
    neuron dynamic as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e64e0be6-5cb2-4b1b-b5f8-188eb4c89d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The multiplicative effect of BPTT forces the gradients to be proportional to
    *W^t*. If the largest absolute eigenvalue (also known as spectral radius) of *W*
    is smaller than 1, then the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4712b54a-8f73-4bdf-9236-cf2c910f8a5b.png)'
  prefs: []
  type: TYPE_IMG
- en: More simply, we can re-express the result saying that the magnitude of the gradients
    is proportional to the length of the sequences and even if the condition is asymptotically
    valid, many experiments confirmed that the limited precision of numeric computations
    and the exponential decay due to subsequent multiplications can force the gradients
    to vanish even when the sequences are not extremely long. This seems to be the
    end of any RNN architecture, but luckily more recent approaches have been designed
    and proposed to resolve this problem, allowing RNNs to learn both short and long-term
    dependencies without particular complications. A new era of RNNs started and the
    results were immediately outstanding.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This model (which represents the state-of-the-art recurrent cell in many fields)
    was proposed in 1997 by Hochreiter and Schmidhuber (in *Long Short-Term Memory,
    Hochreiter S., Schmidhuber J., Neural Computation, Vol. 9, 11/1997*) with the
    emblematic name **long-short-term memory**  (**LSTM**). As the name suggests,
    the idea is to create a more complex artificial recurrent neuron that can be plugged
    into larger networks and trained without the risk of vanishing and, of course,
    exploding gradients. One of the key elements of classic recurrent networks is
    that they are focused on learning, but not on selectively forgetting. This ability
    is indeed necessary for optimizing the memory in order to remember what is really
    important and removing all those pieces of information that are not necessary
    to predict new values.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, LSTM exploits two important features (it's helpful to
    expose them before discussing the model). The first one is an explicit state,
    which is a separate set of variables that store the elements necessary to build
    long and short-term dependencies, including the current state. These variables
    are the building blocks of a mechanism called **constant error carousel** (**CEC**),
    named in this way because it's responsible for the cyclical and internal management
    of the error provided by the backpropagation algorithm. This approach allows the
    correction of the weights without suffering the multiplicative effect anymore.
    The internal LSTM dynamics allow better understanding of how the error is safely
    fed back; however, the exact explanation of the training procedure (which is always
    based on the gradient descent) is beyond the scope of this book and can be found
    in the aforementioned paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second feature is the presence of gates. We can simply define a gate as
    an element that can modulate the amount of information flowing through it. For
    example, if *y = ax* and *a* is a variable bounded between 0 and 1, it can be
    considered as a gate, because when it''s equal to 0, it blocks the input *x;* when
    it''s equal to 1, it allows the input to flow in without restrictions; and when
    it has an intermediate value, it reduces the amount of information proportionally.
    In LSTMs, gates are managed by sigmoid functions, while the activations are based
    on hyperbolic tangents (whose symmetry guarantees better performances). At this
    point, we can show the structural diagram of an LSTM cell and discuss its internal
    dynamics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fec7669a-ede2-4853-b198-b164c777829a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first (and most important) element is the memory state, which is responsible
    for the dependencies and for the actual output. In the diagram, it is represented
    by the upper line and its dynamics are represented by the following general equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66896d56-f095-4293-a24c-12fd521ab81e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the state depends on the previous value, on the current input, and on the
    previous output. Let''s start with the first term, introducing the forget gate.
    As the name says, it''s responsible for the persistence of the existing memory
    elements or for their deletion. In the diagram, it''s represented by the first
    vertical block and its value is obtained by considering the concatenation of previous
    output and current input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6a0590d-869d-45ff-aaa3-fc2ab8033d95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The operation is a classical neuron activation with a vectorial output. An
    alternative version can use two weight matrices and keep the input elements separated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca16f462-02b6-4af1-8bbf-be1ee4a1777b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, I prefer the previous version, because it can better express the homogeneity
    of input and output, and also their consequentiality. Using the forget gate, it''s
    possible to determine the value of *g[1](C^((t)))* using the Hadamard (or element-wise)
    product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07fa0421-c3e2-46de-b9a7-2a5fde4eb16b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The effect of this computation is filtering the content of *C^((t))* that must
    be preserved and the validity degree (which is proportional to the value of *f^((t+1))*).
    If the forget gate outputs a value close to 1, the corresponding element is still
    considered valid, while lower values determine a sort of obsolescence that can
    even lead the cell to completely remove an element when the forget gate value
    is 0 or close to it. The next step is to consider the amount of the input sample
    that must be considered to update the state. This task is achieved by the input
    gate (second vertical block). The equation is perfectly analogous to the previous
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cf6e0b8-698c-4fdd-ae91-e05b9f41d2d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, in this case, we also need to compute the term that must be added
    to the current state. As already mentioned, LSTM cells employ hyperbolic tangents
    for the activations; therefore, the new contribution to the state is obtained
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33a99c4c-f9a4-4d66-9354-3b77cf1a4570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the input gate and the state contribution, it''s possible to determine
    the function *g[2](x^((t+1)), y^((t)))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77269aa9-dd72-4d95-a1fd-3c4719746ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Hence, the complete state equation becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e4227fe-745b-4a0d-8371-8c6f65255ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the inner logic of an LSTM cell is more evident. The state is based on
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A dynamic balance between previous experience and its re-evaluation according
    to new experience (modulated by the forget gate)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *semantic* effect of the current input (modulated by the input gate) and
    the potential additive activation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistic scenarios are many. It's possible that a new input forces the LSTM
    to reset the state and store the new incoming value. On the other hand, the input
    gate can also remain closed, giving a very low priority to the new input (together
    with the previous output). In this case, the LSTM, considering the long-term dependencies,
    can decide to discard a sample that is considered noisy and not necessarily able
    to contribute to an accurate prediction. In other situations, both the forget
    and input gates can be partially open, letting only some values influence the
    state. All these possibilities are managed by the learning process through the
    correction of the weight matrices and the biases. The difference with BPTT is
    that the long-term dependencies are no longer impeded by the vanishing gradients
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is determining the output. The third vertical block is called
    the output gate and controls the information that must transit from the state
    to the output unit. Its equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d0fb809-2543-49ec-8e5f-e321db0b8ab5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The actual output is hence determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82a504a1-3d4d-47ab-964e-53358805a3da.png)'
  prefs: []
  type: TYPE_IMG
- en: An important consideration concerns the gates. They are all fed with the same
    vector, containing the previous output and the current input. As they are homogenous
    values, the concatenation yields a coherent entity that encodes a sort of *inverse*
    cause-effect relationship (this is an improper definition, as we work with previous
    effect and current cause). The gates work like logistic regressions without thresholding;
    therefore, they can be considered as pseudo-probability vectors (not distributions,
    as each element is independent). The forget gate expresses the probability that
    last sequence (effect, cause) is more important than the current state; however,
    only the input gate has the responsibility to grant it the right to influence
    the new state. Moreover, the output gate expresses the probability that the current
    sequence is able to let the current state flow out. The dynamic is indeed very
    complex and has some drawbacks. For example, when the output gate remains closed,
    the output is close to zero and this influences both forget and input gates. As
    they control the new state and the CEC, they could limit the amount of incoming
    information and consequent corrections, leading to poor performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple solution that can mitigate this problem is provided by a variant called
    **peephole** **LSTM**. The idea is to feed the previous state to every gate so
    that they can take decisions more independently. The generic gate equation becomes
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7cde201-8502-45e4-bd6d-827467a76324.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The new set of weights *U[g]* (for all three gates) must be learned in the
    same way as the standard *W[g]* and *b[g]*. The main difference with a classic
    LSTM is that the sequential dynamic: forget gate | input gate | new state | output
    gate | actual output is now partially shortcutted. The presence of the state in
    every gate activation allows them to exploit multiple recurrent connections, yielding
    a better accuracy in many complex situations. Another important consideration
    is about the learning process: in this case, the peepholes are closed and the
    only feedback channel is the output gate. Unfortunately, not every LSTM implementation
    support peepholes; however, several studies confirmed that in most cases all the
    models yield similar performances.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Xingjian et al. (in *Convolutional LSTM Network: A Machine Learning Approach
    for Precipitation Nowcasting, Xingjian S., Zhourong C., Hao W., Dit-Yan Y., Wai-kin
    W., Wang-Chun W., arXiv:1506.04214 [cs.CV]*) proposed a variant called **convolutional
    LSTM**, which clearly mixes Convolutions and LSTM cells. The main internal difference
    concerns the gate computations, which now become (without peepholes, which however,
    can always be added):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed3d78d2-a8a7-48af-9fe0-cf5931c54357.png)'
  prefs: []
  type: TYPE_IMG
- en: '*W[g]* is now a kernel that is convoluted with the input-output vector (which
    is usually the concatenation of two images). Of course, it''s possible to train
    any number of kernels to increase the decoding power of the cell and the output
    will have a shape equal to (*batch size × width × height × kernels*). This kind
    of cell is particularly useful for joining spatial processing with a robust temporal
    approach. Given a sequence of images (for example, satellite images, game screenshots,
    and so on), a convolutional LSTM network can learn long-term relationships that
    are manifested through geometric feature evolutions (for example, cloud movements
    or specific sprite strategies that it''s possible to anticipate considering a
    long history of events). This approach (even with a few modifications) is widely
    employed in Deep Reinforcement Learning in order to solve complex problems where
    the only input is provided by a sequence of images. Of course, the computational
    complexity is very high, in particular when many subsequent layers are used; however,
    the results outperformed any existing method and this approach became one of the
    first choices to manage this kind of problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Another important variant, which is common to many Recurrent Neural Networks,
    is provided by a bidirectional interface. This isn't an actual layer, but a strategy
    that is employed in order to join the forward analysis of a sequence with the
    backward one. Two cellblocks are fed with a sequence and its inverse and the output,
    for example, is concatenated and used for further processing steps. In fields
    such as NLP, this method allows us to dramatically improve the accuracy of classifications
    and real-time translations. The reason is strictly related to the rules underlying
    the structure of a sequence. In natural language, a sentence *w[1] w[2] ... w[n]*
    has forward relationships (for example, a singular noun can be followed by *is*),
    but the knowledge of backward relationships (for example, the sentence *this place
    is pretty awful*) permits avoiding common mistakes that, in the past, had to be
    corrected using post-processing steps (the initial translation of *pretty* could
    be similar to the translation of *nice*, but a subsequent analysis can reveal
    that the adjective mismatches and a special rule can be applied). Deep learning,
    on the other side, is not based on *special* *rules*, but on the ability to learn
    an internal representation that should be autonomous in making final decisions
    (without further external aids) and bidirectional LSTM networks help in reaching
    this goal in many important contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Keras implements the classes `LSTM` since its origins. It also provides a `Bidirectional`
    class wrapper that can be used with every RNN layer in order to obtain a double
    output (computed with the forward and backward sequences). Moreover, in Keras
    2 there are optimized versions of LSTM based on NVIDIA CUDA (`CuDNNLSTM`), which
    provide very high performance when a compatible GPU is available. In the same
    package, it's possible to also find the `ConvLSTM2D` class, which implements a
    convolutional LSTM layer. In this case, the reader can immediately identify many
    of the parameters, as they are the same as a standard convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This model, named **Gated recurrent unit** (**GRU**), proposed by Cho et al.
    (in *Learning Phrase Representations using RNN Encoder-Decoder for Statistical
    Machine Translation*, *Cho K.*, *Van Merrienboer B.*, *Gulcehre C.*, *Bahdanau
    D.*, *Bougares F.*, *Schwenk H.*, *Bengio Y.*, *arXiv:1406.1078 [cs.CL]*) can
    be considered as a simplified LSTM with a few variations. The structure of a generic
    full-gated unit is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd472a98-66f5-4ac4-a1d2-506708f1f911.png)'
  prefs: []
  type: TYPE_IMG
- en: The main differences from LSTM are the presence of only two gates and the absence
    of an explicit state. These simplifications can speed both the training and the
    prediction phases while avoiding the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first gate is called the **reset gate** (conventionally denoted with the
    letter *r*) and its function is analogous to the forget gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14262aa-5a81-4306-8c2e-53b870d730a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the forget gate, its role is to decide what content of the previous
    output must be preserved and the relative degree. In fact, the additive contribution
    to new output is obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e32e648a-d38d-4a9d-acf2-8e70935ef64a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, I''ve preferred to separate the weight matrices
    to better exposes the behavior. The argument of *tanh(•)* is the sum of a linear
    function of the new input and a weighted term that is a function of the previous
    state. Now, it''s clear how the reset gate works: it modulates the amount of history
    (accumulated in the previous output value) that must be preserved and what instead
    can be discarded. However, the reset gate is not enough to determine the right
    output with enough accuracy, considering both short and long-term dependencies.
    In order to increase the expressivity of the unit, an update gate (with a role
    similar to the LSTM input gate) has been added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7af9d899-2abc-46e6-b917-70ceea3d9d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The update gate controls the amount of information that must contribute to
    the new output (and hence to the state). As it''s a value bounded between *0*
    and *1*, GRUs are trained to mix old output and new additive contribution with
    an operation similar to a weighted average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efbd854c-0abc-4659-95c2-c75bf59aee89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the update gate becomes a modulator that can select which components
    of each flow must be output and stored for the next operation. This unit is structurally
    simpler than an LSTM, but several studies confirmed that its performance is on
    average, equivalent to LSTM, with some particular cases when GRU has even outperformed
    the more complex cell. My suggestion is that you test both models, starting with
    LSTM. The computational cost has been dramatically reduced by modern hardware
    and in many contexts the advantage of GRUs is negligible. In both cases, the philosophy
    is the same: the error is kept inside the cell and the weights of the gates are
    corrected in order to maximize the accuracy. This behavior prevents the multiplicative
    cascade of small gradients and increases the ability to learn very complex temporal
    behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: However, a single cell/layer would not be able to successfully achieve the desired
    accuracy. In all these cases, it's possible to stack multiple layers made up of
    a variable number of cells. Every layer can normally output the last value or
    the entire sequence. The former is used when connecting the LSTM/GRU layer to
    a fully-connected one, while the whole sequence is necessary to feed another recurrent
    layer. We are going to see how to implement these techniques with Keras in the
    following example.
  prefs: []
  type: TYPE_NORMAL
- en: Just like for LSTMs, Keras implements the`GRU` class and its NVIDIA CUDA optimized
    version `CuDNNGRU`.
  prefs: []
  type: TYPE_NORMAL
- en: Example of an LSTM network with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we want to test the ability of an LSTM network to learn long-term
    dependencies. For this reason, we employ a dataset called Zuerich Monthly Sunspots
    (freely provided by Andrews and Herzberg in 1985) containing the numbers observed
    in all the months starting from 1749 to 1983 (please read the information box
    for how to download the dataset). As we are not interested in the dates, we need
    to parse the file in order to extract only the values needed for the time series
    (which contains 2,820 steps):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, it''s possible to load the CSV dataset using pandas ([https://pandas.pydata.org](https://pandas.pydata.org)),
    which is a powerful data manipulation/analysis library (for further information,
    please refer to *Learning pandas Second Edition, Heydt M., Packt*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The values are unnormalized and as LSTMs work with hyperbolic tangents, it''s
    helpful to normalize them in the interval `-1` and `1`. We can easily perform
    this step using the Scikit-Learn class `MinMaxScaler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete dataset is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98e25ffc-1ca5-43ea-85c1-b53cf62523ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to train the model, we have decided to use 2,300 samples for training
    and the remaining 500 for validation (corresponding to about 42 years). The input
    of the model is a batch of sequences of 15 samples (shifted along the time axis)
    and the output is the subsequent month; therefore, before training, we need to
    prepare the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create and compile a simple model with a single stateful LSTM layer
    containing four cells, followed by a hyperbolic tangent output neuron (I always
    suggest that the reader experiment with more complex architectures and different
    parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting the `stateful=True` parameter in the `LSTM` class forces Keras not
    to reset the state after each batch. In fact, our goal is learning long-term dependencies
    and the internal LSTM state must reflect the overall trend. When an LSTM network
    is stateful, it''s also necessary to specify the batch size in the input shape
    (through the `batch_input_shape` parameter ). In our case, we have selected a
    batch size equal to 20 samples. The optimizer is `Adam` with a higher decay (to
    avoid instabilities) and a loss based on the mean squared error (which is the
    most common choice in this kind of scenario). At this point, we can train the
    model (for 100 epochs):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an example whose purpose is only didactic; therefore, the final validation
    mean squared error is not extremely low. However, as it''s possible to see in
    the following diagram (representing the predictions on the validation set), the
    model has successfully learned the global trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e8c3134-ec30-489d-9909-926ead5df109.png)'
  prefs: []
  type: TYPE_IMG
- en: LSTM predictions on the Zuerich dataset
  prefs: []
  type: TYPE_NORMAL
- en: The model is still unable to achieve a very high accuracy in correspondence
    of all the very rapid spikes, but it's able to correctly model the amplitude of
    the oscillations and the length of the tails. For the sake of intellectual honesty,
    we must consider that this validation is performed on true data; however, when
    working with time series, it's normal to predict a new value using the ground
    truth. In this case, it's like a moving prediction where each value is obtained
    using the training history and a set of real observations. It's clear that the
    model is able to predict the long-term oscillations and also some local ones (for
    example, the sequence starting from step 300), but it can be improved in order
    to have better performance on the whole validation set. To achieve this goal,
    it is necessary to increase the network complexity and tune up the learning rate
    (it's a very interesting exercise on a real dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Observing the previous diagram, it's possible to see that the model is relatively
    more accurate at some high frequencies (rapid changes), while it's more imprecise
    on others. This is not a strange behavior, because very oscillating functions
    *need more non-linearity* (think about the Taylor expansion and the relative error
    when it's truncated to a specific degree) to achieve high accuracies (this means
    employing more layers). My suggestion is that you repeat the experiment using
    more LSTM layers, considering that we need to pass the whole output sequence to
    the following recurrent layer (this can be achieved by setting the `return_sequences=True` parameter).
    The last layer, instead, must return only the final value (which is the default
    behavior). I also suggest testing the GRU layers, comparing the performance with
    the LSTM version and picking the simplest (benchmarking the training time) and
    most accurate solution.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be freely downloaded in CSV format from [https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line](https://datamarket.com/data/set/22ti/zuerich-monthly-sunspot-numbers-1749-1983#!ds=22ti&display=line).
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed how deep learning is fundamentally based on gray-box models
    that learn how to associate input patterns to specific classification/regression
    outcomes. All the processing pipeline that is often employed to prepare the data
    for specific detections is absorbed by the complexity of the neural architecture.
    However, the price to pay for high accuracies is a proportionally large number
    of training samples. State-of-the-art visual networks are trained with millions
    of images and, obviously, each of them must be properly labeled. Even if there
    are many free datasets that can be employed to train several models, many specific
    scenarios need hard preparatory work that sometimes is very difficult to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, deep neural architectures are hierarchical models that learn in a structured
    way. As we have seen in the examples of deep convolutional networks, the first
    layers become more and more sensitive to detect low-level features, while the
    higher ones concentrate their work on extracting more detailed high-level features.
    In several tasks, it's reasonable to think that a network trained, for example,
    with a large visual dataset (such as ImageNet or Microsoft Coco) could be reused
    to achieve a specialization in a slightly different task. This concept is known
    as **transfer learning** and it's one of the most useful techniques when it's
    necessary to create state-of-the-art models with brand new datasets and specific
    objectives. For example, a customer can ask for a system to monitor a few cameras
    with the goal to segment the images and highlight the boundaries of specific targets.
  prefs: []
  type: TYPE_NORMAL
- en: The input is made up of video frames with the same geometric properties as thousands
    of images employed in training very powerful models (for example, Inception, ResNet,
    or VGG); therefore, we can take a pre-trained model, remove the highest layers
    (normally dense ones ending in a softmax classification layer) and connect the
    flattening layer to an MLP that outputs the coordinates of the bounding boxes.
    The first part of the network can be *frozen* (the weights are not modified anymore),
    while the SGD is applied to tune up the weights of the newly specialized sub-network.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, such an approach can dramatically speed up the training process, because
    the most complex part of the model is already trained and can also guarantee an
    extremely high accuracy (with respect to a naive solution), thanks to the optimization
    already performed on the original model. Obviously, the most natural question
    is how does this method work? Is there any formal proof? Unfortunately, there
    are no mathematical proofs, but there's enough evidence to assure about us of
    this approach. Generally speaking, the goal of a neural training process is to
    specialize each layer in order to provide a more particular (detailed, filtered,
    and so on) representation to the following one. Convolutional networks are a clear
    example of this behavior, but the same is observable in MLPs as well. The analysis
    of very deep convolutional networks showed how the content is still *visual* until
    reaching the flattening layer, where it's sent to a series of dense layers that
    are responsible for feeding the final softmax layer. In other words, the output
    of the convolutional block is a higher-level, segmented representation of the
    input, which is seldom affected by the specific classification problem. For this
    reason, transfer learning is generally sound and doesn't normally require a retraining
    of the lower layers. However, it's difficult to understand which model can yield
    the best performances and it's very useful to know which dataset has been used
    to train the original network. General purpose datasets (for example, ImageNet)
    are very useful in many contexts, while specific ones (such as Cifar-10 or Fashion;
    MNIST can be too restrictive). Luckily, Keras offers (in the package `keras.applications`)
    many models (even quite complex ones) that are always trained with ImageNet datasets
    and that can be immediately employed in a production-ready application. Even if
    using them is extremely simple, it requires a deeper knowledge of this framework,
    which is beyond the scope of this book. I invite the reader interested in this
    topic to check the book *Deep Learning with Keras, Gulli A., Pal S., Packt*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have presented the concept of a deep convolutional network,
    which is a generic architecture that can be employed in any visual processing
    task. The idea is based on hierarchical information management, aimed at extracting
    the features starting from low-level elements and moving forward until the high-level
    details that can be helpful to achieve specific goals.
  prefs: []
  type: TYPE_NORMAL
- en: The first topic was the concept of convolution and how it's applied in discrete
    and finite samples. We discussed the properties of standard convolution, before
    analyzing some important variants such as atrous (or dilated convolution), separable
    (and depthwise separable) convolution and, eventually, transpose convolution.
    All these methods can work with 1D, 2D, and 3D samples, even if the most diffused
    applications are based on bidimensional (not considering the channels) matrices
    representing static images. In the same section, we also discussed how pooling
    layers can be employed to reduce the dimensionality and improve the robustness
    to small translations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we introduced the concept of RNN, emphasizing the issues
    that normally arise when classic models are trained using the backpropagation
    through time algorithm. In particular, we explained why these networks cannot
    easily learn long-term dependencies. For this reason, new models have been proposed,
    whose performance was immediately outstanding. We discussed the most famous recurrent
    cell, called **L****ong-short-term memory** (**LSTM**), which can be used in layers
    that can easily learn all the most important dependencies of a sequence, allowing
    us to minimize the prediction error even in contexts with a very high variance
    (such as stock market quotations). The last topic was a simplified version of
    the idea implemented in LSTMs, which led to a model called a **Gated recurrent
    unit** (**GRU**). This cell is simpler and more computationally efficient, and
    many benchmarks confirmed that its performance is approximately the same as LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 11](8022fb07-a741-4a0e-a965-6678577d3563.xhtml), *Autoencoders* we
    are going to discuss some particular models called autoencoders, whose main property
    is to create internal representations of an arbitrarily complex input distribution.
  prefs: []
  type: TYPE_NORMAL
