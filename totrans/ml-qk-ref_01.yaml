- en: Quantifying Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化学习算法
- en: 'We have stepped into an era where we are building smart or intelligent machines.
    This smartness or intelligence is infused into the machine with the help of smart
    algorithms based on mathematics/statistics. These algorithms enable the system
    or machine to learn automatically without any human intervention. As an example
    of this, today we are surrounded by a number of mobile applications. One of the
    prime messaging apps of today in WhatsApp (currently owned by Facebook). Whenever
    we type a message into a textbox of WhatsApp, and we type, for example, *I am...*,
    we get a few word prompts popping up, such as *..going home*, *Rahul*, *traveling
    tonight*, and so on. Can we guess what''s happening here and why? Multiple questions
    come up:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进入了一个时代，我们正在构建智能或智能机器。这种智能或智能是通过基于数学/统计的智能算法注入到机器中的。这些算法使系统或机器能够自动学习，无需任何人为干预。作为这个例子，今天我们周围有大量的移动应用程序。今天的主要即时通讯应用之一是WhatsApp（目前由Facebook拥有）。每当我们将信息输入WhatsApp的文本框时，例如，输入*I
    am...*，就会出现一些词提示，如*..going home*、*Rahul*、*traveling tonight*等等。我们能猜测这里发生了什么，为什么吗？会涌现出多个问题：
- en: What is it that the system is learning?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统正在学习什么？
- en: Where does it learn from?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从哪里学习？
- en: How does it learn?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是如何学习的？
- en: Let's answer all these questions in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本章中回答所有这些问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Statistical models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型
- en: Learning curves
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习曲线
- en: Curve fitting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: Modeling cultures
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模文化
- en: Overfitting and regularization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合和正则化
- en: Train, validation, and test
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、验证和测试
- en: Cross-validation and model selection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证和模型选择
- en: Bootstrap method
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自举法
- en: Statistical models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计模型
- en: A statistical model is the approximation of the truth that has been captured
    through data and mathematics or statistics, and acts as an enabler here. This
    approximation is used to predict an event. A statistical model is nothing but
    a mathematical equation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 统计模型是通过数据、数学或统计学捕捉到的真理的近似，在这里充当使能者。这种近似用于预测事件。统计模型不过是一个数学方程。
- en: 'For example, let''s say we reach out to a bank for a home loan. What does the
    bank ask us? The first thing they would ask us to do is furnish lots of documents
    such as salary slips, identity proof documents, documents regarding the house
    we are going to purchase, a utility bill, the number of current loans we have,
    the number of dependants we have, and so on. All of these documents are nothing
    but the data that the bank would use to assess and check our creditworthiness:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们向银行申请房屋贷款。银行会问我们什么？他们首先会要求我们提供大量文件，例如工资单、身份证明文件、关于我们要购买的房屋的文件、水费账单、我们当前的贷款数量、我们的抚养人数等等。所有这些文件不过是银行用来评估和检查我们信用度的数据：
- en: '![](img/50ec8721-c5e9-49c9-8a58-f60aacf91fa8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/50ec8721-c5e9-49c9-8a58-f60aacf91fa8.png)'
- en: What this means is that your creditworthiness is a function of the salary, number
    of loans, number of dependants, and so on. We can arrive at this equation or relationship
    mathematically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你的信用度是工资、贷款数量、抚养人数等因素的函数。我们可以通过数学方法得出这个方程或关系。
- en: A statistical model is a mathematical equation that arrives at using given data
    for a particular business scenario.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 统计模型是一个数学方程，用于特定商业场景的给定数据。
- en: In the next section, we will see how models learn and how the model can keep
    getting better.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到模型是如何学习的，以及模型如何不断变得更好。
- en: Learning curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: The basic premise behind the learning curve is that the more time you spend
    doing something, the better you tend to get. Eventually, the time to perform a
    task keeps on plummeting. This is known by different names, such as **improvement
    curve**, **progress curve**, and **startup function**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线的基本前提是，你花在某一件事上的时间越多，你通常越擅长。最终，完成任务所需的时间会不断下降。这被称为不同的名称，如**改进曲线**、**进步曲线**和**启动函数**。
- en: For example, when you start learning to drive a manual car, you undergo a learning
    cycle. Initially, you are extra careful about operating the break, clutch, and
    gear. You have to keep reminding yourself when and how to operate these components.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你开始学习手动驾驶汽车时，你会经历一个学习周期。最初，你非常小心地操作刹车、离合器和档位。你必须不断提醒自己何时以及如何操作这些部件。
- en: But, as the days go by and you continue practicing, your brain gets accustomed
    and trained to the entire process. With each passing day, your driving will keep
    getting smoother and your brain will react to the situation without any realization.
    This is called **subconscious intelligence**. You reach this stage with lots of
    practice and transition from a conscious intelligence to a subconscious intelligence
    that has got a cycle.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，随着日子的推移，你继续练习，你的大脑就会习惯并训练整个流程。随着每一天的过去，你的驾驶会越来越顺畅，你的大脑会对情况做出反应，而无需意识到这一点。这被称为**潜意识智能**。通过大量的练习和从有意识智能过渡到具有循环的潜意识智能，你将达到这个阶段。
- en: Machine learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Let me define machine learning and its components so that you don't get bamboozled
    by lots of jargon when it gets thrown at you.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我来定义机器学习和其组成部分，这样你就不至于在听到很多术语时感到困惑。
- en: In the words of Tom Mitchell, "*A computer program is said to learn from experience
    E with respect to some class of tasks T and performance measure P, if its performance
    at tasks in T, as measured by P, improves with experience E*." Also, another theory
    says that machine learning is the field that gives computers the ability to learn
    without being explicitly programmed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 按照Tom Mitchell的话说，“*如果计算机程序在任务T中，关于性能度量P，从经验E中学习，那么它的性能在T中的任务，按照P来衡量，会随着经验E的提高而提高*。”也有另一种理论说，机器学习是赋予计算机学习能力而不需要明确编程的领域。
- en: For example, if a computer has been given cases such as, *[(father, mother),
    (uncle, aunt), (brother, sisters)]*, based on this, it needs to find out *(son,
    ?)*. That is, given son, what will be the associated item? To solve this problem,
    a computer program will go through the previous records and try to understand
    and learn the association and pattern out of these combinations as it hops from
    one record to another. This is called **learning**, and it takes place through
    algorithms. With more records, that is, more experience, the machine gets smarter
    and smarter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一台计算机被给出了如下案例，*[(父亲，母亲)，(叔叔，阿姨)，(兄弟，姐妹)]*，基于此，它需要找出*(儿子，？)*。也就是说，给定儿子，将是什么相关项？为了解决这个问题，计算机程序将遍历之前的记录，并试图理解和学习从这些组合中跳转到另一个记录时的关联和模式。这被称为**学习**，它通过算法进行。随着记录的增加，即经验的增加，机器会变得越来越聪明。
- en: 'Let''s take a look at the different branches of machine learning, as indicated
    in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下图中所示的不同机器学习分支：
- en: '![](img/171db42c-de53-4ce2-9ec5-5acda2329ca5.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/171db42c-de53-4ce2-9ec5-5acda2329ca5.png)'
- en: 'We will explain the preceding diagram as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如下解释前面的图示：
- en: '**Supervised learning**: In this type of learning, both the input variables
    and output variables are known to us. Here, we are supposed to establish a relationship
    between the input variables and the output, and the learning will be based on
    that. There are two types of problems under it, as follows:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：在这种学习中，输入变量和输出变量都是我们所知的。在这里，我们应建立输入变量和输出之间的关系，学习将基于这一点。在其下有两种类型的问题，如下所示：'
- en: '**Regression problem**: It has got a continuous output. For example, a housing
    price dataset wherein the price of the house needs to be predicted based on input
    variables such as area, region, city, number of rooms, and so on. The price to
    be predicted is a continuous variable.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**：它有一个连续的输出。例如，一个房价数据集，其中需要根据面积、地区、城市、房间数量等输入变量预测房价。要预测的价格是一个连续变量。'
- en: '**Classification**: It has got a discrete output. For example, the prediction
    that an employee would leave an organization or not, based on salary, gender,
    the number of members in their family, and so on.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：它有一个离散的输出。例如，根据薪水、性别、家庭成员数量等预测一个员工是否会离开组织。'
- en: '**Unsupervised learning**: In this type of scenario, there is no output variable.
    We are supposed to extract a pattern based on all the variables given. For example,
    the segmentation of customers based on age, gender, income, and so on.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：在这种场景中，没有输出变量。我们应根据所有给定的变量提取一个模式。例如，根据年龄、性别、收入等对客户进行细分。'
- en: '**Reinforcement learning**: This is an area of machine learning wherein suitable
    action is taken to maximize reward. For example, training a dog to catch a ball
    and give it—we reward the dog if they carry out this action; otherwise, we tell
    them off, leading to a punishment.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：这是机器学习的一个领域，其中采取适当的行动以最大化奖励。例如，训练狗接球并给予奖励——如果它们执行这个动作，我们就奖励狗；否则，我们责备它们，导致惩罚。'
- en: Wright's model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赖特模型
- en: 'In Wright''s model, the learning curve function is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在赖特模型中，学习曲线函数定义为以下：
- en: '![](img/f534dff2-a5c4-4f11-b694-b14f97180b55.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f534dff2-a5c4-4f11-b694-b14f97180b55.png)'
- en: 'The variables are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 变量如下：
- en: '*Y*: The cumulative average time per unit'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*：每单位累积平均时间'
- en: '*X*: The cumulative number of units produced'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*：累积生产的单位数量'
- en: '*a*: Time required to produce the first unit'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*：生产第一个单位所需的时间'
- en: '*b*: Slope of the function when plotted on graph paper* (log of the learning
    rate/log of 2)*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*：函数在图表纸上绘制时的斜率（学习率的对数/2的对数）'
- en: 'The following curve has got a vertical axis (*y* axis) representing the learning
    with respect to a particular work and a horizontal axis that corresponds to the
    time taken to learn. A learning curve with a steep beginning can be comprehended
    as a sign of rapid progress. The following diagram shows **Wright''s Learning
    Curve Model**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下曲线具有一个垂直轴（*y*轴），表示特定工作的学习情况，以及一个对应学习所需时间的水平轴。一个开始陡峭的学习曲线可以理解为快速进步的标志。以下图表显示了**赖特学习曲线模型**：
- en: '![](img/f10a6a22-e064-4c82-9f37-ebe5f7384a35.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f10a6a22-e064-4c82-9f37-ebe5f7384a35.png)'
- en: However, the question that arises is, *How is it connected to machine learning?* We
    will discuss this in detail now.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出现的问题是，*它与机器学习有何关联？*我们现在将详细讨论这个问题。
- en: 'Let''s discuss a scenario that happens to be a supervised learning problem
    by going over the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤讨论一个实际上是监督学习问题的场景：
- en: We take the data and partition it into a training set (on which we are making
    the system learn and come out as a model) and a validation set (on which we are
    testing how well the system has learned).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分成一个训练集（我们在其上让系统学习并形成模型）和一个验证集（我们在其上测试系统学习的好坏）。
- en: The next step would be to take one instance (observation) of the training set
    and make use of it to estimate a model. The model error on the training set will
    be 0.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将是取训练集的一个实例（观察值）并利用它来估计一个模型。训练集上的模型错误将为0。
- en: Finally, we would find out the model error on the validation data.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将找出验证数据上的模型错误。
- en: '*Step 2* and *Step 3* are repeated by taking a number of instances (training
    size) such as 10, 50, and 100 and studying the training error and validation error,
    as well as their relationship with a number of instances (training size). This
    curve—or the relationship—is called a **l****earning curve** in a machine learning
    scenario.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二步*和*第三步*通过取多个实例（训练大小）如10、50和100，研究训练错误和验证错误，以及它们与实例数量（训练大小）的关系来重复。这种曲线——或这种关系——在机器学习场景中被称为**学习曲线**。'
- en: 'Let''s work on a combined power plant dataset. The features comprised hourly
    average ambient variables, that is, **temperature** (**T**), **ambient pressure**
    (**AP**), **relative humidity** (**RH**), and exhaust **vacuum** (**V**), to predict
    the net hourly **electrical energy output** (**PE**) of the plant:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一个综合发电厂数据集。特征包括每小时平均环境变量，即**温度**（**T**）、**环境压力**（**AP**）、**相对湿度**（**RH**）和排气**真空**（**V**），以预测工厂的净每小时**电能输出**（**PE**）：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'From this, we are able to see the data structure of the variables in the data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们能够看到数据中变量的数据结构：
- en: '![](img/05d57d50-1764-4cb0-a172-83bb011dc055.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05d57d50-1764-4cb0-a172-83bb011dc055.png)'
- en: 'The output can be seen as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/52260431-6fcc-4767-87d9-f47a04b70257.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52260431-6fcc-4767-87d9-f47a04b70257.png)'
- en: The second output gives you a good feel for the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出可以让你对数据有一个良好的感觉。
- en: The dataset has five variables, where **ambient temperature** (**AT**) and PE (target
    variable).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有五个变量，其中**环境温度**（**AT**）和PE（目标变量）。
- en: 'Let''s vary the training size of the data and study the impact of it on learning.
    A list is created for `train_size` with varying training sizes, as shown in the
    following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变数据的训练大小并研究它对学习的影响。创建了一个`train_size`列表，包含不同的训练大小，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s generate the `learning_curve`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成`learning_curve`：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/0e15e0c1-657c-495d-a65f-14de600ca4fa.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e15e0c1-657c-495d-a65f-14de600ca4fa.png)'
- en: From the preceding plot, we can see that when the training size is just 1, the
    training error is 0, but the validation error shoots beyond **400**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到当训练集大小仅为1时，训练误差为0，但验证误差超过**400**。
- en: 'As we go on increasing the training set''s size (from 1 to 100), the training
    error continues rising. However, the validation error starts to plummet as the
    model performs better on the validation set. After the training size hits the
    500 mark, the validation error and training error begin to converge. So, what
    can be inferred out of this? The performance of the model won''t change, irrespective
    of the size of the training post. However, if you try to add more features, it
    might make a difference, as shown in the following diagram:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练集大小的不断增加（从1到100），训练误差持续上升。然而，随着模型在验证集上的表现越来越好，验证误差开始急剧下降。当训练集大小达到500时，验证误差和训练误差开始收敛。那么，我们能从中推断出什么呢？模型的性能不会因为训练数据的量而改变。然而，如果你尝试添加更多特征，可能会产生影响，如下面的图表所示：
- en: '![](img/21d6445c-9045-477b-8f6b-6f8203070ac2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21d6445c-9045-477b-8f6b-6f8203070ac2.png)'
- en: 'The preceding diagram shows that the validation and training curve have converged,
    so adding training data will not help at all. However, in the following diagram,
    the curves haven''t converged, so adding training data will be a good idea:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示，验证曲线和训练曲线已经收敛，因此添加训练数据将毫无帮助。然而，在下面的图表中，曲线尚未收敛，因此添加训练数据将是一个好主意：
- en: '![](img/f6ec0074-8426-4428-b50a-f7d40b70373d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6ec0074-8426-4428-b50a-f7d40b70373d.png)'
- en: Curve fitting
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: So far, we have learned about the learning curve and its significance. However,
    it only comes into the picture once we tried fitting a curve on the available
    data and features. But what does curve fitting mean? Let's try to understand this.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了学习曲线及其重要性。然而，它只有在尝试在可用数据和特征上拟合曲线时才会出现。但曲线拟合是什么意思呢？让我们尝试理解这一点。
- en: Curve fitting is nothing but establishing a relationship between a number of
    features and a target. It helps in finding out what kind of association the features
    have with respect to the target.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线拟合实际上就是建立多个特征与目标之间的关系。它有助于找出特征与目标之间的关联类型。
- en: Establishing a relationship (curve fitting) is nothing but coming up with a
    mathematical function that should be able to explain the behavioral pattern in
    such a way that it comes across as a best fit for the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 建立关系（曲线拟合）实际上就是提出一个数学函数，该函数应该能够以某种方式解释行为模式，使其成为数据集的最佳拟合。
- en: 'There are multiple reasons behind why we do curve fitting:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行曲线拟合的原因有很多：
- en: To carry out system simulation and optimization
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行系统模拟和优化
- en: To determine the values of intermediate points (interpolation)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定中间点的值（插值）
- en: To do trend analysis (extrapolation)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行趋势分析（外推）
- en: To carry out hypothesis testing
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行假设检验
- en: 'There are two types of curve fitting:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线拟合有两种类型：
- en: '**Exact fit**: In this scenario, the curve would pass through all the points.
    There is no residual error (we''ll discuss shortly what''s classed as an error)
    in this case. For now, you can understand an error as the difference between the
    actual error and the predicted error. It can be used for interpolation and is
    majorly involved with a distribution fit.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**精确拟合**：在这种情况下，曲线将通过所有点。在这种情况下没有残差误差（我们很快会讨论什么被认为是误差），现在你可以将误差理解为实际误差与预测误差之间的差异。它可以用于插值，并且主要涉及分布拟合。'
- en: 'The following diagram shows the polynomial but exact fit:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了多项式但精确拟合：
- en: '![](img/324d5cff-68ea-43a5-9285-805f65773b79.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/324d5cff-68ea-43a5-9285-805f65773b79.png)'
- en: 'The following diagram shows the line but exact fit:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了直线但精确拟合：
- en: '![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)'
- en: '**Best fit**: The curve doesn''t pass through all the points. There will be
    a residual associated with this.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最佳拟合**：曲线不会通过所有点。将与这个曲线相关的残差。'
- en: Let's look at some different scenarios and study them to understand these differences.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些不同的场景，并研究它们以了解这些差异。
- en: 'Here, we will fit a curve for two numbers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为两个数字拟合曲线：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From this, we will get the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们将得到以下输出：
- en: '![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)'
- en: 'Here, we have used two points to fit the line and we can very well see that
    it becomes an **exact fit**. When introducing three points, we will get the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两个点来拟合直线，我们可以清楚地看到它变成了一个**精确的拟合**。当我们引入三个点时，我们会得到以下结果：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Run the entire code and focus on the output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行整个代码并关注输出：
- en: '![](img/c682382b-fb8a-4d93-abc0-99925922b1b8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c682382b-fb8a-4d93-abc0-99925922b1b8.png)'
- en: Now, you can see the drift and effect of noise. It has started to take the shape
    of a curve. A line might not be a good fit here (however, it's too early to say).
    It's no longer an exact fit.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以看到漂移和噪声的影响。它已经开始呈现出曲线的形状。一条直线可能在这里不是一个好的拟合（然而，说得太早了）。它不再是一个精确的拟合。
- en: What if we try to introduce 100 points and study the effect of that? By now,
    we know how to introduce the number of points.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试引入100个点并研究其效果呢？到目前为止，我们知道如何引入点的数量。
- en: 'By doing this, we get the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们得到以下输出：
- en: '![](img/ef332a8e-1123-44ca-bade-917751904b26.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef332a8e-1123-44ca-bade-917751904b26.png)'
- en: This is not an exact fit, but rather a best fit that tries to generalize the
    whole dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个精确的拟合，而是一个试图概括整个数据集的最佳拟合。
- en: Residual
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差
- en: 'Residuals are the difference between an observed or true value and a predicted
    (fitted) value. For example, in the following diagram, one of the residuals is **(A-B)**,
    where **A** is the observed value and **B** is the fitted value:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是观察值或真实值与预测（拟合）值之间的差异。例如，在以下图中，一个残差是**(A-B)**，其中**A**是观察值，**B**是拟合值：
- en: '![](img/554e01db-d173-45cc-a834-039d7a111e50.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/554e01db-d173-45cc-a834-039d7a111e50.png)'
- en: The preceding scatter plot depicts that we are fitting a line that could represent
    the behavior of all the data points. However, one thing that's noticeable is that
    the line doesn't pass through all of the points. Most of the points are off the
    line.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的散点图表明我们正在拟合一条可能代表所有数据点行为的直线。然而，值得注意的是，这条线并没有穿过所有点。大多数点都在线上方。
- en: The sum and mean of residuals will always be 0. *∑e =0* and mean of *e =0*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 残差的和与平均值总是为0。*∑e =0* 和 *e =0* 的平均值。
- en: Statistical modeling – the two cultures of Leo Breiman
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计建模——莱奥·布雷曼的两种文化
- en: 'Whenever we try to analyze data and finally make a prediction, there are two
    approaches that we consider, both of which were discovered by Leo Breiman, a Berkeley
    professor, in his paper titled *Statistical Modeling: Two Cultures* in 2001.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们试图分析数据并最终做出预测时，我们会考虑两种方法，这两种方法都是由加州大学伯克利分校教授莱奥·布雷曼在他的2001年论文《统计建模：两种文化》中发现的。
- en: 'Any analysis needs data. An analysis can be as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分析都需要数据。分析可以如下进行：
- en: '![](img/5fa263dc-3aee-4d76-a18c-216f46278c34.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fa263dc-3aee-4d76-a18c-216f46278c34.png)'
- en: 'A vector of **X** (**Features**) undergoes a nature box, which translates into
    a response. A nature box tries to establish a relationship between **X** and **Y**.
    Typically, there are goals pertaining to this analysis, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**X**（**特征**）向量经过一个自然箱，这转化为响应。自然箱试图建立**X**和**Y**之间的关系。通常，这个分析有一些目标，如下所示：'
- en: '**Prediction**: To predict the response with the future input features'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：用未来的输入特征来预测响应'
- en: '**Information**: To find out and understand the association between the response
    and driving input variables'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息**：找出并理解响应和驱动输入变量之间的关联'
- en: 'Breiman states that, when it comes to solving business problems, there are
    two distinct approaches:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 布雷曼指出，在解决商业问题时，有两种截然不同的方法：
- en: '**The data modeling culture**: In this kind of model, nature takes the shape
    of a stochastic model that estimates the necessary parameters. Linear regression,
    logistic regression, and the Cox model usually act under the nature box. This
    model talks about observing the pattern of the data and looks to design an approximation
    of what is being observed. Based on their experience, the scientist or a statistician
    would decide which model to be used. It is the case of a model coming before the
    problem and the data, the solutions from this model is more towards the model''s
    architecture. Breiman says that over-reliance on this kind of approach doesn''t
    help the statisticians cater to a diverse set of problems. When it comes to finding
    out solutions pertaining to earthquake prediction, rain prediction, and global
    warming causes, it doesn''t give accurate results, since this approach doesn''t
    focus on accuracy, and instead focuses on the two goals.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据建模文化**：在这种模型中，自然以随机模型的形式出现，该模型估计必要的参数。线性回归、逻辑回归和Cox模型通常在自然框下运行。这种模型讨论的是观察数据的模式，并试图设计出对所观察到的内容的近似。基于他们的经验，科学家或统计学家会决定使用哪种模型。这是模型在问题和数据之前的情况，从这个模型中得出的解决方案更倾向于模型架构。Breiman说，过度依赖这种方法并不能帮助统计学家应对各种问题。当涉及到地震预测、降雨预测和全球变暖原因的解决方案时，它不会给出准确的结果，因为这种方法不关注准确性，而是关注两个目标。'
- en: '**The algorithm modeling culture**: In this approach, pre-designed algorithms
    are used to make a better approximation. Here, the algorithms use complex mathematics
    to reach out to the conclusion and acts inside the nature box. With better computing
    power and using these models, it''s easy to replicate the driving factors as the
    model keeps on running until it learns and understands the pattern that drives
    the outcome. It enables us to address more complex problems, and emphasizes more
    on accuracy. With more data coming through, it can give a much better result than
    the data modeling culture.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法建模文化**：在这种方法中，使用预先设计的算法来做出更好的近似。在这里，算法使用复杂的数学来得出结论，并在自然框内运行。随着计算能力的提高和使用这些模型，很容易复制驱动因素，因为模型会一直运行，直到它学会并理解驱动结果的模式。这使得我们能够解决更复杂的问题，并更强调准确性。随着数据的增加，它比数据建模文化能给出更好的结果。'
- en: Training data development data – test data
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据 - 开发数据 - 测试数据
- en: This is one of the most important steps of building a model and it can lead
    to lots of debate regarding whether we really need all three sets (train, dev,
    and test), and if so, what should be the breakup of those datasets. Let's understand
    these concepts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在构建模型过程中最重要的步骤之一，它可能导致关于我们是否真的需要所有三个集合（训练集、开发集和测试集）以及如果需要，这些数据集应该如何划分的许多争论。让我们理解这些概念。
- en: 'After we have sufficient data to start modelling, the first thing we need to
    do is partition the data into three segments, that is, **Training Set**, **Development**
    **Set**, and **Test Set**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有足够的数据开始建模后，我们首先需要做的是将数据分成三个部分，即**训练集**、**开发集**和**测试集**：
- en: '![](img/936279c1-3f92-41cf-ae3c-751f3f5afc0f.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/936279c1-3f92-41cf-ae3c-751f3f5afc0f.png)'
- en: 'Let''s examine the goal of having these three sets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察拥有这三个集合的目标：
- en: '**Training Set**:The training set is used to train the model. When we apply
    any algorithm, we are fitting the parameter in the training set. In the case of
    a neural network, finding out about the weights takes place.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练集**：训练集用于训练模型。当我们应用任何算法时，我们是在训练集中拟合参数。在神经网络的情况下，找到权重就发生在训练集中。'
- en: 'Let''s say in one scenario that we are trying to fit polynomials of various
    degrees:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在一个场景中，我们正在尝试拟合不同次数的多项式：
- en: '*f(x) = a+ bx* → 1^(st) degree polynomial'
  id: totrans-125
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx* → 1^(st) 次多项式'
- en: '*f(x) = a + bx + cx²* → 2^(nd) degree polynomial'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx + cx²* → 2^(nd) 次多项式'
- en: '*f(x) = a + bx + cx^(2 )+ dx³* → 3^(rd) degree polynomial'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx + cx² + dx³* → 3^(rd) 次多项式'
- en: 'After fitting the model, we calculate the training error for all the fitted
    models:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们计算所有拟合模型的训练误差：
- en: '![](img/4fb00d4e-01ed-4227-9e89-bd91428ee696.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4fb00d4e-01ed-4227-9e89-bd91428ee696.png)'
- en: We cannot assess how good the model is based on the training error. If we do
    that, it will lead us to a biased model that might not be able to perform well
    on unseen data. To counter that, we need to head into the development set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅根据训练误差来评估模型的好坏。如果我们这样做，可能会导致我们得到一个有偏见的模型，这个模型可能无法在未见过的数据上表现良好。为了解决这个问题，我们需要转向开发集。
- en: '**Development** **set**: This is also called the **holdout set** or **validation
    set**. The goal of this set is to tune the parameters that we have got from the
    training set. It is also part of an assessment of how well the model is performing.
    Based on its performance, we have to take steps to tune the parameters. For example,
    controlling the learning rate, minimizing the overfitting, and electing the best
    model of the lot all take place in the development set. Here, again, the development
    set error gets calculated and tuning of the model takes place after seeing which
    model is giving the least error. The model giving the least error at this stage
    still needs tuning to minimize overfitting. Once we are convinced about the best
    model, it is chosen and we head toward the test set.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发集**：这也被称为**保留集**或**验证集**。这个集合的目标是调整我们从训练集得到的参数。它也是对模型性能评估的一部分。根据其性能，我们必须采取步骤来调整参数。例如，控制学习率、最小化过拟合和选择最佳模型都发生在开发集中。在这里，同样会计算开发集误差，并在看到哪个模型产生最小误差后对模型进行调整。在这个阶段产生最小误差的模型仍然需要调整以最小化过拟合。一旦我们确信了最佳模型，它就会被选择，然后我们转向测试集。'
- en: '**Test set**: The test set is primarily used to assess the best selected model.
    At this stage, the accuracy of the model is calculated, and if the model''s accuracy
    is not too deviated from the training accuracy and development accuracy, we send
    this model for deployment.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试集**：测试集主要用于评估最佳选择的模型。在这个阶段，计算模型的准确度，如果模型的准确度没有太大偏离训练准确度和开发准确度，我们就将这个模型部署上线。'
- en: Size of the training, development, and test set
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集、开发集和测试集的大小
- en: 'Typically, machine learning practitioners choose the size of the three sets
    in the ratio of 60:20:20 or 70:15:15\. However, there is no hard and fast rule
    that states that the development and test sets should be of equal size. The following
    diagram shows the different sizes of the training, development, and test sets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习从业者会选择三个集合的大小比例为60:20:20或70:15:15。然而，并没有一条硬性规定说开发集和测试集应该大小相等。以下图表显示了训练集、开发集和测试集的不同大小：
- en: '![](img/9123ff4d-f4ae-454d-9636-21f3f0f47e60.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9123ff4d-f4ae-454d-9636-21f3f0f47e60.png)'
- en: 'Another example of the three different sets is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个三个不同集合的另一个例子：
- en: '![](img/badde5d1-a170-4982-80a6-f8bb01bdc94c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/badde5d1-a170-4982-80a6-f8bb01bdc94c.png)'
- en: But what about the scenarios where we have big data to deal with? For example,
    if we have 10,000,000 records or observations, how would we partition the data?
    In such a scenario, ML practitioners take most of the data for the training set—as
    much as 98-99%—and the rest gets divided up for the development and test sets.
    This is done so that the practitioner can take different kinds of scenarios into
    account. So, even if we have 1% of data for development and the same for the test
    test, we will end up with 100,000 records each, and that is a good number.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何处理大数据的场景呢？例如，如果我们有1000万条记录或观测值，我们该如何划分数据？在这种情况下，机器学习从业者会将大部分数据用于训练集——多达98-99%——其余的则分配给开发集和测试集。这样做是为了让从业者能够考虑不同类型的场景。因此，即使我们只有1%的数据用于开发集和测试集，我们最终也会得到10万条记录，这是一个很好的数字。
- en: Bias-variance trade off
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: 'Before we get into modelling and try to figure out what the trade-off is, let''s
    understand what bias and variance are from the following diagram:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始建模并试图弄清楚权衡是什么之前，让我们从以下图表中了解偏差和方差是什么：
- en: '![](img/13bf10e0-73cb-42ad-afcb-acd933d0dda9.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/13bf10e0-73cb-42ad-afcb-acd933d0dda9.png)'
- en: 'There are two types of errors that are developed in the bias-variance trade
    off, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏差-方差权衡中，产生了两种类型的误差，如下所示：
- en: '**Training error**: This is a measure of deviation of the fitted value from
    the actual value while predicting the output by using the training inputs. This
    error depends majorly on the model''s complexity. As the model''s complexity increases,
    the error appears to plummet.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练误差**：这是在用训练输入预测输出时，拟合值与实际值之间偏差的度量。这个误差主要取决于模型的复杂度。随着模型复杂度的增加，误差似乎会急剧下降。'
- en: '**Development error**: This is a measure of deviation of the predicted value,
    and is used by the development set as input (while using the same model trained
    on training data) from the actual values. Here, the prediction is being done on
    unseen data. We need to minimize this error. Minimizing this error will determine
    how good this model will be in the actual scenario.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发错误**：这是预测值偏差的度量，开发集作为输入（在训练数据上使用相同模型的情况下）用于实际值。在这里，预测是在未见过的数据上进行的。我们需要最小化这个错误。最小化这个错误将决定这个模型在实际场景中的好坏。'
- en: 'As the complexity of the algorithm keeps on increasing, the training error
    goes down. However, the development error or validation error keeps going down
    until a certain point, and then rises, as shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 随着算法复杂性的不断增加，训练错误会下降。然而，开发错误或验证错误会持续下降直到某个点，然后上升，如下面的图表所示：
- en: '![](img/24f381ef-b08b-44e5-9e20-201ac3b1d0cf.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24f381ef-b08b-44e5-9e20-201ac3b1d0cf.png)'
- en: 'The preceding diagram can be explained as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表可以这样解释：
- en: '**Underfitting**: Every dataset has a specific pattern and properties due to
    the existing variables in the dataset. Along with that, it also has a random and
    latent pattern which is caused by the variables that are not part of the dataset.
    Whenever we come up with a model, the model should ideally be learning patterns
    from the existing variables. However, the learning of these patterns also depends
    on how good and robust your algorithm is. Let''s say we have picked up a model
    that is not able to derive even the essential patterns out of the dataset—this
    is called **underfitting**. In the preceding plots, it is a scenario of classification
    and we are trying to classify *x* and *o*. In plot 1, we are trying to use a linear
    classification algorithm to classify the data, but we can see that it is resulting
    in lots of misclassification errors. This is a case of underfitting.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：每个数据集都有特定的模式和属性，这是由于数据集中存在的变量所导致的。除此之外，它还有由不是数据集部分的变量引起的随机和潜在模式。每当我们要提出一个模型时，理想情况下模型应该从现有变量中学习模式。然而，这些模式的学习也取决于你的算法有多好、有多稳健。假设我们选择了一个无法从数据集中推导出基本模式的模型——这被称为**欠拟合**。在前面的图表中，这是一个分类场景，我们正在尝试对*x*和*o*进行分类。在图1中，我们试图使用线性分类算法对数据进行分类，但我们可以看到它导致了大量的误分类错误。这是一个欠拟合的例子。'
- en: '**Overfitting**: Going further afield from plot 1, we are trying to use complex
    algorithms to find out the patterns and classify them. It is noticeable that the
    misclassification errors have gone down in the second plot, since the complex
    model being used here is able to detect the patterns. The development error (as
    shown in the preceding diagram) goes down too. We will increase the complexity
    of the model and see what happens. Plot 3 suggests that there is no misclassification
    error in the model now. However, if we look at the plot below it, we can see that
    the development error is way too high now. This happens because the model is learning
    from the misleading and random patterns that were exhibited due to the non-existent
    variables in the dataset. This means that it has started to learn the noise that''s
    present in the set. This phenomenon is called **overfitting**.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：从图1进一步扩展，我们试图使用复杂的算法来找出模式并进行分类。值得注意的是，在第二个图表中，误分类错误已经下降，因为在这里使用的复杂模型能够检测到模式。开发错误（如前图所示）也下降了。我们将增加模型的复杂性并观察会发生什么。图3表明，现在模型中没有误分类错误。然而，如果我们看下面的图表，我们可以看到开发错误现在非常高。这是因为模型正在从数据集中不存在的变量所展示的误导性和随机模式中学习。这意味着它已经开始学习集合中存在的噪声。这种现象被称为**过拟合**。'
- en: '**Bias**: How often have we seen this? This occurs in a situation wherein we
    have used an algorithm and it doesn''t fit properly. This means that the function
    that''s being used here has been of little relevance to this scenario and it''s
    not able to extract the correct patterns. This causes an error called **bias**.
    It crops up majorly due to making a certain assumption about the data and using
    a model that might be correct but isn''t. For example, if we had to use a second
    degree polynomial for a situation, we would use simple linear regression, which
    doesn''t establish a correct relationship between the response and explanatory
    variables.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：我们见过这种情况有多少次？这种情况发生在我们使用了一个算法，但它并不合适。这意味着在这里被使用的函数对这个场景的相关性很小，并且它无法提取正确的模式。这导致了一个称为**偏差**的错误。这种情况主要是由于对数据做出了一定的假设，并使用了一个可能正确但并不正确的模型。例如，如果我们必须为某种情况使用二次多项式，我们会使用简单的线性回归，这并不能建立响应变量和解释变量之间的正确关系。'
- en: '**Variance**: When we have a dataset that is being used for training the model,
    the model should remain immune, even if we change the training set to a set that''s
    coming from the same population. If variation in the dataset brings in a change
    in the performance of the model, it is termed a **variance error**. This takes
    place due to noise (an unexplained variation) being learned by the model and,
    due to that, this model doesn''t give a good result on unseen data:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**：当我们有一个用于训练模型的训练数据集时，即使我们将训练集更改为来自同一群体的集合，模型也应该保持免疫。如果数据集中的变化导致模型性能的变化，这被称为**方差错误**。这种情况是由于模型学习到了噪声（未解释的变化）而发生的，因此，这个模型在未见过的数据上不能给出好的结果：'
- en: '![](img/c371896e-14ed-449f-a25c-72c9f845cd9f.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c371896e-14ed-449f-a25c-72c9f845cd9f.png)'
- en: 'We will explain the preceding diagram as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如下解释前面的图表：
- en: If the **Training Error** goes down and (**Development Error**-**Training Error**)
    rises, it implies a **High Variance** situation (scenario 1 in the preceding table)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**训练误差**下降，而（**开发误差**-**训练误差**）上升，这意味着**高方差**的情况（前表中的第1种情况）
- en: If the **Training Error** and **Development Error** rises and (**Development
    Error**-**Training Error**) goes down, it implies a **High Bias** situation (scenario
    2 in the preceding table)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**训练误差**和**开发误差**上升，而（**开发误差**-**训练误差**）下降，这意味着**高偏差**的情况（前表中的第2种情况）
- en: If the **Training Error** and **Development Error** rises and (**Development
    Error**-**Training Error**) goes up as well, it implies **High Bias** and **High
    Variance** (scenario 3 in the preceding table)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**训练误差**和**开发误差**上升，并且（**开发误差**-**训练误差**）也上升，这意味着**高偏差**和**高方差**（前表中的第3种情况）
- en: If the **Training Error** goes up and the **Development Error** declines, that
    is, (**Development Error**-**Training Error**) goes down, it implies **Low Bias**
    and **Low Variance** (scenario 4 in the preceding table)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**训练误差**上升，而**开发误差**下降，即（**开发误差**-**训练误差**）下降，这意味着**低偏差**和**低方差**（前表中的第4种情况）
- en: We should always strive for the fourth scenario, which depicts the training
    error being low, as well as a low development set error. In the preceding table,
    this is where we have to find out a bias variance trade-off, which is depicted
    by a vertical line.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该始终努力追求第4种情况，即训练误差低，以及开发集误差低。在前表中，这就是我们必须找出偏差-方差权衡的地方，它由一条垂直线表示。
- en: 'Now, the following question arises: how we can counter overfitting? Let''s
    find out the answer to this by moving on to the next section.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，以下问题产生了：我们如何对抗过拟合？让我们通过继续下一节来找出这个答案。
- en: Regularization
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: We have now got a fair understanding of what overfitting means when it comes
    to machine learning modeling. Just to reiterate, when the model learns the noise
    that has crept into the data, it is trying to learn the patterns that take place
    due to random chance, and so overfitting occurs. Due to this phenomenon, the model's
    generalization runs into jeopardy and it performs poorly on unseen data. As a
    result of that, the accuracy of the model takes a nosedive.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对机器学习建模中过拟合的含义有了相当的了解。只是为了重申，当模型学习到数据中渗透的噪声时，它试图学习由于随机机会发生的模式，因此发生过拟合。由于这种现象，模型的泛化能力受到威胁，它在未见过的数据上的表现不佳。因此，模型的准确性急剧下降。
- en: Can we combat this kind of phenomenon? The answer is yes. Regularization comes
    to the rescue. Let's figure out what it can offer and how it works.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否对抗这种现象？答案是肯定的。正则化来拯救。让我们弄清楚它能提供什么以及它是如何工作的。
- en: Regularization is a technique that enables the model to not become complex to
    avoid overfitting.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种技术，它使模型不会变得复杂，以避免过拟合。
- en: 'Let''s take a look at the following regression equation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的回归方程：
- en: '![](img/107b0b1a-755e-4c96-b235-5e4a9a676cf2.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/107b0b1a-755e-4c96-b235-5e4a9a676cf2.png)'
- en: 'The loss function for this is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数如下所示：
- en: '![](img/69cb5478-5ca8-45e7-9123-c85fd63b62e0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69cb5478-5ca8-45e7-9123-c85fd63b62e0.png)'
- en: The loss function would help in getting the coefficients adjusted and retrieving
    the optimal one. In the case of noise in the training data, the coefficients wouldn't
    generalize well and would run into overfitting. Regularization helps get rid of
    this by making these estimates or coefficients drop toward 0.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数有助于调整系数并检索最优值。在训练数据中存在噪声的情况下，系数不会很好地泛化，并会遇到过拟合。正则化通过使这些估计或系数趋向于0来解决这个问题。
- en: Now, we will cover two types of regularization. In later chapters, the other
    types will be covered.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍两种类型的正则化。在后面的章节中，将介绍其他类型。
- en: Ridge regression (L2)
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 岭回归（L2）
- en: 'Due to ridge regression, we need to make some changes to the loss function.
    The original loss function gets added by a shrinkage component:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于岭回归，我们需要对损失函数做一些修改。原始损失函数通过一个收缩成分来增加：
- en: '![](img/c423cec4-f053-4798-8e0e-bfcf116acdcd.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c423cec4-f053-4798-8e0e-bfcf116acdcd.png)'
- en: Now, this modified loss function needs to be minimized to adjust the estimates
    or coefficients. Here, the lambda is tuning the parameter that regularizes the
    loss function. That is, it decides how much it should penalize the flexibility
    of the model. The flexibility of the model is dependent on the coefficients. If
    the coefficients of the model go up, the flexibility also goes up, which isn't
    a good sign for our model. Likewise, as the coefficients go down, the flexibility
    is restricted and the model starts to perform better. The shrinkage of each estimated
    parameter makes the model better here, and this is what ridge regression does.
    When lambda keeps going higher and higher, that is, *λ → ∞*, the penalty component
    rises, and the estimates start shrinking. However, when *λ **→ 0*, the penalty
    component decreases and starts to become an **ordinary least square** (**OLS**)
    for estimating unknown parameters in a linear regression.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个修改后的损失函数需要被最小化以调整估计或系数。在这里，lambda正在调整正则化损失函数的参数。也就是说，它决定了应该对模型的灵活性进行多少惩罚。模型的灵活性取决于系数。如果模型的系数增加，灵活性也会增加，这对我们的模型来说不是好兆头。同样，当系数减少时，灵活性受到限制，模型开始表现更好。每个估计参数的收缩使模型在这里变得更好，这就是岭回归所做的事情。当lambda不断升高时，即λ
    → ∞，惩罚成分增加，估计开始缩小。然而，当λ **→ 0**时，惩罚成分减少并开始成为一个**普通最小二乘法**（OLS）来估计线性回归中的未知参数。
- en: Least absolute shrinkage and selection operator
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最小绝对收缩和选择算子
- en: 'The **least absolute shrinkage and selection operator** (**LASSO**) is also
    called *L1*. In this case, the preceding penalty parameter is replaced by *|βj|*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**最小绝对收缩和选择算子**（LASSO）也称为*L1*。在这种情况下，前面的惩罚参数被*|βj|*替换：'
- en: '![](img/41f4f28d-b58f-4302-b8b4-39ccc8340d14.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41f4f28d-b58f-4302-b8b4-39ccc8340d14.png)'
- en: By minimizing the preceding function, the coefficients are found and adjusted.
    In this scenario, as lambda becomes larger, *λ → ∞*, the penalty component rises,
    and so estimates start shrinking and become 0 (it doesn't happen in the case of
    ridge regression; rather, it would just be close to 0).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化前面的函数，找到并调整系数。在这种情况下，当lambda变得更大，λ → ∞，惩罚成分增加，因此估计开始缩小并变为0（在岭回归的情况下不会发生；相反，它只会接近0）。
- en: Cross-validation and model selection
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证和模型选择
- en: We have already spoken about overfitting. It is something to do with the stability
    of a model since the real test of a model occurs when it works on unseen and new
    data. One of the most important aspects of a model is that it shouldn't pick up
    on noise, apart from regular patterns.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了过拟合。这与模型的不稳定性有关，因为模型的真正测试发生在它处理未见和新的数据时。模型最重要的一个方面是，它不应该捕捉到噪声，而不仅仅是常规模式。
- en: Validation is nothing but an assurance of the model being a relationship between
    the response and predictors as the outcome of input features and not noise. A
    good indicator of the model is not through training data and error. That's why
    we need cross-validation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 验证只是确保模型是响应和预测变量之间关系的一种保证，作为输入特征的输出结果，而不是噪声。一个良好的模型指标不是通过训练数据和错误来衡量。这就是为什么我们需要交叉验证。
- en: Here, we will stick with k-fold cross-validation and understand how it can be
    used.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将坚持使用k折交叉验证，并了解它是如何被使用的。
- en: K-fold cross-validation
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: 'Let''s walk through the steps of k-fold cross-validation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步了解k折交叉验证的步骤：
- en: The data is divided into k-subsets.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据被划分为k个子集。
- en: One set is kept for testing/development and the model is built on the rest of
    the data (*k-1*). That is, the rest of the data forms the training data.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组数据被保留用于测试/开发，而模型建立在其余数据(*k-1*)上。也就是说，其余的数据形成训练数据。
- en: '*Step 2* is repeated k-times. That is, once the preceding step has been performed,
    we move on to the second set and it forms a test set. The rest of the (*k-1*)
    data is then available for building the model:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第二步*会重复k次。也就是说，一旦前一步完成，我们就转向第二组，并形成测试集。剩下的(*k-1*)数据随后可用于构建模型：'
- en: '![](img/b90b29ab-dfe7-4c11-9a2f-321e84f79495.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b90b29ab-dfe7-4c11-9a2f-321e84f79495.png)'
- en: 4\. An error is calculated and an average is taken over all k-trials.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 计算一个错误并取所有k次试验的平均值。
- en: Every subset gets one chance to be a validation/test set since most of the data
    is used as a training set. This helps in reducing bias. At the same time, almost
    all the data is being used as validation set, which reduces variance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子集都有一次机会成为验证/测试集，因为大部分数据被用作训练集。这有助于减少偏差。同时，几乎所有数据都被用作验证集，这有助于减少方差。
- en: As shown in the preceding diagram, *k = 5* has been selected. This means that
    we have to divide the whole dataset into five subsets. In the first iteration,
    subset 5 becomes the test data and the rest becomes the training data. Likewise,
    in the second iteration, subset 4 turns into the test data and the rest becomes
    the training data. This goes on for five iterations.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，*k = 5*已被选择。这意味着我们必须将整个数据集划分为五个子集。在第一次迭代中，子集5成为测试数据，其余成为训练数据。同样，在第二次迭代中，子集4变为测试数据，其余成为训练数据。这个过程会持续五次迭代。
- en: 'Now, let''s try to do this in Python by splitting the train and test data using
    the K neighbors classifier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试用Python通过使用K近邻分类器来分割训练数据和测试数据来完成这个任务：
- en: '[PRE5]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Model selection using cross-validation
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证进行模型选择
- en: 'We can make use of cross-validation to find out which model is performing better
    by using the following code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码使用交叉验证来找出哪个模型表现更好：
- en: '[PRE6]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The 10-fold cross-validation is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 10折交叉验证如下：
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 0.632 rule in bootstrapping
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 0.632规则在自助法中
- en: Before we get into the 0.632 rule of bootstrapping, we need to understand what
    bootstrapping is. Bootstrapping is the process wherein random sampling is performed
    with a replacement from a population that's comprised of *n* observations. In
    this scenario, a sample can have duplicate observations. For example, if the population
    is (2,3,4,5,6) and we are trying to draw two random samples of size 4 with replacement,
    then sample 1 will be (2,3,3,6) and sample 2 will be (4,4,6,2).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨自助法中的0.632规则之前，我们需要了解什么是自助法。自助法是从由*n*个观察组成的总体中进行有放回随机抽样的过程。在这种情况下，样本可以包含重复的观察。例如，如果总体是(2,3,4,5,6)，而我们试图抽取两个大小为4的随机样本，那么样本1将是(2,3,3,6)，样本2将是(4,4,6,2)。
- en: Now, let's delve into the 0.632 rule.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解0.632规则。
- en: 'We have already seen that the estimate of the training error while using a
    prediction is *1/n ∑L(y[i,]y-hat)*. This is nothing but the loss function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在使用预测估计训练误差时，*1/n ∑L(y[i,]y-hat)*。这实际上就是损失函数：
- en: '![](img/dfea359d-89d5-48f1-93e5-2d6bf1a50c7c.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dfea359d-89d5-48f1-93e5-2d6bf1a50c7c.png)'
- en: 'Cross-validation is a way to estimate the expected output of a sample error:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种估计样本误差预期输出的方法：
- en: '![](img/5863896c-0524-4013-a73a-cd6b176f4183.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5863896c-0524-4013-a73a-cd6b176f4183.png)'
- en: 'However, in the case of k-fold cross-validation, it is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在k折交叉验证的情况下，它如下所示：
- en: '![](img/baff7e70-d144-4371-a088-6ffd520061d0.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/baff7e70-d144-4371-a088-6ffd520061d0.png)'
- en: Here, the training data is *X=(x1,x2.....,xn)* and we take bootstrap samples
    from this set *(z1,.....,zb)* where each *zi* is a set of *n* samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，训练数据是*X=(x1,x2.....,xn)*，我们从该集合中抽取自助样本*(z1,.....,zb)*，其中每个*zi*是一组*n*个样本。
- en: 'In this scenario, the following is our out-of-sample error:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，以下是我们样本外误差：
- en: '![](img/5571d600-2600-4985-a2d5-ae703b7eceec.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5571d600-2600-4985-a2d5-ae703b7eceec.png)'
- en: Here, *fb(xi)* is the predicted value at *xi *from the model that's been fit
    to the bootstrap dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*fb(xi)* 是从拟合到 bootstrap 数据集的模型在 *xi* 处的预测值。
- en: 'Unfortunately, this is not a particularly good estimator because bootstrap
    samples that have been used to produce *fb(xi)* may have contained *xi*. OOSE solves
    the overfitting problem, but is still biased. This bias is due to non-distinct
    observations in the bootstrap samples that result from sampling with replacement.
    The average number of distinct observations in each sample is about *0.632n*. To
    solve the bias problem, Efron and Tibshirani proposed the *0.632* estimator:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这并不是一个特别好的估计器，因为用于生成 *fb(xi)* 的 bootstrap 样本可能已经包含了 *xi*。OOSE 解决了过拟合问题，但仍然存在偏差。这种偏差是由于在带有替换的抽样中产生的
    bootstrap 样本中的非独特观察结果。每个样本中独特观察的平均数量约为 *0.632n*。为了解决偏差问题，Efron 和 Tibshirani 提出了
    *0.632* 估计器：
- en: '![](img/ff94ac0d-9c57-454c-9943-1fb1983f5632.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ff94ac0d-9c57-454c-9943-1fb1983f5632.png)'
- en: Model evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Let's look at some of the model evaluation techniques that are currently being
    used.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看目前正在使用的某些模型评估技术。
- en: Confusion matrix
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix is a table that helps in assessing how good the classification
    model is. It is used when true values/labels are known. Most beginners in the
    field of data science feel intimidated by the confusion matrix and think it looks
    more difficult to comprehend than it really is; let me tell you—it's pretty simple
    and easy.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个表格，有助于评估分类模型的好坏。它在已知真实值/标签时使用。大多数数据科学领域的初学者都会对混淆矩阵感到害怕，认为它比实际情况更难理解；让我告诉你——它相当简单且容易理解。
- en: Let's understand this by going through an example. Let's say that we have built
    a classification model that predicts whether a customer would like to buy a certain
    product or not. To do this, we need to assess the model on unseen data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设我们构建了一个分类模型，用于预测客户是否会购买某种产品。为此，我们需要在未见过的数据上评估模型。
- en: 'There are two classes:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个类别：
- en: '**Yes**: The customer will buy the product'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**是**：客户会购买产品'
- en: '**No**: The customer will not buy the product'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**否**：客户不会购买产品'
- en: 'From this, we have put the matrix together:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们构建了以下矩阵：
- en: '![](img/d2619d94-1b89-4ec3-b363-e816077f5e39.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d2619d94-1b89-4ec3-b363-e816077f5e39.png)'
- en: What are the inferences we can draw from the preceding matrix at first glance?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的矩阵中，我们可以得出哪些推断？
- en: The classifier has made a total of 80 predictions. What this means is that 80
    customers were tested in total to find out whether he/she will buy the product
    or not.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器总共做出了 80 次预测。这意味着总共测试了 80 名客户，以确定他们是否会购买产品。
- en: '**54** customers bought the product and **26** didn''t.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**54** 名客户购买了产品，而 **26** 名没有。'
- en: 'The classifier predicts that **56** customers will buy the product and that **24**
    won''t:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器预测 **56** 名客户会购买产品，而 **24** 名不会：
- en: '![](img/0b20dc6e-24bf-4e3b-bd6a-538c81b2a894.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0b20dc6e-24bf-4e3b-bd6a-538c81b2a894.png)'
- en: 'The different terms pertaining to the confusion matrix are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与混淆矩阵相关的不同术语如下：
- en: '**True Positive (TP)**: These are the cases in which we predicted that the
    customer will buy the product and they did.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性 (TP)**: 这些是我们预测客户会购买产品，而他们确实购买了的情况。'
- en: '**True Negative (TN)**: These are the cases in which we predicted that the
    customer won''t buy the product and they didn''t.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性 (TN)**: 这些是我们预测客户不会购买产品，而他们确实没有购买的情况。'
- en: '**False Positive (FP)**: We predicted *Yes the customer will buy the product*,
    but they didn''t. This is known as a *Type 1* error.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性 (FP)**: 我们预测客户会购买产品，但他们没有。这被称为*类型 1*错误。'
- en: '**False Negative (FN)**: We predicted *No*, but the customer bought the product.
    This is known as a *Type 2* error.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性 (FN)**: 我们预测 *不*，但客户购买了产品。这被称为*类型 2*错误。'
- en: 'Now, let''s talk about a few metrics that are required for the assessment of
    a classification model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈评估分类模型所需的几个指标：
- en: '**Accuracy**: This measures the overall accuracy of the classifier. To calculate
    this, we will use the following formula: *(TP+TN)/Total cases*. In the preceding
    scenario, the accuracy is (50+20)/80, which turns out to be 0.875\. So, we can
    say that this classifier will predict correctly in 87.5% of scenarios.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**: 这衡量了分类器的整体准确度。为了计算这个值，我们将使用以下公式：*(TP+TN)/总案例数*。在先前的场景中，准确率是(50+20)/80，结果是0.875。因此，我们可以说这个分类器在87.5%的场景中会做出正确的预测。'
- en: '**Misclassification rate**: This measures how often the classifier has got
    the results wrong. The formula *(FP+FN)/Total cases* will give the result. In
    the preceding scenario, the misclassification rate is *(6+4)/80*, which is 0.125\.
    So, in 12.5% of cases, it won''t produce correct results. It can also be calculated
    as (1- Accuracy).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误分类率**: 这衡量了分类器结果错误发生的频率。公式*(FP+FN)/总案例数*将给出结果。在先前的场景中，误分类率是*(6+4)/80*，即0.125。因此，在12.5%的案例中，它不会产生正确的结果。它也可以用(1-准确率)来计算。'
- en: '**TP rate**: This is a measure of what the chances are that it would predict
    *yes* as the answer, and the answer actually is *yes*. The formula to calculate
    this is **TP/(Actual:Yes)**. In this scenario, *TPR = (50/54)= 0.92*. It''s also
    called **Sensitivity** or **Recall**.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TP率**: 这是一个衡量预测答案为*是*的概率，并且实际答案确实是*是*的指标。计算这个指标的公式是**TP/(实际:是)**。在这个场景中，*TPR
    = (50/54)= 0.92*。它也被称为**灵敏度**或**召回率**。'
- en: '**FP rate**: This is a measure of what the chances are that it would predict
    *yes*, when the actual answer is *no*. The formula to calculate this rate is **FP/(Actual:No)**.For
    the preceding example, *FPR = (6/26)= 0.23*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FP率**: 这是一个衡量预测答案为*是*，而实际答案是*否*的概率的指标。计算这个率的公式是**FP/(实际:否)**。在先前的例子中，*FPR
    = (6/26)= 0.23*。'
- en: '**TN rate**: This is a measure of what the chances are that it would predict
    *no*, when the answer is actually *no*. The formula to calculate this is *TN/(Actual:No)*.
    In this scenario, *TNR= (20/26)= 0.76*. It can also be calculated using (1-FPR).
    It''s also called **Specificity**.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TN率**: 这衡量的是当答案是*否*时，预测*否*的概率。计算这个率的公式是*TN/(实际:否)*。在这个场景中，*TNR= (20/26)=
    0.76*。它也可以用(1-FPR)来计算。它也被称为**特异性**。'
- en: '**Precision**: This is a measure of correctness of the prediction of *yes* out
    of all the *yes* predictions. It finds out how many times a prediction of *yes*
    was made correctly out of total *yes* predictions. The formula to calculate this
    is *TP/(Predicted:Yes)*. Here, *Precision = (50/56)=0.89*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**: 这衡量了在所有预测为*是*的预测中预测*是*的正确性。它找出在总的*是*预测中，预测为*是*的正确次数。计算这个的公式是*TP/(预测:是)*。在这里，*精确率
    = (50/56)=0.89*。'
- en: '**Prevalence**: This is a measure of how many *yes* were given out of the total
    sample. The formula is *(Actual:Yes/ Total Sample)*. Here, this is *54/80 = 0.67*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**患病率**: 这衡量了在总样本中给出*是*的比例。公式是*(实际:是/总样本)*。在这里，这是*54/80 = 0.67*。'
- en: '**Null error rate**: This is a measure of how wrong the classifier would be
    if it predicted just the majority class. The formula is *(Actual:No/Total Sample)*.
    Here, this is *26/80=0.325*.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空错误率**: 这衡量了如果分类器只预测多数类，它会有多错误。公式是*(实际:否/总样本)*。在这里，这是*26/80=0.325*。'
- en: '**Cohen''s Kappa value**: This is a measure of how well the classifier performed
    compared to how well it would have performed simply by chance.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cohen''s Kappa值**: 这是一个衡量分类器性能相对于仅凭机会表现的好坏程度的指标。'
- en: '**F-Score**: This is a harmonic mean of recall and precision, that is, *(2*Recall*Precision)/(Recall+Precision)*.
    It considers both Recall and Precision as important measures of a model''s evaluation.
    The best value of the F-score is 1, wherein Recall and Precision are at their
    maximum. The worst value of the F-score is 0\. The higher the score, the better
    the model is:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-Score**: 这是召回率和精确率的调和平均数，即*(2*召回率*精确率)/(召回率+精确率)*。它将召回率和精确率都视为模型评估的重要指标。F-Score的最佳值是1，此时召回率和精确率都达到最大。F-Score的最差值是0。分数越高，模型越好：'
- en: '![](img/c1c5e09b-fc65-4721-92c0-474cf4b1ff56.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c1c5e09b-fc65-4721-92c0-474cf4b1ff56.png)'
- en: Receiver operating characteristic curve
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收者操作特征曲线
- en: 'We have come across many budding data scientists who would build a model and,
    in the name of evaluation, are just content with the **overall accuracy**. However,
    that''s not the correct way to go about evaluating a model. For example, let''s
    say there''s a dataset that has got a response variable that has two categories:
    customers willing to buy the product and customers not willing to buy the product.
    Let''s say that the dataset has 95% of customers not willing to buy the product
    and 5% of customers willing to buy it. Let''s say that the classifier is able
    to correctly predict the majority class and not the minority class. So, if there
    are 100 observations, *TP=0*, *TN= 95*, and the rest misclassified, this will
    still result in 95% accuracy. However, it won''t be right to conclude that this
    is a good model as it''s not able to classify the minority class at all.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到过许多初露头角的数据科学家，他们构建模型，并以评估的名义，只是满足于**整体准确性**。然而，这并不是评估模型的正确方法。例如，假设有一个数据集，其响应变量有两个类别：愿意购买产品的客户和不愿意购买产品的客户。假设该数据集中有95%的客户不愿意购买产品，5%的客户愿意购买。假设分类器能够正确预测多数类而不是少数类。所以，如果有100个观测值，*TP=0*，*TN=95*，其余的都被错误分类，这仍然会导致95%的准确率。然而，不能得出这是一个好模型的结论，因为它根本无法对少数类进行分类。
- en: Hence, we need to look beyond accuracy so that we have a better judgement about
    the model. In this situation, Recall, Specificity, Precision, and the **receiver
    operating characteristic** (**ROC**) curve come to rescue. We learned about Recall,
    specificity, and precision in the previous section. Now, let's understand what
    the ROC curve is.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要超越准确性，以便对模型有更好的判断。在这种情况下，召回率、特异性、精确度和**接收者操作特征**（ROC）曲线就派上用场了。我们在上一节中学习了召回率、特异性和精确度。现在，让我们了解ROC曲线是什么。
- en: 'Most of the classifiers produce a score between 0 and 1\. The next step occurs
    when we''re setting up the threshold, and, based on this threshold, the classification
    is decided. Typically, 0.5 is the threshold—if it''s more than 0.5, it creates
    a class, 1, and if the threshold is less than 0.5 it falls into another class,
    2:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分类器产生的分数介于0和1之间。下一步发生在我们设置阈值时，根据这个阈值，分类被决定。通常，0.5是阈值——如果它大于0.5，就创建一个类别1，如果阈值小于0.5，它就属于另一个类别2：
- en: '![](img/46bc9f8a-2b3c-4985-8b09-17bafb7d919a.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46bc9f8a-2b3c-4985-8b09-17bafb7d919a.png)'
- en: 'For ROC, every point between **0.0** and **1.0** is treated as a threshold,
    so the line of threshold keeps on moving from **0.0** to **1.0**. The threshold
    will result in us having a TP, TN, FP, and FN. At every threshold, the following
    metrics are calculated:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ROC，**0.0**到**1.0**之间的每个点都被视为一个阈值，因此阈值线持续从**0.0**移动到**1.0**。阈值将导致我们得到TP、TN、FP和FN。在每一个阈值，以下指标都会被计算：
- en: '*True Positive Rate = TP/(TP+FN)*'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阳性率 = TP/(TP+FN)*'
- en: '*True Negative Rate = TN/(TN + FP)*'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阴性率 = TN/(TN + FP)*'
- en: '*False Positive Rate = 1- True Negative Rate*'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阳性率 = 1 - 真阴性率*'
- en: The calculation of (TPR and FPR) starts from 0\. When the threshold line is
    at 0, we will be able to classify all of the customers who are willing to buy
    (positive cases), whereas those who are not willing to buy will be misclassified
    as there will be too many false positives. This means that the threshold line
    will start moving toward the right from zero. As this happens, the false positive
    starts to decline and the true positive will continue increasing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: （TPR和FPR）的计算从0开始。当阈值线在0时，我们将能够将所有愿意购买的客户（正例）分类，而不愿意购买的客户将被错误分类，因为会有太多的假阳性。这意味着阈值线将从零开始向右移动。随着这种情况的发生，假阳性开始下降，真阳性将继续增加。
- en: 'Finally, we will need to plot a graph of the TPR versus FPR after calculating
    them at every point of the threshold:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要在计算了每个阈值点后的TPR和FPR后，绘制一个TPR与FPR的图表：
- en: '![](img/d6092b1e-d657-46ef-9d5e-a8b25de6f5c5.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6092b1e-d657-46ef-9d5e-a8b25de6f5c5.png)'
- en: The red diagonal line represents the classification at random, that is, classification
    without the model. The perfect ROC curve will go along the *y* axis and will take
    the shape of an absolute triangle, which will pass through the top of the *y*
    axis.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 红色对角线代表随机分类，即没有模型的分类。完美的ROC曲线将沿着**y**轴延伸，并呈现一个绝对三角形的形状，穿过**y**轴的顶部。
- en: Area under ROC
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ROC曲线下的面积
- en: To assess the model/classifier, we need to determine the **area under ROC**
    (**AUROC**). The whole area of this plot is 1 as the maximum value of FPR and TPR
    – both are 1 here. Hence, it takes the shape of a square. The random line is positioned
    perfectly at 45 degrees, which partitions the whole area into two symmetrical
    and equilateral triangles. This means that the areas under and above the red line
    are 0.5\. The best and perfect classifier will be the one that tries to attain
    the AUROC as 1\. The higher the AUROC, the better the model is.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型/分类器，我们需要确定**ROC曲线下面积**（**AUROC**）。这个图的总面积是1，因为FPR和TPR的最大值都是1。因此，它呈现出一个正方形的形状。随机线完美地定位在45度角，将整个区域分成两个对称且等边的三角形。这意味着红色线以下和以上的面积都是0.5。最佳和完美的分类器将是试图将AUROC达到1的那个。AUROC越高，模型就越好。
- en: In a situation where you have got multiple classifiers, you can use AUROC to
    determine which is the best one among the lot.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在你拥有多个分类器的情况下，你可以使用AUROC来确定其中哪一个是最优的。
- en: H-measure
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: H度量
- en: Binary classification has to apply techniques so that it can map independent
    variables to different labels. For example, a number of variables exist such as
    gender, income, number of existing loans, and payment on time/not, that get mapped
    to yield a score that helps us classify the customers into good customers (more
    propensity to pay) and bad customers.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类必须应用技术，以便将独立变量映射到不同的标签。例如，存在一些变量，如性别、收入、现有贷款数量以及按时/不按时付款，这些变量被映射以产生一个分数，帮助我们将客户分类为良好客户（更有可能付款）和不良客户。
- en: Typically, everyone seems to be caught up with the misclassification rate or
    derived form since the **area under curve** (**AUC**) is known to be the best
    evaluator of our classification model. You get this rate by dividing the total
    number of misclassified examples by the total number of examples. But does this
    give us a fair assessment? Let's see. Here, we have a misclassification rate that
    keeps something important under wraps. More often than not, classifiers come up
    with a tuning parameter, the side effect of which tends to be favoring false positives
    over false negatives, or vice versa. Also, picking the AUC as sole model evaluator
    can act as a double whammy for us. AUC has got different misclassification costs
    for different classifiers, which is not desirable. This means that using this is
    equivalent to using different metrics to evaluate different classification rules.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个人都似乎陷入了误分类率或其导出的形式，因为众所周知，**曲线下面积**（**AUC**）是我们分类模型的最佳评估者。你是通过将误分类的实例总数除以实例总数来得到这个率的。但这真的给我们一个公平的评估吗？让我们看看。在这里，我们有一个误分类率，它隐藏了一些重要的东西。很多时候，分类器会提出一个调整参数，其副作用往往是倾向于优先考虑假阳性而不是假阴性，或者反之亦然。此外，将AUC作为唯一的模型评估者可能会对我们产生双重打击。AUC对不同分类器有不同的误分类成本，这是不可取的。这意味着使用这个相当于使用不同的指标来评估不同的分类规则。
- en: As we have already discussed, the real test of any classifier takes place on
    the unseen data, and this takes a toll on the model by some decimal points. Adversely,
    if we have got scenarios like the preceding one, the decision support system will
    not be able to perform well. It will start producing misleading results.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经讨论过的，任何分类器的真正测试发生在未见过的数据上，这会在模型上造成一些小数点的损失。相反，如果我们有前面提到的那种场景，决策支持系统将无法很好地执行。它将开始产生误导性的结果。
- en: 'H-measure overcomes the situation of incurring different misclassification
    costs for different classifiers. It needs a severity ratio as input, which examines
    how much more severe misclassifying a class 0 instance is than misclassifying
    a class 1 instance:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: H度量克服了不同分类器产生不同误分类成本的情况。它需要一个严重性比率作为输入，该比率检查将一个类别0实例误分类比将一个类别1实例误分类严重多少：
- en: '*Severity Ratio = cost_0/cost_1*'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*严重性比率 = cost_0/cost_1*'
- en: Here, *cost_0 > 0* is the cost of misclassifying a class 0 datapoint as class
    1.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*cost_0 > 0* 是将类别0数据点误分类为类别1的成本。
- en: It is sometimes more convenient to consider the normalized cost *c = cost_0/(cost_0
    + cost_1)* instead. For example, *severity.ratio = 2* implies that a false positive
    costs twice as much as a false negative.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有时考虑归一化成本 *c = cost_0/(cost_0 + cost_1)* 更为方便。例如，*severity.ratio = 2* 意味着假阳性成本是假阴性的两倍。
- en: Dimensionality reduction
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度降低
- en: Let's talk about a scenario wherein we have been given a dataset from a bank
    and it has got features pertaining to bank customers. These features comprise
    customer's income, age, gender, payment behavior, and so on. Once you take a look
    at the data dimension, you realize that there are 850 features. You are supposed
    to build a model to predict the customer who is going to default if a loan is
    given. Would you take all of these features and build the model?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一个场景，即我们从一个银行得到了一个数据集，它包含了与银行客户相关的特征。这些特征包括客户的收入、年龄、性别、支付行为等。一旦你查看数据维度，你就会意识到有850个特征。你被要求构建一个模型来预测如果发放贷款，哪些客户将会违约。你会使用所有这些特征来构建模型吗？
- en: The answer should be a clear **no**. The more features in a dataset, the more
    likely it is that the model will overfit. Although having fewer features doesn't
    guarantee that overfitting won't take place, it reduces the chance of that. Not
    a bad deal, right?
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 答案应该是明确的 **不**。数据集中特征越多，模型过拟合的可能性就越大。尽管拥有较少的特征并不能保证不会发生过拟合，但它减少了这种可能性。这不是一个坏交易，对吧？
- en: Dimensionality reduction is one of the ways to deal with this. It implies a
    reduction of dimensions in the feature space.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低是处理这种问题的方法之一。它意味着在特征空间中减少维度。
- en: 'There are two ways this can be achieved:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种实现方式：
- en: '**Feature elimination**: This is a process in which features that are not adding
    value to the model are rejected. Doing this makes the model quite simple. We know
    from Occam''s Razor that we should strive for simplicity when it comes to building
    models. However, doing this step may result in the loss of information as a combination
    of such variables may have an impact on the model.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征消除**：这是一个过程，其中拒绝了对模型没有贡献的特征。这样做会使模型非常简单。我们知道，根据奥卡姆剃刀原理，在构建模型时我们应该追求简单。然而，执行此步骤可能会导致信息丢失，因为这种变量的组合可能对模型有影响。'
- en: '**Feature extraction**: This is a process in which we create new independent
    variables that are a combination of existing variables. Based on the impact of
    these variables, we either keep or drop them.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：这是一个过程，我们创建新的独立变量，这些变量是现有变量的组合。根据这些变量的影响，我们保留或删除它们。'
- en: Principal component analysis is a feature extraction technique that takes all
    of the variables into account and forms a linear combination of the variables.
    Later, the least important variable can be dropped while the most important part
    of that variable is retained.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是一种特征提取技术，它考虑了所有变量，并形成了变量的线性组合。随后，可以删除最不重要的变量，同时保留该变量的最重要部分。
- en: Newly formed variables (components) are independent of each other, which can
    be a boon for a model-building process wherein data distribution is linearly separable.
    Linear models have the underlying assumption that variables are independent of
    each other.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 新形成的变量（成分）彼此独立，这可以是一个模型构建过程中的福音，其中数据分布是线性可分的。线性模型的基本假设是变量彼此独立。
- en: 'To understand the functionality of PCA, we have to become familiar with a few
    terms:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解PCA的功能，我们必须熟悉几个术语：
- en: '**Variance**: This is the average squared deviation from the mean. It is also
    called a **spread**, which measures the variability of the data:'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**：这是从平均值平均平方偏差的度量。它也被称为 **分散度**，它衡量数据的可变性：'
- en: '![](img/7a19ac0f-a890-4f5e-afd1-ed3d108cbf02.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a19ac0f-a890-4f5e-afd1-ed3d108cbf02.png)'
- en: Here, *x* is the mean.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x* 是平均值。
- en: '**Covariance**: This is a measure of the degree to which two variables move
    in the same direction:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协方差**：这是衡量两个变量在同一方向上移动程度的度量：'
- en: '![](img/ad894e3b-abe7-4f79-9626-915f5215f0db.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad894e3b-abe7-4f79-9626-915f5215f0db.png)'
- en: 'In PCA, we find out the pattern of the data as follows: in the case of the
    dataset having high covariance when represented in *n* of dimensions, we represent
    those dimensions with a linear combination of the same *n* dimensions. These combinations
    are orthogonal to each other, which is the reason why they are independent of
    each other. Besides, dimension follows an order by variance. The top combination
    comes first.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在PCA中，我们通过以下方式找出数据的模式：在数据集以 *n* 维度表示时具有高协方差的情况下，我们用相同 *n* 维度的线性组合来表示这些维度。这些组合是相互正交的，这也是它们相互独立的原因。此外，维度按照方差排序。最上面的组合排在第一位。
- en: 'Let''s go over how PCA works by talking about the following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤来了解PCA是如何工作的：
- en: Let's split our dataset into *Y* and *X* sets, and just focus on *X*.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们把我们的数据集分成 *Y* 和 *X* 两部分，并只关注 *X*。
- en: A matrix of *X* is taken and standardized with a mean of 0 and a standard deviation
    of *1*. Let's call the new matrix *Z*.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个以 *X* 为矩阵，并使用均值为 0 和标准差为 *1* 进行标准化的矩阵被选取。让我们称这个新的矩阵为 *Z*。
- en: 'Let''s work on *Z* now. We have to transpose it and multiply the transposed
    matrix by *Z*. By doing this, we have got our covariance matrix:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来处理 *Z*。我们需要对其进行转置，并将转置后的矩阵与 *Z* 相乘。通过这样做，我们得到了我们的协方差矩阵：
- en: '*Covariance Matrix = Z^TZ*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*协方差矩阵 = Z^TZ*'
- en: Now, we need to calculate the eigenvalues and their corresponding eigenvectors
    of *Z^TZ*. Typically, the eigen decomposition of the covariance matrix into *PDP*⁻¹
    is done, where *P *is the matrix of eigenvectors and *D* is the diagonal matrix
    with eigenvalues on the diagonal and values of 0 everywhere else.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要计算 *Z^TZ* 的特征值及其对应的特征向量。通常，协方差矩阵分解为 *PDP*⁻¹，其中 *P* 是特征向量矩阵，*D* 是对角矩阵，其对角线上的元素是特征值，其余位置都是
    0。
- en: Take the eigenvalues *λ₁*, *λ₂*, …, *λp *and sort them from largest to smallest.
    In doing so, sort the eigenvectors in *P* accordingly. Call this sorted matrix
    of eigenvectors *P**.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取特征值 *λ₁*，*λ₂*，…，*λp* 并按从大到小的顺序排序。在这个过程中，按照相应的顺序对 *P* 中的特征向量进行排序。称这个排序后的特征向量矩阵为
    *P**。
- en: Calculate *Z**= *ZP**. This new matrix, *Z**, is a centered/standardized version
    of *X*, but now each observation is a combination of the original variables, where
    the weights are determined by the eigenvector. As a bonus, because our eigenvectors
    in *P** are independent of one another, the columns of *Z** are independent of
    one another.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *Z**= *ZP**。这个新的矩阵，*Z**，是 *X* 的中心化/标准化版本，但现在每个观测值都是原始变量的组合，其中权重由特征向量确定。作为额外的好处，因为我们的
    *P** 中的特征向量彼此独立，所以 *Z** 的列也彼此独立。
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we studied the statistical model, the learning curve, and curve
    fitting. We also studied two cultures that Leo Breiman introduced, which describe
    that any analysis needs data. We went through the different types of training,
    development, and test data, including their sizes. We studied regularization,
    which explains what overfitting means in machine learning modeling.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了统计模型、学习曲线和曲线拟合。我们还研究了 Leo Breiman 介绍的两个文化，它们描述了任何分析都需要数据。我们探讨了不同类型的训练、开发和测试数据，包括它们的大小。我们研究了正则化，它解释了在机器学习建模中过拟合的含义。
- en: This chapter also explained cross validation and model selection, the 0.632
    rule in bootstrapping, and also ROC and AUC in depth.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还解释了交叉验证和模型选择、自助法中的 0.632 规则，以及 ROC 和 AUC 的深入探讨。
- en: In the next chapter, we will study evaluating kernel learning, which is the
    most widely used approach in machine learning.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究评估核学习，这是机器学习中应用最广泛的方法。
