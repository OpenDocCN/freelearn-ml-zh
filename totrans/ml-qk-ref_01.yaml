- en: Quantifying Learning Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化学习算法
- en: 'We have stepped into an era where we are building smart or intelligent machines.
    This smartness or intelligence is infused into the machine with the help of smart
    algorithms based on mathematics/statistics. These algorithms enable the system
    or machine to learn automatically without any human intervention. As an example
    of this, today we are surrounded by a number of mobile applications. One of the
    prime messaging apps of today in WhatsApp (currently owned by Facebook). Whenever
    we type a message into a textbox of WhatsApp, and we type, for example, *I am...*,
    we get a few word prompts popping up, such as *..going home*, *Rahul*, *traveling
    tonight*, and so on. Can we guess what''s happening here and why? Multiple questions
    come up:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经进入了一个时代，我们正在构建智能或智能机器。这种智能或智能是通过基于数学/统计的智能算法注入到机器中的。这些算法使系统或机器能够自动学习，无需任何人为干预。作为这个例子，今天我们周围有大量的移动应用程序。今天的主要即时通讯应用之一是WhatsApp（目前由Facebook拥有）。每当我们将信息输入WhatsApp的文本框时，例如，输入*I
    am...*，就会出现一些词提示，如*..going home*、*Rahul*、*traveling tonight*等等。我们能猜测这里发生了什么，为什么吗？会涌现出多个问题：
- en: What is it that the system is learning?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统正在学习什么？
- en: Where does it learn from?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从哪里学习？
- en: How does it learn?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是如何学习的？
- en: Let's answer all these questions in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在本章中回答所有这些问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Statistical models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型
- en: Learning curves
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习曲线
- en: Curve fitting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: Modeling cultures
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模文化
- en: Overfitting and regularization
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合和正则化
- en: Train, validation, and test
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、验证和测试
- en: Cross-validation and model selection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证和模型选择
- en: Bootstrap method
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自举法
- en: Statistical models
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计模型
- en: A statistical model is the approximation of the truth that has been captured
    through data and mathematics or statistics, and acts as an enabler here. This
    approximation is used to predict an event. A statistical model is nothing but
    a mathematical equation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 统计模型是通过数据、数学或统计学捕捉到的真理的近似，在这里充当使能者。这种近似用于预测事件。统计模型不过是一个数学方程。
- en: 'For example, let''s say we reach out to a bank for a home loan. What does the
    bank ask us? The first thing they would ask us to do is furnish lots of documents
    such as salary slips, identity proof documents, documents regarding the house
    we are going to purchase, a utility bill, the number of current loans we have,
    the number of dependants we have, and so on. All of these documents are nothing
    but the data that the bank would use to assess and check our creditworthiness:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们向银行申请房屋贷款。银行会问我们什么？他们首先会要求我们提供大量文件，例如工资单、身份证明文件、关于我们要购买的房屋的文件、水费账单、我们当前的贷款数量、我们的抚养人数等等。所有这些文件不过是银行用来评估和检查我们信用度的数据：
- en: '![](img/50ec8721-c5e9-49c9-8a58-f60aacf91fa8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/50ec8721-c5e9-49c9-8a58-f60aacf91fa8.png)'
- en: What this means is that your creditworthiness is a function of the salary, number
    of loans, number of dependants, and so on. We can arrive at this equation or relationship
    mathematically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你的信用度是工资、贷款数量、抚养人数等因素的函数。我们可以通过数学方法得出这个方程或关系。
- en: A statistical model is a mathematical equation that arrives at using given data
    for a particular business scenario.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 统计模型是一个数学方程，用于特定商业场景的给定数据。
- en: In the next section, we will see how models learn and how the model can keep
    getting better.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到模型是如何学习的，以及模型如何不断变得更好。
- en: Learning curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习曲线
- en: The basic premise behind the learning curve is that the more time you spend
    doing something, the better you tend to get. Eventually, the time to perform a
    task keeps on plummeting. This is known by different names, such as **improvement
    curve**, **progress curve**, and **startup function**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线的基本前提是，你花在某一件事上的时间越多，你通常越擅长。最终，完成任务所需的时间会不断下降。这被称为不同的名称，如**改进曲线**、**进步曲线**和**启动函数**。
- en: For example, when you start learning to drive a manual car, you undergo a learning
    cycle. Initially, you are extra careful about operating the break, clutch, and
    gear. You have to keep reminding yourself when and how to operate these components.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当你开始学习手动驾驶汽车时，你会经历一个学习周期。最初，你非常小心地操作刹车、离合器和档位。你必须不断提醒自己何时以及如何操作这些部件。
- en: But, as the days go by and you continue practicing, your brain gets accustomed
    and trained to the entire process. With each passing day, your driving will keep
    getting smoother and your brain will react to the situation without any realization.
    This is called **subconscious intelligence**. You reach this stage with lots of
    practice and transition from a conscious intelligence to a subconscious intelligence
    that has got a cycle.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，随着日子的推移，你继续练习，你的大脑就会习惯并训练整个流程。随着每一天的过去，你的驾驶会越来越顺畅，你的大脑会对情况做出反应，而无需意识到这一点。这被称为**潜意识智能**。通过大量的练习和从有意识智能过渡到具有循环的潜意识智能，你将达到这个阶段。
- en: Machine learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Let me define machine learning and its components so that you don't get bamboozled
    by lots of jargon when it gets thrown at you.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我来定义机器学习和其组成部分，这样你就不至于在听到很多术语时感到困惑。
- en: In the words of Tom Mitchell, "*A computer program is said to learn from experience
    E with respect to some class of tasks T and performance measure P, if its performance
    at tasks in T, as measured by P, improves with experience E*." Also, another theory
    says that machine learning is the field that gives computers the ability to learn
    without being explicitly programmed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 按照Tom Mitchell的话说，“*如果计算机程序在任务T中，关于性能度量P，从经验E中学习，那么它的性能在T中的任务，按照P来衡量，会随着经验E的提高而提高*。”也有另一种理论说，机器学习是赋予计算机学习能力而不需要明确编程的领域。
- en: For example, if a computer has been given cases such as, *[(father, mother),
    (uncle, aunt), (brother, sisters)]*, based on this, it needs to find out *(son,
    ?)*. That is, given son, what will be the associated item? To solve this problem,
    a computer program will go through the previous records and try to understand
    and learn the association and pattern out of these combinations as it hops from
    one record to another. This is called **learning**, and it takes place through
    algorithms. With more records, that is, more experience, the machine gets smarter
    and smarter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一台计算机被给出了如下案例，*[(父亲，母亲)，(叔叔，阿姨)，(兄弟，姐妹)]*，基于此，它需要找出*(儿子，？)*。也就是说，给定儿子，将是什么相关项？为了解决这个问题，计算机程序将遍历之前的记录，并试图理解和学习从这些组合中跳转到另一个记录时的关联和模式。这被称为**学习**，它通过算法进行。随着记录的增加，即经验的增加，机器会变得越来越聪明。
- en: 'Let''s take a look at the different branches of machine learning, as indicated
    in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下图中所示的不同机器学习分支：
- en: '![](img/171db42c-de53-4ce2-9ec5-5acda2329ca5.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/171db42c-de53-4ce2-9ec5-5acda2329ca5.png)'
- en: 'We will explain the preceding diagram as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如下解释前面的图示：
- en: '**Supervised learning**: In this type of learning, both the input variables
    and output variables are known to us. Here, we are supposed to establish a relationship
    between the input variables and the output, and the learning will be based on
    that. There are two types of problems under it, as follows:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：在这种学习中，输入变量和输出变量都是我们所知的。在这里，我们应建立输入变量和输出之间的关系，学习将基于这一点。在其下有两种类型的问题，如下所示：'
- en: '**Regression problem**: It has got a continuous output. For example, a housing
    price dataset wherein the price of the house needs to be predicted based on input
    variables such as area, region, city, number of rooms, and so on. The price to
    be predicted is a continuous variable.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**：它有一个连续的输出。例如，一个房价数据集，其中需要根据面积、地区、城市、房间数量等输入变量预测房价。要预测的价格是一个连续变量。'
- en: '**Classification**: It has got a discrete output. For example, the prediction
    that an employee would leave an organization or not, based on salary, gender,
    the number of members in their family, and so on.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：它有一个离散的输出。例如，根据薪水、性别、家庭成员数量等预测一个员工是否会离开组织。'
- en: '**Unsupervised learning**: In this type of scenario, there is no output variable.
    We are supposed to extract a pattern based on all the variables given. For example,
    the segmentation of customers based on age, gender, income, and so on.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：在这种场景中，没有输出变量。我们应根据所有给定的变量提取一个模式。例如，根据年龄、性别、收入等对客户进行细分。'
- en: '**Reinforcement learning**: This is an area of machine learning wherein suitable
    action is taken to maximize reward. For example, training a dog to catch a ball
    and give it—we reward the dog if they carry out this action; otherwise, we tell
    them off, leading to a punishment.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：这是机器学习的一个领域，其中采取适当的行动以最大化奖励。例如，训练狗接球并给予奖励——如果它们执行这个动作，我们就奖励狗；否则，我们责备它们，导致惩罚。'
- en: Wright's model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赖特模型
- en: 'In Wright''s model, the learning curve function is defined as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在赖特模型中，学习曲线函数定义为以下：
- en: '![](img/f534dff2-a5c4-4f11-b694-b14f97180b55.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f534dff2-a5c4-4f11-b694-b14f97180b55.png)'
- en: 'The variables are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 变量如下：
- en: '*Y*: The cumulative average time per unit'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Y*：每单位累积平均时间'
- en: '*X*: The cumulative number of units produced'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*X*：累积生产的单位数量'
- en: '*a*: Time required to produce the first unit'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*：生产第一个单位所需的时间'
- en: '*b*: Slope of the function when plotted on graph paper* (log of the learning
    rate/log of 2)*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*：函数在图表纸上绘制时的斜率（学习率的对数/2的对数）'
- en: 'The following curve has got a vertical axis (*y* axis) representing the learning
    with respect to a particular work and a horizontal axis that corresponds to the
    time taken to learn. A learning curve with a steep beginning can be comprehended
    as a sign of rapid progress. The following diagram shows **Wright''s Learning
    Curve Model**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下曲线具有一个垂直轴（*y*轴），表示特定工作的学习情况，以及一个对应学习所需时间的水平轴。一个开始陡峭的学习曲线可以理解为快速进步的标志。以下图表显示了**赖特学习曲线模型**：
- en: '![](img/f10a6a22-e064-4c82-9f37-ebe5f7384a35.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f10a6a22-e064-4c82-9f37-ebe5f7384a35.png)'
- en: However, the question that arises is, *How is it connected to machine learning?* We
    will discuss this in detail now.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，出现的问题是，*它与机器学习有何关联？*我们现在将详细讨论这个问题。
- en: 'Let''s discuss a scenario that happens to be a supervised learning problem
    by going over the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤讨论一个实际上是监督学习问题的场景：
- en: We take the data and partition it into a training set (on which we are making
    the system learn and come out as a model) and a validation set (on which we are
    testing how well the system has learned).
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将数据分成一个训练集（我们在其上让系统学习并形成模型）和一个验证集（我们在其上测试系统学习的好坏）。
- en: The next step would be to take one instance (observation) of the training set
    and make use of it to estimate a model. The model error on the training set will
    be 0.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将是取训练集的一个实例（观察值）并利用它来估计一个模型。训练集上的模型错误将为0。
- en: Finally, we would find out the model error on the validation data.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将找出验证数据上的模型错误。
- en: '*Step 2* and *Step 3* are repeated by taking a number of instances (training
    size) such as 10, 50, and 100 and studying the training error and validation error,
    as well as their relationship with a number of instances (training size). This
    curve—or the relationship—is called a **l****earning curve** in a machine learning
    scenario.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二步*和*第三步*通过取多个实例（训练大小）如10、50和100，研究训练错误和验证错误，以及它们与实例数量（训练大小）的关系来重复。这种曲线——或这种关系——在机器学习场景中被称为**学习曲线**。'
- en: 'Let''s work on a combined power plant dataset. The features comprised hourly
    average ambient variables, that is, **temperature** (**T**), **ambient pressure**
    (**AP**), **relative humidity** (**RH**), and exhaust **vacuum** (**V**), to predict
    the net hourly **electrical energy output** (**PE**) of the plant:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一个综合发电厂数据集。特征包括每小时平均环境变量，即**温度**（**T**）、**环境压力**（**AP**）、**相对湿度**（**RH**）和排气**真空**（**V**），以预测工厂的净每小时**电能输出**（**PE**）：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'From this, we are able to see the data structure of the variables in the data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们能够看到数据中变量的数据结构：
- en: '![](img/05d57d50-1764-4cb0-a172-83bb011dc055.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05d57d50-1764-4cb0-a172-83bb011dc055.png)'
- en: 'The output can be seen as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/52260431-6fcc-4767-87d9-f47a04b70257.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52260431-6fcc-4767-87d9-f47a04b70257.png)'
- en: The second output gives you a good feel for the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出可以让你对数据有一个良好的感觉。
- en: The dataset has five variables, where **ambient temperature** (**AT**) and PE (target
    variable).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有五个变量，其中**环境温度**（**AT**）和PE（目标变量）。
- en: 'Let''s vary the training size of the data and study the impact of it on learning.
    A list is created for `train_size` with varying training sizes, as shown in the
    following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变数据的训练大小并研究它对学习的影响。创建了一个`train_size`列表，包含不同的训练大小，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s generate the `learning_curve`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成`learning_curve`：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![](img/0e15e0c1-657c-495d-a65f-14de600ca4fa.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e15e0c1-657c-495d-a65f-14de600ca4fa.png)'
- en: From the preceding plot, we can see that when the training size is just 1, the
    training error is 0, but the validation error shoots beyond **400**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到当训练集大小仅为1时，训练误差为0，但验证误差超过**400**。
- en: 'As we go on increasing the training set''s size (from 1 to 100), the training
    error continues rising. However, the validation error starts to plummet as the
    model performs better on the validation set. After the training size hits the
    500 mark, the validation error and training error begin to converge. So, what
    can be inferred out of this? The performance of the model won''t change, irrespective
    of the size of the training post. However, if you try to add more features, it
    might make a difference, as shown in the following diagram:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练集大小的不断增加（从1到100），训练误差持续上升。然而，随着模型在验证集上的表现越来越好，验证误差开始急剧下降。当训练集大小达到500时，验证误差和训练误差开始收敛。那么，我们能从中推断出什么呢？模型的性能不会因为训练数据的量而改变。然而，如果你尝试添加更多特征，可能会产生影响，如下面的图表所示：
- en: '![](img/21d6445c-9045-477b-8f6b-6f8203070ac2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21d6445c-9045-477b-8f6b-6f8203070ac2.png)'
- en: 'The preceding diagram shows that the validation and training curve have converged,
    so adding training data will not help at all. However, in the following diagram,
    the curves haven''t converged, so adding training data will be a good idea:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示，验证曲线和训练曲线已经收敛，因此添加训练数据将毫无帮助。然而，在下面的图表中，曲线尚未收敛，因此添加训练数据将是一个好主意：
- en: '![](img/f6ec0074-8426-4428-b50a-f7d40b70373d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6ec0074-8426-4428-b50a-f7d40b70373d.png)'
- en: Curve fitting
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 曲线拟合
- en: So far, we have learned about the learning curve and its significance. However,
    it only comes into the picture once we tried fitting a curve on the available
    data and features. But what does curve fitting mean? Let's try to understand this.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了学习曲线及其重要性。然而，它只有在尝试在可用数据和特征上拟合曲线时才会出现。但曲线拟合是什么意思呢？让我们尝试理解这一点。
- en: Curve fitting is nothing but establishing a relationship between a number of
    features and a target. It helps in finding out what kind of association the features
    have with respect to the target.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线拟合实际上就是建立多个特征与目标之间的关系。它有助于找出特征与目标之间的关联类型。
- en: Establishing a relationship (curve fitting) is nothing but coming up with a
    mathematical function that should be able to explain the behavioral pattern in
    such a way that it comes across as a best fit for the dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 建立关系（曲线拟合）实际上就是提出一个数学函数，该函数应该能够以某种方式解释行为模式，使其成为数据集的最佳拟合。
- en: 'There are multiple reasons behind why we do curve fitting:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行曲线拟合的原因有很多：
- en: To carry out system simulation and optimization
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行系统模拟和优化
- en: To determine the values of intermediate points (interpolation)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定中间点的值（插值）
- en: To do trend analysis (extrapolation)
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行趋势分析（外推）
- en: To carry out hypothesis testing
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行假设检验
- en: 'There are two types of curve fitting:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 曲线拟合有两种类型：
- en: '**Exact fit**: In this scenario, the curve would pass through all the points.
    There is no residual error (we''ll discuss shortly what''s classed as an error)
    in this case. For now, you can understand an error as the difference between the
    actual error and the predicted error. It can be used for interpolation and is
    majorly involved with a distribution fit.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**精确拟合**：在这种情况下，曲线将通过所有点。在这种情况下没有残差误差（我们很快会讨论什么被认为是误差），现在你可以将误差理解为实际误差与预测误差之间的差异。它可以用于插值，并且主要涉及分布拟合。'
- en: 'The following diagram shows the polynomial but exact fit:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了多项式但精确拟合：
- en: '![](img/324d5cff-68ea-43a5-9285-805f65773b79.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/324d5cff-68ea-43a5-9285-805f65773b79.png)'
- en: 'The following diagram shows the line but exact fit:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了直线但精确拟合：
- en: '![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af0953fd-f87c-4546-a6f8-193044a707eb.png)'
- en: '**Best fit**: The curve doesn''t pass through all the points. There will be
    a residual associated with this.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最佳拟合**：曲线不会通过所有点。将与这个曲线相关的残差。'
- en: Let's look at some different scenarios and study them to understand these differences.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些不同的场景，并研究它们以了解这些差异。
- en: 'Here, we will fit a curve for two numbers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将为两个数字拟合曲线：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From this, we will get the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个结果中，我们将得到以下输出：
- en: '![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4a9a050-461b-4b09-9b31-ac5dd5c9602e.png)'
- en: 'Here, we have used two points to fit the line and we can very well see that
    it becomes an **exact fit**. When introducing three points, we will get the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了两个点来拟合直线，我们可以清楚地看到它变成了一个**精确的拟合**。当我们引入三个点时，我们会得到以下结果：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Run the entire code and focus on the output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 运行整个代码并关注输出：
- en: '![](img/c682382b-fb8a-4d93-abc0-99925922b1b8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c682382b-fb8a-4d93-abc0-99925922b1b8.png)'
- en: Now, you can see the drift and effect of noise. It has started to take the shape
    of a curve. A line might not be a good fit here (however, it's too early to say).
    It's no longer an exact fit.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以看到漂移和噪声的影响。它已经开始呈现出曲线的形状。一条直线可能在这里不是一个好的拟合（然而，说得太早了）。它不再是一个精确的拟合。
- en: What if we try to introduce 100 points and study the effect of that? By now,
    we know how to introduce the number of points.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试引入100个点并研究其效果呢？到目前为止，我们知道如何引入点的数量。
- en: 'By doing this, we get the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们得到以下输出：
- en: '![](img/ef332a8e-1123-44ca-bade-917751904b26.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ef332a8e-1123-44ca-bade-917751904b26.png)'
- en: This is not an exact fit, but rather a best fit that tries to generalize the
    whole dataset.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个精确的拟合，而是一个试图概括整个数据集的最佳拟合。
- en: Residual
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差
- en: 'Residuals are the difference between an observed or true value and a predicted
    (fitted) value. For example, in the following diagram, one of the residuals is **(A-B)**,
    where **A** is the observed value and **B** is the fitted value:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是观察值或真实值与预测（拟合）值之间的差异。例如，在以下图中，一个残差是**(A-B)**，其中**A**是观察值，**B**是拟合值：
- en: '![](img/554e01db-d173-45cc-a834-039d7a111e50.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/554e01db-d173-45cc-a834-039d7a111e50.png)'
- en: The preceding scatter plot depicts that we are fitting a line that could represent
    the behavior of all the data points. However, one thing that's noticeable is that
    the line doesn't pass through all of the points. Most of the points are off the
    line.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的散点图表明我们正在拟合一条可能代表所有数据点行为的直线。然而，值得注意的是，这条线并没有穿过所有点。大多数点都在线上方。
- en: The sum and mean of residuals will always be 0. *∑e =0* and mean of *e =0*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 残差的和与平均值总是为0。*∑e =0* 和 *e =0* 的平均值。
- en: Statistical modeling – the two cultures of Leo Breiman
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计建模——莱奥·布雷曼的两种文化
- en: 'Whenever we try to analyze data and finally make a prediction, there are two
    approaches that we consider, both of which were discovered by Leo Breiman, a Berkeley
    professor, in his paper titled *Statistical Modeling: Two Cultures* in 2001.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们试图分析数据并最终做出预测时，我们会考虑两种方法，这两种方法都是由加州大学伯克利分校教授莱奥·布雷曼在他的2001年论文《统计建模：两种文化》中发现的。
- en: 'Any analysis needs data. An analysis can be as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分析都需要数据。分析可以如下进行：
- en: '![](img/5fa263dc-3aee-4d76-a18c-216f46278c34.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fa263dc-3aee-4d76-a18c-216f46278c34.png)'
- en: 'A vector of **X** (**Features**) undergoes a nature box, which translates into
    a response. A nature box tries to establish a relationship between **X** and **Y**.
    Typically, there are goals pertaining to this analysis, as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**X**（**特征**）向量经过一个自然箱，这转化为响应。自然箱试图建立**X**和**Y**之间的关系。通常，这个分析有一些目标，如下所示：'
- en: '**Prediction**: To predict the response with the future input features'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：用未来的输入特征来预测响应'
- en: '**Information**: To find out and understand the association between the response
    and driving input variables'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息**：找出并理解响应和驱动输入变量之间的关联'
- en: 'Breiman states that, when it comes to solving business problems, there are
    two distinct approaches:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 布雷曼指出，在解决商业问题时，有两种截然不同的方法：
- en: '**The data modeling culture**: In this kind of model, nature takes the shape
    of a stochastic model that estimates the necessary parameters. Linear regression,
    logistic regression, and the Cox model usually act under the nature box. This
    model talks about observing the pattern of the data and looks to design an approximation
    of what is being observed. Based on their experience, the scientist or a statistician
    would decide which model to be used. It is the case of a model coming before the
    problem and the data, the solutions from this model is more towards the model''s
    architecture. Breiman says that over-reliance on this kind of approach doesn''t
    help the statisticians cater to a diverse set of problems. When it comes to finding
    out solutions pertaining to earthquake prediction, rain prediction, and global
    warming causes, it doesn''t give accurate results, since this approach doesn''t
    focus on accuracy, and instead focuses on the two goals.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据建模文化**：在这种模型中，自然以随机模型的形式出现，该模型估计必要的参数。线性回归、逻辑回归和Cox模型通常在自然框下运行。这种模型讨论的是观察数据的模式，并试图设计出对所观察到的内容的近似。基于他们的经验，科学家或统计学家会决定使用哪种模型。这是模型在问题和数据之前的情况，从这个模型中得出的解决方案更倾向于模型架构。Breiman说，过度依赖这种方法并不能帮助统计学家应对各种问题。当涉及到地震预测、降雨预测和全球变暖原因的解决方案时，它不会给出准确的结果，因为这种方法不关注准确性，而是关注两个目标。'
- en: '**The algorithm modeling culture**: In this approach, pre-designed algorithms
    are used to make a better approximation. Here, the algorithms use complex mathematics
    to reach out to the conclusion and acts inside the nature box. With better computing
    power and using these models, it''s easy to replicate the driving factors as the
    model keeps on running until it learns and understands the pattern that drives
    the outcome. It enables us to address more complex problems, and emphasizes more
    on accuracy. With more data coming through, it can give a much better result than
    the data modeling culture.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法建模文化**：在这种方法中，使用预先设计的算法来做出更好的近似。在这里，算法使用复杂的数学来得出结论，并在自然框内运行。随着计算能力的提高和使用这些模型，很容易复制驱动因素，因为模型会一直运行，直到它学会并理解驱动结果的模式。这使得我们能够解决更复杂的问题，并更强调准确性。随着数据的增加，它比数据建模文化能给出更好的结果。'
- en: Training data development data – test data
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据 - 开发数据 - 测试数据
- en: This is one of the most important steps of building a model and it can lead
    to lots of debate regarding whether we really need all three sets (train, dev,
    and test), and if so, what should be the breakup of those datasets. Let's understand
    these concepts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在构建模型过程中最重要的步骤之一，它可能导致关于我们是否真的需要所有三个集合（训练集、开发集和测试集）以及如果需要，这些数据集应该如何划分的许多争论。让我们理解这些概念。
- en: 'After we have sufficient data to start modelling, the first thing we need to
    do is partition the data into three segments, that is, **Training Set**, **Development**
    **Set**, and **Test Set**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有足够的数据开始建模后，我们首先需要做的是将数据分成三个部分，即**训练集**、**开发集**和**测试集**：
- en: '![](img/936279c1-3f92-41cf-ae3c-751f3f5afc0f.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/936279c1-3f92-41cf-ae3c-751f3f5afc0f.png)'
- en: 'Let''s examine the goal of having these three sets:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察拥有这三个集合的目标：
- en: '**Training Set**:The training set is used to train the model. When we apply
    any algorithm, we are fitting the parameter in the training set. In the case of
    a neural network, finding out about the weights takes place.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练集**：训练集用于训练模型。当我们应用任何算法时，我们是在训练集中拟合参数。在神经网络的情况下，找到权重就发生在训练集中。'
- en: 'Let''s say in one scenario that we are trying to fit polynomials of various
    degrees:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在一个场景中，我们正在尝试拟合不同次数的多项式：
- en: '*f(x) = a+ bx* → 1^(st) degree polynomial'
  id: totrans-125
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx* → 1^(st) 次多项式'
- en: '*f(x) = a + bx + cx²* → 2^(nd) degree polynomial'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx + cx²* → 2^(nd) 次多项式'
- en: '*f(x) = a + bx + cx^(2 )+ dx³* → 3^(rd) degree polynomial'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f(x) = a + bx + cx² + dx³* → 3^(rd) 次多项式'
- en: 'After fitting the model, we calculate the training error for all the fitted
    models:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们计算所有拟合模型的训练误差：
- en: '![](img/4fb00d4e-01ed-4227-9e89-bd91428ee696.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4fb00d4e-01ed-4227-9e89-bd91428ee696.png)'
- en: We cannot assess how good the model is based on the training error. If we do
    that, it will lead us to a biased model that might not be able to perform well
    on unseen data. To counter that, we need to head into the development set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅根据训练误差来评估模型的好坏。如果我们这样做，可能会导致我们得到一个有偏见的模型，这个模型可能无法在未见过的数据上表现良好。为了解决这个问题，我们需要转向开发集。
- en: '**Development** **set**: This is also called the **holdout set** or **validation
    set**. The goal of this set is to tune the parameters that we have got from the
    training set. It is also part of an assessment of how well the model is performing.
    Based on its performance, we have to take steps to tune the parameters. For example,
    controlling the learning rate, minimizing the overfitting, and electing the best
    model of the lot all take place in the development set. Here, again, the development
    set error gets calculated and tuning of the model takes place after seeing which
    model is giving the least error. The model giving the least error at this stage
    still needs tuning to minimize overfitting. Once we are convinced about the best
    model, it is chosen and we head toward the test set.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开发集**：这也被称为**保留集**或**验证集**。这个集合的目标是调整我们从训练集得到的参数。它也是对模型性能评估的一部分。根据其性能，我们必须采取步骤来调整参数。例如，控制学习率、最小化过拟合和选择最佳模型都发生在开发集中。在这里，同样会计算开发集误差，并在看到哪个模型产生最小误差后对模型进行调整。在这个阶段产生最小误差的模型仍然需要调整以最小化过拟合。一旦我们确信了最佳模型，它就会被选择，然后我们转向测试集。'
- en: '**Test set**: The test set is primarily used to assess the best selected model.
    At this stage, the accuracy of the model is calculated, and if the model''s accuracy
    is not too deviated from the training accuracy and development accuracy, we send
    this model for deployment.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试集**：测试集主要用于评估最佳选择的模型。在这个阶段，计算模型的准确度，如果模型的准确度没有太大偏离训练准确度和开发准确度，我们就将这个模型部署上线。'
- en: Size of the training, development, and test set
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集、开发集和测试集的大小
- en: 'Typically, machine learning practitioners choose the size of the three sets
    in the ratio of 60:20:20 or 70:15:15\. However, there is no hard and fast rule
    that states that the development and test sets should be of equal size. The following
    diagram shows the different sizes of the training, development, and test sets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，机器学习从业者会选择三个集合的大小比例为60:20:20或70:15:15。然而，并没有一条硬性规定说开发集和测试集应该大小相等。以下图表显示了训练集、开发集和测试集的不同大小：
- en: '![](img/9123ff4d-f4ae-454d-9636-21f3f0f47e60.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9123ff4d-f4ae-454d-9636-21f3f0f47e60.png)'
- en: 'Another example of the three different sets is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个三个不同集合的另一个例子：
- en: '![](img/badde5d1-a170-4982-80a6-f8bb01bdc94c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/badde5d1-a170-4982-80a6-f8bb01bdc94c.png)'
- en: But what about the scenarios where we have big data to deal with? For example,
    if we have 10,000,000 records or observations, how would we partition the data?
    In such a scenario, ML practitioners take most of the data for the training set—as
    much as 98-99%—and the rest gets divided up for the development and test sets.
    This is done so that the practitioner can take different kinds of scenarios into
    account. So, even if we have 1% of data for development and the same for the test
    test, we will end up with 100,000 records each, and that is a good number.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何处理大数据的场景呢？例如，如果我们有1000万条记录或观测值，我们该如何划分数据？在这种情况下，机器学习从业者会将大部分数据用于训练集——多达98-99%——其余的则分配给开发集和测试集。这样做是为了让从业者能够考虑不同类型的场景。因此，即使我们只有1%的数据用于开发集和测试集，我们最终也会得到10万条记录，这是一个很好的数字。
- en: Bias-variance trade off
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: 'Before we get into modelling and try to figure out what the trade-off is, let''s
    understand what bias and variance are from the following diagram:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始建模并试图弄清楚权衡是什么之前，让我们从以下图表中了解偏差和方差是什么：
- en: '![](img/13bf10e0-73cb-42ad-afcb-acd933d0dda9.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/13bf10e0-73cb-42ad-afcb-acd933d0dda9.png)'
- en: 'There are two types of errors that are developed in the bias-variance trade
    off, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在偏差-方差权衡中，产生了两种类型的误差，如下所示：
- en: '**Training error**: This is a measure of deviation of the fitted value from
    the actual value while predicting the output by using the training inputs. This
    error depends majorly on the model''s complexity. As the model''s complexity increases,
    the error appears to plummet.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练误差**：这是在用训练输入预测输出时，拟合值与实际值之间偏差的度量。这个误差主要取决于模型的复杂度。随着模型复杂度的增加，误差似乎会急剧下降。'
- en: '**Development error**: This is a measure of deviation of the predicted value,
    and is used by the development set as input (while using the same model trained
    on training data) from the actual values. Here, the prediction is being done on
    unseen data. We need to minimize this error. Minimizing this error will determine
    how good this model will be in the actual scenario.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发错误**：这是预测值偏差的度量，开发集作为输入（在训练数据上使用相同模型的情况下）用于实际值。在这里，预测是在未见过的数据上进行的。我们需要最小化这个错误。最小化这个错误将决定这个模型在实际场景中的好坏。'
- en: 'As the complexity of the algorithm keeps on increasing, the training error
    goes down. However, the development error or validation error keeps going down
    until a certain point, and then rises, as shown in the following diagram:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 随着算法复杂性的不断增加，训练错误会下降。然而，开发错误或验证错误会持续下降直到某个点，然后上升，如下面的图表所示：
- en: '![](img/24f381ef-b08b-44e5-9e20-201ac3b1d0cf.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24f381ef-b08b-44e5-9e20-201ac3b1d0cf.png)'
- en: 'The preceding diagram can be explained as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表可以这样解释：
- en: '**Underfitting**: Every dataset has a specific pattern and properties due to
    the existing variables in the dataset. Along with that, it also has a random and
    latent pattern which is caused by the variables that are not part of the dataset.
    Whenever we come up with a model, the model should ideally be learning patterns
    from the existing variables. However, the learning of these patterns also depends
    on how good and robust your algorithm is. Let''s say we have picked up a model
    that is not able to derive even the essential patterns out of the dataset—this
    is called **underfitting**. In the preceding plots, it is a scenario of classification
    and we are trying to classify *x* and *o*. In plot 1, we are trying to use a linear
    classification algorithm to classify the data, but we can see that it is resulting
    in lots of misclassification errors. This is a case of underfitting.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：每个数据集都有特定的模式和属性，这是由于数据集中存在的变量所导致的。除此之外，它还有由不是数据集部分的变量引起的随机和潜在模式。每当我们要提出一个模型时，理想情况下模型应该从现有变量中学习模式。然而，这些模式的学习也取决于你的算法有多好、有多稳健。假设我们选择了一个无法从数据集中推导出基本模式的模型——这被称为**欠拟合**。在前面的图表中，这是一个分类场景，我们正在尝试对*x*和*o*进行分类。在图1中，我们试图使用线性分类算法对数据进行分类，但我们可以看到它导致了大量的误分类错误。这是一个欠拟合的例子。'
- en: '**Overfitting**: Going further afield from plot 1, we are trying to use complex
    algorithms to find out the patterns and classify them. It is noticeable that the
    misclassification errors have gone down in the second plot, since the complex
    model being used here is able to detect the patterns. The development error (as
    shown in the preceding diagram) goes down too. We will increase the complexity
    of the model and see what happens. Plot 3 suggests that there is no misclassification
    error in the model now. However, if we look at the plot below it, we can see that
    the development error is way too high now. This happens because the model is learning
    from the misleading and random patterns that were exhibited due to the non-existent
    variables in the dataset. This means that it has started to learn the noise that''s
    present in the set. This phenomenon is called **overfitting**.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：从图1进一步扩展，我们试图使用复杂的算法来找出模式并进行分类。值得注意的是，在第二个图表中，误分类错误已经下降，因为在这里使用的复杂模型能够检测到模式。开发错误（如前图所示）也下降了。我们将增加模型的复杂性并观察会发生什么。图3表明，现在模型中没有误分类错误。然而，如果我们看下面的图表，我们可以看到开发错误现在非常高。这是因为模型正在从数据集中不存在的变量所展示的误导性和随机模式中学习。这意味着它已经开始学习集合中存在的噪声。这种现象被称为**过拟合**。'
- en: '**Bias**: How often have we seen this? This occurs in a situation wherein we
    have used an algorithm and it doesn''t fit properly. This means that the function
    that''s being used here has been of little relevance to this scenario and it''s
    not able to extract the correct patterns. This causes an error called **bias**.
    It crops up majorly due to making a certain assumption about the data and using
    a model that might be correct but isn''t. For example, if we had to use a second
    degree polynomial for a situation, we would use simple linear regression, which
    doesn''t establish a correct relationship between the response and explanatory
    variables.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance**: When we have a dataset that is being used for training the model,
    the model should remain immune, even if we change the training set to a set that''s
    coming from the same population. If variation in the dataset brings in a change
    in the performance of the model, it is termed a **variance error**. This takes
    place due to noise (an unexplained variation) being learned by the model and,
    due to that, this model doesn''t give a good result on unseen data:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c371896e-14ed-449f-a25c-72c9f845cd9f.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'We will explain the preceding diagram as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: If the **Training Error** goes down and (**Development Error**-**Training Error**)
    rises, it implies a **High Variance** situation (scenario 1 in the preceding table)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the **Training Error** and **Development Error** rises and (**Development
    Error**-**Training Error**) goes down, it implies a **High Bias** situation (scenario
    2 in the preceding table)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the **Training Error** and **Development Error** rises and (**Development
    Error**-**Training Error**) goes up as well, it implies **High Bias** and **High
    Variance** (scenario 3 in the preceding table)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the **Training Error** goes up and the **Development Error** declines, that
    is, (**Development Error**-**Training Error**) goes down, it implies **Low Bias**
    and **Low Variance** (scenario 4 in the preceding table)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should always strive for the fourth scenario, which depicts the training
    error being low, as well as a low development set error. In the preceding table,
    this is where we have to find out a bias variance trade-off, which is depicted
    by a vertical line.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the following question arises: how we can counter overfitting? Let''s
    find out the answer to this by moving on to the next section.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now got a fair understanding of what overfitting means when it comes
    to machine learning modeling. Just to reiterate, when the model learns the noise
    that has crept into the data, it is trying to learn the patterns that take place
    due to random chance, and so overfitting occurs. Due to this phenomenon, the model's
    generalization runs into jeopardy and it performs poorly on unseen data. As a
    result of that, the accuracy of the model takes a nosedive.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Can we combat this kind of phenomenon? The answer is yes. Regularization comes
    to the rescue. Let's figure out what it can offer and how it works.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is a technique that enables the model to not become complex to
    avoid overfitting.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following regression equation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/107b0b1a-755e-4c96-b235-5e4a9a676cf2.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'The loss function for this is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69cb5478-5ca8-45e7-9123-c85fd63b62e0.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: The loss function would help in getting the coefficients adjusted and retrieving
    the optimal one. In the case of noise in the training data, the coefficients wouldn't
    generalize well and would run into overfitting. Regularization helps get rid of
    this by making these estimates or coefficients drop toward 0.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will cover two types of regularization. In later chapters, the other
    types will be covered.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Ridge regression (L2)
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Due to ridge regression, we need to make some changes to the loss function.
    The original loss function gets added by a shrinkage component:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c423cec4-f053-4798-8e0e-bfcf116acdcd.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Now, this modified loss function needs to be minimized to adjust the estimates
    or coefficients. Here, the lambda is tuning the parameter that regularizes the
    loss function. That is, it decides how much it should penalize the flexibility
    of the model. The flexibility of the model is dependent on the coefficients. If
    the coefficients of the model go up, the flexibility also goes up, which isn't
    a good sign for our model. Likewise, as the coefficients go down, the flexibility
    is restricted and the model starts to perform better. The shrinkage of each estimated
    parameter makes the model better here, and this is what ridge regression does.
    When lambda keeps going higher and higher, that is, *λ → ∞*, the penalty component
    rises, and the estimates start shrinking. However, when *λ **→ 0*, the penalty
    component decreases and starts to become an **ordinary least square** (**OLS**)
    for estimating unknown parameters in a linear regression.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Least absolute shrinkage and selection operator
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **least absolute shrinkage and selection operator** (**LASSO**) is also
    called *L1*. In this case, the preceding penalty parameter is replaced by *|βj|*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41f4f28d-b58f-4302-b8b4-39ccc8340d14.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: By minimizing the preceding function, the coefficients are found and adjusted.
    In this scenario, as lambda becomes larger, *λ → ∞*, the penalty component rises,
    and so estimates start shrinking and become 0 (it doesn't happen in the case of
    ridge regression; rather, it would just be close to 0).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation and model selection
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already spoken about overfitting. It is something to do with the stability
    of a model since the real test of a model occurs when it works on unseen and new
    data. One of the most important aspects of a model is that it shouldn't pick up
    on noise, apart from regular patterns.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Validation is nothing but an assurance of the model being a relationship between
    the response and predictors as the outcome of input features and not noise. A
    good indicator of the model is not through training data and error. That's why
    we need cross-validation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will stick with k-fold cross-validation and understand how it can be
    used.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross-validation
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s walk through the steps of k-fold cross-validation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The data is divided into k-subsets.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One set is kept for testing/development and the model is built on the rest of
    the data (*k-1*). That is, the rest of the data forms the training data.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 2* is repeated k-times. That is, once the preceding step has been performed,
    we move on to the second set and it forms a test set. The rest of the (*k-1*)
    data is then available for building the model:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b90b29ab-dfe7-4c11-9a2f-321e84f79495.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 4\. An error is calculated and an average is taken over all k-trials.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Every subset gets one chance to be a validation/test set since most of the data
    is used as a training set. This helps in reducing bias. At the same time, almost
    all the data is being used as validation set, which reduces variance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, *k = 5* has been selected. This means that
    we have to divide the whole dataset into five subsets. In the first iteration,
    subset 5 becomes the test data and the rest becomes the training data. Likewise,
    in the second iteration, subset 4 turns into the test data and the rest becomes
    the training data. This goes on for five iterations.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try to do this in Python by splitting the train and test data using
    the K neighbors classifier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Model selection using cross-validation
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can make use of cross-validation to find out which model is performing better
    by using the following code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The 10-fold cross-validation is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 0.632 rule in bootstrapping
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the 0.632 rule of bootstrapping, we need to understand what
    bootstrapping is. Bootstrapping is the process wherein random sampling is performed
    with a replacement from a population that's comprised of *n* observations. In
    this scenario, a sample can have duplicate observations. For example, if the population
    is (2,3,4,5,6) and we are trying to draw two random samples of size 4 with replacement,
    then sample 1 will be (2,3,3,6) and sample 2 will be (4,4,6,2).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's delve into the 0.632 rule.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen that the estimate of the training error while using a
    prediction is *1/n ∑L(y[i,]y-hat)*. This is nothing but the loss function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfea359d-89d5-48f1-93e5-2d6bf1a50c7c.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: 'Cross-validation is a way to estimate the expected output of a sample error:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5863896c-0524-4013-a73a-cd6b176f4183.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: 'However, in the case of k-fold cross-validation, it is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/baff7e70-d144-4371-a088-6ffd520061d0.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Here, the training data is *X=(x1,x2.....,xn)* and we take bootstrap samples
    from this set *(z1,.....,zb)* where each *zi* is a set of *n* samples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'In this scenario, the following is our out-of-sample error:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5571d600-2600-4985-a2d5-ae703b7eceec.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Here, *fb(xi)* is the predicted value at *xi *from the model that's been fit
    to the bootstrap dataset.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, this is not a particularly good estimator because bootstrap
    samples that have been used to produce *fb(xi)* may have contained *xi*. OOSE solves
    the overfitting problem, but is still biased. This bias is due to non-distinct
    observations in the bootstrap samples that result from sampling with replacement.
    The average number of distinct observations in each sample is about *0.632n*. To
    solve the bias problem, Efron and Tibshirani proposed the *0.632* estimator:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff94ac0d-9c57-454c-9943-1fb1983f5632.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Model evaluation
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at some of the model evaluation techniques that are currently being
    used.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A confusion matrix is a table that helps in assessing how good the classification
    model is. It is used when true values/labels are known. Most beginners in the
    field of data science feel intimidated by the confusion matrix and think it looks
    more difficult to comprehend than it really is; let me tell you—it's pretty simple
    and easy.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this by going through an example. Let's say that we have built
    a classification model that predicts whether a customer would like to buy a certain
    product or not. To do this, we need to assess the model on unseen data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two classes:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '**Yes**: The customer will buy the product'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No**: The customer will not buy the product'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From this, we have put the matrix together:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2619d94-1b89-4ec3-b363-e816077f5e39.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: What are the inferences we can draw from the preceding matrix at first glance?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The classifier has made a total of 80 predictions. What this means is that 80
    customers were tested in total to find out whether he/she will buy the product
    or not.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**54** customers bought the product and **26** didn''t.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The classifier predicts that **56** customers will buy the product and that **24**
    won''t:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0b20dc6e-24bf-4e3b-bd6a-538c81b2a894.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: 'The different terms pertaining to the confusion matrix are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive (TP)**: These are the cases in which we predicted that the
    customer will buy the product and they did.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative (TN)**: These are the cases in which we predicted that the
    customer won''t buy the product and they didn''t.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive (FP)**: We predicted *Yes the customer will buy the product*,
    but they didn''t. This is known as a *Type 1* error.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative (FN)**: We predicted *No*, but the customer bought the product.
    This is known as a *Type 2* error.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s talk about a few metrics that are required for the assessment of
    a classification model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This measures the overall accuracy of the classifier. To calculate
    this, we will use the following formula: *(TP+TN)/Total cases*. In the preceding
    scenario, the accuracy is (50+20)/80, which turns out to be 0.875\. So, we can
    say that this classifier will predict correctly in 87.5% of scenarios.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misclassification rate**: This measures how often the classifier has got
    the results wrong. The formula *(FP+FN)/Total cases* will give the result. In
    the preceding scenario, the misclassification rate is *(6+4)/80*, which is 0.125\.
    So, in 12.5% of cases, it won''t produce correct results. It can also be calculated
    as (1- Accuracy).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TP rate**: This is a measure of what the chances are that it would predict
    *yes* as the answer, and the answer actually is *yes*. The formula to calculate
    this is **TP/(Actual:Yes)**. In this scenario, *TPR = (50/54)= 0.92*. It''s also
    called **Sensitivity** or **Recall**.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP rate**: This is a measure of what the chances are that it would predict
    *yes*, when the actual answer is *no*. The formula to calculate this rate is **FP/(Actual:No)**.For
    the preceding example, *FPR = (6/26)= 0.23*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TN rate**: This is a measure of what the chances are that it would predict
    *no*, when the answer is actually *no*. The formula to calculate this is *TN/(Actual:No)*.
    In this scenario, *TNR= (20/26)= 0.76*. It can also be calculated using (1-FPR).
    It''s also called **Specificity**.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: This is a measure of correctness of the prediction of *yes* out
    of all the *yes* predictions. It finds out how many times a prediction of *yes*
    was made correctly out of total *yes* predictions. The formula to calculate this
    is *TP/(Predicted:Yes)*. Here, *Precision = (50/56)=0.89*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prevalence**: This is a measure of how many *yes* were given out of the total
    sample. The formula is *(Actual:Yes/ Total Sample)*. Here, this is *54/80 = 0.67*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Null error rate**: This is a measure of how wrong the classifier would be
    if it predicted just the majority class. The formula is *(Actual:No/Total Sample)*.
    Here, this is *26/80=0.325*.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cohen''s Kappa value**: This is a measure of how well the classifier performed
    compared to how well it would have performed simply by chance.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-Score**: This is a harmonic mean of recall and precision, that is, *(2*Recall*Precision)/(Recall+Precision)*.
    It considers both Recall and Precision as important measures of a model''s evaluation.
    The best value of the F-score is 1, wherein Recall and Precision are at their
    maximum. The worst value of the F-score is 0\. The higher the score, the better
    the model is:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c1c5e09b-fc65-4721-92c0-474cf4b1ff56.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: Receiver operating characteristic curve
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have come across many budding data scientists who would build a model and,
    in the name of evaluation, are just content with the **overall accuracy**. However,
    that''s not the correct way to go about evaluating a model. For example, let''s
    say there''s a dataset that has got a response variable that has two categories:
    customers willing to buy the product and customers not willing to buy the product.
    Let''s say that the dataset has 95% of customers not willing to buy the product
    and 5% of customers willing to buy it. Let''s say that the classifier is able
    to correctly predict the majority class and not the minority class. So, if there
    are 100 observations, *TP=0*, *TN= 95*, and the rest misclassified, this will
    still result in 95% accuracy. However, it won''t be right to conclude that this
    is a good model as it''s not able to classify the minority class at all.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we need to look beyond accuracy so that we have a better judgement about
    the model. In this situation, Recall, Specificity, Precision, and the **receiver
    operating characteristic** (**ROC**) curve come to rescue. We learned about Recall,
    specificity, and precision in the previous section. Now, let's understand what
    the ROC curve is.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the classifiers produce a score between 0 and 1\. The next step occurs
    when we''re setting up the threshold, and, based on this threshold, the classification
    is decided. Typically, 0.5 is the threshold—if it''s more than 0.5, it creates
    a class, 1, and if the threshold is less than 0.5 it falls into another class,
    2:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46bc9f8a-2b3c-4985-8b09-17bafb7d919a.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: 'For ROC, every point between **0.0** and **1.0** is treated as a threshold,
    so the line of threshold keeps on moving from **0.0** to **1.0**. The threshold
    will result in us having a TP, TN, FP, and FN. At every threshold, the following
    metrics are calculated:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '*True Positive Rate = TP/(TP+FN)*'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*True Negative Rate = TN/(TN + FP)*'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*False Positive Rate = 1- True Negative Rate*'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The calculation of (TPR and FPR) starts from 0\. When the threshold line is
    at 0, we will be able to classify all of the customers who are willing to buy
    (positive cases), whereas those who are not willing to buy will be misclassified
    as there will be too many false positives. This means that the threshold line
    will start moving toward the right from zero. As this happens, the false positive
    starts to decline and the true positive will continue increasing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will need to plot a graph of the TPR versus FPR after calculating
    them at every point of the threshold:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6092b1e-d657-46ef-9d5e-a8b25de6f5c5.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: The red diagonal line represents the classification at random, that is, classification
    without the model. The perfect ROC curve will go along the *y* axis and will take
    the shape of an absolute triangle, which will pass through the top of the *y*
    axis.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Area under ROC
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assess the model/classifier, we need to determine the **area under ROC**
    (**AUROC**). The whole area of this plot is 1 as the maximum value of FPR and TPR
    – both are 1 here. Hence, it takes the shape of a square. The random line is positioned
    perfectly at 45 degrees, which partitions the whole area into two symmetrical
    and equilateral triangles. This means that the areas under and above the red line
    are 0.5\. The best and perfect classifier will be the one that tries to attain
    the AUROC as 1\. The higher the AUROC, the better the model is.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: In a situation where you have got multiple classifiers, you can use AUROC to
    determine which is the best one among the lot.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: H-measure
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binary classification has to apply techniques so that it can map independent
    variables to different labels. For example, a number of variables exist such as
    gender, income, number of existing loans, and payment on time/not, that get mapped
    to yield a score that helps us classify the customers into good customers (more
    propensity to pay) and bad customers.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Typically, everyone seems to be caught up with the misclassification rate or
    derived form since the **area under curve** (**AUC**) is known to be the best
    evaluator of our classification model. You get this rate by dividing the total
    number of misclassified examples by the total number of examples. But does this
    give us a fair assessment? Let's see. Here, we have a misclassification rate that
    keeps something important under wraps. More often than not, classifiers come up
    with a tuning parameter, the side effect of which tends to be favoring false positives
    over false negatives, or vice versa. Also, picking the AUC as sole model evaluator
    can act as a double whammy for us. AUC has got different misclassification costs
    for different classifiers, which is not desirable. This means that using this is
    equivalent to using different metrics to evaluate different classification rules.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: As we have already discussed, the real test of any classifier takes place on
    the unseen data, and this takes a toll on the model by some decimal points. Adversely,
    if we have got scenarios like the preceding one, the decision support system will
    not be able to perform well. It will start producing misleading results.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'H-measure overcomes the situation of incurring different misclassification
    costs for different classifiers. It needs a severity ratio as input, which examines
    how much more severe misclassifying a class 0 instance is than misclassifying
    a class 1 instance:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '*Severity Ratio = cost_0/cost_1*'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Here, *cost_0 > 0* is the cost of misclassifying a class 0 datapoint as class
    1.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: It is sometimes more convenient to consider the normalized cost *c = cost_0/(cost_0
    + cost_1)* instead. For example, *severity.ratio = 2* implies that a false positive
    costs twice as much as a false negative.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's talk about a scenario wherein we have been given a dataset from a bank
    and it has got features pertaining to bank customers. These features comprise
    customer's income, age, gender, payment behavior, and so on. Once you take a look
    at the data dimension, you realize that there are 850 features. You are supposed
    to build a model to predict the customer who is going to default if a loan is
    given. Would you take all of these features and build the model?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: The answer should be a clear **no**. The more features in a dataset, the more
    likely it is that the model will overfit. Although having fewer features doesn't
    guarantee that overfitting won't take place, it reduces the chance of that. Not
    a bad deal, right?
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is one of the ways to deal with this. It implies a
    reduction of dimensions in the feature space.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways this can be achieved:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature elimination**: This is a process in which features that are not adding
    value to the model are rejected. Doing this makes the model quite simple. We know
    from Occam''s Razor that we should strive for simplicity when it comes to building
    models. However, doing this step may result in the loss of information as a combination
    of such variables may have an impact on the model.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature extraction**: This is a process in which we create new independent
    variables that are a combination of existing variables. Based on the impact of
    these variables, we either keep or drop them.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis is a feature extraction technique that takes all
    of the variables into account and forms a linear combination of the variables.
    Later, the least important variable can be dropped while the most important part
    of that variable is retained.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Newly formed variables (components) are independent of each other, which can
    be a boon for a model-building process wherein data distribution is linearly separable.
    Linear models have the underlying assumption that variables are independent of
    each other.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the functionality of PCA, we have to become familiar with a few
    terms:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance**: This is the average squared deviation from the mean. It is also
    called a **spread**, which measures the variability of the data:'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7a19ac0f-a890-4f5e-afd1-ed3d108cbf02.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the mean.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariance**: This is a measure of the degree to which two variables move
    in the same direction:'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/ad894e3b-abe7-4f79-9626-915f5215f0db.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'In PCA, we find out the pattern of the data as follows: in the case of the
    dataset having high covariance when represented in *n* of dimensions, we represent
    those dimensions with a linear combination of the same *n* dimensions. These combinations
    are orthogonal to each other, which is the reason why they are independent of
    each other. Besides, dimension follows an order by variance. The top combination
    comes first.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go over how PCA works by talking about the following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Let's split our dataset into *Y* and *X* sets, and just focus on *X*.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A matrix of *X* is taken and standardized with a mean of 0 and a standard deviation
    of *1*. Let's call the new matrix *Z*.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s work on *Z* now. We have to transpose it and multiply the transposed
    matrix by *Z*. By doing this, we have got our covariance matrix:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Covariance Matrix = Z^TZ*'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to calculate the eigenvalues and their corresponding eigenvectors
    of *Z^TZ*. Typically, the eigen decomposition of the covariance matrix into *PDP*⁻¹
    is done, where *P *is the matrix of eigenvectors and *D* is the diagonal matrix
    with eigenvalues on the diagonal and values of 0 everywhere else.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the eigenvalues *λ₁*, *λ₂*, …, *λp *and sort them from largest to smallest.
    In doing so, sort the eigenvectors in *P* accordingly. Call this sorted matrix
    of eigenvectors *P**.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate *Z**= *ZP**. This new matrix, *Z**, is a centered/standardized version
    of *X*, but now each observation is a combination of the original variables, where
    the weights are determined by the eigenvector. As a bonus, because our eigenvectors
    in *P** are independent of one another, the columns of *Z** are independent of
    one another.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we studied the statistical model, the learning curve, and curve
    fitting. We also studied two cultures that Leo Breiman introduced, which describe
    that any analysis needs data. We went through the different types of training,
    development, and test data, including their sizes. We studied regularization,
    which explains what overfitting means in machine learning modeling.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: This chapter also explained cross validation and model selection, the 0.632
    rule in bootstrapping, and also ROC and AUC in depth.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study evaluating kernel learning, which is the
    most widely used approach in machine learning.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
