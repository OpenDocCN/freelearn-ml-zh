- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parametric Tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we introduced the concept of a hypothesis test and
    showed several applications of the z-test. The z-test is a type of hypothesis
    test in a family of hypothesis tests called parametric tests. Parametric tests
    are powerful hypothesis tests, but the application of parametric tests requires
    certain assumptions to be met by the data. While the z-test is a useful test,
    it is limited by the required assumptions. In this chapter, we will discuss several
    more parametric tests, which will expand our parametric tool set. More specifically,
    we will discuss the various applications of the t-test, how to perform tests when
    more than two subgroups of data are present, and the hypothesis test for Pearson’s
    correlation coefficient. We will complete the chapter with a discussion on power
    analysis for parametric tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Assumptions of **parametric tests**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**T-test**—a parametric hypothesis test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tests with more than two groups and **analysis of** **variance** (**ANOVA**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pearson’s** **correlation coefficient**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Power** **analysis** examples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions of parametric tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Parametric tests make assumptions about population data that require the statistics
    practitioner to perform analysis of data prior to modeling, especially when using
    sample data because the sample statistics are leveraged as estimates for the population
    parameters when the true population parameters are unknown. These are the three
    primary assumptions of parametric hypothesis tests:'
  prefs: []
  type: TYPE_NORMAL
- en: Normally distributed population data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples are independent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equal population variances (when comparing two or more groups)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we discuss the z-test, t-test, ANOVA, and Pearson’s correlation.
    These tests are used on continuous data. In addition to these assumptions, Pearson’s
    correlation requires data to contain paired samples. In other words, there must
    be an equal number of samples in each group being compared as Pearson’s correlation
    is based on pairwise comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: While these assumptions are ideal, there are many occasions where these cannot
    be ensured. Consequently, it is useful to understand there is some robustness
    to these assumptions, depending on the test.
  prefs: []
  type: TYPE_NORMAL
- en: Normally distributed population data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because in parametric hypothesis tests we are interested in gaining inferences
    about population parameters, such as the mean or standard deviation, we must assume
    the parameter of choice is representative of the distribution and that it is safe
    to assume a central tendency in the data. We must also assume the statistic (parametric
    value taken from a sample or sampling distribution) is representative of its respective
    population parameter. Therefore, since we assume in parametric hypothesis tests
    that the population is normally distributed, the sample should also be normally
    distributed as well. Otherwise, it is not safe to assume the sample is representative
    of the population.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric hypothesis tests rely heavily on the mean and assume it is strongly
    representative of the data’s central point (all population data is centrally distributed
    around the mean). Consider where the means of two distributions are being compared
    to test if there is a statistically significant difference between them. If the
    distributions are skewed, the mean will not be the center point of the data and,
    consequently, cannot represent the distributions very well. Since this would be
    the case, inference obtained from a test comparing the means would not be reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness to normally distributed data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many hypothesis tests specify degrees of freedom when using samples to make
    estimates about populations. Degrees of freedom force models to assume there is
    extra variance in the distributions used than actually present. While the statistical
    parameters in the analysis remain the same, the assumed extra variance forces
    measures of central tendency closer. Stated differently, using degrees of freedom
    forces measures of central tendency to be more centrally representative of the
    distributions from which they are calculated. The reason for this is that it is
    assumed samples—while representative of their overall populations—represent their
    populations with a margin of error. Consequently, parametric hypothesis tests
    using degrees of freedom have some robustness to violations of the requirement
    for normally distributed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plots shown in *Figure 4**.1*, we have a slightly skewed distribution.
    One applies degrees of freedom while the other does not. We can see the mean and
    median have the same distance between them whether degrees of freedom are used
    or not. However, the distribution using degrees of freedom takes on more errors
    (more variance):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Visualizing the influence of degrees of freedom](img/B18945_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Visualizing the influence of degrees of freedom
  prefs: []
  type: TYPE_NORMAL
- en: When using a hypothesis test that considers the mean, we can see that the mean,
    while not centered (as is the median), approximates the center of the distribution
    much more closely, relative to all data points, when degrees of freedom are used.
    Since parametric hypothesis tests use the mean as the central point, this is important
    for the usefulness of the model as the mean is more representative of the central
    point of the data when degrees of freedom are used. This is a primary reason there
    is some robustness to normality. Some other robustness is in the statistical interpretation,
    such as in choosing the level of confidence; if a distribution is not perfectly
    normally distributed, it may be beneficial to use a 90% level of confidence rather
    than a 99% level of confidence, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for normally distributed data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are multiple methods for determining whether a distribution is normally
    distributed and thus can be used in parametric hypothesis testing. Generally,
    the level of adherence to normality is up to the discretion of the researcher.
    The methods in this section leave some margin for debate on normality based on
    visual inspection as well as levels of statistical significance applied.
  prefs: []
  type: TYPE_NORMAL
- en: Visual inspection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The best tests to identify whether a distribution is normally distributed or
    not are based on visual inspection. We can use **Quantile-Quantile** (**QQ**)
    plots and histograms—among other tests—to visually inspect the distributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we generate plots of the original data as well
    as the QQ plots using the `scipy.stats` module’s `probplot` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 4**.2*, we can see in the first column a histogram of exponentially
    distributed data and, beneath it, its QQ plot. As the points are very far from
    approximating adherence to the 45-degree red line, which represents a pure normal
    distribution, we can conclude the data is not normally distributed. By visually
    inspecting the data in the second column, we can see the histogram exhibits an
    approximately normally distributed dataset. This is backed up by the QQ plot below
    it, where the points mostly approximate the 45-degree red line. With respect to
    the tails of the QQ plot, these data points represent the density of skewness.
    We expect with a normally distributed dataset that the bulk of data points will
    tend toward the center of the red line. With the exponential distribution, we
    can see a heavy density toward the left, lower tail of the red line, and a sparse
    scattering of points toward the upper-right side of the line. The QQ plot can
    be read left to right, mirroring the spread seen in the histogram, where the smallest
    values appear on the left-hand side of the *x* axis and the largest on the right-hand
    side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Visually assessing normality with QQ and histogram plots](img/B18945_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Visually assessing normality with QQ and histogram plots
  prefs: []
  type: TYPE_NORMAL
- en: Visual inspection of the QQ plots and histograms should be enough to help a
    researcher conclude whether the normality assumption has been violated or not.
    However, in cases where one might not want to perform visual inspection—such as
    when constructing a data science pipeline—there are alternative approaches that
    provide specific measurements of normality. Three of the most commonly used tests
    are the **Kolmogorov-Smirnov**, **Anderson-Darling**, and **Shapiro-Wilk** tests.
  prefs: []
  type: TYPE_NORMAL
- en: The Kolmogorov-Smirnov test focuses more on the centrality of the data. Consequently,
    however, the test has less power if there is a wide variance around the center
    of the data. Anderson-Darling focuses more on the tails of the data than the center
    and is more likely to identify non-conformity to normality if data is heavy-tailed
    with extreme outliers. These two tests perform well on large sample sizes but
    do not have as much power when sample sizes are lower. The third test we consider,
    Shapiro-Wilk, is more general than the Kolmogorov-Smirnov and Anderson-Darling
    tests and therefore more robust to small sample sizes. Based on these traits,
    it may be more useful to use Shapiro-Wilk tests in an automated pipeline. Alternatively,
    it may be better to lower the level of confidence for the test being applied.
  prefs: []
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `kstest` function in the `scipy.stats` module, using `stats.norm.cdf` (`scipy`’s
    cumulative density function) performs this one-sample version of the test. The
    two-sample version tests against a specified distribution to determine whether
    the two distributions match. In the two-sample case, the distribution to be tested
    must be provided as a `numpy` array instead of the `stats.norm.cdf` function used
    in the code snippet shown below *Figure 4**.3*. However, this is outside of the
    scope of testing for normality, so we will not look at this.
  prefs: []
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov measures a calculated test statistic against a table-based
    critical value (`kstest` calculates this internally). As with other hypothesis
    tests, if the test statistic is larger than the critical value, the null hypothesis
    that the given distribution is normally distributed can be rejected. This can
    also be assessed if the p-value is low enough to be significant. The test statistic
    is calculated as the absolute value of the maximum distance between all data points
    in the given distribution against the cumulative density function.
  prefs: []
  type: TYPE_NORMAL
- en: Kolmogorov-Smirnov special requirement
  prefs: []
  type: TYPE_NORMAL
- en: The Kolmogorov-Smirnov test requires data to be centered around zero and scaled
    to a standard deviation of one. All data must be transformed for the test, but
    inference can be applied to the pre-transformed distribution; the centered and
    scaled distribution does not need to be the distribution used in further statistical
    testing or analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we test to confirm whether a normally distributed
    dataset, `normally_distributed`, is normally distributed. The dataset has a mean
    of 0 and a standard deviation of 1\. The output confirms the data is normally
    distributed. The plots in *Figure 4**.3* show the normally distributed distribution
    centered around a mean of 0 with a standard deviation of 1, and on the right of
    it is the exponentially transformed version of the same distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.3 – Normally distributed and exponential data](img/B18945_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Normally distributed and exponential data
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we run the Kolmogorov-Smirnov test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `statsmodels` Kolmorogov-Smirnov test yielded the following results for
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KstestResult(statistic=0.0191570377833315, pvalue=0.849436919292824)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the same data, but transform it exponentially to be right-skewed,
    the same test indicates the data is no longer normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The signficant p-value confirms non-normality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KstestResult(statistic=0.5375205782404135, pvalue=9.59979841227121e-271)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let us take a distribution of 1,000 samples with a mean of 100 and a
    standard deviation of 2\. We need to center it to a mean of 0 with unit variance
    (standard deviation of 1). In the following code snippet, we generate the data,
    then perform the scaling and save it to the `normally_distributed_scaled` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the data is centered and scaled as required, we check it using the
    Kolmogorov-Smirnov test. As expected, the data is confirmed normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KstestResult(statistic=0.02597307287070466, pvalue=0.5016041053535877)`'
  prefs: []
  type: TYPE_NORMAL
- en: Anderson-Darling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to the Kolmogorov-Smirnov test, the `scipy`’s `anderson` test, we can
    test against other distributions, but the default argument specifying a normal
    distribution, `dist="norm"`, assumes a null hypothesis that the given distribution
    is statistically the same as a normally distributed distribution. For each distribution
    tested against, a different set of critical values must be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: Anderson-Darling compared to Kolmogorov-Smirnov
  prefs: []
  type: TYPE_NORMAL
- en: Note that while both the Anderson-Darling and Kolmogorov-Smirnov tests use the
    cumulative density frequency distributions to test for normality, the Anderson-Darling
    test is different from the Kolmogorov-Smirnov test because it weights the variance
    in the tails of the cumulative density frequency distribution more than the middle.
    This is because the variance in the tails can be measured in smaller increments
    than in the middle of the distribution. Consequently, the Anderson-Darling test
    is more sensitive to tails than the Kolmogorov-Smirnov test. In line with the
    Kolmogorov-Smirnov test, a test statistic is calculated and measured against a
    critical value. If the test statistic is larger than the critical value, the null
    hypothesis that the given distribution is normally distributed can be rejected
    at the specified level of significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are using the Anderson-Darling test to test a random normal probability
    distribution generated with a mean of 19 and a standard deviation of 1.7\. We
    also test an exponentially transformed version of this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 4**.4* shows plots of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Normal distribution versus heavy-tailed exponential distribution](img/B18945_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Normal distribution versus heavy-tailed exponential distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code and output shown next, in *Figure 4**.5*, we can see the distribution
    is normally distributed at all levels of significance. Recall that the level of
    significance is the p-value (that is, a level of significance = 15.0 means a p-value
    of 0.15 or smaller is significant):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the data generated through the `numpy` `random.normal` function is tested
    with the Anderson-Darling method and confirmed to be normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Test statistic** | **Critical value** | **Significance level** | **Normally
    distributed** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.191482344 | 0.574 | 15 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.191482344 | 0.653 | 10 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.191482344 | 0.784 | 5 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.191482344 | 0.914 | 2.5 | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.191482344 | 1.088 | 1 | Yes |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.5 – Anderson-Darling results for normally distributed data
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we test an exponential transformation of the normally distributed data
    to check for normality. The data is exponentially distributed and should reject
    at all levels of significance. However, we see in *Figure 4**.6* that it has failed
    to reject at the 0.01 level of significance (99% confidence). Therefore, depending
    on the use case, it may be prudent to check all levels of significance, use a
    different test, or make a decision based on multiple tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Anderson-Darling test of non-normally distributed data outputs are as follows
    in *Figure 4**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Test statistic** | **Critical value** | **Significance level** | **Normally
    distributed** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.96277351 | 0.574 | 15 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 0.96277351 | 0.653 | 10 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 0.96277351 | 0.784 | 5 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 0.96277351 | 0.914 | 2.5 | No |'
  prefs: []
  type: TYPE_TB
- en: '| 0.96277351 | 1.088 | 1 | Yes |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.6 – Anderson-Darling results for non-normally distributed data
  prefs: []
  type: TYPE_NORMAL
- en: Shapiro-Wilk
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `scipy.stats shapiro` module, so input data does not need to be altered
    prior to testing. The level of significance for this test in `scipy` is 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: Shapiro-Wilk compared to Kolmogorov-Smirnov and Anderson-Darling
  prefs: []
  type: TYPE_NORMAL
- en: Shapiro-Wilk is ideal, compared to Kolmogorov-Smirnov and Anderson-Darling,
    for testing small sample sizes of roughly less than 50\. However, one drawback
    is that since Shapiro-Wilk uses repeated sampling and testing for the calculated
    test statistic by applying Monte Carlo simulation, the law of large numbers poses
    a risk that as the sample size increases, there is an inherent increase in the
    risk of encountering a *type II* error (a loss of power) and failing to reject
    the null hypothesis, where the null hypothesis states the given distribution is
    normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the same distributions as in the Anderson-Darling test, we test with
    Shapiro-Wilk. We can see with the random normal distribution with a mean of 19
    and a standard deviation of 1.7, the Shapiro-Wilk test has confirmed with a p-value
    of 0.99 that the null hypothesis that the input distribution is normally distributed
    should not be rejected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ShapiroResult(statistic=0.9993802905082703, pvalue=0.9900037050247192)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'When testing using the exponentially transformed version of the normally distributed
    data, we find a significant p-value (p = 0.0), indicating we have enough evidence
    to reject the null hypothesis and conclude the distribution is not normally distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ShapiroResult(statistic=0.37320804595947266, pvalue=0.0)`'
  prefs: []
  type: TYPE_NORMAL
- en: Independent samples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In parametric hypothesis testing, the independence of samples is another important
    assumption. Two effects can occur from non-independent sampling. One effect occurs
    when subgroup sampling is performed. The issue here is that responses in one subgroup
    of the population may be different than responses from another subgroup of the
    same population or even more similar to those of a different population. However,
    when sampling representative of the overall population is taken, this type of
    subgroup difference may not be very representative of the population.
  prefs: []
  type: TYPE_NORMAL
- en: Another effect of non-independent sampling is when samples are taken close enough
    together in time that the occurrence of one precludes or excludes the occurrence
    of another. This is called serial (or auto-) correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Parametric tests are not typically robust to violations of this requirement
    as it has direct, categorical implications on the interpretability of test outcomes.
    With respect to subgroup sampling, this can be prevented through a well-structured
    sampling approach such as those outlined in [*Chapter 1*](B18945_01.xhtml#_idTextAnchor015)*,
    Sampling and Generalization*. However, as regards the serial effect, we can test
    for autoregressive correlation (also called serial correlation) in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Durbin-Watson
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common tests performed to assess a lack of independence in sampling
    is the first-order (also referred to as lag-one) autoregressive test called the
    **Durbin-Watson** test. **Autoregressive** means previous data points are used
    to predict the current data point. First-order means the last sampled data point
    (lag one) is the point most significantly correlated to the most recently sampled
    data point (lag zero) in a sequence of sampled data. In first-order autocorrelation,
    the correlation for each data point is strongest with the previous data point.
    The Durbin-Watson test does not test whether any value is correlated to the value
    before it, but instead if, overall, there is a strong enough relationship between
    each value and the value before it to conclude there is significant autocorrelation.
    In that sense, there is some robustness to non-independent sampling such that
    an accident or two may not completely invalidate a hypothesis test, but a consistent
    recurrence of this type of violation will.
  prefs: []
  type: TYPE_NORMAL
- en: A Durbin-Watson value of 2 indicates no significant autocorrelation, a value
    between 0 and 2 represents positive (direct) autocorrelation, and a value between
    2 and 4 represents negative (inverse) autocorrelation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have two distributions, each with 1,000 samples.
    The distribution on the left is a sinusoidal distribution that exhibits strong
    autoregressive correlation, and the distribution on the right is a set of randomly
    generated data displaying as white-noise variance (random points centered around
    a mean of 0). Using the `durbin_watson()` function from the `statsmodels.stats`
    module, we are able to confirm direct, positive lag-one autocorrelation in the
    sinusoidal pattern (a very small Durbin-Watson value) and a Durbin-Watson statistic
    of 2.1 with the random noise, indicating no autocorrelation. Therefore, in *Figure
    4**.7*, the plot on the left is not composed of independent samples whereas the
    plot on the right is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 4.7 – Serially correlated and normally distributed sequence data](img/B18945_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Serially correlated and normally distributed sequence data
  prefs: []
  type: TYPE_NORMAL
- en: Equal population variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to the assumption of normally distributed data, the assumption of equal
    population variance—also referred to as homogeneity of variance—is about the shape
    of the physical properties of the distributions being compared. Assuming equal
    population variance helps increase the power of a parametric test. This is because
    there is confidence when means are identified as being different; we also know
    the degree of potential distribution overlap. When a test has an intuition about
    the location of the full distributions—in effect, true knowledge of the effect
    size—power increases. Conversely, as variances diverge, power decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness to equal population variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While equal population variance is useful in parametric testing, modifications
    to these tests exist that help results be robust to deviance from equal variance.
    One prominent modified version of these tests uses the **Welch-Satterthwaite**
    adjustment to the degrees of freedom used. Because applying the same degree of
    freedom to each group when each group has a different variance would result in
    a misrepresentation of the data, the Welch-Satterthwaite adjustment accounts for
    variance differences when allocating degrees of freedom to parametric tests that
    assume equal variance. Two common tests that use the Welch-Satterthwaite adjustment
    are Welch’s t-test and Welch’s ANOVA test. When used on small samples, these tests
    may not be reliable, but when used on sample sizes large enough to have sufficient
    power, the results should be approximately the same as their non-Welch counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: Testing for equal variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When testing for equal variance among distributions, we have two prominent
    tests: **Levene’s test for equality of variances** and **Fisher’s F-test**.'
  prefs: []
  type: TYPE_NORMAL
- en: Levene’s test for equality of variances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Levene’s test for equality of variances is useful when testing for homogeneity
    of variance of two or more groups. In the code snippet shown below *Figure 4**.8*,
    we test with three distributions, each having a sample size of 100, a mean of
    0, and standard deviations of 0.9, 1.1, and 2\. *Figure 4**.8* is a plot of the
    three distributions generated using the data output from the code above *Figure
    4**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can see how their different standard deviations impact their range.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Distributions for multiple equality of variance testing](img/B18945_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Distributions for multiple equality of variance testing
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the test is sensitive to violations of non-homogenous variance because
    the result of this is a statistically significant p-value indicating non-homogenous
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The distributions do not have homogenous variance. P-value =` `0.0000`'
  prefs: []
  type: TYPE_NORMAL
- en: Fisher’s F-test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fisher’s F-test is useful when testing for homogeneity of variance for two
    groups at a time. This test compares a test statistic to a critical value to determine
    whether the variances are statistically the same or not. The calculated F-statistic
    is the variance of group one divided by the variance of group two. Group one is
    always the group with the larger variance. Using the preceding data, let us compare
    distribution 1 with distribution 3\. Distribution 3 has a larger variance of 2,
    so that group’s variance will be the numerator when calculating the F-statistic.
    Since each group has a sample size of 100, their degrees of freedom for the table
    lookup will each be 99\. However, since we will use the `scipy` Python package
    to compute the test, here, the table lookup is not needed as `scipy` does this
    for us with the `f.cdf()` function. In line with the results of the Levene test,
    the F-test indicates distribution 1 and distribution 3 do not have homogenous
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This F-test output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The distributions do not have homogenous variance. P-value = 0.0000, F-statistic
    =` `102622.9745`'
  prefs: []
  type: TYPE_NORMAL
- en: T-test – a parametric hypothesis test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, the z-test for means was applied when population standard
    deviations were known. However, in the real world, it is not easy (or virtually
    impossible) to obtain the population standard deviation. In this section, we will
    discuss another hypothesis test called the **t-test**, which is used when the
    population standard deviations are unknown. The mean and the standard deviation
    of a population are estimated by taking the mean and the standard deviation of
    sample data representative of this population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, the method for the t-test for means is very similar to the
    one for the z-test for means, but the calculations for the test statistic and
    p-value are not the same as for the z-test. The test statistic is computed by
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: t =   _ x  − μ _ s/ √ _ n
  prefs: []
  type: TYPE_NORMAL
- en: 'Here,  _ x  , μ, s, and n are the sample mean, population mean, sample standard
    deviation, and sample size, respectively, which has a t**-distribution** when
    the sample data, x, is normally distributed. The following code illustrates the
    standard normal distribution (blue curve) with 1,000 samples and t-distribution
    (green and red curves) with two sample sizes—3 and 16 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The following visuallization is for these 3 distributions considered.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Normal and t-distributions](img/B18945_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Normal and t-distributions
  prefs: []
  type: TYPE_NORMAL
- en: Observe that the three curves have similar symmetry and shapes but there is
    more variability (or, in other words, heavier tails) for a sample with a smaller
    size. Historically, researchers considered a sample standard deviation to represent
    the population when the sample size was greater than 30, that is, the red curve
    approximates the blue curve when n > 30\. It was also common to use the z-test
    if the sample distribution overlapped the standard normal distribution. This practice
    has some reasoning behind it because, previously, critical value tables were stored
    up to a sample size of 50, but nowadays, with the power of computation and the
    internet, the *t* values can be obtained easily with any sample size.
  prefs: []
  type: TYPE_NORMAL
- en: T-test for means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-sample and two-sample t-tests related to a population mean or means of two
    populations where the population variances or population standard deviations are
    unknown will be considered in this part.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform a t-test, the following assumptions need to be satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normality**: The sample is normally distributed'
  prefs: []
  type: TYPE_NORMAL
- en: '**Independence**: Observations are randomly selected from a population to form
    a sample or, in other words, they are independent'
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider the one-sample t-test in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: One-sample t-test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the one-sample z-test, the null and alternative hypotheses need
    to be considered in order to perform the hypothesis test. Three null and alternative
    hypotheses corresponding to left-tailed, right-tailed, and two-tailed t-tests
    are presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : μ ≥ μ 0 H 0 : μ ≤ μ 0 H 0 : μ = μ 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : μ < μ 0 H a : μ > μ 0 H a : μ ≠ μ 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the level of significance, α, needs to be specified following the research
    purpose. There are two approaches: the p-value approach and the critical value
    approach. In the p-value approach, the rejection rule (reject H 0—the null hypothesis)
    is when the p-value is less than or equal to the specified level of significance
    chosen. In the critical value approach, the rejection rule is when the test statistic
    is less than or equal to the critical value − t α for the left-tailed t-test,
    the test statistic is greater than or equal to t α for a right-tailed t-test,
    and the test statistic is less than or equal to − t α/2 or greater than or equal
    to t α/2 for a two-tailed test. The last step is to interpret the statistical
    conclusion for the hypothesis test.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the p-value based on the value of the student t distribution, we can
    use the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, `x` is the test statistic and `df` is the degree of freedom (`df` = n-1
    where *n* is the sample size) in the formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to find the p-value associated with a t-score of 1.9 with the
    degree of freedom 14 in a left-tailed test, this would be the Python implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would be 0.0391\. If the level of significance α = 0.05, then we
    reject the null hypothesis because the p-value is less than α. For a right-tailed
    t-test, similar Python code as in the left-tailed t-test is implemented to find
    the p-value. For a two-tailed test, we need to multiply the value by 2, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Here, `t` is the test statistic and `df` is the degree of freedom (`df` = n-1
    where *n* is the sample size).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the critical value in Python, we use the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `q` is the level of significance and `df` is the degree of freedom to
    be used in the formula. Here is the implementation of the code in Python for left-tailed,
    right-tailed, and two-tailed tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: The critical value is -1.7530503556925552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critical value is 1.7530503556925547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critical values are -2.131449545559323 and 2.131449545559323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the level of significance α = 0.05, for the left-tailed test, the critical
    value is about -1.753\. Since this is a left-tailed test, if the test statistic
    is less than or equal to this critical value, we reject the null hypothesis. Similarly,
    for the right-tailed test, if the test statistic is greater than or equal to 1.753,
    we reject the null hypothesis. For the two-tailed test, we reject the null hypothesis
    if the test statistic is greater than or equal to 2.1314 or less than or equal
    to -2.1314\. Finally, we interpret the statistical conclusion for the hypothesis
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us randomly choose 30 students from a high school and score their IQ. We
    would like to test the claim that the mean IQ score of the distribution of the
    students from this high school is higher than 100\. This means that we will perform
    a right-tailed t-test. The IQ scores of 30 students are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Before conducting the hypothesis testing, we will check normality and independence
    assumptions. The assumption of independence is satisfied if the sample is randomly
    selected from the population of high school students at this school. For normality,
    we will check the histogram and QQ plots of IQ score data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Visually assessing normality of student IQ scores](img/B18945_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Visually assessing normality of student IQ scores
  prefs: []
  type: TYPE_NORMAL
- en: There is little to no evidence from the histogram and QQ plot that the population
    IQ score distribution of the students at the high school is not normal. Since
    the distribution is assumed to be normal, we will proceed with the t-test.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the null hypothesis and the alternative hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : μ ≤ 100'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : μ > 100'
  prefs: []
  type: TYPE_NORMAL
- en: 'We choose the level of significance α=0.05\. You can calculate the test statistic
    by using its mathematical formula by hand or by implementing Python. For the critical
    value and p-value, the implemented code was shown in the previous part. Here,
    we will use another function from the `scipy` library to find the test statistic
    and p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data`: The observations from the sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`popmean`: The expected value in the null hypothesis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alternative`: `''two-sided''` for a two-tailed t-test, `''less''` for a left-tailed
    t-test, and `''greater''` for a right-tailed t-test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Python code is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The test statistic is 6.159178830896832 and the corresponding p-value` `is
    5.15076734562176e-07.`'
  prefs: []
  type: TYPE_NORMAL
- en: Because the p-value < 0.05 where 0.05 is the level of significance, we have
    enough evidence to reject the null hypothesis and conclude that the true mean
    IQ scores of the students from this school is higher than 100.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, with 95% confidence, the mean IQ score lies between 104.08 and
    107.12\. We can perform the calculation for the confidence interval in Python
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The steps to perform a hypothesis test in Python using the left-tailed t-test
    are similar to those of the right-tailed and two-tailed t-tests.
  prefs: []
  type: TYPE_NORMAL
- en: Two-sample t-test – pooled t-test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to what was covered in [*Chapter 3*](B18945_03.xhtml#_idTextAnchor055)*,
    Hypothesis Testing,* (two-sample z-test for means), the two-sample t-test for
    means has three forms for the null and alternative hypotheses. Some assumptions
    need to be satisfied before conducting the test, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normality**: Two samples are drawn from their normally distributed populations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independence**: The observations of one sample are independent of one another'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Homogeneity of variance**: Both populations are assumed to have similar standard
    deviations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For normality, we use visual histograms and also QQ plots of the two samples
    and compare them. Let us assume independence is satisfied. In order to check equal
    standard deviations between the two samples, we could use visualization by observing
    their histograms and also use an F-test to have additional evidence if the visualization
    is inconclusive. This is a hypothesis test to check whether two sample variances
    are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at the IQ scores between two high schools, A and B. The following
    are the scores of 30 students from each school, randomly selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The histograms and QQ plots shown in Figure 4.11 are generated by the IQ data
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Assessing normality of two schools’ IQ scores](img/B18945_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Assessing normality of two schools’ IQ scores
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the normality assumption is satisfied. We also can assume by
    observing the histograms that the equal variance assumption is supported. Another
    F-test to check the equal variance assumption (if necessary) follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code tells us the F-test statistic is 0.9963 and
    the corresponding p-value is 0.50394 > 0.05 (0.05 is the level of significance),
    then we fail to reject the null hypothesis. This means that there is enough evidence
    to say that the standard deviations of these two samples are equal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now define the null hypothesis and the alternative hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : μ A = μ B,'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : μ A ≠ μ B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We choose the level of significance α=0.05\. We use the `statsmodels.stats.weightstats.ttest_ind`
    function to conduct the t-test. The documentation can be found here: [https://www.statsmodels.org/dev/generated/statsmodels.stats.weightstats.ttest_ind.xhtml](https://www.statsmodels.org/dev/generated/statsmodels.stats.weightstats.ttest_ind.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use this function to perform three forms of the alternate hypothesis
    with `alternative=''two-sided''`, `''larger''`, or `''smaller''`. In the pooled-variance
    t-test, when the assumption for equal variances is satisfied, the test statistic
    is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: t =  (‾ x 1 − ‾ x 2) − (μ 1 − μ 2)  _____________  s p √ _  1 _ n 1 +  1 _ n 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, _ x 1, _ x 2, μ 1, μ 2, n 1,and n 2 are sample means, population means,
    and sample sizes of two samples, 1 and 2 respectively, and the pooled standard
    deviation is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: s p = √ ________________   (n 1 − 1) s 1 2 + (n 2 − 1) s 2 2  ________________  n 1
    + n 2 − 2
  prefs: []
  type: TYPE_NORMAL
- en: 'The degree of freedom is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: df = n 1 + n 2 − 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go back to the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The output returns the test statistic 3.78 and the p-value 0.00037, and the
    degrees of freedom used in the t-test are 58 (each sample size has 30 observations,
    then the degrees of freedom are calculated as 30 + 30 - 2 = 58).
  prefs: []
  type: TYPE_NORMAL
- en: Because the p-value <0.05, we reject the null hypothesis. There is sufficient
    evidence to suggest that there is a difference in mean IQ score between students
    at high schools A and B. To perform the confidence level, you can adapt the Python
    code in the last part of the one-sample t-test.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in some situations, if the histograms and QQ plots show some evidence
    of skewness, we can consider testing the medians instead of the means for hypothesis
    testing. As shown in [*Chapter 2*](B18945_02.xhtml#_idTextAnchor029)*, Distributions
    of Data*, we can perform a data transformation (for example, log transformation)
    to obtain the normality assumption. After the transformation, the median of `log(data)`
    is equal to the mean of `log(data)`. This means that the test is performed on
    means of the transformed data.
  prefs: []
  type: TYPE_NORMAL
- en: Two-sample t-test – Welch’s t-test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a practical two-sample t-test when the data is normally distributed
    but the population standard deviations are unknown and unequal. We have the same
    assumption for normality as with a pooled t-test but we can relax the assumption
    for equal variance when performing Welch’s t-test. Let us consider the following
    example where we have two sample datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We assume the independence is satisfied, but we will check the normality and
    equal standard deviation assumptions for these two samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Checking equal variance for Welch’s t-test](img/B18945_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Checking equal variance for Welch’s t-test
  prefs: []
  type: TYPE_NORMAL
- en: 'There is strong visual evidence against equal standard deviations by looking
    at the *x* axis scale of the histograms in *Figure 4**.12*. Let us assume the
    normality assumption is satisfied. In this case, a two-sample pooled t-test is
    not a good idea, but Welch’s t-test would suffice. The null and alternative hypotheses
    are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 :  μ 1 = μ 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a :  μ 1 ≠ μ 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the level of significance 0.05\. To calculate the test statistic
    and p-value, we implement the code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The test statistic is -22.47 and the p-value is <0.05 (the level of significance).
    We reject the null hypothesis. There is strong evidence to suggest the mean of
    sample data 1 is different from the mean of sample data 2.
  prefs: []
  type: TYPE_NORMAL
- en: Paired t-test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The paired t-test is also known as a matched pairs or dependent t-test and
    is used in studies when each element in a sample is tested twice (pre-test and
    post-test or repeated measures) and when the researcher thinks that there are
    some similarities, such as family. The assumptions are set out here:'
  prefs: []
  type: TYPE_NORMAL
- en: Differences are normally distributed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences are independent between observations but dependent from one test
    to another test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paired t-test is used in many studies, especially in medical reasoning
    tests related to pre- and post-treatments. Let’s go back to IQ test scores—a researcher
    recruits a number of students to see whether there is a score difference before
    and after a training section, as represented in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Students** | **Pre-training score** | **Post-training score** | **Differences**
    |'
  prefs: []
  type: TYPE_TB
- en: '| A | 95 | 95 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 98 | 110 | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 90 | 97 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| D | 115 | 112 | -3 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 112 | 117 | 5 |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.13 – Pre-training and post-training scores
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we should not use an independent two-sample t-test. The mean
    of the differences should be tested here. We can check the assumption about normal
    distribution by using histogram and QQ plots, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Checking normality for the paired t-test](img/B18945_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Checking normality for the paired t-test
  prefs: []
  type: TYPE_NORMAL
- en: Evidence for the data being normally distributed is more obvious by looking
    at the QQ plot than the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences are assumed to be independent. The null and alternative hypotheses
    are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : μ pos − μ pre = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : μ pos − μ pre > 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'd i denotes the difference between the pre-training score and the post-training
    score of each student. The null and alternative hypotheses can be rewritten as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'H 0 : μ d = 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'H a : μ d > 0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the test statistic is computed like so:'
  prefs: []
  type: TYPE_NORMAL
- en: t =   _ d  − μ d _  s d _ √ _ n
  prefs: []
  type: TYPE_NORMAL
- en: 'Here,  _ d  is the sample mean of the differences and s d is the sample standard
    deviation of differences. In other words, a paired t-test is reduced to a one-sample
    t-test. However, we can use the following function in `scipy` directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stats.ttest_rel(data_pos, data_pre, alternative = {''two-sided'', ''``less'',
    ''greater''})`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The alternative hypothesis corresponds to a left-tailed, right-tailed, or two-tailed
    test. Here is the Python implementation for the IQ test score study example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The test statistic is 1.594 and the p-value is 0.093\. Therefore, given the
    p-value is <0.05 and the level of significance α = 0.05, we reject the null hypothesis.
    There is sufficient evidence to suggest that training has a significant effect
    on IQ scores.
  prefs: []
  type: TYPE_NORMAL
- en: Tests with more than two groups and ANOVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter and previous sections, we covered tests between two
    groups. In this section, we will cover two methods for testing differences between
    groups, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise tests with the **Bonferroni correction**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANOVA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When testing for differences between more than two groups, we will have to use
    multiple tests, which will affect our *type I* error rate. There are several methods
    to control the error rate. We will see how to utilize the Bonferroni correction
    to control the *Type I* error rate. We will also discuss ANOVA in this section,
    which is used to test for a difference in means of multiple groups.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple tests for significance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, we looked at making a comparison between two groups.
    In this section, we will consider how to perform tests when there are more than
    two groups present. Let’s again consider the factory example where we have several
    models (model A, model B, and model C) of machines on a factory floor, and these
    machines are used to perform the same operation in the factory. A plausible question
    of interest is: *Does one machine model have a higher mean output than the other
    two models?* To make this determination, we would need to do three tests comparing
    the difference in means of each model to the other models, testing that the difference
    in means is different than zero. These are the null hypotheses we would need to
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: μ output,A − μ output,B = 0
  prefs: []
  type: TYPE_NORMAL
- en: μ output,B − μ output,C = 0
  prefs: []
  type: TYPE_NORMAL
- en: μ output,A − μ output,C = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'When performing multiple tests, we will need to apply p-value corrections for
    our expected error rate. Recall that in [*Chapter 3*](B18945_03.xhtml#_idTextAnchor055)*,
    Hypothesis Testing*, we defined the expected error rate for a hypothesis test
    as α. This is the rate at which we expect a *single* hypothesis test to result
    in a *Type I* error. In our example with factory machines, we are making three
    hypothesis tests, which means *we are three times more likely to see a Type I
    error*. While our example specifically considers multiple tests for differences
    in means, this applies to any type of hypothesis test. In these situations with
    multiple tests, we will generally define a **familywise error rate** (**FWER**)
    and apply p-value corrections to control for the FWER. The FWER is the probability
    of making a *Type I* error from a group of hypothesis tests. The error rate from
    tests within the group is the **individual error rate** (**IER**). We will define
    the IER and FWER as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**IER**: The expected *Type I* error rate for an individual hypothesis test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FWER**: The expected *Type I* error rate for a group of hypothesis tests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss one method for p-value correction in this section to provide
    intuition for the rationale.
  prefs: []
  type: TYPE_NORMAL
- en: The Bonferroni correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One method for adjusting the p-value to control multiple hypothesis tests is
    the Bonferroni correction. The Bonferroni correction controls the FWER by uniformly
    reducing the significance level of each individual test in the family of tests.
    Given that we have m tests in a family each with the p-value p i, then the p-value
    correction is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p i ≤  α _ m
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking our previous example of three models of machines, we have a family of
    three tests, making m = 3\. If we let FWER be 0.05, then, with the Bonferroni
    correction, the level of significance for the three individual tests is this:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.05 _ 3  = 0.0167
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this example, any of the individual tests would be required to have
    a p-value of 0.0167 to be considered significant.
  prefs: []
  type: TYPE_NORMAL
- en: Effects on Type I and Type II errors
  prefs: []
  type: TYPE_NORMAL
- en: As discussed, the Bonferroni correction reduces the significance levels of individual
    tests to control the *Type I* error rate at the family level. We should also consider
    how this change impacts the *Type II* error rate. In general, reducing the significance
    level of individual tests will increase the chance of making a *Type II* error
    for that test (as is done in the Bonferroni correction). While we have only discussed
    the Bonferroni correction in this section, there are other methods for p-value
    correction that provide different trade-offs. Check the documentation of `multipletests`
    in `statsmodels` to see a list of p-value corrections implemented in `statsmodels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at an example using the miles per gallon (MPG) data from
    the *Auto MPG* dataset from the *UCI Machine Learning Repository*, which can be
    found at this link: [https://archive.ics.uci.edu/ml/datasets/Auto+MPG](https://archive.ics.uci.edu/ml/datasets/Auto+MPG)
    [1]. This dataset contains various attributes, including `origin`, `mpg`, `cylinders`,
    and `displacement`, for vehicles manufactured between 1970 and 1982\. We will
    show an abbreviated form of the analysis here; the full analysis is included in
    the notebook in the code repository for this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will use the `mpg` and `origin` variables, and test whether
    there is a difference in `mpg` from the different origins with a significance
    level of 0.01\. The group means are shown in the following table (`origin` is
    an integer-encoded label in this dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '| `origin`) | `mpg`) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 20.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 27.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 30.5 |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.15 – Vehicle MPG means for each origin group
  prefs: []
  type: TYPE_NORMAL
- en: 'Running a t-test to compare each mean, we get the following p-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Null hypothesis** | **Uncorrected p-value** |'
  prefs: []
  type: TYPE_TB
- en: '| μ 1 − μ 2 = 0 | 7.946116336281346e-12 |'
  prefs: []
  type: TYPE_TB
- en: '| μ 1 − μ 3 = 0 | 4.608511957238898e-19 |'
  prefs: []
  type: TYPE_TB
- en: '| μ 2 − μ 3 = 0 | 0.0420926104552266 |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.16 – Uncorrected p-values for t-tests on the difference between each
    group
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying the Bonferroni correction to the p-values, we get the following p-values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Null hypothesis** | **Corrected** **p-value (Bonferroni)** |'
  prefs: []
  type: TYPE_TB
- en: '| μ 1 − μ 2 = 0 | 2.38383490e-11 |'
  prefs: []
  type: TYPE_TB
- en: '| μ 1 − μ 3 = 0 | 1.38255359e-18 |'
  prefs: []
  type: TYPE_TB
- en: '| μ 2 − μ 3 = 0 | 0.126277831 |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.17 – Corrected p-values for t-tests on the difference between each
    group
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding p-values, reject the null hypothesis for a difference in
    the means of groups 1 and 2 and a difference in the means of groups 1 and 3, but
    fail to reject the null hypothesis for a difference in the means of groups 2 and
    3 at our significance level of 0.01.
  prefs: []
  type: TYPE_NORMAL
- en: Other p-value correction methods
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we only discussed one method for p-value correction—the Bonferroni
    correction—to provide intuition for the rationale of p-value corrections. However,
    there are other correction methods available that might be better suited for your
    problem. To see a list of p-value correction methods implemented within `statsmodels`,
    check the documentation of `statsmodels.stats.multitest.multipletests`.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section on multiple tests for significance, we saw how to perform
    multiple tests to determine whether means differed between groups. When dealing
    with means of groups, a useful first task is to conduct an analysis of variance.
    ANOVA is a statistical test for determining whether there is a difference between
    means of several groups. The null hypothesis is there is no difference in means,
    and the alternative hypothesis is the means are not all equal. Since ANOVA tests
    for a difference in means, it is commonly used before testing for a difference
    in means with pairwise hypothesis tests. If the ANOVA null hypothesis fails to
    be rejected, then there is no need to perform the pairwise tests. However, if
    the ANOVA null hypothesis is rejected, then pairwise tests can be performed to
    determine which specific means differ.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA versus pairwise tests
  prefs: []
  type: TYPE_NORMAL
- en: While pairwise testing is a general procedure for testing for differences between
    groups, ANOVA can only be used to test for differences in means.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will again consider the MPG of vehicles from the *Auto
    MPG* dataset. Since we have already run pairwise tests and found a significant
    difference in the mean mpg of vehicles based on origin, we expect that ANOVA will
    provide a positive test result (reject the null hypothesis). Performing the ANOVA
    calculation, we get the following output. The small p-value suggests that we should
    reject the null hypothesis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The ANOVA analysis shown here is abbreviated. For the full code, see the associated
    notebook in the code repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered methods for performing hypothesis tests for more
    than two groups of data. The first method was pairwise testing with p-value correction,
    which is a general method that can be used for any type of hypothesis test. The
    other method we covered was ANOVA, which is a specific test for differences in
    the means of groups. This is not a general method such as pairwise testing but
    can be used as a first step before performing pairwise tests for differences in
    means. In the next section, we cover another type of parametric test that can
    be used to determine whether two sets of data are correlated.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson’s correlation coefficient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pearson’s correlation coefficient**, also called Pearson’s *r* (or Pearson’s
    rho (*ρ*) when applied to population data) or the **Pearson product-moment sample
    coefficient of correlation (PPMCC)**, is a bivariate test that measures the linear
    correlation between two variables. The coefficient produces a value ranging from
    -1 to 1 where -1 is a strong, inverse correlation and 1 is a strong, direct correlation.
    A zero-valued coefficient indicates no correlation between the two variables.
    Weak correlation is generally considered to be correlation between +/- 0.1 and
    +/- 0.3, moderate correlation is between +/- 0.3 and +/- 0.5, and strong correlation
    is between +/- 0.5 to +/- 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: This test is considered parametric but does not require assumptions of normal
    distribution or homogeneity of variance. It is, however, required that data be
    independently sampled (both randomly selected and without serial correlation),
    have finite variance—such as with a distribution that has a very heavy tail—and
    be of a continuous data type. The test does not indicate an input variable and
    a response variable; it is simply a measure of the linear relation between two
    variables. The test uses standardized covariance to derive correlation. Recall
    that standardization requires dividing a value by the standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for the population Pearson’s coefficient, ρ, is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: ρ =  σ xy _ σ x σ y
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, σ xy is the population covariance, calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: σ xy =  ∑ i=1 N  (x i − μ x)(y i − μ y)  ______________ N
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation for the sample Pearson’s coefficient, *r*, is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: r =  S xy _ S x S y
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, S xy is the sample covariance, calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: S xy =  ∑ i=1 n  (x i −  _ x )(y i −  _ y )  _____________  n − 1
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can perform this test using the `scipy` `scipy.stats.pearsonr`
    function. In the following code snippet, we generate two normally distributed
    datasets of random numbers using `numpy`. We want to test the hypothesis that
    there is a correlation between the two groups since there is some significant
    overlap:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 4**.18*, we can observe the overlapping variance of the two correlated
    distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Correlated distributions](img/B18945_04_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Correlated distributions
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plot shown in *Figure 4**.18*, we can see the overlap of the populations.
    Now, we want to test the correlation using the `pearsonr()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that at a 0.05 level of significance, we have
    a 0.9327 level of correlation (p-value is 0.0027):'
  prefs: []
  type: TYPE_NORMAL
- en: '`p-value =` `0.0027`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Correlation coefficient =` `0.9327`'
  prefs: []
  type: TYPE_NORMAL
- en: To frame the correlation differently, we could say the level of variance explained
    (r 2, also called **goodness-of-fit** or the **coefficient of determination**)
    in distribution 2 by distribution 1 is 0.9327 2 = 87%, assuming we know that distribution
    2 is a response to distribution 1\. Otherwise, we could simply say there is a
    correlation of 0.93 or an 87% level of variance explained in the relationship
    between the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us look at the *Motor Trend Car Road Tests* dataset from R, which
    we import using the `statsmodels datasets.get_rdataset` function. Here, we have
    the first five rows, which have the variables for miles per gallon (`mpg`), number
    of cylinders (`cyl`), engine displacement (`disp`), horsepower (`hp`), rear-axle
    gear ratio (`drat`), weight (`wt`), minimum time to drive a quarter of a mile
    (`qsec`), engine shape (`vs=0` for v-shaped and `vs=1` for inline), transmission
    (`am=0` for automatic and `am=1` for manual), number of gears (`gear`), and number
    of carburetors (`carb`) (if not fuel injected):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In Figure 4.19, we can see the first five rows of the data set, which contains
    data suitable for Pearson's correlation analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '| **mpg** | **cyl** | **disp** | **hp** | **drat** | **wt** | **qsec** | **Vs**
    | **am** | **gear** | **carb** |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 6 | 160 | 110 | 3.9 | 2.62 | 16.46 | 0 | 1 | 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 21 | 6 | 160 | 110 | 3.9 | 2.875 | 17.02 | 0 | 1 | 4 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 22.8 | 4 | 108 | 93 | 3.85 | 2.32 | 18.61 | 1 | 1 | 4 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 21.4 | 6 | 258 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 18.7 | 8 | 360 | 175 | 3.15 | 3.44 | 17.02 | 0 | 0 | 3 | 2 |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.19 – first five rows from the mtcars dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the dataset, we can plot a correlation matrix with the following code,
    which shows each pairwise correlation for all features in the dataset to see how
    they relate to one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose we are curious about the variables most meaningful for quarter-mile
    time (`qsec`). In *Figure 4**.20*, we can see by looking at the line for `qsec`
    that `vs` (v-shaped) is positively correlated at 0.74\. Since this is a binary
    variable, we can assume, based on this dataset, that inline engines are faster
    than v-shaped engines. However, there are other covariates involved with significant
    correlation. For example, almost as strongly correlated with speed as engine shape
    is horsepower, such that as horsepower goes up, quarter-mile runtime goes down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – Correlation matrix heatmap](img/B18945_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Correlation matrix heatmap
  prefs: []
  type: TYPE_NORMAL
- en: A correlation matrix is useful for exploring the relationships between multiple
    variables at one time. It is also a useful tool for feature selection when building
    statistical and **machine learning** (**ML**) models, such as linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Power analysis examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Power analysis is a statistical method for identifying an appropriate sample
    size required for a hypothesis test to have sufficient power in preventing *Type
    II* errors – or failing to reject the null hypothesis when the null hypothesis
    should be rejected. Power analysis can also be used for identifying, based on
    sample size, a detectable effect size (or difference) between samples tested.
    In other words, based on a specific sample size and distribution, a power analysis
    can provide the analyst with a specific minimum difference the researcher may
    be able to reliably identify with a given test. In this section, we will demonstrate
    a power analysis using a one-sample t-test.
  prefs: []
  type: TYPE_NORMAL
- en: One-sample t-test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s assume a manufacturer sells a type of machine capable of producing 100,000
    units per month with a standard deviation of 2,800 units. A company has bought
    a number of these machines and has found them to only be producing 90,000 units.
    The company wants to know how many of the machines are needed to determine with
    a high level of confidence the machines are not capable of producing 100,000 units.
    The following power analysis indicates that for a t-test, a sample of three machines
    is required to prevent, with an 85% probability, failing to identify a statistically
    significant difference in actual versus marketed machine performance when there
    is one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Additional power analysis example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To see additional examples of power analysis in Python, please refer to this
    book’s GitHub repository. There, we have examples for additional t-tests and F-tests,
    which focus on analyzing variance between sample groups.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered topics of parametric tests. Starting with the assumptions
    of parametric tests, we identified and applied methods for testing the violation
    of these assumptions and discussed scenarios where robustness can be assumed when
    the required assumptions are not met. We then looked at one of the most popular
    alternatives to the z-test, the t-test. We iterated through multiple applications
    of this test, covering one-sample and two-sample versions of this test using pooling,
    pairing, and Welch’s non-pooled version of the two-sample analysis. Next, we explored
    ANOVA techniques, where we looked at using data from multiple groups to identify
    statistically significant differences between them. This included one of the most
    popular adjustments to the p-value for when a high volume of groups is present—the
    Bonferroni correction, which helps prevent inflating the *Type I* error when performing
    multiple tests. We then looked at performing correlation analysis on continuous
    data using Pearson’s correlation coefficient and how to visualize correlation
    using a correlation matrix and accompanying heatmap. Finally, we briefly overviewed
    power analysis, with an example of performing this with a one-sample t-test. In
    the next chapter, we will discuss non-parametric hypothesis testing, including
    new tests in addition to those that pair with the parametric tests in this chapter
    for when assumptions cannot be safely assumed.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] *Dua, D.* and *Graff, C.* (*2019*). *UCI Machine Learning Repository* [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    *Irvine, CA: University of California, School of Information and* *Computer Science*.'
  prefs: []
  type: TYPE_NORMAL
