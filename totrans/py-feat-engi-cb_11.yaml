- en: <st c="0">11</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Extracting Features from Text Variables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="43">Text can be one of the variables in our datasets.</st> <st c="94">For
    example, in insurance, information describing the circumstances of an incident
    can come from free text fields in a form.</st> <st c="219">If a company gathers
    customer reviews, this information will be collected as short pieces of text provided
    by the users.</st> <st c="340">Text data does not show</st> <st c="363">the</st>
    **<st c="368">tabular</st>** <st c="375">pattern of the datasets that we have
    worked with throughout this book.</st> <st c="447">Instead, information in texts
    can vary in length and content, as well as writing style.</st> <st c="535">We
    can extract a lot of information from text variables to use as predictive features
    in machine learning models.</st> <st c="649">The techniques we will cover in this
    chapter belong to the realm of</st> **<st c="717">Natural Language Processing</st>**
    <st c="744">(</st>**<st c="746">NLP</st>**<st c="749">).</st> <st c="753">NLP
    is a subfield of linguistics and computer science.</st> <st c="808">It is</st>
    <st c="813">concerned with the interactions between computer and human language,
    or, in other words, how to program computers to understand human language.</st>
    <st c="958">NLP includes a multitude of techniques to understand the syntax, semantics,
    and discourse of text.</st> <st c="1057">Therefore, to do this field justice would
    require an</st> <st c="1110">entire book.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1122">In this chapter, we will discuss the methods that will allow us
    to quickly extract features from short pieces of text to complement our predictive
    models.</st> <st c="1278">Specifically, we will discuss how to capture a piece
    of text’s complexity by looking at some statistical parameters of the text, such
    as the word length and count, the number of words and unique words used, the number
    of sentences, and so on.</st> <st c="1521">We will use the</st> `<st c="1537">pandas</st>`
    <st c="1543">and</st> `<st c="1548">scikit-learn</st>` <st c="1560">libraries,
    and we will make a shallow dive into a very useful Python NLP toolkit</st> <st
    c="1641">called the</st> **<st c="1653">Natural Language</st>** **<st c="1670">Toolkit</st>**
    <st c="1677">(</st>**<st c="1679">NLTK</st>**<st c="1683">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1686">This chapter includes the</st> <st c="1713">following recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1731">Counting characters, words,</st> <st c="1760">and vocabulary</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1774">Estimating text complexity by</st> <st c="1805">counting sentences</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1823">Creating features with bag-of-words</st> <st c="1860">and n-grams</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1871">Implementing term frequency-inverse</st> <st c="1908">document
    frequency</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1926">Cleaning and stemming</st> <st c="1949">text variabl</st><st c="1961">es</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1964">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1987">In this chapter, we will use the</st> `<st c="2021">pandas</st>`<st
    c="2027">,</st> `<st c="2029">matplotlib</st>`<st c="2039">, and</st> `<st c="2045">scikit-learn</st>`
    <st c="2057">Python libraries.</st> <st c="2076">We will also use</st> `<st c="2093">NLTK</st>`<st
    c="2097">, a comprehensive Python library for NLP and text analysis.</st> <st
    c="2157">You can find the instructions to install</st> `<st c="2198">NLTK</st>`
    <st c="2202">at</st> [<st c="2206">http://www.nltk.org/install.html</st>](http://www.nltk.org/install.html)<st
    c="2238">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2239">If you are using the Python Anaconda distribution, follow the instructions
    to install</st> `<st c="2326">NLTK</st>` <st c="2330">at</st> [<st c="2334">https://anaconda.org/anaconda/nltk</st>](https://anaconda.org/anaconda/nltk)<st
    c="2368">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2369">After you have installed</st> `<st c="2395">NLTK</st>`<st c="2399">,
    open up a Python console and execute</st> <st c="2438">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: <st c="2514">These commands will download the necessary data for you to be able
    to run the recipes in this</st> <st c="2609">chapter successfully.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2630">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2635">If you haven’t downloaded these or the other data sources necessary
    for</st> `<st c="2708">NLTK</st>` <st c="2712">functionality,</st> `<st c="2728">NLTK</st>`
    <st c="2732">will raise an error.</st> <st c="2754">Read the error message carefully
    because it will direct you to download the data required to run the command that
    you are trying</st> <st c="2883">to execut</st><st c="2892">e.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2895">Counting characters, words, and vocabulary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2938">One of the</st> <st c="2950">sal</st><st c="2953">ient</st> <st
    c="2958">characteri</st><st c="2969">stics of text i</st><st c="2985">s its</st>
    <st c="2992">complexity.</st> <st c="3004">Long descriptions are more likely to
    contain more information than short descriptions.</st> <st c="3091">Texts rich
    in different, unique words are more likely to be richer in detail than texts that
    repeat the same words over and over.</st> <st c="3221">In the same way, when we
    speak, we use many short words such as articles and prepositions to build the
    sentence structure, yet the main concept is often derived from the nouns and adjectives
    we use, which tend to be longer words.</st> <st c="3451">So, as you can see, even
    without reading the text, we can start inferring how much information the text
    provides by determining the number of words, the number of unique words (non-repeated
    occurrences of a word), the lexical diversity, and the length of those words.</st>
    <st c="3719">In this recipe, we will learn how to extract these features from
    a text variable</st> <st c="3800">using</st> `<st c="3806">pa</st><st c="3808">ndas</st>`<st
    c="3813">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3814">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="3828">We are</st> <st c="3835">going to use the</st> `<st c="3892">scikit-learn</st>`<st
    c="3904">, which comprises around 18,000 news posts on 20 different topics.</st>
    <st c="3971">More details about this dataset can be found on the</st> <st c="4023">following
    sites:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4039">The scikit</st><st c="4050">-learn dataset</st> <st c="4066">website:</st>
    [<st c="4075">https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset</st>](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4150">The hom</st><st c="4158">e page for the 20 Newsgroup</st> <st c="4187">dataset:</st>
    [<st c="4196">http://qwone.com/~jason/20Newsgroups/</st>](http://qwone.com/~jason/20Newsgroups/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4233">Before jumping into the recipe, let’s discuss the features that
    we are going to derive from these</st> <st c="4332">text</st> <st c="4337">pieces.</st>
    <st c="4345">We mentioned that longer descriptions, more</st> <st c="4389">words
    in the article, a greater variety of unique words, and longer words tend to correlate
    with the amount of information that the article provides.</st> <st c="4539">Hence,
    we can capture text complexity by extracting the following information about</st>
    <st c="4623">the text:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4632">The total number</st> <st c="4650">of characters</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4663">The total number</st> <st c="4681">of words</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4689">The total number of</st> <st c="4710">unique words</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4722">Lexical diversity (total number of words divided by number of</st>
    <st c="4785">unique words)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4798">Word average length (number of characters divided by number</st>
    <st c="4859">of words)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4868">In this recipe, we will extract these numerical features using</st>
    `<st c="4932">pandas</st>`<st c="4938">, which has extensive string processing
    functionalities that can be accessed via the</st> `<st c="5023">str</st>` <st
    c="5026">vectorized string functions</st> <st c="5055">for</st> <st c="5058">series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5066">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5082">Let’s begin by loading</st> `<st c="5106">pandas</st>` <st c="5112">and
    getting the</st> <st c="5129">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5143">Load</st> `<st c="5149">pandas</st>` <st c="5155">and the dataset</st>
    <st c="5172">from</st> `<st c="5177">scikit-learn</st>`<st c="5189">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5259">L</st><st c="5261">et’s load</st> <st c="5271">the train set part
    of the 20 Newsgrou</st><st c="5308">p dataset into a</st> `<st c="5326">pandas</st>`
    <st c="5332">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5432">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5436">You can print an example of a text from the DataFrame by executing</st>
    `<st c="5504">print(df['text'][1])</st>`<st c="5524">. Change the number between</st>
    `<st c="5552">[</st>` <st c="5553">and</st> `<st c="5558">]</st>` <st c="5559">to
    display different texts.</st> <st c="5588">Note how every text description is
    a single string composed of letters, numbers, punctuation, and spaces.</st> <st
    c="5694">You can check the datatype by</st> <st c="5724">executing</st> `<st c="5734">type(df["text"][1])</st>`<st
    c="5753">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5754">Now</st> <st c="5759">that</st> <st c="5764">we</st> <st c="5767">have
    the text variable in a</st> `<st c="5795">pandas</st>` <st c="5801">DataFrame,
    we are ready to extract</st> <st c="5837">the features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5850">Let’s capture the number of characters in each text piece in a</st>
    <st c="5914">new column:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="5963">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5967">You can remove trailing white spaces in a string, including those
    from new lines, before counting the number of characters by adding the</st> `<st
    c="6105">strip()</st>` <st c="6112">method before the</st> `<st c="6131">len()</st>`
    <st c="6136">method, as shown here:</st> `<st c="6160">df['num_char'] =</st>`
    `<st c="6177">df['text'].str.strip().str.len()</st>`<st c="6209">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6210">Let’s capture the number of words in each text in a</st> <st c="6263">new
    column:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6325">To count words, we use the</st> `<st c="6353">pandas</st>` <st
    c="6359">library’s</st> `<st c="6370">split()</st>` <st c="6377">method, which
    splits a text at white spaces.</st> <st c="6423">Check out the output of</st>
    `<st c="6447">split()</st>` <st c="6454">by executing, for instance,</st> `<st
    c="6483">df["text"].loc[1].split()</st>` <st c="6508">to separate the words of
    the second text of</st> <st c="6553">the DataFrame.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="6567">Let’s capture the number of</st> *<st c="6596">unique</st>* <st
    c="6602">words in each text in a</st> <st c="6627">new column:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="6711">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6716">Python interprets the same word as two different words if one has
    a capital letter.</st> <st c="6801">To avoid this behavior, we can apply the</st>
    `<st c="6842">lower()</st>` <st c="6849">method before the</st> `<st c="6868">split()</st>`
    <st c="6875">method.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6883">Let’s create</st> <st c="6896">a</st> <st c="6899">feature that
    captures</st> <st c="6921">the lexical diversity – that is, t</st><st c="6955">he
    total number of words (</st>*<st c="6982">step</st> <st c="6987">4</st>*<st c="6989">)
    compare</st><st c="6998">d to the number of unique words (</st>*<st c="7032">step
    5</st>*<st c="7039">):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7096">Let’s calculate the average word length by dividing the number
    of characters (</st>*<st c="7175">step 3</st>*<st c="7182">) by the number of
    words (</st>*<st c="7209">step 4</st>*<st c="7216">):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="7277">If we execute</st> `<st c="7292">df.head()</st>`<st c="7301">,
    we will see the first five rows of data with the text and the newly</st> <st c="7371">crea</st><st
    c="7375">ted features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics](img/B22396_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7855">Figure 11.1 – A DataFrame with the text variable and features that
    summarize some of the text’s characteristics</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7966">With that, we have extracted five different features that capture
    the text complexity, which we can use</st> <st c="8071">as</st> <st c="8074">inputs
    for our machine</st> <st c="8097">learning</st> <st c="8106">algorithms.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8117">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8122">In this recipe, we created new features from the raw data straight
    away without doing any data cleaning, removing punctuation, or even stemming words.</st>
    <st c="8274">Note that these are steps that are performed ahead of most standard
    NLP procedures.</st> <st c="8358">To learn more about this, visit the</st> *<st
    c="8394">Cleaning and stemming text variables</st>* <st c="8430">recipe at the
    end</st> <st c="8449">of</st> <st c="8452">this chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8465">How it</st> <st c="8472">works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8481">In this recipe, we created</st> <st c="8509">five new features
    that captur</st><st c="8538">e text complexity by utilizing pandas’</st> `<st
    c="8578">str</st>` <st c="8581">to access the built-in</st> `<st c="8605">pandas</st>`
    <st c="8611">functionality to work with strings.</st> <st c="8648">We worked with
    the text column of the</st> `<st c="8686">train</st>` <st c="8691">subset of the
    20 Newsgroup dataset that comes with</st> `<st c="8743">scikit-learn</st>`<st
    c="8755">. Each row in this dataset is composed of a string</st> <st c="8806">with
    text.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8816">We used pandas’</st> `<st c="8833">str</st>`<st c="8836">, followed
    by</st> `<st c="8850">len()</st>`<st c="8855">, to count the number of characters
    in each string – that is, the total number of letters, numbers, symbols, and spaces.</st>
    <st c="8976">We also combined</st> `<st c="8993">str.len()</st>` <st c="9002">with</st>
    `<st c="9008">str.strip()</st>` <st c="9019">to remove trailing white spaces at
    the beginning and end of the string and in new lines, before counting the number</st>
    <st c="9136">of characters.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9150">To count the number of words, we used pandas’</st> `<st c="9197">str</st>`<st
    c="9200">, followed by</st> `<st c="9214">split()</st>`<st c="9221">, to divide
    the string into a list of words.</st> <st c="9266">The</st> `<st c="9270">split()</st>`
    <st c="9277">method creates a list of words by breaking the string at the white
    spaces between words.</st> <st c="9367">Next, we counted those words with</st>
    `<st c="9401">str.len()</st>`<st c="9410">, obtaining the number of words</st>
    <st c="9442">per string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9453">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9458">We can change the behavior of</st> `<st c="9489">str.split()</st>`
    <st c="9500">by passing a string or character that we would like to use to split
    the string.</st> <st c="9581">For example,</st> `<st c="9594">df['text'].str.split(';')</st>`
    <st c="9619">divides a string at each occurrence</st> <st c="9656">of</st> `<st
    c="9659">;</st>`<st c="9660">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9661">To determine the number of unique words, we used pandas’</st> `<st
    c="9719">str.split()</st>` <st c="9730">function to divide the string into a list
    of words.</st> <st c="9783">Next, we applied the built-in Python</st> `<st c="9820">set()</st>`
    <st c="9825">method within pandas’</st> `<st c="9848">apply()</st>` <st c="9855">to
    return a set of words.</st> <st c="9882">Remember that a set contains</st> *<st
    c="9911">unique occurrences</st>* <st c="9929">of the elements in a list – that
    is, unique words.</st> <st c="9981">Next, we counted those words with pandas’</st>
    `<st c="10023">str.len()</st>` <st c="10032">function to return the</st> `<st
    c="10253">lower()</st>` <st c="10260">function to set all the characters to lowercase
    before splitting the string and counting the number of</st> <st c="10364">unique
    words.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10377">To create the</st> <st c="10391">lexical</st> <st c="10400">diversity
    and average word length</st> <st c="10434">features,</st> <st c="10444">we</st>
    <st c="10446">simply pe</st><st c="10456">rformed a vectorized di</st><st c="10480">vision
    of two</st> `<st c="10495">pandas</st>` <st c="10501">s</st><st c="10503">eries.</st>
    <st c="10510">That’s it; we created five new features with information about the
    comp</st><st c="10581">lexity of</st> <st c="10592">the text.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10601">There’s more...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="10617">We can check out the distribution of the features extracted from
    text in each of the 20 different news topics present in the dataset by</st> <st
    c="10754">using visualizations</st><st c="10774">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10775">To make histogram plots of the newly created features, after you
    run all of the steps in the</st> *<st c="10869">How it works...</st>* <st c="10884">section
    of this recipe, follow</st> <st c="10916">these steps:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10928">Import</st> `<st c="10936">matplotlib</st>`<st c="10946">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10980">Add the target with the news topics to the 20</st> <st c="11027">Newsgroup
    DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11074">Create a function that displays a histogram of a feature of your
    choice for each of the</st> <st c="11163">news topics:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11518">Run the function</st> <st c="11536">for</st> <st c="11540">the
    number of</st> <st c="11554">words feature:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11599">The previous command returns the following plot, where you can
    see the distribution of the number of words in each of the 20 news topics, numbered
    from 0 to</st> <st c="11757">19 in the</st> <st c="11767">plot title:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Histograms showing the distribution of the number of words
    per text, segregated by topic discussed in each text](img/B22396_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12760">Figure 11.2 – Histograms showing the distribution of the number
    of words per text, segregated by topic discussed in each text</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12885">The number of</st> <st c="12900">words</st> <st c="12905">shows
    a different</st> <st c="12924">distribution across the different news topics</st><st
    c="12969">. Therefore, this feature is likely useful in a classification algorithm
    to predic</st><st c="13051">t the topic of</st> <st c="13067">the text.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13076">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="13085">To learn more about pandas’ b</st><st c="13115">uilt-in string
    processing functionality</st> <st c="13156">visit</st> [<st c="13162">https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary</st>](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary)<st
    c="13242">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13243">Estimating text complexity by counting senten</st><st c="13289">ces</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="13293">One aspect of a</st> <st c="13309">piece of text that</st> <st
    c="13329">we can capture in features is its complexity.</st> <st c="13375">Usually,
    longer descriptions that contain multiple sentences spread over several paragraphs
    tend to provide more information than descriptions with very few sentences.</st>
    <st c="13543">Therefore, capturing the number of sentences may provide some insight
    into the amou</st><st c="13626">nt of information provided by the text.</st> <st
    c="13667">This process is</st> <st c="13682">called</st> `<st c="14027">NLTK</st>`
    <st c="14031">Python library, which pr</st><st c="14056">ovides</st> <st c="14064">this
    functionality.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14083">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14097">In this recipe, we will use the</st> `<st c="14130">NLTK</st>`
    <st c="14134">Python library.</st> <st c="14151">For guidelines on how to install</st>
    `<st c="14184">NLTK</st>`<st c="14188">, check out the</st> *<st c="14204">Technical
    requirement</st><st c="14225">s</st>* <st c="14227">section of</st> <st c="14239">this
    chapter.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14252">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14268">Let’s begin by importing the required libraries</st> <st c="14317">and
    dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14329">Let’s load</st> `<st c="14341">pandas</st>`<st c="14347">, the
    sentence tokenizer from</st> `<st c="14377">NLTK</st>`<st c="14381">, and the
    dataset</st> <st c="14399">from</st> `<st c="14404">scikit-learn</st>`<st c="14416">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14526">To understand the functionality of the sentence tokenizer from</st>
    `<st c="14590">NLTK</st>`<st c="14594">, let’s create a variable that contains
    a string with</st> <st c="14648">multiple sentences:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="15148">Now, let’s</st> <st c="15160">separate the string from</st> *<st
    c="15185">step 2</st>* <st c="15191">into sentences using</st> `<st c="15213">NLTK</st>`
    <st c="15217">library‘s</st> <st c="15228">sentence tokenizer:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="15267">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15271">If you encounter an error in</st> *<st c="15301">step 3</st>*<st
    c="15307">, read the error message carefully and download the data source required
    by</st> `<st c="15383">NLTK</st>`<st c="15387">, as described in the error message.</st>
    <st c="15424">For more details, check out the</st> *<st c="15456">Technical</st>*
    *<st c="15466">requirements</st>* <st c="15478">section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15487">The sentence tokenizer returns the list of sentences shown in
    the</st> <st c="15554">following output:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: <st c="16066">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16071">The escape character followed by the letter,</st> `<st c="16117">\n</st>`<st
    c="16119">, indic</st><st c="16126">ates a</st> <st c="16134">new line.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16143">Let’s c</st><st c="16151">ount the number of sentences in the</st>
    `<st c="16188">text</st>` <st c="16192">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16227">The previous command returns</st> `<st c="16257">8</st>`<st c="16258">,
    which is the number of sentences in our</st> `<st c="16300">text</st>` <st c="16304">variable.</st>
    <st c="16315">Now, let’s determine the number of sentences in an</st> <st c="16366">entire
    DataFrame.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="16383">Let’s load</st> <st c="16395">the</st> `<st c="16399">train</st>`
    <st c="16404">subset of the 20 Newsgroup dataset into a</st> `<st c="16447">pandas</st>`
    <st c="16453">DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16553">To speed up the following steps, we will only work with the first</st>
    `<st c="16620">10</st>` <st c="16622">rows of</st> <st c="16631">the DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16663">Let’s also remove the first part of the text, which contains information
    about the email sender, subject, and other details that we are not interested
    in.</st> <st c="16819">Most of this information comes before the word</st> `<st
    c="16866">Lines</st>` <st c="16871">followed by</st> `<st c="16884">:</st>`<st
    c="16885">, so let’s split the string at</st> `<st c="16916">Lines:</st>` <st
    c="16922">and capture the second part of</st> <st c="16954">the string:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17032">Fin</st><st c="17036">ally, let’s create a v</st><st c="17059">ariable
    containing the number of sentences</st> <st c="17103">per</st> `<st c="17107">text</st>`<st
    c="17111">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17174">With the</st> `<st c="17184">df</st>` <st c="17186">command, you
    can display the entire DataFrame with the</st> `<st c="17242">text</st>` <st c="17246">variable
    and the new feature containing the nu</st><st c="17293">mber of sentences</st>
    <st c="17312">per text:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text](img/B22396_11_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="17841">Figure 11.3 – A DataFrame with the text variable and the number
    of sentences per text</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17926">Now, we can use this</st> <st c="17947">new feature as input to</st>
    <st c="17972">machine</st> <st c="17980">learning algorithms.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18000">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18016">In this recipe, we separated a string with text into sentences
    using</st> `<st c="18086">sent_tokenizer</st>` <st c="18100">from the</st> `<st
    c="18110">NLTK</st>` <st c="18114">library.</st> `<st c="18124">sent_tokenizer</st>`
    <st c="18138">has been pre-trained to recognize capitalization and different types
    of punctuation that signal the beginning and the end of</st> <st c="18264">a sentence.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18275">First, we applied</st> `<st c="18294">sent_tokenizer</st>` <st
    c="18308">to a manually created string to become familiar with its functionality.</st>
    <st c="18381">The tokenizer divided the text into a list of eight sentences.</st>
    <st c="18444">We combined the tokenizer with the built-in Python</st> `<st c="18495">len()</st>`
    <st c="18500">method to count the number of sentences in</st> <st c="18544">the
    string.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="18555">Next, we loaded a dataset with text and, to speed up the computation,
    we only retained the first 10 rows of the DataFrame using pandas’</st> `<st c="18692">loc[]</st>`
    <st c="18697">function.</st> <st c="18708">Next, we removed the first part of
    the text, which contained information about the email sender and subject.</st>
    <st c="18817">To do this, we split the string at</st> `<st c="18852">Lines:</st>`
    <st c="18858">using pandas’</st> `<st c="18873">str.split("Lines:")</st>` <st
    c="18892">function, which returned a list with two elements: the strings before
    and after</st> `<st c="18973">Lines:</st>`<st c="18979">. Utilizing a lambda function
    within</st> `<st c="19016">apply()</st>`<st c="19023">, we retained the second
    part of the text – that is, the second string in the list returned</st> <st c="19115">by</st>
    `<st c="19118">split()</st>`<st c="19125">.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19126">Finally, we applied</st> `<st c="19147">sent_tokenizer</st>` <st
    c="19161">to each row in the DataFrame with the pandas</st> `<st c="19207">apply()</st>`
    <st c="19214">method to separate the strings into sentences, and then applied
    the built-</st><st c="19289">in Python</st> `<st c="19300">len()</st>` <st c="19305">method
    to th</st><st c="19318">e list of sentences to return the number of sentences
    per string.</st> <st c="19385">This way, we</st> <st c="19398">created a new feature
    that contained the</st> <st c="19439">number of sentences</st> <st c="19459">per
    text.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19468">There’s mo</st><st c="19479">re...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="19485">NLTK</st>` <st c="19490">has functionalities</st> <st c="19511">for
    word tokenization among other useful features, which we can use instead of</st>
    `<st c="19590">pandas</st>` <st c="19596">to count and return the number of words.</st>
    <st c="19638">You can find out more about</st> `<st c="19666">NLTK</st>`<st c="19670">’s</st>
    <st c="19674">functionality here:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="19693">Python 3 Text Processing with NLTK 3 Cookbook</st>*<st c="19739">,
    by Jacob Per</st><st c="19753">kins,</st> <st c="19760">Packt Publishing</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="19776">The</st> `<st c="19781">NLTK</st>` <st c="19785">document</st><st
    c="19794">ation</st> <st c="19801">at</st> [<st c="19804">http://www.nltk.org/</st>](http://www.nltk.org/)<st
    c="19824">.</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="19825">Creating features with bag-of-words and n-grams</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="19873">A</st> **<st c="19876">Bag-of-Words</st>** <st c="19888">(</st>**<st
    c="19890">BoW</st>**<st c="19893">)</st> <st c="19895">is a</st> <st c="19901">simplified</st>
    <st c="19911">representation</st> <st c="19926">of a piece of</st> <st c="19940">text
    that</st> <st c="19951">captures the wor</st><st c="19967">ds that are present
    in the text and the number of times each word appears in the text.</st> <st c="20055">So,
    for the text string</st> *<st c="20079">Dogs like cats, but cats do not like dogs</st>*<st
    c="20120">, the derived BoW is</st> <st c="20141">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats\
    \ do not\uFEFF like dogs](img/B22396_11_04.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="20191">Figure 11.4 – The BoW derived from the sentence Dogs like cats,
    but cats do not</st> <st c="20270">like dogs</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20280">Here, each word</st> <st c="20296">becomes a variable, and th</st><st
    c="20323">e value of the varia</st><st c="20344">ble represents the number of
    times the word appears in the string.</st> <st c="20412">As you can see, the BoW
    captures multiplicity but does not retain word order or grammar.</st> <st c="20501">That
    is why it is a simple, yet useful way of extracting features and capturing some
    information about the texts we are</st> <st c="20621">working with.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20634">To capture some syntax, BoW can be used together with</st> **<st
    c="20689">n-grams</st>**<st c="20696">. An n-gram is a contiguous</st> <st c="20724">sequence
    of</st> *<st c="20736">n</st>* <st c="20737">items in a</st> <st c="20749">given</st>
    <st c="20755">text.</st> <st c="20761">Continuing</st> <st c="20771">with the
    sentence</st> *<st c="20790">Dogs like cats, but cats do not like dogs</st>*<st
    c="20831">, the derived 2-grams are</st> <st c="20857">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20868">Dogs like</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20878">like cats</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20888">cats but</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20897">but do</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20904">do not</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20911">like dogs</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="20921">We can create, together with a BoW, a bag of n-grams, where the
    additional variables are given by the 2-grams and the values for each 2-gram are
    the number of times they appear in each string; for this example, the value is
    1\.</st> <st c="21149">So, our final BoW with 2-grams would look</st> <st c="21191">like
    this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – The BoW with 2-grams](img/B22396_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="21305">Figure 11.5 – The BoW with 2-grams</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21339">In this recipe, we will learn how to create BoWs with or</st>
    <st c="21396">without n-grams</st> <st c="21413">using</st> `<st c="21419">sciki</st><st
    c="21424">t-learn</st>`<st c="21432">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21433">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="21447">Before jumping into this</st> <st c="21472">recipe, let’s get
    familiar with some o</st><st c="21511">f the parameters of a B</st><st c="21535">oW
    that we can adjust to make the BoW comprehensive.</st> <st c="21589">When creating
    a BoW over several pieces of text, a new feature is created for each unique word
    that appears at least once in</st> *<st c="21714">any</st>* <st c="21717">of the
    text pieces we are analyzing.</st> <st c="21755">If the word appears only in one
    piece of text, it will show a value of 1 for that particular text and 0 for all
    of the others.</st> <st c="21882">Therefore, BoWs tend to be sparse matrices,
    where most of the values</st> <st c="21951">are zeros.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21961">The number of columns – that is, the number of words – in a BoW
    can be quite large if we work with huge text corpora, and even larger if we also
    include n-grams.</st> <st c="22124">To limit the number of columns and the sparsity
    of the returned matrix, we can retain words that appear across multiple texts;
    or, in better words, we can retain words that appear in, at least, a certain percentage</st>
    <st c="22339">of texts.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22348">To reduce the number of columns and sparsity of the BoW, we should
    also work with words in the</st> <st c="22444">same</st> <st c="22448">case –
    for</st> <st c="22460">example, lowercase – as</st> <st c="22484">Python identifies
    words in a different case as different words.</st> <st c="22548">We can also reduce
    the number of columns and sparsity by</st> <st c="22605">removing</st> **<st c="22614">stop
    words</st>**<st c="22624">. Stop words are very frequently used words that make
    sentences flow, but that do not, per se, carry any useful information.</st> <st
    c="22749">Examples of stop words are pronouns such as I, you, and he, as well
    as prepositions</st> <st c="22833">and articles.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22846">In this recipe, we will learn how to set words in lowercase, remove
    stop words, retain words with a minimum acceptable frequency, and capture n-grams
    all together with a single transfor</st><st c="23032">mer from</st> `<st c="23042">scikit-learn</st>`<st
    c="23054">:</st> `<st c="23057">CountVectorizer()</st>`<st c="23074">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23075">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23091">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="23155">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23169">Load</st> `<st c="23175">pandas</st>`<st c="23181">,</st> `<st
    c="23183">CountVectorizer</st>`<st c="23198">, and the dataset</st> <st c="23216">from</st>
    `<st c="23221">scikit-learn</st>`<st c="23233">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23368">Let’s load the</st> <st c="23384">train set part</st> <st c="23398">of
    the 20 Newsgroup dataset into a</st> <st c="23434">pandas DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23540">To make interpreting the results easier, let’s remove punctuation
    and numbers from the</st> <st c="23628">text variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="23740">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23745">To learn more about regex with Python, follow this</st> <st c="23797">link:</st>
    [<st c="23803">https://docs.python.org/3/howto/regex.html</st>](https://docs.python.org/3/howto/regex.html)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23845">Now, let’s</st> <st c="23857">set up</st> `<st c="23864">CountVectorizer()</st>`
    <st c="23881">so</st> <st c="23885">that, before</st> <st c="23898">creating</st>
    <st c="23907">the BoW, it puts the text in lowercase, removes stop words, and
    retains words that appear in, at least, 5% of the</st> <st c="24021">text pieces:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24134">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24139">To introduce n-grams as part of the returned columns, we can change
    the value of</st> `<st c="24221">ngrams_range</st>` <st c="24233">to, for example,</st>
    `<st c="24251">(1,2)</st>`<st c="24256">. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams.</st> <st c="24356">In
    the case of</st> `<st c="24371">(1,2)</st>`<st c="24376">,</st> `<st c="24378">CountVectorizer()</st>`
    <st c="24395">will return single words and arrays of two</st> <st c="24439">consecutive
    words.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24457">Let’s fit</st> `<st c="24468">CountVectorizer()</st>` <st c="24485">so
    that it learns which words should be used in</st> <st c="24534">the BoW:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24569">Now, let’s create</st> <st c="24588">the BoW:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24634">Finally, let’s capture</st> <st c="24657">the BoW</st> <st c="24665">in
    a DataFrame with the corresponding</st> <st c="24704">feature names:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="24805">With</st> <st c="24811">that, we</st> <st c="24819">have</st>
    <st c="24824">created</st> <st c="24833">a</st> `<st c="24835">pandas</st>` <st
    c="24841">DataFrame that contains words as columns and the number of times they
    appeared in each text as values.</st> <st c="24945">You can in</st><st c="24955">spect
    the result by</st> <st c="24976">executing</st> `<st c="24986">bagofwords.head()</st>`<st
    c="25003">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset](img/B22396_11_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="25367">Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup
    dataset</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25445">We can use</st> <st c="25456">this BoW as input for a machine</st>
    <st c="25489">learning model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25504">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="25520">scikit-learn’s</st> `<st c="25536">CountVectorizer()</st>` <st
    c="25553">converts a collection of text documents into a matrix of token counts.</st>
    <st c="25625">These tokens can be individual words or arrays of two or more consecutive
    words – that is, n-grams.</st> <st c="25725">In this recipe, we created a B</st><st
    c="25755">oW from a text variable in</st> <st c="25783">a D</st><st c="25786">ataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25796">We loaded the 20 Newsgroup text data</st><st c="25833">set from</st>
    `<st c="25843">scikit-l</st><st c="25851">earn</st>` <st c="25856">and removed
    punctuation and numbers from the text rows using pandas’</st> `<st c="25926">replace()</st>`
    <st c="25935">function, which can be accessed through pandas’</st> `<st c="25984">str</st>`
    <st c="25987">module, to replace digits,</st> `<st c="26015">'\d+'</st>`<st c="26020">,
    or symbols,</st> `<st c="26034">'[^\w\s]'</st>`<st c="26043">, with empty strings,</st>
    `<st c="26065">''</st>`<st c="26067">. Then, we used</st> `<st c="26083">CountVectorizer()</st>`
    <st c="26100">to create the BoW.</st> <st c="26120">We set the</st> `<st c="26131">lowercase</st>`
    <st c="26140">parameter to</st> `<st c="26154">True</st>` <st c="26158">to put
    the words in lowercase before extracting the BoW.</st> <st c="26216">We set the</st>
    `<st c="26227">stop_words</st>` <st c="26237">argument to</st> `<st c="26250">english</st>`
    <st c="26257">to ignore stop words – that is, to avoid stop words in the BoW.</st>
    <st c="26322">We set</st> `<st c="26329">ngram_range</st>` <st c="26340">to the</st>
    `<st c="26348">(1,1)</st>` <st c="26353">tuple to return only single words as
    columns.</st> <st c="26400">Finally, we set</st> `<st c="26416">min_df</st>` <st
    c="26422">to</st> `<st c="26426">0.05</st>` <st c="26430">to return words that
    appeared in at least 5% of the texts, or, in other words, in 5% of the rows in</st>
    <st c="26531">the DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26545">After</st> <st c="26552">setting</st> <st c="26560">up</st> <st
    c="26562">the</st> <st c="26566">transformer, we used the</st> `<st c="26592">fit()</st>`
    <st c="26597">method to allow the transformer to find the words that fulfill the
    preceding criteria.</st> <st c="26685">Finally, using the</st> `<st c="26704">transform()</st>`
    <st c="26715">method, the transformer returned an object containing the BoW with
    its fea</st><st c="26790">ture names, which we captured in a</st> `<st c="26826">pandas</st>`
    <st c="26832">DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26843">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="26852">For more de</st><st c="26864">tails about</st> `<st c="26877">CountVectorizer()</st>`<st
    c="26894">, visit the</st> `<st c="26906">scikit-learn</st>` <st c="26918">library’s
    documentation</st> <st c="26943">at</st> [<st c="26946">https://scikit-learn.org/stable/modules/generated/s</st><st
    c="26997">klearn.feature_extraction.text.CountVectorizer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)<st
    c="27049">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27050">Implementing term frequency-inverse document frequency</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="27105">Term Frequency-Inverse Document Freq</st><st c="27142">uency</st>**
    <st c="27148">(</st>**<st c="27150">TF-IDF</st>**<st c="27156">) is a numerical
    statistic that captures</st> <st c="27198">how relevant a word is in a document
    considering the entire collection of documents.</st> <st c="27283">What does this
    mean?</st> <st c="27304">Some words will appear a lot within a text document as
    well as across documents, such as the English words</st> *<st c="27411">the</st>*<st
    c="27414">,</st> *<st c="27416">a</st>*<st c="27417">, and</st> *<st c="27423">is</st>*<st
    c="27425">, for example.</st> <st c="27440">These words generally convey little
    information about the actual content of the document and don’t make the text stand
    out from the crowd.</st> <st c="27579">TF-IDF provides a way to</st> *<st c="27604">weigh</st>*
    <st c="27609">the importance of a word by considering how many times it appears
    in a document with regards to how often it appears across documents.</st> <st
    c="27745">Hence, commonly occurring words such as</st> *<st c="27785">the</st>*<st
    c="27788">,</st> *<st c="27790">a</st>*<st c="27791">, or</st> *<st c="27796">is</st>*
    <st c="27798">will have a low weight, and words that are more specific to a topic,
    such as</st> *<st c="27876">leopard</st>*<st c="27883">, will have a</st> <st
    c="27897">higher weight.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27911">TF-IDF is the product of two</st> <st c="27941">statistics:</st>
    **<st c="27953">Term Frequency</st>** <st c="27967">(</st>**<st c="27969">tf</st>**<st
    c="27971">) and</st> **<st c="27978">Inverse Document Frequency</st>** <st c="28004">(</st>**<st
    c="28006">idf</st>**<st c="28009">), represented</st> <st c="28025">as follows:</st>
    **<st c="28037">tf-idf = td × idf</st>**<st c="28054">. tf is, in its simplest
    form, the count of the word in an individual text.</st> <st c="28130">So, for
    term</st> *<st c="28143">t</st>*<st c="28144">, the tf is calculated as</st> *<st
    c="28170">tf(t) = count(t)</st>* <st c="28186">and is determined on a text-by-text
    basis.</st> <st c="28230">The idf is a measure of how common the word is across</st>
    *<st c="28284">all</st>* <st c="28287">documents and is usually calculated on
    a logarithmic scale.</st> <st c="28348">A common implementation is given by</st>
    <st c="28384">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="28416">Here,</st> *<st c="28422">n</st>* <st c="28423">is the total number
    of documents, and</st> *<st c="28462">df(t)</st>* <st c="28467">is the number
    of documents in which the term</st> *<st c="28513">t</st>* <st c="28514">appears.</st>
    <st c="28524">The bigger the value of</st> *<st c="28548">df(t)</st>*<st c="28553">,
    the lower the weighting for the term.</st> <st c="28593">The importance of a word
    will be high if it appears a lot of times in a text (high</st> *<st c="28676">tf</st>*<st
    c="28678">) or few times a</st><st c="28695">cross texts (</st><st c="28709">high</st>
    *<st c="28715">idf</st>*<st c="28718">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28721">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28726">TF-IDF can be used together with n-grams.</st> <st c="28769">Similarly,
    to weigh an n-gram, we compound the n-gram frequency in a certain document with
    the frequency of the n-gram</st> <st c="28888">across documents.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28905">In this recipe, we will learn how to extract features u</st><st
    c="28961">sing TF-IDF with or without n-grams</st> <st c="28998">using</st> `<st
    c="29004">scikit-learn</st>`<st c="29016">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29017">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="29031">scikit-learn</st>` <st c="29044">uses a slightly different way
    to calculate the</st> <st c="29092">IDF statistic:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
  prefs: []
  type: TYPE_IMG
- en: <st c="29134">This formulation ensures that a word that appears in all texts
    receives the lowest weight of 1\.</st> <st c="29230">In addition, after calculating
    the TF-IDF for every word,</st> `<st c="29288">scikit-learn</st>` <st c="29300">normalizes
    the feature vector (that wit</st><st c="29340">h all the words) to its Euclidean
    norm.</st> <st c="29381">For more details on the exact formula, visit the</st>
    `<st c="29430">scikit-learn</st>` <st c="29442">documentation</st> <st c="29457">at</st>
    [<st c="29460">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</st>](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)<st
    c="29544">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29545">TF-IDF shares the characteristics of BoW when creating the term
    matrix – that is, high feature space and sparsity.</st> <st c="29661">To reduce
    the number of features and sparsity, we can remove stop words, set the characters
    to lowercase, and retain words that appear in a minimum percentage</st> <st c="29819">of
    observations.</st> <st c="29837">If you are unfamiliar with these terms, visit
    the</st> *<st c="29887">Creating features with bag-of-words and n-grams</st>*
    <st c="29934">recipe in this chapter for</st> <st c="29962">a recap.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29970">In this recipe, we will learn how to set words into lowercase,
    remove stop words, retain words with a minimum acceptable frequency, capture n-grams,
    and then return the TF-IDF statistic of words, all using a</st> <st c="30179">single
    transformer from</st> <st c="30203">scikit-learn:</st> `<st c="30217">TfidfVectorizer()</st>`<st
    c="30234">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30235">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30251">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="30315">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30329">Load</st> `<st c="30335">pandas</st>`<st c="30341">,</st> `<st
    c="30343">TfidfVectorizer()</st>`<st c="30360">, and the dataset</st> <st c="30378">from</st>
    `<st c="30383">scikit-learn</st>`<st c="30395">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30530">Let’s load the train set part of the 20 Newsgroup dataset into
    a</st> <st c="30596">pandas DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30702">To make interpreting the results easier, let’s remove punctuation
    and numbers from the</st> <st c="30790">text variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="30902">Now, let’s set up</st> `<st c="30921">TfidfVectorize</st><st c="30935">r()</st>`
    <st c="30939">from</st> `<st c="30945">scikit-learn</st>` <st c="30957">so that,
    before creating the TF-IDF metrics, it puts all text in lowercase, removes stop
    words, and</st> <st c="31058">retains words that appear in at least 5% of the</st>
    <st c="31106">text pieces:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31219">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31224">To introduce n-grams as part of the returned columns, we can change
    the value of</st> `<st c="31306">ngrams_range</st>` <st c="31318">to, for example,</st>
    `<st c="31336">(1,2)</st>`<st c="31341">. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams.</st> <st c="31441">In
    the case of</st> `<st c="31456">(1,2)</st>`<st c="31461">,</st> `<st c="31463">TfidfVectorizer()</st>`
    <st c="31480">will return single words and arrays of two consecutive words</st>
    <st c="31542">as columns.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31553">Let’s fit</st> `<st c="31564">TfidfVectorizer()</st>` <st c="31581">so
    that it learns which words should be introduced as columns of the TF-IDF matrix
    and determines the</st> <st c="31684">words’</st> `<st c="31691">idf</st>`<st
    c="31694">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31723">Now, let’s create the</st> <st c="31746">TF-IDF matrix:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31797">Finally, let’s capture the TF-IDF matrix in a DataFrame with the
    corresponding</st> <st c="31877">feature names:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="31973">With that, we</st> <st c="31988">have created a</st> `<st c="32003">pandas</st>`
    <st c="32009">DataFrame that contains words as columns and the TF-IDF as valu</st><st
    c="32073">es.</st> <st c="32078">You can inspect the result by</st> <st c="32108">executing</st>
    `<st c="32118">tfidf.head()</st>`<st c="32130">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 – A DataFrame with features resulting from TF-IDF](img/B22396_11_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="32918">Figure 11.7 – A DataFrame with features resulting from TF-IDF</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32979">Now, we can use this t</st><st c="33002">erm frequency DataFrame
    to train machine</st> <st c="33044">lea</st><st c="33047">rning models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33061">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33077">In this recipe, we extracted the TF-IDF values of words present
    in at least 5% of the documents by utilizing</st> `<st c="33187">TfidfVectorizer()</st>`
    <st c="33204">from scikit-learn.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33223">We loaded the 20 Newsgroup text dataset from</st> `<st c="33269">scikit-learn</st>`
    <st c="33281">and then removed punctuation and numbers from the text rows using
    pandas’</st> `<st c="33356">replace()</st>`<st c="33365">, which can be accessed
    through pandas’</st> `<st c="33405">str</st>`<st c="33408">, to replace digits,</st>
    `<st c="33429">'\d+'</st>`<st c="33434">, or symbols,</st> `<st c="33448">'[^\w\s]'</st>`<st
    c="33457">, with empty strings,</st> `<st c="33479">''</st>`<st c="33481">. Then,
    we used</st> `<st c="33497">TfidfVectorizer()</st>` <st c="33514">to create TF-IDF
    statistics for words.</st> <st c="33554">We set the</st> `<st c="33565">lowercase</st>`
    <st c="33574">parameter to</st> `<st c="33588">True</st>` <st c="33592">to put
    words into lowercase before making the calculations.</st> <st c="33653">We set
    the</st> `<st c="33664">stop_words</st>` <st c="33674">argument to</st> `<st c="33687">english</st>`
    <st c="33694">to avoid stop words in the returned matrix.</st> <st c="33739">We
    set</st> `<st c="33746">ngram_range</st>` <st c="33757">to the</st> `<st c="33765">(1,1)</st>`
    <st c="33770">tuple to return single words as features.</st> <st c="33813">Finally,
    we set the</st> `<st c="33833">min_df</st>` <st c="33839">argument to</st> `<st
    c="33852">0.05</st>` <st c="33856">to return words that appear at least in 5%
    of the texts or, in other words, in 5% of</st> <st c="33942">the rows.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33951">After setting up the transformer, we applied the</st> `<st c="34001">fi</st><st
    c="34003">t()</st>` <st c="34007">method to let the transformer find the words
    to</st> <st c="34055">retain in the final term matrix.</st> <st c="34089">With
    the</st> `<st c="34098">transform()</st>` <st c="34109">method, the transformer
    returned an object with the words and their TF-IDF values, which we then captured
    in a pandas DataFrame with the appropriate feature names.</st> <st c="34274">We</st>
    <st c="34277">can now use these features in machine</st> <st c="34315">learning
    algorithms.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34335">See als</st><st c="34343">o</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34345">For more details</st> <st c="34361">on</st> `<st c="34365">TfidfVectorizer()</st>`<st
    c="34382">, visit scikit-learn’s</st> <st c="34405">documentation:</st> [<st c="34420">https://scikit-learn.org/stable/modules/gen</st><st
    c="34463">erated/sklearn.feature_extraction.text.TfidfVectori</st><st c="34515">zer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34524">Cleani</st><st c="34531">ng and stemming text variables</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="34562">Some variables in our</st> <st c="34584">dataset come from free
    text fields, which are manually</st> <st c="34640">completed by users.</st> <st
    c="34660">People have different writing styles, and we use a variety of punctuation
    marks, capitalization patterns, and verb conjugations to convey the content, as
    well as the emotions surrounding it.</st> <st c="34851">We can extract (some)
    information from text without taking the trouble to read it by creating statistical
    parameters that summarize the text’s complexity, keywords, and relevance of words
    in a document.</st> <st c="35054">We discussed these methods in the previous recipes
    of this chapter.</st> <st c="35122">However, to derive these statistics and aggregated
    features, we should clean the</st> <st c="35202">text</st> <st c="35208">variables
    first.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35224">Text cleaning or preprocessing involves punctuation removal, stop
    word elimination, character case setting, and word stemming.</st> <st c="35352">Punctuation
    removal consists of deleting characters that are not letters, numbers, or spaces;
    in some cases, we also remove numbers.</st> <st c="35485">The elimination of stop
    words refers to removing common words that are used in our language to allow for
    the sentence structure and flow, but that individually convey little or no information.</st>
    <st c="35678">Examples of stop words include articles such as</st> *<st c="35726">the</st>*
    <st c="35729">and</st> *<st c="35734">a</st>* <st c="35735">for the English language,
    as well as pronouns such as</st> *<st c="35790">I</st>*<st c="35791">,</st> *<st
    c="35793">you</st>* <st c="35796">and</st> *<st c="35801">they</st>*<st c="35805">,
    and commonly used verbs in their various conjugations, such as the verbs</st>
    *<st c="35880">to be</st>* <st c="35885">and</st> *<st c="35890">to have</st>*<st
    c="35897">, as well as the auxiliary verbs</st> *<st c="35930">would</st>* <st
    c="35935">and</st> *<st c="35940">do</st>*<st c="35942">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35943">To allow computers to identify words correctly, it is also necessary
    to set all the words in the same case, since the words</st> *<st c="36068">Toy</st>*
    <st c="36071">and</st> *<st c="36076">toy</st>* <st c="36079">would be identified
    as being different by a computer due to the uppercase</st> *<st c="36154">T</st>*
    <st c="36155">in the</st> <st c="36163">first one.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36173">Finally, to focus on the</st> *<st c="36199">message</st>* <st
    c="36206">of the text, we don’t want computers to consider words differently if
    they show different conjugations.</st> <st c="36311">Hence, we would use word
    stemming as part of the preprocessing pipeline.</st> <st c="36384">Word stemming
    refers to reducing each word to its root or base so that the words</st> *<st c="36465">playing</st>*<st
    c="36472">,</st> *<st c="36474">plays</st>*<st c="36479">, and</st> *<st c="36485">played</st>*
    <st c="36491">become</st> *<st c="36499">play</st>*<st c="36503">, which, in essence,
    conveys the same or very</st> <st c="36549">similar meaning.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36565">In this recipe, we will learn how to remove punctuation and stop
    words, set words</st> <st c="36648">in lowercase, and perform word stemming with
    pandas</st> <st c="36700">and</st> `<st c="36704">NLTK</st>`<st c="36708">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36709">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="36723">We are going to use the</st> `<st c="36748">NLTK</st>` <st c="36752">stem
    package to perform word stemming, which incorporates different algorithms to stem
    words from English and other languages.</st> <st c="36880">Each method differs
    in the algorithm it uses to find the</st> *<st c="36937">root</st>* <st c="36941">of
    the word; therefore, they may output slightly different results.</st> <st c="37010">I
    recommend reading more about it, trying different methods, and</st> <st c="37075">choosing</st>
    <st c="37084">the</st> <st c="37088">one that serves the project you are</st>
    <st c="37124">working on.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37135">More information about NLTK st</st><st c="37166">emmers can be
    found</st> <st c="37187">at</st> [<st c="37190">https://www.nlt</st><st c="37205">k.org/api/nltk.stem.html</st>](https://www.nltk.org/api/nltk.stem.html)<st
    c="37230">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37231">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="37247">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="37311">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37325">Load</st> `<st c="37331">pandas</st>`<st c="37337">,</st> `<st
    c="37339">stopwords</st>`<st c="37348">, and</st> `<st c="37354">SnowballStemmer</st>`
    <st c="37369">from</st> `<st c="37375">NLTK</st>` <st c="37379">and the dataset</st>
    <st c="37396">from</st> `<st c="37401">scikit-learn</st>`<st c="37413">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37564">Let’s load the train set part of the 20 Newsgroup dataset into
    a</st> <st c="37630">pandas DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37736">Now, let’s begin with the</st> <st c="37763">text cleaning.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="37777">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37782">After executing each of the commands in this recipe, print some
    example texts by executing, for example,</st> `<st c="37888">print(df['text'][10])</st>`
    <st c="37909">so that you can visualize the changes introduced to the text.</st>
    <st c="37972">Go ahead and do it now, and then repeat the command after</st> <st
    c="38030">each step.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38040">Let’s begin by removing</st> <st c="38065">the punctuation:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38131">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38135">You can also remove the punctuation using the built-in</st> `<st
    c="38191">string</st>` <st c="38197">module from Python.</st> <st c="38218">First,
    import the module by executing</st> `<st c="38256">import string</st>` <st c="38269">and
    then execute</st> `<st c="38287">df['text'] =</st>` `<st c="38300">df['text'].str.replace('[{}]</st><st
    c="38328">'.format(string.punctuatio</st><st c="38355">n), '')</st>`<st c="38363">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38364">We can also</st> <st c="38376">remove</st> <st c="38384">characters
    that are numbers, leaving only letters,</st> <st c="38435">as follows:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38506">Now, let’s set all words</st> <st c="38532">into lowercase:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38583">Now, let’s start the p</st><st c="38606">rocess of removing</st>
    <st c="38626">stop words.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="38637">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="38642">Step 6</st>* <st c="38649">may fail if you did not download
    the</st> `<st c="38687">NLTK</st>` <st c="38691">library’s</st> `<st c="38702">stopwords</st>`<st
    c="38711">. Visit the</st> *<st c="38723">Technical requirements</st>* <st c="38745">section
    in this chapter for</st> <st c="38774">more details.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38787">Let’s create a function that splits a string into a list of words,
    removes the stop words, and finally concatenates the remaining words back into</st>
    <st c="38934">a string:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39114">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39119">To be able to process the data with the</st> `<st c="39160">scikit-learn</st>`
    <st c="39172">librar</st><st c="39179">y’s</st> `<st c="39184">CountVectorizer()</st>`
    <st c="39201">or</st> `<st c="39205">TfidfVecto</st><st c="39215">rizer()</st>`<st
    c="39223">, we need the text to be in string format.</st> <st c="39266">Therefore,
    after removing the stop words, we need to return the words as a single string.</st>
    <st c="39356">We have transformed the NLTK library’s stop words list into a set
    because sets are faster to scan than lists.</st> <st c="39466">This improves the</st>
    <st c="39484">computation time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39501">Now, let’s use</st> <st c="39516">the</st> <st c="39521">function
    from</st> *<st c="39535">step 6</st>* <st c="39541">to remove stop words from
    the</st> `<st c="39572">text</st>` <st c="39576">variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39634">If you want to know which words are</st> <st c="39670">stop word</st><st
    c="39680">s,</st> <st c="39684">execute</st> `<st c="39692">stopwords.words('english')</st>`<st
    c="39718">.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="39719">Finally, let’s stem the words in our data.</st> <st c="39763">We
    will use</st> `<st c="39775">SnowballStemmer</st>` <st c="39790">from</st> `<st
    c="39796">NLTK</st>` <st c="39800">to</st> <st c="39804">do so.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="39810">Let’s create an instance of</st> `<st c="39839">SnowballStemer</st>`
    <st c="39853">for the</st> <st c="39862">English language:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39916">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39920">Try the stemmer in a single word to see how it works; for example,
    run</st> `<st c="39992">stemmer.stem('running')</st>`<st c="40015">. You should
    see</st> `<st c="40032">run</st>` <st c="40035">as the result of that command.</st>
    <st c="40067">Try</st> <st c="40071">different words!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40087">Let’s create a function that splits a string into a list of words,
    applies</st> `<st c="40163">stemmer</st>` <st c="40170">to each word, and finally
    concatenates the stemmed word list back into</st> <st c="40242">a string:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40374">Let’s use</st> <st c="40384">the</st> <st c="40389">function from</st>
    *<st c="40403">step 9</st>* <st c="40409">to stem the words in</st> <st c="40431">our
    data:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40483">Now, our text is ready to create features based on character and
    word counts, as well as create BoWs or TF-IDF matrices, as described in the previous
    recipes of</st> <st c="40645">this chapter.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="40658">If we execute</st> `<st c="40673">print(df['text'][10])</st>`<st
    c="40694">, we will see a text example</st> <st c="40723">after cleaning:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="41230">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41235">If you are counting sentences, you need to do that before removing
    punctuation, as punctuation and</st> <st c="41334">capitalization are needed to
    define the b</st><st c="41376">oundaries of</st> <st c="41390">each sentence.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41404">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="41420">In this recipe, we</st> <st c="41440">removed punctuation, numbers,
    and stop words from a text variable, set the words in lowercase, and finally,
    stemmed the words to their root.</st> <st c="41581">We removed punctuation and
    numbers from the text variable using pandas’</st> `<st c="41653">replace()</st>`<st
    c="41662">, which can be accessed through pandas’</st> `<st c="41702">str</st>`<st
    c="41705">, to replace digits,</st> `<st c="41726">'\d+'</st>`<st c="41731">,
    or symbols,</st> `<st c="41745">'[^\w\s]'</st>`<st c="41754">, with empty strings,</st>
    `<st c="41776">''</st>`<st c="41778">. Alternatively, we can use the</st> `<st
    c="41810">punctuation</st>` <st c="41821">module from the built-in</st> `<st c="41847">string</st>`
    <st c="41853">package.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41862">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41866">Run</st> `<st c="41871">string.punctuation</st>` <st c="41889">in
    your Python console after importing</st> `<st c="41929">string</st>` <st c="41935">to
    check out the symbols that will be replaced with</st> <st c="41988">empty strings.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42002">Next, utilizing</st> <st c="42018">pandas’ string</st> <st c="42034">processing
    functionality through</st> `<st c="42067">str</st>`<st c="42070">, we set all
    of the words to lowercase with the</st> `<st c="42118">lower()</st>` <st c="42125">method.</st>
    <st c="42134">To remove stop words from the text, we used the</st> `<st c="42182">stopwords</st>`
    <st c="42191">module from</st> `<st c="42204">NLTK</st>`<st c="42208">, which
    contains a list of words that are considered frequent – that is, the stop words.</st>
    <st c="42297">We created a function that takes a string and splits it into a list
    of words using pandas’</st> `<st c="42388">str.split()</st>`<st c="42399">, and
    then, with list comprehension, we looped over the words in the list and retained
    the non-stop words.</st> <st c="42506">Finally, with the</st> `<st c="42524">join()</st>`
    <st c="42530">method, we concatenated the retained words back into a string.</st>
    <st c="42594">We used the built-in Python</st> `<st c="42622">set()</st>` <st
    c="42627">method over the</st> `<st c="42644">NLTK</st>` <st c="42648">stop words
    list to improve computation efficiency since it is faster to iterate over sets
    than over lists.</st> <st c="42756">Finally, with pandas’</st> `<st c="42778">apply()</st>`<st
    c="42785">, we applied the function to each row of our</st> <st c="42830">text
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42840">Tip</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42844">Run</st> `<st c="42849">stopwords.words('english')</st>` <st c="42875">in
    your Python console after importing</st> `<st c="42915">stopwords</st>` <st c="42924">from</st>
    `<st c="42930">NLTK</st>` <st c="42934">to visualize the list with the stop words
    that will</st> <st c="42987">be removed.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42998">Finally, we stemmed the words using</st> `<st c="43035">SnowballStemmer</st>`
    <st c="43050">from</st> `<st c="43056">NLTK</st>`<st c="43060">.</st> `<st c="43062">SnowballStemmer</st>`
    <st c="43077">works one word at a time.</st> <st c="43104">Therefore, we created
    a function that takes a string and splits it into a list of words using pandas’</st>
    `<st c="43206">str.split()</st>`<st c="43217">. In a list comprehension, we applied</st>
    `<st c="43255">SnowballStemmer</st>` <st c="43270">word per word and then concatenated
    the list of stemmed words back into a string using the</st> `<st c="43362">join()</st>`
    <st c="43368">method.</st> <st c="43377">With pandas’</st> `<st c="43390">apply()</st>`<st
    c="43397">, we applied the</st> <st c="43413">function to stem words to each row
    of</st> <st c="43452">the DataFrame.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43466">The c</st><st c="43472">leaning steps we performed in this recipe
    resulted in strings containing the original text, without punctuation or numbers,
    in lowercase, without common words, and with the root of the word instead of its
    conjugated form.</st> <st c="43695">The data, as it is returned, can be used to
    derive features, as described in the</st> *<st c="43776">Counting characters,
    words, and vocabulary</st>* <st c="43818">recipe, or to create BoWs and TI-IDF
    matrices, as described in the</st> *<st c="43886">Creating features with bag-of-words
    and n-g</st><st c="43929">rams</st>* <st c="43934">and</st> *<st c="43939">Implementing
    term frequency-inverse document</st>* *<st c="43984">frequency</st>* <st c="43993">recipes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44002">Cleaning the texts as</st> <st c="44024">we</st> <st c="44028">have
    shown in this recipe can incur data loss, depending on the characteristics of
    the text, and if we seek to interpret the models after creating BoW or TF-IDF
    matrices, understanding the importance of stemmed words may not be</st> <st c="44256">so
    straightforward.</st>
  prefs: []
  type: TYPE_NORMAL
