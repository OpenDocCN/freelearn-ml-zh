- en: <st c="0">11</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Extracting Features from Text Variables</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="43">Text can be one of the variables in our datasets.</st> <st c="94">For
    example, in insurance, information describing the circumstances of an incident
    can come from free text fields in a form.</st> <st c="219">If a company gathers
    customer reviews, this information will be collected as short pieces of text provided
    by the users.</st> <st c="340">Text data does not show</st> <st c="363">the</st>
    **<st c="368">tabular</st>** <st c="375">pattern of the datasets that we have
    worked with throughout this book.</st> <st c="447">Instead, information in texts
    can vary in length and content, as well as writing style.</st> <st c="535">We
    can extract a lot of information from text variables to use as predictive features
    in machine learning models.</st> <st c="649">The techniques we will cover in this
    chapter belong to the realm of</st> **<st c="717">Natural Language Processing</st>**
    <st c="744">(</st>**<st c="746">NLP</st>**<st c="749">).</st> <st c="753">NLP
    is a subfield of linguistics and computer science.</st> <st c="808">It is</st>
    <st c="813">concerned with the interactions between computer and human language,
    or, in other words, how to program computers to understand human language.</st>
    <st c="958">NLP includes a multitude of techniques to understand the syntax, semantics,
    and discourse of text.</st> <st c="1057">Therefore, to do this field justice would
    require an</st> <st c="1110">entire book.</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1122">In this chapter, we will discuss the methods that will allow us
    to quickly extract features from short pieces of text to complement our predictive
    models.</st> <st c="1278">Specifically, we will discuss how to capture a piece
    of text’s complexity by looking at some statistical parameters of the text, such
    as the word length and count, the number of words and unique words used, the number
    of sentences, and so on.</st> <st c="1521">We will use the</st> `<st c="1537">pandas</st>`
    <st c="1543">and</st> `<st c="1548">scikit-learn</st>` <st c="1560">libraries,
    and we will make a shallow dive into a very useful Python NLP toolkit</st> <st
    c="1641">called the</st> **<st c="1653">Natural Language</st>** **<st c="1670">Toolkit</st>**
    <st c="1677">(</st>**<st c="1679">NLTK</st>**<st c="1683">).</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1686">This chapter includes the</st> <st c="1713">following recipes:</st>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1731">Counting characters, words,</st> <st c="1760">and vocabulary</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1774">Estimating text complexity by</st> <st c="1805">counting sentences</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1823">Creating features with bag-of-words</st> <st c="1860">and n-grams</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1871">Implementing term frequency-inverse</st> <st c="1908">document
    frequency</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1926">Cleaning and stemming</st> <st c="1949">text variabl</st><st c="1961">es</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="1964">Technical requirements</st>
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1987">In this chapter, we will use the</st> `<st c="2021">pandas</st>`<st
    c="2027">,</st> `<st c="2029">matplotlib</st>`<st c="2039">, and</st> `<st c="2045">scikit-learn</st>`
    <st c="2057">Python libraries.</st> <st c="2076">We will also use</st> `<st c="2093">NLTK</st>`<st
    c="2097">, a comprehensive Python library for NLP and text analysis.</st> <st
    c="2157">You can find the instructions to install</st> `<st c="2198">NLTK</st>`
    <st c="2202">at</st> [<st c="2206">http://www.nltk.org/install.html</st>](http://www.nltk.org/install.html)<st
    c="2238">.</st>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用<st c="2021">pandas</st>、<st c="2029">matplotlib</st>和<st c="2045">scikit-learn</st>
    Python库。我们还将使用<st c="2093">NLTK</st>，这是一个用于NLP和文本分析的综合性Python库。您可以在[<st c="2206">http://www.nltk.org/install.html</st>](http://www.nltk.org/install.html)找到安装<st
    c="2198">NLTK</st>的说明。
- en: <st c="2239">If you are using the Python Anaconda distribution, follow the instructions
    to install</st> `<st c="2326">NLTK</st>` <st c="2330">at</st> [<st c="2334">https://anaconda.org/anaconda/nltk</st>](https://anaconda.org/anaconda/nltk)<st
    c="2368">.</st>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是Python Anaconda发行版，请按照说明在[<st c="2334">https://anaconda.org/anaconda/nltk</st>](https://anaconda.org/anaconda/nltk)安装<st
    c="2326">NLTK</st>。
- en: <st c="2369">After you have installed</st> `<st c="2395">NLTK</st>`<st c="2399">,
    open up a Python console and execute</st> <st c="2438">the following:</st>
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在您安装了<st c="2369">NLTK</st>之后，打开一个Python控制台并执行以下操作：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="2514">These commands will download the necessary data for you to be able
    to run the recipes in this</st> <st c="2609">chapter successfully.</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将为您下载运行本章中菜谱所需的数据。
- en: <st c="2630">Note</st>
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: <st c="2635">If you haven’t downloaded these or the other data sources necessary
    for</st> `<st c="2708">NLTK</st>` <st c="2712">functionality,</st> `<st c="2728">NLTK</st>`
    <st c="2732">will raise an error.</st> <st c="2754">Read the error message carefully
    because it will direct you to download the data required to run the command that
    you are trying</st> <st c="2883">to execut</st><st c="2892">e.</st>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未下载这些或其他对<st c="2708">NLTK</st>功能必要的源数据，<st c="2728">NLTK</st>将引发错误。仔细阅读错误消息，因为它将指导您下载运行您尝试执行的命令所需的数据。
- en: <st c="2895">Counting characters, words, and vocabulary</st>
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算字符、单词和词汇
- en: <st c="2938">One of the</st> <st c="2950">sal</st><st c="2953">ient</st> <st
    c="2958">characteri</st><st c="2969">stics of text i</st><st c="2985">s its</st>
    <st c="2992">complexity.</st> <st c="3004">Long descriptions are more likely to
    contain more information than short descriptions.</st> <st c="3091">Texts rich
    in different, unique words are more likely to be richer in detail than texts that
    repeat the same words over and over.</st> <st c="3221">In the same way, when we
    speak, we use many short words such as articles and prepositions to build the
    sentence structure, yet the main concept is often derived from the nouns and adjectives
    we use, which tend to be longer words.</st> <st c="3451">So, as you can see, even
    without reading the text, we can start inferring how much information the text
    provides by determining the number of words, the number of unique words (non-repeated
    occurrences of a word), the lexical diversity, and the length of those words.</st>
    <st c="3719">In this recipe, we will learn how to extract these features from
    a text variable</st> <st c="3800">using</st> `<st c="3806">pa</st><st c="3808">ndas</st>`<st
    c="3813">.</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 文本的一个显著特征是其复杂性。长描述比短描述更有可能包含更多信息。包含不同、独特词汇的文本比反复重复相同词汇的文本更可能包含更丰富的细节。同样，当我们说话时，我们使用许多短词，如冠词和介词来构建句子结构，而主要概念通常来自我们使用的名词和形容词，这些往往是较长的词。所以，正如您所看到的，即使不阅读文本，我们也可以通过确定单词数量、唯一单词数量（单词的非重复出现）、词汇多样性和这些单词的长度来开始推断文本提供的信息量。在本菜谱中，我们将学习如何使用<st
    c="3806">pa</st><st c="3808">ndas</st>从文本变量中提取这些特征。
- en: <st c="3814">Getting ready</st>
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: <st c="3828">We are</st> <st c="3835">going to use the</st> `<st c="3892">scikit-learn</st>`<st
    c="3904">, which comprises around 18,000 news posts on 20 different topics.</st>
    <st c="3971">More details about this dataset can be found on the</st> <st c="4023">following
    sites:</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4039">The scikit</st><st c="4050">-learn dataset</st> <st c="4066">website:</st>
    [<st c="4075">https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset</st>](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4150">The hom</st><st c="4158">e page for the 20 Newsgroup</st> <st c="4187">dataset:</st>
    [<st c="4196">http://qwone.com/~jason/20Newsgroups/</st>](http://qwone.com/~jason/20Newsgroups/)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4233">Before jumping into the recipe, let’s discuss the features that
    we are going to derive from these</st> <st c="4332">text</st> <st c="4337">pieces.</st>
    <st c="4345">We mentioned that longer descriptions, more</st> <st c="4389">words
    in the article, a greater variety of unique words, and longer words tend to correlate
    with the amount of information that the article provides.</st> <st c="4539">Hence,
    we can capture text complexity by extracting the following information about</st>
    <st c="4623">the text:</st>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4632">The total number</st> <st c="4650">of characters</st>
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4663">The total number</st> <st c="4681">of words</st>
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4689">The total number of</st> <st c="4710">unique words</st>
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4722">Lexical diversity (total number of words divided by number of</st>
    <st c="4785">unique words)</st>
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4798">Word average length (number of characters divided by number</st>
    <st c="4859">of words)</st>
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="4868">In this recipe, we will extract these numerical features using</st>
    `<st c="4932">pandas</st>`<st c="4938">, which has extensive string processing
    functionalities that can be accessed via the</st> `<st c="5023">str</st>` <st
    c="5026">vectorized string functions</st> <st c="5055">for</st> <st c="5058">series.</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5066">How to do it...</st>
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="5082">Let’s begin by loading</st> `<st c="5106">pandas</st>` <st c="5112">and
    getting the</st> <st c="5129">dataset ready:</st>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5143">Load</st> `<st c="5149">pandas</st>` <st c="5155">and the dataset</st>
    <st c="5172">from</st> `<st c="5177">scikit-learn</st>`<st c="5189">:</st>
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="5259">L</st><st c="5261">et’s load</st> <st c="5271">the train set part
    of the 20 Newsgrou</st><st c="5308">p dataset into a</st> `<st c="5326">pandas</st>`
    <st c="5332">DataFrame:</st>
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="5432">Tip</st>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5436">You can print an example of a text from the DataFrame by executing</st>
    `<st c="5504">print(df['text'][1])</st>`<st c="5524">. Change the number between</st>
    `<st c="5552">[</st>` <st c="5553">and</st> `<st c="5558">]</st>` <st c="5559">to
    display different texts.</st> <st c="5588">Note how every text description is
    a single string composed of letters, numbers, punctuation, and spaces.</st> <st
    c="5694">You can check the datatype by</st> <st c="5724">executing</st> `<st c="5734">type(df["text"][1])</st>`<st
    c="5753">.</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行`print(df['text'][1])`来打印 DataFrame 中的文本示例。<st c="5524">更改`[` <st c="5552">和</st>
    `<st c="5553">`]` <st c="5559">之间的数字以显示不同的文本。</st> `<st c="5588">注意，每个文本描述都是一个由字母、数字、标点和空格组成的单个字符串。</st>
    `<st c="5694">您可以通过执行`type(df["text"][1])`来检查数据类型。</st>
- en: <st c="5754">Now</st> <st c="5759">that</st> <st c="5764">we</st> <st c="5767">have
    the text variable in a</st> `<st c="5795">pandas</st>` <st c="5801">DataFrame,
    we are ready to extract</st> <st c="5837">the features.</st>
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文本变量放入了一个`pandas` DataFrame 中，我们就可以准备提取特征了。
- en: <st c="5850">Let’s capture the number of characters in each text piece in a</st>
    <st c="5914">new column:</st>
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕获每个文本片段中的字符数：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="5963">Tip</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: <st c="5967">You can remove trailing white spaces in a string, including those
    from new lines, before counting the number of characters by adding the</st> `<st
    c="6105">strip()</st>` <st c="6112">method before the</st> `<st c="6131">len()</st>`
    <st c="6136">method, as shown here:</st> `<st c="6160">df['num_char'] =</st>`
    `<st c="6177">df['text'].str.strip().str.len()</st>`<st c="6209">.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在计数字符数之前，通过在`len()`方法之前添加`strip()`方法来移除字符串末尾的空白字符，包括换行符，如下所示：`df['num_char']
    = df['text'].str.strip().str.len()`。
- en: <st c="6210">Let’s capture the number of words in each text in a</st> <st c="6263">new
    column:</st>
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕获每个文本中的单词数：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="6325">To count words, we use the</st> `<st c="6353">pandas</st>` <st
    c="6359">library’s</st> `<st c="6370">split()</st>` <st c="6377">method, which
    splits a text at white spaces.</st> <st c="6423">Check out the output of</st>
    `<st c="6447">split()</st>` <st c="6454">by executing, for instance,</st> `<st
    c="6483">df["text"].loc[1].split()</st>` <st c="6508">to separate the words of
    the second text of</st> <st c="6553">the DataFrame.</st>
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要计算单词数，我们使用`pandas`库的`split()`方法，该方法在空白处分割文本。<st c="6423">通过执行，例如，`split()`来查看`split()`的输出，`df["text"].loc[1].split()`以分离
    DataFrame 中第二个文本的单词。</st>
- en: <st c="6567">Let’s capture the number of</st> *<st c="6596">unique</st>* <st
    c="6602">words in each text in a</st> <st c="6627">new column:</st>
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在新列中捕获每个文本中的*唯一*单词数：
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="6711">Note</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: <st c="6716">Python interprets the same word as two different words if one has
    a capital letter.</st> <st c="6801">To avoid this behavior, we can apply the</st>
    `<st c="6842">lower()</st>` <st c="6849">method before the</st> `<st c="6868">split()</st>`
    <st c="6875">method.</st>
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个单词有一个大写字母，Python 会将其解释为两个不同的单词。为了避免这种行为，我们可以在`split()`方法之前应用`lower()`方法。
- en: <st c="6883">Let’s create</st> <st c="6896">a</st> <st c="6899">feature that
    captures</st> <st c="6921">the lexical diversity – that is, t</st><st c="6955">he
    total number of words (</st>*<st c="6982">step</st> <st c="6987">4</st>*<st c="6989">)
    compare</st><st c="6998">d to the number of unique words (</st>*<st c="7032">step
    5</st>*<st c="7039">):</st>
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个捕获词汇多样性的特征——也就是说，总单词数（<st c="6982">步骤</st> <st c="6987">4</st>）与唯一单词数（<st
    c="7032">步骤 5</st>）的比较：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="7096">Let’s calculate the average word length by dividing the number
    of characters (</st>*<st c="7175">step 3</st>*<st c="7182">) by the number of
    words (</st>*<st c="7209">step 4</st>*<st c="7216">):</st>
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过将字符数（<st c="7175">步骤 3</st>）除以单词数（<st c="7209">步骤 4</st>）来计算平均单词长度：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="7277">If we execute</st> `<st c="7292">df.head()</st>`<st c="7301">,
    we will see the first five rows of data with the text and the newly</st> <st c="7371">crea</st><st
    c="7375">ted features:</st>
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="7277">如果我们执行</st> `<st c="7292">df.head()</st>`<st c="7301">，我们将看到包含文本和刚刚</st>
    <st c="7371">创</st><st c="7375">建的特征的</st> <st c="7379">前五行数据：</st>
- en: '![Figure 11.1 – A DataFrame with the text variable and features that summarize
    some of the text’s characteristics](img/B22396_11_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – 包含文本变量和总结文本特征的DataFrame](img/B22396_11_01.jpg)'
- en: <st c="7855">Figure 11.1 – A DataFrame with the text variable and features that
    summarize some of the text’s characteristics</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7855">图11.1 – 包含文本变量和总结文本特征的DataFrame</st>
- en: <st c="7966">With that, we have extracted five different features that capture
    the text complexity, which we can use</st> <st c="8071">as</st> <st c="8074">inputs
    for our machine</st> <st c="8097">learning</st> <st c="8106">algorithms.</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7966">有了这些，我们已经提取了五个不同的特征来捕捉文本复杂性，我们可以将这些特征用作</st> <st c="8071">我们的机器</st>
    <st c="8074">学习</st> <st c="8097">算法的</st> <st c="8106">输入。</st>
- en: <st c="8117">Note</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8117">注意</st>
- en: <st c="8122">In this recipe, we created new features from the raw data straight
    away without doing any data cleaning, removing punctuation, or even stemming words.</st>
    <st c="8274">Note that these are steps that are performed ahead of most standard
    NLP procedures.</st> <st c="8358">To learn more about this, visit the</st> *<st
    c="8394">Cleaning and stemming text variables</st>* <st c="8430">recipe at the
    end</st> <st c="8449">of</st> <st c="8452">this chapter.</st>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8122">在这个菜谱中，我们从原始数据中直接创建了新特征，而没有进行任何数据清理、删除标点符号，甚至没有进行词干提取。</st> <st
    c="8274">请注意，这些是在大多数标准NLP程序之前执行的步骤。</st> <st c="8358">要了解更多信息，请访问本章末尾的</st> *<st
    c="8394">清洗和词干提取文本变量</st>* <st c="8430">菜谱。</st>
- en: <st c="8465">How it</st> <st c="8472">works...</st>
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="8465">它是如何工作的...</st>
- en: <st c="8481">In this recipe, we created</st> <st c="8509">five new features
    that captur</st><st c="8538">e text complexity by utilizing pandas’</st> `<st
    c="8578">str</st>` <st c="8581">to access the built-in</st> `<st c="8605">pandas</st>`
    <st c="8611">functionality to work with strings.</st> <st c="8648">We worked with
    the text column of the</st> `<st c="8686">train</st>` <st c="8691">subset of the
    20 Newsgroup dataset that comes with</st> `<st c="8743">scikit-learn</st>`<st
    c="8755">. Each row in this dataset is composed of a string</st> <st c="8806">with
    text.</st>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8481">在这个菜谱中，我们创建了</st> <st c="8509">五个新的特征，通过利用pandas的</st> `<st c="8578">str</st>`
    <st c="8581">来访问内置的</st> `<st c="8605">pandas</st>` <st c="8611">功能来处理字符串。</st>
    <st c="8648">我们处理了20个新闻组数据集的</st> `<st c="8686">train</st>` <st c="8691">子集的文本列，该数据集包含在</st>
    `<st c="8743">scikit-learn</st>`<st c="8755">中。</st> <st c="8806">该数据集中的每一行都由一个包含文本的字符串组成。</st>
- en: <st c="8816">We used pandas’</st> `<st c="8833">str</st>`<st c="8836">, followed
    by</st> `<st c="8850">len()</st>`<st c="8855">, to count the number of characters
    in each string – that is, the total number of letters, numbers, symbols, and spaces.</st>
    <st c="8976">We also combined</st> `<st c="8993">str.len()</st>` <st c="9002">with</st>
    `<st c="9008">str.strip()</st>` <st c="9019">to remove trailing white spaces at
    the beginning and end of the string and in new lines, before counting the number</st>
    <st c="9136">of characters.</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8816">我们使用了pandas的</st> `<st c="8833">str</st>`<st c="8836">，然后是</st>
    `<st c="8850">len()</st>`<st c="8855">，来计算每个字符串中的字符数——即字母、数字、符号和空格的总数。</st> <st
    c="8976">我们还结合了</st> `<st c="8993">str.len()</st>` <st c="9002">和</st> `<st c="9008">str.strip()</st>`
    <st c="9019">来移除字符串开头和结尾的空白字符以及换行符中的空白字符，在计算字符数之前。</st>
- en: <st c="9150">To count the number of words, we used pandas’</st> `<st c="9197">str</st>`<st
    c="9200">, followed by</st> `<st c="9214">split()</st>`<st c="9221">, to divide
    the string into a list of words.</st> <st c="9266">The</st> `<st c="9270">split()</st>`
    <st c="9277">method creates a list of words by breaking the string at the white
    spaces between words.</st> <st c="9367">Next, we counted those words with</st>
    `<st c="9401">str.len()</st>`<st c="9410">, obtaining the number of words</st>
    <st c="9442">per string.</st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9150">要计算单词数，我们使用了pandas的</st> `<st c="9197">str</st>`<st c="9200">，然后是</st>
    `<st c="9214">split()</st>`<st c="9221">，将字符串分割成单词列表。</st> <st c="9266">`<st c="9270">split()</st>`
    <st c="9277">方法通过在单词之间的空白处断开字符串来创建单词列表。</st> <st c="9367">接下来，我们使用</st> `<st c="9401">str.len()</st>`<st
    c="9410">来计算这些单词，得到每个字符串的单词数。</st>
- en: <st c="9453">Note</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9453">注意</st>
- en: <st c="9458">We can change the behavior of</st> `<st c="9489">str.split()</st>`
    <st c="9500">by passing a string or character that we would like to use to split
    the string.</st> <st c="9581">For example,</st> `<st c="9594">df['text'].str.split(';')</st>`
    <st c="9619">divides a string at each occurrence</st> <st c="9656">of</st> `<st
    c="9659">;</st>`<st c="9660">.</st>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9458">我们可以通过传递一个字符串或字符来改变</st> `<st c="9489">str.split()</st>` <st c="9500">的行为，我们希望用它来分割字符串。</st>
    <st c="9581">例如，</st> `<st c="9594">df['text'].str.split(';')</st>` <st c="9619">在</st>
    `<st c="9659">;</st>`<st c="9660">的每个出现处分割字符串。</st>
- en: <st c="9661">To determine the number of unique words, we used pandas’</st> `<st
    c="9719">str.split()</st>` <st c="9730">function to divide the string into a list
    of words.</st> <st c="9783">Next, we applied the built-in Python</st> `<st c="9820">set()</st>`
    <st c="9825">method within pandas’</st> `<st c="9848">apply()</st>` <st c="9855">to
    return a set of words.</st> <st c="9882">Remember that a set contains</st> *<st
    c="9911">unique occurrences</st>* <st c="9929">of the elements in a list – that
    is, unique words.</st> <st c="9981">Next, we counted those words with pandas’</st>
    `<st c="10023">str.len()</st>` <st c="10032">function to return the</st> `<st
    c="10253">lower()</st>` <st c="10260">function to set all the characters to lowercase
    before splitting the string and counting the number of</st> <st c="10364">unique
    words.</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9661">为了确定唯一单词的数量，我们使用了 pandas 的</st> `<st c="9719">str.split()</st>`
    <st c="9730">函数将字符串分割成单词列表。</st> <st c="9783">接下来，我们在 pandas 的</st> `<st c="9848">apply()</st>`
    <st c="9855">方法中应用了内置的 Python</st> `<st c="9820">set()</st>` <st c="9825">方法，以返回一组单词。</st>
    <st c="9882">记住，集合包含</st> *<st c="9911">唯一出现</st>* <st c="9929">的元素列表中的元素 – 那就是唯一单词。</st>
    <st c="9981">接下来，我们使用 pandas 的</st> `<st c="10023">str.len()</st>` <st c="10032">函数来计数这些单词，并返回</st>
    `<st c="10253">lower()</st>` <st c="10260">函数，在分割字符串并计数唯一单词数量之前将所有字符转换为小写。</st>
- en: <st c="10377">To create the</st> <st c="10391">lexical</st> <st c="10400">diversity
    and average word length</st> <st c="10434">features,</st> <st c="10444">we</st>
    <st c="10446">simply pe</st><st c="10456">rformed a vectorized di</st><st c="10480">vision
    of two</st> `<st c="10495">pandas</st>` <st c="10501">s</st><st c="10503">eries.</st>
    <st c="10510">That’s it; we created five new features with information about the
    comp</st><st c="10581">lexity of</st> <st c="10592">the text.</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10377">为了创建</st> <st c="10391">词汇</st> <st c="10400">多样性和平均单词长度</st>
    <st c="10434">特征，</st> <st c="10444">我们</st> <st c="10446">只是</st><st c="10456">执行了两个</st>
    `<st c="10495">pandas</st>` <st c="10501">序列</st><st c="10503">的向量化除法。</st> <st
    c="10510">就是这样；我们创建了五个新的特征，其中包含有关文本复杂性的信息。</st>
- en: <st c="10601">There’s more...</st>
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="10601">还有更多...</st>
- en: <st c="10617">We can check out the distribution of the features extracted from
    text in each of the 20 different news topics present in the dataset by</st> <st
    c="10754">using visualizations</st><st c="10774">.</st>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10617">我们可以通过使用可视化来检查数据集中每个 20 个不同新闻主题中提取的文本特征的分布。</st> <st c="10754">。</st>
- en: <st c="10775">To make histogram plots of the newly created features, after you
    run all of the steps in the</st> *<st c="10869">How it works...</st>* <st c="10884">section
    of this recipe, follow</st> <st c="10916">these steps:</st>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10775">要创建新创建特征的直方图，在运行本食谱中 *<st c="10869">如何工作...</st>* <st c="10884">部分的全部步骤之后，遵循以下步骤：</st>
- en: <st c="10928">Import</st> `<st c="10936">matplotlib</st>`<st c="10946">:</st>
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="10928">导入</st> `<st c="10936">matplotlib</st>`<st c="10946">:</st>
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="10980">Add the target with the news topics to the 20</st> <st c="11027">Newsgroup
    DataFrame:</st>
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="10980">将带有新闻主题的目标添加到 20</st> <st c="11027">Newsgroup DataFrame：</st>
- en: '[PRE9]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="11074">Create a function that displays a histogram of a feature of your
    choice for each of the</st> <st c="11163">news topics:</st>
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="11074">创建一个函数，用于显示每个新闻主题所选特征的直方图：</st>
- en: '[PRE10]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="11518">Run the function</st> <st c="11536">for</st> <st c="11540">the
    number of</st> <st c="11554">words feature:</st>
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="11518">为单词特征运行该函数</st> <st c="11536">的数量</st> <st c="11540">次：</st>
- en: '[PRE11]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="11599">The previous command returns the following plot, where you can
    see the distribution of the number of words in each of the 20 news topics, numbered
    from 0 to</st> <st c="11757">19 in the</st> <st c="11767">plot title:</st>
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="11599">前面的命令返回以下图表，其中你可以看到每个 20 个新闻主题中单词数量的分布，编号从 0 到</st> <st c="11757">19，在</st>
    <st c="11767">图表标题中：</st>
- en: '![Figure 11.2 – Histograms showing the distribution of the number of words
    per text, segregated by topic discussed in each text](img/B22396_11_02.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图11.2 – 每个文本单词数量的直方图，按每个文本中讨论的主题进行细分](img/B22396_11_02.jpg)'
- en: <st c="12760">Figure 11.2 – Histograms showing the distribution of the number
    of words per text, segregated by topic discussed in each text</st>
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12760">图11.2 – 每个文本单词数量的直方图，按每个文本中讨论的主题进行细分</st>
- en: <st c="12885">The number of</st> <st c="12900">words</st> <st c="12905">shows
    a different</st> <st c="12924">distribution across the different news topics</st><st
    c="12969">. Therefore, this feature is likely useful in a classification algorithm
    to predic</st><st c="13051">t the topic of</st> <st c="13067">the text.</st>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12885">单词数量</st> <st c="12900">在不同新闻主题中</st> <st c="12905">显示出不同的</st>
    <st c="12924">分布</st><st c="12969">。因此，这个特征在分类算法中预测文本的主题时可能很有用。</st>
- en: <st c="13076">See also</st>
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="13076">另请参阅</st>
- en: <st c="13085">To learn more about pandas’ b</st><st c="13115">uilt-in string
    processing functionality</st> <st c="13156">visit</st> [<st c="13162">https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary</st>](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary)<st
    c="13242">.</st>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13085">要了解更多关于pandas内置字符串处理功能的信息，请访问</st> [<st c="13162">https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary</st>](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary)<st
    c="13242">。</st>
- en: <st c="13243">Estimating text complexity by counting senten</st><st c="13289">ces</st>
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="13243">通过计算句子数量来估计文本复杂度</st>
- en: <st c="13293">One aspect of a</st> <st c="13309">piece of text that</st> <st
    c="13329">we can capture in features is its complexity.</st> <st c="13375">Usually,
    longer descriptions that contain multiple sentences spread over several paragraphs
    tend to provide more information than descriptions with very few sentences.</st>
    <st c="13543">Therefore, capturing the number of sentences may provide some insight
    into the amou</st><st c="13626">nt of information provided by the text.</st> <st
    c="13667">This process is</st> <st c="13682">called</st> `<st c="14027">NLTK</st>`
    <st c="14031">Python library, which pr</st><st c="14056">ovides</st> <st c="14064">this
    functionality.</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13293">我们可以捕获的文本的一个方面是其复杂性。</st> <st c="13309">通常，包含多个句子并分布在几个段落中的较长的描述，比包含非常少句子的描述提供的信息更多。</st>
    <st c="13375">因此，捕获句子的数量可能有助于了解文本提供的信息量。</st> <st c="13543">这个过程被称为</st> `<st
    c="14027">NLTK</st>` <st c="14031">Python库，它提供了这个功能。</st>
- en: <st c="14083">Getting ready</st>
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="14083">准备工作</st>
- en: <st c="14097">In this recipe, we will use the</st> `<st c="14130">NLTK</st>`
    <st c="14134">Python library.</st> <st c="14151">For guidelines on how to install</st>
    `<st c="14184">NLTK</st>`<st c="14188">, check out the</st> *<st c="14204">Technical
    requirement</st><st c="14225">s</st>* <st c="14227">section of</st> <st c="14239">this
    chapter.</st>
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14097">在这个示例中，我们将使用</st> `<st c="14130">NLTK</st>` <st c="14134">Python库。</st>
    <st c="14151">有关如何安装</st> `<st c="14184">NLTK</st>`<st c="14188">的指南，请参阅本章的</st>
    *<st c="14204">技术要求</st><st c="14225">部分</st>* <st c="14227">。</st>
- en: <st c="14252">How to do it...</st>
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="14252">如何操作...</st>
- en: <st c="14268">Let’s begin by importing the required libraries</st> <st c="14317">and
    dataset:</st>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14268">让我们首先导入所需的库和数据集：</st> <st c="14317">和</st>
- en: <st c="14329">Let’s load</st> `<st c="14341">pandas</st>`<st c="14347">, the
    sentence tokenizer from</st> `<st c="14377">NLTK</st>`<st c="14381">, and the
    dataset</st> <st c="14399">from</st> `<st c="14404">scikit-learn</st>`<st c="14416">:</st>
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="14329">让我们加载</st> `<st c="14341">pandas</st>`<st c="14347">，从</st> `<st
    c="14377">NLTK</st>`<st c="14381">中获取句子分词器，以及来自</st> `<st c="14399">scikit-learn</st>`<st
    c="14404">的数据集</st> <st c="14416">：</st>
- en: '[PRE12]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="14526">To understand the functionality of the sentence tokenizer from</st>
    `<st c="14590">NLTK</st>`<st c="14594">, let’s create a variable that contains
    a string with</st> <st c="14648">multiple sentences:</st>
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="14526">为了理解</st> `<st c="14590">NLTK</st>`<st c="14594">中的句子分词器的功能，让我们创建一个包含多个句子的字符串变量：</st>
- en: '[PRE13]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="15148">Now, let’s</st> <st c="15160">separate the string from</st> *<st
    c="15185">step 2</st>* <st c="15191">into sentences using</st> `<st c="15213">NLTK</st>`
    <st c="15217">library‘s</st> <st c="15228">sentence tokenizer:</st>
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="15148">现在，让我们</st> <st c="15160">使用</st> `<st c="15185">步骤2</st>` <st
    c="15191">中的字符串，通过</st> `<st c="15213">NLTK</st>` <st c="15217">库的</st> <st c="15228">句子分词器</st>
    <st c="15217">将其分割成句子：</st>
- en: '[PRE14]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="15267">Tip</st>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15267">提示</st>
- en: <st c="15271">If you encounter an error in</st> *<st c="15301">step 3</st>*<st
    c="15307">, read the error message carefully and download the data source required
    by</st> `<st c="15383">NLTK</st>`<st c="15387">, as described in the error message.</st>
    <st c="15424">For more details, check out the</st> *<st c="15456">Technical</st>*
    *<st c="15466">requirements</st>* <st c="15478">section.</st>
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 `<st c="15301">步骤 3</st>`<st c="15307">中遇到错误，请仔细阅读错误消息，并下载 `<st c="15383">NLTK</st>`<st
    c="15387">所需的所需数据源，如错误消息中所述。</st> <st c="15424">有关更多详细信息，请查看 *<st c="15456">技术</st>*
    *<st c="15466">要求</st>* <st c="15478">部分。</st>
- en: <st c="15487">The sentence tokenizer returns the list of sentences shown in
    the</st> <st c="15554">following output:</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 句子分词器返回以下输出中显示的句子列表：</st>
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="16066">Note</st>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="16066">注意</st>`'
- en: <st c="16071">The escape character followed by the letter,</st> `<st c="16117">\n</st>`<st
    c="16119">, indic</st><st c="16126">ates a</st> <st c="16134">new line.</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随字母的转义字符，`<st c="16117">\n</st>`<st c="16119">，表示一个新行。</st>
- en: <st c="16143">Let’s c</st><st c="16151">ount the number of sentences in the</st>
    `<st c="16188">text</st>` <st c="16192">variable:</st>
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<st c="16143">让我们计算文本变量中的句子数量：</st>`'
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="16227">The previous command returns</st> `<st c="16257">8</st>`<st c="16258">,
    which is the number of sentences in our</st> `<st c="16300">text</st>` <st c="16304">variable.</st>
    <st c="16315">Now, let’s determine the number of sentences in an</st> <st c="16366">entire
    DataFrame.</st>
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 之前的命令返回了 `<st c="16257">8</st>`<st c="16258">，这是我们文本变量中的句子数量。</st> <st c="16315">现在，让我们确定整个
    DataFrame 中的句子数量。</st>
- en: <st c="16383">Let’s load</st> <st c="16395">the</st> `<st c="16399">train</st>`
    <st c="16404">subset of the 20 Newsgroup dataset into a</st> `<st c="16447">pandas</st>`
    <st c="16453">DataFrame:</st>
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<st c="16383">让我们将 20 个新闻组数据集的 `<st c="16399">训练</st>` <st c="16404">子集加载到
    pandas DataFrame 中：</st>`'
- en: '[PRE17]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="16553">To speed up the following steps, we will only work with the first</st>
    `<st c="16620">10</st>` <st c="16622">rows of</st> <st c="16631">the DataFrame:</st>
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了加快以下步骤，我们只需处理 DataFrame 的前 `<st c="16620">10</st>` `<st c="16622">行：</st>
- en: '[PRE18]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="16663">Let’s also remove the first part of the text, which contains information
    about the email sender, subject, and other details that we are not interested
    in.</st> <st c="16819">Most of this information comes before the word</st> `<st
    c="16866">Lines</st>` <st c="16871">followed by</st> `<st c="16884">:</st>`<st
    c="16885">, so let’s split the string at</st> `<st c="16916">Lines:</st>` <st
    c="16922">and capture the second part of</st> <st c="16954">the string:</st>
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也删除文本的第一部分，这部分包含有关电子邮件发送者、主题和其他我们不感兴趣的信息。</st> <st c="16819">大部分信息都在单词 `<st
    c="16866">Lines</st>` <st c="16871">之后，后面跟着 `<st c="16884">:</st>`<st c="16885">，所以让我们在
    `<st c="16916">Lines:</st>` <st c="16922">处拆分字符串，并捕获字符串的第二部分：</st>
- en: '[PRE19]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="17032">Fin</st><st c="17036">ally, let’s create a v</st><st c="17059">ariable
    containing the number of sentences</st> <st c="17103">per</st> `<st c="17107">text</st>`<st
    c="17111">:</st>
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`<st c="17032">最终，让我们创建一个包含每段文本中句子数量的变量：</st>'
- en: '[PRE20]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: <st c="17174">With the</st> `<st c="17184">df</st>` <st c="17186">command, you
    can display the entire DataFrame with the</st> `<st c="17242">text</st>` <st c="17246">variable
    and the new feature containing the nu</st><st c="17293">mber of sentences</st>
    <st c="17312">per text:</st>
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`<st c="17174">使用</st>` `<st c="17184">df</st>` `<st c="17186">命令，您可以显示包含 `<st
    c="17242">text</st>` `<st c="17246">变量和包含每段文本中句子数量的新特征的整个 DataFrame：</st>`'
- en: '![Figure 11.3 – A DataFrame with the text variable and the number of sentences
    per text](img/B22396_11_03.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 包含文本变量和每段文本中句子数量的 DataFrame](img/B22396_11_03.jpg)'
- en: <st c="17841">Figure 11.3 – A DataFrame with the text variable and the number
    of sentences per text</st>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – 包含文本变量和每段文本中句子数量的 DataFrame](img/B22396_11_03.jpg)'
- en: <st c="17926">Now, we can use this</st> <st c="17947">new feature as input to</st>
    <st c="17972">machine</st> <st c="17980">learning algorithms.</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="17926">现在，我们可以将这个新功能作为输入提供给</st>` <st c="17947">机器学习算法。</st>'
- en: <st c="18000">How it works...</st>
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`<st c="18000">它的工作原理...</st>`'
- en: <st c="18016">In this recipe, we separated a string with text into sentences
    using</st> `<st c="18086">sent_tokenizer</st>` <st c="18100">from the</st> `<st
    c="18110">NLTK</st>` <st c="18114">library.</st> `<st c="18124">sent_tokenizer</st>`
    <st c="18138">has been pre-trained to recognize capitalization and different types
    of punctuation that signal the beginning and the end of</st> <st c="18264">a sentence.</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用 `<st c="18086">sent_tokenizer</st>` 从 `<st c="18110">NLTK</st>`
    `<st c="18114">库</st>` 中将包含文本的字符串分割成句子。`<st c="18124">sent_tokenizer</st>` 已经预先训练以识别大写字母和不同类型的标点符号，这些符号标志着句子的开始和结束。
- en: <st c="18275">First, we applied</st> `<st c="18294">sent_tokenizer</st>` <st
    c="18308">to a manually created string to become familiar with its functionality.</st>
    <st c="18381">The tokenizer divided the text into a list of eight sentences.</st>
    <st c="18444">We combined the tokenizer with the built-in Python</st> `<st c="18495">len()</st>`
    <st c="18500">method to count the number of sentences in</st> <st c="18544">the
    string.</st>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将 `<st c="18294">sent_tokenizer</st>` 应用到一个手动创建的字符串上，以便熟悉其功能。分词器将文本分割成八个句子的列表。我们结合了分词器和内置的
    Python `<st c="18495">len()</st>` 方法来计算字符串中的句子数量。
- en: '<st c="18555">Next, we loaded a dataset with text and, to speed up the computation,
    we only retained the first 10 rows of the DataFrame using pandas’</st> `<st c="18692">loc[]</st>`
    <st c="18697">function.</st> <st c="18708">Next, we removed the first part of
    the text, which contained information about the email sender and subject.</st>
    <st c="18817">To do this, we split the string at</st> `<st c="18852">Lines:</st>`
    <st c="18858">using pandas’</st> `<st c="18873">str.split("Lines:")</st>` <st
    c="18892">function, which returned a list with two elements: the strings before
    and after</st> `<st c="18973">Lines:</st>`<st c="18979">. Utilizing a lambda function
    within</st> `<st c="19016">apply()</st>`<st c="19023">, we retained the second
    part of the text – that is, the second string in the list returned</st> <st c="19115">by</st>
    `<st c="19118">split()</st>`<st c="19125">.</st>'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载了一个包含文本的数据集，为了加快计算速度，我们只保留了 DataFrame 的前 10 行，使用 pandas 的 `<st c="18692">loc[]</st>`
    函数。接下来，我们移除了文本的第一部分，这部分包含了关于电子邮件发送者和主题的信息。为此，我们使用 pandas 的 `<st c="18873">str.split("Lines:")</st>`
    函数在 `<st c="18852">Lines:</st>` 处分割字符串，该函数返回一个包含两个元素的列表：`<st c="18973">Lines:</st>`
    前后的字符串。通过在 `<st c="19016">apply()</st>` 中使用 lambda 函数，我们保留了文本的第二部分——即列表中由 `<st
    c="19118">split()</st>` 返回的第二个字符串。
- en: <st c="19126">Finally, we applied</st> `<st c="19147">sent_tokenizer</st>` <st
    c="19161">to each row in the DataFrame with the pandas</st> `<st c="19207">apply()</st>`
    <st c="19214">method to separate the strings into sentences, and then applied
    the built-</st><st c="19289">in Python</st> `<st c="19300">len()</st>` <st c="19305">method
    to th</st><st c="19318">e list of sentences to return the number of sentences
    per string.</st> <st c="19385">This way, we</st> <st c="19398">created a new feature
    that contained the</st> <st c="19439">number of sentences</st> <st c="19459">per
    text.</st>
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 pandas 的 `<st c="19207">apply()</st>` 方法将 `<st c="19126">sent_tokenizer</st>`
    应用到 DataFrame 的每一行上，将字符串分割成句子，然后使用内置的 Python `<st c="19300">len()</st>` 方法对句子列表进行操作，以返回每个字符串的句子数量。这样，我们创建了一个包含每个文本句子数量的新特征。
- en: <st c="19468">There’s mo</st><st c="19479">re...</st>
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="19468">There’s mo</st><st c="19479">re...</st>
- en: '`<st c="19485">NLTK</st>` <st c="19490">has functionalities</st> <st c="19511">for
    word tokenization among other useful features, which we can use instead of</st>
    `<st c="19590">pandas</st>` <st c="19596">to count and return the number of words.</st>
    <st c="19638">You can find out more about</st> `<st c="19666">NLTK</st>`<st c="19670">’s</st>
    <st c="19674">functionality here:</st>'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="19485">NLTK</st>` `<st c="19490">具有单词分词等有用功能，我们可以使用这些功能而不是 `<st c="19590">pandas</st>`
    来计数和返回单词数量。</st> `<st c="19638">您可以在以下链接中了解更多关于 `<st c="19666">NLTK</st>` `<st
    c="19670">功能的信息：</st>`'
- en: '*<st c="19693">Python 3 Text Processing with NLTK 3 Cookbook</st>*<st c="19739">,
    by Jacob Per</st><st c="19753">kins,</st> <st c="19760">Packt Publishing</st>'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python 3 Text Processing with NLTK 3 Cookbook》*，作者 Jacob Perkkins，Packt Publishing'
- en: <st c="19776">The</st> `<st c="19781">NLTK</st>` <st c="19785">document</st><st
    c="19794">ation</st> <st c="19801">at</st> [<st c="19804">http://www.nltk.org/</st>](http://www.nltk.org/)<st
    c="19824">.</st>
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="19776">The</st> `<st c="19781">NLTK</st>` <st c="19785">文档</st><st c="19794">在</st>
    [<st c="19804">http://www.nltk.org/</st>](http://www.nltk.org/)<st c="19824">。</st>
- en: <st c="19825">Creating features with bag-of-words and n-grams</st>
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="19825">使用词袋和n-gram创建特征</st>
- en: <st c="19873">A</st> **<st c="19876">Bag-of-Words</st>** <st c="19888">(</st>**<st
    c="19890">BoW</st>**<st c="19893">)</st> <st c="19895">is a</st> <st c="19901">simplified</st>
    <st c="19911">representation</st> <st c="19926">of a piece of</st> <st c="19940">text
    that</st> <st c="19951">captures the wor</st><st c="19967">ds that are present
    in the text and the number of times each word appears in the text.</st> <st c="20055">So,
    for the text string</st> *<st c="20079">Dogs like cats, but cats do not like dogs</st>*<st
    c="20120">, the derived BoW is</st> <st c="20141">as follows:</st>
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19873">A</st> **<st c="19876">词袋</st>** <st c="19888">(</st>**<st c="19890">BoW</st>**<st
    c="19893">)</st> <st c="19895">是一种简化的文本表示，它捕捉了文本中存在的单词以及每个单词在文本中出现的次数。</st> <st
    c="19901">所以，对于文本字符串**<st c="20079">Dogs like cats, but cats do not like dogs</st>**<st
    c="20120">，得到的BoW如下：</st> <st c="20141">如下：</st>
- en: "![Figure 11.4 – The BoW derived from the sentence Dogs like cats, but cats\
    \ do not\uFEFF like dogs](img/B22396_11_04.jpg)"
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 从句子“Dogs like cats, but cats do not like dogs”派生的BoW](img/B22396_11_04.jpg)'
- en: <st c="20191">Figure 11.4 – The BoW derived from the sentence Dogs like cats,
    but cats do not</st> <st c="20270">like dogs</st>
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20191">图11.4 – 从句子“Dogs like cats, but cats do not”派生的BoW**<st c="20270">like
    dogs</st>**
- en: <st c="20280">Here, each word</st> <st c="20296">becomes a variable, and th</st><st
    c="20323">e value of the varia</st><st c="20344">ble represents the number of
    times the word appears in the string.</st> <st c="20412">As you can see, the BoW
    captures multiplicity but does not retain word order or grammar.</st> <st c="20501">That
    is why it is a simple, yet useful way of extracting features and capturing some
    information about the texts we are</st> <st c="20621">working with.</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20280">在这里，每个单词都成为一个变量，而这个变量的值表示该单词在字符串中出现的次数。</st> <st c="20296">正如你所看到的，BoW捕捉了多重性，但并不保留单词顺序或语法。</st>
    <st c="20412">这就是为什么它是一种简单但有用的提取特征和捕捉我们所处理文本的一些信息的方法。</st> <st c="20501">这就是为什么它是一种简单但有用的提取特征和捕捉我们所处理文本的一些信息的方法。</st>
- en: <st c="20634">To capture some syntax, BoW can be used together with</st> **<st
    c="20689">n-grams</st>**<st c="20696">. An n-gram is a contiguous</st> <st c="20724">sequence
    of</st> *<st c="20736">n</st>* <st c="20737">items in a</st> <st c="20749">given</st>
    <st c="20755">text.</st> <st c="20761">Continuing</st> <st c="20771">with the
    sentence</st> *<st c="20790">Dogs like cats, but cats do not like dogs</st>*<st
    c="20831">, the derived 2-grams are</st> <st c="20857">as follows:</st>
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20634">为了捕捉一些语法，BoW可以与**<st c="20689">n-gram</st>**<st c="20696">一起使用。n-gram是一个在给定文本中连续的*n*<st
    c="20736">项的序列。</st> <st c="20724">继续使用句子**<st c="20790">Dogs like cats, but cats
    do not like dogs</st>**<st c="20831">，得到的2-gram如下：</st> <st c="20857">如下：</st>
- en: <st c="20868">Dogs like</st>
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20868">Dogs like</st>
- en: <st c="20878">like cats</st>
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20878">like cats</st>
- en: <st c="20888">cats but</st>
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20888">cats but</st>
- en: <st c="20897">but do</st>
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20897">但是做</st>
- en: <st c="20904">do not</st>
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20904">do not</st>
- en: <st c="20911">like dogs</st>
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="20911">like dogs</st>
- en: <st c="20921">We can create, together with a BoW, a bag of n-grams, where the
    additional variables are given by the 2-grams and the values for each 2-gram are
    the number of times they appear in each string; for this example, the value is
    1\.</st> <st c="21149">So, our final BoW with 2-grams would look</st> <st c="21191">like
    this:</st>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20921">我们可以与一个BoW（词袋）一起创建一个n-gram词袋，其中额外的变量由2-gram提供，每个2-gram的值是它们在每个字符串中出现的次数；在这个例子中，值是1。</st>
    <st c="21149">因此，我们的最终BoW（词袋）包含2-gram将看起来像这样：</st> <st c="21191">如下：</st>
- en: '![Figure 11.5 – The BoW with 2-grams](img/B22396_11_05.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 包含2-gram的BoW](img/B22396_11_05.jpg)'
- en: <st c="21305">Figure 11.5 – The BoW with 2-grams</st>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21305">图11.5 – 包含2-gram的BoW</st>
- en: <st c="21339">In this recipe, we will learn how to create BoWs with or</st>
    <st c="21396">without n-grams</st> <st c="21413">using</st> `<st c="21419">sciki</st><st
    c="21424">t-learn</st>`<st c="21432">.</st>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21339">在这个菜谱中，我们将学习如何使用`<st c="21419">scikit-learn</st>`<st c="21424">创建带有或不带n-gram的BoW。</st>
- en: <st c="21433">Getting ready</st>
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="21433">准备就绪</st>
- en: <st c="21447">Before jumping into this</st> <st c="21472">recipe, let’s get
    familiar with some o</st><st c="21511">f the parameters of a B</st><st c="21535">oW
    that we can adjust to make the BoW comprehensive.</st> <st c="21589">When creating
    a BoW over several pieces of text, a new feature is created for each unique word
    that appears at least once in</st> *<st c="21714">any</st>* <st c="21717">of the
    text pieces we are analyzing.</st> <st c="21755">If the word appears only in one
    piece of text, it will show a value of 1 for that particular text and 0 for all
    of the others.</st> <st c="21882">Therefore, BoWs tend to be sparse matrices,
    where most of the values</st> <st c="21951">are zeros.</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21447">在跳入这个</st> <st c="21472">食谱</st> <st c="21511">之前</st><st c="21535">，让我们熟悉一些
    B</st><st c="21535">oW 的参数，以便我们可以调整以使 BoW 更全面。</st> <st c="21589">在创建多个文本的 BoW
    时，对于我们在分析的文本片段中至少出现一次的每个独特单词，都会创建一个新特征。</st> <st c="21714">任何</st> <st c="21717">一个</st>
    <st c="21755">文本片段中出现的单词，如果它只出现在一个文本片段中，那么它在该特定文本中的值将为 1，而在其他所有文本中的值为 0。</st>
    <st c="21882">因此，BoWs 往往是稀疏矩阵，其中大部分值</st> <st c="21951">都是零。</st>
- en: <st c="21961">The number of columns – that is, the number of words – in a BoW
    can be quite large if we work with huge text corpora, and even larger if we also
    include n-grams.</st> <st c="22124">To limit the number of columns and the sparsity
    of the returned matrix, we can retain words that appear across multiple texts;
    or, in better words, we can retain words that appear in, at least, a certain percentage</st>
    <st c="22339">of texts.</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21961">如果我们处理的是巨大的文本语料库，那么 BoW 中的列数——即单词数——可能相当大，如果我们还包括 n-gram，那么列数会更大。</st>
    <st c="22124">为了限制列数和返回矩阵的稀疏性，我们可以保留在多个文本中出现的单词；或者换句话说，我们可以保留至少在某个百分比</st> <st
    c="22339">的文本中出现的单词。</st>
- en: <st c="22348">To reduce the number of columns and sparsity of the BoW, we should
    also work with words in the</st> <st c="22444">same</st> <st c="22448">case –
    for</st> <st c="22460">example, lowercase – as</st> <st c="22484">Python identifies
    words in a different case as different words.</st> <st c="22548">We can also reduce
    the number of columns and sparsity by</st> <st c="22605">removing</st> **<st c="22614">stop
    words</st>**<st c="22624">. Stop words are very frequently used words that make
    sentences flow, but that do not, per se, carry any useful information.</st> <st
    c="22749">Examples of stop words are pronouns such as I, you, and he, as well
    as prepositions</st> <st c="22833">and articles.</st>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22348">为了减少 BoW 的列数和稀疏性，我们还应该使用与单词相同的</st> <st c="22444">大小写</st> <st
    c="22448">——例如，小写——因为 Python 将不同大小写的单词识别为不同的单词。</st> <st c="22484">我们还可以通过</st>
    <st c="22605">删除</st> **<st c="22614">停用词</st>**<st c="22624">来减少列数和稀疏性。停用词是使用频率非常高的单词，使句子流畅，但本身并不携带任何有用的信息。</st>
    <st c="22749">停用词的例子包括代词，如我、你和他，以及介词和冠词。</st>
- en: <st c="22846">In this recipe, we will learn how to set words in lowercase, remove
    stop words, retain words with a minimum acceptable frequency, and capture n-grams
    all together with a single transfor</st><st c="23032">mer from</st> `<st c="23042">scikit-learn</st>`<st
    c="23054">:</st> `<st c="23057">CountVectorizer()</st>`<st c="23074">.</st>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22846">在这个食谱中，我们将学习如何设置单词为小写，删除停用词，保留具有最低可接受频率的单词，并使用来自</st> `<st c="23042">scikit-learn</st>`<st
    c="23054">` 的单个转换器</st> <st c="23057">CountVectorizer()</st> <st c="23074">一起捕获
    n-gram：</st>
- en: <st c="23075">How to do it...</st>
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="23075">如何做...</st>
- en: <st c="23091">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="23155">dataset ready:</st>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23091">让我们首先加载必要的库并准备好</st> <st c="23155">数据集：</st>
- en: <st c="23169">Load</st> `<st c="23175">pandas</st>`<st c="23181">,</st> `<st
    c="23183">CountVectorizer</st>`<st c="23198">, and the dataset</st> <st c="23216">from</st>
    `<st c="23221">scikit-learn</st>`<st c="23233">:</st>
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23169">加载</st> `<st c="23175">pandas</st>`<st c="23181">，`<st c="23183">CountVectorizer</st>`<st
    c="23198">，以及来自</st> `<st c="23221">scikit-learn</st>`<st c="23233">` 的数据集</st>：
- en: '[PRE21]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: <st c="23368">Let’s load the</st> <st c="23384">train set part</st> <st c="23398">of
    the 20 Newsgroup dataset into a</st> <st c="23434">pandas DataFrame:</st>
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23368">让我们将 20 个新闻组数据集的</st> <st c="23384">训练集部分</st> <st c="23398">加载到一个</st>
    <st c="23434">pandas DataFrame</st> 中：</st>
- en: '[PRE22]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="23540">To make interpreting the results easier, let’s remove punctuation
    and numbers from the</st> <st c="23628">text variable:</st>
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23540">为了使结果更容易解释，让我们从</st> <st c="23628">文本变量</st> 中删除标点符号和数字：</st>
- en: '[PRE23]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="23740">Note</st>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23740">注意</st>
- en: <st c="23745">To learn more about regex with Python, follow this</st> <st c="23797">link:</st>
    [<st c="23803">https://docs.python.org/3/howto/regex.html</st>](https://docs.python.org/3/howto/regex.html)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23745">要了解更多关于Python中的正则表达式的信息，请点击此</st> <st c="23797">链接：</st> [<st
    c="23803">https://docs.python.org/3/howto/regex.html</st>](https://docs.python.org/3/howto/regex.html)
- en: <st c="23845">Now, let’s</st> <st c="23857">set up</st> `<st c="23864">CountVectorizer()</st>`
    <st c="23881">so</st> <st c="23885">that, before</st> <st c="23898">creating</st>
    <st c="23907">the BoW, it puts the text in lowercase, removes stop words, and
    retains words that appear in, at least, 5% of the</st> <st c="24021">text pieces:</st>
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="23845">现在，让我们</st> <st c="23857">设置</st> `<st c="23864">CountVectorizer()</st>`
    <st c="23881">，以便在创建BoW之前，它将文本转换为小写，删除停用词，并保留至少出现在5%的</st> <st c="24021">文本片段中的单词：</st>
- en: '[PRE24]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: <st c="24134">Note</st>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24134">注意</st>
- en: <st c="24139">To introduce n-grams as part of the returned columns, we can change
    the value of</st> `<st c="24221">ngrams_range</st>` <st c="24233">to, for example,</st>
    `<st c="24251">(1,2)</st>`<st c="24256">. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams.</st> <st c="24356">In
    the case of</st> `<st c="24371">(1,2)</st>`<st c="24376">,</st> `<st c="24378">CountVectorizer()</st>`
    <st c="24395">will return single words and arrays of two</st> <st c="24439">consecutive
    words.</st>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24139">为了将n-gram作为返回列的一部分引入，我们可以将`<st c="24221">ngrams_range</st>` <st
    c="24233">的值更改为，例如，`<st c="24251">(1,2)</st>`<st c="24256">。这个元组提供了不同n-gram的n值范围的上下边界。</st>
    <st c="24356">在`<st c="24371">(1,2)</st>`<st c="24376">的情况下，`<st c="24378">CountVectorizer()</st>`
    <st c="24395">将返回单个单词和两个连续单词的数组。</st>
- en: <st c="24457">Let’s fit</st> `<st c="24468">CountVectorizer()</st>` <st c="24485">so
    that it learns which words should be used in</st> <st c="24534">the BoW:</st>
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="24457">让我们拟合`<st c="24468">CountVectorizer()</st>` <st c="24485">，以便它学习在BoW中应该使用哪些单词：</st>
- en: '[PRE25]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: <st c="24569">Now, let’s create</st> <st c="24588">the BoW:</st>
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="24569">现在，让我们创建</st> <st c="24588">BoW：</st>
- en: '[PRE26]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: <st c="24634">Finally, let’s capture</st> <st c="24657">the BoW</st> <st c="24665">in
    a DataFrame with the corresponding</st> <st c="24704">feature names:</st>
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="24634">最后，让我们将BoW</st> <st c="24657">捕获到一个具有相应</st> <st c="24704">特征名称</st>
    <st c="24704">的DataFrame中：</st>
- en: '[PRE27]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: <st c="24805">With</st> <st c="24811">that, we</st> <st c="24819">have</st>
    <st c="24824">created</st> <st c="24833">a</st> `<st c="24835">pandas</st>` <st
    c="24841">DataFrame that contains words as columns and the number of times they
    appeared in each text as values.</st> <st c="24945">You can in</st><st c="24955">spect
    the result by</st> <st c="24976">executing</st> `<st c="24986">bagofwords.head()</st>`<st
    c="25003">:</st>
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="24805">有了</st> <st c="24811">这</st> <st c="24819">，</st> <st c="24824">我们</st>
    <st c="24833">已经</st> <st c="24835">创建</st> <st c="24841">了一个包含单词作为列和每个文本中它们出现的次数作为值的`<st
    c="24835">pandas</st>` <st c="24841">DataFrame。</st> <st c="24945">您可以通过执行`<st
    c="24986">bagofwords.head()</st>`<st c="25003">来查看结果：</st>
- en: '![Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup dataset](img/B22396_11_06.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 由20个新闻组数据集生成的BoW的DataFrame](img/B22396_11_06.jpg)'
- en: <st c="25367">Figure 11.6 – A DataFrame with the BoW resulting from the 20 Newsgroup
    dataset</st>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25367">图11.6 – 由20个新闻组数据集生成的BoW的DataFrame</st>
- en: <st c="25445">We can use</st> <st c="25456">this BoW as input for a machine</st>
    <st c="25489">learning model.</st>
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25445">我们可以使用</st> <st c="25456">这个BoW作为机器</st> <st c="25489">学习模型的输入。</st>
- en: <st c="25504">How it works...</st>
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="25504">它是如何工作的...</st>
- en: <st c="25520">scikit-learn’s</st> `<st c="25536">CountVectorizer()</st>` <st
    c="25553">converts a collection of text documents into a matrix of token counts.</st>
    <st c="25625">These tokens can be individual words or arrays of two or more consecutive
    words – that is, n-grams.</st> <st c="25725">In this recipe, we created a B</st><st
    c="25755">oW from a text variable in</st> <st c="25783">a D</st><st c="25786">ataFrame.</st>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25520">scikit-learn的`<st c="25536">CountVectorizer()</st>` <st c="25553">将一组文本文档转换为标记计数的矩阵。</st>
    <st c="25625">这些标记可以是单个单词或两个或更多连续单词的数组——即n-gram。</st> <st c="25725">在这个配方中，我们从一个DataFrame中的文本变量创建了一个BoW。</st>
- en: <st c="25796">We loaded the 20 Newsgroup text data</st><st c="25833">set from</st>
    `<st c="25843">scikit-l</st><st c="25851">earn</st>` <st c="25856">and removed
    punctuation and numbers from the text rows using pandas’</st> `<st c="25926">replace()</st>`
    <st c="25935">function, which can be accessed through pandas’</st> `<st c="25984">str</st>`
    <st c="25987">module, to replace digits,</st> `<st c="26015">'\d+'</st>`<st c="26020">,
    or symbols,</st> `<st c="26034">'[^\w\s]'</st>`<st c="26043">, with empty strings,</st>
    `<st c="26065">''</st>`<st c="26067">. Then, we used</st> `<st c="26083">CountVectorizer()</st>`
    <st c="26100">to create the BoW.</st> <st c="26120">We set the</st> `<st c="26131">lowercase</st>`
    <st c="26140">parameter to</st> `<st c="26154">True</st>` <st c="26158">to put
    the words in lowercase before extracting the BoW.</st> <st c="26216">We set the</st>
    `<st c="26227">stop_words</st>` <st c="26237">argument to</st> `<st c="26250">english</st>`
    <st c="26257">to ignore stop words – that is, to avoid stop words in the BoW.</st>
    <st c="26322">We set</st> `<st c="26329">ngram_range</st>` <st c="26340">to the</st>
    `<st c="26348">(1,1)</st>` <st c="26353">tuple to return only single words as
    columns.</st> <st c="26400">Finally, we set</st> `<st c="26416">min_df</st>` <st
    c="26422">to</st> `<st c="26426">0.05</st>` <st c="26430">to return words that
    appeared in at least 5% of the texts, or, in other words, in 5% of the rows in</st>
    <st c="26531">the DataFrame.</st>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25796">我们从`<st c="25843">scikit-l</st><st c="25851">earn</st>` <st c="25856">加载了20个新闻组的文本数据集</st>，并使用pandas的`<st
    c="25926">replace()</st>` <st c="25935">函数从文本行中删除了标点符号和数字，该函数可以通过pandas的`<st c="25984">str</st>`
    <st c="25987">模块访问，以替换数字，`<st c="26015">'\d+'</st>`<st c="26020">，或符号，`<st c="26034">'[^\w\s]'</st>`<st
    c="26043">，为空字符串，`<st c="26065">''</st>`<st c="26067">。然后，我们使用了`<st c="26083">CountVectorizer()</st>`
    <st c="26100">来创建BoW。</st> <st c="26120">我们将`<st c="26131">lowercase</st>` <st
    c="26140">参数设置为`<st c="26154">True</st>` <st c="26158">，在提取BoW之前将单词转换为小写。</st>
    <st c="26216">我们将`<st c="26227">stop_words</st>` <st c="26237">参数设置为`<st c="26250">english</st>`
    <st c="26257">，以忽略停用词——也就是说，避免BoW中的停用词。</st> <st c="26322">我们将`<st c="26329">ngram_range</st>`
    <st c="26340">设置为`<st c="26348">(1,1)</st>` <st c="26353">元组，以仅返回作为列的单个单词。</st>
    <st c="26400">最后，我们将`<st c="26416">min_df</st>` <st c="26422">设置为`<st c="26426">0.05</st>`
    <st c="26430">，以返回至少出现在5%的文本中的单词，换句话说，在DataFrame中的5%的行中。</st>
- en: <st c="26545">After</st> <st c="26552">setting</st> <st c="26560">up</st> <st
    c="26562">the</st> <st c="26566">transformer, we used the</st> `<st c="26592">fit()</st>`
    <st c="26597">method to allow the transformer to find the words that fulfill the
    preceding criteria.</st> <st c="26685">Finally, using the</st> `<st c="26704">transform()</st>`
    <st c="26715">method, the transformer returned an object containing the BoW with
    its fea</st><st c="26790">ture names, which we captured in a</st> `<st c="26826">pandas</st>`
    <st c="26832">DataFrame.</st>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26545">在</st> <st c="26552">设置</st> <st c="26560">变压器</st> <st c="26562">之后</st>，我们使用了`<st
    c="26592">fit()</st>` <st c="26597">方法来允许变压器找到满足先前标准的单词。</st> <st c="26685">最后，使用`<st
    c="26704">transform()</st>` <st c="26715">方法，变压器返回了一个包含具有其特征名称的BoW的对象，我们将它捕获在一个`<st
    c="26826">pandas</st>` <st c="26832">DataFrame</st>中。</st>
- en: <st c="26843">See also</st>
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="26843">另请参阅</st>
- en: <st c="26852">For more de</st><st c="26864">tails about</st> `<st c="26877">CountVectorizer()</st>`<st
    c="26894">, visit the</st> `<st c="26906">scikit-learn</st>` <st c="26918">library’s
    documentation</st> <st c="26943">at</st> [<st c="26946">https://scikit-learn.org/stable/modules/generated/s</st><st
    c="26997">klearn.feature_extraction.text.CountVectorizer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)<st
    c="27049">.</st>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26852">有关`<st c="26877">CountVectorizer()</st>` <st c="26894">的更多详细信息，请访问</st>
    `<st c="26906">scikit-learn</st>` <st c="26918">库的文档</st> <st c="26943">在</st>
    [<st c="26946">https://scikit-learn.org/stable/modules/generated/s</st><st c="26997">klearn.feature_extraction.text.CountVectorizer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)<st
    c="27049">。</st>
- en: <st c="27050">Implementing term frequency-inverse document frequency</st>
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="27050">实现词频-逆文档频率</st>
- en: '**<st c="27105">Term Frequency-Inverse Document Freq</st><st c="27142">uency</st>**
    <st c="27148">(</st>**<st c="27150">TF-IDF</st>**<st c="27156">) is a numerical
    statistic that captures</st> <st c="27198">how relevant a word is in a document
    considering the entire collection of documents.</st> <st c="27283">What does this
    mean?</st> <st c="27304">Some words will appear a lot within a text document as
    well as across documents, such as the English words</st> *<st c="27411">the</st>*<st
    c="27414">,</st> *<st c="27416">a</st>*<st c="27417">, and</st> *<st c="27423">is</st>*<st
    c="27425">, for example.</st> <st c="27440">These words generally convey little
    information about the actual content of the document and don’t make the text stand
    out from the crowd.</st> <st c="27579">TF-IDF provides a way to</st> *<st c="27604">weigh</st>*
    <st c="27609">the importance of a word by considering how many times it appears
    in a document with regards to how often it appears across documents.</st> <st
    c="27745">Hence, commonly occurring words such as</st> *<st c="27785">the</st>*<st
    c="27788">,</st> *<st c="27790">a</st>*<st c="27791">, or</st> *<st c="27796">is</st>*
    <st c="27798">will have a low weight, and words that are more specific to a topic,
    such as</st> *<st c="27876">leopard</st>*<st c="27883">, will have a</st> <st
    c="27897">higher weight.</st>'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="27105">词频-逆文档频率</st><st c="27142">（TF-IDF）** 是一个数值统计量，它捕捉了在考虑整个文档集合的情况下，一个词在文档中的相关性。这是什么意思？一些词在文本文档中以及跨文档中都会出现很多次，例如英语单词
    *<st c="27411">the</st>*<st c="27414">、*<st c="27416">a</st>*<st c="27417"> 和
    *<st c="27423">is</st>*<st c="27425">，例如。</st> 这些词通常对文档的实际内容传达的信息很少，并且不会使文本脱颖而出。**<st
    c="27579">TF-IDF 提供了一种方法**，通过考虑一个词在文档中出现的次数以及它在文档中出现的频率来 *<st c="27604">权衡</st>*
    该词的重要性。因此，像 *<st c="27785">the</st>*<st c="27788">、*<st c="27790">a</st>*<st c="27791">
    或 *<st c="27796">is</st>* 这样的常用词将具有较低的权重，而像 *<st c="27876">leopard</st>*<st c="27883">
    这样更具体于某个主题的词将具有更高的权重。'
- en: <st c="27911">TF-IDF is the product of two</st> <st c="27941">statistics:</st>
    **<st c="27953">Term Frequency</st>** <st c="27967">(</st>**<st c="27969">tf</st>**<st
    c="27971">) and</st> **<st c="27978">Inverse Document Frequency</st>** <st c="28004">(</st>**<st
    c="28006">idf</st>**<st c="28009">), represented</st> <st c="28025">as follows:</st>
    **<st c="28037">tf-idf = td × idf</st>**<st c="28054">. tf is, in its simplest
    form, the count of the word in an individual text.</st> <st c="28130">So, for
    term</st> *<st c="28143">t</st>*<st c="28144">, the tf is calculated as</st> *<st
    c="28170">tf(t) = count(t)</st>* <st c="28186">and is determined on a text-by-text
    basis.</st> <st c="28230">The idf is a measure of how common the word is across</st>
    *<st c="28284">all</st>* <st c="28287">documents and is usually calculated on
    a logarithmic scale.</st> <st c="28348">A common implementation is given by</st>
    <st c="28384">the following:</st>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="27911">TF-IDF 是两个统计量的乘积**：**<st c="27953">词频</st>**（**<st c="27969">tf</st>**）和**<st
    c="27978">逆文档频率</st>**（**<st c="28004">idf</st>**），表示如下：**<st c="28037">tf-idf
    = tf × idf</st>**。tf 在其最简单形式中，是一个词在单个文本中的计数。因此，对于术语 *<st c="28143">t</st>*<st
    c="28144">，tf 的计算为 *<st c="28170">tf(t) = count(t)</st>*，并且基于文本进行确定。idf 是衡量一个词在
    *<st c="28284">所有</st>* 文档中的普遍性的度量，通常在对数尺度上计算。以下是一个常见的实现方法：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mfenced
    open="(" close=")"><mi>t</mi></mfenced><mo>=</mo><mrow><mrow><mi>log</mi><mo>(</mo></mrow></mrow><mrow><mrow><mfrac><mi>n</mi><mrow><mn>1</mn><mo>+</mo><mi>d</mi><mi>f</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/48.png)'
- en: <st c="28416">Here,</st> *<st c="28422">n</st>* <st c="28423">is the total number
    of documents, and</st> *<st c="28462">df(t)</st>* <st c="28467">is the number
    of documents in which the term</st> *<st c="28513">t</st>* <st c="28514">appears.</st>
    <st c="28524">The bigger the value of</st> *<st c="28548">df(t)</st>*<st c="28553">,
    the lower the weighting for the term.</st> <st c="28593">The importance of a word
    will be high if it appears a lot of times in a text (high</st> *<st c="28676">tf</st>*<st
    c="28678">) or few times a</st><st c="28695">cross texts (</st><st c="28709">high</st>
    *<st c="28715">idf</st>*<st c="28718">).</st>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28416">在这里，</st> *<st c="28422">n</st>* <st c="28423">是文档的总数，</st> *<st
    c="28462">df(t)</st>* <st c="28467">是包含术语</st> *<st c="28513">t</st>* <st c="28514">的文档数量。</st>
    <st c="28524">*<st c="28548">df(t)</st>*<st c="28553">的值越大，该术语的权重越低。</st> <st
    c="28593">如果一个词在文本中频繁出现（高</st> *<st c="28676">tf</st>*<st c="28678">），或者在不同文本中很少出现（</st><st
    c="28695">高</st> *<st c="28709">idf</st>*<st c="28718">），那么这个词的重要性就会很高。</st>
- en: <st c="28721">Note</st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28721">注意</st>
- en: <st c="28726">TF-IDF can be used together with n-grams.</st> <st c="28769">Similarly,
    to weigh an n-gram, we compound the n-gram frequency in a certain document with
    the frequency of the n-gram</st> <st c="28888">across documents.</st>
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28726">TF-IDF可以与n-gram一起使用。</st> <st c="28769">同样，为了权衡一个n-gram，我们将某个文档中的n-gram频率与文档间n-gram的频率相乘。</st>
- en: <st c="28905">In this recipe, we will learn how to extract features u</st><st
    c="28961">sing TF-IDF with or without n-grams</st> <st c="28998">using</st> `<st
    c="29004">scikit-learn</st>`<st c="29016">.</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28905">在这个菜谱中，我们将学习如何使用</st><st c="28961">scikit-learn</st> <st c="28998">提取特征，使用或不使用n-gram。</st>
- en: <st c="29017">Getting ready</st>
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="29017">准备就绪</st>
- en: '`<st c="29031">scikit-learn</st>` <st c="29044">uses a slightly different way
    to calculate the</st> <st c="29092">IDF statistic:</st>'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="29031">scikit-learn</st>` <st c="29044">使用一种稍微不同的方式来计算IDF统计量：</st>'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi
    mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math>](img/49.png)'
- en: <st c="29134">This formulation ensures that a word that appears in all texts
    receives the lowest weight of 1\.</st> <st c="29230">In addition, after calculating
    the TF-IDF for every word,</st> `<st c="29288">scikit-learn</st>` <st c="29300">normalizes
    the feature vector (that wit</st><st c="29340">h all the words) to its Euclidean
    norm.</st> <st c="29381">For more details on the exact formula, visit the</st>
    `<st c="29430">scikit-learn</st>` <st c="29442">documentation</st> <st c="29457">at</st>
    [<st c="29460">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</st>](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)<st
    c="29544">.</st>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29134">这个公式确保了出现在所有文本中的词都得到最低的权重1。</st> <st c="29230">此外，在计算每个词的TF-IDF之后，</st>
    `<st c="29288">scikit-learn</st>` <st c="29300">将（包含所有词的）特征向量归一化到其欧几里得范数。</st>
    <st c="29381">有关确切公式的更多详细信息，请访问</st> `<st c="29430">scikit-learn</st>` <st c="29442">文档</st>
    <st c="29457">在</st> [<st c="29460">https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting</st>](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)<st
    c="29544">。</st>
- en: <st c="29545">TF-IDF shares the characteristics of BoW when creating the term
    matrix – that is, high feature space and sparsity.</st> <st c="29661">To reduce
    the number of features and sparsity, we can remove stop words, set the characters
    to lowercase, and retain words that appear in a minimum percentage</st> <st c="29819">of
    observations.</st> <st c="29837">If you are unfamiliar with these terms, visit
    the</st> *<st c="29887">Creating features with bag-of-words and n-grams</st>*
    <st c="29934">recipe in this chapter for</st> <st c="29962">a recap.</st>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29970">In this recipe, we will learn how to set words into lowercase,
    remove stop words, retain words with a minimum acceptable frequency, capture n-grams,
    and then return the TF-IDF statistic of words, all using a</st> <st c="30179">single
    transformer from</st> <st c="30203">scikit-learn:</st> `<st c="30217">TfidfVectorizer()</st>`<st
    c="30234">.</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30235">How to do it...</st>
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30251">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="30315">dataset ready:</st>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30329">Load</st> `<st c="30335">pandas</st>`<st c="30341">,</st> `<st
    c="30343">TfidfVectorizer()</st>`<st c="30360">, and the dataset</st> <st c="30378">from</st>
    `<st c="30383">scikit-learn</st>`<st c="30395">:</st>
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: <st c="30530">Let’s load the train set part of the 20 Newsgroup dataset into
    a</st> <st c="30596">pandas DataFrame:</st>
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: <st c="30702">To make interpreting the results easier, let’s remove punctuation
    and numbers from the</st> <st c="30790">text variable:</st>
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: <st c="30902">Now, let’s set up</st> `<st c="30921">TfidfVectorize</st><st c="30935">r()</st>`
    <st c="30939">from</st> `<st c="30945">scikit-learn</st>` <st c="30957">so that,
    before creating the TF-IDF metrics, it puts all text in lowercase, removes stop
    words, and</st> <st c="31058">retains words that appear in at least 5% of the</st>
    <st c="31106">text pieces:</st>
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: <st c="31219">Note</st>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31224">To introduce n-grams as part of the returned columns, we can change
    the value of</st> `<st c="31306">ngrams_range</st>` <st c="31318">to, for example,</st>
    `<st c="31336">(1,2)</st>`<st c="31341">. The tuple provides the lower and upper
    boundaries of the range of n-values for different n-grams.</st> <st c="31441">In
    the case of</st> `<st c="31456">(1,2)</st>`<st c="31461">,</st> `<st c="31463">TfidfVectorizer()</st>`
    <st c="31480">will return single words and arrays of two consecutive words</st>
    <st c="31542">as columns.</st>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31553">Let’s fit</st> `<st c="31564">TfidfVectorizer()</st>` <st c="31581">so
    that it learns which words should be introduced as columns of the TF-IDF matrix
    and determines the</st> <st c="31684">words’</st> `<st c="31691">idf</st>`<st
    c="31694">:</st>
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: <st c="31723">Now, let’s create the</st> <st c="31746">TF-IDF matrix:</st>
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: <st c="31797">Finally, let’s capture the TF-IDF matrix in a DataFrame with the
    corresponding</st> <st c="31877">feature names:</st>
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: <st c="31973">With that, we</st> <st c="31988">have created a</st> `<st c="32003">pandas</st>`
    <st c="32009">DataFrame that contains words as columns and the TF-IDF as valu</st><st
    c="32073">es.</st> <st c="32078">You can inspect the result by</st> <st c="32108">executing</st>
    `<st c="32118">tfidf.head()</st>`<st c="32130">:</st>
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 – A DataFrame with features resulting from TF-IDF](img/B22396_11_07.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: <st c="32918">Figure 11.7 – A DataFrame with features resulting from TF-IDF</st>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32979">Now, we can use this t</st><st c="33002">erm frequency DataFrame
    to train machine</st> <st c="33044">lea</st><st c="33047">rning models.</st>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33061">How it works...</st>
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33077">In this recipe, we extracted the TF-IDF values of words present
    in at least 5% of the documents by utilizing</st> `<st c="33187">TfidfVectorizer()</st>`
    <st c="33204">from scikit-learn.</st>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33223">We loaded the 20 Newsgroup text dataset from</st> `<st c="33269">scikit-learn</st>`
    <st c="33281">and then removed punctuation and numbers from the text rows using
    pandas’</st> `<st c="33356">replace()</st>`<st c="33365">, which can be accessed
    through pandas’</st> `<st c="33405">str</st>`<st c="33408">, to replace digits,</st>
    `<st c="33429">'\d+'</st>`<st c="33434">, or symbols,</st> `<st c="33448">'[^\w\s]'</st>`<st
    c="33457">, with empty strings,</st> `<st c="33479">''</st>`<st c="33481">. Then,
    we used</st> `<st c="33497">TfidfVectorizer()</st>` <st c="33514">to create TF-IDF
    statistics for words.</st> <st c="33554">We set the</st> `<st c="33565">lowercase</st>`
    <st c="33574">parameter to</st> `<st c="33588">True</st>` <st c="33592">to put
    words into lowercase before making the calculations.</st> <st c="33653">We set
    the</st> `<st c="33664">stop_words</st>` <st c="33674">argument to</st> `<st c="33687">english</st>`
    <st c="33694">to avoid stop words in the returned matrix.</st> <st c="33739">We
    set</st> `<st c="33746">ngram_range</st>` <st c="33757">to the</st> `<st c="33765">(1,1)</st>`
    <st c="33770">tuple to return single words as features.</st> <st c="33813">Finally,
    we set the</st> `<st c="33833">min_df</st>` <st c="33839">argument to</st> `<st
    c="33852">0.05</st>` <st c="33856">to return words that appear at least in 5%
    of the texts or, in other words, in 5% of</st> <st c="33942">the rows.</st>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33951">After setting up the transformer, we applied the</st> `<st c="34001">fi</st><st
    c="34003">t()</st>` <st c="34007">method to let the transformer find the words
    to</st> <st c="34055">retain in the final term matrix.</st> <st c="34089">With
    the</st> `<st c="34098">transform()</st>` <st c="34109">method, the transformer
    returned an object with the words and their TF-IDF values, which we then captured
    in a pandas DataFrame with the appropriate feature names.</st> <st c="34274">We</st>
    <st c="34277">can now use these features in machine</st> <st c="34315">learning
    algorithms.</st>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34335">See als</st><st c="34343">o</st>
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="34345">For more details</st> <st c="34361">on</st> `<st c="34365">TfidfVectorizer()</st>`<st
    c="34382">, visit scikit-learn’s</st> <st c="34405">documentation:</st> [<st c="34420">https://scikit-learn.org/stable/modules/gen</st><st
    c="34463">erated/sklearn.feature_extraction.text.TfidfVectori</st><st c="34515">zer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="34345">有关</st> `<st c="34365">TfidfVectorizer()</st>` *<st c="34382">的更多详细信息，请访问scikit-learn的文档：[<st
    c="34420">https://scikit-learn.org/stable/modules/gen</st><st c="34463">erated/sklearn.feature_extraction.text.TfidfVectori</st><st
    c="34515">zer.html</st>](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)*'
- en: <st c="34524">Cleani</st><st c="34531">ng and stemming text variables</st>
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*<st c="34524">清理和词干提取文本变量</st>*'
- en: <st c="34562">Some variables in our</st> <st c="34584">dataset come from free
    text fields, which are manually</st> <st c="34640">completed by users.</st> <st
    c="34660">People have different writing styles, and we use a variety of punctuation
    marks, capitalization patterns, and verb conjugations to convey the content, as
    well as the emotions surrounding it.</st> <st c="34851">We can extract (some)
    information from text without taking the trouble to read it by creating statistical
    parameters that summarize the text’s complexity, keywords, and relevance of words
    in a document.</st> <st c="35054">We discussed these methods in the previous recipes
    of this chapter.</st> <st c="35122">However, to derive these statistics and aggregated
    features, we should clean the</st> <st c="35202">text</st> <st c="35208">variables
    first.</st>
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中的一些变量来自自由文本字段，这些字段是由用户手动 *<st c="34640">完成的</st>*。人们的写作风格不同，我们使用各种标点符号、大小写模式和动词变位来传达内容及其相关的情感。通过创建总结文本复杂度、关键词和文档中单词相关性的统计参数，我们可以从文本中提取（一些）信息，而无需费心阅读它。我们已经在本章前面的食谱中讨论了这些方法。然而，为了得出这些统计和聚合特征，我们首先应该清理文本变量。
- en: <st c="35224">Text cleaning or preprocessing involves punctuation removal, stop
    word elimination, character case setting, and word stemming.</st> <st c="35352">Punctuation
    removal consists of deleting characters that are not letters, numbers, or spaces;
    in some cases, we also remove numbers.</st> <st c="35485">The elimination of stop
    words refers to removing common words that are used in our language to allow for
    the sentence structure and flow, but that individually convey little or no information.</st>
    <st c="35678">Examples of stop words include articles such as</st> *<st c="35726">the</st>*
    <st c="35729">and</st> *<st c="35734">a</st>* <st c="35735">for the English language,
    as well as pronouns such as</st> *<st c="35790">I</st>*<st c="35791">,</st> *<st
    c="35793">you</st>* <st c="35796">and</st> *<st c="35801">they</st>*<st c="35805">,
    and commonly used verbs in their various conjugations, such as the verbs</st>
    *<st c="35880">to be</st>* <st c="35885">and</st> *<st c="35890">to have</st>*<st
    c="35897">, as well as the auxiliary verbs</st> *<st c="35930">would</st>* <st
    c="35935">and</st> *<st c="35940">do</st>*<st c="35942">.</st>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*<st c="35224">文本清理或预处理包括标点符号删除、停用词消除、字符大小写设置和词干提取。</st> 标点符号删除包括删除不是字母、数字或空格的字符；在某些情况下，我们也会删除数字。停用词消除是指删除在语言中用于句子结构和流畅性的常用词，但它们单独传达的信息很少或没有。停用词的例子包括英语中的冠词，如
    *<st c="35726">the</st>*、*<st c="35729">and</st>* 和 *<st c="35734">a</st>*，以及代词，如
    *<st c="35790">I</st>*、*<st c="35791">,</st> *<st c="35793">you</st>*、*<st c="35796">and</st>
    *<st c="35801">they</st>*，以及各种变位中的常用动词，如动词 *<st c="35880">to be</st>* 和 *<st c="35885">to
    have</st>*，以及助动词 *<st c="35930">would</st>*、*<st c="35935">and</st> *<st c="35940">do</st>*。</st>'
- en: <st c="35943">To allow computers to identify words correctly, it is also necessary
    to set all the words in the same case, since the words</st> *<st c="36068">Toy</st>*
    <st c="36071">and</st> *<st c="36076">toy</st>* <st c="36079">would be identified
    as being different by a computer due to the uppercase</st> *<st c="36154">T</st>*
    <st c="36155">in the</st> <st c="36163">first one.</st>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让计算机正确识别单词，也需要将所有单词设置为相同的格式，因为单词 *<st c="36068">Toy</st>* 和 *<st c="36076">toy</st>*
    由于第一个单词中的大写字母 *<st c="36154">T</st>*，会被计算机识别为不同的单词。
- en: <st c="36173">Finally, to focus on the</st> *<st c="36199">message</st>* <st
    c="36206">of the text, we don’t want computers to consider words differently if
    they show different conjugations.</st> <st c="36311">Hence, we would use word
    stemming as part of the preprocessing pipeline.</st> <st c="36384">Word stemming
    refers to reducing each word to its root or base so that the words</st> *<st c="36465">playing</st>*<st
    c="36472">,</st> *<st c="36474">plays</st>*<st c="36479">, and</st> *<st c="36485">played</st>*
    <st c="36491">become</st> *<st c="36499">play</st>*<st c="36503">, which, in essence,
    conveys the same or very</st> <st c="36549">similar meaning.</st>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了专注于文本的 *<st c="36199">信息</st>* <st c="36206">，我们不希望计算机在单词有不同的变形时将单词视为不同。因此，我们将使用词干提取作为预处理流程的一部分。词干提取是指将每个单词缩减到其根或基本形式，以便单词
    *<st c="36465">playing</st>*<st c="36472">,</st> *<st c="36474">plays</st>*<st
    c="36479">, 和 *<st c="36485">played</st>* <st c="36491">变成 *<st c="36499">play</st>*<st
    c="36503">，本质上传达了相同或非常 <st c="36549">相似的意义。</st>
- en: <st c="36565">In this recipe, we will learn how to remove punctuation and stop
    words, set words</st> <st c="36648">in lowercase, and perform word stemming with
    pandas</st> <st c="36700">and</st> `<st c="36704">NLTK</st>`<st c="36708">.</st>
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将学习如何使用 pandas 和 `<st c="36704">NLTK</st>`<st c="36708">.</st> 移除标点符号和停用词，将单词转换为小写，并执行词干提取。
- en: <st c="36709">Getting ready</st>
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: <st c="36723">We are going to use the</st> `<st c="36748">NLTK</st>` <st c="36752">stem
    package to perform word stemming, which incorporates different algorithms to stem
    words from English and other languages.</st> <st c="36880">Each method differs
    in the algorithm it uses to find the</st> *<st c="36937">root</st>* <st c="36941">of
    the word; therefore, they may output slightly different results.</st> <st c="37010">I
    recommend reading more about it, trying different methods, and</st> <st c="37075">choosing</st>
    <st c="37084">the</st> <st c="37088">one that serves the project you are</st>
    <st c="37124">working on.</st>
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `<st c="36748">NLTK</st>` <st c="36752">词干提取包来执行词干提取，它结合了不同的算法来从英语和其他语言中提取词干。每种方法都使用不同的算法来找到单词的
    *<st c="36937">根</st>* <st c="36941">，因此它们可能输出略微不同的结果。我建议您了解更多相关信息，尝试不同的方法，并选择适合您正在工作的项目的那个。</st>
    <st c="37010">I recommend reading more about it, trying different methods, and</st>
    <st c="37075">choosing</st> <st c="37084">the</st> <st c="37088">one that serves
    the project you are</st> <st c="37124">working on.</st>
- en: <st c="37135">More information about NLTK st</st><st c="37166">emmers can be
    found</st> <st c="37187">at</st> [<st c="37190">https://www.nlt</st><st c="37205">k.org/api/nltk.stem.html</st>](https://www.nltk.org/api/nltk.stem.html)<st
    c="37230">.</st>
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `<st c="37135">NLTK</st>`<st c="37166">词干提取器</st> 的更多信息可以在 [<st c="37190">https://www.nlt</st><st
    c="37205">k.org/api/nltk.stem.html</st>](https://www.nltk.org/api/nltk.stem.html)<st
    c="37230">.</st> 找到。
- en: <st c="37231">How to do it...</st>
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: <st c="37247">Let’s begin by loading the necessary libraries and getting the</st>
    <st c="37311">dataset ready:</st>
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载必要的库并准备好数据集：
- en: <st c="37325">Load</st> `<st c="37331">pandas</st>`<st c="37337">,</st> `<st
    c="37339">stopwords</st>`<st c="37348">, and</st> `<st c="37354">SnowballStemmer</st>`
    <st c="37369">from</st> `<st c="37375">NLTK</st>` <st c="37379">and the dataset</st>
    <st c="37396">from</st> `<st c="37401">scikit-learn</st>`<st c="37413">:</st>
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 `<st c="37331">pandas</st>`<st c="37337">,</st> `<st c="37339">stopwords</st>`<st
    c="37348">, 和 `<st c="37354">SnowballStemmer</st>` <st c="37369">从 `<st c="37375">NLTK</st>`
    <st c="37379">以及数据集从 `<st c="37396">scikit-learn</st>`<st c="37413">:</st>
- en: '[PRE35]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: <st c="37564">Let’s load the train set part of the 20 Newsgroup dataset into
    a</st> <st c="37630">pandas DataFrame:</st>
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将 20 个新闻组数据集的训练集部分加载到一个 pandas DataFrame 中：
- en: '[PRE36]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: <st c="37736">Now, let’s begin with the</st> <st c="37763">text cleaning.</st>
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们开始进行文本清理。
- en: <st c="37777">Note</st>
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: <st c="37782">After executing each of the commands in this recipe, print some
    example texts by executing, for example,</st> `<st c="37888">print(df['text'][10])</st>`
    <st c="37909">so that you can visualize the changes introduced to the text.</st>
    <st c="37972">Go ahead and do it now, and then repeat the command after</st> <st
    c="38030">each step.</st>
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行这个菜谱中的每个命令后，通过执行例如 `<st c="37888">print(df['text'][10])</st>` <st c="37909">来打印一些示例文本，以便您可以可视化对文本所做的更改。</st>
    <st c="37972">Go ahead and do it now, and then repeat the command after</st> <st
    c="38030">每个步骤。</st>
- en: <st c="38040">Let’s begin by removing</st> <st c="38065">the punctuation:</st>
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们移除标点符号：
- en: '[PRE37]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: <st c="38131">Tip</st>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: <st c="38135">You can also remove the punctuation using the built-in</st> `<st
    c="38191">string</st>` <st c="38197">module from Python.</st> <st c="38218">First,
    import the module by executing</st> `<st c="38256">import string</st>` <st c="38269">and
    then execute</st> `<st c="38287">df['text'] =</st>` `<st c="38300">df['text'].str.replace('[{}]</st><st
    c="38328">'.format(string.punctuatio</st><st c="38355">n), '')</st>`<st c="38363">.</st>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38364">We can also</st> <st c="38376">remove</st> <st c="38384">characters
    that are numbers, leaving only letters,</st> <st c="38435">as follows:</st>
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: <st c="38506">Now, let’s set all words</st> <st c="38532">into lowercase:</st>
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: <st c="38583">Now, let’s start the p</st><st c="38606">rocess of removing</st>
    <st c="38626">stop words.</st>
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="38637">Note</st>
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '*<st c="38642">Step 6</st>* <st c="38649">may fail if you did not download
    the</st> `<st c="38687">NLTK</st>` <st c="38691">library’s</st> `<st c="38702">stopwords</st>`<st
    c="38711">. Visit the</st> *<st c="38723">Technical requirements</st>* <st c="38745">section
    in this chapter for</st> <st c="38774">more details.</st>'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38787">Let’s create a function that splits a string into a list of words,
    removes the stop words, and finally concatenates the remaining words back into</st>
    <st c="38934">a string:</st>
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: <st c="39114">Note</st>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39119">To be able to process the data with the</st> `<st c="39160">scikit-learn</st>`
    <st c="39172">librar</st><st c="39179">y’s</st> `<st c="39184">CountVectorizer()</st>`
    <st c="39201">or</st> `<st c="39205">TfidfVecto</st><st c="39215">rizer()</st>`<st
    c="39223">, we need the text to be in string format.</st> <st c="39266">Therefore,
    after removing the stop words, we need to return the words as a single string.</st>
    <st c="39356">We have transformed the NLTK library’s stop words list into a set
    because sets are faster to scan than lists.</st> <st c="39466">This improves the</st>
    <st c="39484">computation time.</st>
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39501">Now, let’s use</st> <st c="39516">the</st> <st c="39521">function
    from</st> *<st c="39535">step 6</st>* <st c="39541">to remove stop words from
    the</st> `<st c="39572">text</st>` <st c="39576">variable:</st>
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: <st c="39634">If you want to know which words are</st> <st c="39670">stop word</st><st
    c="39680">s,</st> <st c="39684">execute</st> `<st c="39692">stopwords.words('english')</st>`<st
    c="39718">.</st>
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="39719">Finally, let’s stem the words in our data.</st> <st c="39763">We
    will use</st> `<st c="39775">SnowballStemmer</st>` <st c="39790">from</st> `<st
    c="39796">NLTK</st>` <st c="39800">to</st> <st c="39804">do so.</st>
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="39810">Let’s create an instance of</st> `<st c="39839">SnowballStemer</st>`
    <st c="39853">for the</st> <st c="39862">English language:</st>
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: <st c="39916">Tip</st>
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39920">Try the stemmer in a single word to see how it works; for example,
    run</st> `<st c="39992">stemmer.stem('running')</st>`<st c="40015">. You should
    see</st> `<st c="40032">run</st>` <st c="40035">as the result of that command.</st>
    <st c="40067">Try</st> <st c="40071">different words!</st>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39920">尝试在单个单词上使用词干提取器来查看其工作方式；例如，运行</st> `<st c="39992">stemmer.stem('running')</st>`<st
    c="40015">。你应该看到</st> `<st c="40032">run</st>` <st c="40035">作为该命令的结果。</st> <st
    c="40067">尝试</st> <st c="40071">不同的单词！</st>
- en: <st c="40087">Let’s create a function that splits a string into a list of words,
    applies</st> `<st c="40163">stemmer</st>` <st c="40170">to each word, and finally
    concatenates the stemmed word list back into</st> <st c="40242">a string:</st>
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="40087">让我们创建一个函数，该函数将字符串分割成单词列表，将</st> `<st c="40163">stemmer</st>` <st
    c="40170">应用于每个单词，并将还原的单词列表重新连接成字符串：</st> <st c="40242">一个字符串：</st>
- en: '[PRE43]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: <st c="40374">Let’s use</st> <st c="40384">the</st> <st c="40389">function from</st>
    *<st c="40403">step 9</st>* <st c="40409">to stem the words in</st> <st c="40431">our
    data:</st>
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: <st c="40374">让我们使用</st> <st c="40384">从</st> <st c="40389">步骤9</st> <st c="40403">中的函数</st>
    <st c="40409">来还原我们数据中的单词：</st> <st c="40431">我们的数据：</st>
- en: '[PRE44]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: <st c="40483">Now, our text is ready to create features based on character and
    word counts, as well as create BoWs or TF-IDF matrices, as described in the previous
    recipes of</st> <st c="40645">this chapter.</st>
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="40483">现在，我们的文本已经准备好根据字符和单词计数创建特征，以及创建BoWs或TF-IDF矩阵，正如本章前面的食谱中所述。</st>
    <st c="40645">这一章。</st>
- en: <st c="40658">If we execute</st> `<st c="40673">print(df['text'][10])</st>`<st
    c="40694">, we will see a text example</st> <st c="40723">after cleaning:</st>
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="40658">如果我们执行</st> `<st c="40673">print(df['text'][10])</st>`<st c="40694">，我们将看到清理后的文本示例：</st>
    <st c="40723">清理后：</st>
- en: '[PRE45]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: <st c="41230">Note</st>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41230">注意</st>
- en: <st c="41235">If you are counting sentences, you need to do that before removing
    punctuation, as punctuation and</st> <st c="41334">capitalization are needed to
    define the b</st><st c="41376">oundaries of</st> <st c="41390">each sentence.</st>
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41235">如果你正在计算句子数，你需要在移除标点符号之前这样做，因为标点和</st> <st c="41334">大写字母是定义每个句子边界的必要条件。</st>
- en: <st c="41404">How it works...</st>
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="41404">它是如何工作的...</st>
- en: <st c="41420">In this recipe, we</st> <st c="41440">removed punctuation, numbers,
    and stop words from a text variable, set the words in lowercase, and finally,
    stemmed the words to their root.</st> <st c="41581">We removed punctuation and
    numbers from the text variable using pandas’</st> `<st c="41653">replace()</st>`<st
    c="41662">, which can be accessed through pandas’</st> `<st c="41702">str</st>`<st
    c="41705">, to replace digits,</st> `<st c="41726">'\d+'</st>`<st c="41731">,
    or symbols,</st> `<st c="41745">'[^\w\s]'</st>`<st c="41754">, with empty strings,</st>
    `<st c="41776">''</st>`<st c="41778">. Alternatively, we can use the</st> `<st
    c="41810">punctuation</st>` <st c="41821">module from the built-in</st> `<st c="41847">string</st>`
    <st c="41853">package.</st>
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41420">在这个食谱中，我们从文本变量中移除了标点符号、数字和停用词，将单词转换为小写，最后将单词还原到词根。</st> <st c="41440">我们使用pandas的</st>
    `<st c="41653">replace()</st>`<st c="41662">从文本变量中移除了标点符号和数字，这可以通过pandas的</st>
    `<st c="41702">str</st>`<st c="41705">访问，用于替换数字，</st> `<st c="41726">'\d+'</st>`<st
    c="41731">，或符号，</st> `<st c="41745">'[^\w\s]'</st>`<st c="41754">，为空字符串，</st>
    `<st c="41776">''</st>`<st c="41778">。或者，我们可以使用内置的</st> `<st c="41810">punctuation</st>`
    <st c="41821">模块从内置的</st> `<st c="41847">string</st>` <st c="41853">包中。</st>
- en: <st c="41862">Tip</st>
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41862">提示</st>
- en: <st c="41866">Run</st> `<st c="41871">string.punctuation</st>` <st c="41889">in
    your Python console after importing</st> `<st c="41929">string</st>` <st c="41935">to
    check out the symbols that will be replaced with</st> <st c="41988">empty strings.</st>
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41866">运行</st> `<st c="41871">string.punctuation</st>` <st c="41889">在导入</st>
    `<st c="41929">string</st>` <st c="41935">后，在你的Python控制台中查看将被替换为空字符串的符号。</st>
    <st c="41988">空字符串。</st>
- en: <st c="42002">Next, utilizing</st> <st c="42018">pandas’ string</st> <st c="42034">processing
    functionality through</st> `<st c="42067">str</st>`<st c="42070">, we set all
    of the words to lowercase with the</st> `<st c="42118">lower()</st>` <st c="42125">method.</st>
    <st c="42134">To remove stop words from the text, we used the</st> `<st c="42182">stopwords</st>`
    <st c="42191">module from</st> `<st c="42204">NLTK</st>`<st c="42208">, which
    contains a list of words that are considered frequent – that is, the stop words.</st>
    <st c="42297">We created a function that takes a string and splits it into a list
    of words using pandas’</st> `<st c="42388">str.split()</st>`<st c="42399">, and
    then, with list comprehension, we looped over the words in the list and retained
    the non-stop words.</st> <st c="42506">Finally, with the</st> `<st c="42524">join()</st>`
    <st c="42530">method, we concatenated the retained words back into a string.</st>
    <st c="42594">We used the built-in Python</st> `<st c="42622">set()</st>` <st
    c="42627">method over the</st> `<st c="42644">NLTK</st>` <st c="42648">stop words
    list to improve computation efficiency since it is faster to iterate over sets
    than over lists.</st> <st c="42756">Finally, with pandas’</st> `<st c="42778">apply()</st>`<st
    c="42785">, we applied the function to each row of our</st> <st c="42830">text
    data.</st>
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42002">接下来，利用</st> <st c="42018">pandas 的字符串</st> <st c="42034">处理功能通过</st>
    `<st c="42067">str</st>`<st c="42070">，我们使用</st> `<st c="42118">lower()</st>`
    <st c="42125">方法将所有单词转换为小写。</st> <st c="42134">为了从文本中移除停用词，我们使用了来自</st> `<st c="42182">stopwords</st>`
    <st c="42191">模块的</st> `<st c="42204">NLTK</st>`<st c="42208">，其中包含了一组频繁出现的单词列表——即停用词。</st>
    <st c="42297">我们创建了一个函数，该函数接受一个字符串并将其使用 pandas 的</st> `<st c="42388">str.split()</st>`<st
    c="42399">分解成单词列表，然后，使用列表推导式，我们遍历列表中的单词并保留非停用词。</st> <st c="42506">最后，使用</st>
    `<st c="42524">join()</st>` <st c="42530">方法，我们将保留的单词重新连接成一个字符串。</st> <st c="42594">我们使用
    Python 内置的</st> `<st c="42622">set()</st>` <st c="42627">方法对</st> `<st c="42644">NLTK</st>`
    <st c="42648">停用词列表进行操作，以提高计算效率，因为遍历集合比遍历列表要快。</st> <st c="42756">最后，使用 pandas
    的</st> `<st c="42778">apply()</st>`<st c="42785">，我们将该函数应用于我们的</st> <st c="42830">文本数据的每一行。</st>
- en: <st c="42840">Tip</st>
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42840">提示</st>
- en: <st c="42844">Run</st> `<st c="42849">stopwords.words('english')</st>` <st c="42875">in
    your Python console after importing</st> `<st c="42915">stopwords</st>` <st c="42924">from</st>
    `<st c="42930">NLTK</st>` <st c="42934">to visualize the list with the stop words
    that will</st> <st c="42987">be removed.</st>
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42844">在导入</st> `<st c="42849">stopwords.words('english')</st>` <st c="42875">到您的
    Python 控制台后运行，</st> `<st c="42915">stopwords</st>` <st c="42924">来自</st> `<st
    c="42930">NLTK</st>` <st c="42934">，以可视化将要被移除的停用词列表。</st>
- en: <st c="42998">Finally, we stemmed the words using</st> `<st c="43035">SnowballStemmer</st>`
    <st c="43050">from</st> `<st c="43056">NLTK</st>`<st c="43060">.</st> `<st c="43062">SnowballStemmer</st>`
    <st c="43077">works one word at a time.</st> <st c="43104">Therefore, we created
    a function that takes a string and splits it into a list of words using pandas’</st>
    `<st c="43206">str.split()</st>`<st c="43217">. In a list comprehension, we applied</st>
    `<st c="43255">SnowballStemmer</st>` <st c="43270">word per word and then concatenated
    the list of stemmed words back into a string using the</st> `<st c="43362">join()</st>`
    <st c="43368">method.</st> <st c="43377">With pandas’</st> `<st c="43390">apply()</st>`<st
    c="43397">, we applied the</st> <st c="43413">function to stem words to each row
    of</st> <st c="43452">the DataFrame.</st>
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42998">最后，我们使用</st> `<st c="43035">SnowballStemmer</st>` <st c="43050">从</st>
    `<st c="43056">NLTK</st>`<st c="43060">.</st> `<st c="43062">SnowballStemmer</st>`
    <st c="43077">逐个单词进行处理。</st> <st c="43104">因此，我们创建了一个函数，该函数接受一个字符串并将其使用 pandas
    的</st> `<st c="43206">str.split()</st>`<st c="43217">. 在列表推导式中，我们逐个单词应用</st> `<st
    c="43255">SnowballStemmer</st>` <st c="43270">，然后将分解后的单词列表重新连接成一个字符串，使用的是</st>
    `<st c="43362">join()</st>` <st c="43368">方法。</st> <st c="43377">使用 pandas 的</st>
    `<st c="43390">apply()</st>`<st c="43397">，我们将该函数应用于 DataFrame 的每一行中的单词。</st>
- en: <st c="43466">The c</st><st c="43472">leaning steps we performed in this recipe
    resulted in strings containing the original text, without punctuation or numbers,
    in lowercase, without common words, and with the root of the word instead of its
    conjugated form.</st> <st c="43695">The data, as it is returned, can be used to
    derive features, as described in the</st> *<st c="43776">Counting characters,
    words, and vocabulary</st>* <st c="43818">recipe, or to create BoWs and TI-IDF
    matrices, as described in the</st> *<st c="43886">Creating features with bag-of-words
    and n-g</st><st c="43929">rams</st>* <st c="43934">and</st> *<st c="43939">Implementing
    term frequency-inverse document</st>* *<st c="43984">frequency</st>* <st c="43993">recipes.</st>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43466">我们在本食谱中执行的预处理步骤产生了包含原始文本的字符串，没有标点符号或数字，全部小写，没有常用词，并且使用词根而不是其屈折形式。</st>
    <st c="43695">返回的数据可以用来推导特征，如</st> *<st c="43776">计数字符、单词和词汇</st>* <st c="43818">食谱中所述，或者创建BoWs和TI-IDF矩阵，如</st>
    *<st c="43886">使用词袋和n-gram创建特征</st><st c="43929">食谱</st> <st c="43934">和</st>
    *<st c="43939">实现词频-逆文档频率</st> * *<st c="43984">频率</st> * <st c="43993">食谱</st>中所述。
- en: <st c="44002">Cleaning the texts as</st> <st c="44024">we</st> <st c="44028">have
    shown in this recipe can incur data loss, depending on the characteristics of
    the text, and if we seek to interpret the models after creating BoW or TF-IDF
    matrices, understanding the importance of stemmed words may not be</st> <st c="44256">so
    straightforward.</st>
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44002">正如我们在本食谱中所示，清理文本可能会造成数据丢失，这取决于文本的特征，如果我们希望在创建BoW或TF-IDF矩阵后解释模型，理解词根的重要性可能并不</st>
    <st c="44256">那么直接。</st>
