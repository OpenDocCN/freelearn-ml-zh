- en: Chapter 6. Bayesian Classification Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We introduced the classification machine learning task in [Chapter 4](part0034.xhtml#aid-10DJ42
    "Chapter 4. Machine Learning Using Bayesian Inference"), *Machine Learning Using
    Bayesian Inference*, and said that the objective of classification is to assign
    a data record into one of the predetermined classes. Classification is one of
    the most studied machine learning tasks and there are several well-established
    state of the art methods for it. These include logistic regression models, support
    vector machines, random forest models, and neural network models. With sufficient
    labeled training data, these models can achieve accuracies above 95% in many practical
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the obvious question is, why would you need to use Bayesian methods for
    classification? There are two answers to this question. One is that often it is
    difficult to get a large amount of labeled data for training. When there are hundreds
    or thousands of features in a given problem, one often needs a large amount of
    training data for these supervised methods to avoid overfitting. Bayesian methods
    can overcome this problem through Bayesian averaging and hence require only a
    small to medium size training data. Secondly, most of the methods, such as SVM
    or NN, are like black box machines. They will give you very accurate results,
    but little insight as to which variables are important for the example. Often,
    in many practical problems, for example, in the diagnosis of a disease, it is
    important to identify leading causes. Therefore, a black box approach would not
    be sufficient. Bayesian methods have an inherent feature called **Automatic Relevance
    Determination** (**ARD**) by which important variables in a problem can be identified.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, two Bayesian classification models will be discussed. The first
    one is the popular Naïve Bayes method for text classification. The second is the
    Bayesian logistic regression model. Before we discuss each of these models, let's
    review some of the performance metrics that are commonly used in the classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the concepts easily, let''s take the case of binary classification,
    where the task is to classify an input feature vector into one of the two states:
    -1 or 1\. Assume that 1 is the positive class and -1 is the negative class. The
    predicted output contains only -1 or 1, but there can be two types of errors.
    Some of the -1 in the test set could be predicted as 1\. This is called a **false
    positive or type I** error. Similarly, some of the 1 in the test set could be
    predicted as -1\. This is called a **false negative or type II** error. These
    two types of errors can be represented in the case of binary classification as
    a confusion matrix as shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Confusion Matrix | Predicted Class |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Positive | Negative |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Actual Class | Positive | TP | FN |'
  prefs: []
  type: TYPE_TB
- en: '| Negative | FP | TN |'
  prefs: []
  type: TYPE_TB
- en: 'From the confusion matrix, we can derive the following performance metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: ![Performance metrics for classification](img/image00453.jpeg)
    This gives the percentage of correct answers in the output predicted as positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: ![Performance metrics for classification](img/image00454.jpeg)
    This gives the percentage of positives in the test data set that have been correctly
    predicted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F-Score**: ![Performance metrics for classification](img/image00455.jpeg)
    This is the geometric mean of precision and recall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True positive rate**: ![Performance metrics for classification](img/image00456.jpeg)
    This is the same as recall'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive rate**: ![Performance metrics for classification](img/image00457.jpeg)
    This gives the percentage of negative classes classified as positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, *Tpr* is called *sensitivity* and *1 - Fpr* is called *specificity* of
    the classifier. A plot of Tpr versus Fpr (*sensitivity* versus *1 - specificity*)
    is called an **ROC** curve (it stands for **receiver operating characteristic**
    curve). This is used to find the best threshold (operating point of the classifier)
    for deciding whether a predicted output (usually a score or probability) belongs
    to class 1 or -1.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the threshold is taken as the inflation point of the ROC curve that
    gives the best performance with the least false predictions. The area under the
    ROC curve or AUC is another measure of classifier performance. For a purely random
    model, the ROC curve will be a straight line along the diagonal. The corresponding
    value of AUC will be 0.5\. Classifiers with AUC above 0.8 will be considered as
    good, though this very much depends on the problem to be solved.
  prefs: []
  type: TYPE_NORMAL
- en: The Naïve Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The name Naïve Bayes comes from the basic assumption in the model that the
    probability of a particular feature ![The Naïve Bayes classifier](img/image00458.jpeg)
    is independent of any other feature ![The Naïve Bayes classifier](img/image00459.jpeg)
    given the class label ![The Naïve Bayes classifier](img/image00460.jpeg). This
    implies the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00461.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using this assumption and the Bayes rule, one can show that the probability
    of class ![The Naïve Bayes classifier](img/image00460.jpeg), given features ![The
    Naïve Bayes classifier](img/image00462.jpeg), is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00463.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![The Naïve Bayes classifier](img/image00464.jpeg) is the normalization
    term obtained by summing the numerator on all the values of *k*. It is also called
    Bayesian evidence or partition function Z. The classifier selects a class label
    as the target class that maximizes the posterior class probability ![The Naïve
    Bayes classifier](img/image00465.jpeg):'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00466.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The Naïve Bayes classifier is a baseline classifier for document classification.
    One reason for this is that the underlying assumption that each feature (words
    or m-grams) is independent of others, given the class label typically holds good
    for text. Another reason is that the Naïve Bayes classifier scales well when there
    is a large number of documents.
  prefs: []
  type: TYPE_NORMAL
- en: There are two implementations of Naïve Bayes. In Bernoulli Naïve Bayes, features
    are binary variables that encode whether a feature (m-gram) is present or absent
    in a document. In multinomial Naïve Bayes, the features are frequencies of m-grams
    in a document. To avoid issues when the frequency is zero, a Laplace smoothing
    is done on the feature vectors by adding a 1 to each count. Let's look at multinomial
    Naïve Bayes in some detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![The Naïve Bayes classifier](img/image00467.jpeg) be the number of times
    the feature ![The Naïve Bayes classifier](img/image00458.jpeg) occurred in the
    class ![The Naïve Bayes classifier](img/image00468.jpeg) in the training data.
    Then, the likelihood function of observing a feature vector ![The Naïve Bayes
    classifier](img/image00469.jpeg), given a class label ![The Naïve Bayes classifier](img/image00468.jpeg),
    is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00470.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![The Naïve Bayes classifier](img/image00471.jpeg) is the probability
    of observing the feature ![The Naïve Bayes classifier](img/image00458.jpeg) in
    the class ![The Naïve Bayes classifier](img/image00468.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayesian rule, the posterior probability of observing the class ![The
    Naïve Bayes classifier](img/image00468.jpeg), given a feature vector *X*, is given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00472.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Taking logarithm on both the sides and ignoring the constant term *Z*, we get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Naïve Bayes classifier](img/image00473.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So, by taking logarithm of posterior distribution, we have converted the problem
    into a linear regression model with ![The Naïve Bayes classifier](img/image00474.jpeg)
    as the coefficients to be determined from data. This can be easily solved. Generally,
    instead of term frequencies, one uses TF-IDF (term frequency multiplied by inverse
    frequency) with the document length normalized to improve the performance of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The R package **e1071** (*Miscellaneous Functions of the Department of Statistics*)
    by T.U. Wien contains an R implementation of Naïve Bayes. For this chapter, we
    will use the SMS spam dataset from the UCI Machine Learning repository (reference
    1 in the *References* section of this chapter). The dataset consists of 425 SMS
    spam messages collected from the UK forum Grumbletext, where consumers can submit
    spam SMS messages. The dataset also contains 3375 normal (ham) SMS messages from
    the NUS SMS corpus maintained by the National University of Singapore.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be downloaded from the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)).
    Let's say that we have saved this as file `SMSSpamCollection.txt` in the working
    directory of R (actually, you need to open it in Excel and save it is as tab-delimited
    file for it to read in R properly).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the command to read the file into the tm (text mining) package would
    be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will first separate the dependent variable `y` and independent variables
    `x` and split the dataset into training and testing sets in the ratio 80:20, using
    the following R commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since we are dealing with text documents, we need to do some standard preprocessing
    before we can use the data for any machine learning models. We can use the tm
    package in R for this purpose. In the next section, we will describe this in some
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text processing using the tm package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **tm** package has methods for data import, corpus handling, preprocessing,
    metadata management, and creation of term-document matrices. Data can be imported
    into the tm package either from a directory, a vector with each component a document,
    or a data frame. The fundamental data structure in tm is an abstract collection
    of text documents called Corpus. It has two implementations; one is where data
    is stored in memory and is called **VCorpus** (**volatile corpus**) and the second
    is where data is stored in the hard disk and is called **PCorpus** (**permanent
    corpus**).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a corpus of our SMS spam dataset by using the following R commands;
    prior to this, you need to install the tm package and **SnowballC** package by
    using the `install.packages("packagename")` command in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to do some basic text processing, such as removing extra white
    space, changing all words to lowercase, removing stop words, and stemming the
    words. This can be achieved by using the following functions in the tm package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the data is transformed into a form that can be consumed by machine
    learning models. This is the so called document-term matrix form where each document
    (SMS in this case) is a row, the terms appearing in all documents are the columns,
    and the entry in each cell denotes how many times each word occurs in one document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The same set of processes is done on the `xtest` dataset as well. The reason
    we converted *y* to factors and *xtrain* to a data frame is to match the input
    format for the Naïve Bayes classifier in the e1071 package.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You need to first install the e1071 package from CRAN. The `naiveBayes()` function
    can be used to train the Naïve Bayes model. The function can be called using two
    methods. The following is the first method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here `formula` stands for the linear combination of independent variables to
    predict the following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Also, `data` stands for either a data frame or contingency table consisting
    of categorical and numerical variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have the class labels as a vector *y* and dependent variables as a data
    frame *x*, then we can use the second method of calling the function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the second method of calling in our example. Once we have a trained
    model, which is an R object of class `naiveBayes`, we can predict the classes
    of new instances as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we can train the Naïve Bayes model on our training dataset and score on
    the test dataset by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the ROC curve for this model and dataset is shown. This is generated
    using the pROC package in CRAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model training and prediction](img/image00475.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: From the ROC curve and confusion matrix, one can choose the best threshold for
    the classifier, and the precision and recall metrics. Note that the example shown
    here is for illustration purposes only. The model needs be to tuned further to
    improve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also print some of the most frequent words (model features) occurring
    in the two classes and their posterior probabilities generated by the model. This
    will give a more intuitive feeling for the model exercise. The following R code
    does this job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| word | Prob(word&#124;spam) | Prob(word&#124;ham) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| call | 0.6994 | 0.4084 |'
  prefs: []
  type: TYPE_TB
- en: '| free | 0.4294 | 0.3996 |'
  prefs: []
  type: TYPE_TB
- en: '| now | 0.3865 | 0.3120 |'
  prefs: []
  type: TYPE_TB
- en: '| repli | 0.2761 | 0.3094 |'
  prefs: []
  type: TYPE_TB
- en: '| text | 0.2638 | 0.2840 |'
  prefs: []
  type: TYPE_TB
- en: '| spam | 0.2270 | 0.2726 |'
  prefs: []
  type: TYPE_TB
- en: '| txt | 0.2270 | 0.2594 |'
  prefs: []
  type: TYPE_TB
- en: '| get | 0.2209 | 0.2182 |'
  prefs: []
  type: TYPE_TB
- en: '| stop | 0.2086 | 0.2025 |'
  prefs: []
  type: TYPE_TB
- en: The table shows, for example, that given a document is spam, the probability
    of the word *call* appearing in it is 0.6994, whereas the probability of the same
    word appearing in a normal document is only 0.4084.
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name logistic regression comes from the fact that the dependent variable
    of the regression is a logistic function. It is one of the widely used models
    in problems where the response is a binary variable (for example, fraud or not-fraud,
    click or no-click, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'A logistic function is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bayesian logistic regression model](img/image00476.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: It has the particular feature that, as *y* varies from ![The Bayesian logistic
    regression model](img/image00477.jpeg) to ![The Bayesian logistic regression model](img/image00478.jpeg),
    the function value varies from 0 to 1\. Hence, the logistic function is ideal
    for modeling any binary response as the input signal is varied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inverse of the logistic function is called *logit*. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bayesian logistic regression model](img/image00479.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In logistic regression, *y* is treated as a linear function of explanatory
    variables *X*. Therefore, the logistic regression model can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Bayesian logistic regression model](img/image00480.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![The Bayesian logistic regression model](img/image00481.jpeg) is the
    set of basis functions and ![The Bayesian logistic regression model](img/image00482.jpeg)
    are the model parameters as explained in the case of linear regression in [Chapter
    4](part0034.xhtml#aid-10DJ42 "Chapter 4. Machine Learning Using Bayesian Inference"),
    *Machine Learning Using Bayesian Inference*. From the definition of GLM in [Chapter
    5](part0041.xhtml#aid-173721 "Chapter 5. Bayesian Regression Models"), *Bayesian
    Regression Models*, one can immediately recognize that logistic regression is
    a special case of GLM with the **logit** function as the link function.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian treatment of logistic regression is more difficult compared to the
    case of linear regression. Here, the likelihood function consists of a product
    of logistic functions; one for each data point. To compute the posterior, one
    has to normalize this function multiplied by the prior (to get the denominator
    of the Bayes formula). One approach is to use Laplace approximation as explained
    in [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing Bayesian Inference"),
    *Introducing Bayesian Inference*. Readers might recall that in Laplace approximation,
    the posterior is approximated as a Gaussian (normal) distribution about the maximum
    of the posterior. This is achieved by finding the **maximum a posteriori** (**MAP**)
    solution first and computing the second derivative of the negative log likelihood
    around the MAP solution. Interested readers can find the details of Laplace approximation
    to logistic regression in the paper by D.J.C. MacKay (reference 2 in the *References*
    section of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using an analytical approximation, Polson and Scott recently proposed
    a fully Bayesian treatment of this problem using a data augmentation strategy
    (reference 3 in the *References* section of this chapter). The authors have implemented
    their method in the R package: BayesLogit. We will use this package to illustrate
    Bayesian logistic regression in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The BayesLogit R package
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The package can be downloaded from the CRAN website at [http://cran.r-project.org/web/packages/BayesLogit/index.html](http://cran.r-project.org/web/packages/BayesLogit/index.html).
    The package contains the `logit` function that can be used to perform a Bayesian
    logistic regression. The syntax for calling this function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, *Y* is an *N*-dimensional vector containing response values; *X* is an
    *N x P* dimensional matrix containing values of independent variables, *n* is
    an *N*-dimensional vector, ![The BayesLogit R package](img/image00483.jpeg) is
    a *P*-dimensional prior mean, and ![The BayesLogit R package](img/image00484.jpeg)
    is a *P x P* dimensional prior precision. The other two arguments are related
    to MCMC simulation parameters. The number of MCMC simulations saved is denoted
    by `samp` and the number of MCMC simulations discarded at the beginning of the
    run before saving samples is denoted by `burn`.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate Bayesian logistic regression, we use the Parkinsons dataset from
    the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/Parkinsons](https://archive.ics.uci.edu/ml/datasets/Parkinsons)).
    The dataset was used by Little et.al. to detect Parkinson''s disease by analyzing
    voice disorder (reference 4 in the *References* section of this chapter). The
    dataset consists of voice measurements from 31 people, of which 23 people have
    Parkinson''s disease. There are 195 rows corresponding to multiple measurements
    from a single individual. The measurements can be grouped into the following sets:'
  prefs: []
  type: TYPE_NORMAL
- en: The vocal fundamental frequency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jitter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimmer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio of noise to tonal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nonlinear dynamical complexity measures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The signal fractal scaling exponent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nonlinear measures of fundamental frequency variation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In total, there are 22 numerical attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the training and testing datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can train the Bayesian logistic model, we need to do some preprocessing
    of the data. The dataset contains multiple measurements from the same individual.
    Here, we take all observations; each from a sampled set of individuals in order
    to create the training and test sets. Also, we need to separate the dependent
    variable (class label *Y*) from the independent variables (*X*). The following
    R code does this job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Using the Bayesian logistic model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use *xtrain* and *ytrain* to train the Bayesian logistic regression
    model using the `logit( )` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `summary( )` function will give a high-level summary of the fitted model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To predict values of *Y* for a new dataset, we need to write a custom script
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The error of prediction can be computed by comparing it with the actual values
    of *Y* present in *ytest*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'One can plot the ROC curve using the pROC package as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![Using the Bayesian logistic model](img/image00485.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The ROC curve has an AUC of 0.942 suggesting a good classification accuracy.
    Again, the model is presented here to illustrate the purpose and is not tuned
    to obtain maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will use the DBWorld e-mails dataset from the UCI Machine
    Learning repository to compare the relative performance of Naïve Bayes and BayesLogit
    methods. The dataset contains 64 e-mails from the DBWorld newsletter and the task
    is to classify the e-mails into either *announcements of conferences* or *everything
    else*. The reference for this dataset is a course by Prof. Michele Filannino (reference
    5 in the *References* section of this chapter). The dataset can be downloaded
    from the UCI website at [https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#](https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some preprocessing of the dataset would be required to use it for both the methods.
    The dataset is in the ARFF format. You need to download the **foreign** R package
    ([http://cran.r-project.org/web/packages/foreign/index.html](http://cran.r-project.org/web/packages/foreign/index.html))
    and use the `read.arff( )` method in it to read the file into an R data frame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almeida T.A., Gómez Hidalgo J.M., and Yamakami A. "Contributions to the Study
    of SMS Spam Filtering: New Collection and Results". In: 2011 ACM Symposium on
    Document Engineering (DOCENG''11). Mountain View, CA, USA. 2011'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MacKay D.J.C. "The Evidence Framework Applied to Classification Networks". Neural
    Computation 4(5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Bayesian Inference for Logistic Models Using Pólya-Gamma Latent Variables".
    Journal of the American Statistical Association. Volume 108, Issue 504, Page 1339\.
    2013'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Costello D.A.E., Little M.A., McSharry P.E., Moroz I.M., and Roberts S.J. "Exploiting
    Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection".
    BioMedical Engineering OnLine. 2007
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filannino M. "DBWorld e-mail Classification Using a Very Small Corpus". Project
    of Machine Learning Course. University of Manchester. 2011
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the various merits of using Bayesian inference
    for the classification task. We reviewed some of the common performance metrics
    used for the classification task. We also learned two basic and popular methods
    for classification, Naïve Bayes and logistic regression, both implemented using
    the Bayesian approach. Having learned some important Bayesian-supervised machine
    learning techniques, in the next chapter, we will discuss some unsupervised Bayesian
    models.
  prefs: []
  type: TYPE_NORMAL
