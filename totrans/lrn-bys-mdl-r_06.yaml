- en: Chapter 6. Bayesian Classification Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章。贝叶斯分类模型
- en: We introduced the classification machine learning task in [Chapter 4](part0034.xhtml#aid-10DJ42
    "Chapter 4. Machine Learning Using Bayesian Inference"), *Machine Learning Using
    Bayesian Inference*, and said that the objective of classification is to assign
    a data record into one of the predetermined classes. Classification is one of
    the most studied machine learning tasks and there are several well-established
    state of the art methods for it. These include logistic regression models, support
    vector machines, random forest models, and neural network models. With sufficient
    labeled training data, these models can achieve accuracies above 95% in many practical
    problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 4 章](part0034.xhtml#aid-10DJ42 "第 4 章。使用贝叶斯推理进行机器学习")，*使用贝叶斯推理进行机器学习*中介绍了分类机器学习任务，并指出分类的目标是将数据记录分配到预定的类别之一。分类是机器学习中最被研究的任务之一，并且有几种已建立的先进方法。这些包括逻辑回归模型、支持向量机、随机森林模型和神经网络模型。在有足够标记训练数据的情况下，这些模型可以在许多实际问题中实现超过
    95% 的准确率。
- en: Then, the obvious question is, why would you need to use Bayesian methods for
    classification? There are two answers to this question. One is that often it is
    difficult to get a large amount of labeled data for training. When there are hundreds
    or thousands of features in a given problem, one often needs a large amount of
    training data for these supervised methods to avoid overfitting. Bayesian methods
    can overcome this problem through Bayesian averaging and hence require only a
    small to medium size training data. Secondly, most of the methods, such as SVM
    or NN, are like black box machines. They will give you very accurate results,
    but little insight as to which variables are important for the example. Often,
    in many practical problems, for example, in the diagnosis of a disease, it is
    important to identify leading causes. Therefore, a black box approach would not
    be sufficient. Bayesian methods have an inherent feature called **Automatic Relevance
    Determination** (**ARD**) by which important variables in a problem can be identified.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，显然的问题是，为什么你需要使用贝叶斯方法进行分类？对此有两个答案。一是，通常很难获得大量标记数据用于训练。当给定问题中有数百或数千个特征时，通常需要大量训练数据来避免过拟合。贝叶斯方法可以通过贝叶斯平均来克服这个问题，因此只需要少量到中等大小的训练数据。其次，大多数方法，如
    SVM 或 NN，就像黑盒机器。它们会给出非常准确的结果，但很少能洞察到哪些变量对例子很重要。在许多实际问题上，例如疾病的诊断，识别主要原因非常重要。因此，黑盒方法是不够的。贝叶斯方法有一个固有的特性，称为**自动相关性确定（ARD**），可以通过它来识别问题中的重要变量。
- en: In this chapter, two Bayesian classification models will be discussed. The first
    one is the popular Naïve Bayes method for text classification. The second is the
    Bayesian logistic regression model. Before we discuss each of these models, let's
    review some of the performance metrics that are commonly used in the classification
    task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论两种贝叶斯分类模型。第一个是流行的朴素贝叶斯文本分类方法。第二个是贝叶斯逻辑回归模型。在我们讨论这些模型之前，让我们回顾一下在分类任务中常用的一些性能指标。
- en: Performance metrics for classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类性能指标
- en: 'To understand the concepts easily, let''s take the case of binary classification,
    where the task is to classify an input feature vector into one of the two states:
    -1 or 1\. Assume that 1 is the positive class and -1 is the negative class. The
    predicted output contains only -1 or 1, but there can be two types of errors.
    Some of the -1 in the test set could be predicted as 1\. This is called a **false
    positive or type I** error. Similarly, some of the 1 in the test set could be
    predicted as -1\. This is called a **false negative or type II** error. These
    two types of errors can be represented in the case of binary classification as
    a confusion matrix as shown below.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易理解这些概念，让我们以二元分类为例，这里的任务是判断一个输入特征向量属于两种状态之一：-1 或 1。假设 1 是正类，-1 是负类。预测输出只包含
    -1 或 1，但可能存在两种类型的错误。测试集中的某些 -1 可能被预测为 1。这被称为**假阳性或第一类错误**。同样，测试集中的某些 1 可能被预测为
    -1。这被称为**假阴性或第二类错误**。在二元分类的情况下，这两种类型的错误可以用下面的混淆矩阵来表示。
- en: '| Confusion Matrix | Predicted Class |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| 混淆矩阵 | 预测类别 |'
- en: '| --- | --- |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Positive | Negative |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| 正类 | 负类 |'
- en: '| --- | --- |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Actual Class | Positive | TP | FN |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 实际类别 | 正类 | TP | FN |'
- en: '| Negative | FP | TN |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 负类 | FP | TN |'
- en: 'From the confusion matrix, we can derive the following performance metrics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以推导出以下性能指标：
- en: '**Precision**: ![Performance metrics for classification](img/image00453.jpeg)
    This gives the percentage of correct answers in the output predicted as positive'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：![分类性能指标](img/image00453.jpeg) 这给出了输出预测为正的准确答案的百分比'
- en: '**Recall**: ![Performance metrics for classification](img/image00454.jpeg)
    This gives the percentage of positives in the test data set that have been correctly
    predicted'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：![分类性能指标](img/image00454.jpeg) 这给出了测试数据集中正确预测的正类别的百分比'
- en: '**F-Score**: ![Performance metrics for classification](img/image00455.jpeg)
    This is the geometric mean of precision and recall'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F 分数**：![分类性能指标](img/image00455.jpeg) 这是精确度和召回率的几何平均值'
- en: '**True positive rate**: ![Performance metrics for classification](img/image00456.jpeg)
    This is the same as recall'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性率**：![分类性能指标](img/image00456.jpeg) 这与召回率相同'
- en: '**False positive rate**: ![Performance metrics for classification](img/image00457.jpeg)
    This gives the percentage of negative classes classified as positive'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性率**：![分类性能指标](img/image00457.jpeg) 这给出了将负类别分类为正类别的百分比'
- en: Also, *Tpr* is called *sensitivity* and *1 - Fpr* is called *specificity* of
    the classifier. A plot of Tpr versus Fpr (*sensitivity* versus *1 - specificity*)
    is called an **ROC** curve (it stands for **receiver operating characteristic**
    curve). This is used to find the best threshold (operating point of the classifier)
    for deciding whether a predicted output (usually a score or probability) belongs
    to class 1 or -1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*Tpr* 被称为 *灵敏度*，而 *1 - Fpr* 被称为分类器的 *特异性*。Tpr 与 Fpr（灵敏度与 *1 - 特异性*）的图像称为
    **ROC** 曲线（代表 **接收者操作特征** 曲线）。这用于找到决定预测输出（通常是一个分数或概率）属于类别 1 或 -1 的最佳阈值（分类器的操作点）。
- en: Usually, the threshold is taken as the inflation point of the ROC curve that
    gives the best performance with the least false predictions. The area under the
    ROC curve or AUC is another measure of classifier performance. For a purely random
    model, the ROC curve will be a straight line along the diagonal. The corresponding
    value of AUC will be 0.5\. Classifiers with AUC above 0.8 will be considered as
    good, though this very much depends on the problem to be solved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，阈值被取为 ROC 曲线的膨胀点，该曲线给出了最佳性能且错误预测最少。ROC 曲线下方的面积或 AUC 是分类器性能的另一个衡量标准。对于纯随机模型，ROC
    曲线将沿着对角线是一条直线。相应的 AUC 值将为 0.5。AUC 大于 0.8 的分类器将被认为是好的，尽管这很大程度上取决于要解决的问题。
- en: The Naïve Bayes classifier
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: 'The name Naïve Bayes comes from the basic assumption in the model that the
    probability of a particular feature ![The Naïve Bayes classifier](img/image00458.jpeg)
    is independent of any other feature ![The Naïve Bayes classifier](img/image00459.jpeg)
    given the class label ![The Naïve Bayes classifier](img/image00460.jpeg). This
    implies the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes 的名字来源于模型中的基本假设，即特定特征 ![朴素贝叶斯分类器](img/image00458.jpeg) 的概率在给定类别标签
    ![朴素贝叶斯分类器](img/image00459.jpeg) 的情况下与其他任何特征 ![朴素贝叶斯分类器](img/image00460.jpeg)
    独立。这暗示了以下内容：
- en: '![The Naïve Bayes classifier](img/image00461.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00461.jpeg)'
- en: 'Using this assumption and the Bayes rule, one can show that the probability
    of class ![The Naïve Bayes classifier](img/image00460.jpeg), given features ![The
    Naïve Bayes classifier](img/image00462.jpeg), is given by:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个假设和贝叶斯定理，可以证明，在特征 ![朴素贝叶斯分类器](img/image00460.jpeg) 给定的情况下，类别 ![朴素贝叶斯分类器](img/image00462.jpeg)
    的概率如下：
- en: '![The Naïve Bayes classifier](img/image00463.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00463.jpeg)'
- en: 'Here, ![The Naïve Bayes classifier](img/image00464.jpeg) is the normalization
    term obtained by summing the numerator on all the values of *k*. It is also called
    Bayesian evidence or partition function Z. The classifier selects a class label
    as the target class that maximizes the posterior class probability ![The Naïve
    Bayes classifier](img/image00465.jpeg):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![朴素贝叶斯分类器](img/image00464.jpeg) 是通过求和所有 *k* 值的分子得到的归一化项。它也被称为贝叶斯证据或配分函数
    Z。分类器选择一个类别标签作为目标类别，该类别最大化后验类别概率 ![朴素贝叶斯分类器](img/image00465.jpeg)：
- en: '![The Naïve Bayes classifier](img/image00466.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00466.jpeg)'
- en: The Naïve Bayes classifier is a baseline classifier for document classification.
    One reason for this is that the underlying assumption that each feature (words
    or m-grams) is independent of others, given the class label typically holds good
    for text. Another reason is that the Naïve Bayes classifier scales well when there
    is a large number of documents.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是文档分类的基线分类器。其中一个原因是，在给定类别标签的情况下，每个特征（单词或 m-gram）与其他特征相互独立的基本假设通常适用于文本。另一个原因是，当存在大量文档时，朴素贝叶斯分类器的扩展性很好。
- en: There are two implementations of Naïve Bayes. In Bernoulli Naïve Bayes, features
    are binary variables that encode whether a feature (m-gram) is present or absent
    in a document. In multinomial Naïve Bayes, the features are frequencies of m-grams
    in a document. To avoid issues when the frequency is zero, a Laplace smoothing
    is done on the feature vectors by adding a 1 to each count. Let's look at multinomial
    Naïve Bayes in some detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯有两种实现方式。在伯努利朴素贝叶斯中，特征是二进制变量，表示一个特征（m-gram）是否存在于文档中。在多项式朴素贝叶斯中，特征是文档中 m-gram
    的频率。为了避免频率为零时的问题，通过对特征向量添加 1 来进行拉普拉斯平滑。让我们详细看看多项式朴素贝叶斯。
- en: 'Let ![The Naïve Bayes classifier](img/image00467.jpeg) be the number of times
    the feature ![The Naïve Bayes classifier](img/image00458.jpeg) occurred in the
    class ![The Naïve Bayes classifier](img/image00468.jpeg) in the training data.
    Then, the likelihood function of observing a feature vector ![The Naïve Bayes
    classifier](img/image00469.jpeg), given a class label ![The Naïve Bayes classifier](img/image00468.jpeg),
    is given by:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![朴素贝叶斯分类器](img/image00467.jpeg) 为特征 ![朴素贝叶斯分类器](img/image00458.jpeg) 在训练数据中类别
    ![朴素贝叶斯分类器](img/image00468.jpeg) 中出现的次数。那么，给定类别标签 ![朴素贝叶斯分类器](img/image00468.jpeg)
    观察到特征向量 ![朴素贝叶斯分类器](img/image00469.jpeg) 的似然函数如下：
- en: '![The Naïve Bayes classifier](img/image00470.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00470.jpeg)'
- en: Here, ![The Naïve Bayes classifier](img/image00471.jpeg) is the probability
    of observing the feature ![The Naïve Bayes classifier](img/image00458.jpeg) in
    the class ![The Naïve Bayes classifier](img/image00468.jpeg).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![朴素贝叶斯分类器](img/image00471.jpeg) 是在类别 ![朴素贝叶斯分类器](img/image00468.jpeg) 中观察到特征
    ![朴素贝叶斯分类器](img/image00458.jpeg) 的概率。
- en: 'Using Bayesian rule, the posterior probability of observing the class ![The
    Naïve Bayes classifier](img/image00468.jpeg), given a feature vector *X*, is given
    by:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯规则，给定特征向量 *X* 观察到类别 ![朴素贝叶斯分类器](img/image00468.jpeg) 的后验概率如下：
- en: '![The Naïve Bayes classifier](img/image00472.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00472.jpeg)'
- en: 'Taking logarithm on both the sides and ignoring the constant term *Z*, we get
    the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对等式两边取对数并忽略常数项 *Z*，我们得到以下结果：
- en: '![The Naïve Bayes classifier](img/image00473.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/image00473.jpeg)'
- en: So, by taking logarithm of posterior distribution, we have converted the problem
    into a linear regression model with ![The Naïve Bayes classifier](img/image00474.jpeg)
    as the coefficients to be determined from data. This can be easily solved. Generally,
    instead of term frequencies, one uses TF-IDF (term frequency multiplied by inverse
    frequency) with the document length normalized to improve the performance of the
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过对后验分布取对数，我们将问题转化为一个线性回归模型，其中 ![朴素贝叶斯分类器](img/image00474.jpeg) 作为需要从数据中确定的系数。这可以很容易地解决。通常，人们使用
    TF-IDF（词频乘以逆频率）而不是词频，并将文档长度归一化以提高模型性能。
- en: The R package **e1071** (*Miscellaneous Functions of the Department of Statistics*)
    by T.U. Wien contains an R implementation of Naïve Bayes. For this chapter, we
    will use the SMS spam dataset from the UCI Machine Learning repository (reference
    1 in the *References* section of this chapter). The dataset consists of 425 SMS
    spam messages collected from the UK forum Grumbletext, where consumers can submit
    spam SMS messages. The dataset also contains 3375 normal (ham) SMS messages from
    the NUS SMS corpus maintained by the National University of Singapore.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: R 包 **e1071** (*统计学部门的各种函数*) 由 T.U. Wien 编写，其中包含了对朴素贝叶斯的 R 实现。对于本章，我们将使用来自 UCI
    机器学习仓库的 SMS 炎言数据集（本章“参考文献”部分的第 1 个参考文献）。该数据集包含从英国论坛 Grumbletext 收集的 425 条短信垃圾信息，消费者可以在此提交垃圾短信。数据集还包含来自新加坡国立大学维护的
    SMS 语料库的 3375 条正常（非垃圾）短信。
- en: The dataset can be downloaded from the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)).
    Let's say that we have saved this as file `SMSSpamCollection.txt` in the working
    directory of R (actually, you need to open it in Excel and save it is as tab-delimited
    file for it to read in R properly).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集可以从 UCI 机器学习仓库下载（[https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)）。假设我们已经将其保存为
    R 的工作目录中的文件 `SMSSpamCollection.txt`（实际上，您需要将其在 Excel 中打开并保存为制表符分隔的文件，以便 R 正确读取）。
- en: 'Then, the command to read the file into the tm (text mining) package would
    be the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将文件读入 tm（文本挖掘）包的命令如下：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will first separate the dependent variable `y` and independent variables
    `x` and split the dataset into training and testing sets in the ratio 80:20, using
    the following R commands:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将因变量 `y` 和自变量 `x` 分离，并将数据集按 80:20 的比例分为训练集和测试集，使用以下 R 命令：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we are dealing with text documents, we need to do some standard preprocessing
    before we can use the data for any machine learning models. We can use the tm
    package in R for this purpose. In the next section, we will describe this in some
    detail.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们处理的是文本文档，在我们可以使用数据为任何机器学习模型之前，我们需要进行一些标准的预处理。我们可以使用 R 中的 tm 包来完成这个目的。在下一节中，我们将对此进行详细描述。
- en: Text processing using the tm package
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 tm 包进行文本处理
- en: The **tm** package has methods for data import, corpus handling, preprocessing,
    metadata management, and creation of term-document matrices. Data can be imported
    into the tm package either from a directory, a vector with each component a document,
    or a data frame. The fundamental data structure in tm is an abstract collection
    of text documents called Corpus. It has two implementations; one is where data
    is stored in memory and is called **VCorpus** (**volatile corpus**) and the second
    is where data is stored in the hard disk and is called **PCorpus** (**permanent
    corpus**).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**tm** 包提供了数据导入、语料库处理、预处理、元数据管理和创建词-文档矩阵的方法。数据可以导入到 tm 包中，无论是从目录、每个组件都是一个文档的向量，还是从数据框中。tm
    的基本数据结构是一个抽象的文本文档集合，称为 Corpus。它有两种实现方式；一种是将数据存储在内存中，称为 **VCorpus**（**易失性语料库**），另一种是将数据存储在硬盘上，称为
    **PCorpus**（**永久性语料库**）。'
- en: 'We can create a corpus of our SMS spam dataset by using the following R commands;
    prior to this, you need to install the tm package and **SnowballC** package by
    using the `install.packages("packagename")` command in R:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下 R 命令创建我们的 SMS 垃圾邮件数据集的语料库；在此之前，您需要使用 R 中的 `install.packages("packagename")`
    命令安装 tm 包和 **SnowballC** 包：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'First, we need to do some basic text processing, such as removing extra white
    space, changing all words to lowercase, removing stop words, and stemming the
    words. This can be achieved by using the following functions in the tm package:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要进行一些基本的文本处理，例如删除额外的空白字符，将所有单词转换为小写，删除停用词，以及进行词干提取。这可以通过使用 tm 包中的以下函数来实现：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, the data is transformed into a form that can be consumed by machine
    learning models. This is the so called document-term matrix form where each document
    (SMS in this case) is a row, the terms appearing in all documents are the columns,
    and the entry in each cell denotes how many times each word occurs in one document:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据被转换成机器学习模型可以消费的形式。这就是所谓的文档-词矩阵形式，其中每个文档（在这种情况下为 SMS）是一行，所有文档中出现的术语是列，每个单元格中的条目表示每个单词在一个文档中出现的次数：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The same set of processes is done on the `xtest` dataset as well. The reason
    we converted *y* to factors and *xtrain* to a data frame is to match the input
    format for the Naïve Bayes classifier in the e1071 package.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的处理过程也应用于 `xtest` 数据集。我们将 *y* 转换为因子，将 *xtrain* 转换为数据框的原因是为了匹配 e1071 包中朴素贝叶斯分类器的输入格式。
- en: Model training and prediction
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和预测
- en: 'You need to first install the e1071 package from CRAN. The `naiveBayes()` function
    can be used to train the Naïve Bayes model. The function can be called using two
    methods. The following is the first method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要首先从 CRAN 安装 e1071 包。可以使用 `naiveBayes()` 函数来训练朴素贝叶斯模型。该函数可以通过两种方法调用。以下为第一种方法：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here `formula` stands for the linear combination of independent variables to
    predict the following class:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `formula` 代表独立变量的线性组合，用于预测以下类别：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Also, `data` stands for either a data frame or contingency table consisting
    of categorical and numerical variables.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`data`代表一个数据框或由分类和数值变量组成的列联表。
- en: 'If we have the class labels as a vector *y* and dependent variables as a data
    frame *x*, then we can use the second method of calling the function, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个向量*y*作为类别标签和依赖变量作为数据框*x*，那么我们可以使用调用函数的第二种方法，如下所示：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will use the second method of calling in our example. Once we have a trained
    model, which is an R object of class `naiveBayes`, we can predict the classes
    of new instances as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用调用方法的第二种方法。一旦我们有一个训练好的模型，它是一个名为`naiveBayes`的R对象，我们就可以预测新实例的类别，如下所示：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So, we can train the Naïve Bayes model on our training dataset and score on
    the test dataset by using the following commands:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以在我们的训练数据集上训练朴素贝叶斯模型，并使用以下命令在测试数据集上进行评分：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, the ROC curve for this model and dataset is shown. This is generated
    using the pROC package in CRAN:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，展示了该模型和数据的ROC曲线。这是使用CRAN中的pROC包生成的：
- en: '![Model training and prediction](img/image00475.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![模型训练和预测](img/image00475.jpeg)'
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: From the ROC curve and confusion matrix, one can choose the best threshold for
    the classifier, and the precision and recall metrics. Note that the example shown
    here is for illustration purposes only. The model needs be to tuned further to
    improve accuracy.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从ROC曲线和混淆矩阵中，可以选择分类器最佳阈值和精确度、召回率指标。请注意，这里所示示例仅用于说明目的。模型需要进一步调整以提高准确性。
- en: 'We can also print some of the most frequent words (model features) occurring
    in the two classes and their posterior probabilities generated by the model. This
    will give a more intuitive feeling for the model exercise. The following R code
    does this job:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以打印出两个类别中最频繁出现的单词（模型特征）及其由模型生成的后验概率。这将使我们对模型练习有更直观的感觉。以下R代码完成这项工作：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output table is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表格如下：
- en: '| word | Prob(word&#124;spam) | Prob(word&#124;ham) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| word | Prob(word|spam) | Prob(word|ham) |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| call | 0.6994 | 0.4084 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| call | 0.6994 | 0.4084 |'
- en: '| free | 0.4294 | 0.3996 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| free | 0.4294 | 0.3996 |'
- en: '| now | 0.3865 | 0.3120 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| now | 0.3865 | 0.3120 |'
- en: '| repli | 0.2761 | 0.3094 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| repli | 0.2761 | 0.3094 |'
- en: '| text | 0.2638 | 0.2840 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| text | 0.2638 | 0.2840 |'
- en: '| spam | 0.2270 | 0.2726 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| spam | 0.2270 | 0.2726 |'
- en: '| txt | 0.2270 | 0.2594 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| txt | 0.2270 | 0.2594 |'
- en: '| get | 0.2209 | 0.2182 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| get | 0.2209 | 0.2182 |'
- en: '| stop | 0.2086 | 0.2025 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| stop | 0.2086 | 0.2025 |'
- en: The table shows, for example, that given a document is spam, the probability
    of the word *call* appearing in it is 0.6994, whereas the probability of the same
    word appearing in a normal document is only 0.4084.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示，例如，给定一个文档是垃圾邮件，该文档中出现单词*call*的概率为0.6994，而该单词出现在正常文档中的概率仅为0.4084。
- en: The Bayesian logistic regression model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯逻辑回归模型
- en: The name logistic regression comes from the fact that the dependent variable
    of the regression is a logistic function. It is one of the widely used models
    in problems where the response is a binary variable (for example, fraud or not-fraud,
    click or no-click, and so on).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的名称来源于回归的依赖变量是逻辑函数。它是响应变量为二元变量（例如，欺诈或非欺诈、点击或未点击等）的问题中广泛使用的模型之一。
- en: 'A logistic function is defined by the following equation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数由以下方程定义：
- en: '![The Bayesian logistic regression model](img/image00476.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯逻辑回归模型](img/image00476.jpeg)'
- en: It has the particular feature that, as *y* varies from ![The Bayesian logistic
    regression model](img/image00477.jpeg) to ![The Bayesian logistic regression model](img/image00478.jpeg),
    the function value varies from 0 to 1\. Hence, the logistic function is ideal
    for modeling any binary response as the input signal is varied.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有特定的特征，即当*y*从![贝叶斯逻辑回归模型](img/image00477.jpeg)变化到![贝叶斯逻辑回归模型](img/image00478.jpeg)时，函数值从0变化到1。因此，逻辑函数非常适合模拟任何二元响应，因为输入信号的变化。
- en: 'The inverse of the logistic function is called *logit*. It is defined as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑函数的逆称为*logit*。它定义如下：
- en: '![The Bayesian logistic regression model](img/image00479.jpeg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯逻辑回归模型](img/image00479.jpeg)'
- en: 'In logistic regression, *y* is treated as a linear function of explanatory
    variables *X*. Therefore, the logistic regression model can be defined as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，*y*被视为解释变量*X*的线性函数。因此，逻辑回归模型可以定义为以下：
- en: '![The Bayesian logistic regression model](img/image00480.jpeg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯逻辑回归模型](img/image00480.jpeg)'
- en: Here, ![The Bayesian logistic regression model](img/image00481.jpeg) is the
    set of basis functions and ![The Bayesian logistic regression model](img/image00482.jpeg)
    are the model parameters as explained in the case of linear regression in [Chapter
    4](part0034.xhtml#aid-10DJ42 "Chapter 4. Machine Learning Using Bayesian Inference"),
    *Machine Learning Using Bayesian Inference*. From the definition of GLM in [Chapter
    5](part0041.xhtml#aid-173721 "Chapter 5. Bayesian Regression Models"), *Bayesian
    Regression Models*, one can immediately recognize that logistic regression is
    a special case of GLM with the **logit** function as the link function.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![贝叶斯逻辑回归模型](img/image00481.jpeg)是基函数集合，![贝叶斯逻辑回归模型](img/image00482.jpeg)是模型参数，如[第4章](part0034.xhtml#aid-10DJ42
    "第4章。使用贝叶斯推理进行机器学习")中所述，*使用贝叶斯推理进行机器学习*。从[第5章](part0041.xhtml#aid-173721 "第5章。贝叶斯回归模型")中广义线性模型（GLM）的定义，*贝叶斯回归模型*，可以立即识别出逻辑回归是具有**logit**函数作为连接函数的GLM的特殊情况。
- en: Bayesian treatment of logistic regression is more difficult compared to the
    case of linear regression. Here, the likelihood function consists of a product
    of logistic functions; one for each data point. To compute the posterior, one
    has to normalize this function multiplied by the prior (to get the denominator
    of the Bayes formula). One approach is to use Laplace approximation as explained
    in [Chapter 3](part0030.xhtml#aid-SJGS2 "Chapter 3. Introducing Bayesian Inference"),
    *Introducing Bayesian Inference*. Readers might recall that in Laplace approximation,
    the posterior is approximated as a Gaussian (normal) distribution about the maximum
    of the posterior. This is achieved by finding the **maximum a posteriori** (**MAP**)
    solution first and computing the second derivative of the negative log likelihood
    around the MAP solution. Interested readers can find the details of Laplace approximation
    to logistic regression in the paper by D.J.C. MacKay (reference 2 in the *References*
    section of this chapter).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归相比，贝叶斯处理逻辑回归更为复杂。在这里，似然函数由逻辑函数的乘积组成；每个数据点对应一个。为了计算后验概率，必须将这个函数与先验概率相乘（以得到贝叶斯公式的分母）。一种方法是使用拉普拉斯近似，如[第3章](part0030.xhtml#aid-SJGS2
    "第3章。介绍贝叶斯推理")中所述，*介绍贝叶斯推理*。读者可能还记得，在拉普拉斯近似中，后验概率被近似为关于后验概率最大值的正态（高斯）分布。这是通过首先找到**最大后验概率**（**MAP**）解，然后计算MAP解周围负对数似然函数的二阶导数来实现的。感兴趣的读者可以在D.J.C.
    MacKay的论文中找到逻辑回归拉普拉斯近似的详细内容（本章*参考文献*部分的第2条参考文献）。
- en: 'Instead of using an analytical approximation, Polson and Scott recently proposed
    a fully Bayesian treatment of this problem using a data augmentation strategy
    (reference 3 in the *References* section of this chapter). The authors have implemented
    their method in the R package: BayesLogit. We will use this package to illustrate
    Bayesian logistic regression in this chapter.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '除了使用解析近似，Polson和Scott最近提出了一种使用数据增强策略的完全贝叶斯处理方法（本章*参考文献*部分的第3条参考文献）。作者们将他们的方法实现在了R包：BayesLogit中。我们将使用这个包在本章中展示贝叶斯逻辑回归。 '
- en: The BayesLogit R package
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BayesLogit R包
- en: 'The package can be downloaded from the CRAN website at [http://cran.r-project.org/web/packages/BayesLogit/index.html](http://cran.r-project.org/web/packages/BayesLogit/index.html).
    The package contains the `logit` function that can be used to perform a Bayesian
    logistic regression. The syntax for calling this function is as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该包可以从CRAN网站[http://cran.r-project.org/web/packages/BayesLogit/index.html](http://cran.r-project.org/web/packages/BayesLogit/index.html)下载。该包包含一个`logit`函数，可用于执行贝叶斯逻辑回归。调用此函数的语法如下：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, *Y* is an *N*-dimensional vector containing response values; *X* is an
    *N x P* dimensional matrix containing values of independent variables, *n* is
    an *N*-dimensional vector, ![The BayesLogit R package](img/image00483.jpeg) is
    a *P*-dimensional prior mean, and ![The BayesLogit R package](img/image00484.jpeg)
    is a *P x P* dimensional prior precision. The other two arguments are related
    to MCMC simulation parameters. The number of MCMC simulations saved is denoted
    by `samp` and the number of MCMC simulations discarded at the beginning of the
    run before saving samples is denoted by `burn`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The dataset
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate Bayesian logistic regression, we use the Parkinsons dataset from
    the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/Parkinsons](https://archive.ics.uci.edu/ml/datasets/Parkinsons)).
    The dataset was used by Little et.al. to detect Parkinson''s disease by analyzing
    voice disorder (reference 4 in the *References* section of this chapter). The
    dataset consists of voice measurements from 31 people, of which 23 people have
    Parkinson''s disease. There are 195 rows corresponding to multiple measurements
    from a single individual. The measurements can be grouped into the following sets:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The vocal fundamental frequency
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jitter
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimmer
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio of noise to tonal components
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nonlinear dynamical complexity measures
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The signal fractal scaling exponent
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nonlinear measures of fundamental frequency variation
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In total, there are 22 numerical attributes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the training and testing datasets
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can train the Bayesian logistic model, we need to do some preprocessing
    of the data. The dataset contains multiple measurements from the same individual.
    Here, we take all observations; each from a sampled set of individuals in order
    to create the training and test sets. Also, we need to separate the dependent
    variable (class label *Y*) from the independent variables (*X*). The following
    R code does this job:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using the Bayesian logistic model
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use *xtrain* and *ytrain* to train the Bayesian logistic regression
    model using the `logit( )` function:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `summary( )` function will give a high-level summary of the fitted model:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To predict values of *Y* for a new dataset, we need to write a custom script
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The error of prediction can be computed by comparing it with the actual values
    of *Y* present in *ytest*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'One can plot the ROC curve using the pROC package as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Using the Bayesian logistic model](img/image00485.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: The ROC curve has an AUC of 0.942 suggesting a good classification accuracy.
    Again, the model is presented here to illustrate the purpose and is not tuned
    to obtain maximum performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this exercise, we will use the DBWorld e-mails dataset from the UCI Machine
    Learning repository to compare the relative performance of Naïve Bayes and BayesLogit
    methods. The dataset contains 64 e-mails from the DBWorld newsletter and the task
    is to classify the e-mails into either *announcements of conferences* or *everything
    else*. The reference for this dataset is a course by Prof. Michele Filannino (reference
    5 in the *References* section of this chapter). The dataset can be downloaded
    from the UCI website at [https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#](https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#).
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用来自UCI机器学习仓库的DBWorld电子邮件数据集来比较朴素贝叶斯和BayesLogit方法的相对性能。该数据集包含64封来自DBWorld通讯的电子邮件，任务是将电子邮件分类为*会议公告*或*其他一切*。该数据集的参考是Michele
    Filannino教授的课程（本章*参考文献*部分的第5条）。数据集可以从UCI网站下载，网址为[https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#](https://archive.ics.uci.edu/ml/datasets/DBWorld+e-mails#)。
- en: Some preprocessing of the dataset would be required to use it for both the methods.
    The dataset is in the ARFF format. You need to download the **foreign** R package
    ([http://cran.r-project.org/web/packages/foreign/index.html](http://cran.r-project.org/web/packages/foreign/index.html))
    and use the `read.arff( )` method in it to read the file into an R data frame.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这两种方法都需要对数据集进行一些预处理。数据集采用ARFF格式。您需要下载**外国**的R包([http://cran.r-project.org/web/packages/foreign/index.html](http://cran.r-project.org/web/packages/foreign/index.html))，并使用其中的`read.arff()`方法将文件读入R数据框。
- en: References
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Almeida T.A., Gómez Hidalgo J.M., and Yamakami A. "Contributions to the Study
    of SMS Spam Filtering: New Collection and Results". In: 2011 ACM Symposium on
    Document Engineering (DOCENG''11). Mountain View, CA, USA. 2011'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Almeida T.A., Gómez Hidalgo J.M. 和 Yamakami A. "对短信垃圾邮件过滤研究的新贡献：新的收集和结果". 在：2011年ACM文档工程研讨会（DOCENG'11）.
    加利福尼亚州山景城，美国. 2011
- en: MacKay D.J.C. "The Evidence Framework Applied to Classification Networks". Neural
    Computation 4(5)
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MacKay D.J.C. "将证据框架应用于分类网络". 神经计算 4(5)
- en: '"Bayesian Inference for Logistic Models Using Pólya-Gamma Latent Variables".
    Journal of the American Statistical Association. Volume 108, Issue 504, Page 1339\.
    2013'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"使用Pólya-Gamma潜在变量进行逻辑模型的贝叶斯推理". 美国统计学会杂志. 第108卷，第504期，第1339页. 2013'
- en: Costello D.A.E., Little M.A., McSharry P.E., Moroz I.M., and Roberts S.J. "Exploiting
    Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection".
    BioMedical Engineering OnLine. 2007
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Costello D.A.E., Little M.A., McSharry P.E., Moroz I.M. 和 Roberts S.J. "利用非线性回溯和分形尺度特性进行语音障碍检测".
    生物医学工程在线. 2007
- en: Filannino M. "DBWorld e-mail Classification Using a Very Small Corpus". Project
    of Machine Learning Course. University of Manchester. 2011
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Filannino M. "使用非常小语料库的DBWorld电子邮件分类". 机器学习课程项目. 曼彻斯特大学. 2011
- en: Summary
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the various merits of using Bayesian inference
    for the classification task. We reviewed some of the common performance metrics
    used for the classification task. We also learned two basic and popular methods
    for classification, Naïve Bayes and logistic regression, both implemented using
    the Bayesian approach. Having learned some important Bayesian-supervised machine
    learning techniques, in the next chapter, we will discuss some unsupervised Bayesian
    models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了使用贝叶斯推理进行分类任务的多种优点。我们回顾了一些用于分类任务的常见性能指标。我们还学习了两种基本且流行的分类方法，即朴素贝叶斯和逻辑回归，这两种方法都采用了贝叶斯方法实现。在了解了某些重要的贝叶斯监督机器学习技术之后，在下一章中，我们将讨论一些无监督的贝叶斯模型。
