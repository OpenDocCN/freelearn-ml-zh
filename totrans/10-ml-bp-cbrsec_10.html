<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-138" class="chapter-number"><a id="_idTextAnchor021"/>9</h1>
			<h1 id="_idParaDest-139">Attacking Models with Adversarial Machine Learning</h1>
			<p>Recent advances in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) have increased our reliance<a id="_idIndexMarker646"/> on intelligent algorithms and systems. ML systems<a id="_idIndexMarker647"/> are used to make decisions on the fly in several critical applications. For example, whether a credit card transaction should be authorized or not or whether a particular Twitter account is a bot or not is decided by a model within seconds, and this decision affects steps taken in the real world (such as the transaction or account being flagged as fraudulent). Attackers use the reduced human involvement to their advantage and aim to attack models deployed in the real world. <strong class="bold">Adversarial ML</strong> (<strong class="bold">AML</strong>) is a field of ML that focuses on detecting<a id="_idIndexMarker648"/> and exploiting flaws in <span class="No-Break">ML models.</span></p>
			<p>Adversarial attacks can come in several forms. Attackers may try to manipulate the features of a data point so that it is misclassified by the model. Another threat vector is data poisoning, where attackers introduce perturbations into the training data itself so that the model learns from incorrect data and thus performs poorly. An attacker may also attempt to run membership inference attacks to determine whether an individual was included in the training data or not. Protecting ML models from adversarial attacks and, therefore, understanding the nature and workings of such attacks is essential for data scientists in the <span class="No-Break">cybersecurity space.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Introduction <span class="No-Break">to AML</span></li>
				<li>Attacking <span class="No-Break">image models</span></li>
				<li>Attacking <span class="No-Break">text models</span></li>
				<li>Developing robustness against <span class="No-Break">adversarial attacks</span></li>
			</ul>
			<p>This chapter will help you understand how adversarial attacks can manifest themselves, which will then help you uncover gaps and vulnerabilities in your <span class="No-Break">ML infrastructure.</span></p>
			<h1 id="_idParaDest-140">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209"><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-141">Introduction to AML</h1>
			<p>In this section, we will learn about<a id="_idIndexMarker649"/> what AML exactly is. We will begin by understanding the importance ML plays in today’s world, followed by the various kinds of adversarial attacks <span class="No-Break">on models.</span></p>
			<h2 id="_idParaDest-142">The importance of ML</h2>
			<p>In recent times, our reliance<a id="_idIndexMarker650"/> on ML has increased. Automated systems and models are in every sphere of our life. These systems often allow for fast decision-making without the need for manual human intervention. ML is a boon to security tasks; a model can learn from historical behavior, identify and recognize patterns, extract features, and render a decision much faster and more efficiently than a human can. Examples of some ML systems handling security-critical decisions are <span class="No-Break">given here:</span></p>
			<ul>
				<li>Real-time fraud detection in credit card usage often uses ML. Whenever a transaction is made, the model looks at your location, the amount, the billing code, your past transactions, historical patterns, and other behavioral features. These are fed into an ML model, which will render a decision of <em class="italic">FRAUD</em> or <span class="No-Break"><em class="italic">NOT FRAUD</em></span><span class="No-Break">.</span></li>
				<li>Malware detection systems use ML models to detect malicious applications. The model uses API calls made, permissions requested, domains connected, and so on to classify the application as <em class="italic">MALWARE</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">BENIGN</em></span><span class="No-Break">.</span></li>
				<li>Social media platforms use ML to identify hate speech or toxic content. Models can extract text and image content, topics, keywords, and URLs to determine whether a post is <em class="italic">TOXIC</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">NON-TOXIC</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>What is the goal behind listing these applications? In each case, you can see that ML plays a prominent role in detecting or identifying an adversary or attacker. The attacker, therefore, has an incentive<a id="_idIndexMarker651"/> to degrade the performance of the model. This leads us to the branch of AML and <span class="No-Break">adversarial attacks.</span></p>
			<h2 id="_idParaDest-143">Adversarial attacks</h2>
			<p>AML is a subfield of AI and ML<a id="_idIndexMarker652"/> concerned with the design and analysis of algorithms that can robustly and securely operate in adversarial environments. In these scenarios, an adversary with malicious intent can manipulate input data to disrupt the behavior of ML models, either by causing incorrect predictions or by compromising the confidentiality and privacy of <span class="No-Break">the data.</span></p>
			<p>AML attacks are intentionally crafted inputs to ML models that cause them to behave in unintended and potentially harmful ways. They can be used for malicious purposes, such as compromising the security and privacy of ML models, disrupting their normal operation, or undermining their accuracy and reliability. In these scenarios, attackers may use adversarial examples to trick ML models into making incorrect predictions, compromising the confidentiality and privacy of sensitive data, or causing harm to the system or the people who <span class="No-Break">use it.</span></p>
			<p>For example, we listed the applications of ML models in some critical tasks earlier. Here is how an attacker could manipulate them to <span class="No-Break">their benefit:</span></p>
			<ul>
				<li>In a fraud detection system, a smart attacker may try to abuse the credit card with multiple small purchases instead of a large one. The model may be fooled by the purchase amounts<a id="_idIndexMarker653"/> and will not flag them as abnormal. Or, the attacker may use a <strong class="bold">virtual private network</strong> (<strong class="bold">VPN</strong>) connection to appear closer to the victim and purchase gift cards online, thus evading the model’s <span class="No-Break">location-based features.</span></li>
				<li>A malware developer may know which features indicate malware presence. Therefore, they may try to mask that behavior by requesting some normal permissions or making redundant API calls so as to throw the classifier off in <span class="No-Break">the predictions.</span></li>
				<li>A user who wants to post toxic content or hate speech knows which words indicate abusive content. They will try to misspell those words so that they are not flagged by <span class="No-Break">the model.</span></li>
			</ul>
			<p>Using adversarial attacks, an attacker<a id="_idIndexMarker654"/> can potentially fool a system and escape undetected. It is, therefore, important for researchers and practitioners in the field of ML to understand adversarial attacks and to develop methods for detecting and defending against them. This requires a deep understanding of the underlying mechanisms of these attacks and the development of new algorithms and techniques to <span class="No-Break">prevent them.</span></p>
			<h2 id="_idParaDest-144">Adversarial tactics</h2>
			<p>The end goal of an adversarial attack<a id="_idIndexMarker655"/> is to degrade the performance of an ML model. Adversarial attacks generally employ one of three strategies: input perturbation, data poisoning, or model inversion attacks. We will cover these in <span class="No-Break">detail next.</span></p>
			<h3>Input perturbation attacks</h3>
			<p>In input perturbation<a id="_idIndexMarker656"/> attacks, the attacker maliciously<a id="_idIndexMarker657"/> crafts input examples so that they will be misclassified by the model. The attacker makes slight changes to the input that are neither discernible to the naked eye nor large enough to be detected as anomalous or noisy. Typically, this can include changing a few pixels<a id="_idIndexMarker658"/> in an image or altering some characters in a word. <strong class="bold">Deep learning</strong> (<strong class="bold">DL</strong>) systems, favored because of their power, are very susceptible to input perturbation attacks. Because of non-linear functions and transforms, a small change in the input can cause a significant unexpected change in <span class="No-Break">the output.</span></p>
			<p>For example, consider the following screenshot from a 2017 study showing two images of a <span class="No-Break"><strong class="bold">STOP</strong></span><span class="No-Break"> sign:</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B19327_09_01.jpg" alt="Figure 9.1 – An actual image of a STOP sign (left) and the adversarially manipulated image (right)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – An actual image of a STOP sign (left) and the adversarially manipulated image (right)</p>
			<p>The one on the left is the actual one from the street, and the one on the right is an identical one with some pieces of tape. These pieces of tape represent an input perturbation on the original. The researchers found that the image on the left was correctly detected as a STOP sign, but the model<a id="_idIndexMarker659"/> was fooled by the right<a id="_idIndexMarker660"/> one and detected it as a 45 MPH speed <span class="No-Break">limit sign.</span></p>
			<h3>Data poisoning attacks</h3>
			<p>Data poisoning attacks <a id="_idIndexMarker661"/>are malicious attacks<a id="_idIndexMarker662"/> in which an adversary manipulates or corrupts training data in order to degrade the performance of an ML model or cause it to behave in unexpected ways. The goal of these attacks is to cause the model to make incorrect predictions or decisions, leading to security vulnerabilities or privacy breaches. If the quality of data (in terms of the correctness of labels presented to the model) is bad, naturally the resulting model will also be bad. Due to incorrect labels, the model will learn correlations and <span class="No-Break">features incorrectly.</span></p>
			<p>For example, in a <strong class="bold">supervised ML</strong> (<strong class="bold">SML</strong>) scenario, an adversary may manipulate<a id="_idIndexMarker663"/> the labeled data used for training in order to cause a classifier to misclassify certain instances, leading to security vulnerabilities. In another scenario, an adversary may add malicious instances to the training data in order to cause the model to overfit, leading to a decrease in performance on unseen data. For example, if an attacker adds several requests from a malicious domain and marks them as safe or benign in the training data, the model may learn that this domain indicates safe behavior and, therefore, will not mark other requests from that domain <span class="No-Break">as malicious.</span></p>
			<p>These attacks can be particularly dangerous because ML models are becoming increasingly widely used in a variety<a id="_idIndexMarker664"/> of domains, including<a id="_idIndexMarker665"/> security and <span class="No-Break">privacy-sensitive applications.</span></p>
			<h3>Model inversion attacks</h3>
			<p>Model inversion attacks<a id="_idIndexMarker666"/> are a type of privacy<a id="_idIndexMarker667"/> attack in which an adversary tries to reverse-engineer an ML model to obtain sensitive information about the training data or the individuals represented by the data. The goal of these attacks is to reveal information about the training data that would otherwise <span class="No-Break">be protected.</span></p>
			<p>For example, in a fraud detection scenario, a financial institution might use an ML model to identify instances of fraud in financial transactions. An adversary might attempt to reverse-engineer the model in order to obtain information about the characteristics of fraudulent transactions, such as the types of goods or services that are commonly purchased in fraud cases. The attacker may discover the important features that are used to discern fraud and, therefore, know what to manipulate. This information could then be used to commit more sophisticated fraud in <span class="No-Break">the future.</span></p>
			<p>To carry out a model inversion attack, an adversary might start by submitting queries to the ML model with various inputs and observing the model’s predictions. Over time, the adversary could use this information to build an approximation of the model’s internal representation of the data. In some cases, the adversary might be able to obtain information about the training data itself, such as the values of sensitive features—for example, the age or address of individuals represented by <span class="No-Break">the data.</span></p>
			<p>This concludes our discussion of various kinds of adversarial attacks. In the next section, we will turn to implementing a few adversarial attacks on <span class="No-Break">image-based models.</span></p>
			<h1 id="_idParaDest-145">Attacking image models</h1>
			<p>In this section, we will look<a id="_idIndexMarker668"/> at two popular <a id="_idIndexMarker669"/>attacks on image classification systems: <strong class="bold">Fast Gradient Sign Method</strong> (<strong class="bold">FGSM</strong>) and the <strong class="bold">Projected Gradient Descent</strong> (<strong class="bold">PGD</strong>) method. We will first look at the theoretical<a id="_idIndexMarker670"/> concepts underlying each attack, followed by actual implementation <span class="No-Break">in Python.</span></p>
			<h2 id="_idParaDest-146">FGSM</h2>
			<p>FGSM is one of the earliest methods<a id="_idIndexMarker671"/> used for crafting adversarial examples for image classification models. Proposed<a id="_idIndexMarker672"/> by Goodfellow in 2014, it is a simple and powerful attack against <strong class="bold">neural network</strong> (<strong class="bold">NN</strong>)-based <span class="No-Break">image classifiers.</span></p>
			<h3>FGSM working</h3>
			<p>Recall that NNs<a id="_idIndexMarker673"/> are layers of neurons placed one after the other, and there are connections from neurons in one layer to the next. Each connection has an associated weight, and the weights represent the model parameters. The final layer produces an output that can be compared with the available ground truth to calculate the loss, which is a measure of how far off the prediction is from the actual ground truth. The loss is <em class="italic">backpropagated</em>, and the model <em class="italic">learns</em> by adjusting the parameters<a id="_idIndexMarker674"/> based on the gradient of the loss. This process is known as <em class="italic">gradient descent</em>. If <span class="_-----MathTools-_Math_Variable_v-normal">θ</span> is the parameter and <span class="_-----MathTools-_Math_Variable_v-normal">L</span> is the loss, the adjusted parameter <span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span>is calculated <span class="No-Break">as follows:</span></p>
			<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">η</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">L</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Space"> </span></p>
			<p>Here, the<a id="_idIndexMarker675"/> derivative term <span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">L</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">δ</span><span class="_-----MathTools-_Math_Variable_v-normal">θ</span><span class="_-----MathTools-_Math_Variable_v-normal"> </span><span class="_-----MathTools-_Math_Space"> </span>is known as <span class="No-Break">the </span><span class="No-Break"><em class="italic">gradient</em></span><span class="No-Break">.</span></p>
			<p>The FGSM adversarial attack leverages the gradients to craft adversarial examples. While the learning algorithm <em class="italic">minimizes</em> the loss by adjusting the weights, the FGSM attack works to adjust the input data so as to <em class="italic">maximize</em> the loss. During backpropagation, a small perturbation is added to the image based on the sign of <span class="No-Break">the gradient.</span></p>
			<p>Formally speaking, given an image <span class="_-----MathTools-_Math_Variable">X</span>, a new (adversarial) image <span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span> can be calculated <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∇</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ℒ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>Here, <span class="_-----MathTools-_Math_Symbol_Extended">ℒ</span> represents the loss, <span class="_-----MathTools-_Math_Variable">θ</span> represents the model parameters, and<span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span> refers to the ground-truth label. The term <span class="_-----MathTools-_Math_Symbol_Extended">ℒ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span> calculates the loss based on the model prediction and ground truth, and <span class="_-----MathTools-_Math_Operator_Extended">∇</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span> calculates the gradient. The term <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span> is the perturbation added, which is either positive or negative depending on the sign of <span class="No-Break">the gradient.</span></p>
			<p>A popular example that demonstrates the effectiveness of the FGSM attack is <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B19327_09_02.jpg" alt="Figure 9.2 – Adding noise to an image with FGSM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Adding noise to an image with FGSM</p>
			<p>The original image is predicted to be a panda with a confidence of 57.7%, which indicates that the model made the correct prediction. Adversarial noise is added to the image depending on the sign of the gradient per pixel (so, either 0.007, 0, or -0.007). The resulting image is identical to the original one—no difference can be seen to the naked eye. This is expected because the human eye is not sensitive to such small differences at the pixel level. However, the model<a id="_idIndexMarker676"/> now predicts the image to be a gibbon with a <span class="No-Break">99.3% confidence.</span></p>
			<h3>FGSM implementation</h3>
			<p>Let us now implement the FGSM attack<a id="_idIndexMarker677"/> method using the <span class="No-Break">PyTorch library.</span></p>
			<p>We begin, as usual, by importing the <span class="No-Break">required libraries:</span></p>
			<pre class="source-code">
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torchvision import datasets, transforms</pre>
			<p>Next, we will define a function that performs the FGSM attack and generates an adversarial example. This function<a id="_idIndexMarker678"/> calculates the gradient, followed by the perturbation, and generates an adversarial image. The <strong class="source-inline">epsilon</strong> parameter is passed to it, which indicates the degree of perturbation to be added. In short, the function works <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Takes in as input an image or an array <span class="No-Break">of images.</span></li>
				<li>Calculates the predicted label by running it through <span class="No-Break">the model.</span></li>
				<li>Calculates the loss by comparing the predicted label with the actual <span class="No-Break">ground truth.</span></li>
				<li>Backpropagates the loss and calculates <span class="No-Break">the gradient.</span></li>
				<li>Calculates the perturbation by multiplying <strong class="source-inline">epsilon</strong> with the sign of <span class="No-Break">the gradient.</span></li>
				<li>Adds this to the image to obtain the <span class="No-Break">adversarial image.</span></li>
			</ol>
			<p>The following code snippet shows how to perform the <span class="No-Break">FGSM attack:</span></p>
			<pre class="source-code">
def Generate_FGSM_Image(model,
                        x,
                        epsilon):
  # Check if epsilon is 0
  # If so, that means no perturbation is added
  # We can avoid gradient calculations
  if epsilon == 0:
    return x
  # Convert x to a float and having gradients enabled
  x = x.clone().detach()
  x = x.to(torch.float)
  x – x.requires_grad_(True)
  # Get original label as predicted by model
  _, y = torch.max(model(x), 1)
  # Compute Loss
  loss_function = nn.CrossEntropyLoss()
  loss = loss_function(model(x), y)
  # Backpropagate Loss
  loss.backward()
  # Calculate perturbation using the FGSM equation
  perturbation = epsilon * torch.sign(x.grad)
  # Calculate the adversarial image
  x_adversarial = x + perturbation
  return x_adversarial</pre>
			<p>Next, we need a basic image classifier to use as the model to attack. As the data at hand is images, we will use a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>). For this, we define a class<a id="_idIndexMarker679"/> that has two functions, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">The constructor</strong>: This function defines the basic structure<a id="_idIndexMarker680"/> of the NN to be used for the classification. We define the convolutional and fully connected layers that we need. The number of neurons and the number of layers here are all <span class="No-Break">design choices.</span></li>
				<li><strong class="bold">The forward function</strong>: This function defines what happens <a id="_idIndexMarker681"/>during the forward pass of the NN. We take in the input data and pass it through the first convolutional layer. The output of this layer is processed through a ReLU activation function and then passed to the next layer. This continues for all convolutional layers we have. Finally, we flatten the output of the last convolutional layer and pass it through the fully <span class="No-Break">connected layer.</span></li>
			</ul>
			<p>The process is illustrated<a id="_idIndexMarker682"/> in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
class BasicImageNetCNN(nn.Module):
    def __init__(self, in_channels=1):
        super(BasicImageNetCNN, self).__init__()
        # Define the convolutional layers
        self.conv1 = nn.Conv2d(in_channels, 64, 8, 1)
        self.conv2 = nn.Conv2d(64, 128, 6, 2)
        self.conv3 = nn.Conv2d(128, 128, 5, 2)
        # Define the fully connected layer
        self.fc = nn.Linear(128 * 3 * 3, 10)
    def forward(self, x):
        # Pass the image through convolutional layers one by one
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        # Flatten the output of the convolutional layer and pass to fully connected layer
        x = x.view(-1, 128 * 3 * 3)
        x = self.fc(x)
        return x</pre>
			<p>We will now write a function that loads<a id="_idIndexMarker683"/> the required datasets. For our experiment, we will be using the <em class="italic">CIFAR-10</em> dataset. Developed by the <strong class="bold">Canadian Institute for Advanced Research</strong> (<strong class="bold">CIFAR</strong>), the dataset consists of 60,000 images from 10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). Each image is color and of size 32 x 32 pixels. The dataset has been divided into 50,000 training and 10,000 test images. As a standardized dataset in the world of ML, it is well integrated with Python and PyTorch. The following function provides the code to load the train and test sets. If the data<a id="_idIndexMarker684"/> is not locally available, it will first download it and then <span class="No-Break">load it:</span></p>
			<pre class="source-code">
def load_cifar10_datasets(datapath):
    # Load the transformations
    train_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
    test_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])
    # Obtain the datasets
    # Download them if they are not present
    train_dataset = torchvision.datasets.CIFAR10(root=datapath, train=True,
    transform=train_transforms, download=True)
    test_dataset = torchvision.datasets.CIFAR10(root=datapath, train=False,
    transform=test_transforms, download=True)
    # Create Data Loaders
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128,
    shuffle=True, num_workers=2)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128,
    shuffle=False, num_workers=2)
    return train_loader, test_loader</pre>
			<p>PyTorch provides a standard functionality<a id="_idIndexMarker685"/> known as data loaders that facilitates<a id="_idIndexMarker686"/> easy data manipulation for ML. Data loaders can generate the data needed by applying specific transformations, and the generated data can be consumed by ML models. Note that when defining the data loader, we have specified a <strong class="source-inline">batch_size</strong> parameter. This defines the number of data instances that will be read at a time. In our case, it is set to <strong class="source-inline">128</strong>, which means that the forward pass, loss calculation, backpropagation, and gradient descent will happen one by one for batches where each batch is of <span class="No-Break">128 images.</span></p>
			<p>Next, we will train a vanilla NN model for image classification on the CIFAR dataset. We first perform some boilerplate setup that includes <span class="No-Break">the following:</span></p>
			<ol>
				<li>Setting the number of epochs to be used <span class="No-Break">for training.</span></li>
				<li>Loading the train and test <span class="No-Break">data loaders.</span></li>
				<li>Initializing the model with the basic CNN model we <span class="No-Break">defined earlier.</span></li>
				<li>Defining the loss function to be cross-entropy and the optimizer to <span class="No-Break">be Adam.</span></li>
				<li>Moving the model to CUDA if it <span class="No-Break">is available.</span></li>
			</ol>
			<p>Then, we begin the training loop. In each iteration of the training loop, we do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Load a batch of <span class="No-Break">training data.</span></li>
				<li>Move it to the GPU (CUDA) if needed <span class="No-Break">and available.</span></li>
				<li>Zero out the <span class="No-Break">optimizer gradients.</span></li>
				<li>Calculate the predicted output by running inference on <span class="No-Break">the model.</span></li>
				<li>Calculate the loss based on prediction and <span class="No-Break">ground truth.</span></li>
				<li>Backpropagate <span class="No-Break">the loss.</span></li>
			</ol>
			<p>Here is a code snippet that executes<a id="_idIndexMarker687"/> these steps one <span class="No-Break">by one:</span></p>
			<pre class="source-code">
NUM_EPOCHS = 10
train_data, test_data = load_cifar10_datasets(datapath = "./data")
model = BasicImageNetCNN(in_channels = 3)
loss_function = torch.nn.CrossEntropyLoss(reduction="mean")
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
if torch.cuda.is_available():
  device = "cuda"
  model = model.cuda()
else:
  device = "cpu"
model.train()
for epoch in range(NUM_EPOCHS):
  train_loss = 0.0
  for x, y in train_data:
    # Move image and labels to device if applicable
    x = x.to(device)
    y = y.to(device)
    # Zero out the gradients from previous epoch if any
    optimizer.zero_grad()
    # Calculate predicted value and loss
    y_pred = model(x)
    loss = loss_function(y_pred, y)
    # Backpropagation
    loss.backward()
    optimizer.step()
    # Keep track of the loss
    train_loss = train_loss + loss.item()
    # Print some information for logging
    print("EPOCH: {} ---------- Loss: {}".format(epoch, train_loss))</pre>
			<p>Finally, we evaluate our model. While<a id="_idIndexMarker688"/> evaluating, we evaluate the model on two sets of data—the original test data and the adversarial test data that we created <span class="No-Break">using FGSM.</span></p>
			<p>We first set the model to evaluation mode, which means that gradients are not computed and stored. This makes the operations more efficient, as we do not need the overhead of gradients during inference on the model. For every batch of the training data, we calculate the adversarial images using FGSM. Here, we have set the value of <strong class="source-inline">epsilon</strong> to 0.005. Then, we run inference on the model using both the clean images (the original test set) and adversarial images (generated through FGSM). For every batch, we will calculate the number of examples for which the predicted and ground-truth labels match, which will give us the accuracy of the model. Comparing the accuracy of the clean and adversarial set shows us how effective our adversarial <span class="No-Break">attack is:</span></p>
			<pre class="source-code">
model.eval()
clean_correct = 0
fgsm_correct = 0
total = 0
for x, y in test_data:
    # Move image and labels to device if applicable
    x = x.to(device)
    y = y.to(device)
    # Calculate the adversarial images
    x_fgsm = Generate_FGSM_Image(model, x, epsilon = 0.005)
    # Run inference for predicted values on clean and adversarial examples
    _, y_pred_clean = torch.max(model(x), 1)
    _, y_pred_fgsm = torch.max(model(x_fgsm), 1)
    # Calculate accuracy of clean and adversarial predictions
    clean_correct = clean_correct + y_pred_clean.eq(y).sum().item()
    fgsm_correct = fgsm_correct + y_pred_fgsm.eq(y).sum().item()
    total = total + y.size(0)
clean_accuracy = clean_correct / total
fgsm_accuracy = fgsm_correct / total</pre>
			<p>This concludes our discussion of the FGSM attack. You can compare the accuracy before and after the adversarial perturbation (<strong class="source-inline">clean_accuracy</strong> and <strong class="source-inline">fgsm_accuracy</strong>, respectively). The drop<a id="_idIndexMarker689"/> in accuracy indicates the effectiveness of the <span class="No-Break">adversarial attack.</span></p>
			<h2 id="_idParaDest-147">PGD</h2>
			<p>In the previous section, we discussed<a id="_idIndexMarker690"/> the FGSM attack method and how it can be used to generate adversarial images by adding small perturbations to the input image based on the sign of the gradient. The <strong class="bold">PGD</strong> method extends FGSM by applying <span class="No-Break">it iteratively.</span></p>
			<h3>PGD working</h3>
			<p>Specifically, the PGD<a id="_idIndexMarker691"/> attack will, for an input image, calculate a perturbation based on the FGSM attack. Adding this to the image will give us the perturbed image. Whereas the FGSM attack stops here, the PGD attack goes a step further. Once an adversarial image has been generated, we clip the image. Clipping refers to adjusting the image<a id="_idIndexMarker692"/> so that it remains in the neighborhood of the original image. Clipping is done on a per-pixel basis. After the image has been clipped, we repeat the process multiple times iteratively to obtain the final <span class="No-Break">adversarial image.</span></p>
			<p>Formally speaking, given an image <span class="_-----MathTools-_Math_Variable">X</span>, a series of adversarial images can be calculated <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">C</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">X</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Space">  </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">⋅</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Operator_Extended">∇</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ℒ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">x</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">y</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>The notation here is slightly different from that for the FGSM attack. Here, <span class="_-----MathTools-_Math_Variable">α</span> serves the same role as <span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span> did in FGSM; it controls the amount of perturbation. Typically, it is set to 1, which means that each pixel is modified by at most one unit in each step. The process is repeated iteratively for a predetermined number <span class="No-Break">of steps.</span></p>
			<p>The function that implements this is quite straightforward. It simply uses FGSM iteratively and clips the generated images. The FGSM function must be modified to take in the predicted ground-truth label by the model, as it will change in every step and should not be recalculated by FGSM. So, we pass it the ground truth as a parameter and use that instead of recalculating it as a model prediction. In the FGSM function, we simply use the value that is passed in instead of running inference on <span class="No-Break">the model.</span></p>
			<p>The modified FGSM function is shown <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def Generate_FGSM_Image_V2(model,x,y, // New Parameter.epsilon):
  # Check if epsilon is 0
  # If so, that means no perturbation is added
  # We can avoid gradient calculations
  if epsilon == 0:
    return x
  # Convert x to a float and having gradients enabled
  x = x.clone().detach()
  x = x.to(torch.float)
  x - x.requires_grad_(True)
  # Compute Loss
  loss_function = nn.CrossEntropyLoss()
  loss = loss_function(model(x), y)
  # Backpropagate Loss
  loss.backward()
  # Calculate perturbation using the FGSM equation
  perturbation = epsilon * torch.sign(x.grad)
  # Calculate the adversarial image
  x_adversarial = x + perturbation
  return x_adversarial</pre>
			<p>For every image, the PGD method attack function completes the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Calculates the predicted label by running inference on <span class="No-Break">the model.</span></li>
				<li>Sets the original image as the initial adversarial <span class="No-Break">image </span><span class="No-Break"><em class="italic">X</em></span><span class="No-Break"><span class="subscript">0</span></span><span class="No-Break">.</span></li>
				<li>Calculates the adversarial image using the FGSM attack method described in the previous section. In doing so, it passes the predicted value as a parameter so that FGSM does not recompute it in <span class="No-Break">every cycle.</span></li>
				<li>Computes the difference between the image and the adversarially generated image. This is the perturbation to <span class="No-Break">be added.</span></li>
				<li>Clips this perturbation so that the adversarial image is within the neighborhood of the <span class="No-Break">original image.</span></li>
				<li>Adds the clipped perturbation to the image to obtain the <span class="No-Break">adversarial image.</span></li>
				<li>Repeats <em class="italic">steps 2</em>-<em class="italic">6</em> for the desired number of iterations to obtain the final <span class="No-Break">adversarial image.</span></li>
			</ol>
			<p>As you can see, the overarching idea remains the same as with FGSM, but only the process of generating the adversarial<a id="_idIndexMarker693"/> <span class="No-Break">images changes.</span></p>
			<h3>PGD implementation</h3>
			<p>A code snippet<a id="_idIndexMarker694"/> for the PGD method is shown <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
def Generate_PGDM_Image(model,x,epsilon,num_iterations):
  # Obtain actual clean predictions from model
  _, y = torch.max(model(x), 1)
  # Calculate the initial adversarial value
  eta = torch.zeros_like(x)
  eta = torch.clamp(eta, -1*eps, 1*eps)
  x_adv = x + eta
  # For every iteration, do FGSM and clipping
  for _ in range(num_iterations):
    # Note that the FGSM function signature has changed
    # We are passing it the predicted value y as a parameter
    # Thus this will not be recomputed
    x_adv = Generate_FGSM_Image_V2(model,x_adv,y,epsilon = 0.01)
    eta = x_adv - x
    eta = torch.clamp(eta, -1*eps, 1*eps)
    x_adv= x + eta
  # Return the final image
  return x_adv</pre>
			<p>This function can be used to generate<a id="_idIndexMarker695"/> adversarial images given an image using the PGD method. We will not repeat the experiment of model setup, training, and evaluation. Simply using the <strong class="source-inline">Generate_PGDM_Image()</strong> function instead of the <strong class="source-inline">Generate_FGSM_Image()</strong> function should allow you to run our analysis using this attack. How does the performance of this attack compare to the <span class="No-Break">FGSM attack?</span></p>
			<p>This concludes our discussion of attacking image models. In the next section, we will discuss attacking <span class="No-Break">text models.</span></p>
			<h1 id="_idParaDest-148">Attacking text models</h1>
			<p><em class="italic">Please note that this section contains examples of hate speech and racist </em><span class="No-Break"><em class="italic">content online.</em></span></p>
			<p>Just as with images, text models<a id="_idIndexMarker696"/> are also susceptible to adversarial attacks. Attackers can modify the text so as to trigger a misclassification by ML models. Doing so can allow an adversary to <span class="No-Break">escape detection.</span></p>
			<p>A good example of this can be seen on social media platforms. Most platforms have rules against abusive language and hate speech. Automated systems such as keyword-based filters and ML models are used to detect such content, flag it, and remove it. If something outrageous is posted, the platform will block it at the source (that is, not allow it to be posted at all) or remove it in the span of a <span class="No-Break">few minutes.</span></p>
			<p>A malicious adversary can purposely manipulate the content in order to fool a model into thinking that the words are out of vocabulary or are not certain abusive words. For example, according to a study (<em class="italic">Poster | Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em> (<a href="https://dl.acm.org/doi/abs/10.1145/3319535.3363271">https://dl.acm.org/doi/abs/10.1145/3319535.3363271</a>)), the attacker can manipulate their post <span class="No-Break">as shown:</span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B19327_09_03.jpg" alt="Figure 9.3 – A hateful tweet and the adversarially manipulated version"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – A hateful tweet and the adversarially manipulated version</p>
			<p>The original tweet that says “<em class="italic">go back to where you came from -- these fucking immigrants are destroying America!!!</em>” is clearly hate speech and racism against immigrants. Originally, this was classified to be 95% toxic, that is, a toxicity classifier assigned it the label <em class="italic">toxic</em> with 95% probability. Obviously, this classification <span class="No-Break">is correct.</span></p>
			<p>In the obfuscated tweet, the attacker modified three words by eliminating one letter from three words. Note that we still very much recognize those words for what they are. The intent is clear, and this is still very much hate speech. An automated system, however, will not know this. Models and rules work by looking at words. To them, these new words are out of their vocabulary. They have not seen the words <em class="italic">imigrants</em>, <em class="italic">fuckng</em>, or <em class="italic">destroyin</em> in prior examples of hate speech during training. Therefore, the model misclassifies it and assigns it a label of not being toxic content with a probability of 63%. The attacker thus succeeded in fooling a classifier to pass off their toxic content <span class="No-Break">as benign.</span></p>
			<p>The principle of adversarial attacks<a id="_idIndexMarker697"/> in text is the same as that of those in images: manipulating the input so as to confuse the model and not allowing it to recognize certain important features. However, two key differences set adversarial manipulation in text apart from adversarial manipulation <span class="No-Break">in images.</span></p>
			<p>First, the adversarially generated input should be reasonably similar to the original input. For example, we saw in the preceding section that the two images of the panda were nearly identical. This will not be the case with text—the changes made through manipulation will be visible and discernible to the naked eye. Looking at the screenshot of tweets that we just discussed, we can clearly see that the words are different. Manipulation in text is obvious whereas that in images is not. As a result, there is a limit to how much we can manipulate. We cannot change every word—too many changes will clearly indicate the manipulation and lead to discovery, thus defeating <span class="No-Break">the goal.</span></p>
			<p>Second, images are robust to change as compared to text. Changing multiple pixels in an image will still leave the larger image mainly unchanged (that is, a panda will still be recognizable, maybe with some distortion). On the other hand, text depends on words for the meaning it provides. Changing<a id="_idIndexMarker698"/> a few words will change the meaning entirely, or render the text senseless. This would be unacceptable—the goal of an adversarial attack is to still have the <span class="No-Break">original meaning.</span></p>
			<h2 id="_idParaDest-149">Manipulating text</h2>
			<p>In this section, we will explore<a id="_idIndexMarker699"/> techniques for manipulating text so that it may fool an ML classifier. We will show a few sample techniques and provide general guidelines at the end for <span class="No-Break">further exploration.</span></p>
			<p>Recall the example of hate speech online that we discussed earlier: attackers can manipulate text so as to escape detection and post toxic content online. In this section, we will attempt to build such techniques and examine whether we can beat <span class="No-Break">ML models.</span></p>
			<h3>Data</h3>
			<p>For this<a id="_idIndexMarker700"/> experiment, we will use the <em class="italic">Toxic Tweets</em> dataset (<em class="italic">Toxic Tweets Dataset | Kaggle</em>) (<a href="https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset">https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset</a>). This data is made available freely as part of a Kaggle challenge online. You will have to download the data, and then unzip it to extract the CSV file. The data can then be read <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
import pandas as pd
import numpy as np
df = pd.read_csv("FinalBalancedDataset.csv", skiprows = 1, names= ["TweetId","Toxicity","Tweet"])
df.head()</pre>
			<p>This should show you what the data looks like, <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B19327_09_04.jpg" alt="Figure 9.4 – Hate speech dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Hate speech dataset</p>
			<p>You can also look at the distribution of the labels <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
df.groupby("Toxicity").count()["TweetId"]</pre>
			<p>This will show you the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B19327_09_05.jpg" alt="Figure 9.5 – Distribution of tweets by toxicity label"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Distribution of tweets by toxicity label</p>
			<p>The dataset<a id="_idIndexMarker701"/> contains approximately 24,000 tweets that are toxic and 32,500 that are not. In the next section, we will extract features from <span class="No-Break">this data.</span></p>
			<h3>Extracting features</h3>
			<p>In our earlier chapters, we discussed<a id="_idIndexMarker702"/> that there needs to be a method to extract features from text, and such features must be numeric in value. One such method, which<a id="_idIndexMarker703"/> we have already used, is the <strong class="bold">Term Frequency-Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>) approach. Let us do a brief recap <span class="No-Break">of TF-IDF.</span></p>
			<p>TF-IDF is a commonly used technique in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) to convert text into numeric<a id="_idIndexMarker704"/> features. Every word in the text is assigned a score that indicates how important the word is in that text. This is done by multiplying two metrics, <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">TF</strong>: How frequently does the word<a id="_idIndexMarker705"/> appear in the text sample? This can be normalized by the length of the text in words, as texts that differ in length by a large number can cause skews. The TF metric measures how common a word is in this <span class="No-Break">particular text.</span></li>
				<li><strong class="bold">IDF</strong>: How frequently does the word <a id="_idIndexMarker706"/>appear in the rest of the corpus? First, the number of text samples containing this word is obtained. The total number of samples is divided by this number. Simply put, IDF is the inverse of the fraction of text samples containing the word. The IDF metric measures how common the word is in the rest of <span class="No-Break">the corpus.</span></li>
			</ul>
			<p>More details on TF-IDF can be found in <a href="B19327_07.xhtml#_idTextAnchor019"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Attributing Authorship and How to Evade It</em>. For now, here is a code snippet<a id="_idIndexMarker707"/> to extract the TF-IDF features for a list <span class="No-Break">of sentences:</span></p>
			<pre class="source-code">
from sklearn.feature_extraction.text import TfidfVectorizer
def Extract_TF_IDF(train_data, test_data):
    tf_idf = TfidfVectorizer()
    X_train_TFIDF = tf_idf.fit_transform(train_data)
    X_test_TFIDF = tf_idf.transform(test_data)
    return X_train_TFIDF, X_test_TFIDF</pre>
			<h3>Adversarial attack strategies</h3>
			<p>We will attempt to evade our ML models<a id="_idIndexMarker708"/> using two adversarial strategies: appending a letter<a id="_idIndexMarker709"/> at the end of some words, and repeating certain vowels from some words. In each case, our end goal is to fool the classifier into thinking that there are words that it has not seen and, therefore, it does not recognize those words. Let us discuss these strategies one <span class="No-Break">by one.</span></p>
			<h4>Doubling the last letter</h4>
			<p>In this strategy, we simply<a id="_idIndexMarker710"/> misspell the word by appending an additional letter at the end. We double the last letter so that the word appears to be unchanged, and is still recognizable. For example, <em class="italic">America</em> will become <em class="italic">Americaa</em>, and <em class="italic">immigrant</em> will become <em class="italic">immigrantt</em>. To a machine, these words are totally different from <span class="No-Break">one another.</span></p>
			<p>Here is the code to <span class="No-Break">implement this:</span></p>
			<pre class="source-code">
def double_last_letter(sentences, max_perturbations = 3):
    # Output array
    modified_sentences = []
    for sentence in sentences:
        # Split into words
        words = sentence.split(' ')
        # Randomly choose words to manipulate
        rand_indices = np.random.randint(0, len(words), max_perturbations)
        for idx in rand_indices:
            # Check if the word is blank, if yes, skip
            if len(words[idx]) == 0:
              continue
            # Double the last letter in the chosen word
            words[idx]+=words[idx][-1]
        # Join back to make sentence
        modified_sentences.append(' '.join(word for word in words))
    return modified_sentences</pre>
			<h4>Doubling vowels</h4>
			<p>In this attack, we will look for words<a id="_idIndexMarker711"/> with vowels, and on finding the first<a id="_idIndexMarker712"/> vowel, we will repeat it. For example, <em class="italic">Facebook</em> will become <em class="italic">Faacebook</em>, and <em class="italic">Coronavirus</em> will become <em class="italic">Cooronavirus</em>. It is fairly intuitive that repeated vowels often go unnoticed when reading text; this means that the text will appear to be unchanged to a quick reader. The following code snippet implements <span class="No-Break">this attack:</span></p>
			<pre class="source-code">
def double_vowel(sentences, max_perturbations = 3):
    total_perturbations = 0
    # Output array
    modified_sentences = []
    for sentence in sentences:
        # Split into words
        words = sentence.split(' ')
        for i in range(len(words)):
            # Check if maximum perturbations done
            # If so, break the loop and don't do any more!
            if total_perturbations&gt;max_perturbations:
                break
            for vowel in ['a','e','i','o','u']:
                if vowel in words[i]:
                    words[i] = words[i].replace(vowel,vowel+vowel,1)
                    total_perturbations+=1
                    # Here replace only for one vowel
                    # So once replacement is done, break out
                    # This will break only this loop
                    break
        modified_sentences.append(' '.join(word for word in words))
    return modified_sentences</pre>
			<h3>Executing the attacks</h3>
			<p>Now that we have defined the two attacks<a id="_idIndexMarker713"/> we will implement, it is time to actually execute them. To achieve this, we will do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Split the data into training and <span class="No-Break">test sets.</span></li>
				<li>Build a TF-IDF model on the training data and use it to extract features from the <span class="No-Break">training set.</span></li>
				<li>Train a model based on the features extracted in <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">.</span></li>
				<li>Use the same TF-IDF model to extract features on the test set and run inference on the <span class="No-Break">trained model.</span></li>
				<li>Calculate metrics for classification—accuracy, precision, recall, and F-1 score. These are the <span class="No-Break">baseline scores.</span></li>
				<li>Now, apply the attack functions and derive the adversarial <span class="No-Break">test set.</span></li>
				<li>Use the same TF-IDF model to extract features from the adversarial test set and run inference on the <span class="No-Break">trained model.</span></li>
				<li>Calculate metrics for classification—accuracy, precision, recall, and F-1 score. These are the scores upon <span class="No-Break">adversarial attack.</span></li>
			</ol>
			<p>Comparing the scores obtained in <em class="italic">steps 5</em> and <em class="italic">8</em> will tell us what the effectiveness of our <span class="No-Break">attack was.</span></p>
			<p>First, we split the data and extract features for our <span class="No-Break">baseline model:</span></p>
			<pre class="source-code">
X = df["Tweet"].tolist()
y = df["Toxicity"].tolist()
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,y,
test_size = 0.3,stratify = y)
X_train_features, X_test_features = Extract_TF_IDF(X_train, X_test)</pre>
			<p>Let us also set up an evaluation function that takes in the actual and predicted values and prints out our metrics, such<a id="_idIndexMarker714"/> as accuracy, precision, recall, and <span class="No-Break">F-1 score:</span></p>
			<pre class="source-code">
from sklearn.metrics import confusion_matrix
def evaluate_model(actual, predicted):
  confusion = confusion_matrix(actual, predicted)
  tn, fp, fn, tp = confusion.ravel()
  total = tp + fp + tn + fn
  accuracy = 1.0 * (tp + tn) / total
  if tp + fp != 0:
    precision = tp / (tp + fp)
  else:
    precision = 0
  if tp + fn != 0:
    recall = tp / (tp + fn)
  else:
    recall = 0
  if precision == 0 or recall == 0:
    f1 = 0
  else:
    f1 = 2 * precision * recall / (precision + recall)
  evaluation = { 'accuracy': accuracy,
                 'precision': precision,
                 'recall': recall,
                 'f1': f1}
  return evaluation</pre>
			<p>Now, we build and evaluate<a id="_idIndexMarker715"/> our baseline model. This model has no <span class="No-Break">adversarial perturbation:</span></p>
			<pre class="source-code">
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 100)
model.fit(X_train_features, Y_train)
Y_predicted = model.predict(X_test_features)
evaluation = evaluate_model(Y_test, Y_predicted)
print("Accuracy: {}".format(str(evaluation['accuracy'])))
print("Precision: {}".format(str(evaluation['precision'])))
print("Recall: {}".format(str(evaluation['recall'])))
print("F-1: {}".format(str(evaluation['f1'])))</pre>
			<p>This should result in an output <span class="No-Break">as shown:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B19327_09_06.jpg" alt="Figure 9.6 – Metrics for classification of normal data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Metrics for classification of normal data</p>
			<p>Next, we actually execute<a id="_idIndexMarker716"/> the attack. We obtain adversarial samples but train the model on the clean data (as during training we do not have access to the attacker’s adversarial set). Here, we use the <strong class="source-inline">double_last_letter()</strong> adversarial function to compute our adversarial set. We then evaluate the model on the <span class="No-Break">adversarial samples:</span></p>
			<pre class="source-code">
# Obtain adversarial samples
X_test_adversarial = double_last_letter(X_test, max_perturbations=5)
# Extract features
X_train_features, X_test_features = Extract_TF_IDF(X_train, X_test_adversarial)
# Train model
model = RandomForestClassifier(n_estimators = 100)
model.fit(X_train_features, Y_train)
# Predict on adversarial samples
Y_predicted = model.predict(X_test_features)
# Evaluate
evaluation = evaluate_model(Y_test, Y_predicted)
print("Accuracy: {}".format(str(evaluation['accuracy'])))
print("Precision: {}".format(str(evaluation['precision'])))
print("Recall: {}".format(str(evaluation['recall'])))
print("F-1: {}".format(str(evaluation['f1'])))</pre>
			<p>This should show you another set <span class="No-Break">of metrics:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B19327_09_07.jpg" alt="Figure 9.7 – Metrics for classification of adversarial data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Metrics for classification of adversarial data</p>
			<p>Carefully note the differences<a id="_idIndexMarker717"/> between the scores obtained with the clean and adversarial data; we will compare them side by side for clarity, <span class="No-Break">as follows:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Metric</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Clean</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Adversarial</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Accuracy</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.93</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.88</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Precision</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.93</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.91</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Recall</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.90</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.79</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">F-1 score</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.92</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">0.85</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 9.1 – Comparing accuracy of normal versus adversarial data</p>
			<p>You can clearly see that our adversarial attack has been successful—while accuracy dropped only by 5%, the recall dropped by 11%, causing a 7% drop in the F-1 score. If you consider this at the scale of a social media network such as Twitter, it translates to significant <span class="No-Break">performance degradation.</span></p>
			<p>You can similarly evaluate<a id="_idIndexMarker718"/> the effect caused by the double-vowel attack. Simply generate adversarial examples using the double-vowel function instead of the double-last letter function. We will leave this as an exercise to <span class="No-Break">the reader.</span></p>
			<h2 id="_idParaDest-150">Further attacks</h2>
			<p>The previous section covered only two basic attacks for attacking text-based models. As a data scientist, you must think of all possible attack surfaces and come up with potential new attacks. You are encouraged to develop and implement new attacks and examine how they affect model performance. Some potential attacks<a id="_idIndexMarker719"/> are <span class="No-Break">presented here:</span></p>
			<ul>
				<li>Combining two words from the sentence (for example, <em class="italic">Live and Let Live</em> will become <em class="italic">Liveand </em><span class="No-Break"><em class="italic">Let Live</em></span><span class="No-Break">)</span></li>
				<li>Splitting a long word into two words (for example, <em class="italic">Immigrants will take away our jobs</em> will become <em class="italic">Immi grants will take away </em><span class="No-Break"><em class="italic">our jobs</em></span><span class="No-Break">)</span></li>
				<li>Adding hyphens or commas to long words (for example, <em class="italic">Immigrants will take away our jobs</em> will become <em class="italic">Im-migrants will take away </em><span class="No-Break"><em class="italic">our jobs</em></span><span class="No-Break">)</span></li>
			</ul>
			<p>Additionally, readers should also experiment with different feature extraction methods to examine whether any of them is more robust to adversarial attacks than the others, or our TF-IDF method. A few examples of such methods are set <span class="No-Break">out here:</span></p>
			<ul>
				<li><span class="No-Break">Word embeddings</span></li>
				<li>Contextual<a id="_idIndexMarker720"/> embeddings (<strong class="bold">Bidirectional Encoder Representations from Transformers</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">BERT</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Character-level features</span></li>
			</ul>
			<p>This completes our discussion<a id="_idIndexMarker721"/> on how text models can be fooled. In the next section, we will briefly discuss how models can be made robust against <span class="No-Break">adversarial attacks.</span></p>
			<h1 id="_idParaDest-151">Developing robustness against adversarial attacks</h1>
			<p>Adversarial attacks<a id="_idIndexMarker722"/> can be a serious threat to the security and reliability of ML systems. Several techniques<a id="_idIndexMarker723"/> can be used to improve<a id="_idIndexMarker724"/> the robustness of ML models against adversarial attacks. Some of these are <span class="No-Break">described next.</span></p>
			<h2 id="_idParaDest-152">Adversarial training</h2>
			<p>Adversarial training<a id="_idIndexMarker725"/> is a technique where the model is trained on adversarial examples in addition to the original training data. Adversarial examples are generated by perturbing the original input data in such a way that the perturbed input is misclassified by the model. By training the model on both the original and adversarial examples, the model learns to be more robust to adversarial attacks. The idea behind adversarial training is to simulate the types of attacks that the model is likely to face in the real world and make the model more resistant <span class="No-Break">to them.</span></p>
			<h2 id="_idParaDest-153">Defensive distillation</h2>
			<p>Defensive distillation is a technique<a id="_idIndexMarker726"/> that involves training a model on soft targets rather than hard targets. Soft targets are probability distributions over the classes, while hard targets are one-hot vectors indicating the correct class. By training on soft targets, the decision boundaries of the model become smoother and more difficult to attack. This is because the soft targets contain more information about the distribution of the classes, which makes it more difficult to create an adversarial example that will fool <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-154">Gradient regularization</h2>
			<p>Gradient regularization is a technique<a id="_idIndexMarker727"/> that involves adding a penalty term to the loss function of the model that encourages the gradients of the model to be small. This helps to prevent an attacker from creating an adversarial example by perturbing the input in the direction of the gradient. The penalty term can be added to the loss function in various ways, such as through L1 or L2 regularization or by using adversarial training. Gradient regularization can be combined with other techniques to improve the robustness of <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-155">Input preprocessing</h2>
			<p>Input preprocessing involves modifying<a id="_idIndexMarker728"/> the input data before it is fed into the model. This can include techniques such as data normalization, which helps to reduce the sensitivity of the model to small changes in the input. Other techniques include randomization of the input, which can help to disrupt the pattern of the adversarial attack, or filtering out anomalous input that may be indicative of an adversarial attack. Input preprocessing can be tailored to the specific model and the type of input data that <span class="No-Break">it receives.</span></p>
			<h2 id="_idParaDest-156">Ensemble methods</h2>
			<p>Ensemble methods involve combining<a id="_idIndexMarker729"/> multiple models to make a prediction. This can improve the robustness of the model by making it more difficult for an attacker to craft an adversarial example that will fool all the models in the ensemble. Ensemble methods can be used in conjunction with other techniques to further improve the robustness of <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-157">Certified defenses</h2>
			<p>Certified defenses involve creating<a id="_idIndexMarker730"/> a provable guarantee that a model will be robust to a certain level of adversarial attack. This can be done using techniques such as interval-bound propagation or randomized smoothing. Interval-bound propagation involves computing a range of values that the model’s output can take given a certain range of inputs. This range can be used to create a provable bound on the robustness of the model. Randomized smoothing involves adding random noise to the input data to make the model more robust to adversarial attacks. Certified defenses are a relatively new area of research, but hold promise for creating more robust <span class="No-Break">ML models.</span></p>
			<p>It’s worth noting that while these techniques can improve the robustness of ML models against adversarial attacks, they are not foolproof, and there is still ongoing research in this area. It’s important to use multiple techniques<a id="_idIndexMarker731"/> in combination to improve the robustness of <span class="No-Break">the model.</span></p>
			<p>With that, we have come to the end of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-158">Summary</h1>
			<p>In recent times, human reliance on ML has grown exponentially. ML models are involved in several security-critical applications such as fraud, abuse, and other kinds of cybercrime. However, many models are susceptible to adversarial attacks, where attackers manipulate the input so as to fool the model. This chapter covered the basics of AML and the goals and strategies that attackers employ. We then discussed two popular adversarial attack methods, FGSM and PGD, along with their implementation in Python. Next, we learned about methods for manipulating text and <span class="No-Break">their implementation.</span></p>
			<p>Because of the importance and prevalence of ML in our lives, it is necessary for security data scientists to understand adversarial attacks and learn to defend against them. This chapter provides a solid foundation for AML and the kinds of <span class="No-Break">attacks involved.</span></p>
			<p>So far, we have discussed multiple aspects of ML for security problems. In the next chapter, we will pivot to a closely <span class="No-Break">related topic—privacy.</span></p>
		</div>
	</body></html>